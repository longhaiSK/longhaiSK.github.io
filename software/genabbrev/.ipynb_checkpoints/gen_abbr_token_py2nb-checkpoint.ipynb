{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Table of Contents\n",
    "\n",
    "# # Converting .py and .ipynb files\n",
    "\n",
    "# # Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79699ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import re\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import textwrap\n",
    "\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "DEBUG = \"streamlit\" not in hostname.lower()  # Assume cloud has \"streamlit\" in hostname\n",
    "\n",
    "\n",
    "# # Preprocessing Text with Space Inserted or Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Functions for normalizing and extracting abbrs\n",
    "\n",
    "# Code block prepared on Thursday, April 3, 2025 at 12:38:43 AM CST in Saskatoon, Saskatchewan, Canada.\n",
    "# Optional import for error display if using Streamlit\n",
    "# try:\n",
    "#     import streamlit as st\n",
    "# except ImportError:\n",
    "#     st = None # Define st as None if not available\n",
    "\n",
    "# This list is used by normalize_latex_math\n",
    "upper_greek_cmds = [\n",
    "    'Gamma', 'Delta', 'Theta', 'Lambda', 'Xi', 'Pi',\n",
    "    'Sigma', 'Upsilon', 'Phi', 'Psi', 'Omega'\n",
    "]\n",
    "\n",
    "\n",
    "# ## normalize_dollar_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fae3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Here's a summary of the functions:\n",
    "\n",
    "# normalize_dollar_spacing(text) (from artifact normalize_dollar_spacing_code)\n",
    "\n",
    "# Purpose: This function cleans up LaTeX text strings. Specifically, it looks for the dollar signs ($) used for inline math. It removes any extra whitespace found immediately after an opening $ and immediately before a closing $. This helps standardize the formatting around inline math expressions.\n",
    "\n",
    "# render_dataframe_with_latex(df) (from artifact render_df_latex_code)\n",
    "\n",
    "# Purpose: This function takes a data table (specifically, a Pandas DataFrame) that contains text with LaTeX math code in its cells. It converts this table into HTML format. Importantly, it also includes the necessary setup for the MathJax library within that HTML. The result is an HTML object that, when displayed in a compatible environment (like a Jupyter notebook or a web browser), will show the table with the LaTeX code rendered as proper mathematical symbols and equations, rather than just the raw code.\n",
    "\n",
    "# In short, one function cleans up spacing around LaTeX math delimiters in text, and the other helps display a data table containing LaTeX math correctly rendered in environments that support HTML and JavaScript.\n",
    "\n",
    "\n",
    "\n",
    "def normalize_dollar_spacing(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace immediately following an opening inline math '$' AND\n",
    "    whitespace immediately preceding a closing inline math '$'.\n",
    "    Handles escaped '\\$'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string potentially containing LaTeX.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed string.\n",
    "    \"\"\"\n",
    "    processed_chars = []\n",
    "    in_math_mode = False\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while i < n:\n",
    "        char = text[i]\n",
    "\n",
    "        # Check for escaped dollar sign or backslash first\n",
    "        if char == '\\\\' and i + 1 < n:\n",
    "            # Keep backslash and the next character (e.g., '\\$' or '\\\\')\n",
    "            processed_chars.append(char)\n",
    "            processed_chars.append(text[i+1])\n",
    "            i += 2 # Skip both characters\n",
    "            continue\n",
    "\n",
    "        # Check for unescaped dollar sign\n",
    "        if char == '$':\n",
    "            if not in_math_mode:\n",
    "                # --- This is an OPENING dollar sign ---\n",
    "                processed_chars.append(char) # Keep the opening dollar\n",
    "                in_math_mode = True\n",
    "                # Check if the next characters are whitespace and skip them\n",
    "                j = i + 1\n",
    "                while j < n and text[j].isspace():\n",
    "                    j += 1\n",
    "                # Advance 'i' past the dollar and the skipped whitespace\n",
    "                i = j\n",
    "                continue # Continue to next iteration\n",
    "            else:\n",
    "                # --- This is a CLOSING dollar sign ---\n",
    "                in_math_mode = False\n",
    "                # Remove any trailing whitespace added just before this closing '$'\n",
    "                while processed_chars and processed_chars[-1].isspace():\n",
    "                    processed_chars.pop()\n",
    "                processed_chars.append(char) # Append the closing dollar\n",
    "                # Advance 'i' past the dollar for the next iteration\n",
    "                i += 1\n",
    "                continue # Continue to next iteration\n",
    "        else:\n",
    "            # Any other character\n",
    "            processed_chars.append(char)\n",
    "            i += 1 # Advance 'i' past the character\n",
    "\n",
    "    return \"\".join(processed_chars)\n",
    "\n",
    "\n",
    "# ## Normalization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Normalization Function ---\n",
    "def normalize_latex_math(text):\n",
    "    \"\"\"\n",
    "    Preprocesses LaTeX text:\n",
    "    1. Converts LaTeX inline math \\( ... \\) to $ ... $.\n",
    "    2. Removes LaTeX comments (% to end of line), respecting \\%.\n",
    "    3. Removes preamble/end tags if \\begin{document} is found.\n",
    "    4a. Adds space BEFORE and AFTER opening curly braces ({).\n",
    "    4b. Adds space BEFORE and AFTER closing curly braces (}).\n",
    "    5. Adds space after specific uppercase Greek commands (\\Cmd) if not present. (Note: Using corrected pattern)\n",
    "    6. Adds space after lowercase LaTeX commands (\\cmd) if not already present. (Note: Pattern may be restrictive)\n",
    "    7. Removes whitespace immediately following $. (Moved Step)\n",
    "    8. Cleans up extra blank lines and trims whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        print(\"Warning: Input to normalize_latex_math was not a string.\")\n",
    "        return text\n",
    "\n",
    "    processed_text = text\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        # 0. Remove space inside $ $\n",
    "        processed_text =  normalize_dollar_spacing(processed_text)\n",
    "\n",
    "        # 1. Normalize math \\(...\\) to $...$\n",
    "        processed_text = re.sub(\n",
    "            r'\\\\\\(\\s*(.*?)\\s*\\\\\\)',\n",
    "            lambda match: f\"${match.group(1).strip()}$\",\n",
    "            processed_text\n",
    "        )\n",
    "\n",
    "        # 2. Remove LaTeX comment lines (respects \\%)\n",
    "        processed_text = re.sub(r'(?<!\\\\)%.*$', '', processed_text, flags=re.MULTILINE)\n",
    "\n",
    "        # 3. Remove preamble IF \\begin{document} exists\n",
    "        begin_doc_marker = r'\\begin{document}'\n",
    "        begin_doc_index = processed_text.find(begin_doc_marker)\n",
    "        if begin_doc_index != -1:\n",
    "            processed_text = processed_text[begin_doc_index + len(begin_doc_marker):]\n",
    "        # 3b. Remove \\end{document} if present near the end\n",
    "        end_doc_marker = r'\\end{document}'\n",
    "        end_doc_index = processed_text.rfind(end_doc_marker)\n",
    "        if end_doc_index != -1 and len(processed_text) - end_doc_index < 30: # Heuristic check\n",
    "            processed_text = processed_text[:end_doc_index]\n",
    "\n",
    "        # --- Spacing Adjustments ---\n",
    "        # 4a. Add space BEFORE and AFTER { (handles existing spaces robustly)\n",
    "        processed_text = re.sub(r'\\s*\\{\\s*', r' { ', processed_text)\n",
    "        # 4b. Add space BEFORE and AFTER } (handles existing spaces robustly)\n",
    "        processed_text = re.sub(r'\\s*\\}\\s*', r' } ', processed_text)\n",
    "        # 4c. Add space BEFORE ( (handles no space before ()\n",
    "        processed_text = re.sub(r'\\s*\\(', r' (', processed_text)\n",
    "        \n",
    "        # 5. Add space after specific uppercase Greek commands (\\Cmd) if not followed by space\n",
    "        pattern_part = '|'.join(upper_greek_cmds)\n",
    "        # Using corrected pattern (no space after \\\\)\n",
    "        pattern_upper = rf'(\\\\({pattern_part}))(?!\\s)'\n",
    "        processed_text = re.sub(pattern_upper, r'\\1 ', processed_text)\n",
    "\n",
    "        # 6. Add space after lowercase commands (\\cmd) if not followed by specific pattern\n",
    "        # !!! Note: This pattern (?=[A-Z][^a-z]) might be too restrictive.\n",
    "        processed_text = re.sub(r'(\\\\[a-z]+)(?=[A-Z][^a-z])', r'\\1 ', processed_text)\n",
    "\n",
    "\t\t# 7. Remove one or more whitespace characters (\\s+) immediately after a dollar sign ($) (Moved Step)\n",
    "        #processed_text = re.sub(r'\\$\\s+', '$', processed_text)\n",
    "\n",
    "        # 8. Clean up potential excessive blank lines and trim overall whitespace\n",
    "        processed_text = re.sub(r'(\\n\\s*){2,}', '\\n', processed_text) # Collapse blank lines\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text) # Collapse blank lines\n",
    "        \n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during LaTeX text preprocessing: {e}\"\n",
    "        try:\n",
    "            # Attempt to use streamlit for error display if available\n",
    "            import streamlit as st\n",
    "            st.error(error_message)\n",
    "        except ImportError:\n",
    "            # Fallback to print if streamlit is not available\n",
    "            print(error_message)\n",
    "        return text # Return original text on error\n",
    "\n",
    "\n",
    "# # Finding Matching\n",
    "\n",
    "# ## KNOWN_COMMAND_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Expanded KNOWN_COMMAND_NAMES Set ---\n",
    "\n",
    "KNOWN_COMMAND_NAMES = {\n",
    "    # Lowercase Greek\n",
    "    'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'eta', 'theta',\n",
    "    'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi', 'omicron', 'pi', 'rho',\n",
    "    'sigma', 'tau', 'upsilon', 'phi', 'chi', 'psi', 'omega',\n",
    "\n",
    "    # Uppercase Greek (Ensure exact case matching LaTeX)\n",
    "    'Gamma', 'Delta', 'Theta', 'Lambda', 'Xi', 'Pi', 'Sigma', 'Upsilon',\n",
    "    'Phi', 'Psi', 'Omega',\n",
    "\n",
    "    # --- NEWLY ADDED COMMON MATH/SCIENCE COMMANDS ---\n",
    "\n",
    "    # Common Functions\n",
    "    'sin', 'cos', 'tan', 'csc', 'sec', 'cot',\n",
    "    'arcsin', 'arccos', 'arctan',\n",
    "    'sinh', 'cosh', 'tanh',\n",
    "    'log', 'ln', 'exp',\n",
    "    'sqrt',\n",
    "    'frac', # Often takes arguments, but name itself is semantic\n",
    "\n",
    "    # Limits / Bounds / Operators\n",
    "    'lim', 'max', 'min', 'sup', 'inf',\n",
    "    'det', 'dim', 'deg', # Degree symbol often ^\\circ, but deg exists\n",
    "    'gcd', 'hom', 'ker', 'Pr', # Probability\n",
    "\n",
    "    # Large Operators (Symbols but represent operation)\n",
    "    'sum', 'prod', 'int', 'oint', # Integral variants\n",
    "    'bigcup', 'bigcap', 'bigvee', 'bigwedge',\n",
    "\n",
    "    # Calculus / Vector Calculus\n",
    "    'partial', 'nabla', 'mathrm', # d in mathrm often used for derivative/integral dx\n",
    "\n",
    "    # Symbols with semantic meaning often abbreviated\n",
    "    'infty', # Infinity\n",
    "    'prime', # As in f'\n",
    "\n",
    "    # Common Logic/Set Theory (Add more if needed for your domain)\n",
    "    'forall', 'exists', 'in', 'notin', 'subset', 'supset',\n",
    "    # 'subseteq', 'supseteq', 'cup', 'cap', 'vee', 'wedge',\n",
    "\n",
    "    # Common Relations (Add more if needed)\n",
    "    'leq', 'geq', 'equiv', 'approx', 'propto', 'sim', 'simeq',\n",
    "\n",
    "    # Other Potentials (Consider based on your specific field)\n",
    "    # 'vec', 'hat', 'bar', 'tilde', # Accents - Treat by name or first letter? (Leaving out for now)\n",
    "    # 'mathbb', 'mathcal', 'mathbf', 'mathit', # Fonts - Usually formatting (Leaving out)\n",
    "    # 'text', # Usually wraps non-math text (Leaving out)\n",
    "\n",
    "    # Common Operators with names (less likely abbreviation targets?)\n",
    "    'pm', 'mp', 'times', 'div', 'cdot', 'ast', 'star',\n",
    "}\n",
    "\n",
    "# --- Reminder: How this set is used by the helper functions ---\n",
    "\n",
    "# In get_letters_abbrs(abbr_string):\n",
    "#   If a command like `\\sqrt` is found, and 'sqrt' is in KNOWN_COMMAND_NAMES,\n",
    "#   the comparable item added is 'sqrt'.\n",
    "#   If a command like `\\textbf` is found, and 'textbf' is NOT in KNOWN_COMMAND_NAMES,\n",
    "#   the comparable item added is 't'.\n",
    "\n",
    "# In get_letters_words(word, debug=False):\n",
    "#   If a word starts with `\\sqrt`, and 'sqrt' is in KNOWN_COMMAND_NAMES,\n",
    "#   this function returns 'sqrt'.\n",
    "#   If a word starts with `\\textbf{Word}`, and 'textbf' is NOT in KNOWN_COMMAND_NAMES,\n",
    "#   this function proceeds to find the first letter of the content ('w').\n",
    "\n",
    "\n",
    "# ##  Get Abbr Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ed16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- REVISED get_letters_abbrs (Handles consecutive numbers \\d+) ---\n",
    "def get_letters_abbrs(abbr_string):\n",
    "    \"\"\"\n",
    "    Parses abbreviation string component by component using finditer.\n",
    "    - Known commands (\\\\alpha) -> comparable name ('alpha').\n",
    "    - Unknown commands (\\\\textbf) -> first letter lowercase ('t').\n",
    "    - Letter-Number combinations (N1) -> comparable string ('n1').\n",
    "    - Uppercase letters (G) -> first letter lowercase ('g').\n",
    "    - Standalone lowercase letters/words (a, word) -> first letter ('a', 'w').\n",
    "    - Consecutive Digits (2025) -> digit string ('2025'). # MODIFIED RULE\n",
    "    Returns list of comparables AND list of corresponding original segments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (representative_items, original_parts)\n",
    "               - representative_items: list (e.g., 'alpha', 't', 'g', 'm', 'n1', '2025')\n",
    "               - original_parts: list of original strings (e.g., '\\\\alpha', '\\\\textbf', 'G', 'M', 'N1', '2025')\n",
    "    \"\"\"\n",
    "    representative_items = []\n",
    "    original_parts = []\n",
    "    # --- MODIFIED REGEX: Changed (\\d) to (\\d+) for Group 6 ---\n",
    "    # Regex captures: \\cmd | CapNum | Upper | OptionalTrailingLower | StandaloneLower | Num+\n",
    "    regex_pattern = r'(\\\\[a-zA-Z]+)|([A-Z][0-9]+)|([A-Z])([a-z]+)?|([a-z])(?:[a-z]*)|(\\d+)'\n",
    "    # Groups:      (1:Cmd)        (2:CapNum)     (3:Upper)(4:TrailLow) (5:StdLow)       (6:Num+)\n",
    "\n",
    "    for match_obj in re.finditer(regex_pattern, abbr_string):\n",
    "         original_segment = match_obj.group(0) # The whole segment matched\n",
    "         command = match_obj.group(1)          # Group 1: e.g., \\alpha\n",
    "         cap_num = match_obj.group(2)          # Group 2: e.g., N1\n",
    "         upper = match_obj.group(3)              # Group 3: e.g., G\n",
    "         # trailing_lower = match_obj.group(4) # Group 4 - Ignored\n",
    "         standalone_lower = match_obj.group(5) # Group 5: e.g., p in protein\n",
    "         number_seq = match_obj.group(6)       # Group 6: e.g., 2025\n",
    "\n",
    "         current_repr_item = None # Use None to track if set\n",
    "         current_orig_part = original_segment # Store the originally matched segment\n",
    "\n",
    "         if command:\n",
    "             command_name = command[1:]\n",
    "             if command_name in KNOWN_COMMAND_NAMES:\n",
    "                 current_repr_item = command_name.lower() # Known command -> name\n",
    "             else:\n",
    "                 current_repr_item = command_name[0].lower() # Unknown command -> first letter\n",
    "             current_orig_part = command # Original part is the command itself\n",
    "\n",
    "         elif cap_num: # Handle N1, H2 etc.\n",
    "             current_repr_item = cap_num.lower() # Use 'n1', 'h2' lowercase\n",
    "             current_orig_part = cap_num # Original part is 'N1'\n",
    "\n",
    "         elif upper: # Handle G, L, M, or R in Rsp etc.\n",
    "             current_repr_item = upper.lower() # Use first letter only\n",
    "             current_orig_part = original_segment # Original part includes trailing lower if present (e.g. \"Rsp\")\n",
    "\n",
    "         elif standalone_lower: # Handle 'a', or first letter of 'protein'\n",
    "             current_repr_item = standalone_lower # Group 5 only captures first letter\n",
    "             current_orig_part = original_segment # Original part is the full lowercase word\n",
    "\n",
    "         elif number_seq: # Handle '2025' etc.\n",
    "             # --- MODIFIED LOGIC: Use the whole number sequence ---\n",
    "             current_repr_item = number_seq # Use the full number string '2025'\n",
    "             current_orig_part = number_seq # Original part is the number string\n",
    "             # --- END MODIFICATION ---\n",
    "\n",
    "         # Append only if a representative item was derived\n",
    "         if current_repr_item is not None:\n",
    "             representative_items.append(current_repr_item)\n",
    "             original_parts.append(current_orig_part)\n",
    "\n",
    "    return representative_items, original_parts\n",
    "\n",
    "if (False):\n",
    "# --- Example Usage ---\n",
    "    print(\"--- Testing get_letters_abbrs (Corrected for Consecutive Numbers) ---\")\n",
    "    abbr1 = \"GLM\"\n",
    "    abbr2 = r\"\\alpha SP\"\n",
    "    abbr3 = \"BFN1\"\n",
    "    abbr4 = \"Version 3\" # Should yield v, 3\n",
    "    abbr5 = \"H2O\"\n",
    "    abbr6 = \"Li et al. 2025\" # Should yield l, e, a, l, 2025\n",
    "    \n",
    "    abbreviations_to_test = [abbr1, abbr2, abbr3, abbr4, abbr5, abbr6]\n",
    "    \n",
    "    for abbr_str in abbreviations_to_test:\n",
    "        comparable_items, original_segments = get_letters_abbrs(abbr_str)\n",
    "        print(f\"Input:  '{abbr_str}'\")\n",
    "        print(f\"  -> Comparables: {comparable_items}\")\n",
    "        print(f\"  -> Originals:   {original_segments}\")\n",
    "        print(\"-\" * 20)\n",
    "    \n",
    "\n",
    "\n",
    "# ## Get Words Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assume KNOWN_COMMAND_NAMES set is defined\n",
    "\n",
    "def get_letters_words(word: str, debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Derives a comparable unit (lowercase) from a word token.\n",
    "    - If the word is primarily a known LaTeX command -> returns command name ('alpha').\n",
    "    - For other words (including unknown commands like \\\\textbf{Word}) ->\n",
    "      returns the single effective first letter ('g' for 'Generalized', 'w' for Word).\n",
    "    - Returns '' for separators or unprocessable words.\n",
    "    \"\"\"\n",
    "    original_word = word; word = word.strip()\n",
    "    if not word: return ''\n",
    "\n",
    "    try:\n",
    "        # 1. Check if it starts with a LaTeX command\n",
    "        command_match = re.match(r'\\$?(\\\\[a-zA-Z]+)', word)\n",
    "        if command_match:\n",
    "            command_name = command_match.group(1)[1:]\n",
    "            # --- MODIFIED Logic: Only return name if KNOWN ---\n",
    "            if command_name in KNOWN_COMMAND_NAMES:\n",
    "                 # If known command, return its name lowercase\n",
    "                 return command_name.lower()\n",
    "            # --- If it's an unknown command (\\textbf etc.), DON'T return here.\n",
    "            # Let processing continue below to find first letter of content/command name itself.\n",
    "\n",
    "        # 2. Find first effective letter (applies to regular words AND unknown commands)\n",
    "        word_to_check = word\n",
    "        # Strip leading command \\cmd{ or \\cmd (handles unknown like \\textbf{ )\n",
    "        word_to_check = re.sub(r'^\\s*\\\\([a-zA-Z]+)\\s*\\{?', '', word_to_check)\n",
    "        # Strip other common leading junk more aggressively\n",
    "        word_to_check = word_to_check.lstrip(' ${}')\n",
    "        # Strip trailing brace more aggressively\n",
    "        if word_to_check.endswith('}'): word_to_check = word_to_check[:-1].rstrip()\n",
    "\n",
    "        # Find first letter in remaining text OR in the command name if that's all left\n",
    "        first_letter_match = re.search(r'[a-zA-Z]', word_to_check)\n",
    "        if first_letter_match:\n",
    "            return first_letter_match.group(0).lower()\n",
    "        # --- ADDED: Fallback specifically for unknown commands where content was empty ---\n",
    "        elif command_match: # If we detected an unknown command earlier but found no letter in content\n",
    "             unknown_command_name = command_match.group(1)[1:]\n",
    "             return unknown_command_name[0].lower() # Use first letter of the command itself\n",
    "        # --- END ADDED Fallback ---\n",
    "\n",
    "\n",
    "        # Fallback: Check original word directly if others failed\n",
    "        fallback_match_orig = re.search(r'[a-zA-Z]', original_word)\n",
    "        if fallback_match_orig: return fallback_match_orig.group(0).lower()\n",
    "\n",
    "        return '' # No letter found\n",
    "\n",
    "    except Exception as e:\n",
    "        # if debug: print(f\"      Error in get_letters_words for '{original_word}': {e}\") # Keep minimal\n",
    "        # Fallback on error\n",
    "        fallback_match = re.search(r'[a-zA-Z]', original_word)\n",
    "        return fallback_match.group(0).lower() if fallback_match else ''\n",
    "\n",
    "\n",
    "# ## find_abbreviation_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e968bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_abbreviation_matches(words_ahead, abbr_string, debug=True):\n",
    "    \"\"\"\n",
    "    Performs backward matching using startswith comparison logic.\n",
    "    Calculates and returns match indices, abbreviation match ratio,\n",
    "    and matched words ratio within the identified phrase range.\n",
    "    If debug=True, prints debug information including a final DataFrame.\n",
    "\n",
    "    Args:\n",
    "        words_ahead (list): Word tokens from the definition part.\n",
    "        abbr_string (str): The original abbreviation string.\n",
    "        debug (bool): Flag to enable console debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (match_indices, match_ratio, perc_words_matched)\n",
    "               - match_indices: list mapping abbr_idx -> word_idx (-1 if no match).\n",
    "               - match_ratio: float, fraction of matched abbr items (0.0 to 1.0).\n",
    "               - perc_words_matched: float, fraction of 'real' words in the matched\n",
    "                 phrase range (from first match to end) that were matched by an abbr item.\n",
    "               Returns ([], 0.0, 0.0) if abbreviation parsing fails or yields no items.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abbr_items, original_abbr_parts = get_letters_abbrs(abbr_string)\n",
    "        num_abbr_items = len(abbr_items)\n",
    "        if not abbr_items:\n",
    "             if debug: print(f\"Warning: Abbr string '{abbr_string}' yielded no items.\")\n",
    "             return [], 0.0, 0.0\n",
    "    except Exception as e_parse:\n",
    "         if debug: print(f\"Error parsing abbr string '{abbr_string}': {e_parse}.\")\n",
    "         return [], 0.0, 0.0\n",
    "\n",
    "    num_words = len(words_ahead)\n",
    "    match_indices = [-1] * num_abbr_items\n",
    "    last_matched_index = num_words\n",
    "    words_ahead_comparables = [get_letters_words(word, debug=False) for word in words_ahead]\n",
    "\n",
    "    if debug:\n",
    "        # Print initial state for debugging\n",
    "        print(\"\\n\" + (\"=\" * 100))\n",
    "        #print(f\"Starting find_abbreviation_matches for ('{abbr_string}')\\n\")\n",
    "        # print(f\"  Words Ahead ({num_words}): {words_ahead}\")\n",
    "        # print(f\"  Words Ahead Comparables ({len(words_ahead_comparables)}): {words_ahead_comparables}\")\n",
    "        print(f\"  Abbr Items (Letter/CmdName) ({num_abbr_items}): {abbr_items}\")\n",
    "        # print(f\"  Original Abbr Parts ({len(original_abbr_parts)}): {original_abbr_parts}\")\n",
    "        # # Removed threshold print\n",
    "        # print(\"-\" * 20)\n",
    "\n",
    "    # --- Matching Loop ---\n",
    "    for abbr_idx in range(num_abbr_items - 1, -1, -1):\n",
    "        target_comparable = abbr_items[abbr_idx]; match_found_for_abbr = False\n",
    "        if not target_comparable: continue\n",
    "        for word_idx in range(last_matched_index - 1, -1, -1):\n",
    "            word_comparable = words_ahead_comparables[word_idx]\n",
    "            if not word_comparable: continue\n",
    "            current_match_found = word_comparable.startswith(target_comparable)\n",
    "            if current_match_found:\n",
    "                match_indices[abbr_idx] = word_idx; last_matched_index = word_idx; match_found_for_abbr = True;\n",
    "                # Removed verbose internal prints\n",
    "                break\n",
    "    # --- END Matching Loop ---\n",
    "\n",
    "    # --- Calculate Results ---\n",
    "    successful_match_indices = [idx for idx in match_indices if idx != -1]\n",
    "    count_matched = len(successful_match_indices)\n",
    "    match_ratio = count_matched / num_abbr_items if num_abbr_items > 0 else 0.0\n",
    "    perc_words_matched = 0.0; matchable_words_in_range = 0\n",
    "    if successful_match_indices:\n",
    "        start_range_idx = min(successful_match_indices)\n",
    "        if 0 <= start_range_idx < num_words:\n",
    "            for i in range(start_range_idx, num_words):\n",
    "                if words_ahead_comparables[i]: matchable_words_in_range += 1\n",
    "            if matchable_words_in_range > 0: perc_words_matched = min(count_matched / matchable_words_in_range, 1.0)\n",
    "    # --- END Calculate Results ---\n",
    "\n",
    "    # --- Debugging Output Section ---\n",
    "    if debug:\n",
    " \n",
    "        # Keep calculation and printing of the final debug DataFrame\n",
    "        matched_abbrs_string = [''] * num_words; matched_abbrs_comparable = [''] * num_words\n",
    "        for i, w_idx in enumerate(match_indices):\n",
    "            if w_idx != -1 and 0 <= w_idx < num_words:\n",
    "                if 0 <= i < len(abbr_items): matched_abbrs_comparable[w_idx] = abbr_items[i]\n",
    "                if 0 <= i < len(original_abbr_parts): matched_abbrs_string[w_idx] = original_abbr_parts[i]\n",
    "        #print(\"\\n  Matching Details (Debug DataFrame):\")\n",
    "        try:\n",
    "            debug_data = {'Words Ahead': words_ahead,'Words Ahead Comparables': words_ahead_comparables,'Matched Abbrs (String)': matched_abbrs_string,'Matched Abbrs (Comparable)': matched_abbrs_comparable}\n",
    "            if all(len(lst) == num_words for lst in debug_data.values()):\n",
    "                df_debug = pd.DataFrame(debug_data); print(f\"\\n  Matching Result (Rows: Words, Comparables, MatchOrig, MatchComp):\\n{textwrap.indent(df_debug.T.to_string(), '    ')}\")\n",
    "            else: print(\"\\n  [DEBUG] Error: Length mismatch for debug DataFrame.\")\n",
    "        except Exception as e_debug: print(f\"\\n  [DEBUG] Error creating debug DataFrame: {e_debug}\")\n",
    "        #print(\"--- Ending find_abbreviation_matches ---\\n\")\n",
    "        \n",
    "       # Print calculated results\n",
    "        print(f\"\\nMatching Complete: Matched {count_matched}/{num_abbr_items} items.\")\n",
    "        print(f\"  Abbr Match Ratio: {match_ratio:.2f}\")\n",
    "        print(f\"  Words Matched Ratio (in range): {perc_words_matched:.2f}\")\n",
    "        print(f\"  Final match indices (abbr_idx -> word_idx): {match_indices}\"+\"\\n\") # Keep this useful index map\n",
    "\n",
    "    # --- END Debugging Output Section ---\n",
    "\n",
    "    # --- Return the results ---\n",
    "    return match_indices, match_ratio, perc_words_matched # Return tuple\n",
    "\n",
    "\n",
    "# # Extracting Abbreviations\n",
    "\n",
    "# ## Collect_abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a31a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "# Assumes find_abbreviation_matches function is defined elsewhere\n",
    "\n",
    "# --- collect_abbreviations Function (Syntax Corrected) ---\n",
    "\n",
    "def collect_abbreviations(text, debug=False):\n",
    "    \"\"\"\n",
    "    Finds all potential abbreviation candidates Def (Abbr), calculates usage,\n",
    "    calls find_abbreviation_matches to get matching details (indices and ratios),\n",
    "    and reconstructs potential full name.\n",
    "    Does NOT filter based on match ratio or usage count.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text (ideally normalized).\n",
    "        debug (bool): Flag to enable detailed debug printing in this function\n",
    "                      and called functions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all candidates with columns:\n",
    "                      'abbreviation', 'full_name', 'usage_count',\n",
    "                      'perc_abbr_matches', 'perc_words_matched'.\n",
    "                      Returns empty DataFrame if no candidates found.\n",
    "    \"\"\"\n",
    "    # Initial pattern find candidates\n",
    "    #pattern = r'((?:[\\w\\\\\\$\\{\\}]+[ -]+){1,10}(?:[\\w\\\\\\$\\{\\}]+)[ -]?)\\(([^\\(\\)]*[a-zA-Z0-9]{2,}[^\\(\\)]*)\\)'\n",
    "    pattern = r'((?:[\\w\\\\\\$\\{\\}]+[ -]+){1,10}(?:[\\w\\\\\\$\\{\\}]+)[ -]?)\\((?=[^\\(\\)\\,]*[A-Z])([^\\(\\)\\,]*[a-zA-Z0-9]{2,}[^\\(\\)\\,]*)\\)'\n",
    "#                                                                    ^^^^^^^^^^^^^^^^^^^  <-- Added Positive Lookahead\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Define expected columns for the output DataFrame\n",
    "    required_columns = ['abbreviation', 'full_name', 'usage_count', 'perc_abbr_matches', 'perc_words_matched']\n",
    "    empty_df = pd.DataFrame(columns=required_columns)\n",
    "\n",
    "    if not matches:\n",
    "        # Return empty DataFrame early if no initial regex matches found\n",
    "        # if debug: print(\"collect_abbreviations: No potential candidates found by initial regex.\") # Keep debug minimal\n",
    "        return empty_df\n",
    "\n",
    "    # if debug: print(f\"collect_abbreviations: Found {len(matches)} potential candidates.\") # Keep debug minimal\n",
    "\n",
    "    # Calculate usage counts first\n",
    "    abbr_usage_count = {}\n",
    "    all_potential_abbrs = [match[1].strip() for match in matches]\n",
    "    for abbr in set(all_potential_abbrs):\n",
    "        abbr_search_string = re.sub(r'[\\(\\)]','',abbr) # Basic cleaning\n",
    "        # Using lookaround pattern for usage count robustness\n",
    "        aup = rf'(?<![a-zA-Z\\(\\)]){re.escape(abbr_search_string)}(?![a-zA-Z\\)\\)])'\n",
    "        try:\n",
    "            abbr_usage_count[abbr] = len(re.findall(aup,text))\n",
    "        except re.error as e:\n",
    "            # --- CORRECTED INDENTATION ---\n",
    "            if debug: # Indented\n",
    "                print(f\"  Regex error counting usage for '{abbr}': {e}\")\n",
    "            abbr_usage_count[abbr] = 0 # Indented - Set count to 0 on error\n",
    "            # --- END CORRECTION ---\n",
    "    # if debug: print(f\"  Usage Counts Calculated: {len(abbr_usage_count)} unique abbreviations.\") # Keep debug minimal\n",
    "\n",
    "    # Process each candidate found by the initial regex\n",
    "    all_candidate_data = []\n",
    "    for match_idx, match in enumerate(matches):\n",
    "        words_before_abbr_text = match[0].strip()\n",
    "        abbr_string = match[1].strip()\n",
    "        current_usage_count = abbr_usage_count.get(abbr_string, 0) # Get pre-calculated count\n",
    "\n",
    "        # if debug: print(f\"\\n--- Collecting Candidate {match_idx+1}: Abbr='{abbr_string}' ---\") # Keep debug minimal\n",
    "\n",
    "        # Tokenize words before abbreviation\n",
    "        split_pattern = r'([ -]+)'; split_list = re.split(split_pattern, words_before_abbr_text);\n",
    "        words_ahead = [item for item in split_list if item]\n",
    "\n",
    "        # Initialize results for this candidate\n",
    "        full_name = \"\"; match_ratio = 0.0; perc_words = 0.0; match_indices = []\n",
    "\n",
    "        if words_ahead:\n",
    "            if (debug): \n",
    "                print (\"=\" * 100)\n",
    "                print(\"\\n\" + f\"Starting the matching process for Candidate {match_idx}\" + \"\\n\")\n",
    "                print(\" \" * 10 + f\" {words_before_abbr_text} ({abbr_string})\")           \n",
    "                # Call find_abbreviation_matches to get indices and ratios\n",
    "            # Assumes find_abbreviation_matches is defined and returns tuple(list, float, float)\n",
    "            try:\n",
    "                 match_indices, ratio_result, perc_words_result = find_abbreviation_matches(\n",
    "                    words_ahead,\n",
    "                    abbr_string,\n",
    "                    debug=debug # Pass debug flag down\n",
    "                )\n",
    "                 match_ratio = ratio_result\n",
    "                 perc_words = perc_words_result\n",
    "            except NameError as e_find:\n",
    "                 if debug: print(f\"  Error: find_abbreviation_matches function not defined? {e_find}\")\n",
    "                 # Keep defaults (empty name, zero ratios)\n",
    "            except Exception as e_find_other:\n",
    "                 if debug: print(f\"  Error calling find_abbreviation_matches: {e_find_other}\")\n",
    "                 # Keep defaults\n",
    "\n",
    "            # Reconstruct name based on match_indices (even if ratio is low)\n",
    "            if match_indices: # Check if list is not empty (find_... returns [] on error/no parse)\n",
    "                successful_match_indices = [idx for idx in match_indices if idx != -1]\n",
    "                if successful_match_indices:\n",
    "                    min_idx_py = min(successful_match_indices)\n",
    "                    # Slice from first matched word to end\n",
    "                    if 0 <= min_idx_py < len(words_ahead):\n",
    "                        full_phrase_words_slice=words_ahead[min_idx_py:]\n",
    "                        fn=''.join(full_phrase_words_slice).strip()\n",
    "                        full_name=fn\n",
    "                    # elif debug: print(f\"  Warning: Invalid min_idx {min_idx_py} for reconstruction.\") # Keep debug minimal\n",
    "        # elif debug: print(\"  Note: No words found before abbreviation for reconstruction.\") # Keep debug minimal\n",
    "\n",
    "        # Append data for every candidate\n",
    "        all_candidate_data.append({\n",
    "            'abbreviation': abbr_string,\n",
    "            'full_name': full_name,\n",
    "            'usage_count': current_usage_count,\n",
    "            'perc_abbr_matches': match_ratio,\n",
    "            'perc_words_matched': perc_words\n",
    "        })\n",
    "        # if debug: print(f\"  Collected: AbbrRatio={match_ratio:.2f}, WordRatio={perc_words:.2f}, Usage={current_usage_count}, Name='{full_name}'\") # Keep debug minimal\n",
    "\n",
    "    # Create the final DataFrame from all collected data\n",
    "    if not all_candidate_data:\n",
    "        return empty_df # Return empty DF with correct columns\n",
    "    else:\n",
    "        collected_df = pd.DataFrame(all_candidate_data)\n",
    "        # Ensure columns exist and are in the desired order before returning\n",
    "        for col in required_columns:\n",
    "            if col not in collected_df.columns:\n",
    "                 # Add missing column - should only happen if upstream changes break expected output\n",
    "                 if 'perc' in col: collected_df[col] = 0.0\n",
    "                 elif 'usage' in col: collected_df[col] = 0\n",
    "                 else: collected_df[col] = \"\"\n",
    "        return collected_df[required_columns]\n",
    "\n",
    "\n",
    "# ## Select Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 5.2 select_abbreviations ---\n",
    "def select_abbreviations(\n",
    "    collected_df,\n",
    "    threshold_perc_abbr_matches=0.7,\n",
    "    threshold_usage=0,\n",
    "    threshold_perc_words_matched=0.0, # Added threshold for new metric\n",
    "    debug=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame of abbreviation candidates based on criteria.\n",
    "    DOES NOT SORT or add/remove display columns like 'Row No.'.\n",
    "\n",
    "    Args:\n",
    "        collected_df (pd.DataFrame): DataFrame from collect_abbreviations.\n",
    "        threshold_perc_abbr_matches (float): Min abbr match ratio required.\n",
    "        threshold_usage (int): Min usage count required.\n",
    "        threshold_perc_words_matched (float): Min word match ratio required.\n",
    "        debug(bool): Enable printing status messages.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with original columns from collect_abbreviations.\n",
    "                      Returns empty DataFrame if none meet criteria or input invalid.\n",
    "    \"\"\"\n",
    "    # Define expected columns from input/output of this stage\n",
    "    final_columns = ['abbreviation', 'full_name', 'usage_count', 'perc_abbr_matches', 'perc_words_matched']\n",
    "    empty_df = pd.DataFrame(columns=final_columns)\n",
    "\n",
    "    if not isinstance(collected_df, pd.DataFrame) or collected_df.empty:\n",
    "        if debug: print(\"select_abbreviations: Input DataFrame empty/invalid.\")\n",
    "        return empty_df\n",
    "    required_cols = final_columns\n",
    "    if not all(col in collected_df.columns for col in required_cols):\n",
    "         if debug: print(f\"select_abbreviations: Input missing required columns.\"); return empty_df\n",
    "\n",
    "    #if debug: print(f\"\\n--- Starting select_abbreviations ---\\n Input: {len(collected_df)}\\n Filters: AbbrRatio>={threshold_perc_abbr_matches}, Usage>={threshold_usage}, WordRatio>={threshold_perc_words_matched}\")\n",
    "\n",
    "    # Apply Filters (including new one)\n",
    "    filtered_df = collected_df[\n",
    "        (collected_df['perc_abbr_matches'] >= threshold_perc_abbr_matches) &\n",
    "        (collected_df['usage_count'] >= threshold_usage) &\n",
    "        (collected_df['perc_words_matched'] >= threshold_perc_words_matched) # Filter by word match %\n",
    "    ].copy()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        if debug: print(f\"  Filtering Result: No abbreviations met criteria.\")\n",
    "        return empty_df # Return empty DF with correct columns\n",
    "    #if debug: print(f\"  Filtering Result: {len(filtered_df)} passed criteria.\\n--- Ending select_abbreviations ---\")\n",
    "\n",
    "    # Return the filtered DataFrame (unsorted, contains all collected columns)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# # Formatting abbrs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 6. Formatting abbrs ---\n",
    "\n",
    "def format_abbreviations(abbr_df, format_type):\n",
    "     \"\"\"\n",
    "     Formats selected DataFrame for export (e.g., plain text, LaTeX table).\n",
    "     Ignores columns like percentages before formatting.\n",
    "     Assumes input abbr_df is the final filtered (and possibly sorted) data.\n",
    "     \"\"\"\n",
    "     # Drop columns not typically needed for final export format\n",
    "     cols_to_drop = [col for col in ['Row No.', 'perc_abbr_matches', 'perc_words_matched'] if col in abbr_df.columns]\n",
    "     df_to_format = abbr_df.drop(columns=cols_to_drop)\n",
    "\n",
    "     if df_to_format.empty: return \"No abbreviations selected.\"\n",
    "\n",
    "     # Generate output based on format_type\n",
    "     if format_type == \"tabular\":\n",
    "         # Use f-strings and ensure proper escaping for LaTeX special chars in content if needed\n",
    "         header = \"\\\\begin{tabular}{ll}\\n\\\\hline\\n\\\\textbf{Abbreviation} & \\\\textbf{Full Name} \\\\\\\\\\n\\\\hline\\n\"\n",
    "         footer = \"\\\\hline\\n\\\\end{tabular}\\n\"\n",
    "         rows = [f\"{row.get('abbreviation','')} & {row.get('full_name','')} \\\\\\\\\" for _, row in df_to_format.iterrows()]\n",
    "         return header + \"\\n\".join(rows) + \"\\n\" + footer\n",
    "     elif format_type == \"nomenclature\":\n",
    "         header = \"\\\\usepackage{nomencl}\\n\\\\makenomenclature\\n\"\n",
    "         rows = [f\"\\\\nomenclature{{{row.get('abbreviation','')}}}{{{row.get('full_name','')}}}\" for _, row in df_to_format.iterrows()]\n",
    "         return header + \"\\n\".join(rows)\n",
    "     else: # plain text\n",
    "          rows = [f\"{row.get('abbreviation','')}: {row.get('full_name','')}\" for _, row in df_to_format.iterrows()]\n",
    "          return \"; \\n\".join(rows)\n",
    "\n",
    "\n",
    "\n",
    "# # Example Text and Testing\n",
    "\n",
    "# ## example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "example_text = r\"\"\"Paste your latex text (LT)  and enjoy the app (ETA). There is no limitation of the length of text (LT).  \n",
    "\n",
    "What is regarded as abbreviations (RA):\n",
    "The abbreviations like accelerated failure time (AFT), or \\textbf{Time-Constant (TC) Data}. \n",
    "The full definitions and abbrievations can contain greek symbols or simple latex commands like \\frac, for example,  \n",
    "$\\alpha$ Predictive p-value (aPP), \n",
    "$\\alpha$ synclein protein ($\\alpha$-synclein), \n",
    "$\\beta$-Z residual (BZR), $\\sigma$-Z residual ($\\sigma$-ZR). \n",
    "\n",
    "What is desregarded as abbreviations (DA):\n",
    "Citations and explanations in brackets will be omitted, eg. this one (Li et al. 2025), and this ($\\beta$). The $T$ in $f(T)$ is not an abbreviation too.   \n",
    "%This abbreviation, comment text (CT) or the line starting with % will be omitted. \n",
    "\n",
    "Note: the extraction is not perfect as it cannot accommodate all possible abbreviations and may include those you don't want. Modify the results as necessary.\n",
    "\n",
    "The abbreviations used above include: AFT, BZR,  DA,  ETA, LT, RSP,  RA, TC, $\\alpha$-SP, $\\sigma$-ZR. ETA! \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ## Testing\n",
    "\n",
    "# # Streamlit UI\n",
    "\n",
    "# ## Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ec58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Define Explanation Text Variables ---\n",
    "# Copied from above for inclusion\n",
    "summary_expander_label = \"ⓘ How Abbreviation Extraction Works (Summary)\"\n",
    "summary_explanation_text = r\"\"\"\n",
    "This tool tries to automatically find abbreviations defined within parentheses immediately following their full phrase, like `Full Definition Phrase (Abbr)`, even within text containing common LaTeX commands and math.\n",
    "\n",
    "Here’s a simplified overview:\n",
    "\n",
    "1.  **Cleaning:** The input LaTeX text is first cleaned up – comments are removed, math delimiters are standardized, and perhaps most importantly, consistent spacing is added around commands, braces `{}` and parentheses `()` to help separate items.\n",
    "2.  **Finding Candidates:** It scans the cleaned text for the `Phrase (...)` pattern using regular expressions.\n",
    "3.  **Parsing & Comparing:**\n",
    "    * The potential abbreviation inside the parentheses (e.g., `GLM`, `\\alpha SP`) is broken down into its core components. These become either a single lowercase letter (`g`, `l`, `m`, `s`, `p`) or a lowercase command name (`alpha`).\n",
    "    * The words *before* the parentheses are also processed to get comparable units – either the full lowercase word (`generalized`, `linear`, `model`, `protein`) or a lowercase command name (`alpha`, `beta`).\n",
    "    * The algorithm works backward from the end of the abbreviation and the end of the phrase, checking if the **word unit `startswith` the abbreviation unit** (e.g., does `generalized` start with `g`? does `alpha` start with `alpha`? does `alpha` start with `a`?). It tries to find a consistent match for all abbreviation components.\n",
    "4.  **Calculating Match Quality:** Two \"match percentage\" metrics are calculated:\n",
    "    * **% Abbr Matched:** What percentage of the abbreviation's components successfully matched a preceding word.\n",
    "    * **% Words Matched:** Within the range of words considered part of the definition (from first match to end), what percentage of those 'real' words were matched by an abbreviation component.\n",
    "5.  **Reconstructing Phrase:** If a match is found, the tool reconstructs the likely full phrase by taking the text from where the *first* component of the abbreviation matched, all the way up to the opening parenthesis.\n",
    "6.  **Counting Usage:** It also counts how many times the literal abbreviation appears elsewhere in the text.\n",
    "7.  **Filtering & Sorting:** Finally, you can use the filter controls to select only those abbreviations that meet your desired minimum criteria for both match percentages and usage count. You can also sort the results by different columns.\n",
    "\n",
    "*(Disclaimer: This process uses specific rules and heuristics. It may not catch all possible ways abbreviations are defined and might sometimes misinterpret patterns, especially with very complex LaTeX.)*\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "detailed_expander_label = \"ⓘ Detailed Algorithm Explanation\"\n",
    "detailed_description_text = r\"\"\"\n",
    "This algorithm identifies and extracts abbreviation definitions like `Full Definition Phrase (Abbr)` from potentially LaTeX-formatted text.\n",
    "\n",
    "**Core Steps:**\n",
    "\n",
    "1.  **Normalization (`normalize_latex_math`):** (Optional) Input text preprocessing: comments removed, math delimiters standardized (`$...$`), spacing adjusted around `{}()`, commands.\n",
    "2.  **Candidate Identification (Regex):** Finds all `Potential Phrase (Abbr)` patterns, capturing the phrase part and the `abbr_string`.\n",
    "3.  **Usage Counting:** Counts occurrences of each unique `abbr_string` elsewhere in the text.\n",
    "4.  **Processing Each Candidate (`collect_abbreviations` loop):**\n",
    "    * **Phrase Tokenization:** Splits the phrase part into `words_ahead` (list of words and delimiters).\n",
    "    * **Abbreviation Parsing (`get_letters_abbrs`):** Gets `abbr_items` (list of comparable units: `alpha` or `g`) and `original_abbr_parts` (list: `\\alpha` or `G`).\n",
    "    * **Word Parsing (`get_letters_words`):** Pre-calculates `words_ahead_comparables` (list of comparable units: `alpha` or `generalized`).\n",
    "    * **Backward Matching (`find_abbreviation_matches`):**\n",
    "        * Compares `abbr_items` to `words_ahead_comparables` backward using `word_comparable.startswith(abbr_comparable)`.\n",
    "        * Uses `last_matched_index` to constrain search.\n",
    "        * Calculates `match_indices` (list: `abbr_idx` -> `word_idx`).\n",
    "        * Calculates `match_ratio` (% Abbr Matched).\n",
    "        * Calculates `perc_words_matched` (% Words Matched in derived range).\n",
    "        * Returns `(match_indices, match_ratio, perc_words_matched)`.\n",
    "    * **Phrase Reconstruction:** Uses `match_indices` to find the first matched word index (`min_idx_py`). Reconstructs `full_name` from `words_ahead[min_idx_py:]`.\n",
    "    * **Data Collection:** Stores candidate `abbreviation`, `full_name`, `usage_count`, `perc_abbr_matches`, `perc_words_matched`.\n",
    "5.  **DataFrame Creation (`collect_abbreviations` return):** Returns a DataFrame of *all* candidates.\n",
    "6.  **Filtering (`select_abbreviations`):** Filters the collected DataFrame based on user-set thresholds for `perc_abbr_matches`, `usage_count`, and `perc_words_matched`.\n",
    "7.  **Sorting (UI):** Sorts the filtered DataFrame based on the user's choice from the \"Sort by\" dropdown.\n",
    "8.  **Display (UI):** Displays the filtered and sorted data using `st.markdown` rendering a Markdown table (`to_markdown(index=True)`). Shows default row index, renders `$LaTeX$` math. Includes both percentage columns. Table is static.\n",
    "\"\"\"\n",
    "# --- END Define explanation text variables ---\n",
    "\n",
    "\n",
    "# ## App UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3788f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Streamlit App Code ---\n",
    "# Assumes 'example_text' variable, 'DEBUG' variable, and all necessary functions\n",
    "# (normalize_*, get_*, find_*, collect_*, select_*, format_*) are defined ABOVE this block.\n",
    "# Assumes 'import streamlit as st', 'import pandas as pd', 'import re', 'import textwrap' are done ABOVE.\n",
    "\n",
    "# --- Define Explanation Text Variables ---\n",
    "# Using the full text provided previously\n",
    "summary_expander_label = \"ⓘ How Abbreviation Extraction Works (Summary)\"\n",
    "summary_explanation_text = r\"\"\"\n",
    "This tool tries to automatically find abbreviations defined within parentheses immediately following their full phrase, like `Full Definition Phrase (Abbr)`, even within text containing common LaTeX commands and math.\n",
    "\n",
    "Here’s a simplified overview:\n",
    "\n",
    "1.  **Cleaning:** The input LaTeX text is first cleaned up – comments are removed, math delimiters are standardized, and perhaps most importantly, consistent spacing is added around commands, braces `{}` and parentheses `()` to help separate items.\n",
    "2.  **Finding Candidates:** It scans the cleaned text for the `Phrase (...)` pattern using regular expressions.\n",
    "3.  **Parsing & Comparing:**\n",
    "    * The potential abbreviation inside the parentheses (e.g., `GLM`, `\\alpha SP`) is broken down into its core components. These become either a single lowercase letter (`g`, `l`, `m`, `s`, `p`) or a lowercase command name (`alpha`).\n",
    "    * The words *before* the parentheses are also processed to get comparable units – either the full lowercase word (`generalized`, `linear`, `model`, `protein`) or a lowercase command name (`alpha`, `beta`).\n",
    "    * The algorithm works backward from the end of the abbreviation and the end of the phrase, checking if the **word unit `startswith` the abbreviation unit** (e.g., does `generalized` start with `g`? does `alpha` start with `alpha`? does `alpha` start with `a`?). It tries to find a consistent match for all abbreviation components.\n",
    "4.  **Calculating Match Quality:** Two \"match percentage\" metrics are calculated:\n",
    "    * **% Abbr Matched:** What percentage of the abbreviation's components successfully matched a preceding word.\n",
    "    * **% Words Matched:** Within the range of words considered part of the definition (from first match to end), what percentage of those 'real' words were matched by an abbreviation component.\n",
    "5.  **Reconstructing Phrase:** If a match is found, the tool reconstructs the likely full phrase by taking the text from where the *first* component of the abbreviation matched, all the way up to the opening parenthesis.\n",
    "6.  **Counting Usage:** It also counts how many times the literal abbreviation appears elsewhere in the text.\n",
    "7.  **Filtering & Sorting:** Finally, you can use the filter controls to select only those abbreviations that meet your desired minimum criteria for both match percentages and usage count. You can also sort the results by different columns.\n",
    "\n",
    "*(Disclaimer: This process uses specific rules and heuristics. It may not catch all possible ways abbreviations are defined and might sometimes misinterpret patterns, especially with very complex LaTeX.)*\n",
    "\"\"\"\n",
    "\n",
    "detailed_expander_label = \"ⓘ Detailed Algorithm Explanation\"\n",
    "detailed_description_text = r\"\"\"\n",
    "This algorithm identifies and extracts abbreviation definitions like `Full Definition Phrase (Abbr)` from potentially LaTeX-formatted text.\n",
    "\n",
    "**Core Steps:**\n",
    "\n",
    "1.  **Normalization (`normalize_latex_math`):** (Optional) Input text preprocessing: comments removed, math delimiters standardized (`$...$`), spacing adjusted around `{}()`, commands.\n",
    "2.  **Candidate Identification (Regex):** Finds all `Potential Phrase (Abbr)` patterns, capturing the phrase part and the `abbr_string`.\n",
    "3.  **Usage Counting:** Counts occurrences of each unique `abbr_string` elsewhere in the text.\n",
    "4.  **Processing Each Candidate (`collect_abbreviations` loop):**\n",
    "    * **Phrase Tokenization:** Splits the phrase part into `words_ahead` (list of words and delimiters).\n",
    "    * **Abbreviation Parsing (`get_letters_abbrs`):** Gets `abbr_items` (list of comparable units: `alpha` or `g`) and `original_abbr_parts` (list: `\\alpha` or `G`).\n",
    "    * **Word Parsing (`get_letters_words`):** Pre-calculates `words_ahead_comparables` (list of comparable units: `alpha` or `generalized`).\n",
    "    * **Backward Matching (`find_abbreviation_matches`):**\n",
    "        * Compares `abbr_items` to `words_ahead_comparables` backward using `word_comparable.startswith(abbr_comparable)`.\n",
    "        * Uses `last_matched_index` to constrain search.\n",
    "        * Calculates `match_indices` (list: `abbr_idx` -> `word_idx`).\n",
    "        * Calculates `match_ratio` (% Abbr Matched).\n",
    "        * Calculates `perc_words_matched` (% Words Matched in derived range).\n",
    "        * Returns `(match_indices, match_ratio, perc_words_matched)`.\n",
    "    * **Phrase Reconstruction:** Uses `match_indices` to find the first matched word index (`min_idx_py`). Reconstructs `full_name` from `words_ahead[min_idx_py:]`.\n",
    "    * **Data Collection:** Stores candidate `abbreviation`, `full_name`, `usage_count`, `perc_abbr_matches`, `perc_words_matched`.\n",
    "5.  **DataFrame Creation (`collect_abbreviations` return):** Returns a DataFrame of *all* candidates.\n",
    "6.  **Filtering (`select_abbreviations`):** Filters the collected DataFrame based on user-set thresholds for `perc_abbr_matches`, `usage_count`, and `perc_words_matched`.\n",
    "7.  **Sorting (UI):** Sorts the filtered DataFrame based on the user's choice from the \"Sort by\" dropdown.\n",
    "8.  **Display (UI):** Displays the filtered and sorted data using `st.markdown` rendering a Markdown table (`to_markdown(index=True)`). Shows default row index, renders `$LaTeX$` math. Includes both percentage columns. Table is static.\n",
    "\"\"\"\n",
    "# --- END Define explanation text variables ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad0534",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title(r\"Extracting Abbreviations from $\\LaTeX$ Text\")\n",
    "\n",
    "# --- Initialize Session State (Essential for UI statefulness) ---\n",
    "if 'collected_df' not in st.session_state: st.session_state.collected_df = None\n",
    "if 'last_input_text_processed' not in st.session_state: st.session_state.last_input_text_processed = None\n",
    "if 'last_input_text' not in st.session_state:\n",
    "    try: st.session_state.last_input_text = example_text # Uses example_text if defined in your scope\n",
    "    except NameError: st.session_state.last_input_text = \"\"\n",
    "\n",
    "# Initialize filter/sort states with defaults IF THEY DON'T EXIST\n",
    "if 'usage_filter' not in st.session_state: st.session_state.usage_filter = 0\n",
    "if 'perc_abbr_match_filter' not in st.session_state: st.session_state.perc_abbr_match_filter = 0.7\n",
    "if 'perc_words_match_filter' not in st.session_state: st.session_state.perc_words_match_filter = 0.3\n",
    "if 'clear_duplicates_option' not in st.session_state: st.session_state.clear_duplicates_option = 'No'\n",
    "if 'sort_selector' not in st.session_state: st.session_state.sort_selector = 'Abbreviation'\n",
    "\n",
    "# Assume DEBUG is defined (e.g., DEBUG = False)\n",
    "try: _ = DEBUG\n",
    "except NameError: DEBUG = False # Default if not defined elsewhere\n",
    "\n",
    "\n",
    "# --- UI Layout (Top to Bottom) ---\n",
    "\n",
    "# 1. Input Area\n",
    "st.subheader(\"Paste Your Text\")\n",
    "input_text = st.text_area(label=\"...\", label_visibility=\"collapsed\", value=st.session_state.last_input_text, height=250, key=\"input_text_area\")\n",
    "st.caption(\"Privacy: this app does not save your text.\")\n",
    "process_button = st.button(\"Process Text and Extract Abbreviations\", type=\"primary\")\n",
    "\n",
    "\n",
    "# 2. Processing Logic (Run Collection)\n",
    "collection_needed = False\n",
    "if process_button: collection_needed = True; st.session_state.last_input_text = input_text\n",
    "elif input_text != st.session_state.get('last_input_text_processed', None): collection_needed = True\n",
    "\n",
    "if collection_needed and input_text:\n",
    "    st.session_state.clear_duplicates_option = 'No' # Reset selectbox\n",
    "    with st.spinner(\"Collecting...\"):\n",
    "        try:\n",
    "            # Assumes normalize_latex_math and collect_abbreviations are defined elsewhere\n",
    "            normalized_text = normalize_latex_math(input_text)\n",
    "            st.session_state.collected_df = collect_abbreviations(normalized_text, debug=DEBUG)\n",
    "            st.session_state.last_input_text_processed = input_text\n",
    "        except NameError as e: st.error(f\"Function missing: {e}\"); st.session_state.collected_df = None; st.session_state.last_input_text_processed = input_text\n",
    "        except Exception as e: st.error(f\"Error during collection: {e}\"); st.session_state.collected_df = None; st.session_state.last_input_text_processed = input_text\n",
    "elif collection_needed and not input_text:\n",
    "    st.warning(\"Please enter text.\")\n",
    "    st.session_state.collected_df = None\n",
    "    st.session_state.last_input_text_processed = None\n",
    "    st.session_state.clear_duplicates_option = 'No'\n",
    "\n",
    "\n",
    "# --- Display Persistent Collection Status ---\n",
    "if st.session_state.get('collected_df') is not None:\n",
    "    if isinstance(st.session_state.collected_df, pd.DataFrame):\n",
    "          count = len(st.session_state.collected_df)\n",
    "          st.info(f\"{count} Possible Abbreviation{'s' if count != 1 else ''} Extracted. Filter them below.\")\n",
    "    pass # No message if data is None or invalid\n",
    "\n",
    "\n",
    "# --- Prepare Data Source (Handle Duplicates) ---\n",
    "# Define column names just before use\n",
    "collected_data_columns = ['abbreviation', 'full_name', 'perc_abbr_matches', 'perc_words_matched', 'usage_count']\n",
    "collected_data_raw = st.session_state.get('collected_df', pd.DataFrame(columns=collected_data_columns)).copy()\n",
    "data_to_filter = collected_data_raw\n",
    "if st.session_state.clear_duplicates_option == 'Yes' and isinstance(collected_data_raw, pd.DataFrame) and not collected_data_raw.empty and 'abbreviation' in collected_data_raw.columns:\n",
    "    try:\n",
    "        data_to_filter = collected_data_raw.drop_duplicates(subset=['abbreviation'], keep='first', ignore_index=True)\n",
    "    except Exception as e_dup:\n",
    "         st.warning(f\"Issue removing duplicates: {e_dup}\")\n",
    "\n",
    "\n",
    "# --- Filtering Controls (Layout Adjusted) ---\n",
    "st.divider()\n",
    "\n",
    "st.subheader(\"Filtering Controls\") # Renamed subheade\n",
    "# Row 1: Header and Action Buttons (Adjusted Ratios)\n",
    "col_reset, col_show_all, col_spacer_h = st.columns([4, 3, 35]) # Ratio [2, 1, 1, 4] makes header tighter with buttons\n",
    "\n",
    "\n",
    "with col_reset:\n",
    "    if st.button(\"Reset Filters\", key=\"reset_filters_button\", help=\"Reset filters to default values.\"):\n",
    "        st.session_state.usage_filter = 0\n",
    "        st.session_state.perc_abbr_match_filter = 0.7\n",
    "        st.session_state.perc_words_match_filter = 0.3\n",
    "        st.session_state.clear_duplicates_option = 'No'\n",
    "        st.rerun() # Rerun immediately on click\n",
    "\n",
    "with col_show_all:\n",
    "    if st.button(\"Show All\", key=\"show_all_button\", help=\"Show all collected items (minimum filters).\"):\n",
    "        st.session_state.usage_filter = 0\n",
    "        st.session_state.perc_abbr_match_filter = 0.0\n",
    "        st.session_state.perc_words_match_filter = 0.0\n",
    "        st.session_state.clear_duplicates_option = 'No'\n",
    "        st.rerun() # Rerun immediately on click\n",
    "\n",
    "\n",
    "# Define options needed for the selectboxes\n",
    "usage_options = list(range(11))\n",
    "abbr_perc_options = [round(i * 0.1, 1) for i in range(11)]\n",
    "word_perc_options = [round(i * 0.1, 1) for i in range(11)]\n",
    "duplicate_options = ['No', 'Yes']\n",
    "\n",
    "# Row 2: Filter Selectboxes (Occupying ~2/3 Width)\n",
    "col_f1, col_f2, col_f3, col_dup, _ = st.columns([1, 1, 1, 1, 3]) # Ratio [1, 1, 1, 1, 2] = 4 filters (total 4 parts) + spacer (2 parts) = 2/3 width\n",
    "\n",
    "with col_f1:\n",
    "    st.selectbox( \"Usage \\u2265\", options=usage_options, key=\"usage_filter\" )\n",
    "with col_f2:\n",
    "    st.selectbox( \"% Abbr Match \\u2265\", options=abbr_perc_options, key=\"perc_abbr_match_filter\", format_func=lambda x: f\"{x*100:.0f}%\" )\n",
    "with col_f3:\n",
    "    st.selectbox( \"% Words Match \\u2265\", options=word_perc_options, key=\"perc_words_match_filter\", format_func=lambda x: f\"{x*100:.0f}%\" )\n",
    "with col_dup:\n",
    "    st.selectbox( \"Clear Duplicates:\", options=duplicate_options, key='clear_duplicates_option', help=\"Show only the first occurrence ('Yes') or all occurrences ('No') of each abbreviation based on collection order (before filtering).\" )\n",
    "# The 5th column (_) is intentionally left empty as a spacer.\n",
    "\n",
    "\n",
    "\n",
    "# --- END OF Filtering Controls SECTION ---\n",
    "\n",
    "\n",
    "# --- Apply Selection (Filtering) using values from session_state ---\n",
    "filtered_dataframe = pd.DataFrame(columns=collected_data_columns) # Default empty\n",
    "try:\n",
    "    if isinstance(data_to_filter, pd.DataFrame) and not data_to_filter.empty:\n",
    "        # Assumes select_abbreviations is defined elsewhere\n",
    "        filtered_dataframe = select_abbreviations(\n",
    "            data_to_filter,\n",
    "            threshold_perc_abbr_matches=st.session_state.perc_abbr_match_filter,\n",
    "            threshold_usage=st.session_state.usage_filter,\n",
    "            threshold_perc_words_matched=st.session_state.perc_words_match_filter,\n",
    "            debug=DEBUG\n",
    "        )\n",
    "    elif isinstance(data_to_filter, pd.DataFrame) and data_to_filter.empty:\n",
    "        filtered_dataframe = data_to_filter.copy()\n",
    "except NameError as e:\n",
    "    st.error(f\"Function `select_abbreviations` is not defined: {e}\")\n",
    "    filtered_dataframe = pd.DataFrame(columns=collected_data_columns)\n",
    "except Exception as e_select:\n",
    "    st.error(f\"Error during selection: {e_select}\")\n",
    "    filtered_dataframe = pd.DataFrame(columns=collected_data_columns)\n",
    "\n",
    "\n",
    "# --- Apply Sorting using value from session_state ---\n",
    "# Define sort map just before use\n",
    "sort_column_map = {'Abbreviation': 'abbreviation', 'Full Phrase': 'full_name', 'Usage': 'usage_count', '% Abbr Matched': 'perc_abbr_matches', '% Words Matched': 'perc_words_matched'}\n",
    "display_dataframe = filtered_dataframe.copy()\n",
    "if not display_dataframe.empty:\n",
    "    sort_by_display = st.session_state.sort_selector\n",
    "    sort_by_actual = sort_column_map.get(sort_by_display, 'abbreviation')\n",
    "    sort_ascending = not (sort_by_actual in ['usage_count', 'perc_abbr_matches', 'perc_words_matched'])\n",
    "    secondary_sort = 'abbreviation'; secondary_ascending = True\n",
    "    if sort_by_actual == 'abbreviation': secondary_sort = 'usage_count'; secondary_ascending = False\n",
    "    # Add other secondary sort logic if needed\n",
    "    cols_to_sort_by = [sort_by_actual]; ascending_list = [sort_ascending]\n",
    "    if secondary_sort != sort_by_actual and secondary_sort in display_dataframe.columns:\n",
    "        cols_to_sort_by.append(secondary_sort); ascending_list.append(secondary_ascending)\n",
    "    if all(col in display_dataframe.columns for col in cols_to_sort_by):\n",
    "        try: display_dataframe = display_dataframe.sort_values( by=cols_to_sort_by, ascending=ascending_list, ignore_index=True, )\n",
    "        except Exception as sort_e: st.error(f\"Error sorting data: {sort_e}\")\n",
    "    else:\n",
    "        missing_cols = [col for col in cols_to_sort_by if col not in display_dataframe.columns]\n",
    "        st.warning(f\"Cannot sort because column(s) not found: {', '.join(missing_cols)}.\")\n",
    "\n",
    "\n",
    "# --- Display Table and Info Notes ---\n",
    "st.write(\"\") # Spacer from controls\n",
    "\n",
    "# Section Header and Sort Control (Adjusted Ratio)\n",
    "# Define sort options just before use\n",
    "\n",
    "st.subheader(\"Selected Abbreviations\")\n",
    "sort_options = ['Abbreviation', 'Full Phrase', 'Usage', '% Abbr Matched', '% Words Matched']\n",
    "col_header_left, _ = st.columns([2, 15]) \n",
    "with col_header_left: \n",
    "    st.selectbox( \"Sort by:\", options=sort_options, key=\"sort_selector\", )\n",
    "\n",
    "# Display Table using Markdown\n",
    "if not display_dataframe.empty:\n",
    "    display_df_formatted = display_dataframe.rename(columns={ 'abbreviation': 'Abbreviation', 'full_name': 'Full Phrase', 'usage_count': 'Usage', 'perc_abbr_matches': '% Abbr Matched', 'perc_words_matched': '% Words Matched' })\n",
    "    display_columns_order = ['Abbreviation', 'Full Phrase', 'Usage', '% Abbr Matched', '% Words Matched']\n",
    "    display_columns_exist = [col for col in display_columns_order if col in display_df_formatted.columns]\n",
    "    display_df_formatted = display_df_formatted[display_columns_exist]\n",
    "    try:\n",
    "        if '% Abbr Matched' in display_df_formatted.columns: display_df_formatted['% Abbr Matched'] = display_df_formatted['% Abbr Matched'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) and isinstance(x, (int, float)) else x)\n",
    "        if '% Words Matched' in display_df_formatted.columns: display_df_formatted['% Words Matched'] = display_df_formatted['% Words Matched'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) and isinstance(x, (int, float)) else x)\n",
    "    except Exception as fmt_e: st.warning(f\"Could not format percentage columns: {fmt_e}\")\n",
    "    markdown_table = display_df_formatted.to_markdown(index=True)\n",
    "    st.markdown(markdown_table, unsafe_allow_html=False)\n",
    "else:\n",
    "    if st.session_state.get('collected_df') is not None and isinstance(st.session_state.collected_df, pd.DataFrame) and not st.session_state.collected_df.empty:\n",
    "        st.info(\"No abbreviations match the current filter criteria.\")\n",
    "    # else: # No message if no data loaded initially, handled by collection status\n",
    "\n",
    "# Display Info Notes (Corrected logic from before)\n",
    "col_note_left, _ = st.columns([1, 1])\n",
    "with col_note_left:\n",
    "    if not display_dataframe.empty:\n",
    "        notes_found = False; duplicates_df = pd.DataFrame()\n",
    "        if st.session_state.clear_duplicates_option == 'No':\n",
    "            duplicate_mask_display = display_dataframe['abbreviation'].duplicated(keep=False)\n",
    "            if duplicate_mask_display.any():\n",
    "                notes_found = True; duplicates_df = display_dataframe[duplicate_mask_display].sort_values(by=['abbreviation', 'full_name'])\n",
    "                if not duplicates_df.empty:\n",
    "                    dup_grouped = duplicates_df.groupby('abbreviation')['full_name'].apply(lambda names: f\"{names.name} ({', '.join(names)})\").tolist()\n",
    "                    note_text = \"**Multiply defined abbreviations displayed:** \" + \"; \".join(dup_grouped); st.markdown(f\"ℹ️ {note_text}\", unsafe_allow_html=False)\n",
    "        zero_usage_df = display_dataframe[display_dataframe['usage_count'] == 0]\n",
    "        if not zero_usage_df.empty:\n",
    "            all_zero_usage_abbrs = zero_usage_df['abbreviation'].unique().tolist()\n",
    "            if not duplicates_df.empty: reported_duplicates = duplicates_df['abbreviation'].unique(); zero_usage_abbrs_to_report = [abbr for abbr in all_zero_usage_abbrs if abbr not in reported_duplicates]\n",
    "            else: zero_usage_abbrs_to_report = all_zero_usage_abbrs\n",
    "            if zero_usage_abbrs_to_report:\n",
    "                notes_found = True; abbr_list_str = \", \".join(zero_usage_abbrs_to_report)\n",
    "                note_text = \"**Zero usage found for displayed abbreviations:** \" + abbr_list_str; st.markdown(f\"ℹ️ {note_text}\", unsafe_allow_html=False)\n",
    "        if notes_found: st.write(\"\")\n",
    "\n",
    "\n",
    "# --- Export Section ---\n",
    "st.divider(); st.subheader(\"Export Selected Abbreviations\")\n",
    "df_export = display_dataframe\n",
    "col_exp_sel, _ = st.columns([1, 1])\n",
    "with col_exp_sel: selected_format = st.selectbox(\"Export Format:\", options=['plain', 'tabular', 'nomenclature'], index=0, key='format_selector', label_visibility=\"collapsed\")\n",
    "formatted_output = \"No abbreviations selected.\";\n",
    "if df_export is not None and not df_export.empty:\n",
    "    try:\n",
    "        # Assumes format_abbreviations is defined elsewhere\n",
    "        formatted_output = format_abbreviations(df_export, format_type=selected_format)\n",
    "    except NameError as e: formatted_output = f\"Error: `format_abbreviations` fn missing.\"; st.error(formatted_output)\n",
    "    except Exception as format_e: formatted_output = f\"Error during formatting: {format_e}\"; st.error(formatted_output)\n",
    "elif 'collected_df' not in st.session_state or st.session_state.collected_df is None: formatted_output = \"Process text first.\"\n",
    "st.text_area(\"Formatted Output:\", value=formatted_output, height=150, help=\"Copy output.\", key=\"output_text_area\", label_visibility=\"visible\")\n",
    "\n",
    "# --- Explanations & Footer ---\n",
    "# Assumes summary_expander_label, summary_explanation_text, etc. are defined\n",
    "st.divider(); st.subheader(\"About the Algorithm\")\n",
    "col1_exp, col2_exp = st.columns(2)\n",
    "try:\n",
    "    with col1_exp:\n",
    "        with st.expander(summary_expander_label): st.markdown(summary_explanation_text)\n",
    "    with col2_exp:\n",
    "        with st.expander(detailed_expander_label): st.markdown(detailed_description_text)\n",
    "except NameError:\n",
    "    st.warning(\"Explanation text variables (e.g., summary_expander_label) not defined.\") # Add warning if variables missing\n",
    "\n",
    "st.markdown(\"---\"); st.caption(\"Author: Longhai Li, https://longhaisk.github.io, Saskatoon, SK, Canada\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
