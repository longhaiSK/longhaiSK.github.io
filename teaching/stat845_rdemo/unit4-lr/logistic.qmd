---
title: "Logistic Regression"
author:
  - name: "Longhai Li"
    url: "https://longhaisk.github.io/"
date: today
from: markdown+tex_math_single_backslash+tex_math_dollars
format:
  # make sure these files get copied alongside the page
  html:
    theme: cosmo            # (Rmd `theme: null`)
    embed-resources: false # (Rmd `self_contained: false`)
    toc: false
    page-layout: full
    #css: mystyles.css
    code-fold: true
    number-sections: true
    df-print: paged        # (Rmd `df_print: paged`)
    highlight-style: tango # (Rmd `highlight: tango`)
    include-in-header:
      text: |
        <link rel="stylesheet" href="/resources/mystyles.css?v=2">
        <script src="/resources/rendernav.js" defer></script>
        <script src="/resources/loadtoc.js" defer></script>
editor: source
execute:
  echo: true
  warning: false
  message: false
  cache: false
  fig-width: 9
  fig-height: 6
    
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
jit <- 0.05
```

# Odds as a Function of Probability

For an event with probability \(p\), the odds is 
\[\mathrm{odds}(p)=\frac{p}{1-p}\] 
and the log-odds (logit) is 
\[\mathrm{logit}(p)=\log\left(\frac{p}{1-p}\right)\]. 



```{r}
#| label: plot-odds-colored
#| echo: true
# Plot odds(p) with a right-hand axis for log(odds(p)),
# using different line colors for the two curves.
# Defaults: p in [0.01, 0.99].
# Args:
#   p_min, p_max : endpoints for p-grid (0<p_min<p_max<1)
#   n            : number of grid points
#   annotate     : add reference lines/labels if TRUE
#   odds_col     : color for odds(p)
#   logit_col    : color for log(odds(p))
#   lwd1, lwd2   : line widths for the two curves
plot_odds <- function(p_min = 0.01, p_max = 0.99, n = 400,
                      annotate = TRUE,
                      odds_col = "steelblue",
                      logit_col = "firebrick",
                      lwd1 = 2, lwd2 = 2) {
  stopifnot(p_min > 0, p_max < 1, p_min < p_max, n >= 10)
  p <- seq(p_min, p_max, length.out = n)
  odds <- p / (1 - p)
  logit <- log(odds)

  # Left y-axis: odds(p)
  plot(p, odds, type = "l", lwd = lwd1, col = odds_col,
       xlab = "Probability p",
       ylab = "odds(p) = p / (1 - p)")
  if (annotate) {
    abline(h = 1, v = 0.5, lty = 2)
    text(0.52, 1.05, "p = 0.5 → odds = 1", adj = 0)
  }

  # Right y-axis: logit(p) = log(odds)
  op <- par(new = TRUE)
  on.exit(par(op), add = TRUE)
  plot(p, logit, type = "l", lwd = lwd2, col = logit_col,
       axes = FALSE, xlab = "", ylab = "")
  axis(4)
  mtext("log{odds(p)} = log{p/(1 - p)}", side = 4, line = 3)

  if (annotate) {
    abline(v = 0.5, lty = 2)
    # logit(0.5) = 0 reference (horizontal) on the right-axis scale
    usr <- par("usr")
    segments(x0 = usr[1], y0 = 0, x1 = 0.5, y1 = 0, lty = 3)
  }

  legend("topleft",
         legend = c("odds(p)", "log{odds(p)}"),
         col = c(odds_col, logit_col),
         lwd = c(lwd1, lwd2), bty = "n")

  invisible(list(p = p, odds = odds, logit = logit))
}

# Example usage:
# plot_odds()  # defaults: steelblue for odds, firebrick for log-odds (right axis)
plot_odds(odds_col = "#1f77b4", logit_col = "#d62728", n = 600)
```

Logistic regression models **log-odds** linearly in predictors, which both keeps fitted probabilities in \((0,1)\) and turns multiplicative effects on odds into **additive** effects on the linear predictor.


---

# Simulate and Plot a One-Predictor Logistic Model

We simulate data from a logistic model where the **logit** is a linear function of $x$:

$$
\operatorname{logit}{p(x)}
=
\log\left(\frac{p(x)}{1-p(x)}\right)
=
\beta_0 + \beta_1 x,
$$

so that

$$
p(x)
=
\operatorname{logit}^{-1}(\beta_0+\beta_1 x)
=
\frac{1}{1+\exp{-(\beta_0+\beta_1 x)}}.
$$

We then display the observed $y_i$ (binary outcomes) and the true probability curve $p(x)$ in red.


```{r}
#| label: simulate-logistic-1x
#| echo: true
set.seed(123)

# -- Truth (edit as desired) --
n     <- 200
beta0 <- -0.5
beta1 <-  3

# -- Simulate --
x   <- runif(n, -1, 1)                  # predictor
eta <- beta0 + beta1 * x
p   <- plogis(eta)                      # true p(x)
y   <- rbinom(n, size = 1, prob = p)    # outcomes

dat <- data.frame(x = x, y = y, p = p)

# -- Optional: fit a model to the simulated data --
fit <- glm(y ~ x, data = dat, family = binomial())
p_fit <- predict(fit, newdata = data.frame(x = x), type = "response")

# -- Plot: points for y_i (jittered), red line for true p(x) --
# jitter to separate 0/1 visually
yj <- jitter(dat$y, amount = jit)

plot(dat$x, yj,
     pch = 16, col = rgb(0, 0, 0, 0.45),
     xlab = "x",
     ylab = "Observed y (points) & p(x) (curves)",
     ylim = c(-0.1, 1.1))

# True probability curve (red)
xg <- seq(min(x), max(x), length.out = 500)
lines(xg, plogis(beta0 + beta1 * xg), col = "red", lwd = 2)

# Optional: add fitted probability curve (dashed dark red)
lines(xg, predict(fit, newdata = data.frame(x = xg), type = "response"),
      col = "darkred", lwd = 2, lty = 2)

legend("topleft",
       legend = c("y (jittered points)", "true p(x)", "fitted p(x)"),
       pch    = c(16, NA, NA),
       lty    = c(NA, 1, 2),
       col    = c(rgb(0,0,0,0.45), "red", "darkred"),
       lwd    = c(NA, 2, 2),
       bty    = "n")


```

# Example of Coronary Heart Disease Data

## Load a dataset
This dataset is about a follow-up study to determine the development of coronary heart disease (CHD) over 9 years of follow-up of 609 white males from Evans County, Georgia.

**Variable meanings (as provided):**

* `chd`: 1 if a person has the disease, 0 otherwise.
* `smk`: 1 if smoker, 0 if not.
* `cat`: 1 if catecholamine level is high, 0 if low.
* `sbp`: systolic blood pressure (continuous).
* `age`: age in years (continuous).
* `chl`: cholesterol level (continuous).
* `ecg`: 1 if electrocardiogram is abnormal, 0 if normal.
* `hpt`: 1 if high blood pressure, 0 if normal.

```{r}
#| label: setup
# Adjust the path if needed. The default is your original V: drive path.
data_path <- "evans.dat"

# Read data (expects a header row)
CHD.data <- read.table(data_path, header = TRUE)

CHD.data

colnames(CHD.data)

```


## Fit Logistic Regression Model for a Single Variable

```{r}
#| label: chd-univariate-2x2-jitter-fixedlogit
#| echo: true

vars <- c("smk", "sbp", "age", "chl")
#jit  <- 0.01  # global jitter amount for y

# par(mfrow = c(2, 2), mar = c(4, 4, 2, 4) + 0.1)  # extra right margin for axis(4)

for (v in vars) {
  # Univariate logistic regression using ORIGINAL variable name in the formula
  fit <- glm(
    formula = reformulate(v, response = "chd"),
    data    = CHD.data,
    family  = binomial()
  )
  print(summary(fit))

  # Base scatter of chd with small jitter (left axis: probability scale)
  plot(
    CHD.data[[v]],
    jitter(CHD.data$chd, amount = jit),
    pch  = 16, col = rgb(0, 0, 0, 0.45),
    xlab = v, ylab = "chd (jittered)",
    main = paste("chd vs", v),
    ylim = c(-0.1, 1.1)
  )

  # Fitted π(x) in red (left axis)
  if (length(unique(CHD.data[[v]])) == 2) {
    # binary predictor
    xcat <- sort(unique(CHD.data[[v]]))
    nd   <- setNames(data.frame(xcat), v)
    pcat <- predict(fit, newdata = nd, type = "response")
    points(xcat, pcat, pch = 19, col = "red")
    lines(xcat, pcat, col = "red", lwd = 2)

    # Right-axis: logit{π(x)} with fixed y-limits
    logit_p <- log(pcat / (1 - pcat))
    par(new = TRUE)
    plot(
      xcat, logit_p, type = "l", lwd = 2, col = "blue",
      axes = FALSE, xlab = "", ylab = "",
      xlim = range(CHD.data[[v]]), ylim = c(-2.5, 0)
    )
    axis(4)
    mtext("logit(p(x))", side = 4, line = 3)
    par(new = FALSE)

  } else {
    # continuous predictor
    xg <- seq(min(CHD.data[[v]]), max(CHD.data[[v]]), length.out = 400)
    nd <- setNames(data.frame(xg), v)
    pg <- predict(fit, newdata = nd, type = "response")
    lines(xg, pg, col = "red", lwd = 2)

    # Right-axis: logit{π(x)} with fixed y-limits
    logit_pg <- log(pg / (1 - pg))
    par(new = TRUE)
    plot(
      xg, logit_pg, type = "l", lwd = 2, col = "blue",
      axes = FALSE, xlab = "", ylab = "",
      xlim = range(xg), ylim = c(-2.5, 0)
    )
    axis(4)
    mtext("logit(p(x))", side = 4, line = 3)
    par(new = FALSE)
  }
}

```



## Fit Logistic Regression Model with all variables

We fit a logistic regression with a logit link:

```{r}
#| label: fit-logit
fit1 <- glm(
  chd ~ smk + cat + sbp + age + chl + ecg + hpt,
  data = CHD.data,
  family = binomial(link = "logit")
)
summary(fit1)
```

**Notes for interpretation:**

* Positive coefficients increase the log-odds of CHD; negative coefficients decrease it.
* For indicator variables (e.g., `smk`), `exp(beta)` is the adjusted odds ratio comparing the group with value 1 versus 0, holding others fixed.
* For continuous predictors (e.g., `sbp`, `age`), `exp(beta)` is the multiplicative change in the odds for a one‑unit increase. For a *d*-unit increase, the OR is `exp(d * beta)`.

## Inference for Coefficients: Confidence Intervals and Covariance Matrix

We extract profile‑likelihood CIs and the covariance matrix to confirm standard errors.

```{r}
#| label: ci-cov
ci_95 <- confint(fit1, level = 0.95)     # profile-likelihood CI
vcov_mat <- vcov(fit1)                    # covariance matrix of coefficients
se_vec   <- sqrt(diag(vcov_mat))          # standard errors

ci_95
vcov_mat
se_vec  # should match the SE column in summary(fit1)
```

## Inference for Odds Ratios


### Interpretation of Odds Ratios in Logistic Regression

A multiple logistic regression model expresses the log-odds (logit) of an event as a linear function of predictors:

$$
\log\left(\frac{p}{1-p}\right)
= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k.
$$

Here,

* $p = \Pr(Y = 1 \mid x_1, x_2, \ldots, x_k)$ is the probability of the event,
* $\beta_0$ is the intercept, and
* each $\beta_j$ represents the **change in the log-odds** of the event per one-unit increase in $x_j$, *holding all other variables constant*.

Exponentiating both sides gives the model in odds form:

$$
\frac{p}{1-p}
= \exp(\beta_0)
\times \exp(\beta_1 x_1)
\times \exp(\beta_2 x_2)
\times \cdots
\times \exp(\beta_k x_k).
$$

**An R function for OR at two Profiles**

The or_from_predict R function is a utility designed to calculate the Odds Ratio (OR) and its 95% confidence interval (CI) between two specific covariate profiles (new1 and new0) for a given logistic regression model (fit). The calculation is performed on the link (logit) scale. For a logistic model $\text{logit}(p) = \eta = \mathbf{X}\boldsymbol{\beta}$, the log-Odds Ratio (logOR) is the difference between the linear predictors ($\eta_1, \eta_0$) for the two profiles:
$$\widehat{\text{logOR}} = \eta_1 - \eta_0 = (\mathbf{x}_1^T - \mathbf{x}_0^T) \boldsymbol{\beta} = \mathbf{c}^T \boldsymbol{\beta}$${#eq-logor}

Here, $\mathbf{c} = \mathbf{x}_1 - \mathbf{x}_0$ is the linear contrast vector derived from the model matrices of the two profiles. The function estimates the variance of this contrast as $\text{Var}(\widehat{\text{logOR}}) = \mathbf{c}^T \mathbf{V} \mathbf{c}$, where $\mathbf{V}$ is the model's variance-covariance matrix (vcov(fit)). The standard error $SE = \sqrt{\mathbf{c}^T \mathbf{V} \mathbf{c}}$ is used to compute the $100(1-\alpha)\%$ confidence interval for the logOR: 
$$\widehat{\text{logOR}} \pm z_{1-\alpha/2} \times SE.$$ 
These values (estimate and CI bounds) are then exponentiated to produce the final $\widehat{\text{OR}} = \exp(\widehat{\text{logOR}})$ and its 95% CI. The function also prints two helpful summaries to the console: a data frame showing only the variables that differ between the new0 and new1 profiles, and a 2x3 table presenting the estimates and CIs for both the OR and the logOR.


```{r}
#| label: OR function
# Compute OR and 95% CI via predict() on the LINK scale
# OR = exp( eta(new1) - eta(new0) ), where eta(.) = logit{π(.)}
# Compute OR via predict() contrast on the LINK scale, also:
# (i) print fitted model with plugged coefficients: log(p/(1-p)) = ...
# (ii) print a 2-row data.frame of only variables that differ between new0 and new1
# (iii) print a 2x3 table (rows: OR, logOR; cols: Estimate, CI_low, CI_up)
or_from_predict <- function(fit, new1, new0, level = 0.95, digits = 4, tol = 1e-12) {
  stopifnot(is.data.frame(new1), is.data.frame(new0))

  # ---- (i) Fitted model with numeric coefficients on logit scale ----
  format_logit_formula <- function(fit, digits = 4) {
    b   <- coef(fit)
    nm  <- names(b)
    fmt <- function(x) formatC(x, digits = digits, format = "fg", flag = "#")
    rhs <- character(0)
    for (j in seq_along(b)) {
      term <- if (j == 1L) "(Intercept)" else nm[j]
      coef_j <- b[j]
      if (j == 1L) {
        rhs <- c(rhs, fmt(coef_j))
      } else {
        sign <- if (coef_j >= 0) " + " else " - "
        rhs  <- c(rhs, paste0(sign, fmt(abs(coef_j)), " * ", term))
      }
    }
    paste0("log(p/(1-p)) = ", paste(rhs, collapse = ""))
  }

  # ---- (ii) Two-row data.frame with only changed variables ----
  changes_df <- (function(new0, new1, tol) {
    common <- intersect(names(new0), names(new1))
    diffv  <- vapply(common, function(nm) {
      x0 <- new0[[nm]]; x1 <- new1[[nm]]
      if (is.numeric(x0) && is.numeric(x1)) {
        !isTRUE(all.equal(as.numeric(x0), as.numeric(x1), tolerance = tol))
      } else {
        !identical(x0, x1)
      }
    }, logical(1))
    keep <- common[diffv]
    if (length(keep) == 0L) {
      out <- data.frame(`_no_changes_` = "no differences")
      rownames(out) <- c("new0", "new1")
      return(out)
    }
    out <- rbind(new0[keep], new1[keep])
    rownames(out) <- c("new0", "new1")
    out
  })(new0, new1, tol)

  # ---- Linear contrast for log-OR and its variance ----
  X1 <- model.matrix(delete.response(terms(fit)), data = new1)
  X0 <- model.matrix(delete.response(terms(fit)), data = new0)
  cvec      <- as.numeric(X1 - X0)
  b         <- coef(fit)
  V         <- vcov(fit)
  logOR_hat <- sum(cvec * b)
  se_logOR  <- sqrt(as.numeric(t(cvec) %*% V %*% cvec))

  alpha  <- 1 - level
  z      <- qnorm(1 - alpha / 2)
  ci_log <- c(logOR_hat - z * se_logOR, logOR_hat + z * se_logOR)

  # ---- 2x3 table: rows OR and logOR; columns Estimate, CI_low, CI_up ----
  res_tab <- data.frame(
    Estimate = c(exp(logOR_hat),              logOR_hat),
    CI_low   = c(exp(ci_log[1L]),            ci_log[1L]),
    CI_up    = c(exp(ci_log[2L]),            ci_log[2L]),
    row.names = c("OR", "logOR")
  )

  # ---- Print requested items ----
  cat("\nVariables that differ between new0 and new1:\n")
  print(changes_df)
  cat("\nOdds Ratio summary (2 x 3):\n")
  print(res_tab)

  # ---- Return (invisibly) ----
  invisible(list(
    OR        = exp(logOR_hat),
    CI_OR     = exp(ci_log),
    logOR     = logOR_hat,
    CI_logOR  = ci_log,
    se_logOR  = se_logOR,
    changes   = changes_df,
    table     = res_tab
  ))
}

# --- Example usage ---
# base_prof <- as.data.frame(lapply(CHD.data, function(col) if (is.numeric(col)) mean(col) else col[1]))
# new0 <- base_prof; new0$smk <- 0
# new1 <- base_prof; new1$smk <- 1
# or_from_predict(fit1, new1 = new1, new0 = new0)

# ---- Helpers to build profiles by fixing "other" covariates ----
mean_profile <- function(data, vars_binary_as = c(0,1)) {
  # Build a single-row data.frame of typical values:
  out <- lapply(data, function(col) {
    if (is.numeric(col)) {
      # If strictly 0/1, keep mean (works fine for GLM prediction),
      # or switch to mode if you prefer.
      if (all(col %in% c(0,1))) mean(col) else mean(col, na.rm = TRUE)
    } else {
      # Fallback to first level for factors/characters
      if (is.factor(col)) levels(col)[1] else unique(col)[1]
    }
  })
  as.data.frame(out)
}

```


### Examples of Finding ORs and Their CIs

#### OR Smoking (`smk`) (1 vs 0)



```{r include=FALSE}
#| label: or-smk
OR_smk <- exp(coef(fit1)["smk"])  # adjusted OR for smokers vs non-smokers
ci_95 <- confint(fit1, level = 0.95)     # profile-likelihood CI
CI_OR_smk <- exp(ci_95["smk", ])  # 95% CI for the OR
OR_smk
CI_OR_smk

```
**Using the helper function**

```{r}
## 1) Smoking OR: smk = 1 vs 0 (other vars at their means)
# Example profiles at sample means (adjust as you like)
base_prof <- mean_profile(CHD.data)
new0 <- base_prof; new0$smk <- 0
new1 <- base_prof; new1$smk <- 1

res_smk <- or_from_predict(fit1, new1 = new1, new0 = new0)

```


**How to read this:**

* `OR_smk > 1` suggests higher odds of CHD among smokers (adjusted for other variables). If the 95% CI excludes 1, the association is statistically significant at the 5% level.

#### OR for Systolic Blood Pressure (`sbp`): from 120 to 160

We compute the adjusted OR for a 40‑unit increase in `sbp` (from 120 to 160):

```{r include=FALSE}
#| label: or-sbp-160-120
step_sbp <- 160 - 120
A_sbp <- step_sbp * coef(fit1)["sbp"]
OR_sbp_160_vs_120 <- exp(A_sbp)

# Delta-method CI using the model-based variance
var_A_sbp <- step_sbp^2 * vcov_mat["sbp", "sbp"]
alpha <- 0.05
z <- qnorm(1 - alpha/2)
ci_lin <- c(A_sbp - z * sqrt(var_A_sbp), A_sbp + z * sqrt(var_A_sbp))
CI_OR_sbp_160_vs_120 <- exp(ci_lin)

OR_sbp_160_vs_120
CI_OR_sbp_160_vs_120

```

**Using the helper function**

```{r}
## 2) SBP OR: 160 vs 120 (other vars at their means)
new0 <- base_prof; new0$sbp <- 120
new1 <- base_prof; new1$sbp <- 160

res_sbp <- or_from_predict(fit1, new1 = new1, new0 = new0)
res_sbp


```

### OR for Combined Effects of Two Variables: Smoking with an Age Difference

Suppose we compare two groups that differ in **smoking status** and **age**:

* **Group A:** `smk = 1`, `age = 50` (all other covariates equal)
* **Group B:** `smk = 0`, `age = 30`

The log‑odds contrast is ($A = \beta_{smk} + (50-20)\beta_{age}$), so the OR is ($\exp(A)$).

```{r include=FALSE}
#| label: combined-smk-age
age_diff <- 50 - 30
A_smk_age <- coef(fit1)["smk"] + age_diff * coef(fit1)["age"]
OR_smk_age <- exp(A_smk_age)

# Variance of the linear combo: Var(b1 + a*b_age) = Var(b1) + a^2 Var(b_age) + 2a Cov(b1, b_age)
var_A_smk_age <- vcov_mat["smk", "smk"] +
  age_diff^2 * vcov_mat["age", "age"] +
  2 * age_diff * vcov_mat["smk", "age"]

CI_OR_smk_age <- exp(c(A_smk_age - z * sqrt(var_A_smk_age),
                       A_smk_age + z * sqrt(var_A_smk_age)))

OR_smk_age
CI_OR_smk_age
```

**Using the helper function**

```{r}
new0 <- base_prof; new0$age <- 30; new0$smk <- 0
new1 <- base_prof; new1$age <- 50; new1$smk <- 1

res_ageAsmk <- or_from_predict(fit1, new1 = new1, new0 = new0)


```

## Overall Regression Significance with Wilks' Theorem

In the context of logistic regression, Wilks' theorem provides the basis for the Likelihood Ratio Test (LRT) used to assess the significance of predictor variables. The theorem states that when comparing a full model ($M_1$) to a nested null model ($M_0$), the test statistic, $D$, asymptotically follows a chi-squared ($\chi^2$) distribution under the null hypothesis (i.e., that the simpler model $M_0$ is correct).The statistic $D$ is calculated as the difference in the maximized log-likelihoods:
$$D = -2(\log L_0 - \log L_1)$${#eq-LRT}
where $\log L_0$ and $\log L_1$ are the log-likelihoods of the null and full models, respectively. In logistic regression, this is equivalent to the difference in the deviances: $D = \text{Deviance}_0 - \text{Deviance}_1$. For assessing the overall significance of a regression model (fit1), this involves comparing it to its corresponding intercept-only (null) model. The degrees of freedom for the $\chi^2$ test is the difference in the number of parameters between the two models, $df = df_0 - df_1$, which equals the number of predictors in the full model.Here is an R code chunk demonstrating how to compute this p-value directly from a glm fit object, assuming it is named fit1.
```{r}



#| label: compute-lrt-pvalue
#| echo: true

# Calculate the Likelihood Ratio Test statistic (D) and degrees of freedom (df)
# by comparing the model's deviance to the null (intercept-only) deviance,
# both of which are stored in the 'fit1' object.
lrt_statistic <- fit1$null.deviance - fit1$deviance
lrt_df <- fit1$df.null - fit1$df.residual

# Compute the p-value from the chi-squared distribution
# We use lower.tail = FALSE to get P(ChiSq > D)
p_value <- pchisq(lrt_statistic, lrt_df, lower.tail = FALSE)

# Print the result
cat(paste("Likelihood Ratio Test Statistic (D):", round(lrt_statistic, 4), "\n"))
cat(paste("Degrees of Freedom (df):", lrt_df, "\n"))
cat(paste("Overall Model P-value:", format.pval(p_value, digits = 4), "\n"))

```

