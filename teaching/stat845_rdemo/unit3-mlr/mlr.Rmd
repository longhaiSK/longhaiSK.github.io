---
title: "Multiple Linear Regression"
author: <a href="/">Longhai Li</a>
date: "September, 2025"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    highlight: tango
    code_folding: show
    df_print: paged
    css: mystyles.css
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=TRUE}
library(knitr)
library(rmarkdown)
# This chunk sets up the global options for the document.
knitr::opts_chunk$set(
	fig.height = 6,
	fig.width = 9,
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	comment = "#",
	include = TRUE
)
```


# Multiple Linear Regression for Wire Bond Strength Dataset

## Loading Data and Visualization

**Note:** You must change the file paths in the `read.csv()` functions below to match the location of the files on your computer for example "C:\Users\<YourUsername\>\Documents" on windows.

```{r load-bond-data}

# Read data. Change the path as necessary.
# Example: bond.data <- read.csv("data/wire-bond.csv")
bond.data <- read.csv("wire-bond.csv")

# This will now be automatically rendered as a paged table
bond.data
```

```{r dummy.data, include=FALSE, eval=FALSE}
# Use set.seed() to make sure you get the exact same random data every time
set.seed(42)

# Define the number of data points to create
n <- 150

# Create the bond.data data frame
bond.data <- data.frame(
  # Generate 'length' as random numbers between 10 and 50
  length = runif(n, 10, 50),
  # Generate 'height' as random numbers between 5 and 25
  height = runif(n, 5, 25)
)

# Create 'strength' based on a linear relationship with the other variables
# strength = (some_value * length) + (another_value * height) + random_error
bond.data$strength <- (2.5 * bond.data$length) + 
                      (4.1 * bond.data$height) + 
                      rnorm(n, mean = 0, sd = 20) # Add random noise

# Display the first few rows of the new data frame
head(bond.data)
```

**2D Visualization**


```{r load-bond-data22}
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))

# 1) length vs strength
i1 <- which(!is.na(bond.data$length) & !is.na(bond.data$strength))
plot(bond.data$length[i1], bond.data$strength[i1],
     xlab = "Wire Length", ylab = "Pull strength", pch = 19)
# label with original row indices (use seq_along(i1) if you prefer 1..n)
text(bond.data$length[i1], bond.data$strength[i1],
     labels = i1, pos = 1, offset = 0.4, cex = 0.75)

# 2) height vs strength
i2 <- which(!is.na(bond.data$height) & !is.na(bond.data$strength))
plot(bond.data$height[i2], bond.data$strength[i2],
     xlab = "Die height", ylab = "Pull strength", pch = 19)
text(bond.data$height[i2], bond.data$strength[i2],
     labels = i2, pos = 1, offset = 0.4, cex = 0.75)

```


**3D Visualize**

```{r}
# 1. Install and load the required package

library(scatterplot3d)

par (mfrow = c(1,1))
# 2. Create the 3D scatterplot and store its object
s3d <- with(bond.data, scatterplot3d(
  x = length,
  y = height,
  z = strength,
  pch = 19,
  color = "steelblue",
  main = "3D Scatterplot: Strength vs. Length and Height",
  xlab = "Length",
  ylab = "Height",
  zlab = "Strength",
  angle = 60
))

# 3. Fit a linear model
fit <- lm(strength ~ length + height, data = bond.data)

# 4. Add the regression plane to the plot
s3d$plane3d(fit, lty.box = "solid")

```

**Dynamic 3D Visualization of 3D Data**

```{r}
# 1. Load necessary libraries
library(plotly)
library(dplyr)

# NOTE: This script assumes you have a data frame named 'bond.data'
# with columns 'length', 'height', and 'strength' already loaded.

# 2. Create the initial scatter plot object
fig <- plot_ly(
  data = bond.data, 
  x = ~length, 
  y = ~height, 
  z = ~strength
) %>%
  add_markers(
    marker = list(
      color = ~strength,
      colorscale = 'Viridis',
      showscale = TRUE,
      colorbar = list(title = 'Strength')
    ),
    name = 'Data Points'
  ) %>%
  layout(
    title = "Interactive 3D Scatter Plot of Bond Strength",
    scene = list(
      xaxis = list(title = 'Length'),
      yaxis = list(title = 'Height'),
      zaxis = list(title = 'Strength')
    )
  )

# 3. Display the scatter plot
# In an interactive session (like RStudio), this line will render the plot.
fig
```

**Add Fitted Surface**

```{r}
# 4. Fit a linear model to the data
fit <- lm(strength ~ length + height, data = bond.data)

# 5. Prepare data for the regression plane surface
resolution <- 20
axis_x <- seq(min(bond.data$length), max(bond.data$length), length.out = resolution)
axis_y <- seq(min(bond.data$height), max(bond.data$height), length.out = resolution)

z_matrix <- outer(
  axis_x,
  axis_y,
  function(len, ht) {
    predict(fit, newdata = data.frame(length = len, height = ht))
  }
)

# 6. Add the surface to the EXISTING plot object and display the result
fig <- fig %>%
  add_surface(
    x = ~axis_x,
    y = ~axis_y,
    z = ~z_matrix,
    colorscale = 'Blues',
    opacity = 0.7,
    showscale = FALSE,
    name = 'Regression Plane'
  )

# 7. Display the final, updated plot
fig
```

## Model Fitting and Summary

We fit a multiple linear regression model with `strength` as the response variable and `length` and `height` as predictors.

```{r fit-bond-model}
# Fit the linear model
fit <- lm(strength ~ length + height, data = bond.data)

# Display the classic summary output (this is not a data frame, so it's unaffected)
summary(fit)
```

The summary provides the ANOVA F-test for overall significance, $R^2$, adjusted $R^2$, and t-tests for individual coefficients.

## Confidence Intervals and Model Components

We can extract confidence intervals for the regression coefficients, as well as fitted values and residuals.

```{r bond-model-components}
# Confidence intervals (matrix output, will be auto-paged)
confint(fit)

# Fitted values and ordinary residuals (matrix output, will be auto-paged)
pred <- fitted.values(fit)
e <- resid(fit)
data.frame(y = bond.data$strength, y.hat = pred, e = e)

# Extract covariance matrix (matrix output, will be auto-paged)
cov.mat <- vcov(fit)
cov.mat

# Standard errors (vector output, needs to be a data.frame to be paged)
data.frame(std.error = sqrt(diag(cov.mat)))
```

# Partial F-test and t-test

## The distribution of RSS reduction ($SS_R$ or SSR)

```{r include=FALSE}

require ("latex2exp")
require ("animation")
```

### When $H_0$ is true

```{r RSS-animate-H0, cache=TRUE, fig.show="animate", class.source="fold-hide", fig.height=8}
# Install/load necessary packages
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("patchwork", quietly = TRUE)) install.packages("patchwork")
library(ggplot2)
library(patchwork)

# -----------------------------------------------------------
# Setup and Data Generation (same as before)
# -----------------------------------------------------------

n <- 30
p_max <- 10
beta <- rep(0, p_max)
sigma <- 1
x_lim <- c(0, n)
ks <- seq(0, p_max, by = 2)

set.seed(42)
X <- replicate(p_max, rnorm(n))
colnames(X) <- paste0("x", 1:p_max)
X <- as.data.frame(X)

y <- as.numeric(as.matrix(X) %*% beta + rnorm(n, sd = sigma))

stats_list <- lapply(ks, function(k) {
  num_params <- k + 1
  if (k == 0) {
    fitsim <- lm(y ~ 1)
  } else {
    fml <- as.formula(
      paste("y ~", paste(paste0("x", 1:k), collapse = " + "))
    )
    fitsim <- lm(fml, data = X)
  }
  
  s <- summary(fitsim)
  f_val <- s$fstatistic
  
  data.frame(
    p = num_params,
    RSS = sum(residuals(fitsim)^2),
    R2 = ifelse(k == 0, 0, s$r.squared),
    R2_adj = ifelse(k == 0, 0, s$adj.r.squared),
    F_stat = ifelse(is.null(f_val), NA, f_val[1])
  )
})

df <- do.call(rbind, stats_list)
saturated_row <- data.frame(p = n, RSS = 0, R2 = 1, R2_adj = NA, F_stat = NA)
df <- rbind(df, saturated_row)


# --- Plotting Logic ---

# Plot 1: RSS vs. Model Size (no changes here)
g1 <- ggplot(df, aes(x = p, y = RSS)) +
  geom_line(color = "black") +
  geom_point(color = "black") +
  geom_point(data = subset(df, p == n), size = 2) +
  annotate("text", x = n, y = 0, label = "(n, 0)", vjust = -0.8, size = 3) +
  scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
  labs(
    title = "Residual Sum of Squares vs. Number of Parameters",
    x = "Number of parameters (p)",
    y = "Residual Sum of Squares (RSS)"
  ) +
  theme_minimal(base_size = 13)

# Plot 2: F-statistic and R-squared values (with updated labels)
scale_factor <- max(df$F_stat, na.rm = TRUE)

g2 <- ggplot(df, aes(x = p)) +
  geom_line(aes(y = F_stat, color = "F-statistic")) +
  geom_point(aes(y = F_stat, color = "F-statistic")) +
  geom_line(aes(y = R2 * scale_factor, color = "R2")) +
  geom_point(aes(y = R2 * scale_factor, color = "R2")) +
  geom_line(aes(y = R2_adj * scale_factor, color = "R2_adj"), linetype = "dashed") +
  geom_point(aes(y = R2_adj * scale_factor, color = "R2_adj")) +
  
  scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
  scale_y_continuous(
    name = "F-statistic",
    # UPDATED: Use expression() for the axis title
    sec.axis = sec_axis(~ . / scale_factor, name = expression(paste(R^2, " / ", R[adj]^2)), labels = scales::percent)
  ) +
  # UPDATED: Use expression() in the labels for the legend
  scale_color_manual(
    name = "Metric",
    values = c("F-statistic" = "firebrick", "R2" = "blue", "R2_adj" = "cyan4"),
    labels = c("F-statistic", expression(R^2), expression(R[adj]^2))
  ) +
  labs(
    title = "Model Fit Statistics vs. Number of Parameters",
    x = "Number of parameters (p)"
  ) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "top")


# Combine and print the plots
print(g1 / g2)
```

### When $H_1$ is true

```{r RSS-animate-H1, cache=TRUE, fig.show="animate", class.source="fold-hide", fig.height=8}

# -----------------------------------------------------------
# RSS vs. model size demo with saturated model endpoint
# -----------------------------------------------------------
  
n <- 30
p <- 10
beta <- c(1, rep(0, p - 1))           
sigma <- 1
x_lim <- c(0, 30)
y_lim <- c(0, 110)
ks   <- seq(0,p, by =2)
# Loop to generate frames for an animation
for (n.sim in 1:50) {

  # Generate predictors and true model
  X <- replicate(p, rnorm(n))
  colnames(X) <- paste0("x", 1:p)
  X <- as.data.frame(X)
  

  y <- as.numeric(as.matrix(X) %*% beta + rnorm(n, sd = sigma))
  
  # Fit models with first k predictors; collect RSS
  
  RSS  <- numeric(length(ks))
  pars <- ks + 1   # number of parameters incl. intercept
  
  for (i in seq_along(ks)) {
    k <- ks[i]
    if (k == 0) {
      fitsim <- lm(y ~ 1)                   
    } else {
      fml <- as.formula(
        paste("y ~", paste(paste0("x", 1:k), collapse = " + "))
      )
      fitsim <- lm(fml, data = X)
    }
    RSS[i] <- sum(residuals(fitsim)^2)      
  }
  
  # Add saturated model: p = n, RSS = 0
  pars <- c(pars, n)
  RSS  <- c(RSS, 0)
  
  # Plot
  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    plot(pars, RSS, type = "b", xlab = "Number of parameters (p)",
         ylab = "Residual Sum of Squares (RSS)",
         main = "RSS vs. Model Size (with saturated model)")
  } else {
    library(ggplot2)
    df <- data.frame(p = pars, RSS = RSS)
    g <- ggplot(df, aes(p, RSS)) +
      geom_line() +
      geom_point() +
      geom_point(data = subset(df, p == n), size = 2) +
      annotate("text", x = n, y = 0, label = "(n, 0)", vjust = -0.6, size = 3) +
      scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
      scale_y_continuous(limits = y_lim, breaks = pretty(y_lim)) + 
      labs(
        title = "RSS vs. Model Size (with saturated model)",
        x = "Number of parameters (p)",
        y = "Residual Sum of Squares (RSS)"
      ) +
      theme_minimal(base_size = 13)
  }
  print(g)
}

```

## Example

This section uses data on the weight, height, and age of children to demonstrate partial F-tests.

```{r setup-child-data}
# Data: Weight, height and age of children
wgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)
hgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)
age <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)
child.data <- data.frame(wgt, hgt, age)
```

### Problem 1: Height then Age

Here we test if adding `age` significantly improves the model that already contains `hgt`.

```{r partial-f-test-2}
fit_age_hgt <- lm(wgt ~ hgt + age, data = child.data)
summary(fit_age_hgt)

# Compare the nested models (anova output is a data frame, will be auto-paged)
fit_hgt <- lm(wgt ~ hgt, data = child.data)
summary(fit_hgt)
anova(fit_hgt, fit_age_hgt)
```

### Problem 2: Age then Height

Here we test if adding `age` significantly improves the model that already contains `hgt`.

```{r partial-f-test-age-height}
fit_age <- lm(wgt ~  age, data = child.data)
summary(fit_age)

# Compare the nested models (anova output is a data frame, will be auto-paged)
fit_age_hgt <- lm(wgt ~ age+hgt, data = child.data)
summary(fit_age_hgt)
anova(fit_age, fit_age_hgt)
```
## Relationship between t-test and partial F-test

The **partial F-test** is a statistical test used to determine if adding one or more covariates to a regression model provides a statistically significant improvement in explaining the dependent variable's variance. It works by comparing a full model (with the new covariates) to a reduced model (without them).

A t-test for an individual coefficient is simply a special case of the partial F-test. In this scenario, the reduced model is the one where the specific covariate being tested has been omitted. The mathematical relationship is direct: the squared value of the t-statistic for that single coefficient is identical to the F-statistic from the partial F-test.

Therefore, the **p-value** of the t-test for an individual coefficient is about the statistical significance of that variable's **unique contribution** after accounting for (or removing) the effects of all other covariates already in the model. It essentially asks, "Does this specific variable add any significant explanatory power, given that the other variables are already included?" This is why a variable might be significant on its own but not in a multiple regression model if its effect is already captured by other predictors.

# Mean Response and Prediction Intervals

This section demonstrates how to calculate confidence and prediction intervals for the wire bond strength data.

## Confidence Interval for Mean Response

Construct a 95% CI on the mean pull strength for a wire bond with wire length = 8 and die height = 275.

```{r confidence-interval}
# We use the model 'fit' from the first section (predict output is a matrix)
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "confidence", level = 0.95)
```

## Prediction Interval for a New Observation

Construct a 95% PI on the pull strength for a new wire bond with wire length = 8 and die height = 275.

```{r prediction-interval}
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "prediction", level = 0.95)
```

# Model Diagnostics

This section covers various methods for diagnosing the model fit using residuals and other measures.

## Residual Calculations

We can calculate hat values, and various types of residuals. They are combined here into a single data frame which will be auto-paged.

```{r calculate-residuals}
residuals_df <- data.frame(
  hat_values = hatvalues(fit),
  ordinary_resid = resid(fit),
  standardized_resid = resid(fit) / sigma(fit),
  studentized_internal = rstandard(fit),
  studentized_external = rstudent(fit)
)
residuals_df
```

## Residual Plots

A common practice is to analyze plots of the studentized residuals to check for non-linearity, non-constant variance, and non-normality.

```{r plot-residuals}
# Residual analysis using internally studentized residuals
n <- nrow(bond.data)
r <- rstandard(fit) 
y.hat <- fitted.values(fit)

par(mfrow = c(2, 3))
qqnorm(r, main = "Normal Q-Q Plot")
qqline(r)
plot(y.hat, r, xlab = "Fitted values", ylab = "Studentized Residuals")
abline(h = 0)
plot(1:n, r, xlab = "Observation Number", ylab = "Studentized Residuals")
abline(h = 0)
plot(bond.data$length, r, xlab = "Wire Length", ylab = "Studentized Residuals")
abline(h = 0)
plot(bond.data$height, r, xlab = "Die Height", ylab = "Studentized Residuals")
abline(h = 0)
```

# Influential Observations

We can identify influential data points using metrics like DFFITS, DFBETAS, and Cook's D.

```{r influential-obs-calcs}
# Find DFFITS, DFBETAS and Cook's D for the wire bond strength data.
influence_df <- data.frame(dffits = dffits(fit), cook.D = cooks.distance(fit), dfbetas(fit))
influence_df
```

## Plotting with the `olsrr` Package

The `olsrr` package provides convenient functions for plotting these influence diagnostics. Note: You only need to run `install.packages("olsrr")` once.

```{r plot-influential-obs, fig.height=12}
# install.packages("olsrr") # Run this once if you don't have the package
library(olsrr)

# Plot for Cook's D
ols_plot_cooksd_chart(fit)

# Plot for DFFITS
ols_plot_dffits(fit)

# Plot for DFBETAS
ols_plot_dfbetas(fit)
```

# Polynomial Regression

This example uses data on the cost and production lot size of airplane sidewall panels.

```{r poly-regression-setup}
y <- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)
x <- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)
fit_poly <- lm(y ~ x + I(x^2))
summary(fit_poly)
```

The plot shows the fitted quadratic curve.

```{r poly-regression-plot}
plot(x, y, xlab = "Lot size, x", ylab = "Average cost per unit, y")
lines(x, predict(fit_poly, newdata = data.frame(x = x)), type = "l")
```

We can use a partial F-test to see if the quadratic term significantly improves the model.

```{r poly-regression-ftest}
fit1 <- lm(y ~ x)
anova(fit1, fit_poly)
```

# Indicator Variables

This section uses the SBP (Systolic Blood Pressure) dataset to model the effect of a categorical variable (`sex`).

```{r indicator-setup}
# Note: Update this path to your local file location
sbpdata <- read.csv("sbpdata.csv")

# Fit the full model with an interaction term
fit.full <- lm(sbp ~ age + sex + age:sex, data = sbpdata)
summary(fit.full)
```

## Hypothesis Tests for Indicator Models

We can perform various hypothesis tests to see if the lines are coincident, parallel, or have equal intercepts.

```{r indicator-hyp-tests}
# Test of Coincidence. H0: beta2=beta3=0
fit.coin <- lm(sbp ~ age, data = sbpdata)
anova(fit.coin, fit.full)

# Test of Parallelism. H0: beta3=0
fit.para <- lm(sbp ~ age + sex, data = sbpdata)
anova(fit.para, fit.full)

# Test for Equal Intercepts. H0: beta2=0 (assuming parallel lines)
fit1.inter <- lm(sbp ~ age + sex, data = sbpdata)
fit2.inter <- lm(sbp ~ age, data = sbpdata)
anova(fit2.inter, fit1.inter)
```

## Plotting the Fitted Model

The plot below shows the separate regression lines for males and females.

```{r indicator-plot}
# Define color for males (sex=1) and females (sex=0).
colors <- ifelse(sbpdata$sex == 1, "black", "gray")

# Scatter plot
plot(sbpdata$age, sbpdata$sbp, xlab = "Age", ylab = "SBP", col = colors)

# Add the fitted lines for males and females
# The design matrix is data.frame(1, age, 1, age) for sex = 1,
# and data.frame(1, age, 0, 0) for sex = 0.
lines(sbpdata$age, cbind(1, sbpdata$age, 1, sbpdata$age) %*% coef(fit.full), col = "black")
lines(sbpdata$age, cbind(1, sbpdata$age, 0, 0) %*% coef(fit.full), col = "gray")

# Add legend
legend("topleft", legend = c("sex = 1 (male)", "sex = 0 (female)"), 
       lty = c(1, 1), col = c("black", "gray"))
```

# Model Building

This section uses the `olsrr` package and wine quality data to demonstrate automated model selection techniques.

```{r model-building-setup}
library(olsrr)
# Note: Update this path to your local file location
wine <- read.csv("wine.csv")

# A dot in the lm function means: use all other variables as predictors.
model <- lm(quality ~ ., data = wine)
```

## All Possible Regression

This method evaluates every possible combination of predictors. The output is a data frame and will be auto-paged.

```{r model-building-all-possible}
ols_step_best_subset(model)
```

## Automated Stepwise Procedures

We can also use backward, forward, or stepwise selection to find a good model. These functions also return data frames.

```{r model-building-stepwise}
# Backward Elimination (alpha_out = 0.1)
ols_step_backward_p(model, p_val = 0.1)

# Forward Selection (alpha_in = 0.1)
ols_step_forward_p(model, p_val = 0.1)

# Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)
ols_step_both_p(model, p_enter = 0.1, p_remove = 0.1)
```

# Multicollinearity

This section explores the issue of multicollinearity, where predictors are highly correlated.

## A Simple Example

```{r multicollinearity-simple}
y <- c(19, 20, 37, 39, 36, 38)
x1 <- c(4, 4, 7, 7, 7.1, 7.1)
x2 <- c(16, 16, 49, 49, 50.4, 50.4)
cor(data.frame(x1, x2))

# Full model
fit_multi <- lm(y ~ x1 + x2)
summary(fit_multi)

# Simple model
fit1_multi <- lm(y ~ x1)
summary(fit1_multi)
```

## VIFs in the Wine Quality Data

We can calculate the Variance Inflation Factor (VIF) for each predictor to diagnose multicollinearity. A VIF \> 10 is often considered problematic.

```{r multicollinearity-wine}
wine.x <- wine[, -ncol(wine)] # Assuming quality is the last column
cor(wine.x)

# Find VIF's using the olsrr package (returns a data frame).
ols_vif_tol(model)
```
