
---
title: "Understanding the Externally Studentized Residuals in Linear Models"
subtitle: "An Illustration with R"
author: "Longhai Li"
date: today
format: 
  html:
    theme: cosmo
    toc: true
    code-fold: true
    number-sections: true
    math: mathjax
    mathjax: true
    md_extensions: +tex_math_dollars
  pdf:
    toc: false
    number-sections: true
    include-in-header:
      text: |
        \usepackage{amsmath}
---

## Introduction

In statistical modeling, identifying data points that don't fit‚Äîthe **outliers**‚Äîis a critical step. The most reliable tool for this job is the **externally studentized residual**. Its power comes from a simple, intuitive idea: to judge a point fairly, you shouldn't use that point when building your model. This is the core principle of **Leave-One-Out Cross-Validation (LOOCV)**.

This article provides a complete walkthrough of this essential concept. We'll start with the basic linear model, introduce the necessary notation, explore the flaws of simpler residuals, and then formally define and prove the equivalence of the conceptual and computational formulas for studentized residuals. Finally, we'll make it all concrete with a simple example.



## The Linear Model

Our discussion is based on the standard multiple linear regression model. In matrix form, the relationship between a response vector **Y** and a predictor matrix **X** is:
\begin{equation}
\mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
\end{equation}
Where:

* **Y** is an *n* x 1 vector of the observed outcomes.
* **X** is the *n* x *p* design matrix of predictor variables (where *p* is the number of coefficients, including the intercept).
* **$\beta$** is the *p* x 1 vector of unknown true coefficients we want to estimate.
* **$\epsilon$** is an *n* x 1 vector of unobservable random errors, assumed to be independent and identically distributed with a mean of 0 and a variance of $\sigma^2$.



## Our Notations üí°

To discuss models fit with all data versus those with one point removed, we need clear notation.

**Full Data Model (Using all *n* observations)**

* **$\hat{\beta}$**: The estimated coefficient vector.
* **$\hat{y}_i$**: The predicted value for observation *i* from this model.
* **$e_i$**: The **ordinary residual** ($e_i = y_i - \hat{y}_i$).
* **$\hat{\sigma}$**: The estimated standard deviation of the errors (Residual Standard Error).
* **$h_{ii}$**: The **leverage** of observation *i*, a measure of how much its x-values influence the model.

**Leave-One-Out (LOOCV) Model**

* **$\hat{\beta}_{-i}$**: The coefficient vector estimated after **removing** observation *i*.
* **$\hat{y}_{i,-i}$**: The predicted value for observation *i*, from the model fit **without** observation *i*.
* **$e_{i,-i}$**: The **deleted residual** ($e_{i,-i} = y_i - \hat{y}_{i,-i}$).
* **$\hat{\sigma}_{-i}$**: The standard deviation of the errors estimated from the model fit **without** observation *i*.



## Non-studentized Residuals

Before getting to the correct solution, it's crucial to understand why simpler methods of standardizing residuals are flawed.

### **The Ordinary Residual ($e_i$): Too Small and $x_i$ Dependent**
The most basic residual, $e_i$, is problematic for two key reasons.

An outlier has an undue influence on the model, pulling the regression line towards itself. This makes its own predicted value, $\hat{y}_i$, artificially close to its actual value, $y_i$. As a result, its residual, $e_i$, is **deceptively small** and doesn't reflect the true magnitude of the error.

The variance of an ordinary residual is not constant; it depends on the point's leverage.
    \begin{equation}
    \text{Var}(e_i) = \sigma^2(1 - h_{ii})
    \end{equation}
We know
\begin{equation}
\hat{y}_i = \mathbf{x}_i^\top \hat{\beta} = \mathbf{x}_i^\top\!\bigl(\beta + (X^\top X)^{-1} X^\top \epsilon\bigr).
\end{equation}
So the residual is
\begin{equation}
e_i = y_i - \hat{y}_i = \epsilon_i - \mathbf{x}_i^\top (X^\top X)^{-1} X^\top \epsilon.
\end{equation}

The variance can be derived from the hat matrix $H = X(X^\top X)^{-1}X^\top$. Since $\hat{y} = HY$, we have
\begin{equation}
e = (I - H)\epsilon.
\end{equation}
Thus,
\begin{equation}
\mathrm{Var}(e) = (I-H)\,\sigma^2\,(I-H)^\top = (I-H)\sigma^2.
\end{equation}
Therefore, for the $i$th residual,
\begin{equation}
\mathrm{Var}(e_i) = \sigma^2(1 - h_{ii}).
\end{equation}

Since leverage ($h_{ii}$) is always greater than 0, the variance of an ordinary residual is always **less than the true error variance, $\sigma^2$**. High-leverage points act as "anchors" for the line and have even smaller variance.

### **The LOOCV Residual ($e_{i,-i}$): Too large and $x_i$-Dependent Variance**
The deleted residual, $e_{i,-i}$, solves the "too small" problem. Because the model isn't influenced by the point it's predicting, the residual is an honest measure of prediction error. However, its variance is still not constant. The variance of a deleted residual also depends on leverage, but in the opposite way.
    \begin{equation}
    \text{Var}(e_{i,-i}) = \frac{\sigma^2}{1 - h_{ii}}
    \end{equation}

From the key identity @eq-key,
\begin{equation}
e_{i,-i} = \frac{e_i}{1 - h_{ii}}.
\end{equation}
Therefore,
\begin{equation}
\mathrm{Var}(e_{i,-i}) = \frac{\mathrm{Var}(e_i)}{(1-h_{ii})^2}
= \frac{\sigma^2(1-h_{ii})}{(1-h_{ii})^2}
= \frac{\sigma^2}{1-h_{ii}}.
\end{equation}

Since $1-h_{ii}$ is less than 1, the variance of a deleted residual is always **greater than the true error variance, $\sigma^2$**. This is because it has two sources of randomness: the error in the point itself ($y_i$) and the error in the prediction ($\hat{y}_{i,-i}$).


## Studentized Residuals 

### Studentized LOOCV (Deleted) Residual

The correct solution is to take the LOOCV residual and divide it by its true standard error, which properly accounts for its larger, x-dependent variance. This is the **externally studentized residual**, $t_i$, defined as follows:


$$
t_i = \frac{e_{i,-i}}{\text{SE}(e_{i,-i})} = \frac{e_{i,-i}}{\frac{\hat{\sigma}_{-i}}{\sqrt{1-h_{ii}}}}
$${#eq-sloo}
This final value is a reliable diagnostic. Under the null hypothesis that the observation is not an outlier, it follows a **Student's t-distribution** with $n - p - 1$ degrees of freedom.



### Studentized Full Data Residuals  

Calculating the conceptual formula appears to require fitting *n* different regression models‚Äîa computationally expensive task. Fortunately, a mathematical identity allows us to calculate the exact same value using only the results from the single, full data model.

$$
t_i = \frac{e_i}{\hat{\sigma}_{-i}\sqrt{1 - h_{ii}}}
$${#eq-sfull}
This is not an approximation; it is an **exact algebraic rearrangement** of the conceptual definition.

### Equivalence of @eq-sfull and @eq-sloo

#### Key Identity Linking $e_{i,-i}$ and $e_i$

$$e_{i,-i} = \frac{e_i}{1 - h_{ii}}$${#eq-key}

This identity shows that we can find the "pure" leave-one-out residual without actually refitting the model; all we need are the results from the single model fit on the full dataset.

#### Proof of Equivalence ‚öôÔ∏è



Let's start with the conceptual definition of the studentized LOOCV residuals @eq-sloo and show how it transforms into @eq-sfull.

- **Start with the conceptual LOOCV definition:**
    \begin{equation}
    t_i = \frac{e_{i,-i}}{\text{SE}(e_{i,-i})} = \frac{e_{i,-i}}{\frac{\hat{\sigma}_{-i}}{\sqrt{1-h_{ii}}}}
    \end{equation}

- **Substitute the key identity** into the numerator:
    \begin{equation}
    t_i = \frac{\frac{e_i}{1 - h_{ii}}}{\frac{\hat{\sigma}_{-i}}{\sqrt{1 - h_{ii}}}}
    \end{equation}

- **Simplify the complex fraction.** We can do this by multiplying the numerator by the reciprocal of the denominator:
    \begin{equation}
    t_i = \frac{e_i}{1-h_{ii}} \cdot \frac{\sqrt{1-h_{ii}}}{\hat{\sigma}_{-i}}
    \end{equation}

- **Cancel the terms.** Since $1 - h_{ii} = (\sqrt{1 - h_{ii}})^2$, one of the $\sqrt{1 - h_{ii}}$ terms in the denominator cancels with the term in the numerator. This leaves us with the computational shortcut formula:
    \begin{equation}
    t_i = \frac{e_i}{\hat{\sigma}_{-i}\sqrt{1-h_{ii}}}
    \end{equation}

This proves that the two formulas are mathematically identical. The computational shortcut is simply a clever algebraic rearrangement of the more intuitive LOOCV definition, allowing for efficient and accurate calculation. ‚úÖ

## List of Residuals

In this article, we will compare the four residuals given as:


| Short Name | Full Name | Formula |
| :--- | :--- | :--- |
| **NS-Full** | Non-studentized Full-Data Residual | $$\frac{e_i}{\hat{\sigma}}$$ |
| **NS-LOO** | Non-studentized LOOCV Residual | $$\frac{e_{i,-i}}{\hat{\sigma}_{-i}}$$ |
| **ST-LOO** | Studentized LOOCV Residual | $$\frac{e_{i,-i}}{\hat{\sigma}_{-i}/\sqrt{1-h_{ii}}}$$ |
| **ST-Full** | Studentized Full-Data Residual| $$\frac{e_i}{\hat{\sigma}_{-i}\sqrt{1-h_{ii}}}$$ |

Table: {#tbl-residuals Four Plausible Residuals}

## Example


```{r}
#| label: plaintext-column-names
#| echo: true
#| message: false
#| warning: false

# Load libraries
library(dplyr)
library(knitr)

# -------------------------------
# 1) Data and full-model fit
# -------------------------------
set.seed(123)
n <- 20
x <- 1:n
y <- 2 + 3 * x + rnorm(n, mean = 0, sd = 5)
y[10] <- y[10] + 25  # add an outlier at x = 6
y[11] <- y[11] - 10  # add an outlier at x = 6

full_data  <- data.frame(x = x, y = y)

full_model <- lm(y ~ x, data = full_data)
leverage   <- hatvalues(full_model)
e_full     <- resid(full_model)
sigma_hat_val  <- summary(full_model)$sigma

# -------------------------------
# 2) LOOCV quantities (refit n times)
# -------------------------------
e_del_val <- numeric(n)
sigma_minus_i_val <- numeric(n)

for (i in 1:n) {
  loocv_model <- lm(y ~ x, data = full_data[-i, ])
  yhat_minus  <- predict(loocv_model, newdata = full_data[i, , drop = FALSE])
  e_del_val[i]    <- full_data$y[i] - yhat_minus
  sigma_minus_i_val[i] <- summary(loocv_model)$sigma
}

# -------------------------------
# 3) Assemble and round results
# -------------------------------
residuals_df <- data.frame(
  x = full_data$x,
  h = as.numeric(leverage),
  e_i = as.numeric(e_full),
  sigma_hat = as.numeric(sigma_hat_val),
  e_i_minus_i = as.numeric(e_del_val),
  e_i_minus_i_2 = e_full/(1-leverage),
  sigma_minus_i = as.numeric(sigma_minus_i_val),
  `NS-Full` = e_full / sigma_hat_val,
  `NS-LOO` = e_del_val / sigma_minus_i_val,
  `ST-LOO` = e_del_val / (sigma_minus_i_val / sqrt(1 - leverage)),
  `ST-Full` = rstudent(full_model)
) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))


# 4) Create simple display names
display_names <- c("$x_i$", "$h_i$", "$e_i$", "$\\hat{\\sigma}$", 
                   "$e_{i,-i}$", "$\\hat{e}_{i,-i}$","$\\hat{\\sigma}_{-i}$", 
                   "NS-Full", "NS-LOO", "ST-LOO", "ST-Full")

# 5) Display the table
knitr::kable(
  residuals_df,
  caption = "Residual variants with plain-text column names.",
  col.names = display_names,
  escape=FALSE,
  align = rep("r", length(display_names))
)
```






```{r}
#| label: plot-four-residuals-new-names
#| echo: true
#| message: false
#| warning: false
#| fig-cap: "Four residual variants plotted against the predictor variable x."

# Load libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)


# -------------------------------
# 3) Plotting Code with Updated Names
# -------------------------------

# Prepare data for plotting
plot_df <- residuals_df %>%
  # Use the new, simple column names (R converts '-' to '.')
  select(
    x,
    NS.Full,
    NS.LOO,
    ST.LOO,
    ST.Full
  ) %>%
  pivot_longer(
    cols = -x,
    names_to = "residual_type",
    values_to = "residual_value"
  )

# Update the names in the mapping vectors
shape_map <- c(
  NS.Full = 16,  # solid circle
  NS.LOO  = 1,   # hollow circle
  ST.LOO  = 8,   # asterisk
  ST.Full = 8    # asterisk
)

labels_map <- c(
  NS.Full = "NS-Full",
  NS.LOO  = "NS-LOO",
  ST.LOO  = "ST-LOO",
  ST.Full = "ST-Full"
)

color_map <- c(
  NS.Full = "#1f77b4",  # blue
  NS.LOO  = "#ff7f0e", # orange
  ST.LOO  = "#2ca02c",  # green
  ST.Full = "#d62728"   # red
)

# Generate the plot
ggplot(
  plot_df,
  aes(x = x, y = residual_value,
      shape = residual_type, color = residual_type, group = residual_type)
) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(size = 3, stroke = 1.2) + # Increased stroke for visibility
  scale_shape_manual(
    values = shape_map,
    breaks = names(labels_map),
    labels = unname(labels_map),
    name = "Residual Type"
  ) +
  scale_color_manual(
    values = color_map,
    breaks = names(labels_map),
    labels = unname(labels_map),
    name = "Residual Type"
  ) +
  labs(
    title = "Four Residual Variants vs x",
    x = "x_i",
    y = "Residual value"
  ) +
  theme_bw() +
  theme(
    legend.position = "right",
    legend.title = element_text(face = "bold")
  )
```

