[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Methods for Research",
    "section": "",
    "text": "1 Preface\nThis book contains lecture notes for STAT 845: Statistical Methods for Research at University of Saskatchewan",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Methods for Research</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html",
    "href": "unit2-slr/slr.html",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "3 Overview of Simple Linear Regression\nTo make the simple linear regression model concrete, let’s first visualize a simulated dataset that follows \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\qquad\n\\varepsilon_i \\sim \\mathcal N(0, \\sigma^2).\n\\]\nHere, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\varepsilon_i\\) represents random noise.\nCode\nset.seed(2025)\nn &lt;- 40\nbeta0 &lt;- 2; beta1 &lt;- 1.5; sigma &lt;- 2\nx &lt;- runif(n, 0, 10)\ny &lt;- beta0 + beta1 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x, y)\n\nfit &lt;- lm(y ~ x, data = dat)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"Predictor X\", ylab = \"Response Y\",\n     main = \"Simulated Data with Fitted Linear Regression Line\")\nabline(fit, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Observed data\", \"Fitted line\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"steelblue\", \"red\"), bty = \"n\")\nThe scatterplot shows data points scattered around a line — the red line is the fitted regression model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#least-squares-estimation",
    "href": "unit2-slr/slr.html#least-squares-estimation",
    "title": "2  Simple Linear Regression",
    "section": "3.1 Least Squares Estimation",
    "text": "3.1 Least Squares Estimation\nGoal: Find \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize \\[\n\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2.\n\\]\nSolutions: \\[\n\\hat\\beta_1 = \\frac{\\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2}\n= \\frac{S_{xy}}{S_{xx}},\n\\qquad\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\,\\bar x.\n\\]\nHere \\[\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y),\n\\qquad\nS_{xx} = \\sum_i (x_i - \\bar x)^2.\n\\]\nShortcut (computational) formulas: \\[\nS_{xy} = \\sum_i x_i y_i - n\\,\\bar x\\,\\bar y,\n\\qquad\nS_{xx} = \\sum_i x_i^2 - n\\,\\bar x^2.\n\\]\nInterpretation:\n- \\(\\hat\\beta_1\\) measures the estimated change in \\(Y\\) for each unit increase in \\(X\\).\n- \\(\\hat\\beta_0\\) represents the fitted value of \\(Y\\) when \\(X=0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#residual-and-sum-of-squares-definitions",
    "href": "unit2-slr/slr.html#residual-and-sum-of-squares-definitions",
    "title": "2  Simple Linear Regression",
    "section": "3.2 Residual and Sum of Squares Definitions",
    "text": "3.2 Residual and Sum of Squares Definitions\nLet \\(\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i\\) and \\(e_i = y_i - \\hat y_i\\).\n\n\n\n\n\n\n\n\nSymbol\nDefinition\nComputing Formula (in terms of \\(S_{xx}, S_{xy}\\), etc.)\n\n\n\n\nSST\nTotal Sum of Squares\n\\(\\displaystyle \\sum_i (y_i - \\bar y)^2 = S_{yy} = \\sum_i y_i^2 - n\\,\\bar y^2\\)\n\n\nSSR\nRegression Sum of Squares\n\\(\\displaystyle \\sum_i (\\hat y_i - \\bar y)^2 = \\hat\\beta_1^2 S_{xx} = \\frac{S_{xy}^2}{S_{xx}}\\)\n\n\nSSE\nError (Residual) Sum of Squares\n\\(\\displaystyle \\sum_i (y_i - \\hat y_i)^2 = S_{yy} - \\frac{S_{xy}^2}{S_{xx}}\\)\n\n\n\nIdentity: \\[\n\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}.\n\\]\nHere, \\[\nS_{xx} = \\sum_i (x_i - \\bar x)^2 = \\sum_i x_i^2 - n\\bar x^2, \\qquad\nS_{yy} = \\sum_i (y_i - \\bar y)^2 = \\sum_i y_i^2 - n\\bar y^2, \\qquad\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y) = \\sum_i x_i y_i - n\\bar x \\bar y.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#coefficient-of-determination-r2",
    "href": "unit2-slr/slr.html#coefficient-of-determination-r2",
    "title": "2  Simple Linear Regression",
    "section": "3.3 Coefficient of Determination (\\(R^2\\))",
    "text": "3.3 Coefficient of Determination (\\(R^2\\))\nMeasures the proportion of total variation in \\(Y\\) explained by \\(X\\): \\[\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}}\n= 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}.\n\\]\nInterpretation:\n\n\\(R^2 = 1\\) means perfect linear fit;\n\\(R^2 = 0\\) means the model explains none of the variation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#f-test-for-overall-significance",
    "href": "unit2-slr/slr.html#f-test-for-overall-significance",
    "title": "2  Simple Linear Regression",
    "section": "3.4 F-test for Overall Significance",
    "text": "3.4 F-test for Overall Significance\nTests whether \\(X\\) is linearly related to \\(Y\\).\nHypotheses: \\[\nH_0: \\beta_1 = 0\n\\quad \\text{vs.} \\quad\nH_A: \\beta_1 \\ne 0.\n\\]\nTest Statistic: \\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n= \\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\n\\sim F_{1,n-2}\\quad (H_0).\n\\]\np-value approach for observe \\(F^{\\mathrm{obs}}\\):\nGiven the observed statistic (F^{}) with ((1,,n-2)) df, \\[\np-\\text{value} \\;=\\; \\Pr\\!\\big(F_{1,\\,n-2} \\ge F^{\\text{obs}}\\big)\n\\;=\\; \\mathrm{pf}\\!\\big(F^{\\text{obs}},\\,1,\\,n-2,\\ \\text{lower.tail}= \\mathrm{FALSE}\\big).\n\\]\n\n\nCode\n# -- Inputs (provide these from your analysis context) -------------------------\n# n   &lt;- ...   # sample size\n# SSR &lt;- ...   # regression sum of squares\n# SSE &lt;- ...   # error sum of squares\nn   &lt;- 20\nSSR &lt;- 5\nSSE &lt;- 40\n\n\n\n\ndf1  &lt;- 1\ndf2  &lt;- n - 2\nFobs &lt;- (SSR/df1) / (SSE/df2)         # observed F\npval &lt;- pf(Fobs, df1 = df1, df2 = df2, lower.tail = FALSE)\npval\n\n\n# [1] 0.1509505\n\n\nCode\n# -- Plot F density and shade the p-value tail (with proper annotations) -------\nxmax &lt;- max(qf(0.995, df1, df2), Fobs * 1.2)  # extra space for labels\npeak &lt;- max(df(seq(0, xmax, length.out = 500), df1, df2))\n\n# Density curve\ncurve(df(x, df1, df2), from = 0, to = xmax,\n      xlab = \"F\", ylab = \"Density\",\n      main = sprintf(\"F(%d, %d) density  |  observed F = %.3f\", df1, df2, Fobs))\n\n# Shade right tail (p-value region)\nxs &lt;- seq(Fobs, xmax, length.out = 300)\nys &lt;- df(xs, df1, df2)\npolygon(c(Fobs, xs, xmax), c(0, ys, 0),\n        col = rgb(0, 0, 0, 0.18), border = NA)\n\n# Vertical line at Fobs (optional visual aid)\nabline(v = Fobs, lwd = 2)\n\n# ---- Annotation for F^obs pointing to the x-axis value (Fobs, 0) -------------\nx_txt_F &lt;- Fobs + 0.06 * xmax\ny_txt_F &lt;- 0.45 * peak\narrows(x0 = x_txt_F, y0 = y_txt_F, x1 = Fobs, y1 = 0,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_F, y_txt_F,\n     labels = bquote(F^{obs} == .(format(Fobs, digits = 3))),\n     pos = 4)\n\n# ---- Annotation for p-value pointing into the shaded tail --------------------\nx_tip_p &lt;- (Fobs + xmax) / 1.7\ny_tip_p &lt;- df(x_tip_p, df1, df2)\nx_txt_p &lt;- Fobs + 0.08 * xmax\ny_txt_p &lt;- 0.80 * peak\narrows(x0 = x_txt_p, y0 = y_txt_p, x1 = x_tip_p, y1 = y_tip_p,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_p, y_txt_p,\n     labels = bquote(p == .(format(pval, digits = 4, scientific = TRUE))),\n     pos = 4)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#t-test-for-the-slope-beta_1",
    "href": "unit2-slr/slr.html#t-test-for-the-slope-beta_1",
    "title": "2  Simple Linear Regression",
    "section": "3.5 t-test for the Slope \\(\\beta_1\\)",
    "text": "3.5 t-test for the Slope \\(\\beta_1\\)\nEquivalent to the \\(F\\)-test in simple regression since \\(t^2 = F\\).\nFormula: \\[\nt = \\frac{\\hat\\beta_1}{\\operatorname{SE}(\\hat\\beta_1)},\n\\qquad\n\\operatorname{SE}(\\hat\\beta_1) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_i (x_i - \\bar x)^2}},\n\\qquad\n\\hat\\sigma^2 = \\frac{\\mathrm{SSE}}{n-2}.\n\\]\nDistribution: \\[\nt \\sim t_{n-2}\\quad (H_0:\\beta_1=0).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#prediction-for-a-new-case-x_0",
    "href": "unit2-slr/slr.html#prediction-for-a-new-case-x_0",
    "title": "2  Simple Linear Regression",
    "section": "3.6 Prediction for a New Case \\(x_0\\)",
    "text": "3.6 Prediction for a New Case \\(x_0\\)\nPredicted mean response: \\[\n\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0.\n\\]\n95% Confidence interval for mean response: \\[\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n\\]\n95% Prediction interval for a new observation: \\[\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n\\]\n\nSummary Cheat Sheet\n\n\n\n\n\n\n\nConcept\nKey Formula\n\n\n\n\nModel\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\)\n\n\nLS Estimates\n\\(\\hat\\beta_1 = S_{xy}/S_{xx}\\), \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\)\n\n\nDecomposition\n\\(\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}\\)\n\n\n\\(R^2\\)\n\\(R^2 = 1 - \\mathrm{SSE}/\\mathrm{SST}\\)\n\n\n\\(F\\)-test\n\\(F = (\\mathrm{SSR}/1)/(\\mathrm{SSE}/(n-2))\\)\n\n\n\\(t\\)-test\n\\(t = \\hat\\beta_1 / \\operatorname{SE}(\\hat\\beta_1)\\)\n\n\nPrediction\n\\(\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#input-data",
    "href": "unit2-slr/slr.html#input-data",
    "title": "2  Simple Linear Regression",
    "section": "4.1 Input data",
    "text": "4.1 Input data\n\n\nCode\nissu &lt;- data.frame(\n  driving = c(5, 2, 12, 9, 15, 6, 25, 16),\n  premium = c(64, 87, 50, 71, 44, 56, 42, 60)\n)\n\ny &lt;- issu$premium\nx &lt;- issu$driving\nxbar &lt;- mean(x); ybar &lt;- mean(y); n &lt;- length(y)\n\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Vehicle Insurance: Premium vs. Driving\")\nabline(h = ybar, lty = 3)\n\n\n\n\n\n\n\n\n\nNarrative. The horizontal line at \\(\\bar y\\) represents the intercept-only model. Any fitted line that tilts away from this must earn its keep by reducing residual variation enough to offset the loss of one degree of freedom.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#estimating-regression-coefficients",
    "href": "unit2-slr/slr.html#estimating-regression-coefficients",
    "title": "2  Simple Linear Regression",
    "section": "4.2 Estimating regression coefficients",
    "text": "4.2 Estimating regression coefficients\n\n\nCode\nfit.issu &lt;- lm(y ~ x)\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Fitted Simple Linear Regression\")\nabline(fit.issu, lwd = 2)\n\n\n\n\n\n\n\n\n\nThe slope estimate \\(\\hat\\beta_1\\) captures the marginal change in premium per unit of driving (units of \\(y\\) per unit of \\(x\\)). Inference on \\(\\beta_1\\) tells us whether the pattern rises above noise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#residuals-and-fitted-values-geometry-picture",
    "href": "unit2-slr/slr.html#residuals-and-fitted-values-geometry-picture",
    "title": "2  Simple Linear Regression",
    "section": "4.3 Residuals and fitted values (geometry picture)",
    "text": "4.3 Residuals and fitted values (geometry picture)\nLet \\(\\hat y_i=\\hat\\beta_0+\\hat\\beta_1 x_i\\) and \\(\\tilde y_i=\\bar y\\). Residuals are \\(e_i=y_i-\\hat y_i\\) (model) and \\(y_i-\\bar y\\) (null). Visualizing all three clarifies the ANOVA identity.\n\n\nCode\nbeta0 &lt;- coef(fit.issu)[1]\nbeta1 &lt;- coef(fit.issu)[2]\nfitted1 &lt;- beta0 + beta1 * x\nfitted0 &lt;- rep(ybar, n)\nresidual1 &lt;- y - fitted1\nresidual0 &lt;- y - fitted0\n\ndata.frame(y, fitted0, residual0, fitted1, residual1,\n           diff.fitted = fitted1 - fitted0)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#sst-ssr-sse-and-their-meanings",
    "href": "unit2-slr/slr.html#sst-ssr-sse-and-their-meanings",
    "title": "2  Simple Linear Regression",
    "section": "4.4 SST, SSR, SSE and their meanings",
    "text": "4.4 SST, SSR, SSE and their meanings\n\n\\(\\text{SST}=\\sum (y_i-\\bar y)^2\\) quantifies total variability around the mean.\n\\(\\text{SSR}=\\sum (\\hat y_i-\\bar y)^2\\) is the part explained by \\(x\\).\n\\(\\text{SSE}=\\sum (y_i-\\hat y_i)^2\\) is the leftover (unexplained) variability.\n\n\n\nCode\nSST &lt;- sum((y - fitted0)^2); SST\n\n\n# [1] 1557.5\n\n\nCode\nSSE &lt;- sum((y - fitted1)^2); SSE\n\n\n# [1] 639.0065\n\n\nCode\nSSR &lt;- SST - SSE; SSR\n\n\n# [1] 918.4935\n\n\nDirect check: \\(\\text{SSR}=\\sum(\\hat y_i-\\bar y)^2\\).\n\n\nCode\nsum((fitted1 - fitted0)^2)\n\n\n# [1] 918.4935",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#visual-anova-on-an-rss-plot",
    "href": "unit2-slr/slr.html#visual-anova-on-an-rss-plot",
    "title": "2  Simple Linear Regression",
    "section": "4.5 Visual ANOVA on an RSS plot",
    "text": "4.5 Visual ANOVA on an RSS plot\nWe place the residual sum of squares against model dimension to show the trade-off between fit and df.\n\n\nCode\n# Recompute cleanly\nSST &lt;- sum((y - mean(y))^2)\nSSE &lt;- sum(resid(fit.issu)^2)\nSSR &lt;- SST - SSE\ndf_SSR &lt;- 1\ndf_SSE &lt;- n - 2\n\npar(mar = c(6, 4, 4, 2) + 0.1)\nplot(c(1, 2, n), c(SST, SSE, 0), type = \"b\", pch = 19,\n     xlab = \"Number of Parameters in Model\",\n     ylab = \"Residual Sum of Squares (RSS)\",\n     main = \"ANOVA Geometry on RSS vs. Model Size\",\n     xlim = c(0, 14), ylim = c(-400, SST * 1.1), xaxt = \"n\")\naxis(1, at = c(1, 2, n), labels = c(\"1 (Intercept)\", \"2 (+Slope)\", paste(n, \"(Saturated)\")))\nabline(h = seq(0, 2000, by = 100), lty = 3, col = \"grey\")\n\npar(xpd = TRUE)\narrows(9, 0, 9, SSE, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, SSE/2, \"SSE\", col = \"blue\", pos = 4, font = 2, cex = 1.2)\n\narrows(9, SSE, 9, SST, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, (SST + SSE)/2, \"SSR\", col = \"red\", pos = 4, font = 2, cex = 1.2)\n\narrows(2, -200, n, -200, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext((2 + n)/2, -250, paste(\"df_SSE =\", df_SSE), col = \"blue\", font = 2)\n\narrows(1, -200, 2, -200, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(1.5, -250, paste(\"df_SSR =\", df_SSR), col = \"red\", font = 2)\npar(xpd = FALSE)\n\nf_value &lt;- (SSR/df_SSR) / (SSE/df_SSE)\np_value &lt;- pf(f_value, df1 = df_SSR, df2 = df_SSE, lower.tail = FALSE)\nlegend(\"topright\",\n       legend = c(sprintf(\"F-statistic: %.2f\", f_value),\n                  sprintf(\"p-value: %.3f\", p_value)),\n       title = \"ANOVA Results\", bty = \"o\", cex = 0.9)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#r2-f-and-a-compact-anova-table",
    "href": "unit2-slr/slr.html#r2-f-and-a-compact-anova-table",
    "title": "2  Simple Linear Regression",
    "section": "4.6 \\(R^2\\), \\(F\\) and a compact ANOVA table",
    "text": "4.6 \\(R^2\\), \\(F\\) and a compact ANOVA table\n\n\nCode\nR2 &lt;- SSR / SST; R2\n\n\n# [1] 0.5897229\n\n\nCode\nf  &lt;- (SSR/1) / (SSE/(n-2)); f\n\n\n# [1] 8.624264\n\n\nCode\npvf &lt;- pf(f, df1 = 1, df2 = n-2, lower.tail = FALSE); pvf\n\n\n# [1] 0.0260588\n\n\nCode\nFtable &lt;- data.frame(\n  Source = c(\"Regression\", \"Error\"),\n  df     = c(1, n - 2),\n  SS     = c(SSR, SSE),\n  MS     = c(SSR/1, SSE/(n-2)),\n  F      = c(f, NA),\n  pvalue = c(pvf, NA),\n  R2part = c(SSR, SSE) / SST\n)\nFtable\n\n\n\n  \n\n\n\nA call to anova() reproduces the same test:\n\n\nCode\nanova(fit.issu)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#sampling-distributions-via-animation",
    "href": "unit2-slr/slr.html#sampling-distributions-via-animation",
    "title": "2  Simple Linear Regression",
    "section": "4.7 Sampling distributions via animation",
    "text": "4.7 Sampling distributions via animation\nUnder \\(H_0:\\beta_1=0\\), \\(F\\) follows \\(F_{1,n-2}\\). Under \\(H_A\\), the distribution shifts right (noncentral \\(F\\)).\n\n4.7.1 Null world (\\(H_0\\) true)\n\n\nCode\noriginal_fit &lt;- lm(y ~ x)\nintercept_H0 &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\n\nfor (iter in 1:50) {\n  sim.y &lt;- intercept_H0 + rnorm(n, 0, model_sigma)\n\n  par(mfrow = c(1,2))\n  fit.sim.y &lt;- lm(sim.y ~ x)\n  plot(sim.y ~ x, ylim = c(0,100), main = \"Data under H0 (slope = 0)\")\n  abline(fit.sim.y, col = \"red\", lwd = 2)\n  abline(h = mean(sim.y), col = \"blue\")\n\n  a &lt;- anova(fit.sim.y)\n  ss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\n  num.par &lt;- cumsum(c(1, a$Df))\n  plot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\n  abline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\n  f_value &lt;- a$\"F value\"[1]\n  p_value &lt;- a$\"Pr(&gt;F)\"[1]\n  legend(\"topright\",\n         legend = c(sprintf(\"F: %.2f\", f_value),\n                    sprintf(\"p: %.3f\", p_value)),\n         title = \"ANOVA\", bty = \"o\", cex = 0.9)\n}\n\n\n\n\n\n\n\n\n\n\n\n4.7.2 Alternative world (\\(H_1\\) true)\n\n\nCode\noriginal_fit &lt;- lm(y ~ x)\nintercept_HA &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\nslope_HA     &lt;- -2  # pick a non-zero slope\n\nfor (iter in 1:50) {\n  mean_y &lt;- intercept_HA + slope_HA * x\n  sim.y  &lt;- mean_y + rnorm(n, 0, model_sigma)\n\n  par(mfrow = c(1,2))\n  fit.sim.y &lt;- lm(sim.y ~ x)\n  plot(sim.y ~ x, ylim = c(0,80), main = \"Data under HA (slope = -2)\")\n  abline(fit.sim.y, col = \"red\", lwd = 2)\n  abline(h = mean(sim.y), col = \"blue\")\n\n  a &lt;- anova(fit.sim.y)\n  ss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\n  num.par &lt;- cumsum(c(1, a$Df))\n  plot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\n  abline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\n  f_value &lt;- a$\"F value\"[1]\n  p_value &lt;- a$\"Pr(&gt;F)\"[1]\n  legend(\"topright\",\n         legend = c(sprintf(\"F: %.2f\", f_value),\n                    sprintf(\"p: %.3e\", p_value)),\n         title = \"ANOVA\", bty = \"o\", cex = 0.9)\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#data",
    "href": "unit2-slr/slr.html#data",
    "title": "2  Simple Linear Regression",
    "section": "5.1 Data",
    "text": "5.1 Data\n\n\nCode\nx &lt;- c(0.99, 1.02, 1.15, 1.29, 1.46, 1.36, 0.87, 1.23, 1.55, 1.40, 1.19,\n       1.15, 0.98, 1.01, 1.11, 1.20, 1.26, 1.32, 1.43, 0.95)\ny &lt;- c(90.01, 89.05, 91.43, 93.74, 96.73, 94.45, 87.59, 91.77, 99.42, 93.65,\n       93.54, 92.52, 90.56, 89.54, 89.85, 90.39, 93.25, 93.41, 94.98, 87.33)\nn &lt;- length(x); n\n\n\n# [1] 20\n\n\nCode\npurity.data &lt;- data.frame(x = x, y = y)\nhead(purity.data)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#fit-and-quick-summary",
    "href": "unit2-slr/slr.html#fit-and-quick-summary",
    "title": "2  Simple Linear Regression",
    "section": "5.2 Fit and quick summary",
    "text": "5.2 Fit and quick summary\n\n\nCode\nfit &lt;- lm(y ~ x, data = purity.data)\nsummary(fit)\n\n\n# \n# Call:\n# lm(formula = y ~ x, data = purity.data)\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -1.83029 -0.73334  0.04497  0.69969  1.96809 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)   74.283      1.593   46.62  &lt; 2e-16 ***\n# x             14.947      1.317   11.35 1.23e-09 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 1.087 on 18 degrees of freedom\n# Multiple R-squared:  0.8774,  Adjusted R-squared:  0.8706 \n# F-statistic: 128.9 on 1 and 18 DF,  p-value: 1.227e-09\n\n\nInterpretation. The slope’s sign gives the direction of association; its \\(t\\) test (or \\(F\\) with 1 df) assesses evidence for a trend. Look at \\(\\hat\\sigma\\) for noise scale and \\(R^2\\) for variance explained.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#scatter-with-fitted-line",
    "href": "unit2-slr/slr.html#scatter-with-fitted-line",
    "title": "2  Simple Linear Regression",
    "section": "5.3 Scatter with fitted line",
    "text": "5.3 Scatter with fitted line\n\n\nCode\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Oxygen Purity vs Hydrocarbon Level\")\nabline(fit, col = \"red\", lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#coefficient-cis-and-anova",
    "href": "unit2-slr/slr.html#coefficient-cis-and-anova",
    "title": "2  Simple Linear Regression",
    "section": "5.4 Coefficient CIs and ANOVA",
    "text": "5.4 Coefficient CIs and ANOVA\n\n\nCode\nconfint(fit, level = 0.95)\n\n\n#                2.5 %   97.5 %\n# (Intercept) 70.93555 77.63108\n# x           12.18107 17.71389\n\n\nCode\nanova(fit)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#mean-response-and-prediction-bands",
    "href": "unit2-slr/slr.html#mean-response-and-prediction-bands",
    "title": "2  Simple Linear Regression",
    "section": "5.5 Mean-response and prediction bands",
    "text": "5.5 Mean-response and prediction bands\nThe mean-response CI narrows near \\(\\bar x\\) and widens at the extremes; the prediction band is wider by the irreducible noise term.\n\n\nCode\nx0 &lt;- seq(min(purity.data$x), max(purity.data$x), length = 50)\nnewdata &lt;- data.frame(x = x0)\n\nest.mean &lt;- predict(fit, newdata = newdata, interval = \"confidence\", level = 0.95)\npred.new &lt;- predict(fit, newdata = newdata, interval = \"prediction\", level = 0.95)\n\n\n\n\nCode\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Regression Line with Confidence and Prediction Bands\")\nabline(fit)\nmatlines(x0, est.mean[, 2:3], col = \"blue\", lty = 2, lwd = 2)\nmatlines(x0, pred.new[, 2:3], col = \"red\",  lty = 3, lwd = 2)\nlegend(\"topleft\", c(\"Confidence Bands (mean)\", \"Prediction Bands (new y)\"),\n       col = c(\"blue\", \"red\"), lty = 2:3, bty = \"n\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#residual-diagnostics-assumptions-check",
    "href": "unit2-slr/slr.html#residual-diagnostics-assumptions-check",
    "title": "2  Simple Linear Regression",
    "section": "5.6 Residual diagnostics (assumptions check)",
    "text": "5.6 Residual diagnostics (assumptions check)\nWe look for no pattern in residuals vs. fits and approximate straightness in the Q–Q plot.\n\n\nCode\npred &lt;- fitted.values(fit)\ne &lt;- resid(fit)\nd &lt;- e / summary(fit)$sigma\n\npar(mfrow = c(2,2))\nplot(purity.data$x, purity.data$y, xlab = \"x\", ylab = \"y\"); abline(fit)\nqqnorm(d, main = \"Normal Q–Q\"); qqline(d)\nplot(pred, d, xlab = \"Fitted\", ylab = \"Std. residuals\", main = \"Residuals vs Fits\"); abline(h = 0, lty = 2)\nplot(1:n, d, xlab = \"Order\", ylab = \"Std. residuals\", main = \"Residuals vs Order\"); abline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#data-and-scatter",
    "href": "unit2-slr/slr.html#data-and-scatter",
    "title": "2  Simple Linear Regression",
    "section": "6.1 Data and scatter",
    "text": "6.1 Data and scatter\n\n\nCode\nstrength &lt;- c(9.95,24.45,31.75,35.00,25.02,16.86,14.38,9.60,24.35,\n              27.50,17.08,37.00,41.95,11.66,21.65,17.89,69.00,10.30,\n              34.93,46.59,44.88,54.12,56.63,22.13,21.15)\nlength &lt;- c(2,8,11,10,8,4,2,2,9,8,4,11,12,2,4,4,20,1,10,\n            15,15,16,17,6,5)\nplot(length, strength, xlab = \"Length\", ylab = \"Strength\",\n     main = \"Strength vs Length (scatter)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#pearson-correlation-and-test",
    "href": "unit2-slr/slr.html#pearson-correlation-and-test",
    "title": "2  Simple Linear Regression",
    "section": "6.2 Pearson correlation and test",
    "text": "6.2 Pearson correlation and test\n\n\nCode\ncor(strength, length)\n\n\n# [1] 0.9818118\n\n\nCode\ncor.test(strength, length)\n\n\n# \n#   Pearson's product-moment correlation\n# \n# data:  strength and length\n# t = 24.801, df = 23, p-value &lt; 2.2e-16\n# alternative hypothesis: true correlation is not equal to 0\n# 95 percent confidence interval:\n#  0.9585414 0.9920735\n# sample estimates:\n#       cor \n# 0.9818118\n\n\nNote. A large \\(|r|\\) and small \\(p\\) indicate linear association; regression further quantifies the slope and supports prediction, with diagnostics to check assumptions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html",
    "href": "unit3-mlr/mlr.html",
    "title": "3  Multiple Linear Regression",
    "section": "",
    "text": "4 Multiple Linear Regression for Wire Bond Strength Dataset",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#loading-data-and-visualization",
    "href": "unit3-mlr/mlr.html#loading-data-and-visualization",
    "title": "3  Multiple Linear Regression",
    "section": "4.1 Loading Data and Visualization",
    "text": "4.1 Loading Data and Visualization\nNote: You must change the file paths in the read.csv() functions below to match the location of the files on your computer (for example C:\\\\Users\\\\&lt;YourUsername&gt;\\\\Documents on Windows).\n\n\nCode\n# Read data. Change the path as necessary.\n# Example: bond.data &lt;- read.csv(\"wire-bond.csv\")\nbond.data &lt;- read.csv(\"wire-bond.csv\")\n\n# This will now be automatically rendered as a paged table\nbond.data\n\n\n\n  \n\n\n\n2D Visualization\n\n\nCode\npar(mfrow = c(1, 3), mar = c(5, 4, 2, 1))\n\n# 1) length vs strength\ni1 &lt;- which(!is.na(bond.data$length) & !is.na(bond.data$strength))\nplot(bond.data$length[i1], bond.data$strength[i1],\n     xlab = \"Wire Length\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$length[i1], bond.data$strength[i1],\n     labels = i1, pos = 1, offset = 0.4, cex = 0.75)\n\n# 2) height vs strength\ni2 &lt;- which(!is.na(bond.data$height) & !is.na(bond.data$strength))\nplot(bond.data$height[i2], bond.data$strength[i2],\n     xlab = \"Die height\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$height[i2], bond.data$strength[i2],\n     labels = i2, pos = 1, offset = 0.4, cex = 0.75)\n\n# 3) height vs length\ni3 &lt;- which(!is.na(bond.data$height) & !is.na(bond.data$length))\nplot(bond.data$height[i3], bond.data$length[i3],\n     xlab = \"Die height\", ylab = \"Length\", pch = 19)\ntext(bond.data$height[i3], bond.data$length[i3],\n     labels = i3, pos = 1, offset = 0.4, cex = 0.75)\n\n\n\n\n\n\n\n\n\n3D Visualize\n\n\nCode\nlibrary(scatterplot3d)\n\npar(mfrow = c(1,1))\ns3d &lt;- with(bond.data, scatterplot3d(\n  x = length,\n  y = height,\n  z = strength,\n  pch = 19,\n  color = \"steelblue\",\n  main = \"3D Scatterplot: Strength vs. Length and Height\",\n  xlab = \"Length\",\n  ylab = \"Height\",\n  zlab = \"Strength\",\n  angle = 60\n))\n\nfit &lt;- lm(strength ~ length + height, data = bond.data)\ns3d$plane3d(fit, lty.box = \"solid\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#model-fitting-and-summary",
    "href": "unit3-mlr/mlr.html#model-fitting-and-summary",
    "title": "3  Multiple Linear Regression",
    "section": "4.2 Model Fitting and Summary",
    "text": "4.2 Model Fitting and Summary\nWe fit a multiple linear regression model with strength as the response variable and length and height as predictors.\n\n\nCode\nfit &lt;- lm(strength ~ length + height, data = bond.data)\nsummary(fit)\n\n\n\nCall:\nlm(formula = strength ~ length + height, data = bond.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.865 -1.542 -0.362  1.196  5.841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.263791   1.060066   2.136 0.044099 *  \nlength      2.744270   0.093524  29.343  &lt; 2e-16 ***\nheight      0.012528   0.002798   4.477 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.288 on 22 degrees of freedom\nMultiple R-squared:  0.9811,    Adjusted R-squared:  0.9794 \nF-statistic: 572.2 on 2 and 22 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary provides the ANOVA F-test for overall significance, \\(R^2\\), adjusted \\(R^2\\), and t-tests for individual coefficients.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#confidence-intervals-and-model-components",
    "href": "unit3-mlr/mlr.html#confidence-intervals-and-model-components",
    "title": "3  Multiple Linear Regression",
    "section": "4.3 Confidence Intervals and Model Components",
    "text": "4.3 Confidence Intervals and Model Components\n\n\nCode\n# Confidence intervals\nconfint(fit)\n\n\n                  2.5 %     97.5 %\n(Intercept) 0.065348613 4.46223426\nlength      2.550313061 2.93822623\nheight      0.006724246 0.01833138\n\n\nCode\n# Fitted values and residuals\npred &lt;- fitted.values(fit)\ne &lt;- resid(fit)\ndata.frame(y = bond.data$strength, y.hat = pred, e = e)\n\n\n\n  \n\n\n\nCode\n# Covariance matrix and standard errors\ncov.mat &lt;- vcov(fit)\ncov.mat\n\n\n             (Intercept)        length        height\n(Intercept)  1.123740429 -3.921612e-02 -1.781991e-03\nlength      -0.039216122  8.746709e-03 -9.903775e-05\nheight      -0.001781991 -9.903775e-05  7.831149e-06\n\n\nCode\ndata.frame(std.error = sqrt(diag(cov.mat)))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#rss-based-quantities",
    "href": "unit3-mlr/mlr.html#rss-based-quantities",
    "title": "3  Multiple Linear Regression",
    "section": "5.1 RSS-Based Quantities",
    "text": "5.1 RSS-Based Quantities\n\n5.1.1 RSS-Based Quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\n\\(R^2\\)\ndf\nMean Squares\n\\(F\\)\nSS\\(_\\mathrm{adj}\\)\n\\(\\hat{\\sigma}^2\\)\n\\(R^2_{\\mathrm{adj}}\\)\n\n\n\n\n\\(x^\\top\\beta\\)\n\\(\\mathrm{SSR} = \\displaystyle \\sum_{i=1}^n (\\hat y_i - \\bar y)^2\\)\n\\(\\displaystyle \\frac{\\mathrm{SSR}}{\\mathrm{SST}}\\)\n\\(k\\)\n\\(\\displaystyle \\mathrm{MSR} = \\frac{\\mathrm{SSR}}{k}\\)\n\\(\\displaystyle \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}\\)\n\\(\\mathrm{SSR}_{\\mathrm{adj}}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{x^\\top\\beta} = \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}\\)\n\\(\\displaystyle \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{\\mathrm{SST}} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{MST}}\\)\n\n\n\\(\\epsilon\\)\n\\(\\mathrm{SSE} = \\displaystyle \\sum_{i=1}^n (y_i - \\hat y_i)^2\\)\n—\n\\(n-p\\)\n\\(\\displaystyle \\mathrm{MSE} = \\frac{\\mathrm{SSE}}{n-p}\\)\n—\n\\(\\mathrm{SSE}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{\\epsilon} = \\mathrm{MSE}\\)\n—\n\n\n\\(y\\)\n\\(\\mathrm{SST} = \\displaystyle \\sum_{i=1}^n (y_i - \\bar y)^2\\)\n—\n\\(n-1\\)\n\\(\\displaystyle \\mathrm{MST} = \\frac{\\mathrm{SST}}{n-1}\\)\n—\n\\(\\mathrm{SST}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{y} = \\mathrm{MST}\\)\n—\n\n\n\n\nInterpretation of the \\(\\hat{\\sigma}^2\\) Column\nThe \\(\\hat{\\sigma}^2\\) column highlights how each sum of squares corresponds to an estimated variance.\nThis view makes the adjusted coefficient of determination clear:\n\\[\nR^2_{\\mathrm{adj}}\n= 1 - \\frac{\\hat{\\sigma}^2_\\epsilon}{\\hat{\\sigma}^2_y}\n= \\frac{\\hat{\\sigma}^2_{x^\\top\\beta}}{\\hat{\\sigma}^2_y}.\n\\]\nHence, the adjusted \\(R^2\\) simply expresses the proportion of total estimated variance attributable to the fitted model \\(X\\beta\\) rather than the residual noise \\(\\epsilon\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#remarks",
    "href": "unit3-mlr/mlr.html#remarks",
    "title": "3  Multiple Linear Regression",
    "section": "5.2 Remarks",
    "text": "5.2 Remarks\n\n5.2.1 Fundamental Identities\n\\[\n\\begin{aligned}\n\\mathrm{SST} &= \\mathrm{SSR} + \\mathrm{SSE}, \\\\\n\\mathrm{MST} &= \\mathrm{MSE} + \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}.\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathrm{SSR}_{\\mathrm{adj}} = (n-1)MST-(n-p+k)\\mathrm{MSE} = \\mathrm{SST}-\\mathrm{SSE} - k\\,\\mathrm{MSE} = \\mathrm{SSR} - k\\,\\mathrm{MSE}.\n\\]\n\n\n\n5.2.2 Difference of \\(\\hat{\\sigma}^2\\) and Mean Squares\nThe quantity \\(\\hat{\\sigma}^2\\) represents the estimated variance associated with each component of the model. MSE and MST are the estimated variances of the \\(\\epsilon\\) and \\(y\\) itself. However, the MSR, although called Mean Square for Regression (MSR) is NOT an estimate of the variance or sample variance of \\(x^\\top \\beta\\). The name of “mean” here is used to indicate a different thing. Its name “Mean Square” reflects that it is also an estimate estimate of noise variance \\(\\sigma^2\\) under \\(H_0\\!:\\,\\beta = 0\\):\n\\[\nE[\\mathrm{MSR} \\mid H_0] = \\sigma^2,\n\\qquad\nE[\\mathrm{MSR} \\mid H_1] &gt; \\sigma^2.\n\\]\nHence the F-statistic\n\\[\nF = \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}\n\\] is approximately equal to 1 subject to the variability as characterized with F-distribution with degree freedoms of \\(k\\) and \\(n-p\\). This test is to test whether any regression coefficients are not equal to 0.\n\n\n\n5.2.3 \\(\\hat \\sigma^2_{x^\\top\\beta}=\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}\\)\n\\(\\hat \\sigma^2_{x^\\top\\beta}\\) is an unbiased estimator of the variance of linear signal when \\(x\\) is a regarded as a random variable. This can be seen from the following equations: \\[\nE[\\mathrm{SSR}] = k\\,\\sigma^2 + \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta,\n\\qquad\nE[\\mathrm{MSE}] = \\sigma^2.\n\\] Hence, \\[\n\\begin{aligned}\nE[\\mathrm{SSR}_{\\mathrm{adj}}]\n&= E[\\mathrm{SSR}] - k\\,E[\\mathrm{MSE}] \\\\\n&= \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta \\\\\n&= \\sum_{i=1}^n (\\mu_i - \\bar\\mu)^2,\n\\end{aligned}\n\\]\nwhere \\[\n\\begin{aligned}\n\\mu_i &= x_i^\\top \\beta \\\\\n\\bar\\mu &= \\tfrac{1}{n}\\sum_{i=1}^n \\mu_i\n\\end{aligned}\n\\] For fixed \\(X\\), \\(\\mathrm{SSR}_{\\text{adj}}/(n-1)\\) equals the sample variance of the true means \\(\\{\\mu_i\\}\\) over the observed design points. If the rows of \\(X\\) are independently sampled with covariance matrix \\(\\Sigma_X\\) (the random-\\(X\\) model), then\n\\[\n\\mathbb{E}_X\\!\\left[\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}\\right]\n= \\beta^\\top \\Sigma_X \\beta\n= \\mathrm{Var}(x^\\top \\beta),\n\\]\n\n\n5.2.4 Connection to Rao-Blackwell Formula\nThe decomposition of \\(\\hat{\\sigma}^2\\) is consistent with the Rao–Blackwell formula for total variance:\n\\[\n\\mathrm{Var}(y) = \\mathrm{Var}\\!\\big(E[y \\mid x]\\big) + E\\!\\big(\\mathrm{Var}[y \\mid x]\\big).\n\\]\nHere,\n\n\\(\\mathrm{Var}\\!\\big(E[y \\mid x]\\big)\\) corresponds to the explained variation due to the regression component \\(x^\\top\\beta\\), and\n\n\\(E\\!\\big(\\mathrm{Var}[y \\mid x]\\big)\\) corresponds to the residual variation due to \\(\\epsilon\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#a-simulation-study-to-understand-the-distributions-of-rss",
    "href": "unit3-mlr/mlr.html#a-simulation-study-to-understand-the-distributions-of-rss",
    "title": "3  Multiple Linear Regression",
    "section": "5.3 A Simulation Study to Understand the Distributions of RSS",
    "text": "5.3 A Simulation Study to Understand the Distributions of RSS\nData Generating Model\nFor \\(n=30\\) and \\(p_{max}=20\\), simulate with either \\(H_0:\\beta=\\mathbf 0\\) or \\(H_1\\) where only \\(\\beta_1\\neq 0\\); \\(\\epsilon_i\\sim N(0,1)\\).\nSequence of Fitted Models\n\n\n\n\n\n\n\n\n\nModel Name\n# of Predictors (k)\n# of Parameters (p)\nR Formula\n\n\n\n\nModel 0\n0\n1\ny ~ 1\n\n\nModel 1\n2\n3\ny ~ x_1 + x_2\n\n\n…\n…\n…\n…\n\n\nFinal Model\n20\n21\ny ~ x_1 + ... + x_20\n\n\n\n\n5.3.1 When \\(H_0\\) is true\n\nCode\ncat(sprintf('&lt;video controls style=\"width:100%%;height:auto;\"&gt;&lt;source src=\"%s\" type=\"video/mp4\"&gt;&lt;/video&gt;',\"rss-h0.mp4\"))\n\n\n\n\n\n\n5.3.2 When \\(H_1\\) is true\n\nCode\ncat(sprintf('&lt;video controls style=\"width:100%%;height:auto;\"&gt;&lt;source src=\"%s\" type=\"video/mp4\"&gt;&lt;/video&gt;',\"rss-h1.mp4\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#example",
    "href": "unit3-mlr/mlr.html#example",
    "title": "3  Multiple Linear Regression",
    "section": "5.4 Example",
    "text": "5.4 Example\n\n\nCode\n# Data: Weight, height and age of children\nwgt &lt;- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt &lt;- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage &lt;- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\nchild.data &lt;- data.frame(wgt, hgt, age)\n\n\n\n5.4.1 Problem 1: Height then Age\n\n\nCode\nfit_age_hgt &lt;- lm(wgt ~ hgt + age, data = child.data)\nsummary(fit_age_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ hgt + age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nhgt           0.7220     0.2608   2.768   0.0218 *\nage           2.0501     0.9372   2.187   0.0565 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78, Adjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n\n\nCode\nfit_hgt &lt;- lm(wgt ~ hgt, data = child.data)\nsummary(fit_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8736 -3.8973 -0.4402  2.2624 11.8375 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   6.1898    12.8487   0.482  0.64035   \nhgt           1.0722     0.2417   4.436  0.00126 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.471 on 10 degrees of freedom\nMultiple R-squared:  0.663, Adjusted R-squared:  0.6293 \nF-statistic: 19.67 on 1 and 10 DF,  p-value: 0.001263\n\n\nCode\nanova(fit_hgt, fit_age_hgt)\n\n\n\n  \n\n\n\n\n\n5.4.2 Problem 2: Age then Height\n\n\nCode\nfit_age &lt;- lm(wgt ~ age, data = child.data)\nsummary(fit_age)\n\n\n\nCall:\nlm(formula = wgt ~ age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.000  -3.911   1.143   4.071  10.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  30.5714     8.6137   3.549  0.00528 **\nage           3.6429     0.9551   3.814  0.00341 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.015 on 10 degrees of freedom\nMultiple R-squared:  0.5926,    Adjusted R-squared:  0.5519 \nF-statistic: 14.55 on 1 and 10 DF,  p-value: 0.003407\n\n\nCode\nfit_age_hgt &lt;- lm(wgt ~ age + hgt, data = child.data)\nsummary(fit_age_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ age + hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nage           2.0501     0.9372   2.187   0.0565 .\nhgt           0.7220     0.2608   2.768   0.0218 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78, Adjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n\n\nCode\nanova(fit_age, fit_age_hgt)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#relationship-between-t-test-and-partial-f-test",
    "href": "unit3-mlr/mlr.html#relationship-between-t-test-and-partial-f-test",
    "title": "3  Multiple Linear Regression",
    "section": "5.5 Relationship between t-test and partial F-test",
    "text": "5.5 Relationship between t-test and partial F-test\n\nA t-test for a single coefficient is a special case of the partial F-test; the relationship is \\(F = t^2\\) for 1 df in the numerator.\nThe p-value from t-test (output of summary(lm())) is the same as anova test for: \\(H_0: \\beta_j = 0\\) vs \\(H_1\\): all covaraites have non-zero effects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#confidence-interval-for-mean-response",
    "href": "unit3-mlr/mlr.html#confidence-interval-for-mean-response",
    "title": "3  Multiple Linear Regression",
    "section": "6.1 Confidence Interval for Mean Response",
    "text": "6.1 Confidence Interval for Mean Response\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"confidence\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 26.66324 28.66296",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#prediction-interval-for-a-new-observation",
    "href": "unit3-mlr/mlr.html#prediction-interval-for-a-new-observation",
    "title": "3  Multiple Linear Regression",
    "section": "6.2 Prediction Interval for a New Observation",
    "text": "6.2 Prediction Interval for a New Observation\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"prediction\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 22.81378 32.51241",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#residual-calculations",
    "href": "unit3-mlr/mlr.html#residual-calculations",
    "title": "3  Multiple Linear Regression",
    "section": "7.1 Residual Calculations",
    "text": "7.1 Residual Calculations\n\n\nCode\nresiduals_df &lt;- data.frame(\n  hat_values = hatvalues(fit),\n  ordinary_resid = resid(fit),\n  standardized_resid = resid(fit) / sigma(fit),\n  studentized_internal = rstandard(fit),\n  studentized_external = rstudent(fit)\n)\nresiduals_df",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#residual-plots",
    "href": "unit3-mlr/mlr.html#residual-plots",
    "title": "3  Multiple Linear Regression",
    "section": "7.2 Residual Plots",
    "text": "7.2 Residual Plots\n\n\nCode\nn &lt;- nrow(bond.data)\nr &lt;- rstudent(fit) \ny.hat &lt;- fitted.values(fit)\n\npar(mfrow = c(2, 3))\nqqnorm(r, main = \"Normal Q-Q Plot\"); qqline(r)\nplot(y.hat, r, xlab = \"Fitted values\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(1:n, r, xlab = \"Observation Number\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$length, r, xlab = \"Wire Length\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$height, r, xlab = \"Die Height\", ylab = \"Studentized Residuals\"); abline(h = 0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#plotting-with-the-olsrr-package",
    "href": "unit3-mlr/mlr.html#plotting-with-the-olsrr-package",
    "title": "3  Multiple Linear Regression",
    "section": "8.1 Plotting with the olsrr Package",
    "text": "8.1 Plotting with the olsrr Package\n\n\nCode\n# install.packages(\"olsrr\") # Run once if needed\nlibrary(olsrr)\n\nols_plot_cooksd_chart(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dffits(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dfbetas(fit)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#four-models-involving-sex",
    "href": "unit3-mlr/mlr.html#four-models-involving-sex",
    "title": "3  Multiple Linear Regression",
    "section": "10.1 Four Models Involving “sex”",
    "text": "10.1 Four Models Involving “sex”\n\n10.1.1 Coincidence Model (Age Only)\n\n\nCode\n# Ensure sex is a factor (labels will appear in the legend)\nsbpdata$sex &lt;- as.factor(sbpdata$sex)\n\n# Fit (you already have this)\nfit.age &lt;- lm(sbp ~ age, data = sbpdata)\n\n# Generate predictions over the observed age range\nnew_age &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n               max(sbpdata$age, na.rm = TRUE),\n               length.out = 200)\npred &lt;- predict(fit.age, newdata = data.frame(age = new_age))\n\n# Simple palette for the sex levels (works for 1–3 levels; expand if needed)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n# Scatter plot with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n# Add predicted line\nlines(new_age, pred, lwd = 2)\n\n# Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.age)) \n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age))\n\n\nAnalysis of Variance Table\n\nResponse: sbp\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1 14951.3 14951.3  121.27 &lt; 2.2e-16 ***\nResiduals 67  8260.5   123.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.1.2 Additive Effect Model (Age + Sex)\n\n\nCode\n# Parallelism: H0: beta3=0 (Sex has additive effect)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n# Ensure sex is a factor for labeling/colors\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n# Fit (additive: parallelism)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n# X-range and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n# Scatter with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n# Parallel fitted lines: one per sex (same slope, different intercepts)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.agePLUSsex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n# Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.agePLUSsex))\n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age, fit.agePLUSsex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     67 8260.5                                  \n2     66 5202.0  1    3058.5 38.805 3.701e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.1.3 Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)\n\n\nCode\n# Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n# Fit (interaction: different slopes by sex)\nfit.age.TIMES.sex &lt;- lm(sbp ~ age + sex + age:sex, data = sbpdata)\n\n# Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n# Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n# Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.age.TIMES.sex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n# Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\nModel Matrix and ANOVA\n\n\nCode\ndata.frame(model.matrix(fit.age.TIMES.sex))\n\n\n\n  \n\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nprint(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\nModel 3: sbp ~ age + sex + age:sex\n  Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)    \n1     67 8260.5                                   \n2     66 5202.0  1   3058.52 38.2210 4.692e-08 ***\n3     65 5201.4  1      0.55  0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.1.4 Varying Slope, Equal Intercept Model (Age + Age:Sex)\n\n\nCode\n# Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n# Fit (interaction: different slopes by sex)\nfit.equal.intercept &lt;- lm(sbp ~ age + age:sex, data = sbpdata)\n\n\n# Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n# Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n# Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.equal.intercept, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n# Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#orders-of-terms-matters-in-anova-and-warnings-in-interpreting-t-test-tables",
    "href": "unit3-mlr/mlr.html#orders-of-terms-matters-in-anova-and-warnings-in-interpreting-t-test-tables",
    "title": "3  Multiple Linear Regression",
    "section": "10.2 Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables",
    "text": "10.2 Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables\n\n\nCode\nfit.int &lt;- lm(sbp ~ 1, data = sbpdata)\nfit.sex &lt;- lm(sbp ~ sex, data = sbpdata)\n\nprint(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5202.0  1    3058.5  38.2210 4.692e-08 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + age:sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5474.9  1    2785.6  34.8107 1.437e-07 ***\n4     65  5201.4  1     273.4   3.4171   0.06907 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ sex\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67 19282.5  1    3929.2  49.1017 1.684e-09 ***\n3     66  5202.0  1   14080.6 175.9583 &lt; 2.2e-16 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(fit.age)\n\n\n\nCall:\nlm(formula = sbp ~ age, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.782  -7.632   1.968   8.201  22.651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.34905    4.33190   23.86   &lt;2e-16 ***\nage           0.98333    0.08929   11.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 67 degrees of freedom\nMultiple R-squared:  0.6441,    Adjusted R-squared:  0.6388 \nF-statistic: 121.3 on 1 and 67 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.equal.intercept)\n\n\n\nCall:\nlm(formula = sbp ~ age + age:sex, data = sbpdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.6338  -4.3067   0.9922   4.9819  20.2753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.12501    3.55578  29.283  &lt; 2e-16 ***\nage           0.80908    0.07918  10.219 3.14e-15 ***\nage:sex1      0.26705    0.04608   5.795 2.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.108 on 66 degrees of freedom\nMultiple R-squared:  0.7641,    Adjusted R-squared:  0.757 \nF-statistic: 106.9 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.agePLUSsex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.705  -3.299   1.248   4.325  21.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 96.77353    3.62085  26.727  &lt; 2e-16 ***\nage          0.95606    0.07153  13.366  &lt; 2e-16 ***\nsex1        13.51345    2.16932   6.229  3.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.878 on 66 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7691 \nF-statistic: 114.2 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#all-possible-regression",
    "href": "unit3-mlr/mlr.html#all-possible-regression",
    "title": "3  Multiple Linear Regression",
    "section": "11.1 All Possible Regression",
    "text": "11.1 All Possible Regression\n\n\nCode\nols_step_best_subset(model.wine)\n\n\n             Best Subsets Regression             \n-------------------------------------------------\nModel Index    Predictors\n-------------------------------------------------\n     1         flavor                             \n     2         flavor oakiness                    \n     3         aroma flavor oakiness              \n     4         clarity aroma flavor oakiness      \n     5         clarity aroma body flavor oakiness \n-------------------------------------------------\n\n                                                  Subsets Regression Summary                                                   \n-------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                         \nModel    R-Square    R-Square    R-Square     C(p)       AIC        SBIC        SBC        MSEP       FPE       HSP       APC  \n-------------------------------------------------------------------------------------------------------------------------------\n  1        0.6242      0.6137      0.5868    9.0436    130.0214    21.6859    134.9341    61.4102    1.7010    0.0462    0.4176 \n  2        0.6611      0.6417      0.6058    6.8132    128.0901    20.1242    134.6404    57.0033    1.6171    0.0441    0.3970 \n  3        0.7038      0.6776      0.6379    3.9278    124.9781    18.0702    133.1661    51.3383    1.4906    0.0409    0.3659 \n  4        0.7147      0.6801      0.6102    4.6747    125.5480    19.2854    135.3736    50.9872    1.5143    0.0418    0.3717 \n  5        0.7206      0.6769       0.587    6.0000    126.7552    21.0956    138.2183    51.5452    1.5649    0.0436    0.3842 \n-------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#automated-stepwise-procedures",
    "href": "unit3-mlr/mlr.html#automated-stepwise-procedures",
    "title": "3  Multiple Linear Regression",
    "section": "11.2 Automated Stepwise Procedures",
    "text": "11.2 Automated Stepwise Procedures\n\n\nCode\n# Backward Elimination (alpha_out = 0.1)\nols_step_backward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Full Model    126.755    138.218    21.096    0.72060    0.67694 \n 1      body          125.548    135.374    19.285    0.71471    0.68013 \n 2      clarity       124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n----------------------------------------------------------------------------------------\n\n\nCode\n# Forward Selection (alpha_in = 0.1)\nols_step_forward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor        130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness      128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma         124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n\n\nCode\n# Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)\nols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)\n\n\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable          AIC        SBC       SBIC       R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model      165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor (+)      130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness (+)    128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma (+)       124.978    133.166    18.070    0.70377    0.67763 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#a-simple-example",
    "href": "unit3-mlr/mlr.html#a-simple-example",
    "title": "3  Multiple Linear Regression",
    "section": "12.1 A Simple Example",
    "text": "12.1 A Simple Example\n\n\nCode\ny &lt;- c(19, 20, 37, 39, 36, 38)\nx1 &lt;- c(4, 4, 7, 7, 7.1, 7.1)\nx2 &lt;- c(16, 16, 49, 49, 50.4, 50.4)\ncor(data.frame(x1, x2))\n\n\n          x1        x2\nx1 1.0000000 0.9999713\nx2 0.9999713 1.0000000\n\n\nCode\nfit_multi &lt;- lm(y ~ x1 + x2)\nsummary(fit_multi)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n   1    2    3    4    5    6 \n-0.5  0.5 -1.0  1.0 -1.0  1.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -156.056    117.158  -1.332    0.275\nx1            65.444     45.890   1.426    0.249\nx2            -5.389      4.152  -1.298    0.285\n\nResidual standard error: 1.225 on 3 degrees of freedom\nMultiple R-squared:  0.9897,    Adjusted R-squared:  0.9829 \nF-statistic: 144.3 on 2 and 3 DF,  p-value: 0.001043\n\n\nCode\nfit1_multi &lt;- lm(y ~ x1)\nsummary(fit1_multi)\n\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.5260  0.4740 -0.1925  1.8075 -1.7814  0.2186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -4.0293     2.3332  -1.727    0.159    \nx1            5.8888     0.3762  15.654 9.73e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.325 on 4 degrees of freedom\nMultiple R-squared:  0.9839,    Adjusted R-squared:  0.9799 \nF-statistic: 245.1 on 1 and 4 DF,  p-value: 9.725e-05\n\n\nCode\nols_vif_tol(fit_multi)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#vifs-in-the-wine-quality-data",
    "href": "unit3-mlr/mlr.html#vifs-in-the-wine-quality-data",
    "title": "3  Multiple Linear Regression",
    "section": "12.2 VIFs in the Wine Quality Data",
    "text": "12.2 VIFs in the Wine Quality Data\n\n\nCode\nwine.x &lt;- wine[, -ncol(wine)] # Assuming quality is the last column\ncor(wine.x)\n\n\n             clarity     aroma       body      flavor  oakiness\nclarity   1.00000000 0.0619021 -0.3083783 -0.08515993 0.1832147\naroma     0.06190210 1.0000000  0.5489102  0.73656121 0.2016444\nbody     -0.30837826 0.5489102  1.0000000  0.64665917 0.1521059\nflavor   -0.08515993 0.7365612  0.6466592  1.00000000 0.1797605\noakiness  0.18321471 0.2016444  0.1521059  0.17976051 1.0000000\n\n\nCode\n# VIF using olsrr (data frame output)\nols_vif_tol(model.wine)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#vifs-in-the-children-height-data",
    "href": "unit3-mlr/mlr.html#vifs-in-the-children-height-data",
    "title": "3  Multiple Linear Regression",
    "section": "12.3 VIFs in the Children Height Data",
    "text": "12.3 VIFs in the Children Height Data\n\n\nCode\n# Data: Weight, height and age of children\nwgt &lt;- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt &lt;- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage &lt;- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\n\nfit_age_hgt &lt;- lm(wgt ~ hgt + age, data = child.data)\nols_vif_tol(fit_age_hgt)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html",
    "href": "unit3-mlr/residuals.html",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "",
    "text": "5 Introduction\nIn statistical modeling, identifying data points that don’t fit—the outliers—is a critical step. The most reliable tool for this job is the externally studentized residual. Its power comes from a simple, intuitive idea: to judge a point fairly, you shouldn’t use that point when building your model. This is the core principle of Leave-One-Out Cross-Validation (LOOCV).\nThis article provides a complete walkthrough of this essential concept. We’ll start with the basic linear model, introduce the necessary notation, explore the flaws of simpler residuals, and then formally define and prove the equivalence of the conceptual and computational formulas for studentized residuals. Finally, we’ll make it all concrete with a simple example.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#our-notations",
    "href": "unit3-mlr/residuals.html#our-notations",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "7.1 Our Notations",
    "text": "7.1 Our Notations\nTo discuss models fit with all data versus those with one point removed, we need clear notation.\nFull Data Model (Using all n observations)\n\n\\(\\hat{\\beta}\\): The estimated coefficient vector.\n\\(\\hat{y}_i\\): The predicted value for observation i from this model.\n\\(e_i\\): The ordinary residual (\\(e_i = y_i - \\hat{y}_i\\)).\n\\(\\hat{\\sigma}\\): The estimated standard deviation of the errors (Residual Standard Error).\n\\(h_{ii}\\): The leverage of observation i, a measure of how much its x-values influence the model.\n\nLeave-One-Out (LOOCV) Model\n\n\\(\\hat{\\beta}_{-i}\\): The coefficient vector estimated after removing observation i.\n\\(\\hat{y}_{i,-i}\\): The predicted value for observation i, from the model fit without observation i.\n\\(e_{i,-i}\\): The deleted residual (\\(e_{i,-i} = y_i - \\hat{y}_{i,-i}\\)).\n\\(\\hat{\\sigma}_{-i}\\): The standard deviation of the errors estimated from the model fit without observation i.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#non-studentized-residuals",
    "href": "unit3-mlr/residuals.html#non-studentized-residuals",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "7.2 Non-studentized Residuals",
    "text": "7.2 Non-studentized Residuals\nBefore getting to the correct solution, it’s crucial to understand why simpler methods of standardizing residuals are flawed.\n\n7.2.1 The Ordinary Residual (\\(e_i\\)): Too Small and \\(x_i\\) Dependent\nThe most basic residual, \\(e_i\\), is problematic for two key reasons.\nAn outlier has an undue influence on the model, pulling the regression line towards itself. This makes its own predicted value, \\(\\hat{y}_i\\), artificially close to its actual value, \\(y_i\\). As a result, its residual, \\(e_i\\), is deceptively small and doesn’t reflect the true magnitude of the error.\nThe variance of an ordinary residual is not constant; it depends on the point’s leverage. The variance can be derived from the hat matrix \\(H = X(X^\\top X)^{-1}X^\\top\\). Since \\[\\hat{y} = HY,\\] we have \\[\\begin{equation}\ne = (I - H)\\epsilon.\n\\end{equation}\\] Thus, \\[\\begin{equation}\n\\mathrm{Var}(e) = (I-H)\\,\\sigma^2\\,(I-H)^\\top = (I-H)\\sigma^2.\n\\end{equation}\\] Therefore, for the \\(i\\)th residual, \\[\\begin{equation}\n\\mathrm{Var}(e_i) = \\sigma^2(1 - h_{ii}).\n\\end{equation}\\]\nSince leverage (\\(h_{ii}\\)) is always greater than 0, the variance of an ordinary residual is always less than the true error variance, \\(\\sigma^2\\). High-leverage points act as “anchors” for the line and have even smaller variance.\n\n\n7.2.2 The LOOCV Residual (\\(e_{i,-i}\\)): Too large and \\(x_i\\)-Dependent Variance\nThe deleted residual, \\(e_{i,-i}\\), solves the “too small” problem. Because the model isn’t influenced by the point it’s predicting, the residual is an honest measure of prediction error. However, its variance is still not constant. The variance of a deleted residual also depends on leverage, but in the opposite way. \\[\\begin{equation}\n    \\text{Var}(e_{i,-i}) = \\frac{\\sigma^2}{1 - h_{ii}}\n    \\end{equation}\\]\nFrom the key identity Equation 8.1, \\[\\begin{equation}\ne_{i,-i} = \\frac{e_i}{1 - h_{ii}}.\n\\end{equation}\\] Therefore, \\[\\begin{equation}\n\\mathrm{Var}(e_{i,-i}) = \\frac{\\mathrm{Var}(e_i)}{(1-h_{ii})^2}\n= \\frac{\\sigma^2(1-h_{ii})}{(1-h_{ii})^2}\n= \\frac{\\sigma^2}{1-h_{ii}}.\n\\end{equation}\\]\nSince \\(1-h_{ii}\\) is less than 1, the variance of a deleted residual is always greater than the true error variance, \\(\\sigma^2\\). This is because it has two sources of randomness: the error in the point itself (\\(y_i\\)) and the error in the prediction (\\(\\hat{y}_{i,-i}\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#studentized-residuals",
    "href": "unit3-mlr/residuals.html#studentized-residuals",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "7.3 Studentized Residuals",
    "text": "7.3 Studentized Residuals\n\n7.3.1 Studentized LOOCV (Deleted) Residual\nThe correct solution is to take the LOOCV residual and divide it by its true standard error, which properly accounts for its larger, x-dependent variance. This is the externally studentized residual, \\(t_i\\), defined as follows:\n\\[\nt_i = \\frac{e_{i,-i}}{\\text{SE}(e_{i,-i})} = \\frac{e_{i,-i}}{\\frac{\\hat{\\sigma}_{-i}}{\\sqrt{1-h_{ii}}}}\n\\tag{7.1}\\] This final value is a reliable diagnostic. Under the null hypothesis that the observation is not an outlier, it follows a Student’s t-distribution with \\(n - p - 1\\) degrees of freedom.\n\n\n7.3.2 Studentized Full Data Residuals\nCalculating the conceptual formula appears to require fitting n different regression models—a computationally expensive task. Fortunately, a mathematical identity allows us to calculate the exact same value using only the results from the single, full data model.\n\\[\nt_i = \\frac{e_i}{\\hat{\\sigma}_{-i}\\sqrt{1 - h_{ii}}}\n\\tag{7.2}\\] This is not an approximation; it is an exact algebraic rearrangement of the conceptual definition.\n\n\n7.3.3 Equivalence of Equation 7.2 and Equation 7.1\n\n7.3.3.1 Proof of Equivalence ⚙️\nLet’s start with the conceptual definition of the studentized LOOCV residuals Equation 7.1 and show how it transforms into Equation 7.2.\n\nStart with the conceptual LOOCV definition: \\[\\begin{equation}\n  t_i = \\frac{e_{i,-i}}{\\text{SE}(e_{i,-i})} = \\frac{e_{i,-i}}{\\frac{\\hat{\\sigma}_{-i}}{\\sqrt{1-h_{ii}}}}\n  \\end{equation}\\]\nSubstitute the key identity into the numerator: \\[\\begin{equation}\n  t_i = \\frac{\\frac{e_i}{1 - h_{ii}}}{\\frac{\\hat{\\sigma}_{-i}}{\\sqrt{1 - h_{ii}}}}\n  \\end{equation}\\]\nSimplify the complex fraction. We can do this by multiplying the numerator by the reciprocal of the denominator: \\[\\begin{equation}\n  t_i = \\frac{e_i}{1-h_{ii}} \\cdot \\frac{\\sqrt{1-h_{ii}}}{\\hat{\\sigma}_{-i}}\n  \\end{equation}\\]\nCancel the terms. Since \\(1 - h_{ii} = (\\sqrt{1 - h_{ii}})^2\\), one of the \\(\\sqrt{1 - h_{ii}}\\) terms in the denominator cancels with the term in the numerator. This leaves us with the computational shortcut formula: \\[\\begin{equation}\n  t_i = \\frac{e_i}{\\hat{\\sigma}_{-i}\\sqrt{1-h_{ii}}}\n  \\end{equation}\\]\n\nThis proves that the two formulas are mathematically identical. The computational shortcut is simply a clever algebraic rearrangement of the more intuitive LOOCV definition, allowing for efficient and accurate calculation. ✅\nOf course. Here is the modified .qmd file with the standardized residual (which you’ve labeled STD-Full) added to the table, the descriptions, and the plot.\nThe main changes include:\n\nAdding the STD-Full column to the residuals_df data frame using R’s rstandard() function.\nUpdating the list of calculated columns to include a description of STD-Full.\nModifying the plotting code to include STD-Full with its own distinct color and shape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#list-of-residuals",
    "href": "unit3-mlr/residuals.html#list-of-residuals",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "7.4 List of Residuals",
    "text": "7.4 List of Residuals\nIn this article, we will compare the four residuals given as:\n\n\n\nTable 7.1\n\n\n\n\n\n\n\n\n\n\nShort Name\nFull Name\nFormula\n\n\n\n\nNS-Full\nNon-studentized Full-Data Residual\n\\[\\frac{e_i}{\\hat{\\sigma}}\\]\n\n\nNS-LOO\nNon-studentized LOOCV Residual\n\\[\\frac{e_{i,-i}}{\\hat{\\sigma}_{-i}}\\]\n\n\nST-LOO\nStudentized LOOCV Residual\n\\[\\frac{e_{i,-i}}{\\hat{\\sigma}_{-i}/\\sqrt{1-h_{ii}}}\\]\n\n\nST-Full\nStudentized Full-Data Residual\n\\[\\frac{e_i}{\\hat{\\sigma}_{-i}\\sqrt{1-h_{ii}}}\\]\n\n\nSTD-Full\nStandardized (Internal Studentized) Residual\n\\[\\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#example-of-various-residuals",
    "href": "unit3-mlr/residuals.html#example-of-various-residuals",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "7.5 Example of Various Residuals",
    "text": "7.5 Example of Various Residuals\n\n7.5.1 The Linear Model\nThe simulation uses a simple linear regression model to describe the relationship between a single predictor variable, \\(x_i\\), and a response variable, \\(y_i\\). The underlying “true” model from which the data is generated is: \\[y_i = 2 + 3x_i + \\epsilon_i\\] This means we have a true intercept of 2, a true slope of 3, and a random error term, \\(\\epsilon_i\\), drawn from a normal distribution with a mean of 0 and a standard deviation of 5. One artificial outlier is added to this data to test the behavior of the different residual types. 5 unrelated predictors are added to the dataset.\n\n\nCode\n# Load libraries\nlibrary(dplyr)\nlibrary(knitr)\n\n# -------------------------------\n# 1) Data and full-model fit\n# -------------------------------\nset.seed(123)\nn &lt;- 20\nx &lt;- 1:n\ny &lt;- 2 + 3 * x + rnorm(n, mean = 0, sd = 5)\ny[1] &lt;- y[1] + 30  \n#y[11] &lt;- y[11] - 30 \no.index &lt;- c(1)\nflag.outlier &lt;- rep(20, n)\nflag.outlier[o.index] &lt;- 2\nfull_data  &lt;- data.frame(x = x, replicate(5, rnorm (n)), y = y)\n\nplot(y~x, data=full_data, pch= flag.outlier, col=flag.outlier)\nfit &lt;- lm(y~., data=full_data)\nabline (fit)\nabline (a=2, b=3, col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Description of Calculated Columns\nThe final table compiles several important quantities calculated during the simulation. Here’s what each column represents:\n\n\\(x_i\\): The predictor variable, which is simply the index of the observation from 1 to 20.\n\\(h_i\\): The leverage of the i-th observation. It measures how influential a point’s x-value is in determining the model’s fit. A higher value indicates a more influential point.\n\\(e_i\\): The ordinary residual, calculated as the difference between the actual value (\\(y_i\\)) and the predicted value (\\(\\hat{y}_i\\)) from the model fit on all data.\n\\(\\hat{\\sigma}\\): The residual standard error (or Root Mean Square Error) of the full model, representing the typical size of an ordinary residual.\n\\(e_{i,-i}\\): The deleted (or LOOCV) residual. This is the difference between the actual value (\\(y_i\\)) and the value predicted for it by a model that was fit on all other data except point i.\n\\(\\hat{e}_{i,-i}\\): This column shows the deleted residual calculated using the efficient algebraic shortcut (\\(e_i / (1-h_{ii})\\)), verifying it’s identical to the brute-force \\(e_{i,-i}\\).\n\\(\\hat{\\sigma}_{-i}\\): The LOOCV residual standard error, calculated from a model that was fit after removing observation i.\n\\(\\tilde{\\sigma}_{-i}\\): The LOOCV residual standard error, calculated from the shortcut formula ?eq-sigma_-i.\nNS-Full: The Non-studentized Full-Data Residual, calculated as the ordinary residual divided by the full model’s standard error (\\(e_i / \\hat{\\sigma}\\)).\nNS-LOO: The Non-studentized LOOCV Residual, calculated as the deleted residual divided by the corresponding LOOCV standard error (\\(e_{i,-i} / \\hat{\\sigma}_{-i}\\)).\nSTD-Full: The Standardized (or Internally Studentized) Residual, calculated as the ordinary residual divided by its estimated standard error (\\(e_i / (\\hat{\\sigma}\\sqrt{1-h_{ii}})\\)). This is provided by R’s rstandard() function.\nST-LOO: The Studentized LOOCV Residual, calculated using the conceptual formula by dividing the deleted residual by its true standard error.\nST-Full: The Studentized Full-Data Residual, calculated using the efficient shortcut formula, which is provided by R’s rstudent() function.\n\n\n\n\nCode\nlibrary(kableExtra)\n\nfull_model &lt;- lm(y ~ ., data = full_data)\np &lt;- length(coef(full_model))\nleverage    &lt;- hatvalues(full_model)\ne_full      &lt;- resid(full_model)\nsigma_hat_val  &lt;- summary(full_model)$sigma\n\nrss_full &lt;- sum(e_full^2)\ndf_loo &lt;- n - p - 1\nsigma_minus_i_shortcut &lt;- sqrt((rss_full - (e_full^2 / (1 - leverage))) / df_loo)\n\n# -------------------------------\n# 2) LOOCV quantities (refit n times)\n# -------------------------------\ne_del_val &lt;- numeric(n)\nsigma_minus_i_val &lt;- numeric(n)\n\nfor (i in 1:n) {\n  loocv_model &lt;- lm(y ~ ., data = full_data[-i, ])\n  yhat_minus  &lt;- predict(loocv_model, newdata = full_data[i, , drop = FALSE])\n  e_del_val[i]    &lt;- full_data$y[i] - yhat_minus\n  sigma_minus_i_val[i] &lt;- summary(loocv_model)$sigma\n}\n\n# -------------------------------\n# 3) Assemble and round results\n# -------------------------------\nresiduals_df &lt;- data.frame(\n  x = full_data$x,\n  h = as.numeric(leverage),\n  e_i = as.numeric(e_full),\n  sigma_hat = as.numeric(sigma_hat_val),\n  e_i_minus_i = as.numeric(e_del_val),\n  e_i_minus_i_2 = e_full/(1-leverage),\n  sigma_minus_i = as.numeric(sigma_minus_i_val),\n  sigma_minus_i_shortcut = as.numeric(sigma_minus_i_shortcut),\n  `NS-Full` = e_full / sigma_hat_val,\n  `NS-LOO` = e_del_val / sigma_minus_i_val,\n  `STD-Full` = rstandard(full_model), # &lt;-- ADDED STANDARDIZED RESIDUAL\n  `ST-LOO` = e_del_val / (sigma_minus_i_val / sqrt(1 - leverage)),\n  `ST-Full` = rstudent(full_model)\n) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n\n# 4) Create simple display names\ndisplay_names &lt;- c(\"$x_i$\", \"$h_i$\", \"$e_i$\", \"$\\\\hat{\\\\sigma}$\", \n                   \"$e_{i,-i}$\", \"$\\\\hat{e}_{i,-i}$\",\n                   \"$\\\\hat{\\\\sigma}_{-i}$\", \"$\\\\tilde{\\\\sigma}_{-i}$\", \n                   \"NS-Full\", \"NS-LOO\", \"STD-Full\", \"ST-LOO\", \"ST-Full\") # &lt;-- ADDED LABEL\n\n# 5) Display the table\n# Conditional check for output format\nif (knitr::is_html_output()) {\n  # --- Code for HTML Output (using kableExtra) ---\n  knitr::kable(\n    residuals_df,\n    caption = \"Residual variants\",\n    col.names = display_names,\n    align = \"r\",\n    #format=\"html\",\n    escape = FALSE # Allows LaTeX and &lt;br/&gt; to render\n  ) \n} else {\n  # --- Code for PDF/Other Output (using kableExtra) ---\n  knitr::kable(\n    residuals_df,\n    caption = \"Residual variants.\",\n    col.names = display_names,\n    align = \"r\",\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE # Allows LaTeX and \\\\ to render\n  ) %&gt;%\n    kable_styling(\n      latex_options = \"scale_down\"\n    )\n}\n\n\n\nResidual variants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(h_i\\)\n\\(e_i\\)\n\\(\\hat{\\sigma}\\)\n\\(e_{i,-i}\\)\n\\(\\hat{e}_{i,-i}\\)\n\\(\\hat{\\sigma}_{-i}\\)\n\\(\\tilde{\\sigma}_{-i}\\)\nNS-Full\nNS-LOO\nSTD-Full\nST-LOO\nST-Full\n\n\n\n\n1\n0.247\n17.638\n7.57\n23.434\n23.434\n5.257\n5.257\n2.330\n4.457\n2.686\n3.867\n3.867\n\n\n2\n0.241\n-7.909\n7.57\n-10.418\n-10.418\n7.431\n7.431\n-1.045\n-1.402\n-1.199\n-1.222\n-1.222\n\n\n3\n0.364\n-3.271\n7.57\n-5.147\n-5.147\n7.790\n7.790\n-0.432\n-0.661\n-0.542\n-0.527\n-0.527\n\n\n4\n0.555\n1.553\n7.57\n3.490\n3.490\n7.851\n7.851\n0.205\n0.445\n0.308\n0.297\n0.297\n\n\n5\n0.257\n-1.515\n7.57\n-2.038\n-2.038\n7.863\n7.863\n-0.200\n-0.259\n-0.232\n-0.223\n-0.223\n\n\n6\n0.400\n-0.400\n7.57\n-0.666\n-0.666\n7.878\n7.878\n-0.053\n-0.085\n-0.068\n-0.066\n-0.066\n\n\n7\n0.253\n0.247\n7.57\n0.331\n0.331\n7.879\n7.879\n0.033\n0.042\n0.038\n0.036\n0.036\n\n\n8\n0.303\n-8.972\n7.57\n-12.871\n-12.871\n7.243\n7.243\n-1.185\n-1.777\n-1.419\n-1.484\n-1.484\n\n\n9\n0.347\n-8.394\n7.57\n-12.852\n-12.852\n7.287\n7.287\n-1.109\n-1.764\n-1.372\n-1.425\n-1.425\n\n\n10\n0.573\n-5.696\n7.57\n-13.342\n-13.342\n7.467\n7.467\n-0.752\n-1.787\n-1.152\n-1.168\n-1.168\n\n\n11\n0.117\n7.175\n7.57\n8.127\n8.127\n7.565\n7.565\n0.948\n1.074\n1.009\n1.009\n1.009\n\n\n12\n0.441\n0.998\n7.57\n1.786\n1.786\n7.870\n7.870\n0.132\n0.227\n0.176\n0.170\n0.170\n\n\n13\n0.359\n1.298\n7.57\n2.026\n2.026\n7.865\n7.865\n0.171\n0.258\n0.214\n0.206\n0.206\n\n\n14\n0.395\n0.698\n7.57\n1.153\n1.153\n7.875\n7.875\n0.092\n0.146\n0.119\n0.114\n0.114\n\n\n15\n0.228\n-1.221\n7.57\n-1.583\n-1.583\n7.869\n7.869\n-0.161\n-0.201\n-0.184\n-0.177\n-0.177\n\n\n16\n0.402\n7.998\n7.57\n13.365\n13.365\n7.292\n7.292\n1.057\n1.833\n1.366\n1.418\n1.418\n\n\n17\n0.433\n3.996\n7.57\n7.048\n7.048\n7.729\n7.729\n0.528\n0.912\n0.701\n0.687\n0.687\n\n\n18\n0.414\n-3.369\n7.57\n-5.746\n-5.746\n7.776\n7.776\n-0.445\n-0.739\n-0.581\n-0.566\n-0.566\n\n\n19\n0.258\n3.073\n7.57\n4.141\n4.141\n7.812\n7.812\n0.406\n0.530\n0.471\n0.457\n0.457\n\n\n20\n0.414\n-3.930\n7.57\n-6.707\n-6.707\n7.739\n7.739\n-0.519\n-0.867\n-0.678\n-0.663\n-0.663\n\n\n\n\n\n\n\nCode\n# Load libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n\n# -------------------------------\n# 3) Plotting Code with Updated Names\n# -------------------------------\n\n# Prepare data for plotting\nplot_df &lt;- residuals_df %&gt;%\n  # Use the new, simple column names (R converts '-' to '.')\n  select(\n    x,\n    NS.Full,\n    NS.LOO,\n    STD.Full, # &lt;-- ADDED FOR PLOTTING\n    ST.LOO,\n    ST.Full\n  ) %&gt;%\n  pivot_longer(\n    cols = -x,\n    names_to = \"residual_type\",\n    values_to = \"residual_value\"\n  )\n\n# Update the names in the mapping vectors\nshape_map &lt;- c(\n  NS.Full  = 16,  # solid circle\n  NS.LOO   = 1,   # hollow circle\n  STD.Full = 2,   # hollow triangle &lt;-- ADDED\n  ST.LOO   = 6,   # asterisk\n  ST.Full  = 10   # asterisk\n)\n\nlabels_map &lt;- c(\n  NS.Full  = \"NS-Full\",\n  NS.LOO   = \"NS-LOO\",\n  STD.Full = \"STD-Full\", # &lt;-- ADDED\n  ST.LOO   = \"ST-LOO\",\n  ST.Full  = \"ST-Full\"\n)\n\ncolor_map &lt;- c(\n  NS.Full  = \"#1f77b4\",  # blue\n  NS.LOO   = \"#ff7f0e\", # orange\n  STD.Full = \"#9467bd\",  # purple &lt;-- ADDED\n  ST.LOO   = \"#2ca02c\",  # green\n  ST.Full  = \"#d62728\"   # red\n)\n\n# Generate the plot\nggplot(\n  plot_df,\n  aes(x = x, y = residual_value,\n      shape = residual_type, color = residual_type, group = residual_type)\n) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(size = 3, stroke = 1.2) + # Increased stroke for visibility\n  scale_shape_manual(\n    values = shape_map,\n    breaks = names(labels_map),\n    labels = unname(labels_map),\n    name = \"Residual Type\"\n  ) +\n  scale_color_manual(\n    values = color_map,\n    breaks = names(labels_map),\n    labels = unname(labels_map),\n    name = \"Residual Type\"\n  ) +\n  labs(\n    title = \"Five Residual Variants vs x\",\n    x = \"x_i\",\n    y = \"Residual value\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\nFive residual variants plotted against the predictor variable x.\n\n\n\n\nFrom the above simulation results, we observe the following important facts:\n\nLeverage and Influence: The simulation confirms that leverage (\\(h_{ii}\\)) measures an observation’s influence on the model’s coefficients. It shows that points with higher leverage pull the regression line toward them, resulting in smaller, deceptively conservative full-data residuals (\\(e_i\\)).\nConservative Residuals: The study highlights that the ordinary residual (\\(e_i\\)) is a “conservative” measure of error because its value for an outlier is systematically reduced by that same outlier’s influence on the model.\nIdentity Verification: The numerical results validated the key algebraic identity that connects the full-data residual (\\(e_i\\)) to the leave-one-out (deleted) residual (\\(e_{i,-i}\\)), as well as the identity for calculating the LOOCV standard error (\\(\\hat{\\sigma}_{-i}\\)) from the full model’s statistics. This demonstrates that all key LOOCV errors can be calculated efficiently from a single model fit.\nEffective Studentization: The final step of studentization, which uses leverage to properly scale the residuals, is shown to be crucial. It successfully transforms the residuals into a reliable diagnostic tool with a constant variance across all predictor values (\\(x_i\\)), causing them to behave much more like a standard normal or t-distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#definition-from-the-change-in-coefficients",
    "href": "unit3-mlr/residuals.html#definition-from-the-change-in-coefficients",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "8.1 Definition from the change in coefficients",
    "text": "8.1 Definition from the change in coefficients\nLet \\(\\hat{\\boldsymbol\\beta}\\) be the OLS estimate on all \\(n\\) cases and \\(\\hat{\\boldsymbol\\beta}_{(-i)}\\) the estimate after deleting case \\(i\\).\nWith \\(p\\) parameters (including intercept) and \\(\\hat\\sigma^2=\\mathrm{MSE}\\),\n\\[\nD_i\n= \\frac{\\big(\\hat{\\boldsymbol\\beta}-\\hat{\\boldsymbol\\beta}_{(-i)}\\big)^{\\!\\top}\n(X^{\\!\\top}X)\\,\\big(\\hat{\\boldsymbol\\beta}-\\hat{\\boldsymbol\\beta}_{(-i)}\\big)}{p\\,\\hat\\sigma^{2}} \\, .\n\\]\nThis measures how far the whole coefficient vector moves (in the \\(X^{\\!\\top}X\\) metric) when case \\(i\\) is removed, scaled per parameter.\n\n8.1.1 Express \\(D_i\\) via the studentized LOOCV residual: \\(t_i\\)\nLet \\(h_{ii}\\) be the leverage and define the LOOCV quantities \\(e_{i,-i}\\) and \\(\\hat\\sigma_{-i}\\) from the model refit without case \\(i\\).\nThe externally studentized residual is\n\\[\nt_i\n=\\frac{e_i}{\\hat{\\sigma}_{-i}\\sqrt{\\,1-h_{ii}\\,}}\n=\\frac{e_{i,-i}\\,\\sqrt{\\,1-h_{ii}\\,}}{\\hat{\\sigma}_{-i}},\n\\qquad\\text{since}\\quad e_{i,-i}=\\frac{e_i}{1-h_{ii}}.\n\\]\nThen Cook’s distance can be written as\n\\[\nD_i\n= \\frac{n-p}{p}\\;\\frac{h_{ii}}{1-h_{ii}}\\;\n\\frac{t_i^{\\,2}}{(n-p-1)+t_i^{\\,2}} \\, .\n\\]\n\n\n8.1.2 Exact null distribution\nUnder the classical linear model,\n\\[\nt_i^{\\,2}\\ \\sim\\ F_{1,\\ \\nu},\\qquad \\nu=n-p-1 .\n\\]\nLet\n\\[\nc_i=\\frac{n-p}{p}\\cdot\\frac{h_{ii}}{1-h_{ii}},\\qquad\nW_i=\\frac{t_i^{\\,2}}{\\nu+t_i^{\\,2}} .\n\\]\nBecause \\(t_i^{\\,2}\\sim F_{1,\\nu}\\),\n\\[\nW_i\\sim \\mathrm{Beta}\\!\\Big(\\tfrac12,\\tfrac{\\nu}{2}\\Big),\n\\qquad\n\\frac{D_i}{c_i}=W_i\\in[0,1] \\, .\n\\]\nThis yields exact, per–case \\(p\\)-values and critical values:\n\\[\np_i=\\Pr\\!\\left(W_i\\ge \\frac{D_i}{c_i}\\right)\n= S_\\mathrm{Beta}\\!\\left(\\frac{D_i}{c_i};\\,\\tfrac12,\\,\\tfrac{n-p-1}{2}\\right),\n\\]\n\\[\nd_{i,\\alpha}=c_i\\ q_\\mathrm{Beta}\\!\\Big(1-\\alpha;\\tfrac12,\\tfrac{n-p-1}{2}\\Big).\n\\]\nwhere \\(S_\\mathrm{Beta}\\) and \\(q_\\mathrm{Beta}\\) stand for the survival and quantile functions of Beta distribution.\n\n\n8.1.3 The rough \\(4/n\\) rule (average-leverage simplification)\nApproximating a “typical” case by average leverage \\(h_{ii}\\approx p/n\\) gives\n\\[\nc_i=\\frac{n-p}{p}\\cdot\\frac{p/n}{1-p/n}\\approx 1 .\n\\]\nThe 95th percentile of \\(W_i\\) is\n\\[\n\\mathrm{qbeta}\\!\\Big(0.95;\\tfrac12,\\tfrac{n-p-1}{2}\\Big)\n\\ \\approx\\ \\frac{F_{1,\\nu,\\,0.95}}{\\nu+F_{1,\\nu,\\,0.95}}\n\\ \\approx\\ \\frac{3.84}{\\,n-p-1\\,}\n\\ \\approx\\ \\frac{4}{n}\\quad (\\text{when }p\\ll n).\n\\]\nSo \\(4/n\\) is a rule-of-thumb 95% cutoff for an average-leverage point; the exact leverage–aware cutoff is \\(d_{i,\\alpha}\\) above (larger for high \\(h_{ii}\\), smaller for low \\(h_{ii}\\)).\n\n\n8.1.4 Comparing \\(4/n\\) rules with the actual critical values\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Exact 95% Cook's D cutoff under average leverage h_ii = p/n (so c_i = 1):\n# d_{i,0.95} = qbeta(0.95; 1/2, (n - p - 1)/2), valid when nu = n - p - 1 &gt; 0\ncook_crit_avg &lt;- function(n, p, alpha = 0.05) {\n  nu &lt;- n - p - 1\n  if (nu &lt;= 0) return(NA_real_)\n  stats::qbeta(1 - alpha, shape1 = 0.5, shape2 = nu / 2)\n}\n\n# Grids (edit as needed)\nn_vals &lt;- c(20, 30, 50, 80, 100, 150, 200, 500)\np_vals &lt;- c(2, 3, 5, 10, 15, 20, 30, 50)\n\ndf &lt;- tidyr::crossing(n = n_vals, p = p_vals) %&gt;%\n  mutate(valid = p &lt;= n - 2,\n         nu    = n - p - 1L) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    cook_crit_95 = if (valid) cook_crit_avg(n, p, 0.05) else NA_real_,\n    `4/n`        = 4 / n,\n    ratio        = cook_crit_95 / `4/n`,\n    `p/n`        = p / n\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(valid) %&gt;%\n  select(n, p, `p/n`, nu, cook_crit_95, `4/n`, ratio) %&gt;%\n  mutate(\n    `p/n`         = round(`p/n`, 3),\n    cook_crit_95  = round(cook_crit_95, 6),\n    `4/n`         = round(`4/n`, 6),\n    ratio         = round(ratio, 4)\n  )\n\n\nif (knitr::is_html_output()) {\n  # HTML → Quarto prints `df` as a paged table and uses tbl-cap\n  library(DT)\n  DT::datatable(\n    df,\n    rownames = FALSE,\n    options = list(pageLength = 10, scrollX = TRUE),\n    caption = htmltools::tags$caption(\n      style = 'caption-side: top; text-align: left;',\n      htmltools::HTML(\"Exact 95% Cook&#39;s D critical value (average leverage $h_{ii}=p/n \\\\Rightarrow c_i=1$) vs heuristic $4/n$.\")\n    )\n  )\n} else {\n  # Non-HTML (PDF, DOCX) → fall back to kable\n\n  knitr::kable(\n    df,\n    align = \"r\",\n    booktabs = TRUE,\n    caption = \"Exact 95% Cook's D critical value (average leverage $h_{ii}=p/n \\\\Rightarrow c_i=1$) vs heuristic $4/n$.\"\n  ) \n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#example-for-cooks-distance",
    "href": "unit3-mlr/residuals.html#example-for-cooks-distance",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "8.2 Example for Cook’s Distance:",
    "text": "8.2 Example for Cook’s Distance:\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# --- 1) Data and full-model fit (your setup)\n\nn &lt;- 20\nx &lt;- 1:n\ny &lt;- 2 + 3 * x + rnorm(n, mean = 0, sd = 5)\ny[1] &lt;- y[1] + 30   # add an outlier at i = 1\ny[11] &lt;- y[11] + 30\nfull_data  &lt;- data.frame(x = x, replicate(5, rnorm(n)), y = y)\n\nfit &lt;- lm(y ~ ., data = full_data)\n\n# --- 2) Ingredients for Cook's D and thresholds\np      &lt;- length(coef(fit))        # includes intercept\nh      &lt;- hatvalues(fit)           # leverage h_ii\nD      &lt;- cooks.distance(fit)      # Cook's D_i\nnu     &lt;- n - p - 1                # df for deleted-studentized residual\nalpha  &lt;- 0.05\n\n# Per-case scaling factor: c_i = ((n-p)/p) * (h_ii/(1-h_ii))\nc_i        &lt;- ((n - p) / p) * (h / (1 - h))\n\n# Exact per-case Beta 95% critical values:\n# D_i / c_i ~ Beta(1/2, (n-p-1)/2)  =&gt;  D_crit_i = c_i * qbeta(0.95, 1/2, (n-p-1)/2)\ncrit_beta_i &lt;- c_i * qbeta(1 - alpha, shape1 = 0.5, shape2 = nu / 2)\n\n# Average-leverage Beta 95% critical value:\n# If h_ii = p/n (average leverage), then c_i = 1 exactly\ncrit_beta_avg &lt;- qbeta(1 - alpha, shape1 = 0.5, shape2 = nu / 2)\n\n# Heuristic 4/n line\ncrit_heur &lt;- 4 / n\n\n# --- 3) Assemble results table\ndf_cook &lt;- tibble(\n  i            = seq_len(n),\n  h_ii         = as.numeric(h),\n  D_i          = as.numeric(D),\n  c_i          = as.numeric(c_i),\n  crit_beta_i  = as.numeric(crit_beta_i),\n  crit_beta_avg = crit_beta_avg,\n  crit_heur    = crit_heur\n)\n\n# Print a compact table\nknitr::kable(\n  df_cook %&gt;% mutate(across(where(is.numeric), ~ round(.x, 5))),\n  caption = \"Cook's D, leverage, and thresholds: per-case Beta 95%, average-leverage Beta 95%, and 4/n.\"\n)\n\n\n\nCook’s D, leverage, and thresholds: per-case Beta 95%, average-leverage Beta 95%, and 4/n.\n\n\ni\nh_ii\nD_i\nc_i\ncrit_beta_i\ncrit_beta_avg\ncrit_heur\n\n\n\n\n1\n0.67204\n1.62738\n3.80555\n1.07873\n0.28346\n0.2\n\n\n2\n0.31296\n0.07715\n0.84595\n0.23979\n0.28346\n0.2\n\n\n3\n0.49722\n0.08784\n1.83660\n0.52061\n0.28346\n0.2\n\n\n4\n0.61111\n0.23875\n2.91831\n0.82724\n0.28346\n0.2\n\n\n5\n0.21633\n0.02983\n0.51265\n0.14532\n0.28346\n0.2\n\n\n6\n0.14309\n0.00469\n0.31011\n0.08790\n0.28346\n0.2\n\n\n7\n0.47629\n0.07632\n1.68900\n0.47877\n0.28346\n0.2\n\n\n8\n0.41204\n0.00004\n1.30148\n0.36892\n0.28346\n0.2\n\n\n9\n0.40499\n0.29801\n1.26407\n0.35832\n0.28346\n0.2\n\n\n10\n0.17593\n0.00026\n0.39648\n0.11239\n0.28346\n0.2\n\n\n11\n0.39917\n0.73199\n1.23380\n0.34974\n0.28346\n0.2\n\n\n12\n0.10715\n0.00088\n0.22287\n0.06318\n0.28346\n0.2\n\n\n13\n0.21158\n0.01277\n0.49837\n0.14127\n0.28346\n0.2\n\n\n14\n0.30757\n0.00026\n0.82491\n0.23383\n0.28346\n0.2\n\n\n15\n0.26786\n0.04549\n0.67944\n0.19260\n0.28346\n0.2\n\n\n16\n0.71261\n0.48354\n4.60490\n1.30532\n0.28346\n0.2\n\n\n17\n0.20630\n0.01202\n0.48271\n0.13683\n0.28346\n0.2\n\n\n18\n0.27965\n0.00082\n0.72098\n0.20437\n0.28346\n0.2\n\n\n19\n0.29482\n0.03388\n0.77642\n0.22009\n0.28346\n0.2\n\n\n20\n0.29132\n0.00775\n0.76341\n0.21640\n0.28346\n0.2\n\n\n\nCook’s D by case with heuristic 4/n (red dotted), per-case Beta 95% critical values (blue dashed), and average-leverage Beta 95% line (purple dot-dash).\n\n\nCode\n# --- 4) Plot D_i with thresholds\nthresh_df &lt;- bind_rows(\n  df_cook %&gt;% transmute(i, value = crit_beta_i,   Type = \"Beta 95% (per-case)\"),\n  tibble(i = seq_len(n), value = crit_beta_avg,   Type = \"Beta 95% (avg leverage)\"),\n  tibble(i = seq_len(n), value = crit_heur,       Type = \"4/n rule\")\n)\n\nggplot(df_cook, aes(x = i, y = D_i)) +\n  geom_point(size = 2) +\n  geom_line(data = thresh_df,\n            aes(y = value, color = Type, linetype = Type),\n            linewidth = 0.9) +\n  scale_color_manual(values = c(\n    \"Beta 95% (per-case)\"      = \"#1f77b4\",\n    \"Beta 95% (avg leverage)\"  = \"#7f3c8d\",\n    \"4/n rule\"                 = \"#d62728\"\n  )) +\n  scale_linetype_manual(values = c(\n    \"Beta 95% (per-case)\"      = \"dashed\",\n    \"Beta 95% (avg leverage)\"  = \"dotdash\",\n    \"4/n rule\"                 = \"dotted\"\n  )) +\n  labs(x = \"Observation i\", y = \"Cook's D_i\") +\n  theme_bw() +\n  theme(legend.title = element_blank())\n\n\n\n\n\nCook’s D by case with heuristic 4/n (red dotted), per-case Beta 95% critical values (blue dashed), and average-leverage Beta 95% line (purple dot-dash).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#finding-the-loocv-residual-e_i-i-from-the-ordinary-residual-e_i",
    "href": "unit3-mlr/residuals.html#finding-the-loocv-residual-e_i-i-from-the-ordinary-residual-e_i",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "Finding the LOOCV Residual (\\(e_{i,-i}\\)) from the Ordinary Residual (\\(e_i\\))",
    "text": "Finding the LOOCV Residual (\\(e_{i,-i}\\)) from the Ordinary Residual (\\(e_i\\))\nThis identity shows that we can find the “pure” leave-one-out residual using only the results from the single model fit on all data.\n\\[e_{i,-i} = \\frac{e_i}{1 - h_{ii}} \\tag{8.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/residuals.html#finding-the-loocv-standard-error-from-the-full-model-standard-error",
    "href": "unit3-mlr/residuals.html#finding-the-loocv-standard-error-from-the-full-model-standard-error",
    "title": "4  Understanding the Leverage for Adjusting Residuals of OLS",
    "section": "Finding the LOOCV Standard Error from the Full-Model Standard Error",
    "text": "Finding the LOOCV Standard Error from the Full-Model Standard Error\nSimilarly, this formula provides an efficient shortcut to see how the model’s overall error changes when a single point is removed.\n\\[\\hat{\\sigma}_{-i} = \\sqrt{\\frac{(n-p)\\hat{\\sigma}^2 - \\frac{e_i^2}{1-h_{ii}}}{n-p-1}} \\tag{8.2}\\]\nThe derivation of this formula relies on first proving the relationship between the full model’s Residual Sum of Squares (\\(RSS\\)) and the leave-one-out version (\\(RSS_{-i}\\)).\n\nStart with the definition of the leave-one-out residual sum of squares: \\[\nRSS_{-i} = \\sum_{k \\neq i} (y_k - \\mathbf{x}_k^T\\hat{\\beta}_{-i})^2\n\\]\nIntroduce the key identity that relates the leave-one-out coefficient vector (\\(\\hat{\\beta}_{-i}\\)) to the full model’s coefficient vector (\\(\\hat{\\beta}\\)): \\[\n\\hat{\\beta}_{-i} = \\hat{\\beta} - (X^TX)^{-1}\\mathbf{x}_i \\frac{e_i}{1 - h_{ii}}\n\\]\nSubstitute this identity into the expression for a generic leave-one-out residual, \\(e_{k,-i} = y_k - \\mathbf{x}_k^T\\hat{\\beta}_{-i}\\). After simplification, this yields: \\[\ne_{k,-i} = e_k + h_{ki} \\frac{e_i}{1 - h_{ii}}\n\\] where \\(e_k\\) is the ordinary residual and \\(h_{ki}\\) is the \\((k,i)\\)-th element of the hat matrix.\nSubstitute this back into the definition of \\(RSS_{-i}\\). After expanding the squared term and performing the summation (which involves considerable but standard matrix algebra), the expression simplifies to the elegant result: \\[\nRSS_{-i} = RSS - \\frac{e_i^2}{1 - h_{ii}}\n\\]\nFinally, derive the formula for \\(\\hat{\\sigma}_{-i}\\). We know that \\(\\hat{\\sigma}^2_{-i} = \\frac{RSS_{-i}}{n-p-1}\\) and that \\(RSS = (n-p)\\hat{\\sigma}^2\\). By substituting the result from Step 4, we arrive at the formula for the variance, and taking the square root gives us the standard error. ✅",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Understanding the Leverage for Adjusting Residuals of OLS</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html",
    "href": "unit4-lr/logistic.html",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "6 Odds as a Function of Probability\nFor an event with probability \\(p\\), the odds is \\[\\mathrm{odds}(p)=\\frac{p}{1-p}\\] and the log-odds (logit) is \\[\\mathrm{logit}(p)=\\log\\left(\\frac{p}{1-p}\\right)\\].\nCode\n# Plot odds(p) with a right-hand axis for log(odds(p)),\n# using different line colors for the two curves.\n# Defaults: p in [0.01, 0.99].\n# Args:\n#   p_min, p_max : endpoints for p-grid (0&lt;p_min&lt;p_max&lt;1)\n#   n            : number of grid points\n#   annotate     : add reference lines/labels if TRUE\n#   odds_col     : color for odds(p)\n#   logit_col    : color for log(odds(p))\n#   lwd1, lwd2   : line widths for the two curves\n\n\nplot_odds &lt;- function(p_min = 0.01, p_max = 0.99, n = 400,\n                      annotate = TRUE,\n                      odds_col = \"steelblue\",\n                      logit_col = \"firebrick\",\n                      lwd1 = 2, lwd2 = 2) {\n  stopifnot(p_min &gt; 0, p_max &lt; 1, p_min &lt; p_max, n &gt;= 10)\n  p &lt;- seq(p_min, p_max, length.out = n)\n  odds &lt;- p / (1 - p)\n  logit &lt;- log(odds)\n\n  # Left y-axis: odds(p)\n  plot(p, odds, type = \"l\", lwd = lwd1, col = odds_col,\n       xlab = \"Probability p\",\n       ylab = \"odds(p) = p / (1 - p)\")\n  if (annotate) {\n    abline(h = 1, v = 0.5, lty = 2)\n    text(0.52, 1.05, \"p = 0.5 → odds = 1\", adj = 0)\n  }\n\n  # Right y-axis: logit(p) = log(odds)\n  op &lt;- par(new = TRUE)\n  on.exit(par(op), add = TRUE)\n  plot(p, logit, type = \"l\", lwd = lwd2, col = logit_col,\n       axes = FALSE, xlab = \"\", ylab = \"\")\n  axis(4)\n  mtext(\"log{odds(p)} = log{p/(1 - p)}\", side = 4, line = 3)\n\n  if (annotate) {\n    abline(v = 0.5, lty = 2)\n    # logit(0.5) = 0 reference (horizontal) on the right-axis scale\n    usr &lt;- par(\"usr\")\n    segments(x0 = usr[1], y0 = 0, x1 = 0.5, y1 = 0, lty = 3)\n  }\n\n  legend(\"topleft\",\n         legend = c(\"odds(p)\", \"log{odds(p)}\"),\n         col = c(odds_col, logit_col),\n         lwd = c(lwd1, lwd2), bty = \"n\")\n\n  invisible(list(p = p, odds = odds, logit = logit))\n}\n\n# Example usage:\n# plot_odds()  # defaults: steelblue for odds, firebrick for log-odds (right axis)\nplot_odds(odds_col = \"#1f77b4\", logit_col = \"#d62728\", n = 600)\nLogistic regression models log-odds linearly in predictors, which both keeps fitted probabilities in \\((0,1)\\) and turns multiplicative effects on odds into additive effects on the linear predictor.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#fit-a-logistic-model-to-the-simulated-data",
    "href": "unit4-lr/logistic.html#fit-a-logistic-model-to-the-simulated-data",
    "title": "5  Logistic Regression",
    "section": "7.1 Fit a logistic model to the simulated data",
    "text": "7.1 Fit a logistic model to the simulated data\n\n\nCode\n# -- Optional: fit a model to the simulated data --\nsim.fit &lt;- glm(y ~ x, data = sim.data, family = binomial())\np_fit &lt;- predict(sim.fit, newdata = data.frame(x = x), type = \"response\")\n\n# -- Plot: points for y_i (jittered), red line for true p(x) --\n# Define jitter amount\njit &lt;- 0.05 \n# jitter to separate 0/1 visually\nyj &lt;- jitter(sim.data$y, amount = jit) \n\nplot(sim.data$x, yj,\n     pch = 16, col = rgb(0, 0, 0, 0.45),\n     xlab = \"x\",\n     ylab = \"Observed y (points) & p(x) (curves)\",\n     ylim = c(-0.1, 1.1))\n\n# True probability curve (red)\nxg &lt;- seq(min(x), max(x), length.out = 500)\nlines(xg, plogis(beta0 + beta1 * xg), col = \"red\", lwd = 2)\n\n# Optional: add fitted probability curve (dashed dark red)\nlines(xg, predict(sim.fit, newdata = data.frame(x = xg), type = \"response\"),\n      col = \"darkred\", lwd = 2, lty = 2)\n\nlegend(\"topleft\",\n       legend = c(\"y (jittered points)\", \"true p(x)\", \"fitted p(x)\"),\n       pch    = c(16, NA, NA),\n       lty    = c(NA, 1, 2),\n       col    = c(rgb(0,0,0,0.45), \"red\", \"darkred\"),\n       lwd    = c(NA, 2, 2),\n       bty    = \"n\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#load-a-dataset",
    "href": "unit4-lr/logistic.html#load-a-dataset",
    "title": "5  Logistic Regression",
    "section": "8.1 Load a dataset",
    "text": "8.1 Load a dataset\nThis dataset is about a follow-up study to determine the development of coronary heart disease (CHD) over 9 years of follow-up of 609 white males from Evans County, Georgia.\nVariable meanings (as provided):\n\nchd: 1 if a person has the disease, 0 otherwise.\nsmk: 1 if smoker, 0 if not.\ncat: 1 if catecholamine level is high, 0 if low.\nsbp: systolic blood pressure (continuous).\nage: age in years (continuous).\nchl: cholesterol level (continuous).\necg: 1 if electrocardiogram is abnormal, 0 if normal.\nhpt: 1 if high blood pressure, 0 if normal.\n\n\n\nCode\n# Adjust the path if needed. The default is your original V: drive path.\ndata_path &lt;- \"evans.dat\"\n\n# Read data (expects a header row)\nCHD.data &lt;- read.table(data_path, header = TRUE)\n\nCHD.data\n\n\n\n  \n\n\n\nCode\ncolnames(CHD.data)\n\n\n [1] \"id\"  \"chd\" \"age\" \"cat\" \"chl\" \"dbp\" \"ecg\" \"sbp\" \"smk\" \"hpt\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#fit-logistic-regression-model-for-a-single-variable",
    "href": "unit4-lr/logistic.html#fit-logistic-regression-model-for-a-single-variable",
    "title": "5  Logistic Regression",
    "section": "8.2 Fit Logistic Regression Model for a Single Variable",
    "text": "8.2 Fit Logistic Regression Model for a Single Variable\n\n\nCode\nvars &lt;- c(\"smk\", \"sbp\", \"age\", \"chl\")\n#jit  &lt;- 0.01  # global jitter amount for y\n\n# par(mfrow = c(2, 2), mar = c(4, 4, 2, 4) + 0.1)  # extra right margin for axis(4)\n\nfor (v in vars) {\n  # Univariate logistic regression using ORIGINAL variable name in the formula\n  fit &lt;- glm(\n    formula = reformulate(v, response = \"chd\"),\n    data    = CHD.data,\n    family  = binomial()\n  )\n  print(summary(fit))\n\n  # Base scatter of chd with small jitter (left axis: probability scale)\n  plot(\n    CHD.data[[v]],\n    jitter(CHD.data$chd, amount = jit),\n    pch  = 16, col = rgb(0, 0, 0, 0.45),\n    xlab = v, ylab = \"chd (jittered)\",\n    main = paste(\"chd vs\", v),\n    ylim = c(-0.1, 1.1)\n  )\n\n  # Fitted π(x) in red (left axis)\n  if (length(unique(CHD.data[[v]])) == 2) {\n    # binary predictor\n    xcat &lt;- sort(unique(CHD.data[[v]]))\n    nd   &lt;- setNames(data.frame(xcat), v)\n    pcat &lt;- predict(fit, newdata = nd, type = \"response\")\n    points(xcat, pcat, pch = 19, col = \"red\")\n    lines(xcat, pcat, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{π(x)} with fixed y-limits\n    logit_p &lt;- log(pcat / (1 - pcat))\n    par(new = TRUE)\n    plot(\n      xcat, logit_p, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(CHD.data[[v]]), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n\n  } else {\n    # continuous predictor\n    xg &lt;- seq(min(CHD.data[[v]]), max(CHD.data[[v]]), length.out = 400)\n    nd &lt;- setNames(data.frame(xg), v)\n    pg &lt;- predict(fit, newdata = nd, type = \"response\")\n    lines(xg, pg, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{π(x)} with fixed y-limits\n    logit_pg &lt;- log(pg / (1 - pg))\n    par(new = TRUE)\n    plot(\n      xg, logit_pg, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(xg), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n  }\n}\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.4898     0.2524  -9.865   &lt;2e-16 ***\nsmk           0.6706     0.2919   2.297   0.0216 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 432.81  on 607  degrees of freedom\nAIC: 436.81\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.837912   0.629805  -6.094  1.1e-09 ***\nsbp          0.012154   0.004036   3.011   0.0026 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 430.06  on 607  degrees of freedom\nAIC: 434.06\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.47833    0.75610  -5.923 3.16e-09 ***\nage          0.04445    0.01315   3.381 0.000723 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 427.22  on 607  degrees of freedom\nAIC: 431.22\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.538260   0.686879  -5.151 2.59e-07 ***\nchl          0.007004   0.003064   2.286   0.0223 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 433.42  on 607  degrees of freedom\nAIC: 437.42\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#fit-logistic-regression-model-with-all-variables",
    "href": "unit4-lr/logistic.html#fit-logistic-regression-model-with-all-variables",
    "title": "5  Logistic Regression",
    "section": "8.3 Fit Logistic Regression Model with all variables",
    "text": "8.3 Fit Logistic Regression Model with all variables\nWe fit a logistic regression with a logit link:\n\n\nCode\nfit1_chd &lt;- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = CHD.data,\n  family = binomial(link = \"logit\")\n)\nsummary(fit1_chd)\n\n\n\nCall:\nglm(formula = chd ~ smk + cat + sbp + age + chl + ecg + hpt, \n    family = binomial(link = \"logit\"), data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.048892   1.345165  -4.497  6.9e-06 ***\nsmk          0.855951   0.306505   2.793  0.00523 ** \ncat          0.732763   0.376129   1.948  0.05139 .  \nsbp         -0.006995   0.006976  -1.003  0.31600    \nage          0.033956   0.015344   2.213  0.02690 *  \nchl          0.008970   0.003274   2.740  0.00615 ** \necg          0.417776   0.295553   1.414  0.15750    \nhpt          0.655498   0.359976   1.821  0.06861 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 399.35  on 601  degrees of freedom\nAIC: 415.35\n\nNumber of Fisher Scoring iterations: 5\n\n\nNotes for interpretation:\n\nPositive coefficients increase the log-odds of CHD; negative coefficients decrease it.\nFor indicator variables (e.g., smk), exp(beta) is the adjusted odds ratio comparing the group with value 1 versus 0, holding others fixed.\nFor continuous predictors (e.g., sbp, age), exp(beta) is the multiplicative change in the odds for a one‑unit increase. For a d-unit increase, the OR is exp(d * beta).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#interpretation-of-odds-ratios-in-logistic-regression",
    "href": "unit4-lr/logistic.html#interpretation-of-odds-ratios-in-logistic-regression",
    "title": "5  Logistic Regression",
    "section": "10.1 Interpretation of Odds Ratios in Logistic Regression",
    "text": "10.1 Interpretation of Odds Ratios in Logistic Regression\nA multiple logistic regression model expresses the log-odds (logit) of an event as a linear function of predictors:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right)\n= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k.\n\\]\nHere,\n\n\\(p = \\Pr(Y = 1 \\mid x_1, x_2, \\ldots, x_k)\\) is the probability of the event,\n\\(\\beta_0\\) is the intercept, and\neach \\(\\beta_j\\) represents the change in the log-odds of the event per one-unit increase in \\(x_j\\), holding all other variables constant.\n\nExponentiating both sides gives the model in odds form:\n\\[\n\\frac{p}{1-p}\n= \\exp(\\beta_0)\n\\times \\exp(\\beta_1 x_1)\n\\times \\exp(\\beta_2 x_2)\n\\times \\cdots\n\\times \\exp(\\beta_k x_k).\n\\]\nAn R function for OR at two Profiles\nThe or_from_predict R function is a utility designed to calculate the Odds Ratio (OR) and its 95% confidence interval (CI) between two specific covariate profiles (new1 and new0) for a given logistic regression model (fit). The calculation is performed on the link (logit) scale. For a logistic model \\(\\text{logit}(p) = \\eta = \\mathbf{X}\\boldsymbol{\\beta}\\), the log-Odds Ratio (logOR) is the difference between the linear predictors (\\(\\eta_1, \\eta_0\\)) for the two profiles: \\[\\widehat{\\text{logOR}} = \\eta_1 - \\eta_0 = (\\mathbf{x}_1^T - \\mathbf{x}_0^T) \\boldsymbol{\\beta} = \\mathbf{c}^T \\boldsymbol{\\beta} \\tag{10.1}\\]\nHere, \\(\\mathbf{c} = \\mathbf{x}_1 - \\mathbf{x}_0\\) is the linear contrast vector derived from the model matrices of the two profiles. The function estimates the variance of this contrast as \\(\\text{Var}(\\widehat{\\text{logOR}}) = \\mathbf{c}^T \\mathbf{V} \\mathbf{c}\\), where \\(\\mathbf{V}\\) is the model’s variance-covariance matrix (vcov(fit)). The standard error \\(SE = \\sqrt{\\mathbf{c}^T \\mathbf{V} \\mathbf{c}}\\) is used to compute the \\(100(1-\\alpha)\\%\\) confidence interval for the logOR: \\[\\widehat{\\text{logOR}} \\pm z_{1-\\alpha/2} \\times SE.\\] These values (estimate and CI bounds) are then exponentiated to produce the final \\(\\widehat{\\text{OR}} = \\exp(\\widehat{\\text{logOR}})\\) and its 95% CI. The function also prints two helpful summaries to the console: a data frame showing only the variables that differ between the new0 and new1 profiles, and a 2x3 table presenting the estimates and CIs for both the OR and the logOR.\nThe R function to find ORs\n\n\nCode\n# Compute OR and 95% CI via predict() on the LINK scale\n# OR = exp( eta(new1) - eta(new0) ), where eta(.) = logit{π(.)}\n# Compute OR via predict() contrast on the LINK scale, also:\n# (ii) print a 2-row data.frame of only variables that differ between new0 and new1\n# (iii) print a 2x3 table (rows: OR, logOR; cols: Estimate, CI_low, CI_up)\nor_from_predict &lt;- function(fit, new1, new0, level = 0.95, digits = 4, tol = 1e-12) {\n  stopifnot(is.data.frame(new1), is.data.frame(new0))\n\n  # --- REFACTORED SECTION START ---\n  # ---- (ii) Two-row data.frame with only changed variables ----\n  \n  # Helper function to find differing variables between two profiles\n  # This is defined *inside* or_from_predict for encapsulation\n  get_changed_vars &lt;- function(d0, d1, tolerance) {\n    common &lt;- intersect(names(d0), names(d1))\n    diffv  &lt;- vapply(common, function(nm) {\n      x0 &lt;- d0[[nm]]; x1 &lt;- d1[[nm]]\n      if (is.numeric(x0) && is.numeric(x1)) {\n        !isTRUE(all.equal(as.numeric(x0), as.numeric(x1), tolerance = tolerance))\n      } else {\n        !identical(x0, x1)\n      }\n    }, logical(1))\n    \n    keep &lt;- common[diffv]\n    if (length(keep) == 0L) {\n      out &lt;- data.frame(`_no_changes_` = \"no differences\")\n      rownames(out) &lt;- c(\"new0\", \"new1\")\n      return(out)\n    }\n    out &lt;- rbind(d0[keep], d1[keep])\n    rownames(out) &lt;- c(\"new0\", \"new1\")\n    out\n  }\n  \n  # Call the helper function\n  changes_df &lt;- get_changed_vars(new0, new1, tol)\n  # --- REFACTORED SECTION END ---\n\n\n  # ---- Linear contrast for log-OR and its variance ----\n  \n  # (i) Calculate logOR estimate using predict(type=\"link\")\n  # eta(.) = logit{p(.)}\n  eta1 &lt;- predict(fit, newdata = new1, type = \"link\")\n  eta0 &lt;- predict(fit, newdata = new0, type = \"link\")\n  logOR_hat &lt;- as.numeric(eta1 - eta0) # logOR = eta1 - eta0\n  \n  # (ii) Calculate standard error using the contrast vector 'cvec'\n  X1 &lt;- model.matrix(delete.response(terms(fit)), data = new1)\n  X0 &lt;- model.matrix(delete.response(terms(fit)), data = new0)\n  cvec      &lt;- as.numeric(X1 - X0)\n  V         &lt;- vcov(fit)\n  se_logOR  &lt;- sqrt(as.numeric(t(cvec) %*% V %*% cvec))\n\n  alpha  &lt;- 1 - level\n  z      &lt;- qnorm(1 - alpha / 2)\n  ci_log &lt;- c(logOR_hat - z * se_logOR, logOR_hat + z * se_logOR)\n\n  # ---- 2x3 table: rows OR and logOR; columns Estimate, CI_low, CI_up ----\n  res_tab &lt;- data.frame(\n    Estimate = c(exp(logOR_hat),          logOR_hat),\n    CI_low   = c(exp(ci_log[1L]),         ci_log[1L]),\n    CI_up    = c(exp(ci_log[2L]),         ci_log[2L]),\n    row.names = c(\"OR\", \"logOR\")\n  )\n\n  # ---- Print requested items ----\n  cat(\"\\nVariables that differ between new0 and new1:\\n\")\n  print(changes_df)\n  cat(\"\\nOdds Ratio summary:\\n\")\n  print(round(res_tab, digits = digits)) # Added rounding for neatness\n\n  # ---- Return (invisibly) ----\n  invisible(list(\n    OR        = exp(logOR_hat),\n    CI_OR     = exp(ci_log),\n    logOR     = logOR_hat,\n    CI_logOR  = ci_log,\n    se_logOR  = se_logOR,\n    changes   = changes_df,\n    table     = res_tab\n  ))\n}\n\n# --- Example usage ---\n# Suppose 'fit1_chd' is your fitted model and 'CHD.data' is your data\n# base_prof &lt;- as.data.frame(lapply(CHD.data, function(col) if (is.numeric(col)) mean(col) else col[1]))\n# new0 &lt;- base_prof; new0$smk &lt;- 0\n# new1 &lt;- base_prof; new1$smk &lt;- 1\n# or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\nmean_profile &lt;- function(data, vars_binary_as = c(0,1)) {\n  # Build a single-row data.frame of typical values:\n  out &lt;- lapply(data, function(col) {\n    if (is.numeric(col)) {\n      # If strictly 0/1, keep mean (works fine for GLM prediction),\n      # or switch to mode if you prefer.\n      if (all(col %in% c(0,1))) mean(col) else mean(col, na.rm = TRUE)\n    } else {\n      # Fallback to first level for factors/characters\n      if (is.factor(col)) levels(col)[1] else unique(col)[1]\n    }\n  })\n  as.data.frame(out)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#examples-of-finding-ors-and-their-cis-for-the-chd-dataset",
    "href": "unit4-lr/logistic.html#examples-of-finding-ors-and-their-cis-for-the-chd-dataset",
    "title": "5  Logistic Regression",
    "section": "10.2 Examples of Finding ORs and Their CIs for the CHD Dataset",
    "text": "10.2 Examples of Finding ORs and Their CIs for the CHD Dataset\n\n10.2.1 OR Smoking (smk) (1 vs 0)\n\n\nCode\n## 1) Smoking OR: smk = 1 vs 0 (other vars at their means)\n# Example profiles at sample means (adjust as you like)\nbase_prof &lt;- mean_profile(CHD.data)\nnew0 &lt;- base_prof; new0$smk &lt;- 0\nnew1 &lt;- base_prof; new1$smk &lt;- 1\n\nres_smk &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     smk\nnew0   0\nnew1   1\n\nOdds Ratio summary:\n      Estimate CI_low  CI_up\nOR      2.3536 1.2907 4.2917\nlogOR   0.8560 0.2552 1.4567\n\n\nHow to read this:\n\nOR_smk &gt; 1 suggests higher odds of CHD among smokers (adjusted for other variables). If the 95% CI excludes 1, the association is statistically significant at the 5% level.\n\n\n\n10.2.2 OR for Systolic Blood Pressure (sbp): from 120 to 160\nWe compute the adjusted OR for a 40‑unit increase in sbp (from 120 to 160):\n\n\nCode\n## 2) SBP OR: 160 vs 120 (other vars at their means)\nnew0 &lt;- base_prof; new0$sbp &lt;- 120\nnew1 &lt;- base_prof; new1$sbp &lt;- 160\n\nres_sbp &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     sbp\nnew0 120\nnew1 160\n\nOdds Ratio summary:\n      Estimate  CI_low  CI_up\nOR      0.7559  0.4375 1.3062\nlogOR  -0.2798 -0.8267 0.2671\n\n\n\n\n10.2.3 OR for Combined Effects of Two Variables: Smoking with an Age Difference\nSuppose we compare two groups that differ in smoking status and age:\n\nGroup A: smk = 1, age = 50 (all other covariates equal)\nGroup B: smk = 0, age = 30\n\nThe log‑odds contrast is (\\(A = \\beta_{smk} + (50-20)\\beta_{age}\\)), so the OR is (\\(\\exp(A)\\)).\n\n\nCode\nnew0 &lt;- base_prof; new0$age &lt;- 30; new0$smk &lt;- 0\nnew1 &lt;- base_prof; new1$age &lt;- 50; new1$smk &lt;- 1\n\nres_ageAsmk &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     age smk\nnew0  30   0\nnew1  50   1\n\nOdds Ratio summary:\n      Estimate CI_low   CI_up\nOR      4.6417 1.8546 11.6168\nlogOR   1.5351 0.6177  2.4525",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#understanding-the-confusion-matrix-and-metrics",
    "href": "unit4-lr/logistic.html#understanding-the-confusion-matrix-and-metrics",
    "title": "5  Logistic Regression",
    "section": "12.1 Understanding the Confusion Matrix and Metrics",
    "text": "12.1 Understanding the Confusion Matrix and Metrics\nTo evaluate a model’s predictive performance, we classify its probabilistic predictions using a threshold (typically 0.5) and compare them to the true outcomes in a Confusion Matrix:\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\nFrom this matrix, we derive several key performance metrics:\n\nMisclassification Error Rate (ER): The proportion of all predictions that were incorrect. \\[\n  \\text{Error Rate} = \\frac{FP + FN}{TP + TN + FP + FN}\n  \\]\nPrecision (Positive Predictive Value): Answers: “Of all the times the model predicted positive, how often was it correct?” This is crucial when the cost of a False Positive is high. \\[\n  \\text{Precision} = \\frac{TP}{TP + FP}\n  \\]\nRecall (Sensitivity or True Positive Rate): Answers: “Of all the actual positive cases, how many did the model find?” This is crucial when the cost of a False Negative is high. \\[\n  \\text{Recall (TPR)} = \\frac{TP}{TP + FN}\n  \\]\nROC Curve and AUC: An ROC (Receiver Operating Characteristic) Curve is a graph that shows a model’s diagnostic ability across all possible classification thresholds. It plots the True Positive Rate (Recall) on the y-axis against the False Positive Rate (FPR = \\(\\frac{FP}{FP + TN}\\)) on the x-axis.\n\nInterpretation: The curve shows the trade-off between sensitivity (finding all the positives) and specificity (not mislabeling negatives). A random “no-skill” classifier is represented by a diagonal line from (0,0) to (1,1). A perfect classifier would hug the top-left corner (TPR = 1, FPR = 0).\nAUC (Area Under the Curve): The AUC summarizes the entire curve into a single number from 0 to 1. An AUC of 0.5 corresponds to a random guess, while an AUC of 1.0 represents a perfect model.\n\nPrecision-Recall (PR) Curve: A PR Curve plots Precision (y-axis) against Recall (x-axis) at all possible thresholds.\n\nInterpretation: This curve shows the trade-off between how reliable a positive prediction is (Precision) and how complete the model is at finding all positives (Recall).\nWhen to Use: The PR curve is particularly informative when the dataset is imbalanced (i.e., one class, like “fraud” or “disease,” is much rarer than the other). Unlike the ROC curve, the PR curve’s baseline (the “no-skill” line) is a horizontal line at the proportion of positive cases, which makes it easier to see if the model is performing significantly better than chance in a low-positive-rate scenario. A perfect classifier would hug the top-right corner (Precision = 1, Recall = 1).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#illustration-with-the-simulated-dataset",
    "href": "unit4-lr/logistic.html#illustration-with-the-simulated-dataset",
    "title": "5  Logistic Regression",
    "section": "12.2 Illustration with the Simulated Dataset",
    "text": "12.2 Illustration with the Simulated Dataset\nThis section applies the train/test split and model evaluation workflow to the sim.data created in the previous step.\n\n\nCode\n# Load the pROC library for AUC calculation\n# install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n# --- 1. Split the data ---\n# We use 'sim.data' which has 200 rows\nset.seed(123) # for reproducibility\nn_sim &lt;- nrow(sim.data)\ntrain_size_sim &lt;- floor(2/3 * n_sim)\ntrain_indices_sim &lt;- sample(1:n_sim, size = train_size_sim)\ntrain_data_sim &lt;- sim.data[train_indices_sim, ]\ntest_data_sim  &lt;- sim.data[-train_indices_sim, ]\n\n# --- 2. Refit the model on the training data ---\n# We fit the model y ~ x on the training data\nfit_train_sim &lt;- glm(\n  y ~ x,\n  data = train_data_sim,\n  family = binomial(link = \"logit\")\n)\n\n# --- 3. Make predictions on the test data ---\n# Note: The true probabilities 'p' are also in test_data_sim\n# We predict from the *fitted* model\npred_probs_sim &lt;- predict(fit_train_sim, newdata = test_data_sim, type = \"response\")\n\n\nPlotting the Predictive Probabilities with True Labels\n\n\nCode\n# --- 5. Plot sorted predicted probabilities ---\n\n# Create a data frame for plotting\nplot_data_sim &lt;- data.frame(\n  Prob = pred_probs_sim,\n  Actual = as.factor(test_data_sim$y),\n  TrueProb = test_data_sim$p # Include true probs for comparison\n)\n\n# Sort by predicted probability\nplot_data_sim &lt;- plot_data_sim[order(plot_data_sim$Prob), ]\nplot_data_sim$Rank &lt;- 1:nrow(plot_data_sim)\n\n# Create the plot\nplot(\n  plot_data_sim$Rank,\n  plot_data_sim$Prob,\n  pch = ifelse(plot_data_sim$Actual == 0, 1, 4),\n  col = ifelse(plot_data_sim$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class (Simulated Data)\",\n  ylim = c(0, 1)\n)\nabline(h = 0.5, lty = 2, col = \"black\")\nabline(h = 0.1, lty = 3, col = \"grey\")\n\n# Add the true probability curve (sorted by predicted prob)\n# This shows how well the fitted model's predictions align with the true probs\n#lines(plot_data_sim$Rank, plot_data_sim$TrueProb[order(plot_data_sim$Prob)], col = \"darkgreen\", lwd = 2)\n\n\n# Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  lty = c(NA, NA),\n  lwd = c(NA, NA),\n  col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nConfusion Matrix with threshold=0.5\n\n\nCode\n# --- 4. Assess accuracy ---\n\n# 4a. Misclassification Error Rate (using 0.5 threshold)\nthreshold &lt;- 0.5\npred_class_sim &lt;- ifelse(pred_probs_sim &gt; threshold, 1, 0)\nconf_matrix_sim &lt;- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n# --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.5):\\n\")\n\n\nConfusion Matrix (Counts, threshold = 0.5):\n\n\nCode\nprint(conf_matrix_sim)\n\n\n      Predicted\nActual  0  1\n     0 26  5\n     1  9 27\n\n\nCode\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n\n\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n\n\nCode\n# margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.839 0.161\n     1 0.250 0.750\n\n\nCode\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n\n\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n\n\nCode\n# margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.743 0.156\n     1 0.257 0.844\n\n\nCode\n# --- MODIFIED LINES END ---\n\n\n# Check if matrix has 2x2 dimensions, otherwise metrics will fail\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN &lt;- conf_matrix_sim[1, 1]\n  FP &lt;- conf_matrix_sim[1, 2]\n  FN &lt;- conf_matrix_sim[2, 1]\n  TP &lt;- conf_matrix_sim[2, 2]\n\n  # Calculate metrics\n  error_rate &lt;- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall &lt;- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR &lt;- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision &lt;- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n\n\n\nMisclassification Error Rate: 0.209 \nTrue Positive Rate (Recall): 0.75 \nFalse Positive Rate: 0.1613 \nPrecision: 0.8438 \n\n\nConfusion Matrix with threshold=0.1\n\n\nCode\nthreshold &lt;- 0.1\npred_class_sim &lt;- ifelse(pred_probs_sim &gt; threshold, 1, 0)\nconf_matrix_sim &lt;- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n# --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.1):\\n\")\n\n\nConfusion Matrix (Counts, threshold = 0.1):\n\n\nCode\nprint(conf_matrix_sim)\n\n\n      Predicted\nActual  0  1\n     0 15 16\n     1  1 35\n\n\nCode\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n\n\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n\n\nCode\n# margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.484 0.516\n     1 0.028 0.972\n\n\nCode\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n\n\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n\n\nCode\n# margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.938 0.314\n     1 0.062 0.686\n\n\nCode\n# --- MODIFIED LINES END ---\n\n\n# Check if matrix has 2x2 dimensions\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN &lt;- conf_matrix_sim[1, 1]\n  FP &lt;- conf_matrix_sim[1, 2]\n  FN &lt;- conf_matrix_sim[2, 1]\n  TP &lt;- conf_matrix_sim[2, 2]\n\n  # Calculate metrics\n  error_rate &lt;- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall &lt;- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR &lt;- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision &lt;- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n\n\n\nMisclassification Error Rate: 0.2537 \nTrue Positive Rate (Recall): 0.9722 \nFalse Positive Rate: 0.5161 \nPrecision: 0.6863 \n\n\nROC curve and Area Under the ROC (AUC)\n\n\nCode\n# 4b. Area Under the Curve (AUC)\nroc_curve_sim &lt;- roc(test_data_sim$y, pred_probs_sim, quiet = TRUE)\n\n# Plot the ROC curve\nplot(roc_curve_sim, main = \"ROC Curve (Simulated Test Data)\", print.auc = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nauc_value_sim &lt;- auc(roc_curve_sim)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value_sim, 4), \"\\n\\n\"))\n\n\nArea Under the Curve (AUC): 0.8996 \n\n\nPR curve and Area Under PR Curve (AUPR)\n\n\nCode\n# Load the ROCR library\n# install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n# --- 1. Create a 'prediction' object ---\n# 'prediction' takes all predictions and all true labels\npred_obj &lt;- prediction(pred_probs_sim, test_data_sim$y)\n\n# --- 2. Create a 'performance' object for PR ---\n# \"prec\" is for precision, \"rec\" is for recall\nperf_pr &lt;- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n# --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc &lt;- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value &lt;- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n\n\nArea Under PR Curve (AUPR): 0.9157 \n\n\nCode\n# --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Simulated Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n# --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision_sim &lt;- sum(test_data_sim$y == 1) / length(test_data_sim$y)\nabline(h = baseline_precision_sim, col = \"blue\", lty = 2)\n\n# --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # &lt;-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision_sim, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#application-to-the-chd-dataset",
    "href": "unit4-lr/logistic.html#application-to-the-chd-dataset",
    "title": "5  Logistic Regression",
    "section": "12.3 Application to the CHD Dataset",
    "text": "12.3 Application to the CHD Dataset\n\n\nCode\n# Load the pROC library for AUC calculation\n# install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n# --- 1. Split the data ---\nset.seed(123) # for reproducibility\nn &lt;- nrow(CHD.data)\ntrain_size &lt;- floor(2/3 * n)\ntrain_indices &lt;- sample(1:n, size = train_size)\ntrain_data &lt;- CHD.data[train_indices, ]\ntest_data  &lt;- CHD.data[-train_indices, ]\n\n# --- 2. Refit the model on the training data ---\nfit_train &lt;- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = train_data,\n  family = binomial(link = \"logit\")\n)\n\n# --- 3. Make predictions on the test data ---\npred_probs &lt;- predict(fit_train, newdata = test_data, type = \"response\")\n\n\nPlot Predictive Probabilities\n\n\nCode\n# --- 5. Plot sorted predicted probabilities ---\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Prob = pred_probs,\n  Actual = as.factor(test_data$chd)\n)\n\n# Sort by predicted probability\nplot_data &lt;- plot_data[order(plot_data$Prob), ]\nplot_data$Rank &lt;- 1:nrow(plot_data)\n\n# Create the plot\n# We use 'pch' (plot character) to set different symbols\n# 'pch = 1' is 'o' (default)\n# 'pch = 4' is 'x'\nplot(\n  plot_data$Rank,\n  plot_data$Prob,\n  pch = ifelse(plot_data$Actual == 0, 1, 4),\n  col = ifelse(plot_data$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Log-odds of Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class\",\n  ylim = c(0,1)\n)\nabline(h=0.5)\nabline(h=0.1, col=\"grey\")\n\n# Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nROC curve and Area Under the ROC (AUC)\n\n\nCode\n# 4b. Area Under the Curve (AUC)\nroc_curve &lt;- roc(test_data$chd, pred_probs, quiet = TRUE)\n\n# Plot the ROC curve\nplot(roc_curve, main = \"ROC Curve (Test Data)\", print.auc = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nauc_value &lt;- auc(roc_curve)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value, 4), \"\\n\\n\"))\n\n\nArea Under the Curve (AUC): 0.6872 \n\n\nPR curve and Area Under PR Curve (AUPR)\n\n\nCode\n# Load the ROCR library\n# install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n# --- 1. Create a 'prediction' object ---\n# 'prediction' takes all predictions and all true labels\n# We use 'pred_probs' and 'test_data$chd' from the CHD data split\npred_obj &lt;- prediction(pred_probs, test_data$chd)\n\n# --- 2. Create a 'performance' object for PR ---\n# \"prec\" is for precision, \"rec\" is for recall\nperf_pr &lt;- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n# --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc &lt;- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value &lt;- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n\n\nArea Under PR Curve (AUPR): 0.2826 \n\n\nCode\n# --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n# --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision &lt;- sum(test_data$chd == 1) / length(test_data$chd)\nabline(h = baseline_precision, col = \"blue\", lty = 2)\n\n# --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # &lt;-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html",
    "href": "unit5-factor/crbd.html",
    "title": "6  Analysis of Randomized Complete Block Design",
    "section": "",
    "text": "7 Completely Randomized Design",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html#plasma-etching-experiment",
    "href": "unit5-factor/crbd.html#plasma-etching-experiment",
    "title": "6  Analysis of Randomized Complete Block Design",
    "section": "7.1 Plasma Etching Experiment",
    "text": "7.1 Plasma Etching Experiment\nThis section analyzes data from a Completely Randomized Design (CRD). In a CRD, experimental units (in this case, the silicon wafers being etched) are assigned to treatments (the RF Power levels) completely at random. The primary goal is to determine if changing the RF Power level has a statistically significant effect on the mean etch rate.\n\n7.1.1 Data and Visualization\nWe begin by loading the data into a single, tidy data.frame. The response variable, rate, contains all the etch rate observations. The predictor variable, power, is a factor, which is R’s way of representing a categorical variable. This tells R to treat the different power levels as distinct groups.\n\n\nCode\n# Define the data vectors\nrate &lt;- c(575, 542, 530, 539, 570, 565, 593, 590, 579, 610,\n          600, 651, 610, 637, 629, 725, 700, 715, 685, 710)\npower_levels &lt;- c(160, 180, 200, 220)\n\n# Create the data frame\netching_df &lt;- data.frame(\n  rate = rate,\n  power = factor(rep(power_levels, each = 5))\n)\n\n# Display the first few rows\netching_df\n\n\n\n  \n\n\n\nGrouped Boxplots\n\n\nCode\nboxplot(rate~power, data=etching_df)\n\n\n\n\n\n\n\n\n\nUsing ggplot to visualize grouped data\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr) # Using dplyr for easier data manipulation\n\n# Calculate group means and their start/end indices\nmean_rates &lt;- etching_df %&gt;%\n  mutate(obs_index = row_number()) %&gt;%\n  group_by(power) %&gt;%\n  summarise(\n    mean_rate = mean(rate),\n    x_start = min(obs_index) - 0.5,\n    x_end = max(obs_index) + 0.5\n  )\n\nggplot(etching_df, aes(x = 1:nrow(etching_df), y = rate, color = power)) +\n  geom_point(size = 3, alpha = 0.7) + # Plot individual data points\n  geom_segment(\n    data = mean_rates, \n    aes(x = x_start, xend = x_end, y = mean_rate, yend = mean_rate),\n    linetype = \"dashed\", \n    size = 1.2\n  ) + # Add line segments for group means\n  labs(\n    title = \"Etch Rate Observations by RF Power Level\",\n    x = \"Observation Index\",\n    y = \"Etch Rate\",\n    color = \"RF Power (W)\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nIndex Plot of Etch Rate with Group-Specific Mean Lines\n\n\n\n\n\n\n7.1.2 Model Fitting with Sum-to-Zero Constraint\nWe fit a linear model using the lm() function to perform an Analysis of Variance (ANOVA). The model is specified as rate ~ power, and we now include the data = etching_df argument.\nTo get interpretable estimates for the treatment effects (\\(\\tau_i\\)), we use a sum-to-zero constraint (contr.sum), which forces the sum of the treatment effects to be zero (\\(\\sum \\tau_i = 0\\)).\n\n\nCode\nfit &lt;- lm(rate ~ power, data = etching_df, contrasts = list(power = contr.sum))\ncat (\"Model Matrix:\\n\")\n\n\nModel Matrix:\n\n\nCode\nmodel.matrix(fit)\n\n\n   (Intercept) power1 power2 power3\n1            1      1      0      0\n2            1      1      0      0\n3            1      1      0      0\n4            1      1      0      0\n5            1      1      0      0\n6            1      0      1      0\n7            1      0      1      0\n8            1      0      1      0\n9            1      0      1      0\n10           1      0      1      0\n11           1      0      0      1\n12           1      0      0      1\n13           1      0      0      1\n14           1      0      0      1\n15           1      0      0      1\n16           1     -1     -1     -1\n17           1     -1     -1     -1\n18           1     -1     -1     -1\n19           1     -1     -1     -1\n20           1     -1     -1     -1\nattr(,\"assign\")\n[1] 0 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$power\n    [,1] [,2] [,3]\n160    1    0    0\n180    0    1    0\n200    0    0    1\n220   -1   -1   -1\n\n\nCode\nsummary.fit &lt;- summary(fit)\ncat (\"Summary of lm fitting results:\\n\")\n\n\nSummary of lm fitting results:\n\n\nCode\nsummary.fit\n\n\n\nCall:\nlm(formula = rate ~ power, data = etching_df, contrasts = list(power = contr.sum))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  617.750      4.085 151.234  &lt; 2e-16 ***\npower1       -66.550      7.075  -9.406 6.39e-08 ***\npower2       -30.350      7.075  -4.290 0.000563 ***\npower3         7.650      7.075   1.081 0.295602    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9261,    Adjusted R-squared:  0.9122 \nF-statistic:  66.8 on 3 and 16 DF,  p-value: 2.883e-09\n\n\n\n\n7.1.3 Point Estimation of Parameters\nThe output of the model provides estimates for the overall mean (\\(\\hat{\\mu}\\)) and the treatment effects for the first k-1 levels (\\(\\hat{\\tau}_1, \\hat{\\tau}_2, \\hat{\\tau}_3\\)).\n\n\\(\\hat{\\mu}\\) (the Intercept) is the estimate of the grand mean etch rate across all power levels.\n\\(\\hat{\\tau}_i\\) is the estimated effect of the i-th power level, representing how much that level’s mean deviates from the grand mean.\n\nUsing the sum-to-zero constraint, we can manually calculate the effect for the final level, \\(\\hat{\\tau}_4\\).\n\n\nCode\n# Extract coefficients\nest &lt;- coef(fit)\ntau4.hat &lt;- -sum(est[-1])\ntaui.hat &lt;- c(est[-1], tau4.hat)\nprint(\"Estimated Treatment Effects (tau_i):\")\n\n\n[1] \"Estimated Treatment Effects (tau_i):\"\n\n\nCode\nprint(taui.hat)\n\n\npower1 power2 power3        \n-66.55 -30.35   7.65  89.25 \n\n\nCode\n# Estimates of treatment means (mu_i)\nmu.hat &lt;- est[1]\nmui.hat &lt;- mu.hat + taui.hat\nprint(\"Estimated Treatment Means (mu_i):\")\n\n\n[1] \"Estimated Treatment Means (mu_i):\"\n\n\nCode\nprint(mui.hat)\n\n\npower1 power2 power3        \n 551.2  587.4  625.4  707.0 \n\n\n\n\n7.1.4 ANOVA Table\nThe ANOVA table partitions the total variation into variation between treatment groups (power) and variation within treatment groups (random error). The p-value (Pr(&gt;F)) indicates if the treatment has a significant effect.\n\n\nCode\nanova(fit)\n\n\n\n  \n\n\n\n\n\n7.1.5 95% Confidence Intervals for Treatment Means\nA confidence interval provides a range of plausible values for the true mean etch rate at each power level.\n\n\nCode\n# Number of replicates\nn &lt;- 5 \n# Extract sqrt(MSE) and error df\nsqrt.MSE &lt;- summary.fit$sigma\nDF &lt;- fit$df.residual\n# Find t-value\nt.value &lt;- qt(0.975, DF)\n# Calculate CIs\nCI.lower &lt;- mui.hat - t.value * sqrt.MSE / sqrt(n)\nCI.upper &lt;- mui.hat + t.value * sqrt.MSE / sqrt(n)\n\n# Display CIs\ndata.frame(Power_Level = power_levels, Mean = mui.hat, Lower_CI = CI.lower, Upper_CI = CI.upper)\n\n\n\n  \n\n\n\nAlternatively, one can use a model without intercept\n\n\nCode\nfit_nointercpt &lt;- lm(rate ~ 0+power, data = etching_df)\nsummary(fit_nointercpt)\n\n\n\nCall:\nlm(formula = rate ~ 0 + power, data = etching_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \npower160  551.200      8.169   67.47   &lt;2e-16 ***\npower180  587.400      8.169   71.90   &lt;2e-16 ***\npower200  625.400      8.169   76.55   &lt;2e-16 ***\npower220  707.000      8.169   86.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.9991 \nF-statistic:  5768 on 4 and 16 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(fit_nointercpt)\n\n\n            2.5 %   97.5 %\npower160 533.8815 568.5185\npower180 570.0815 604.7185\npower200 608.0815 642.7185\npower220 689.6815 724.3185\n\n\n\n\n7.1.6 Comparison with Default “Treatment” Contrast\nFitting the model without specifying contrasts uses R’s default (“treatment” contrast), which sets \\(\\tau_1 = 0\\). The fundamental results (ANOVA, treatment means) remain unchanged.\n\n\nCode\nfit1 &lt;- lm(rate ~ power, data = etching_df)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = rate ~ power, data = etching_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  551.200      8.169  67.471  &lt; 2e-16 ***\npower180      36.200     11.553   3.133  0.00642 ** \npower200      74.200     11.553   6.422 8.44e-06 ***\npower220     155.800     11.553  13.485 3.73e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9261,    Adjusted R-squared:  0.9122 \nF-statistic:  66.8 on 3 and 16 DF,  p-value: 2.883e-09\n\n\n\n\n7.1.7 Pairwise Comparisons\nSince our ANOVA result was significant, we perform post-hoc tests to determine exactly which pairs of power levels have different means.\n\n7.1.7.1 Tukey’s HSD Test\nTukey’s Honest Significant Difference (HSD) controls the family-wise error rate, adjusting p-values to account for multiple comparisons.\n\n\nCode\nfit.aov &lt;- aov(rate ~ power, data = etching_df)\nTukeyHSD(fit.aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = rate ~ power, data = etching_df)\n\n$power\n         diff        lwr       upr     p adj\n180-160  36.2   3.145624  69.25438 0.0294279\n200-160  74.2  41.145624 107.25438 0.0000455\n220-160 155.8 122.745624 188.85438 0.0000000\n200-180  38.0   4.945624  71.05438 0.0215995\n220-180 119.6  86.545624 152.65438 0.0000001\n220-200  81.6  48.545624 114.65438 0.0000146\n\n\n\n\n7.1.7.2 Fisher’s LSD Test\nThe Fisher’s Least Significant Difference (LSD) test does not control the family-wise error rate but is more powerful.\n\n\nCode\nwith(etching_df, pairwise.t.test(rate, power, p.adj = \"none\"))\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  rate and power \n\n    160     180     200    \n180 0.0064  -       -      \n200 8.4e-06 0.0046  -      \n220 3.7e-10 1.7e-08 2.7e-06\n\nP value adjustment method: none \n\n\n\n\n\n7.1.8 Checking Model Assumptions\nThe validity of our ANOVA results depends on three key assumptions about the model’s residuals. We use diagnostic plots to check them.\n\n\nCode\nr &lt;- rstudent(fit)\nfitted &lt;- fitted.values(fit)\n\n\n\n7.1.8.1 Normality of Residuals\nA Normal Q-Q plot is used to check if the residuals are normally distributed. The points should fall closely along the straight diagonal line.\n\n\nCode\nqqnorm(r)\nqqline(r)\n\n\n\n\n\nNormal Q-Q plot of standardized residuals.\n\n\n\n\n\n\n7.1.8.2 Independence of Residuals\nA plot of residuals versus run order helps check for independence. We look for random scatter around the zero line.\n\n\nCode\nplot(r, ylab = \"Standardized residuals\", xlab = \"Run order\",\n     main = \"Plot of residuals vs. run order\")\nabline(h = 0)\n\n\n\n\n\nStandardized residuals vs. run order.\n\n\n\n\n\n\n7.1.8.3 Constant Variance (Homoscedasticity)\nA plot of residuals versus fitted values helps check for constant variance. The spread of residuals should be roughly constant across all fitted values.\n\n\nCode\nplot(fitted, r, ylab = \"Standardized residuals\", \n     xlab = \"Fitted values\", main = \"Plot of residuals vs. fitted values\")\nabline(h = 0)\n\n\n\n\n\nStandardized residuals vs. fitted values.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html#vascular-graft-experiment",
    "href": "unit5-factor/crbd.html#vascular-graft-experiment",
    "title": "6  Analysis of Randomized Complete Block Design",
    "section": "9.1 Vascular Graft Experiment",
    "text": "9.1 Vascular Graft Experiment\nThis section analyzes a Randomized Complete Block Design (RCBD), used to control for a known source of variability (here, “batches of resin,” treated as blocks).\n\n9.1.1 Data and Visulization\nWe structure the data in a data.frame to identify the response, treatment (pressure), and block (batch) for each observation.\n\n\nCode\n# Define data vectors\nstrength &lt;- c(90.3, 89.2, 98.2, 93.9, 87.4, 97.9,\n              92.5, 89.5, 90.6, 94.7, 87.0, 95.8,\n              85.5, 90.8, 89.6, 86.2, 88.0, 93.4,\n              82.5, 89.5, 85.6, 87.4, 78.9, 90.7)\npressure_levels &lt;- rep(c(8500, 8700, 8900, 9100), each = 6)\nbatch_levels &lt;- rep(1:6, 4)\n\n# Create the data frame\ngraft_df &lt;- data.frame(\n  strength = strength,\n  pressure = factor(pressure_levels),\n  batch = factor(batch_levels)\n)\n\n\ngraft_df\n\n\n\n  \n\n\n\nVisualize the Block and Treatment Effects\n\n\nCode\npar (mfrow = c(1,2))\n#boxplot\nplot(strength ~ batch, data=graft_df , main = \"Block\")\nplot(strength ~ pressure, data=graft_df , main = \"Pressure\")\n\n\n\n\n\n\n\n\n\nInteraction Plots\n\n\nCode\nggplot(graft_df, aes(x = pressure, y = strength, group = batch, color = batch)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  labs(\n    title = \"Interaction Plot: Batch and Pressure\",\n    x = \"Pressue\",\n    y = \"Strength\",\n    color = \"Batch\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nInteraction between Material Type and Temperature.\n\n\n\n\n\n### Model Fitting and ANOVA\n\nThe model `strength ~ pressure + batch` partitions the total variance into treatment, block, and error components. Our primary interest is in the significance of the `pressure` factor.\n\n::: {.cell}\n\n```{.r .cell-code}\nrcbd.fit1 &lt;- aov(strength ~ pressure + batch, data = graft_df)\nanova(rcbd.fit1)\n\n\n  \n\n\n:::\nThe small p-value for pressure (0.0019) provides strong evidence that extrusion pressure significantly affects graft strength after accounting for batch differences.\n\n\n9.1.2 Model Adequacy Checks\nThe assumptions for an RCBD are the same as for a CRD. We perform the same diagnostic checks.\n\nCode\nrcbd.r1 &lt;- rstudent(rcbd.fit1)\nrcbd.fitted1 &lt;- fitted.values(rcbd.fit1)\n\nqqnorm(rcbd.r1, main = \"Normal Q-Q Plot\")\nqqline(rcbd.r1)\nplot(rcbd.fitted1, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Fitted values\", main = \"Residuals vs. Fitted\")\nabline(h = 0)\nplot(graft_df$pressure, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Extrusion pressure\", main = \"Residuals vs. Treatment\")\nabline(h = 0)\nplot(graft_df$batch, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Batches of raw material\", main = \"Residuals vs. Block\")\nabline(h = 0)\n\n\n\n\n\n\n\nNormal Q-Q Plot\n\n\n\n\n\n\n\nResiduals vs. Fitted Values\n\n\n\n\n\n\n\n\n\nResiduals vs. Treatment (Pressure)\n\n\n\n\n\n\n\nResiduals vs. Block (Batch)\n\n\n\n\n\n\n\n9.1.3 Pairwise Comparisons\nAgain, since the treatment factor (pressure) is significant, we perform post-hoc tests.\n\n9.1.3.1 Tukey’s HSD Test\nTukey’s HSD compares all pairs of treatment levels while controlling the family-wise error rate.\n\n\nCode\nTukeyHSD(rcbd.fit1, which = \"pressure\")\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = strength ~ pressure + batch, data = graft_df)\n\n$pressure\n               diff        lwr       upr     p adj\n8700-8500 -1.133333  -5.637161  3.370495 0.8854831\n8900-8500 -3.900000  -8.403828  0.603828 0.1013084\n9100-8500 -7.050000 -11.553828 -2.546172 0.0020883\n8900-8700 -2.766667  -7.270495  1.737161 0.3245644\n9100-8700 -5.916667 -10.420495 -1.412839 0.0086667\n9100-8900 -3.150000  -7.653828  1.353828 0.2257674\n\n\n\n\n9.1.3.2 Fisher’s LSD Test\nThe LSD.test() function from the agricolae package correctly handles the error structure of an RCBD.\n\n\nCode\n# install.packages(\"agricolae\")\nlibrary(agricolae)\n\nout &lt;- LSD.test(rcbd.fit1, trt = \"pressure\", p.adj = \"none\", group = FALSE)\nprint(out$comparison)\n\n\n            difference pvalue signif.        LCL       UCL\n8500 - 8700   1.133333 0.4795         -2.1974047  4.464071\n8500 - 8900   3.900000 0.0247       *  0.5692620  7.230738\n8500 - 9100   7.050000 0.0004     ***  3.7192620 10.380738\n8700 - 8900   2.766667 0.0970       . -0.5640714  6.097405\n8700 - 9100   5.916667 0.0018      **  2.5859286  9.247405\n8900 - 9100   3.150000 0.0621       . -0.1807380  6.480738",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of Randomized Complete Block Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html",
    "href": "unit6-factorial/factorial.html",
    "title": "Analysis of a Two-Factor Factorial Design",
    "section": "",
    "text": "Battery Design Experiment\nThis analysis explores data from a two-factor factorial experiment designed to assess the lifespan of a battery. The experiment investigates two factors: material type (with 3 levels) and operating temperature (with 3 levels: 15°C, 70°C, and 125°C). The primary goal is to understand not only how each factor individually affects battery life but, more importantly, whether the effect of temperature depends on the material type used. This combined effect is known as an interaction.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of a Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#battery-design-experiment",
    "href": "unit6-factorial/factorial.html#battery-design-experiment",
    "title": "Analysis of a Two-Factor Factorial Design",
    "section": "",
    "text": "Data Setup and Preparation\nFirst, we organize the raw data into a structured data.frame. This is a best practice in R that makes the data easier to manage and the code more readable. We create columns for the response variable life and the two factors, material and temperature, ensuring they are treated as categorical variables (factors) for the analysis.\n\n\nCode\n# Response variable: battery life\nlife &lt;- c(130,155,74,180,  34,40,80,75,   20,70,82,58,\n          150,188,159,126, 136,122,106,115, 25,70,58,45,\n          138,110,168,160, 174,120,150,139, 96,104,82,60)\n\n# Create the data frame\nbattery_df &lt;- data.frame(\n  life = life,\n  material = factor(rep(1:3, each = 12)),\n  temperature = factor(rep(rep(c(15, 70, 125), each = 4), 3))\n)\n\n# Preview the data\nbattery_df\n\n\n\n  \n\n\n\n\n\nExploratory Data Analysis and Visualization\nBefore fitting a formal model, we visualize the data to get an intuition for the relationships between the factors and the response.\n\nBoxplots of Main Effects\nBoxplots are excellent for examining the distribution of battery life for each level of our factors independently. This gives us a preliminary look at the main effects—the individual impact of material type and temperature.\n\nCode\nlibrary(ggplot2)\n\n# Boxplot for Material Type\nggplot(battery_df, aes(x = material, y = life, fill = material)) +\n  geom_boxplot() +\n  labs(title = \"Battery Life by Material Type\", x = \"Material Type\", y = \"Life (hours)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n# Boxplot for Temperature\nggplot(battery_df, aes(x = temperature, y = life, fill = temperature)) +\n  geom_boxplot() +\n  labs(title = \"Battery Life by Temperature\", x = \"Temperature (°C)\", y = \"Life (hours)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nDistribution of Battery Life by Material and Temperature.\n\n\n\n\n\n\n\nDistribution of Battery Life by Material and Temperature.\n\n\n\n\n\n\n\nInteraction Plot\nThe most crucial plot for a factorial experiment is the interaction plot. It displays the mean battery life for each combination of material and temperature. If the lines are parallel, it suggests there is no interaction. If the lines are not parallel (i.e., they cross or diverge), it indicates that the effect of temperature on battery life is different for each material type, signaling a likely interaction.\n\n\nCode\nggplot(battery_df, aes(x = temperature, y = life, group = material, color = material)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  labs(\n    title = \"Interaction Plot: Material Type and Temperature\",\n    x = \"Temperature (°C)\",\n    y = \"Average Battery Life (hours)\",\n    color = \"Material Type\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nInteraction between Material Type and Temperature.\n\n\n\n\nThe non-parallel lines in the plot strongly suggest that a significant interaction effect is present. Specifically, the performance of Material 3 drops less dramatically with increasing temperature compared to Materials 1 and 2.\n\n\n\nModel Fitting and Analysis of Variance (ANOVA)\nWe now fit a linear model to formally test the significance of the main effects and the interaction term. The model life ~ material * temperature is shorthand for life ~ material + temperature + material:temperature. We use a sum-to-zero contrast (contr.sum) for balanced interpretation of the effects. The ANOVA table will tell us if the variation caused by our factors is statistically significant compared to the random variation in the data.\n\n\nCode\n# Fit the full factorial model\nbattery_fit &lt;- lm(life ~ material * temperature, \n                  data = battery_df,\n                  contrasts = list(material = contr.sum, temperature = contr.sum))\n\nsummary(battery_fit)\n\n\n\nCall:\nlm(formula = life ~ material * temperature, data = battery_df, \n    contrasts = list(material = contr.sum, temperature = contr.sum))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.750 -14.625   1.375  17.938  45.250 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             105.528      4.331  24.367  &lt; 2e-16 ***\nmaterial1               -22.361      6.125  -3.651  0.00111 ** \nmaterial2                 2.806      6.125   0.458  0.65057    \ntemperature1             39.306      6.125   6.418  7.1e-07 ***\ntemperature2              2.056      6.125   0.336  0.73975    \nmaterial1:temperature1   12.278      8.662   1.417  0.16778    \nmaterial2:temperature1    8.111      8.662   0.936  0.35735    \nmaterial1:temperature2  -27.972      8.662  -3.229  0.00325 ** \nmaterial2:temperature2    9.361      8.662   1.081  0.28936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.98 on 27 degrees of freedom\nMultiple R-squared:  0.7652,    Adjusted R-squared:  0.6956 \nF-statistic:    11 on 8 and 27 DF,  p-value: 9.426e-07\n\n\nCode\n# Generate the ANOVA table\nanova(battery_fit)\n\n\n\n  \n\n\n\nThe ANOVA table shows very small p-values (Pr(&gt;F)) for material, temperature, and, most importantly, the material:temperature interaction. This confirms our visual inspection: all effects are statistically significant. Because the interaction is significant, our interpretation should focus on the interaction itself rather than the main effects in isolation.\n\n\nModel Adequacy Checks\nThe validity of our ANOVA results depends on the model’s residuals meeting certain assumptions (normality, constant variance, independence). We check these with diagnostic plots.\n\nCode\n# Extract standardized residuals and fitted values\nbattery_fit_diag &lt;- data.frame(\n  residuals = rstandard(battery_fit),\n  fitted = fitted.values(battery_fit)\n)\n\n# Normal Q-Q Plot\np1 &lt;- ggplot(battery_fit_diag, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"Normal Q-Q Plot\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n# Residuals vs. Fitted Plot\np2 &lt;- ggplot(battery_fit_diag, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\", x = \"Fitted Values\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\np1 \np2\n\n\n\n\n\n\n\nDiagnostic plots for the battery life model.\n\n\n\n\n\n\n\nDiagnostic plots for the battery life model.\n\n\n\n\n\nThe Normal Q-Q plot shows the points falling roughly along the line, suggesting the normality assumption is met. The Residuals vs. Fitted plot shows a random scatter of points around the zero line, indicating that the variance is reasonably constant. The model assumptions appear to be satisfied.\n\n\nPost-Hoc Analysis: Pairwise Comparisons\nSince the interaction is significant, we must compare the means of the nine specific treatment combinations (3 materials × 3 temperatures). Simply comparing the average effect of Material 1 vs. Material 2 would be misleading, as that difference depends on the temperature.\n\nTukey’s HSD Test\nTukey’s Honest Significant Difference (HSD) test is a post-hoc test that compares all possible pairs of means while controlling the family-wise error rate. We apply it to an aov model object. The output for the material:temperature interaction shows which specific combinations are significantly different from one another.\n\n\nCode\n# Fit the model using aov() for Tukey's test\nbattery_aov &lt;- aov(life ~ material * temperature, data = battery_df)\n\n# Perform Tukey's HSD test\nTukeyHSD(battery_aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = life ~ material * temperature, data = battery_df)\n\n$material\n        diff       lwr      upr     p adj\n2-1 25.16667 -1.135677 51.46901 0.0627571\n3-1 41.91667 15.614323 68.21901 0.0014162\n3-2 16.75000 -9.552344 43.05234 0.2717815\n\n$temperature\n            diff        lwr       upr     p adj\n70-15  -37.25000  -63.55234 -10.94766 0.0043788\n125-15 -80.66667 -106.96901 -54.36432 0.0000001\n125-70 -43.41667  -69.71901 -17.11432 0.0009787\n\n$`material:temperature`\n               diff         lwr        upr     p adj\n2:15-1:15     21.00  -40.823184  82.823184 0.9616404\n3:15-1:15      9.25  -52.573184  71.073184 0.9998527\n1:70-1:15    -77.50 -139.323184 -15.676816 0.0065212\n2:70-1:15    -15.00  -76.823184  46.823184 0.9953182\n3:70-1:15     11.00  -50.823184  72.823184 0.9994703\n1:125-1:15   -77.25 -139.073184 -15.426816 0.0067471\n2:125-1:15   -85.25 -147.073184 -23.426816 0.0022351\n3:125-1:15   -49.25 -111.073184  12.573184 0.2016535\n3:15-2:15    -11.75  -73.573184  50.073184 0.9991463\n1:70-2:15    -98.50 -160.323184 -36.676816 0.0003449\n2:70-2:15    -36.00  -97.823184  25.823184 0.5819453\n3:70-2:15    -10.00  -71.823184  51.823184 0.9997369\n1:125-2:15   -98.25 -160.073184 -36.426816 0.0003574\n2:125-2:15  -106.25 -168.073184 -44.426816 0.0001152\n3:125-2:15   -70.25 -132.073184  -8.426816 0.0172076\n1:70-3:15    -86.75 -148.573184 -24.926816 0.0018119\n2:70-3:15    -24.25  -86.073184  37.573184 0.9165175\n3:70-3:15      1.75  -60.073184  63.573184 1.0000000\n1:125-3:15   -86.50 -148.323184 -24.676816 0.0018765\n2:125-3:15   -94.50 -156.323184 -32.676816 0.0006078\n3:125-3:15   -58.50 -120.323184   3.323184 0.0742711\n2:70-1:70     62.50    0.676816 124.323184 0.0460388\n3:70-1:70     88.50   26.676816 150.323184 0.0014173\n1:125-1:70     0.25  -61.573184  62.073184 1.0000000\n2:125-1:70    -7.75  -69.573184  54.073184 0.9999614\n3:125-1:70    28.25  -33.573184  90.073184 0.8281938\n3:70-2:70     26.00  -35.823184  87.823184 0.8822881\n1:125-2:70   -62.25 -124.073184  -0.426816 0.0474675\n2:125-2:70   -70.25 -132.073184  -8.426816 0.0172076\n3:125-2:70   -34.25  -96.073184  27.573184 0.6420441\n1:125-3:70   -88.25 -150.073184 -26.426816 0.0014679\n2:125-3:70   -96.25 -158.073184 -34.426816 0.0004744\n3:125-3:70   -60.25 -122.073184   1.573184 0.0604247\n2:125-1:125   -8.00  -69.823184  53.823184 0.9999508\n3:125-1:125   28.00  -33.823184  89.823184 0.8347331\n3:125-2:125   36.00  -25.823184  97.823184 0.5819453\n\n\n\n\nFisher’s LSD Method\nThe Fisher’s Least Significant Difference (LSD) method is another option for pairwise comparisons. To test the interaction means, we must specify both factors in the trt argument.\n\n\nCode\nlibrary(agricolae)\n\n# Perform LSD test on the interaction term\nlsd_results &lt;- LSD.test(battery_aov, trt = c(\"material\", \"temperature\"),\n                        p.adj = \"none\", group = FALSE)\n\n# Print the comparison table\nprint(lsd_results$comparison)\n\n\n              difference pvalue signif.         LCL        UCL\n1:125 - 1:15      -77.25 0.0003     *** -114.950479 -39.549521\n1:125 - 1:70        0.25 0.9892          -37.450479  37.950479\n1:125 - 2:125       8.00 0.6667          -29.700479  45.700479\n1:125 - 2:15      -98.25 0.0000     *** -135.950479 -60.549521\n1:125 - 2:70      -62.25 0.0022      **  -99.950479 -24.549521\n1:125 - 3:125     -28.00 0.1392          -65.700479   9.700479\n1:125 - 3:15      -86.50 0.0001     *** -124.200479 -48.799521\n1:125 - 3:70      -88.25 0.0001     *** -125.950479 -50.549521\n1:15 - 1:70        77.50 0.0002     ***   39.799521 115.200479\n1:15 - 2:125       85.25 0.0001     ***   47.549521 122.950479\n1:15 - 2:15       -21.00 0.2631          -58.700479  16.700479\n1:15 - 2:70        15.00 0.4214          -22.700479  52.700479\n1:15 - 3:125       49.25 0.0124       *   11.549521  86.950479\n1:15 - 3:15        -9.25 0.6187          -46.950479  28.450479\n1:15 - 3:70       -11.00 0.5544          -48.700479  26.700479\n1:70 - 2:125        7.75 0.6765          -29.950479  45.450479\n1:70 - 2:15       -98.50 0.0000     *** -136.200479 -60.799521\n1:70 - 2:70       -62.50 0.0021      ** -100.200479 -24.799521\n1:70 - 3:125      -28.25 0.1358          -65.950479   9.450479\n1:70 - 3:15       -86.75 0.0001     *** -124.450479 -49.049521\n1:70 - 3:70       -88.50 0.0000     *** -126.200479 -50.799521\n2:125 - 2:15     -106.25 0.0000     *** -143.950479 -68.549521\n2:125 - 2:70      -70.25 0.0007     *** -107.950479 -32.549521\n2:125 - 3:125     -36.00 0.0605       .  -73.700479   1.700479\n2:125 - 3:15      -94.50 0.0000     *** -132.200479 -56.799521\n2:125 - 3:70      -96.25 0.0000     *** -133.950479 -58.549521\n2:15 - 2:70        36.00 0.0605       .   -1.700479  73.700479\n2:15 - 3:125       70.25 0.0007     ***   32.549521 107.950479\n2:15 - 3:15        11.75 0.5279          -25.950479  49.450479\n2:15 - 3:70        10.00 0.5907          -27.700479  47.700479\n2:70 - 3:125       34.25 0.0732       .   -3.450479  71.950479\n2:70 - 3:15       -24.25 0.1980          -61.950479  13.450479\n2:70 - 3:70       -26.00 0.1685          -63.700479  11.700479\n3:125 - 3:15      -58.50 0.0036      **  -96.200479 -20.799521\n3:125 - 3:70      -60.25 0.0029      **  -97.950479 -22.549521\n3:15 - 3:70        -1.75 0.9248          -39.450479  35.950479\n\n\nThe results from both Tukey’s HSD and Fisher’s LSD provide detailed p-values for comparing pairs of treatment combinations, allowing us to make specific conclusions, such as “at 125°C, Material 3 has a significantly longer life than Materials 1 and 2.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of a Two-Factor Factorial Design</span>"
    ]
  }
]