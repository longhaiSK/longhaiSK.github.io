[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods for Research",
    "section": "",
    "text": "Welcome\nThis book contains lecture notes for STAT 845: Statistical Methods for Research at the University of Saskatchewan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Methods for Research</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Statistical Methods for Research",
    "section": "",
    "text": "Table of Contents",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Methods for Research</span>"
    ]
  },
  {
    "objectID": "unit1-introR/introR.html",
    "href": "unit1-introR/introR.html",
    "title": "2  A Quick Introduction to using R for Data Analysis",
    "section": "",
    "text": "2.1 Basic R Objects and Operations\nCode\n## create a vector\nx &lt;- 1:10\nx &lt;- seq (30,3, by = -2)\na &lt;- c(66.32, 69.87, 70.12, 90.37, 50.08, 61.20, 65.00, 57.65)\nd &lt;- a [1]\na [1] &lt;- 85.34\n\nmean (a)\n\n\n[1] 68.70375\n\n\nCode\nma &lt;- mean (a)\n## read a vector of numbers from a file\nx &lt;- scan(\"numbers.txt\")\nx2 &lt;- scan(\"number2.txt\")\n\n## one can also read number withoug saving to a file\ny &lt;- scan(text = \"7  8  9 10 11 12 13 13 14 17 17 45\")\n\n## create a matrix\nA &lt;- matrix (0, 4, 2)\n\nA &lt;- matrix (1:8, 4,2)\n\nA\n\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nCode\nD &lt;- matrix (a, 4, 2, byrow=T)\n\nD &lt;- matrix(1:8, 2, 4)\nD\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\n\nCode\n## create another matrix with all entry 0\nB &lt;- matrix (1:5000, 100, 50)\n\n## assign a number to B\nB[2,4] &lt;- 45\nB[1,]\n\n\n [1]    1  101  201  301  401  501  601  701  801  901 1001 1101 1201 1301 1401\n[16] 1501 1601 1701 1801 1901 2001 2101 2201 2301 2401 2501 2601 2701 2801 2901\n[31] 3001 3101 3201 3301 3401 3501 3601 3701 3801 3901 4001 4101 4201 4301 4401\n[46] 4501 4601 4701 4801 4901\n\n\nCode\nB[,1]\n\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nCode\nB[1,] &lt;- 1:50\n\n\n## create a list\nE &lt;- list (newa = a, newA = A)\n## list the names of components\nnames (E)\n\n\n[1] \"newa\" \"newA\"\n\n\nCode\n## to look at the component of E\nE$newA \n\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nCode\nE$newa &lt;- 10:17\n\n## create a dataframe\nscores &lt;- c (30, 45, 50)\nnames &lt;- c(\"Peter\", \"John\", \"Alice\")\nstat245_scores &lt;- data.frame (names, scores)\nstat245_scores\n\n\n\n  \n\n\n\nCode\nstat245_scores$names\n\n\n[1] \"Peter\" \"John\"  \"Alice\"\n\n\nCode\nstat245_scores$scores [1] &lt;- 40\nstat245_scores\n\n\n\n  \n\n\n\nCode\nstat245_scores$perc &lt;- stat245_scores$scores/50 * 100\nstat245_scores\n\n\n\n  \n\n\n\nCode\nstat245_scores$adj &lt;- stat245_scores$perc + 10\nstat245_scores\n\n\n\n  \n\n\n\nCode\n###############################################################################",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Quick Introduction to using R for Data Analysis</span>"
    ]
  },
  {
    "objectID": "unit1-introR/introR.html#import-a-dataset-into-r-environment-and-simple-operation",
    "href": "unit1-introR/introR.html#import-a-dataset-into-r-environment-and-simple-operation",
    "title": "2  A Quick Introduction to using R for Data Analysis",
    "section": "2.2 Import a dataset into R environment and Simple Operation",
    "text": "2.2 Import a dataset into R environment and Simple Operation\n\n\nCode\n###############################################################################\n\n## import myagpop.csv into an R data frame called 'myagpop'\nagpop &lt;- read.csv(\"agpop.csv\")\n\n## Now, we can use the data:\n\n## preview agpop\nhead (agpop)\n\n\n\n  \n\n\n\nCode\n## look at the variable name\ncolnames (agpop) \n\n\n [1] \"county\"   \"state\"    \"acres92\"  \"acres87\"  \"acres82\"  \"farms92\" \n [7] \"farms87\"  \"farms82\"  \"largef92\" \"largef87\" \"largef82\" \"smallf92\"\n[13] \"smallf87\" \"smallf82\" \"region\"  \n\n\nCode\n## find number of cols\nncol (agpop) \n\n\n[1] 15\n\n\nCode\n## find number of rows\nnrow (agpop) \n\n\n[1] 3078\n\n\nCode\n## access a certain row \nagpop [2, ]\n\n\n\n  \n\n\n\nCode\n## access a certain column\nagpop [1:20, \"acres92\"] ## equivalent to \n\n\n [1] 683533  47146 141338    210  50810 107259 167832 177189  48022 137426\n[11] 144799  96427  73841 109555 121504  99466  67950  61426  68478  47200\n\n\nCode\nagpop$acres92[1:20]\n\n\n [1] 683533  47146 141338    210  50810 107259 167832 177189  48022 137426\n[11] 144799  96427  73841 109555 121504  99466  67950  61426  68478  47200\n\n\nCode\nagpop$largef92[1:20]\n\n\n [1] 14  9 25  0  9 25 24 40  6  9 29 18  4 22 24  8  9 13  4  5\n\n\nCode\n## find mean of acres92\nmean (agpop $acres92)\n\n\n[1] 306677\n\n\nCode\n## find sd of acres92\nsd (agpop $acres92)\n\n\n[1] 424686.7\n\n\nCode\nagpop_AK  &lt;- agpop [agpop$state == \"AK\", ]\n\nagpop_AK &lt;- subset (agpop, state == \"AK\")\n\nagpop_W &lt;- subset (agpop, region == \"W\")\n\nagpop_largefarm &lt;- subset (agpop, largef92 &gt; 10)\n\n\nhist (agpop$acres92)\n\n\n\n\n\n\n\n\n\nProduce Plots\n\n\nCode\n#pdf (\"hist_acres92.pdf\") ## use this command and dev.off to save the output to a file\nhist (agpop$acres92)\n\n\n\n\n\n\n\n\n\nCode\n#dev.off()\n\n#jpeg (\"agpop_acres_87v92.jpg\")\n\nplot (agpop$acres87, agpop$acres92)\nabline (a = 0, b = 1)\n\n\n\n\n\n\n\n\n\nCode\n#dev.off()## this is used to close the jpeg file",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Quick Introduction to using R for Data Analysis</span>"
    ]
  },
  {
    "objectID": "unit1-introR/introR.html#create-your-own-function",
    "href": "unit1-introR/introR.html#create-your-own-function",
    "title": "2  A Quick Introduction to using R for Data Analysis",
    "section": "2.3 Create your own function",
    "text": "2.3 Create your own function\n\n\nCode\n### data is a matrix or data.frame\nmeans_col &lt;- function (data)\n{\n    n &lt;- ncol (data)\n    cmeans &lt;- rep (NA, n)\n    for (j in 1:n)\n    {\n        cmeans[j] &lt;- mean (data[,j])\n        \n    }\n    cmeans\n}\n\n### apply function\nmeans_col (agpop[, 3:13])\n\n\n [1] 306676.97141 313016.37817 320193.69298    625.50357    678.28428\n [6]    728.06238     56.17674     54.86160     52.62248     54.09227\n[11]     59.53769\n\n\nCode\n### R built-in function\ncolMeans (agpop[, 3:13])\n\n\n     acres92      acres87      acres82      farms92      farms87      farms82 \n306676.97141 313016.37817 320193.69298    625.50357    678.28428    728.06238 \n    largef92     largef87     largef82     smallf92     smallf87 \n    56.17674     54.86160     52.62248     54.09227     59.53769",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Quick Introduction to using R for Data Analysis</span>"
    ]
  },
  {
    "objectID": "unit1-introR/introR.html#include-images-saved-in-an-external-file",
    "href": "unit1-introR/introR.html#include-images-saved-in-an-external-file",
    "title": "2  A Quick Introduction to using R for Data Analysis",
    "section": "2.4 Include Images Saved in An External File",
    "text": "2.4 Include Images Saved in An External File\nUsing the following R code to include your images saved in an external file.\n\n\nCode\nknitr::include_graphics(\"handwriting.png\")\n\n\n\n\n\n\n\n\n\nYou can hide the above R code by setting “echo=FALSE” for the r chunk. For example, I will include the image once again as follows:\n\n\n\n\n\n\n\n\nFigure 2.1: This is a figure inserted from the file called “handwriting.png”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A Quick Introduction to using R for Data Analysis</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html",
    "href": "unit2-slr/slr.html",
    "title": "3  Simple Linear Regression",
    "section": "",
    "text": "3.1 Overview of Simple Linear Regression\nTo make the simple linear regression model concrete, let’s first visualize a simulated dataset that follows \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\qquad\n\\varepsilon_i \\sim \\mathcal N(0, \\sigma^2).\n\\]\nHere, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\varepsilon_i\\) represents random noise.\nCode\nset.seed(2025)\nn &lt;- 40\nbeta0 &lt;- 2; beta1 &lt;- 1.5; sigma &lt;- 2\nx &lt;- runif(n, 0, 10)\ny &lt;- beta0 + beta1 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x, y)\n\nfit &lt;- lm(y ~ x, data = dat)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"Predictor X\", ylab = \"Response Y\",\n     main = \"Simulated Data with Fitted Linear Regression Line\")\nabline(fit, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Observed data\", \"Fitted line\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"steelblue\", \"red\"), bty = \"n\")\nThe scatterplot shows data points scattered around a line — the red line is the fitted regression model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#overview-of-simple-linear-regression",
    "href": "unit2-slr/slr.html#overview-of-simple-linear-regression",
    "title": "3  Simple Linear Regression",
    "section": "",
    "text": "3.1.1 Least Squares Estimation\nGoal: Find \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize \\[\n\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2.\n\\]\nSolutions: \\[\n\\hat\\beta_1 = \\frac{\\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2}\n= \\frac{S_{xy}}{S_{xx}},\n\\qquad\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\,\\bar x.\n\\]\nHere \\[\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y),\n\\qquad\nS_{xx} = \\sum_i (x_i - \\bar x)^2.\n\\]\nShortcut (computational) formulas: \\[\nS_{xy} = \\sum_i x_i y_i - n\\,\\bar x\\,\\bar y,\n\\qquad\nS_{xx} = \\sum_i x_i^2 - n\\,\\bar x^2.\n\\]\nInterpretation:\n- \\(\\hat\\beta_1\\) measures the estimated change in \\(Y\\) for each unit increase in \\(X\\).\n- \\(\\hat\\beta_0\\) represents the fitted value of \\(Y\\) when \\(X=0\\).\n\n\n\n3.1.2 Residual and Sum of Squares Definitions\nLet \\(\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i\\) and \\(e_i = y_i - \\hat y_i\\).\n\n\n\n\n\n\n\n\nSymbol\nDefinition\nComputing Formula (in terms of \\(S_{xx}, S_{xy}\\), etc.)\n\n\n\n\nSST\nTotal Sum of Squares\n\\(\\displaystyle \\sum_i (y_i - \\bar y)^2 = S_{yy} = \\sum_i y_i^2 - n\\,\\bar y^2\\)\n\n\nSSR\nRegression Sum of Squares\n\\(\\displaystyle \\sum_i (\\hat y_i - \\bar y)^2 = \\hat\\beta_1^2 S_{xx} = \\frac{S_{xy}^2}{S_{xx}}\\)\n\n\nSSE\nError (Residual) Sum of Squares\n\\(\\displaystyle \\sum_i (y_i - \\hat y_i)^2 = S_{yy} - \\frac{S_{xy}^2}{S_{xx}}\\)\n\n\n\nIdentity: \\[\n\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}.\n\\]\nHere, \\[\nS_{xx} = \\sum_i (x_i - \\bar x)^2 = \\sum_i x_i^2 - n\\bar x^2, \\qquad\nS_{yy} = \\sum_i (y_i - \\bar y)^2 = \\sum_i y_i^2 - n\\bar y^2, \\qquad\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y) = \\sum_i x_i y_i - n\\bar x \\bar y.\n\\]\n\n\n\n3.1.3 Coefficient of Determination (\\(R^2\\))\nMeasures the proportion of total variation in \\(Y\\) explained by \\(X\\): \\[\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}}\n= 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}.\n\\]\nInterpretation:\n\n\\(R^2 = 1\\) means perfect linear fit;\n\\(R^2 = 0\\) means the model explains none of the variation.\n\n\n\n\n3.1.4 F-test for Overall Significance\nTests whether \\(X\\) is linearly related to \\(Y\\).\nHypotheses: \\[\nH_0: \\beta_1 = 0\n\\quad \\text{vs.} \\quad\nH_A: \\beta_1 \\ne 0.\n\\]\nTest Statistic: \\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n= \\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\n\\sim F_{1,n-2}\\quad (H_0).\n\\]\np-value approach for observe \\(F^{\\mathrm{obs}}\\):\nGiven the observed statistic \\(F^{\\text{obs}}\\) with \\((1,\\,n-2)\\) df, \\[\np-\\text{value} \\;=\\; \\Pr\\!\\big(F_{1,\\,n-2} \\ge F^{\\text{obs}}\\big)\n\\;=\\; \\mathrm{pf}\\!\\big(F^{\\text{obs}},\\,1,\\,n-2,\\ \\text{lower.tail}= \\mathrm{FALSE}\\big).\n\\]\n\n\nCode\n## -- Inputs (provide these from your analysis context) -------------------------\n## n   &lt;- ...   # sample size\n## SSR &lt;- ...   # regression sum of squares\n## SSE &lt;- ...   # error sum of squares\nn   &lt;- 20\nSSR &lt;- 5\nSSE &lt;- 40\n\n\n\n\ndf1  &lt;- 1\ndf2  &lt;- n - 2\nFobs &lt;- (SSR/df1) / (SSE/df2)         # observed F\npval &lt;- pf(Fobs, df1 = df1, df2 = df2, lower.tail = FALSE)\npval\n\n\n[1] 0.1509505\n\n\nCode\n## -- Plot F density and shade the p-value tail (with proper annotations) -------\nxmax &lt;- max(qf(0.995, df1, df2), Fobs * 1.2)  # extra space for labels\npeak &lt;- max(df(seq(0, xmax, length.out = 500), df1, df2))\n\n## Density curve\ncurve(df(x, df1, df2), from = 0, to = xmax,\n      xlab = \"F\", ylab = \"Density\",\n      main = sprintf(\"F(%d, %d) density  |  observed F = %.3f\", df1, df2, Fobs))\n\n## Shade right tail (p-value region)\nxs &lt;- seq(Fobs, xmax, length.out = 300)\nys &lt;- df(xs, df1, df2)\npolygon(c(Fobs, xs, xmax), c(0, ys, 0),\n        col = rgb(0, 0, 0, 0.18), border = NA)\n\n## Vertical line at Fobs (optional visual aid)\nabline(v = Fobs, lwd = 2)\n\n## ---- Annotation for F^obs pointing to the x-axis value (Fobs, 0) -------------\nx_txt_F &lt;- Fobs + 0.06 * xmax\ny_txt_F &lt;- 0.45 * peak\narrows(x0 = x_txt_F, y0 = y_txt_F, x1 = Fobs, y1 = 0,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_F, y_txt_F,\n     labels = bquote(F^{obs} == .(format(Fobs, digits = 3))),\n     pos = 4)\n\n## ---- Annotation for p-value pointing into the shaded tail --------------------\nx_tip_p &lt;- (Fobs + xmax) / 1.7\ny_tip_p &lt;- df(x_tip_p, df1, df2)\nx_txt_p &lt;- Fobs + 0.08 * xmax\ny_txt_p &lt;- 0.80 * peak\narrows(x0 = x_txt_p, y0 = y_txt_p, x1 = x_tip_p, y1 = y_tip_p,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_p, y_txt_p,\n     labels = bquote(p == .(format(pval, digits = 4, scientific = TRUE))),\n     pos = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.5 t-test for the Slope \\(\\beta_1\\)\nEquivalent to the \\(F\\)-test in simple regression since \\(t^2 = F\\).\nFormula: \\[\nt = \\frac{\\hat\\beta_1}{\\operatorname{SE}(\\hat\\beta_1)},\n\\qquad\n\\operatorname{SE}(\\hat\\beta_1) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_i (x_i - \\bar x)^2}},\n\\qquad\n\\hat\\sigma^2 = \\frac{\\mathrm{SSE}}{n-2}.\n\\]\nDistribution: \\[\nt \\sim t_{n-2}\\quad (H_0:\\beta_1=0).\n\\]\n\n\n\n3.1.6 Prediction for a New Case \\(x_0\\)\nPredicted mean response: \\[\n\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0.\n\\]\n95% Confidence interval for mean response: \\[\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n\\]\n95% Prediction interval for a new observation: \\[\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n\\]\n\nSummary Cheat Sheet\n\n\n\n\n\n\n\nConcept\nKey Formula\n\n\n\n\nModel\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\)\n\n\nLS Estimates\n\\(\\hat\\beta_1 = S_{xy}/S_{xx}\\), \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\)\n\n\nDecomposition\n\\(\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}\\)\n\n\n\\(R^2\\)\n\\(R^2 = 1 - \\mathrm{SSE}/\\mathrm{SST}\\)\n\n\n\\(F\\)-test\n\\(F = (\\mathrm{SSR}/1)/(\\mathrm{SSE}/(n-2))\\)\n\n\n\\(t\\)-test\n\\(t = \\hat\\beta_1 / \\operatorname{SE}(\\hat\\beta_1)\\)\n\n\nPrediction\n\\(\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#example-1-vehicle-insurance-premium-warm-up",
    "href": "unit2-slr/slr.html#example-1-vehicle-insurance-premium-warm-up",
    "title": "3  Simple Linear Regression",
    "section": "3.2 Example 1: Vehicle Insurance Premium (warm-up)",
    "text": "3.2 Example 1: Vehicle Insurance Premium (warm-up)\nWe examine premiums \\(y\\) vs. driving amount \\(x\\). The scatterplot hints at a downward trend.\n\n3.2.1 Input data\n\n\nCode\nissu &lt;- data.frame(\n  driving = c(5, 2, 12, 9, 15, 6, 25, 16),\n  premium = c(64, 87, 50, 71, 44, 56, 42, 60)\n)\n\ny &lt;- issu$premium\nx &lt;- issu$driving\nxbar &lt;- mean(x); ybar &lt;- mean(y); n &lt;- length(y)\n\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Vehicle Insurance: Premium vs. Driving\")\nabline(h = ybar, lty = 3)\n\n\n\n\n\n\n\n\n\nNarrative. The horizontal line at \\(\\bar y\\) represents the intercept-only model. Any fitted line that tilts away from this must earn its keep by reducing residual variation enough to offset the loss of one degree of freedom.\n\n\n3.2.2 Estimating regression coefficients\n\n\nCode\nfit.issu &lt;- lm(y ~ x)\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Fitted Simple Linear Regression\")\nabline(fit.issu, lwd = 2)\n\n\n\n\n\n\n\n\n\nThe slope estimate \\(\\hat\\beta_1\\) captures the marginal change in premium per unit of driving (units of \\(y\\) per unit of \\(x\\)). Inference on \\(\\beta_1\\) tells us whether the pattern rises above noise.\n\n\n3.2.3 Residuals and fitted values (geometry picture)\nLet \\(\\hat y_i=\\hat\\beta_0+\\hat\\beta_1 x_i\\) and \\(\\tilde y_i=\\bar y\\). Residuals are \\(e_i=y_i-\\hat y_i\\) (model) and \\(y_i-\\bar y\\) (null). Visualizing all three clarifies the ANOVA identity.\n\n\nCode\nbeta0 &lt;- coef(fit.issu)[1]\nbeta1 &lt;- coef(fit.issu)[2]\nfitted1 &lt;- beta0 + beta1 * x\nfitted0 &lt;- rep(ybar, n)\nresidual1 &lt;- y - fitted1\nresidual0 &lt;- y - fitted0\n\ndata.frame(y, fitted0, residual0, fitted1, residual1,\n           diff.fitted = fitted1 - fitted0)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.4 SST, SSR, SSE and their meanings\n\n\\(\\text{SST}=\\sum (y_i-\\bar y)^2\\) quantifies total variability around the mean.\n\\(\\text{SSR}=\\sum (\\hat y_i-\\bar y)^2\\) is the part explained by \\(x\\).\n\\(\\text{SSE}=\\sum (y_i-\\hat y_i)^2\\) is the leftover (unexplained) variability.\n\n\n\nCode\nSST &lt;- sum((y - fitted0)^2); SST\n\n\n[1] 1557.5\n\n\nCode\nSSE &lt;- sum((y - fitted1)^2); SSE\n\n\n[1] 639.0065\n\n\nCode\nSSR &lt;- SST - SSE; SSR\n\n\n[1] 918.4935\n\n\nDirect check: \\(\\text{SSR}=\\sum(\\hat y_i-\\bar y)^2\\).\n\n\nCode\nsum((fitted1 - fitted0)^2)\n\n\n[1] 918.4935\n\n\n\n\n3.2.5 Visual ANOVA on an RSS plot\nWe place the residual sum of squares against model dimension to show the trade-off between fit and df.\n\n\nCode\n## Recompute cleanly\nSST &lt;- sum((y - mean(y))^2)\nSSE &lt;- sum(resid(fit.issu)^2)\nSSR &lt;- SST - SSE\ndf_SSR &lt;- 1\ndf_SSE &lt;- n - 2\n\npar(mar = c(6, 4, 4, 2) + 0.1)\nplot(c(1, 2, n), c(SST, SSE, 0), type = \"b\", pch = 19,\n     xlab = \"Number of Parameters in Model\",\n     ylab = \"Residual Sum of Squares (RSS)\",\n     main = \"ANOVA Geometry on RSS vs. Model Size\",\n     xlim = c(0, 14), ylim = c(-400, SST * 1.1), xaxt = \"n\")\naxis(1, at = c(1, 2, n), labels = c(\"1 (Intercept)\", \"2 (+Slope)\", paste(n, \"(Saturated)\")))\nabline(h = seq(0, 2000, by = 100), lty = 3, col = \"grey\")\n\npar(xpd = TRUE)\narrows(9, 0, 9, SSE, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, SSE/2, \"SSE\", col = \"blue\", pos = 4, font = 2, cex = 1.2)\n\narrows(9, SSE, 9, SST, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, (SST + SSE)/2, \"SSR\", col = \"red\", pos = 4, font = 2, cex = 1.2)\n\narrows(2, -200, n, -200, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext((2 + n)/2, -250, paste(\"df_SSE =\", df_SSE), col = \"blue\", font = 2)\n\narrows(1, -200, 2, -200, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(1.5, -250, paste(\"df_SSR =\", df_SSR), col = \"red\", font = 2)\npar(xpd = FALSE)\n\nf_value &lt;- (SSR/df_SSR) / (SSE/df_SSE)\np_value &lt;- pf(f_value, df1 = df_SSR, df2 = df_SSE, lower.tail = FALSE)\nlegend(\"topright\",\n       legend = c(sprintf(\"F-statistic: %.2f\", f_value),\n                  sprintf(\"p-value: %.3f\", p_value)),\n       title = \"ANOVA Results\", bty = \"o\", cex = 0.9)\n\n\n\n\n\n\n\n\n\n\n\n3.2.6 \\(R^2\\), \\(F\\) and a compact ANOVA table\n\n\nCode\nR2 &lt;- SSR / SST; R2\n\n\n[1] 0.5897229\n\n\nCode\nf  &lt;- (SSR/1) / (SSE/(n-2)); f\n\n\n[1] 8.624264\n\n\nCode\npvf &lt;- pf(f, df1 = 1, df2 = n-2, lower.tail = FALSE); pvf\n\n\n[1] 0.0260588\n\n\nCode\nFtable &lt;- data.frame(\n  Source = c(\"Regression\", \"Error\"),\n  df     = c(1, n - 2),\n  SS     = c(SSR, SSE),\n  MS     = c(SSR/1, SSE/(n-2)),\n  F      = c(f, NA),\n  pvalue = c(pvf, NA),\n  R2part = c(SSR, SSE) / SST\n)\nFtable\n\n\n\n  \n\n\n\nA call to anova() reproduces the same test:\n\n\nCode\nanova(fit.issu)\n\n\n\n  \n\n\n\n\n\n3.2.7 Sampling distributions via animation\nUnder \\(H_0:\\beta_1=0\\), \\(F\\) follows \\(F_{1,n-2}\\). Under \\(H_A\\), the distribution shifts right (noncentral \\(F\\)).\n\n3.2.7.1 Null world (\\(H_0\\) true)\n```{r}\n#| label: fig-simulation-animation\n#| context: html\n#| fig.show: \"animate\"\n#| fig.height: 4\n#| cache: false\n#| class.source: \"fold-hide\"\n\n## --- This code only runs for HTML output ---\noriginal_fit &lt;- lm(y ~ x)\nintercept_H0 &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\n\nfor (iter in 1:50) {\n  sim.y &lt;- intercept_H0 + rnorm(n, 0, model_sigma)\n\n  par(mfrow = c(1,2))\n  fit.sim.y &lt;- lm(sim.y ~ x)\n  plot(sim.y ~ x, ylim = c(0,100), main = \"Data under H0 (slope = 0)\")\n  abline(fit.sim.y, col = \"red\", lwd = 2)\n  abline(h = mean(sim.y), col = \"blue\")\n\n  a &lt;- anova(fit.sim.y)\n  ss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\n  num.par &lt;- cumsum(c(1, a$Df))\n  plot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\n  abline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\n  f_value &lt;- a$\"F value\"[1]\n  p_value &lt;- a$\"Pr(&gt;F)\"[1]\n  legend(\"topright\",\n         legend = c(sprintf(\"F: %.2f\", f_value),\n                    sprintf(\"p: %.3f\", p_value)),\n         title = \"ANOVA\", bty = \"o\", cex = 0.9)\n}\n```\n\n\nCode\n## --- This code only runs for PDF output ---\noriginal_fit &lt;- lm(y ~ x)\nintercept_H0 &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\n\n## We run the simulation just once for the static plot\nsim.y &lt;- intercept_H0 + rnorm(n, 0, model_sigma)\n\npar(mfrow = c(1,2))\nfit.sim.y &lt;- lm(sim.y ~ x)\nplot(sim.y ~ x, ylim = c(0,100), main = \"Data under H0 (slope = 0)\")\nabline(fit.sim.y, col = \"red\", lwd = 2)\nabline(h = mean(sim.y), col = \"blue\")\n\na &lt;- anova(fit.sim.y)\nss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\nnum.par &lt;- cumsum(c(1, a$Df))\nplot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\nabline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\nf_value &lt;- a$\"F value\"[1]\np_value &lt;- a$\"Pr(&gt;F)\"[1]\nlegend(\"topright\",\n       legend = c(sprintf(\"F: %.2f\", f_value),\n                  sprintf(\"p: %.3f\", p_value)),\n       title = \"ANOVA\", bty = \"o\", cex = 0.9)\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n\n\n3.2.7.2 Alternative world (\\(H_1\\) true)\n```{r}\n#| label: fig-simulation-animation-H1-html\n#| context: html\n#| fig.show: \"animate\"\n#| fig.height: 4\n#| cache: false\n#| class.source: \"fold-hide\"\n\noriginal_fit &lt;- lm(y ~ x)\nintercept_HA &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\nslope_HA     &lt;- -2  # pick a non-zero slope\n\nfor (iter in 1:50) {\n  mean_y &lt;- intercept_HA + slope_HA * x\n  sim.y  &lt;- mean_y + rnorm(n, 0, model_sigma)\n\n  par(mfrow = c(1,2))\n  fit.sim.y &lt;- lm(sim.y ~ x)\n  plot(sim.y ~ x, ylim = c(0,80), main = \"Data under HA (slope = -2)\")\n  abline(fit.sim.y, col = \"red\", lwd = 2)\n  abline(h = mean(sim.y), col = \"blue\")\n\n  a &lt;- anova(fit.sim.y)\n  ss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\n  num.par &lt;- cumsum(c(1, a$Df))\n  plot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\n  abline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\n  f_value &lt;- a$\"F value\"[1]\n  p_value &lt;- a$\"Pr(&gt;F)\"[1]\n  legend(\"topright\",\n         legend = c(sprintf(\"F: %.2f\", f_value),\n                    sprintf(\"p: %.3e\", p_value)),\n         title = \"ANOVA\", bty = \"o\", cex = 0.9)\n}\n```\n\n\nCode\noriginal_fit &lt;- lm(y ~ x)\nintercept_HA &lt;- coef(original_fit)[1]\nmodel_sigma  &lt;- sigma(original_fit)\nslope_HA     &lt;- -2  # pick a non-zero slope\n\nfor (iter in 1:1) {\n  mean_y &lt;- intercept_HA + slope_HA * x\n  sim.y  &lt;- mean_y + rnorm(n, 0, model_sigma)\n\n  par(mfrow = c(1,2))\n  fit.sim.y &lt;- lm(sim.y ~ x)\n  plot(sim.y ~ x, ylim = c(0,80), main = \"Data under HA (slope = -2)\")\n  abline(fit.sim.y, col = \"red\", lwd = 2)\n  abline(h = mean(sim.y), col = \"blue\")\n\n  a &lt;- anova(fit.sim.y)\n  ss &lt;- a$`Sum Sq`; rss &lt;- rev(cumsum(c(0, rev(ss))))\n  num.par &lt;- cumsum(c(1, a$Df))\n  plot(rss ~ num.par, xlab = \"Number of Parameters\", ylab = \"RSS\", type = \"b\")\n  abline(v = 1:25, h = (0:50)*100, lty = 3, col = \"grey\")\n\n  f_value &lt;- a$\"F value\"[1]\n  p_value &lt;- a$\"Pr(&gt;F)\"[1]\n  legend(\"topright\",\n         legend = c(sprintf(\"F: %.2f\", f_value),\n                    sprintf(\"p: %.3e\", p_value)),\n         title = \"ANOVA\", bty = \"o\", cex = 0.9)\n}\n\n\n\n\n\n\n\n\nFigure 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#example-2-oxygen-purity-data",
    "href": "unit2-slr/slr.html#example-2-oxygen-purity-data",
    "title": "3  Simple Linear Regression",
    "section": "3.3 Example 2: Oxygen Purity Data",
    "text": "3.3 Example 2: Oxygen Purity Data\nWe model oxygen purity \\(y\\) as a function of hydrocarbon level \\(x\\) and report both mean response and prediction uncertainty.\n\n3.3.1 Data\n\n\nCode\nx &lt;- c(0.99, 1.02, 1.15, 1.29, 1.46, 1.36, 0.87, 1.23, 1.55, 1.40, 1.19,\n       1.15, 0.98, 1.01, 1.11, 1.20, 1.26, 1.32, 1.43, 0.95)\ny &lt;- c(90.01, 89.05, 91.43, 93.74, 96.73, 94.45, 87.59, 91.77, 99.42, 93.65,\n       93.54, 92.52, 90.56, 89.54, 89.85, 90.39, 93.25, 93.41, 94.98, 87.33)\nn &lt;- length(x); n\n\n\n[1] 20\n\n\nCode\npurity.data &lt;- data.frame(x = x, y = y)\nhead(purity.data)\n\n\n\n  \n\n\n\n\n\n3.3.2 Fit and quick summary\n\n\nCode\nfit &lt;- lm(y ~ x, data = purity.data)\nsummary(fit)\n\n\n\nCall:\nlm(formula = y ~ x, data = purity.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83029 -0.73334  0.04497  0.69969  1.96809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   74.283      1.593   46.62  &lt; 2e-16 ***\nx             14.947      1.317   11.35 1.23e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.087 on 18 degrees of freedom\nMultiple R-squared:  0.8774,    Adjusted R-squared:  0.8706 \nF-statistic: 128.9 on 1 and 18 DF,  p-value: 1.227e-09\n\n\nInterpretation. The slope’s sign gives the direction of association; its \\(t\\) test (or \\(F\\) with 1 df) assesses evidence for a trend. Look at \\(\\hat\\sigma\\) for noise scale and \\(R^2\\) for variance explained.\n\n\n3.3.3 Scatter with fitted line\n\n\nCode\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Oxygen Purity vs Hydrocarbon Level\")\nabline(fit, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Coefficient CIs and ANOVA\n\n\nCode\nconfint(fit, level = 0.95)\n\n\n               2.5 %   97.5 %\n(Intercept) 70.93555 77.63108\nx           12.18107 17.71389\n\n\nCode\nanova(fit)\n\n\n\n  \n\n\n\n\n\n3.3.5 Mean-response and prediction bands\nThe mean-response CI narrows near \\(\\bar x\\) and widens at the extremes; the prediction band is wider by the irreducible noise term.\n\n\nCode\nx0 &lt;- seq(min(purity.data$x), max(purity.data$x), length = 50)\nnewdata &lt;- data.frame(x = x0)\n\nest.mean &lt;- predict(fit, newdata = newdata, interval = \"confidence\", level = 0.95)\npred.new &lt;- predict(fit, newdata = newdata, interval = \"prediction\", level = 0.95)\n\n\n\n\nCode\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Regression Line with Confidence and Prediction Bands\")\nabline(fit)\nmatlines(x0, est.mean[, 2:3], col = \"blue\", lty = 2, lwd = 2)\nmatlines(x0, pred.new[, 2:3], col = \"red\",  lty = 3, lwd = 2)\nlegend(\"topleft\", c(\"Confidence Bands (mean)\", \"Prediction Bands (new y)\"),\n       col = c(\"blue\", \"red\"), lty = 2:3, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.6 Residual diagnostics (assumptions check)\nWe look for no pattern in residuals vs. fits and approximate straightness in the Q–Q plot.\n\n\nCode\npred &lt;- fitted.values(fit)\ne &lt;- resid(fit)\nd &lt;- e / summary(fit)$sigma\n\npar(mfrow = c(2,2))\nplot(purity.data$x, purity.data$y, xlab = \"x\", ylab = \"y\"); abline(fit)\nqqnorm(d, main = \"Normal Q–Q\"); qqline(d)\nplot(pred, d, xlab = \"Fitted\", ylab = \"Std. residuals\", main = \"Residuals vs Fits\"); abline(h = 0, lty = 2)\nplot(1:n, d, xlab = \"Order\", ylab = \"Std. residuals\", main = \"Residuals vs Order\"); abline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#correlation-analysis-for-comparison-not-causation",
    "href": "unit2-slr/slr.html#correlation-analysis-for-comparison-not-causation",
    "title": "3  Simple Linear Regression",
    "section": "3.4 Correlation analysis (for comparison, not causation)",
    "text": "3.4 Correlation analysis (for comparison, not causation)\nCorrelation summarizes linear association without fitting a line or making model assumptions.\n\n3.4.1 Data and scatter\n\n\nCode\nstrength &lt;- c(9.95,24.45,31.75,35.00,25.02,16.86,14.38,9.60,24.35,\n              27.50,17.08,37.00,41.95,11.66,21.65,17.89,69.00,10.30,\n              34.93,46.59,44.88,54.12,56.63,22.13,21.15)\nlength &lt;- c(2,8,11,10,8,4,2,2,9,8,4,11,12,2,4,4,20,1,10,\n            15,15,16,17,6,5)\nplot(length, strength, xlab = \"Length\", ylab = \"Strength\",\n     main = \"Strength vs Length (scatter)\")\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Pearson correlation and test\n\n\nCode\ncor(strength, length)\n\n\n[1] 0.9818118\n\n\nCode\ncor.test(strength, length)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  strength and length\nt = 24.801, df = 23, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9585414 0.9920735\nsample estimates:\n      cor \n0.9818118 \n\n\nNote. A large \\(|r|\\) and small \\(p\\) indicate linear association; regression further quantifies the slope and supports prediction, with diagnostics to check assumptions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit2-slr/slr.html#what-to-report-checklist",
    "href": "unit2-slr/slr.html#what-to-report-checklist",
    "title": "3  Simple Linear Regression",
    "section": "3.5 What to report (checklist)",
    "text": "3.5 What to report (checklist)\n\nEstimated line \\(\\hat y=\\hat\\beta_0+\\hat\\beta_1 x\\) with units.\n\\(t\\)/\\(F\\) test for slope, \\(p\\)-value and CI for \\(\\beta_1\\).\n\\(R^2\\) and \\(\\hat\\sigma\\) (RMSE) for fit quality.\nMean-response and prediction intervals at substantively relevant \\(x_0\\).\nResidual diagnostics and any remedies (transformations, robust methods) if needed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html",
    "href": "unit3-mlr/mlr.html",
    "title": "4  Multiple Linear Regression",
    "section": "",
    "text": "4.1 An Example: Wire Bond Strength Dataset",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#an-example-wire-bond-strength-dataset",
    "href": "unit3-mlr/mlr.html#an-example-wire-bond-strength-dataset",
    "title": "4  Multiple Linear Regression",
    "section": "",
    "text": "4.1.1 Loading Data and Visualization\nNote: You must change the file paths in the read.csv() functions below to match the location of the files on your computer (for example C:\\\\Users\\\\&lt;YourUsername&gt;\\\\Documents on Windows).\n\n\nCode\n## Read data. Change the path as necessary.\n## Example: bond.data &lt;- read.csv(\"wire-bond.csv\")\nbond.data &lt;- read.csv(\"wire-bond.csv\")\n\n## This will now be automatically rendered as a paged table\nbond.data\n\n\n\n  \n\n\n\n2D Visualization\n\n\nCode\npar(mfrow = c(1, 3), mar = c(5, 4, 2, 1))\n\n## 1) length vs strength\ni1 &lt;- which(!is.na(bond.data$length) & !is.na(bond.data$strength))\nplot(bond.data$length[i1], bond.data$strength[i1],\n     xlab = \"Wire Length\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$length[i1], bond.data$strength[i1],\n     labels = i1, pos = 1, offset = 0.4, cex = 0.75)\n\n## 2) height vs strength\ni2 &lt;- which(!is.na(bond.data$height) & !is.na(bond.data$strength))\nplot(bond.data$height[i2], bond.data$strength[i2],\n     xlab = \"Die height\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$height[i2], bond.data$strength[i2],\n     labels = i2, pos = 1, offset = 0.4, cex = 0.75)\n\n## 3) height vs length\ni3 &lt;- which(!is.na(bond.data$height) & !is.na(bond.data$length))\nplot(bond.data$height[i3], bond.data$length[i3],\n     xlab = \"Die height\", ylab = \"Length\", pch = 19)\ntext(bond.data$height[i3], bond.data$length[i3],\n     labels = i3, pos = 1, offset = 0.4, cex = 0.75)\n\n\n\n\n\n\n\n\n\n3D Visualize\n\n\nCode\nlibrary(scatterplot3d)\n\npar(mfrow = c(1,1))\ns3d &lt;- with(bond.data, scatterplot3d(\n  x = length,\n  y = height,\n  z = strength,\n  pch = 19,\n  color = \"steelblue\",\n  main = \"3D Scatterplot: Strength vs. Length and Height\",\n  xlab = \"Length\",\n  ylab = \"Height\",\n  zlab = \"Strength\",\n  angle = 60\n))\n\nfit &lt;- lm(strength ~ length + height, data = bond.data)\ns3d$plane3d(fit, lty.box = \"solid\")\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Model Fitting and Summary\nWe fit a multiple linear regression model with strength as the response variable and length and height as predictors.\n\n\nCode\nfit &lt;- lm(strength ~ length + height, data = bond.data)\nsummary(fit)\n\n\n\nCall:\nlm(formula = strength ~ length + height, data = bond.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.865 -1.542 -0.362  1.196  5.841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.263791   1.060066   2.136 0.044099 *  \nlength      2.744270   0.093524  29.343  &lt; 2e-16 ***\nheight      0.012528   0.002798   4.477 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.288 on 22 degrees of freedom\nMultiple R-squared:  0.9811,    Adjusted R-squared:  0.9794 \nF-statistic: 572.2 on 2 and 22 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary provides the ANOVA F-test for overall significance, \\(R^2\\), adjusted \\(R^2\\), and t-tests for individual coefficients.\n\n\n4.1.3 Confidence Intervals and Model Components\n\n\nCode\n## Confidence intervals\nconfint(fit)\n\n\n                  2.5 %     97.5 %\n(Intercept) 0.065348613 4.46223426\nlength      2.550313061 2.93822623\nheight      0.006724246 0.01833138\n\n\nCode\n## Fitted values and residuals\npred &lt;- fitted.values(fit)\ne &lt;- resid(fit)\ndata.frame(y = bond.data$strength, y.hat = pred, e = e)\n\n\n\n  \n\n\n\nCode\n## Covariance matrix and standard errors\ncov.mat &lt;- vcov(fit)\ncov.mat\n\n\n             (Intercept)        length        height\n(Intercept)  1.123740429 -3.921612e-02 -1.781991e-03\nlength      -0.039216122  8.746709e-03 -9.903775e-05\nheight      -0.001781991 -9.903775e-05  7.831149e-06\n\n\nCode\ndata.frame(std.error = sqrt(diag(cov.mat)))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#rss-based-inference-f-test-and-adjusted-r2",
    "href": "unit3-mlr/mlr.html#rss-based-inference-f-test-and-adjusted-r2",
    "title": "4  Multiple Linear Regression",
    "section": "4.2 RSS-based Inference: F-test, and adjusted \\(R^2\\)",
    "text": "4.2 RSS-based Inference: F-test, and adjusted \\(R^2\\)\nThe General Linear Model\nThe general linear model is:\n\\[y = X\\beta + \\epsilon\\]\n\n\\(y\\): \\(n \\times 1\\) vector of responses\n\\(X\\): \\(n \\times p\\) design matrix (first column often ones)\n\\(\\beta\\): \\(p \\times 1\\) parameter vector, where \\(p=k+1\\)\n\\(\\epsilon\\): \\(n \\times 1\\) error vector\n\n\n4.2.1 RSS-Based Quantities\n\n4.2.1.1 RSS-Based Quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\n\\(R^2\\)\ndf\nMean Squares\n\\(F\\)\nSS\\(_\\mathrm{adj}\\)\n\\(\\hat{\\sigma}^2\\)\n\\(R^2_{\\mathrm{adj}}\\)\n\n\n\n\n\\(x^\\top\\beta\\)\n\\(\\mathrm{SSR} = \\displaystyle \\sum_{i=1}^n (\\hat y_i - \\bar y)^2\\)\n\\(\\displaystyle \\frac{\\mathrm{SSR}}{\\mathrm{SST}}\\)\n\\(k\\)\n\\(\\displaystyle \\mathrm{MSR} = \\frac{\\mathrm{SSR}}{k}\\)\n\\(\\displaystyle \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}\\)\n\\(\\mathrm{SSR}_{\\mathrm{adj}}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{x^\\top\\beta} = \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}\\)\n\\(\\displaystyle \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{\\mathrm{SST}} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{MST}}\\)\n\n\n\\(\\epsilon\\)\n\\(\\mathrm{SSE} = \\displaystyle \\sum_{i=1}^n (y_i - \\hat y_i)^2\\)\n—\n\\(n-p\\)\n\\(\\displaystyle \\mathrm{MSE} = \\frac{\\mathrm{SSE}}{n-p}\\)\n—\n\\(\\mathrm{SSE}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{\\epsilon} = \\mathrm{MSE}\\)\n—\n\n\n\\(y\\)\n\\(\\mathrm{SST} = \\displaystyle \\sum_{i=1}^n (y_i - \\bar y)^2\\)\n—\n\\(n-1\\)\n\\(\\displaystyle \\mathrm{MST} = \\frac{\\mathrm{SST}}{n-1}\\)\n—\n\\(\\mathrm{SST}\\)\n\\(\\displaystyle \\hat{\\sigma}^2_{y} = \\mathrm{MST}\\)\n—\n\n\n\n\nInterpretation of the \\(\\hat{\\sigma}^2\\) Column\nThe \\(\\hat{\\sigma}^2\\) column highlights how each sum of squares corresponds to an estimated variance.\nThis view makes the adjusted coefficient of determination clear:\n\\[\nR^2_{\\mathrm{adj}}\n= 1 - \\frac{\\hat{\\sigma}^2_\\epsilon}{\\hat{\\sigma}^2_y}\n= \\frac{\\hat{\\sigma}^2_{x^\\top\\beta}}{\\hat{\\sigma}^2_y}.\n\\]\nHence, the adjusted \\(R^2\\) simply expresses the proportion of total estimated variance attributable to the fitted model \\(X\\beta\\) rather than the residual noise \\(\\epsilon\\).\n\n\n\n4.2.2 Remarks\n\n4.2.2.1 Fundamental Identities\n\\[\n\\begin{aligned}\n\\mathrm{SST} &= \\mathrm{SSR} + \\mathrm{SSE}, \\\\\n\\mathrm{MST} &= \\mathrm{MSE} + \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}.\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathrm{SSR}_{\\mathrm{adj}} = (n-1)MST-(n-p+k)\\mathrm{MSE} = \\mathrm{SST}-\\mathrm{SSE} - k\\,\\mathrm{MSE} = \\mathrm{SSR} - k\\,\\mathrm{MSE}.\n\\]\n\n\n\n4.2.2.2 Difference of \\(\\hat{\\sigma}^2\\) and Mean Squares\nThe quantity \\(\\hat{\\sigma}^2\\) represents the estimated variance associated with each component of the model. MSE and MST are the estimated variances of the \\(\\epsilon\\) and \\(y\\) itself. However, the MSR, although called Mean Square for Regression (MSR) is NOT an estimate of the variance or sample variance of \\(x^\\top \\beta\\). The name of “mean” here is used to indicate a different thing. Its name “Mean Square” reflects that it is also an estimate estimate of noise variance \\(\\sigma^2\\) under \\(H_0\\!:\\,\\beta = 0\\):\n\\[\nE[\\mathrm{MSR} \\mid H_0] = \\sigma^2,\n\\qquad\nE[\\mathrm{MSR} \\mid H_1] &gt; \\sigma^2.\n\\]\nHence the F-statistic\n\\[\nF = \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}\n\\] is approximately equal to 1 subject to the variability as characterized with F-distribution with degree freedoms of \\(k\\) and \\(n-p\\). This test is to test whether any regression coefficients are not equal to 0.\n\n\n\n4.2.2.3 \\(\\hat \\sigma^2_{x^\\top\\beta}=\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}\\)\n\\(\\hat \\sigma^2_{x^\\top\\beta}\\) is an unbiased estimator of the variance of linear signal when \\(x\\) is a regarded as a random variable. This can be seen from the following equations: \\[\nE[\\mathrm{SSR}] = k\\,\\sigma^2 + \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta,\n\\qquad\nE[\\mathrm{MSE}] = \\sigma^2.\n\\] Hence, \\[\n\\begin{aligned}\nE[\\mathrm{SSR}_{\\mathrm{adj}}]\n&= E[\\mathrm{SSR}] - k\\,E[\\mathrm{MSE}] \\\\\n&= \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta \\\\\n&= \\sum_{i=1}^n (\\mu_i - \\bar\\mu)^2,\n\\end{aligned}\n\\]\nwhere \\[\n\\begin{aligned}\n\\mu_i &= x_i^\\top \\beta \\\\\n\\bar\\mu &= \\tfrac{1}{n}\\sum_{i=1}^n \\mu_i\n\\end{aligned}\n\\] For fixed \\(X\\), \\(\\mathrm{SSR}_{\\text{adj}}/(n-1)\\) equals the sample variance of the true means \\(\\{\\mu_i\\}\\) over the observed design points. If the rows of \\(X\\) are independently sampled with covariance matrix \\(\\Sigma_X\\) (the random-\\(X\\) model), then\n\\[\n\\mathbb{E}_X\\!\\left[\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}\\right]\n= \\beta^\\top \\Sigma_X \\beta\n= \\mathrm{Var}(x^\\top \\beta),\n\\]\n\n\n4.2.2.4 Connection to Rao-Blackwell Formula\nThe decomposition of \\(\\hat{\\sigma}^2\\) is consistent with the Rao–Blackwell formula for total variance:\n\\[\n\\mathrm{Var}(y) = \\mathrm{Var}\\!\\big(E[y \\mid x]\\big) + E\\!\\big(\\mathrm{Var}[y \\mid x]\\big).\n\\]\nHere,\n\n\\(\\mathrm{Var}\\!\\big(E[y \\mid x]\\big)\\) corresponds to the explained variation due to the regression component \\(x^\\top\\beta\\), and\n\n\\(E\\!\\big(\\mathrm{Var}[y \\mid x]\\big)\\) corresponds to the residual variation due to \\(\\epsilon\\).\n\n\n\n\n4.2.3 A Simulation Study to Understand the Distributions of RSS\nData Generating Model\nFor \\(n=30\\) and \\(p_{max}=20\\), simulate with either \\(H_0:\\beta=\\mathbf 0\\) or \\(H_1\\) where only \\(\\beta_1\\neq 0\\); \\(\\epsilon_i\\sim N(0,1)\\).\nSequence of Fitted Models\n\n\n\n\n\n\n\n\n\nModel Name\n# of Predictors (k)\n# of Parameters (p)\nR Formula\n\n\n\n\nModel 0\n0\n1\ny ~ 1\n\n\nModel 1\n2\n3\ny ~ x_1 + x_2\n\n\n…\n…\n…\n…\n\n\nFinal Model\n20\n21\ny ~ x_1 + ... + x_20\n\n\n\n\n4.2.3.1 When \\(H_0\\) is true\n\n\n\n\n\n\nA single representative frame from the simulation.\n\n\n\n\n\n\n4.2.3.2 When \\(H_1\\) is true\n\n\nCode\nlibrary(av)\npngs &lt;- list.files(\"frames1\", \"\\\\.png$\", full.names = TRUE); pngs &lt;- pngs[order(pngs)]\nav_encode_video(pngs, \"rss-h1.mp4\", framerate = 1, verbose = FALSE)  # H.264 MP4\n\n[1] “/Users/lol553/GitHub/longhaiSK.github.io/teaching/stat845_rdemo/book/unit3-mlr/rss-h1.mp4”\n\nCode\ncat(sprintf('&lt;video controls style=\"width:100%%;height:auto;\"&gt;&lt;source src=\"%s\" type=\"video/mp4\"&gt;&lt;/video&gt;',\"rss-h1.mp4\"))\n\n\n\n\n:::\n\n\n\n\n\nA single representative frame from the simulation.\n\n\n\n\n\n4.2.4 Example\n\n\nCode\n## Data: Weight, height and age of children\nwgt &lt;- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt &lt;- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage &lt;- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\nchild.data &lt;- data.frame(wgt, hgt, age)\n\n\n\n4.2.4.1 Problem 1: Height then Age\n\n\nCode\nfit_age_hgt &lt;- lm(wgt ~ hgt + age, data = child.data)\nsummary(fit_age_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ hgt + age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nhgt           0.7220     0.2608   2.768   0.0218 *\nage           2.0501     0.9372   2.187   0.0565 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78, Adjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n\n\nCode\nfit_hgt &lt;- lm(wgt ~ hgt, data = child.data)\nsummary(fit_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8736 -3.8973 -0.4402  2.2624 11.8375 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   6.1898    12.8487   0.482  0.64035   \nhgt           1.0722     0.2417   4.436  0.00126 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.471 on 10 degrees of freedom\nMultiple R-squared:  0.663, Adjusted R-squared:  0.6293 \nF-statistic: 19.67 on 1 and 10 DF,  p-value: 0.001263\n\n\nCode\nanova(fit_hgt, fit_age_hgt)\n\n\n\n  \n\n\n\n\n\n4.2.4.2 Problem 2: Age then Height\n\n\nCode\nfit_age &lt;- lm(wgt ~ age, data = child.data)\nsummary(fit_age)\n\n\n\nCall:\nlm(formula = wgt ~ age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.000  -3.911   1.143   4.071  10.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  30.5714     8.6137   3.549  0.00528 **\nage           3.6429     0.9551   3.814  0.00341 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.015 on 10 degrees of freedom\nMultiple R-squared:  0.5926,    Adjusted R-squared:  0.5519 \nF-statistic: 14.55 on 1 and 10 DF,  p-value: 0.003407\n\n\nCode\nfit_age_hgt &lt;- lm(wgt ~ age + hgt, data = child.data)\nsummary(fit_age_hgt)\n\n\n\nCall:\nlm(formula = wgt ~ age + hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nage           2.0501     0.9372   2.187   0.0565 .\nhgt           0.7220     0.2608   2.768   0.0218 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78, Adjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n\n\nCode\nanova(fit_age, fit_age_hgt)\n\n\n\n  \n\n\n\n\n\n\n4.2.5 Relationship between t-test and partial F-test\n\nA t-test for a single coefficient is a special case of the partial F-test; the relationship is \\(F = t^2\\) for 1 df in the numerator.\nThe p-value from t-test (output of summary(lm())) is the same as anova test for: \\(H_0: \\beta_j = 0\\) vs \\(H_1\\): all covaraites have non-zero effects.\n\n\n\n4.3 Predictions for Mean Response and a Future Observation\n\n4.3.1 Confidence Interval for Mean Response\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"confidence\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 26.66324 28.66296\n\n\n\n\n4.3.2 Prediction Interval for a New Observation\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"prediction\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 22.81378 32.51241\n\n\n\n\n\n4.4 Model Diagnostics\n\n4.4.1 Residual Calculations\n\n\nCode\nresiduals_df &lt;- data.frame(\n  hat_values = hatvalues(fit),\n  ordinary_resid = resid(fit),\n  standardized_resid = resid(fit) / sigma(fit),\n  studentized_internal = rstandard(fit),\n  studentized_external = rstudent(fit)\n)\nresiduals_df\n\n\n\n  \n\n\n\n\n\n4.4.2 Residual Plots\n\n\nCode\nn &lt;- nrow(bond.data)\nr &lt;- rstudent(fit) \ny.hat &lt;- fitted.values(fit)\n\npar(mfrow = c(2, 3))\nqqnorm(r, main = \"Normal Q-Q Plot\"); qqline(r)\nplot(y.hat, r, xlab = \"Fitted values\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(1:n, r, xlab = \"Observation Number\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$length, r, xlab = \"Wire Length\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$height, r, xlab = \"Die Height\", ylab = \"Studentized Residuals\"); abline(h = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 Influential Observations\n\n\nCode\ninfluence_df &lt;- data.frame(dffits = dffits(fit),\n                           cook.D = cooks.distance(fit),\n                           dfbetas(fit))\ninfluence_df\n\n\n\n  \n\n\n\n\n4.5.1 Plotting with the olsrr Package\n\n\nCode\n## install.packages(\"olsrr\") # Run once if needed\nlibrary(olsrr)\n\nols_plot_cooksd_chart(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dffits(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dfbetas(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n4.6 Polynomial Regression\n\n\nCode\ny &lt;- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)\nx &lt;- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)\nfit_poly &lt;- lm(y ~ x + I(x^2))\nsummary(fit_poly)\n\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0174763 -0.0065087  0.0001297  0.0071482  0.0151887 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.198e+00  2.255e-02   97.48 6.38e-15 ***\nx           -2.252e-02  9.424e-04  -23.90 1.88e-09 ***\nI(x^2)       1.251e-04  8.658e-06   14.45 1.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01219 on 9 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9969 \nF-statistic:  1767 on 2 and 9 DF,  p-value: 2.096e-12\n\n\n\n\nCode\nplot(x, y, xlab = \"Lot size, x\", ylab = \"Average cost per unit, y\")\nlines(x, predict(fit_poly, newdata = data.frame(x = x)), type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit1 &lt;- lm(y ~ x)\nanova(fit1, fit_poly)\n\n\n\n  \n\n\n\n\n\n4.7 Handling Categorical Variables with Dummy Variables\nInvestigate the common observation that males tend to have higher blood pressure than females of similar age.\n\n\nCode\n## Note: Update this path to your local file location\nsbpdata &lt;- read.csv(\"sbpdata.csv\")\nsbpdata\n\n\n\n  \n\n\n\n\n4.7.1 Four Models Involving “sex”\n\n4.7.1.1 Coincidence Model (Age Only)\n\n\nCode\n## Ensure sex is a factor (labels will appear in the legend)\nsbpdata$sex &lt;- as.factor(sbpdata$sex)\n\n## Fit (you already have this)\nfit.age &lt;- lm(sbp ~ age, data = sbpdata)\n\n## Generate predictions over the observed age range\nnew_age &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n               max(sbpdata$age, na.rm = TRUE),\n               length.out = 200)\npred &lt;- predict(fit.age, newdata = data.frame(age = new_age))\n\n## Simple palette for the sex levels (works for 1–3 levels; expand if needed)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter plot with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Add predicted line\nlines(new_age, pred, lwd = 2)\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.age)) \n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age))\n\n\nAnalysis of Variance Table\n\nResponse: sbp\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1 14951.3 14951.3  121.27 &lt; 2.2e-16 ***\nResiduals 67  8260.5   123.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.2 Additive Effect Model (Age + Sex)\n\n\nCode\n## Parallelism: H0: beta3=0 (Sex has additive effect)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n## Ensure sex is a factor for labeling/colors\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (additive: parallelism)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n## X-range and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Parallel fitted lines: one per sex (same slope, different intercepts)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.agePLUSsex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.agePLUSsex))\n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age, fit.agePLUSsex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     67 8260.5                                  \n2     66 5202.0  1    3058.5 38.805 3.701e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.3 Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)\n\n\nCode\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.age.TIMES.sex &lt;- lm(sbp ~ age + sex + age:sex, data = sbpdata)\n\n## Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.age.TIMES.sex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\nModel Matrix and ANOVA\n\n\nCode\ndata.frame(model.matrix(fit.age.TIMES.sex))\n\n\n\n  \n\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nprint(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\nModel 3: sbp ~ age + sex + age:sex\n  Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)    \n1     67 8260.5                                   \n2     66 5202.0  1   3058.52 38.2210 4.692e-08 ***\n3     65 5201.4  1      0.55  0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.4 Varying Slope, Equal Intercept Model (Age + Age:Sex)\n\n\nCode\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.equal.intercept &lt;- lm(sbp ~ age + age:sex, data = sbpdata)\n\n\n## Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.equal.intercept, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.2 Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables\n\n\nCode\nfit.int &lt;- lm(sbp ~ 1, data = sbpdata)\nfit.sex &lt;- lm(sbp ~ sex, data = sbpdata)\n\nprint(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5202.0  1    3058.5  38.2210 4.692e-08 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + age:sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5474.9  1    2785.6  34.8107 1.437e-07 ***\n4     65  5201.4  1     273.4   3.4171   0.06907 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ sex\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67 19282.5  1    3929.2  49.1017 1.684e-09 ***\n3     66  5202.0  1   14080.6 175.9583 &lt; 2.2e-16 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(fit.age)\n\n\n\nCall:\nlm(formula = sbp ~ age, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.782  -7.632   1.968   8.201  22.651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.34905    4.33190   23.86   &lt;2e-16 ***\nage           0.98333    0.08929   11.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 67 degrees of freedom\nMultiple R-squared:  0.6441,    Adjusted R-squared:  0.6388 \nF-statistic: 121.3 on 1 and 67 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.equal.intercept)\n\n\n\nCall:\nlm(formula = sbp ~ age + age:sex, data = sbpdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.6338  -4.3067   0.9922   4.9819  20.2753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.12501    3.55578  29.283  &lt; 2e-16 ***\nage           0.80908    0.07918  10.219 3.14e-15 ***\nage:sex1      0.26705    0.04608   5.795 2.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.108 on 66 degrees of freedom\nMultiple R-squared:  0.7641,    Adjusted R-squared:  0.757 \nF-statistic: 106.9 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.agePLUSsex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.705  -3.299   1.248   4.325  21.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 96.77353    3.62085  26.727  &lt; 2e-16 ***\nage          0.95606    0.07153  13.366  &lt; 2e-16 ***\nsex1        13.51345    2.16932   6.229  3.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.878 on 66 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7691 \nF-statistic: 114.2 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n4.8 Model Building\n\n\nCode\nlibrary(olsrr)\n## Note: Update this path to your local file location\nwine &lt;- read.csv(\"wine.csv\")\n\nmodel.wine &lt;- lm(quality ~ ., data = wine)\n\n\n\n4.8.1 All Possible Regression\n\n\nCode\nols_step_best_subset(model.wine)\n\n\n             Best Subsets Regression             \n-------------------------------------------------\nModel Index    Predictors\n-------------------------------------------------\n     1         flavor                             \n     2         flavor oakiness                    \n     3         aroma flavor oakiness              \n     4         clarity aroma flavor oakiness      \n     5         clarity aroma body flavor oakiness \n-------------------------------------------------\n\n                                                  Subsets Regression Summary                                                   \n-------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                         \nModel    R-Square    R-Square    R-Square     C(p)       AIC        SBIC        SBC        MSEP       FPE       HSP       APC  \n-------------------------------------------------------------------------------------------------------------------------------\n  1        0.6242      0.6137      0.5868    9.0436    130.0214    21.6859    134.9341    61.4102    1.7010    0.0462    0.4176 \n  2        0.6611      0.6417      0.6058    6.8132    128.0901    20.1242    134.6404    57.0033    1.6171    0.0441    0.3970 \n  3        0.7038      0.6776      0.6379    3.9278    124.9781    18.0702    133.1661    51.3383    1.4906    0.0409    0.3659 \n  4        0.7147      0.6801      0.6102    4.6747    125.5480    19.2854    135.3736    50.9872    1.5143    0.0418    0.3717 \n  5        0.7206      0.6769       0.587    6.0000    126.7552    21.0956    138.2183    51.5452    1.5649    0.0436    0.3842 \n-------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\n\n\n4.8.2 Automated Stepwise Procedures\n\n\nCode\n## Backward Elimination (alpha_out = 0.1)\nols_step_backward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Full Model    126.755    138.218    21.096    0.72060    0.67694 \n 1      body          125.548    135.374    19.285    0.71471    0.68013 \n 2      clarity       124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n----------------------------------------------------------------------------------------\n\n\nCode\n## Forward Selection (alpha_in = 0.1)\nols_step_forward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor        130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness      128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma         124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n\n\nCode\n## Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)\nols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)\n\n\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable          AIC        SBC       SBIC       R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model      165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor (+)      130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness (+)    128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma (+)       124.978    133.166    18.070    0.70377    0.67763 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n\n\n\n\n\n4.9 Multicollinearity\n\n4.9.1 A Simple Example\n\n\nCode\ny &lt;- c(19, 20, 37, 39, 36, 38)\nx1 &lt;- c(4, 4, 7, 7, 7.1, 7.1)\nx2 &lt;- c(16, 16, 49, 49, 50.4, 50.4)\ncor(data.frame(x1, x2))\n\n\n          x1        x2\nx1 1.0000000 0.9999713\nx2 0.9999713 1.0000000\n\n\nCode\nfit_multi &lt;- lm(y ~ x1 + x2)\nsummary(fit_multi)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n   1    2    3    4    5    6 \n-0.5  0.5 -1.0  1.0 -1.0  1.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -156.056    117.158  -1.332    0.275\nx1            65.444     45.890   1.426    0.249\nx2            -5.389      4.152  -1.298    0.285\n\nResidual standard error: 1.225 on 3 degrees of freedom\nMultiple R-squared:  0.9897,    Adjusted R-squared:  0.9829 \nF-statistic: 144.3 on 2 and 3 DF,  p-value: 0.001043\n\n\nCode\nfit1_multi &lt;- lm(y ~ x1)\nsummary(fit1_multi)\n\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.5260  0.4740 -0.1925  1.8075 -1.7814  0.2186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -4.0293     2.3332  -1.727    0.159    \nx1            5.8888     0.3762  15.654 9.73e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.325 on 4 degrees of freedom\nMultiple R-squared:  0.9839,    Adjusted R-squared:  0.9799 \nF-statistic: 245.1 on 1 and 4 DF,  p-value: 9.725e-05\n\n\nCode\nols_vif_tol(fit_multi)\n\n\n\n  \n\n\n\n\n\n4.9.2 VIFs in the Wine Quality Data\n\n\nCode\nwine.x &lt;- wine[, -ncol(wine)] # Assuming quality is the last column\ncor(wine.x)\n\n\n             clarity     aroma       body      flavor  oakiness\nclarity   1.00000000 0.0619021 -0.3083783 -0.08515993 0.1832147\naroma     0.06190210 1.0000000  0.5489102  0.73656121 0.2016444\nbody     -0.30837826 0.5489102  1.0000000  0.64665917 0.1521059\nflavor   -0.08515993 0.7365612  0.6466592  1.00000000 0.1797605\noakiness  0.18321471 0.2016444  0.1521059  0.17976051 1.0000000\n\n\nCode\n## VIF using olsrr (data frame output)\nols_vif_tol(model.wine)\n\n\n\n  \n\n\n\n\n\n4.9.3 VIFs in the Children Height Data\n\n\nCode\n## Data: Weight, height and age of children\nwgt &lt;- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt &lt;- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage &lt;- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\n\nfit_age_hgt &lt;- lm(wgt ~ hgt + age, data = child.data)\nols_vif_tol(fit_age_hgt)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#predictions-for-mean-response-and-a-future-observation",
    "href": "unit3-mlr/mlr.html#predictions-for-mean-response-and-a-future-observation",
    "title": "4  Multiple Linear Regression",
    "section": "4.3 Predictions for Mean Response and a Future Observation",
    "text": "4.3 Predictions for Mean Response and a Future Observation\n\n4.3.1 Confidence Interval for Mean Response\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"confidence\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 26.66324 28.66296\n\n\n\n\n4.3.2 Prediction Interval for a New Observation\n\n\nCode\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"prediction\", level = 0.95)\n\n\n      fit      lwr      upr\n1 27.6631 22.81378 32.51241",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#model-diagnostics",
    "href": "unit3-mlr/mlr.html#model-diagnostics",
    "title": "4  Multiple Linear Regression",
    "section": "4.4 Model Diagnostics",
    "text": "4.4 Model Diagnostics\n\n4.4.1 Residual Calculations\n\n\nCode\nresiduals_df &lt;- data.frame(\n  hat_values = hatvalues(fit),\n  ordinary_resid = resid(fit),\n  standardized_resid = resid(fit) / sigma(fit),\n  studentized_internal = rstandard(fit),\n  studentized_external = rstudent(fit)\n)\nresiduals_df\n\n\n\n  \n\n\n\n\n\n4.4.2 Residual Plots\n\n\nCode\nn &lt;- nrow(bond.data)\nr &lt;- rstudent(fit) \ny.hat &lt;- fitted.values(fit)\n\npar(mfrow = c(2, 3))\nqqnorm(r, main = \"Normal Q-Q Plot\"); qqline(r)\nplot(y.hat, r, xlab = \"Fitted values\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(1:n, r, xlab = \"Observation Number\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$length, r, xlab = \"Wire Length\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$height, r, xlab = \"Die Height\", ylab = \"Studentized Residuals\"); abline(h = 0)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#influential-observations",
    "href": "unit3-mlr/mlr.html#influential-observations",
    "title": "4  Multiple Linear Regression",
    "section": "4.5 Influential Observations",
    "text": "4.5 Influential Observations\n\n\nCode\ninfluence_df &lt;- data.frame(dffits = dffits(fit),\n                           cook.D = cooks.distance(fit),\n                           dfbetas(fit))\ninfluence_df\n\n\n\n  \n\n\n\n\n4.5.1 Plotting with the olsrr Package\n\n\nCode\n## install.packages(\"olsrr\") # Run once if needed\nlibrary(olsrr)\n\nols_plot_cooksd_chart(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dffits(fit)\n\n\n\n\n\n\n\n\n\nCode\nols_plot_dfbetas(fit)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#polynomial-regression",
    "href": "unit3-mlr/mlr.html#polynomial-regression",
    "title": "4  Multiple Linear Regression",
    "section": "4.6 Polynomial Regression",
    "text": "4.6 Polynomial Regression\n\n\nCode\ny &lt;- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)\nx &lt;- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)\nfit_poly &lt;- lm(y ~ x + I(x^2))\nsummary(fit_poly)\n\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0174763 -0.0065087  0.0001297  0.0071482  0.0151887 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.198e+00  2.255e-02   97.48 6.38e-15 ***\nx           -2.252e-02  9.424e-04  -23.90 1.88e-09 ***\nI(x^2)       1.251e-04  8.658e-06   14.45 1.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01219 on 9 degrees of freedom\nMultiple R-squared:  0.9975,    Adjusted R-squared:  0.9969 \nF-statistic:  1767 on 2 and 9 DF,  p-value: 2.096e-12\n\n\n\n\nCode\nplot(x, y, xlab = \"Lot size, x\", ylab = \"Average cost per unit, y\")\nlines(x, predict(fit_poly, newdata = data.frame(x = x)), type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit1 &lt;- lm(y ~ x)\nanova(fit1, fit_poly)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#handling-categorical-variables-with-dummy-variables",
    "href": "unit3-mlr/mlr.html#handling-categorical-variables-with-dummy-variables",
    "title": "4  Multiple Linear Regression",
    "section": "4.7 Handling Categorical Variables with Dummy Variables",
    "text": "4.7 Handling Categorical Variables with Dummy Variables\nInvestigate the common observation that males tend to have higher blood pressure than females of similar age.\n\n\nCode\n## Note: Update this path to your local file location\nsbpdata &lt;- read.csv(\"sbpdata.csv\")\nsbpdata\n\n\n\n  \n\n\n\n\n4.7.1 Four Models Involving “sex”\n\n4.7.1.1 Coincidence Model (Age Only)\n\n\nCode\n## Ensure sex is a factor (labels will appear in the legend)\nsbpdata$sex &lt;- as.factor(sbpdata$sex)\n\n## Fit (you already have this)\nfit.age &lt;- lm(sbp ~ age, data = sbpdata)\n\n## Generate predictions over the observed age range\nnew_age &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n               max(sbpdata$age, na.rm = TRUE),\n               length.out = 200)\npred &lt;- predict(fit.age, newdata = data.frame(age = new_age))\n\n## Simple palette for the sex levels (works for 1–3 levels; expand if needed)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter plot with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Add predicted line\nlines(new_age, pred, lwd = 2)\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.age)) \n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age))\n\n\nAnalysis of Variance Table\n\nResponse: sbp\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1 14951.3 14951.3  121.27 &lt; 2.2e-16 ***\nResiduals 67  8260.5   123.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.2 Additive Effect Model (Age + Sex)\n\n\nCode\n## Parallelism: H0: beta3=0 (Sex has additive effect)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n## Ensure sex is a factor for labeling/colors\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (additive: parallelism)\nfit.agePLUSsex &lt;- lm(sbp ~ age + sex, data = sbpdata)\n\n## X-range and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Parallel fitted lines: one per sex (same slope, different intercepts)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.agePLUSsex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata.frame(model.matrix(fit.agePLUSsex))\n\n\n\n  \n\n\n\nCode\nprint(anova(fit.age, fit.agePLUSsex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     67 8260.5                                  \n2     66 5202.0  1    3058.5 38.805 3.701e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.3 Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)\n\n\nCode\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.age.TIMES.sex &lt;- lm(sbp ~ age + sex + age:sex, data = sbpdata)\n\n## Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.age.TIMES.sex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\nModel Matrix and ANOVA\n\n\nCode\ndata.frame(model.matrix(fit.age.TIMES.sex))\n\n\n\n  \n\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nprint(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\nModel 3: sbp ~ age + sex + age:sex\n  Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)    \n1     67 8260.5                                   \n2     66 5202.0  1   3058.52 38.2210 4.692e-08 ***\n3     65 5201.4  1      0.55  0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.7.1.4 Varying Slope, Equal Intercept Model (Age + Age:Sex)\n\n\nCode\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex &lt;- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.equal.intercept &lt;- lm(sbp ~ age + age:sex, data = sbpdata)\n\n\n## Age grid and palette\nages &lt;- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  &lt;- levels(sbpdata$sex)\ncols &lt;- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd &lt;- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat &lt;- predict(fit.equal.intercept, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.2 Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables\n\n\nCode\nfit.int &lt;- lm(sbp ~ 1, data = sbpdata)\nfit.sex &lt;- lm(sbp ~ sex, data = sbpdata)\n\nprint(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5202.0  1    3058.5  38.2210 4.692e-08 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + age:sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 &lt; 2.2e-16 ***\n3     66  5474.9  1    2785.6  34.8107 1.437e-07 ***\n4     65  5201.4  1     273.4   3.4171   0.06907 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nprint(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))\n\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ sex\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1     68 23211.8                                    \n2     67 19282.5  1    3929.2  49.1017 1.684e-09 ***\n3     66  5202.0  1   14080.6 175.9583 &lt; 2.2e-16 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(fit.age)\n\n\n\nCall:\nlm(formula = sbp ~ age, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.782  -7.632   1.968   8.201  22.651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.34905    4.33190   23.86   &lt;2e-16 ***\nage           0.98333    0.08929   11.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 67 degrees of freedom\nMultiple R-squared:  0.6441,    Adjusted R-squared:  0.6388 \nF-statistic: 121.3 on 1 and 67 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.equal.intercept)\n\n\n\nCall:\nlm(formula = sbp ~ age + age:sex, data = sbpdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.6338  -4.3067   0.9922   4.9819  20.2753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.12501    3.55578  29.283  &lt; 2e-16 ***\nage           0.80908    0.07918  10.219 3.14e-15 ***\nage:sex1      0.26705    0.04608   5.795 2.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.108 on 66 degrees of freedom\nMultiple R-squared:  0.7641,    Adjusted R-squared:  0.757 \nF-statistic: 106.9 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.agePLUSsex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.705  -3.299   1.248   4.325  21.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 96.77353    3.62085  26.727  &lt; 2e-16 ***\nage          0.95606    0.07153  13.366  &lt; 2e-16 ***\nsex1        13.51345    2.16932   6.229  3.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.878 on 66 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7691 \nF-statistic: 114.2 on 2 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.age.TIMES.sex)\n\n\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 97.07708    5.17046  18.775  &lt; 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,    Adjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#model-building",
    "href": "unit3-mlr/mlr.html#model-building",
    "title": "4  Multiple Linear Regression",
    "section": "4.8 Model Building",
    "text": "4.8 Model Building\n\n\nCode\nlibrary(olsrr)\n## Note: Update this path to your local file location\nwine &lt;- read.csv(\"wine.csv\")\n\nmodel.wine &lt;- lm(quality ~ ., data = wine)\n\n\n\n4.8.1 All Possible Regression\n\n\nCode\nols_step_best_subset(model.wine)\n\n\n             Best Subsets Regression             \n-------------------------------------------------\nModel Index    Predictors\n-------------------------------------------------\n     1         flavor                             \n     2         flavor oakiness                    \n     3         aroma flavor oakiness              \n     4         clarity aroma flavor oakiness      \n     5         clarity aroma body flavor oakiness \n-------------------------------------------------\n\n                                                  Subsets Regression Summary                                                   \n-------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                         \nModel    R-Square    R-Square    R-Square     C(p)       AIC        SBIC        SBC        MSEP       FPE       HSP       APC  \n-------------------------------------------------------------------------------------------------------------------------------\n  1        0.6242      0.6137      0.5868    9.0436    130.0214    21.6859    134.9341    61.4102    1.7010    0.0462    0.4176 \n  2        0.6611      0.6417      0.6058    6.8132    128.0901    20.1242    134.6404    57.0033    1.6171    0.0441    0.3970 \n  3        0.7038      0.6776      0.6379    3.9278    124.9781    18.0702    133.1661    51.3383    1.4906    0.0409    0.3659 \n  4        0.7147      0.6801      0.6102    4.6747    125.5480    19.2854    135.3736    50.9872    1.5143    0.0418    0.3717 \n  5        0.7206      0.6769       0.587    6.0000    126.7552    21.0956    138.2183    51.5452    1.5649    0.0436    0.3842 \n-------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\n\n\n\n4.8.2 Automated Stepwise Procedures\n\n\nCode\n## Backward Elimination (alpha_out = 0.1)\nols_step_backward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Full Model    126.755    138.218    21.096    0.72060    0.67694 \n 1      body          125.548    135.374    19.285    0.71471    0.68013 \n 2      clarity       124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n----------------------------------------------------------------------------------------\n\n\nCode\n## Forward Selection (alpha_in = 0.1)\nols_step_forward_p(model.wine, p_val = 0.1)\n\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor        130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness      128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma         124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n\n\nCode\n## Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)\nols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)\n\n\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable          AIC        SBC       SBIC       R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model      165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor (+)      130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness (+)    128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma (+)       124.978    133.166    18.070    0.70377    0.67763 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit3-mlr/mlr.html#multicollinearity",
    "href": "unit3-mlr/mlr.html#multicollinearity",
    "title": "4  Multiple Linear Regression",
    "section": "4.9 Multicollinearity",
    "text": "4.9 Multicollinearity\n\n4.9.1 A Simple Example\n\n\nCode\ny &lt;- c(19, 20, 37, 39, 36, 38)\nx1 &lt;- c(4, 4, 7, 7, 7.1, 7.1)\nx2 &lt;- c(16, 16, 49, 49, 50.4, 50.4)\ncor(data.frame(x1, x2))\n\n\n          x1        x2\nx1 1.0000000 0.9999713\nx2 0.9999713 1.0000000\n\n\nCode\nfit_multi &lt;- lm(y ~ x1 + x2)\nsummary(fit_multi)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n   1    2    3    4    5    6 \n-0.5  0.5 -1.0  1.0 -1.0  1.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -156.056    117.158  -1.332    0.275\nx1            65.444     45.890   1.426    0.249\nx2            -5.389      4.152  -1.298    0.285\n\nResidual standard error: 1.225 on 3 degrees of freedom\nMultiple R-squared:  0.9897,    Adjusted R-squared:  0.9829 \nF-statistic: 144.3 on 2 and 3 DF,  p-value: 0.001043\n\n\nCode\nfit1_multi &lt;- lm(y ~ x1)\nsummary(fit1_multi)\n\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.5260  0.4740 -0.1925  1.8075 -1.7814  0.2186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -4.0293     2.3332  -1.727    0.159    \nx1            5.8888     0.3762  15.654 9.73e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.325 on 4 degrees of freedom\nMultiple R-squared:  0.9839,    Adjusted R-squared:  0.9799 \nF-statistic: 245.1 on 1 and 4 DF,  p-value: 9.725e-05\n\n\nCode\nols_vif_tol(fit_multi)\n\n\n\n  \n\n\n\n\n\n4.9.2 VIFs in the Wine Quality Data\n\n\nCode\nwine.x &lt;- wine[, -ncol(wine)] # Assuming quality is the last column\ncor(wine.x)\n\n\n             clarity     aroma       body      flavor  oakiness\nclarity   1.00000000 0.0619021 -0.3083783 -0.08515993 0.1832147\naroma     0.06190210 1.0000000  0.5489102  0.73656121 0.2016444\nbody     -0.30837826 0.5489102  1.0000000  0.64665917 0.1521059\nflavor   -0.08515993 0.7365612  0.6466592  1.00000000 0.1797605\noakiness  0.18321471 0.2016444  0.1521059  0.17976051 1.0000000\n\n\nCode\n## VIF using olsrr (data frame output)\nols_vif_tol(model.wine)\n\n\n\n  \n\n\n\n\n\n4.9.3 VIFs in the Children Height Data\n\n\nCode\n## Data: Weight, height and age of children\nwgt &lt;- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt &lt;- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage &lt;- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\n\nfit_age_hgt &lt;- lm(wgt ~ hgt + age, data = child.data)\nols_vif_tol(fit_age_hgt)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html",
    "href": "unit4-lr/logistic.html",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "5.1 Odds as a Function of Probability\nFor an event with probability \\(p\\), the odds is \\[\\mathrm{odds}(p)=\\frac{p}{1-p}\\] and the log-odds (logit) is \\[\\mathrm{logit}(p)=\\log\\left(\\frac{p}{1-p}\\right)\\].\nCode\n## Plot odds(p) with a right-hand axis for log(odds(p)),\n## using different line colors for the two curves.\n## Defaults: p in [0.01, 0.99].\n## Args:\n##   p_min, p_max : endpoints for p-grid (0&lt;p_min&lt;p_max&lt;1)\n##   n            : number of grid points\n##   annotate     : add reference lines/labels if TRUE\n##   odds_col     : color for odds(p)\n##   logit_col    : color for log(odds(p))\n##   lwd1, lwd2   : line widths for the two curves\n\n\nplot_odds &lt;- function(p_min = 0.01, p_max = 0.99, n = 400,\n                      annotate = TRUE,\n                      odds_col = \"steelblue\",\n                      logit_col = \"firebrick\",\n                      lwd1 = 2, lwd2 = 2) {\n  stopifnot(p_min &gt; 0, p_max &lt; 1, p_min &lt; p_max, n &gt;= 10)\n  p &lt;- seq(p_min, p_max, length.out = n)\n  odds &lt;- p / (1 - p)\n  logit &lt;- log(odds)\n\n  ## Left y-axis: odds(p)\n  plot(p, odds, type = \"l\", lwd = lwd1, col = odds_col,\n       xlab = \"Probability p\",\n       ylab = \"odds(p) = p / (1 - p)\")\n  if (annotate) {\n    abline(h = 1, v = 0.5, lty = 2)\n    text(0.52, 1.05, \"p = 0.5 → odds = 1\", adj = 0)\n  }\n\n  ## Right y-axis: logit(p) = log(odds)\n  op &lt;- par(new = TRUE)\n  on.exit(par(op), add = TRUE)\n  plot(p, logit, type = \"l\", lwd = lwd2, col = logit_col,\n       axes = FALSE, xlab = \"\", ylab = \"\")\n  axis(4)\n  mtext(\"log{odds(p)} = log{p/(1 - p)}\", side = 4, line = 3)\n\n  if (annotate) {\n    abline(v = 0.5, lty = 2)\n    # logit(0.5) = 0 reference (horizontal) on the right-axis scale\n    usr &lt;- par(\"usr\")\n    segments(x0 = usr[1], y0 = 0, x1 = 0.5, y1 = 0, lty = 3)\n  }\n\n  legend(\"topleft\",\n         legend = c(\"odds(p)\", \"log{odds(p)}\"),\n         col = c(odds_col, logit_col),\n         lwd = c(lwd1, lwd2), bty = \"n\")\n\n  invisible(list(p = p, odds = odds, logit = logit))\n}\n\n## Example usage:\n## plot_odds()  # defaults: steelblue for odds, firebrick for log-odds (right axis)\nplot_odds(odds_col = \"#1f77b4\", logit_col = \"#d62728\", n = 600)\nLogistic regression models log-odds linearly in predictors, which both keeps fitted probabilities in \\((0,1)\\) and turns multiplicative effects on odds into additive effects on the linear predictor.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#a-simulated-data",
    "href": "unit4-lr/logistic.html#a-simulated-data",
    "title": "5  Logistic Regression",
    "section": "5.2 A Simulated Data",
    "text": "5.2 A Simulated Data\nWe simulate data from a logistic model where the logit is a linear function of \\(x\\):\n\\[\n\\operatorname{logit}{p(x)}\n=\n\\log\\left(\\frac{p(x)}{1-p(x)}\\right)\n=\n\\beta_0 + \\beta_1 x,\n\\]\nso that\n\\[\np(x)\n=\n\\operatorname{logit}^{-1}(\\beta_0+\\beta_1 x)\n=\n\\frac{1}{1+\\exp{-(\\beta_0+\\beta_1 x)}}.\n\\]\nWe then display the observed \\(y_i\\) (binary outcomes) and the true probability curve \\(p(x)\\) in red.\n\n\nCode\nset.seed(123)\n\n## -- Truth (edit as desired) --\nn     &lt;- 200\nbeta0 &lt;- 0\nbeta1 &lt;-  4\n\n## -- Simulate --\nx   &lt;- runif(n, -1, 1)             # predictor\neta &lt;- beta0 + beta1 * x\np   &lt;- plogis(eta)                 # true p(x)\ny   &lt;- rbinom(n, size = 1, prob = p) # outcomes\n\nsim.data &lt;- data.frame(x = x, y = y, p = p)\n\n\n\n5.2.1 Fit a logistic model to the simulated data\n\n\nCode\n## -- Optional: fit a model to the simulated data --\nsim.fit &lt;- glm(y ~ x, data = sim.data, family = binomial())\np_fit &lt;- predict(sim.fit, newdata = data.frame(x = x), type = \"response\")\n\n## -- Plot: points for y_i (jittered), red line for true p(x) --\n## Define jitter amount\njit &lt;- 0.05 \n## jitter to separate 0/1 visually\nyj &lt;- jitter(sim.data$y, amount = jit) \n\nplot(sim.data$x, yj,\n     pch = 16, col = rgb(0, 0, 0, 0.45),\n     xlab = \"x\",\n     ylab = \"Observed y (points) & p(x) (curves)\",\n     ylim = c(-0.1, 1.1))\n\n## True probability curve (red)\nxg &lt;- seq(min(x), max(x), length.out = 500)\nlines(xg, plogis(beta0 + beta1 * xg), col = \"red\", lwd = 2)\n\n## Optional: add fitted probability curve (dashed dark red)\nlines(xg, predict(sim.fit, newdata = data.frame(x = xg), type = \"response\"),\n      col = \"darkred\", lwd = 2, lty = 2)\n\nlegend(\"topleft\",\n       legend = c(\"y (jittered points)\", \"true p(x)\", \"fitted p(x)\"),\n       pch    = c(16, NA, NA),\n       lty    = c(NA, 1, 2),\n       col    = c(rgb(0,0,0,0.45), \"red\", \"darkred\"),\n       lwd    = c(NA, 2, 2),\n       bty    = \"n\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#example-of-coronary-heart-disease-data",
    "href": "unit4-lr/logistic.html#example-of-coronary-heart-disease-data",
    "title": "5  Logistic Regression",
    "section": "5.3 Example of Coronary Heart Disease Data",
    "text": "5.3 Example of Coronary Heart Disease Data\n\n5.3.1 Load a dataset\nThis dataset is about a follow-up study to determine the development of coronary heart disease (CHD) over 9 years of follow-up of 609 white males from Evans County, Georgia.\nVariable meanings (as provided):\n\nchd: 1 if a person has the disease, 0 otherwise.\nsmk: 1 if smoker, 0 if not.\ncat: 1 if catecholamine level is high, 0 if low.\nsbp: systolic blood pressure (continuous).\nage: age in years (continuous).\nchl: cholesterol level (continuous).\necg: 1 if electrocardiogram is abnormal, 0 if normal.\nhpt: 1 if high blood pressure, 0 if normal.\n\n\n\nCode\n## Adjust the path if needed. The default is your original V: drive path.\ndata_path &lt;- \"evans.dat\"\n\n## Read data (expects a header row)\nCHD.data &lt;- read.table(data_path, header = TRUE)\n\nCHD.data\n\n\n\n  \n\n\n\nCode\ncolnames(CHD.data)\n\n\n [1] \"id\"  \"chd\" \"age\" \"cat\" \"chl\" \"dbp\" \"ecg\" \"sbp\" \"smk\" \"hpt\"\n\n\n\n\n5.3.2 Fit Logistic Regression Model for a Single Variable\n\n\nCode\nvars &lt;- c(\"smk\", \"sbp\", \"age\", \"chl\")\n#jit  &lt;- 0.01  # global jitter amount for y\n\n## par(mfrow = c(2, 2), mar = c(4, 4, 2, 4) + 0.1)  # extra right margin for axis(4)\n\nfor (v in vars) {\n  ## Univariate logistic regression using ORIGINAL variable name in the formula\n  fit &lt;- glm(\n    formula = reformulate(v, response = \"chd\"),\n    data    = CHD.data,\n    family  = binomial()\n  )\n  print(summary(fit))\n\n  ## Base scatter of chd with small jitter (left axis: probability scale)\n  plot(\n    CHD.data[[v]],\n    jitter(CHD.data$chd, amount = jit),\n    pch  = 16, col = rgb(0, 0, 0, 0.45),\n    xlab = v, ylab = \"chd (jittered)\",\n    main = paste(\"chd vs\", v),\n    ylim = c(-0.1, 1.1)\n  )\n\n  ## Fitted π(x) in red (left axis)\n  if (length(unique(CHD.data[[v]])) == 2) {\n    # binary predictor\n    xcat &lt;- sort(unique(CHD.data[[v]]))\n    nd   &lt;- setNames(data.frame(xcat), v)\n    pcat &lt;- predict(fit, newdata = nd, type = \"response\")\n    points(xcat, pcat, pch = 19, col = \"red\")\n    lines(xcat, pcat, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{π(x)} with fixed y-limits\n    logit_p &lt;- log(pcat / (1 - pcat))\n    par(new = TRUE)\n    plot(\n      xcat, logit_p, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(CHD.data[[v]]), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n\n  } else {\n    # continuous predictor\n    xg &lt;- seq(min(CHD.data[[v]]), max(CHD.data[[v]]), length.out = 400)\n    nd &lt;- setNames(data.frame(xg), v)\n    pg &lt;- predict(fit, newdata = nd, type = \"response\")\n    lines(xg, pg, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{π(x)} with fixed y-limits\n    logit_pg &lt;- log(pg / (1 - pg))\n    par(new = TRUE)\n    plot(\n      xg, logit_pg, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(xg), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n  }\n}\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.4898     0.2524  -9.865   &lt;2e-16 ***\nsmk           0.6706     0.2919   2.297   0.0216 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 432.81  on 607  degrees of freedom\nAIC: 436.81\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.837912   0.629805  -6.094  1.1e-09 ***\nsbp          0.012154   0.004036   3.011   0.0026 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 430.06  on 607  degrees of freedom\nAIC: 434.06\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.47833    0.75610  -5.923 3.16e-09 ***\nage          0.04445    0.01315   3.381 0.000723 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 427.22  on 607  degrees of freedom\nAIC: 431.22\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\n\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.538260   0.686879  -5.151 2.59e-07 ***\nchl          0.007004   0.003064   2.286   0.0223 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 433.42  on 607  degrees of freedom\nAIC: 437.42\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Fit Logistic Regression Model with all variables\nWe fit a logistic regression with a logit link:\n\n\nCode\nfit1_chd &lt;- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = CHD.data,\n  family = binomial(link = \"logit\")\n)\nsummary(fit1_chd)\n\n\n\nCall:\nglm(formula = chd ~ smk + cat + sbp + age + chl + ecg + hpt, \n    family = binomial(link = \"logit\"), data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.048892   1.345165  -4.497  6.9e-06 ***\nsmk          0.855951   0.306505   2.793  0.00523 ** \ncat          0.732763   0.376129   1.948  0.05139 .  \nsbp         -0.006995   0.006976  -1.003  0.31600    \nage          0.033956   0.015344   2.213  0.02690 *  \nchl          0.008970   0.003274   2.740  0.00615 ** \necg          0.417776   0.295553   1.414  0.15750    \nhpt          0.655498   0.359976   1.821  0.06861 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 399.35  on 601  degrees of freedom\nAIC: 415.35\n\nNumber of Fisher Scoring iterations: 5\n\n\nNotes for interpretation:\n\nPositive coefficients increase the log-odds of CHD; negative coefficients decrease it.\nFor indicator variables (e.g., smk), exp(beta) is the adjusted odds ratio comparing the group with value 1 versus 0, holding others fixed.\nFor continuous predictors (e.g., sbp, age), exp(beta) is the multiplicative change in the odds for a one‑unit increase. For a d-unit increase, the OR is exp(d * beta).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#inference-for-coefficients-confidence-intervals-and-covariance-matrix",
    "href": "unit4-lr/logistic.html#inference-for-coefficients-confidence-intervals-and-covariance-matrix",
    "title": "5  Logistic Regression",
    "section": "5.4 Inference for Coefficients: Confidence Intervals and Covariance Matrix",
    "text": "5.4 Inference for Coefficients: Confidence Intervals and Covariance Matrix\nWe extract profile‑likelihood CIs and the covariance matrix to confirm standard errors.\n\n\nCode\nci_95 &lt;- confint(fit1_chd, level = 0.95)     # profile-likelihood CI\nvcov_mat &lt;- vcov(fit1_chd)                    # covariance matrix of coefficients\nse_vec   &lt;- sqrt(diag(vcov_mat))          # standard errors\n\nci_95\n\n\n                   2.5 %       97.5 %\n(Intercept) -8.718003347 -3.427904298\nsmk          0.275699158  1.483333169\ncat         -0.006873216  1.471885644\nsbp         -0.021166144  0.006266328\nage          0.003687290  0.064005215\nchl          0.002533226  0.015404292\necg         -0.171584621  0.990632546\nhpt         -0.050184520  1.364993401\n\n\nCode\nvcov_mat\n\n\n             (Intercept)           smk           cat           sbp\n(Intercept)  1.809468553 -1.014526e-01  0.1391440386 -4.908229e-03\nsmk         -0.101452600  9.394560e-02 -0.0032961000 -1.653230e-04\ncat          0.139144039 -3.296100e-03  0.1414730484 -9.299960e-04\nsbp         -0.004908229 -1.653230e-04 -0.0009299960  4.866901e-05\nage         -0.011142995  7.738971e-04 -0.0017879998 -1.311191e-05\nchl         -0.002111134  3.161443e-05  0.0003146354 -1.821907e-06\necg          0.003442546  9.255483e-03 -0.0204455233 -2.982539e-04\nhpt          0.139817180  6.954592e-03 -0.0044220690 -1.486400e-03\n                      age           chl           ecg           hpt\n(Intercept) -1.114300e-02 -2.111134e-03  3.442546e-03  1.398172e-01\nsmk          7.738971e-04  3.161443e-05  9.255483e-03  6.954592e-03\ncat         -1.788000e-03  3.146354e-04 -2.044552e-02 -4.422069e-03\nsbp         -1.311191e-05 -1.821907e-06 -2.982539e-04 -1.486400e-03\nage          2.354442e-04 -1.480501e-06 -4.972374e-05  4.044434e-04\nchl         -1.480501e-06  1.071766e-05  5.040548e-05 -6.046197e-05\necg         -4.972374e-05  5.040548e-05  8.735130e-02  9.506863e-04\nhpt          4.044434e-04 -6.046197e-05  9.506863e-04  1.295828e-01\n\n\nCode\nse_vec  # should match the SE column in summary(fit1_chd)\n\n\n(Intercept)         smk         cat         sbp         age         chl \n1.345164879 0.306505459 0.376129032 0.006976318 0.015344190 0.003273784 \n        ecg         hpt \n0.295552531 0.359976081",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#inference-for-odds-ratios",
    "href": "unit4-lr/logistic.html#inference-for-odds-ratios",
    "title": "5  Logistic Regression",
    "section": "5.5 Inference for Odds Ratios",
    "text": "5.5 Inference for Odds Ratios\n\n5.5.1 Interpretation of Odds Ratios in Logistic Regression\nA multiple logistic regression model expresses the log-odds (logit) of an event as a linear function of predictors:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right)\n= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k.\n\\]\nHere,\n\n\\(p = \\Pr(Y = 1 \\mid x_1, x_2, \\ldots, x_k)\\) is the probability of the event,\n\\(\\beta_0\\) is the intercept, and\neach \\(\\beta_j\\) represents the change in the log-odds of the event per one-unit increase in \\(x_j\\), holding all other variables constant.\n\nExponentiating both sides gives the model in odds form:\n\\[\n\\frac{p}{1-p}\n= \\exp(\\beta_0)\n\\times \\exp(\\beta_1 x_1)\n\\times \\exp(\\beta_2 x_2)\n\\times \\cdots\n\\times \\exp(\\beta_k x_k).\n\\]\nAn R function for OR at two Profiles\nThe or_from_predict R function is a utility designed to calculate the Odds Ratio (OR) and its 95% confidence interval (CI) between two specific covariate profiles (new1 and new0) for a given logistic regression model (fit). The calculation is performed on the link (logit) scale. For a logistic model \\(\\text{logit}(p) = \\eta = \\mathbf{X}\\boldsymbol{\\beta}\\), the log-Odds Ratio (logOR) is the difference between the linear predictors (\\(\\eta_1, \\eta_0\\)) for the two profiles: \\[\\widehat{\\text{logOR}} = \\eta_1 - \\eta_0 = (\\mathbf{x}_1^T - \\mathbf{x}_0^T) \\boldsymbol{\\beta} = \\mathbf{c}^T \\boldsymbol{\\beta} \\tag{5.1}\\]\nHere, \\(\\mathbf{c} = \\mathbf{x}_1 - \\mathbf{x}_0\\) is the linear contrast vector derived from the model matrices of the two profiles. The function estimates the variance of this contrast as \\(\\text{Var}(\\widehat{\\text{logOR}}) = \\mathbf{c}^T \\mathbf{V} \\mathbf{c}\\), where \\(\\mathbf{V}\\) is the model’s variance-covariance matrix (vcov(fit)). The standard error \\(SE = \\sqrt{\\mathbf{c}^T \\mathbf{V} \\mathbf{c}}\\) is used to compute the \\(100(1-\\alpha)\\%\\) confidence interval for the logOR: \\[\\widehat{\\text{logOR}} \\pm z_{1-\\alpha/2} \\times SE.\\] These values (estimate and CI bounds) are then exponentiated to produce the final \\(\\widehat{\\text{OR}} = \\exp(\\widehat{\\text{logOR}})\\) and its 95% CI. The function also prints two helpful summaries to the console: a data frame showing only the variables that differ between the new0 and new1 profiles, and a 2x3 table presenting the estimates and CIs for both the OR and the logOR.\nThe R function to find ORs\n\n\nCode\n## Compute OR and 95% CI via predict() on the LINK scale\n## OR = exp( eta(new1) - eta(new0) ), where eta(.) = logit{π(.)}\n## Compute OR via predict() contrast on the LINK scale, also:\n## (ii) print a 2-row data.frame of only variables that differ between new0 and new1\n## (iii) print a 2x3 table (rows: OR, logOR; cols: Estimate, CI_low, CI_up)\nor_from_predict &lt;- function(fit, new1, new0, level = 0.95, digits = 4, tol = 1e-12) {\n  stopifnot(is.data.frame(new1), is.data.frame(new0))\n\n  ## --- REFACTORED SECTION START ---\n  ## ---- (ii) Two-row data.frame with only changed variables ----\n  \n  ## Helper function to find differing variables between two profiles\n  ## This is defined *inside* or_from_predict for encapsulation\n  get_changed_vars &lt;- function(d0, d1, tolerance) {\n    common &lt;- intersect(names(d0), names(d1))\n    diffv  &lt;- vapply(common, function(nm) {\n      x0 &lt;- d0[[nm]]; x1 &lt;- d1[[nm]]\n      if (is.numeric(x0) && is.numeric(x1)) {\n        !isTRUE(all.equal(as.numeric(x0), as.numeric(x1), tolerance = tolerance))\n      } else {\n        !identical(x0, x1)\n      }\n    }, logical(1))\n    \n    keep &lt;- common[diffv]\n    if (length(keep) == 0L) {\n      out &lt;- data.frame(`_no_changes_` = \"no differences\")\n      rownames(out) &lt;- c(\"new0\", \"new1\")\n      return(out)\n    }\n    out &lt;- rbind(d0[keep], d1[keep])\n    rownames(out) &lt;- c(\"new0\", \"new1\")\n    out\n  }\n  \n  ## Call the helper function\n  changes_df &lt;- get_changed_vars(new0, new1, tol)\n  ## --- REFACTORED SECTION END ---\n\n\n  ## ---- Linear contrast for log-OR and its variance ----\n  \n  ## (i) Calculate logOR estimate using predict(type=\"link\")\n  ## eta(.) = logit{p(.)}\n  eta1 &lt;- predict(fit, newdata = new1, type = \"link\")\n  eta0 &lt;- predict(fit, newdata = new0, type = \"link\")\n  logOR_hat &lt;- as.numeric(eta1 - eta0) # logOR = eta1 - eta0\n  \n  ## (ii) Calculate standard error using the contrast vector 'cvec'\n  X1 &lt;- model.matrix(delete.response(terms(fit)), data = new1)\n  X0 &lt;- model.matrix(delete.response(terms(fit)), data = new0)\n  cvec      &lt;- as.numeric(X1 - X0)\n  V         &lt;- vcov(fit)\n  se_logOR  &lt;- sqrt(as.numeric(t(cvec) %*% V %*% cvec))\n\n  alpha  &lt;- 1 - level\n  z      &lt;- qnorm(1 - alpha / 2)\n  ci_log &lt;- c(logOR_hat - z * se_logOR, logOR_hat + z * se_logOR)\n\n  ## ---- 2x3 table: rows OR and logOR; columns Estimate, CI_low, CI_up ----\n  res_tab &lt;- data.frame(\n    Estimate = c(exp(logOR_hat),          logOR_hat),\n    CI_low   = c(exp(ci_log[1L]),         ci_log[1L]),\n    CI_up    = c(exp(ci_log[2L]),         ci_log[2L]),\n    row.names = c(\"OR\", \"logOR\")\n  )\n\n  ## ---- Print requested items ----\n  cat(\"\\nVariables that differ between new0 and new1:\\n\")\n  print(changes_df)\n  cat(\"\\nOdds Ratio summary:\\n\")\n  print(round(res_tab, digits = digits)) # Added rounding for neatness\n\n  ## ---- Return (invisibly) ----\n  invisible(list(\n    OR        = exp(logOR_hat),\n    CI_OR     = exp(ci_log),\n    logOR     = logOR_hat,\n    CI_logOR  = ci_log,\n    se_logOR  = se_logOR,\n    changes   = changes_df,\n    table     = res_tab\n  ))\n}\n\n## --- Example usage ---\n## Suppose 'fit1_chd' is your fitted model and 'CHD.data' is your data\n## base_prof &lt;- as.data.frame(lapply(CHD.data, function(col) if (is.numeric(col)) mean(col) else col[1]))\n## new0 &lt;- base_prof; new0$smk &lt;- 0\n## new1 &lt;- base_prof; new1$smk &lt;- 1\n## or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\nmean_profile &lt;- function(data, vars_binary_as = c(0,1)) {\n  ## Build a single-row data.frame of typical values:\n  out &lt;- lapply(data, function(col) {\n    if (is.numeric(col)) {\n      # If strictly 0/1, keep mean (works fine for GLM prediction),\n      # or switch to mode if you prefer.\n      if (all(col %in% c(0,1))) mean(col) else mean(col, na.rm = TRUE)\n    } else {\n      # Fallback to first level for factors/characters\n      if (is.factor(col)) levels(col)[1] else unique(col)[1]\n    }\n  })\n  as.data.frame(out)\n}\n\n\n\n\n5.5.2 Examples of Finding ORs and Their CIs for the CHD Dataset\n\n5.5.2.1 OR Smoking (smk) (1 vs 0)\n\n\nCode\n### 1) Smoking OR: smk = 1 vs 0 (other vars at their means)\n## Example profiles at sample means (adjust as you like)\nbase_prof &lt;- mean_profile(CHD.data)\nnew0 &lt;- base_prof; new0$smk &lt;- 0\nnew1 &lt;- base_prof; new1$smk &lt;- 1\n\nres_smk &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     smk\nnew0   0\nnew1   1\n\nOdds Ratio summary:\n      Estimate CI_low  CI_up\nOR      2.3536 1.2907 4.2917\nlogOR   0.8560 0.2552 1.4567\n\n\nHow to read this:\n\nOR_smk &gt; 1 suggests higher odds of CHD among smokers (adjusted for other variables). If the 95% CI excludes 1, the association is statistically significant at the 5% level.\n\n\n\n5.5.2.2 OR for Systolic Blood Pressure (sbp): from 120 to 160\nWe compute the adjusted OR for a 40‑unit increase in sbp (from 120 to 160):\n\n\nCode\n### 2) SBP OR: 160 vs 120 (other vars at their means)\nnew0 &lt;- base_prof; new0$sbp &lt;- 120\nnew1 &lt;- base_prof; new1$sbp &lt;- 160\n\nres_sbp &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     sbp\nnew0 120\nnew1 160\n\nOdds Ratio summary:\n      Estimate  CI_low  CI_up\nOR      0.7559  0.4375 1.3062\nlogOR  -0.2798 -0.8267 0.2671\n\n\n\n\n5.5.2.3 OR for Combined Effects of Two Variables: Smoking with an Age Difference\nSuppose we compare two groups that differ in smoking status and age:\n\nGroup A: smk = 1, age = 50 (all other covariates equal)\nGroup B: smk = 0, age = 30\n\nThe log‑odds contrast is (\\(A = \\beta_{smk} + (50-20)\\beta_{age}\\)), so the OR is (\\(\\exp(A)\\)).\n\n\nCode\nnew0 &lt;- base_prof; new0$age &lt;- 30; new0$smk &lt;- 0\nnew1 &lt;- base_prof; new1$age &lt;- 50; new1$smk &lt;- 1\n\nres_ageAsmk &lt;- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\n\nVariables that differ between new0 and new1:\n     age smk\nnew0  30   0\nnew1  50   1\n\nOdds Ratio summary:\n      Estimate CI_low   CI_up\nOR      4.6417 1.8546 11.6168\nlogOR   1.5351 0.6177  2.4525",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#assessing-statistical-significance-with-wilks-theorem-analogue-of-f-test-for-ols",
    "href": "unit4-lr/logistic.html#assessing-statistical-significance-with-wilks-theorem-analogue-of-f-test-for-ols",
    "title": "5  Logistic Regression",
    "section": "5.6 Assessing Statistical Significance with Wilks’ Theorem (Analogue of F-test for OLS)",
    "text": "5.6 Assessing Statistical Significance with Wilks’ Theorem (Analogue of F-test for OLS)\nIn the context of logistic regression, Wilks’ theorem provides the basis for the Likelihood Ratio Test (LRT) used to assess the significance of predictor variables. The theorem states that when comparing a full model (\\(M_1\\)) to a nested null model (\\(M_0\\)), the test statistic, \\(\\Lambda\\), asymptotically follows a chi-squared (\\(\\chi^2\\)) distribution under the null hypothesis (i.e., that the simpler model \\(M_0\\) is correct).\nThe statistic \\(\\Lambda\\) is calculated as the difference in the maximized log-likelihoods: \\[\\Lambda = -2(\\log L_0 - \\log L_1) \\tag{5.2}\\] where \\(\\log L_0\\) and \\(\\log L_1\\) are the log-likelihoods of the null and full models, respectively. In logistic regression, this is equivalent to the difference in the deviances: \\(\\Lambda = \\text{Deviance}_0 - \\text{Deviance}_1\\). This test statistic \\(\\Lambda\\) represents the reduction in deviance (a measure of badness-of-fit) achieved by adding the extra predictors to the model.\nThe following R code chunk generates a conceptual plot of this relationship:\n\n\nCode\n#| label: plot-lrt-concept\n#| echo: false\n#| fig-cap: \"Conceptual plot of Deviance versus Number of Parameters, illustrating the Likelihood Ratio Test statistic (Λ) and Residual Deviance. Λ is the drop from the Null Model to the Full Model. The Saturated Model represents perfect fit (Deviance = 0).\"\n\nlibrary(ggplot2)\n\n## 1. Create conceptual data for the plot\n## These are just for illustration\nn_obs &lt;- 60 # Number of observations\np0 &lt;- 1     # Parameters in null model (intercept)\np1 &lt;- 21    # Parameters in full model (e.g., intercept + 7 predictors)\npsat &lt;- n_obs # Parameters in saturated model (1 per observation)\n\nD0 &lt;- 41 # Null deviance\nD1 &lt;- 20 # Full model deviance (residual deviance of M1)\nD_sat &lt;- 0 # Saturated model deviance\n\n## Data frame for the three points\nplot_data &lt;- data.frame(\n  model = c(\"M_0 (Null)\", \"M_1 (Full)\", \"M_Sat (Saturated)\"),\n  params = c(p0, p1, psat),\n  deviance = c(D0, D1, D_sat),\n  ## Add custom justification and nudges for labels\n  hjust_val = c(0.5, 0.5, 1.1), # Right-align the last label\n  nudge_x_val = c(0, 0, 0) \n)\n\n## 2. Create the ggplot\nggplot(plot_data, aes(x = params, y = deviance)) +\n  ## Draw dashed guide lines for D0 and D1\n  geom_segment(aes(x = p0, y = D0, xend = p1, yend = D0), linetype = \"dashed\", color = \"grey70\") +\n  geom_segment(aes(x = p1, y = D1, xend = psat, yend = D1), linetype = \"dashed\", color = \"grey70\") +\n  \n  ## Connect the points with lines\n  geom_line(color = \"black\", linetype = \"solid\", linewidth = 0.5) +\n  \n  ## Draw the main points\n  geom_point(size = 4, aes(color = model)) +\n  \n  ## --- MODIFIED LABEL PLACEMENT ---\n  ## Label the points using custom nudge/justification\n  geom_text(\n    aes(label = model, hjust = hjust_val, nudge_x = nudge_x_val), \n    nudge_y = 2.5,  # Use a much smaller vertical nudge\n    size = 4\n  ) +\n  \n  ## --- ADDED BACK D0 and D1 ANNOTATIONS ---\n  ## D0 (Null Deviance)\n  geom_segment(\n    aes(x = p0 - 2, y = D0, xend = p0 - 2, yend = D_sat), # Nudged left\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"darkgreen\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = p0 - 3, y = D0 / 2, # Nudged left\n    label = \"D[0]\", parse = TRUE,\n    color = \"darkgreen\", hjust = 0.5, size = 5\n  ) +\n  \n  ## D1 (Residual Deviance of Full Model)\n  geom_segment(\n    aes(x = psat + 2, y = D1, xend = psat + 2, yend = D_sat), # Nudged right\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"darkblue\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = psat + 3, y = D1 / 2, # Nudged right\n    label = \"D[1]\", parse = TRUE,\n    color = \"darkblue\", hjust = 0.5, size = 5\n  ) +\n  \n  ## LRT statistic Λ = D0 - D1\n  geom_segment(\n    aes(x = p1 + 2, y = D0, xend = p1 + 2, yend = D1), # Nudged right\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"red\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = p1 + 3, y = D1 + (D0 - D1) / 2, # Nudged right\n    label = \"Lambda == D[0] - D[1]\",\n    parse = TRUE,\n    color = \"red\", hjust = 0, size = 5\n  ) +\n  \n  ## Customize axes to show the symbolic labels\n  scale_x_continuous(\n    breaks = c(p0, p1, psat),\n    labels = c(expression(p[0]), expression(p[1]), expression(n)),\n    expand = expansion(mult = 0.1) # Add some padding\n  ) +\n  scale_y_continuous(\n    breaks = c(D_sat, D1, D0),\n    labels = c(expression(0), expression(D[1]), expression(D[0]))\n  ) +\n  \n  ## Labels and Title\n  labs(\n    title = \"Relationship between Deviance and Model Complexity\",\n    x = \"Number of Parameters\",\n    y = \"Model Deviance\"\n  ) +\n  \n  ## Clean theme\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"none\" # Remove legend, as points are labeled\n  ) +\n  scale_color_manual(values = c(\"M_0 (Null)\" = \"blue\", \"M_1 (Full)\" = \"blue\", \"M_Sat (Saturated)\" = \"red\"))\n\n\n\n\n\n\n\n\n\nAs the diagram illustrates, the null model (\\(M_0\\)) has fewer parameters (\\(p_0\\)) and a higher deviance (\\(D_0\\), or worse fit), while the full model (\\(M_1\\)) has more parameters (\\(p_1\\)) and a lower deviance (\\(D_1\\)). The Likelihood Ratio Test statistic \\(D\\) is the magnitude of this drop in deviance.\nFor assessing the overall significance of a regression model (fit1_chd), this involves comparing it to its corresponding intercept-only (null) model. The degrees of freedom for the \\(\\chi^2\\) test is the difference in the number of parameters, \\(df = p_1 - p_0\\), which equals the number of predictors in the full model.\nHere is an R code chunk demonstrating how to compute this p-value directly from a glm fit object, assuming it is named fit1_chd.\n\n\nCode\n## Calculate the Likelihood Ratio Test statistic (D) and degrees of freedom (df)\n## by comparing the model's deviance to the null (intercept-only) deviance,\n## both of which are stored in the 'fit1_chd' object.\nsummary(fit1_chd)\n\n\n\nCall:\nglm(formula = chd ~ smk + cat + sbp + age + chl + ecg + hpt, \n    family = binomial(link = \"logit\"), data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.048892   1.345165  -4.497  6.9e-06 ***\nsmk          0.855951   0.306505   2.793  0.00523 ** \ncat          0.732763   0.376129   1.948  0.05139 .  \nsbp         -0.006995   0.006976  -1.003  0.31600    \nage          0.033956   0.015344   2.213  0.02690 *  \nchl          0.008970   0.003274   2.740  0.00615 ** \necg          0.417776   0.295553   1.414  0.15750    \nhpt          0.655498   0.359976   1.821  0.06861 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 399.35  on 601  degrees of freedom\nAIC: 415.35\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nlrt_statistic &lt;- fit1_chd$null.deviance - fit1_chd$deviance\nlrt_df &lt;- fit1_chd$df.null - fit1_chd$df.residual\n\n## Compute the p-value from the chi-squared distribution\n## We use lower.tail = FALSE to get P(ChiSq &gt; D)\np_value &lt;- pchisq(lrt_statistic, lrt_df, lower.tail = FALSE)\n\n## Create and print the result in an ANOVA-like table\n## Row 1: Null model\n## Row 2: Full model (fit1_chd), showing the test against the null\nlrt_table &lt;- data.frame(\n  \"Resid. Df\" = c(fit1_chd$df.null, fit1_chd$df.residual),\n  \"Resid. Dev\" = c(round(fit1_chd$null.deviance, 4), round(fit1_chd$deviance, 4)),\n  \"Test Df\" = c(NA, lrt_df),\n  \"Test Statistic (D)\" = c(NA, round(lrt_statistic, 4)),\n  \"p-value\" = c(NA, format.pval(p_value, digits = 4)),\n  row.names = c(\"Null Model\", \"Full Model (fit1_chd)\"),\n  check.names = FALSE # Prevent R from changing 'p-value' to 'p.value'\n)\n\ncat(\"Likelihood Ratio Test for Model Significance:\\n\")\n\n\nLikelihood Ratio Test for Model Significance:\n\n\nCode\nlrt_table\n\n\n\n  \n\n\n\nUsing built-in anova() function\n\n\nCode\nfit0_chd &lt;- glm (chd~1, data = CHD.data, family = binomial())\nanova(fit0_chd, fit1_chd)\n\n\n\n  \n\n\n\nCode\nanova(fit1_chd, test=\"LRT\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit4-lr/logistic.html#assessing-predictive-effect-size-anologue-to-r2_mathrmadj",
    "href": "unit4-lr/logistic.html#assessing-predictive-effect-size-anologue-to-r2_mathrmadj",
    "title": "5  Logistic Regression",
    "section": "5.7 Assessing Predictive Effect-Size (Anologue to \\(R^2_\\mathrm{adj}\\))",
    "text": "5.7 Assessing Predictive Effect-Size (Anologue to \\(R^2_\\mathrm{adj}\\))\nWhile the LRT assesses overall model significance (in-sample fit), it’s also crucial to evaluate how well the model predicts new, unseen data (out-of-sample performance). A common method is to split the data into a training set (e.g., 2/3 of the data) and a test set (e.g., 1/3). The model is fit using only the training data and then used to make predictions for the test data. We can then compare these predictions to the actual outcomes in the test set.\n\n5.7.1 Understanding the Confusion Matrix and Metrics\nTo evaluate a model’s predictive performance, we classify its probabilistic predictions using a threshold (typically 0.5) and compare them to the true outcomes in a Confusion Matrix:\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\nFrom this matrix, we derive several key performance metrics:\n\nMisclassification Error Rate (ER): The proportion of all predictions that were incorrect. \\[\n  \\text{Error Rate} = \\frac{FP + FN}{TP + TN + FP + FN}\n  \\]\nPrecision (Positive Predictive Value): Answers: “Of all the times the model predicted positive, how often was it correct?” This is crucial when the cost of a False Positive is high. \\[\n  \\text{Precision} = \\frac{TP}{TP + FP}\n  \\]\nRecall (Sensitivity or True Positive Rate): Answers: “Of all the actual positive cases, how many did the model find?” This is crucial when the cost of a False Negative is high. \\[\n  \\text{Recall (TPR)} = \\frac{TP}{TP + FN}\n  \\]\nROC Curve and AUC: An ROC (Receiver Operating Characteristic) Curve is a graph that shows a model’s diagnostic ability across all possible classification thresholds. It plots the True Positive Rate (Recall) on the y-axis against the False Positive Rate (FPR = \\(\\frac{FP}{FP + TN}\\)) on the x-axis.\n\nInterpretation: The curve shows the trade-off between sensitivity (finding all the positives) and specificity (not mislabeling negatives). A random “no-skill” classifier is represented by a diagonal line from (0,0) to (1,1). A perfect classifier would hug the top-left corner (TPR = 1, FPR = 0).\nAUC (Area Under the Curve): The AUC summarizes the entire curve into a single number from 0 to 1. An AUC of 0.5 corresponds to a random guess, while an AUC of 1.0 represents a perfect model.\n\nPrecision-Recall (PR) Curve: A PR Curve plots Precision (y-axis) against Recall (x-axis) at all possible thresholds.\n\nInterpretation: This curve shows the trade-off between how reliable a positive prediction is (Precision) and how complete the model is at finding all positives (Recall).\nWhen to Use: The PR curve is particularly informative when the dataset is imbalanced (i.e., one class, like “fraud” or “disease,” is much rarer than the other). Unlike the ROC curve, the PR curve’s baseline (the “no-skill” line) is a horizontal line at the proportion of positive cases, which makes it easier to see if the model is performing significantly better than chance in a low-positive-rate scenario. A perfect classifier would hug the top-right corner (Precision = 1, Recall = 1).\n\n\n\n\n5.7.2 Illustration with the Simulated Dataset\nThis section applies the train/test split and model evaluation workflow to the sim.data created in the previous step.\n\n\nCode\n## Load the pROC library for AUC calculation\n## install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n## --- 1. Split the data ---\n## We use 'sim.data' which has 200 rows\nset.seed(123) # for reproducibility\nn_sim &lt;- nrow(sim.data)\ntrain_size_sim &lt;- floor(2/3 * n_sim)\ntrain_indices_sim &lt;- sample(1:n_sim, size = train_size_sim)\ntrain_data_sim &lt;- sim.data[train_indices_sim, ]\ntest_data_sim  &lt;- sim.data[-train_indices_sim, ]\n\n## --- 2. Refit the model on the training data ---\n## We fit the model y ~ x on the training data\nfit_train_sim &lt;- glm(\n  y ~ x,\n  data = train_data_sim,\n  family = binomial(link = \"logit\")\n)\n\n## --- 3. Make predictions on the test data ---\n## Note: The true probabilities 'p' are also in test_data_sim\n## We predict from the *fitted* model\npred_probs_sim &lt;- predict(fit_train_sim, newdata = test_data_sim, type = \"response\")\n\n\nPlotting the Predictive Probabilities with True Labels\n\n\nCode\n## --- 5. Plot sorted predicted probabilities ---\n\n## Create a data frame for plotting\nplot_data_sim &lt;- data.frame(\n  Prob = pred_probs_sim,\n  Actual = as.factor(test_data_sim$y),\n  TrueProb = test_data_sim$p # Include true probs for comparison\n)\n\n## Sort by predicted probability\nplot_data_sim &lt;- plot_data_sim[order(plot_data_sim$Prob), ]\nplot_data_sim$Rank &lt;- 1:nrow(plot_data_sim)\n\n## Create the plot\nplot(\n  plot_data_sim$Rank,\n  plot_data_sim$Prob,\n  pch = ifelse(plot_data_sim$Actual == 0, 1, 4),\n  col = ifelse(plot_data_sim$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class (Simulated Data)\",\n  ylim = c(0, 1)\n)\nabline(h = 0.5, lty = 2, col = \"black\")\nabline(h = 0.1, lty = 3, col = \"grey\")\n\n## Add the true probability curve (sorted by predicted prob)\n## This shows how well the fitted model's predictions align with the true probs\n#lines(plot_data_sim$Rank, plot_data_sim$TrueProb[order(plot_data_sim$Prob)], col = \"darkgreen\", lwd = 2)\n\n\n## Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  lty = c(NA, NA),\n  lwd = c(NA, NA),\n  col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nConfusion Matrix with threshold=0.5\n\n\nCode\n## --- 4. Assess accuracy ---\n\n## 4a. Misclassification Error Rate (using 0.5 threshold)\nthreshold &lt;- 0.5\npred_class_sim &lt;- ifelse(pred_probs_sim &gt; threshold, 1, 0)\nconf_matrix_sim &lt;- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n## --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.5):\\n\")\n\n\nConfusion Matrix (Counts, threshold = 0.5):\n\n\nCode\nprint(conf_matrix_sim)\n\n\n      Predicted\nActual  0  1\n     0 26  5\n     1  9 27\n\n\nCode\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n\n\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n\n\nCode\n## margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.839 0.161\n     1 0.250 0.750\n\n\nCode\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n\n\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n\n\nCode\n## margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.743 0.156\n     1 0.257 0.844\n\n\nCode\n## --- MODIFIED LINES END ---\n\n\n## Check if matrix has 2x2 dimensions, otherwise metrics will fail\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN &lt;- conf_matrix_sim[1, 1]\n  FP &lt;- conf_matrix_sim[1, 2]\n  FN &lt;- conf_matrix_sim[2, 1]\n  TP &lt;- conf_matrix_sim[2, 2]\n\n  ## Calculate metrics\n  error_rate &lt;- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall &lt;- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR &lt;- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision &lt;- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n\n\n\nMisclassification Error Rate: 0.209 \nTrue Positive Rate (Recall): 0.75 \nFalse Positive Rate: 0.1613 \nPrecision: 0.8438 \n\n\nConfusion Matrix with threshold=0.1\n\n\nCode\nthreshold &lt;- 0.1\npred_class_sim &lt;- ifelse(pred_probs_sim &gt; threshold, 1, 0)\nconf_matrix_sim &lt;- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n## --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.1):\\n\")\n\n\nConfusion Matrix (Counts, threshold = 0.1):\n\n\nCode\nprint(conf_matrix_sim)\n\n\n      Predicted\nActual  0  1\n     0 15 16\n     1  1 35\n\n\nCode\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n\n\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n\n\nCode\n## margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.484 0.516\n     1 0.028 0.972\n\n\nCode\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n\n\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n\n\nCode\n## margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n\n\n      Predicted\nActual     0     1\n     0 0.938 0.314\n     1 0.062 0.686\n\n\nCode\n## --- MODIFIED LINES END ---\n\n\n## Check if matrix has 2x2 dimensions\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN &lt;- conf_matrix_sim[1, 1]\n  FP &lt;- conf_matrix_sim[1, 2]\n  FN &lt;- conf_matrix_sim[2, 1]\n  TP &lt;- conf_matrix_sim[2, 2]\n\n  ## Calculate metrics\n  error_rate &lt;- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall &lt;- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR &lt;- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision &lt;- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n\n\n\nMisclassification Error Rate: 0.2537 \nTrue Positive Rate (Recall): 0.9722 \nFalse Positive Rate: 0.5161 \nPrecision: 0.6863 \n\n\nROC curve and Area Under the ROC (AUC)\n\n\nCode\n## 4b. Area Under the Curve (AUC)\nroc_curve_sim &lt;- roc(test_data_sim$y, pred_probs_sim, quiet = TRUE)\n\n## Plot the ROC curve\nplot(roc_curve_sim, main = \"ROC Curve (Simulated Test Data)\", print.auc = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nauc_value_sim &lt;- auc(roc_curve_sim)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value_sim, 4), \"\\n\\n\"))\n\n\nArea Under the Curve (AUC): 0.8996 \n\n\nPR curve and Area Under PR Curve (AUPR)\n\n\nCode\n## Load the ROCR library\n## install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n## --- 1. Create a 'prediction' object ---\n## 'prediction' takes all predictions and all true labels\npred_obj &lt;- prediction(pred_probs_sim, test_data_sim$y)\n\n## --- 2. Create a 'performance' object for PR ---\n## \"prec\" is for precision, \"rec\" is for recall\nperf_pr &lt;- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n## --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc &lt;- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value &lt;- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n\n\nArea Under PR Curve (AUPR): 0.9157 \n\n\nCode\n## --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Simulated Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n## --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision_sim &lt;- sum(test_data_sim$y == 1) / length(test_data_sim$y)\nabline(h = baseline_precision_sim, col = \"blue\", lty = 2)\n\n## --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # &lt;-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision_sim, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 Application to the CHD Dataset\n\n\nCode\n## Load the pROC library for AUC calculation\n## install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n## --- 1. Split the data ---\nset.seed(123) # for reproducibility\nn &lt;- nrow(CHD.data)\ntrain_size &lt;- floor(2/3 * n)\ntrain_indices &lt;- sample(1:n, size = train_size)\ntrain_data &lt;- CHD.data[train_indices, ]\ntest_data  &lt;- CHD.data[-train_indices, ]\n\n## --- 2. Refit the model on the training data ---\nfit_train &lt;- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = train_data,\n  family = binomial(link = \"logit\")\n)\n\n## --- 3. Make predictions on the test data ---\npred_probs &lt;- predict(fit_train, newdata = test_data, type = \"response\")\n\n\nPlot Predictive Probabilities\n\n\nCode\n## --- 5. Plot sorted predicted probabilities ---\n\n## Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Prob = pred_probs,\n  Actual = as.factor(test_data$chd)\n)\n\n## Sort by predicted probability\nplot_data &lt;- plot_data[order(plot_data$Prob), ]\nplot_data$Rank &lt;- 1:nrow(plot_data)\n\n## Create the plot\n## We use 'pch' (plot character) to set different symbols\n## 'pch = 1' is 'o' (default)\n## 'pch = 4' is 'x'\nplot(\n  plot_data$Rank,\n  plot_data$Prob,\n  pch = ifelse(plot_data$Actual == 0, 1, 4),\n  col = ifelse(plot_data$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Log-odds of Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class\",\n  ylim = c(0,1)\n)\nabline(h=0.5)\nabline(h=0.1, col=\"grey\")\n\n## Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  col = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\nROC curve and Area Under the ROC (AUC)\n\n\nCode\n## 4b. Area Under the Curve (AUC)\nroc_curve &lt;- roc(test_data$chd, pred_probs, quiet = TRUE)\n\n## Plot the ROC curve\nplot(roc_curve, main = \"ROC Curve (Test Data)\", print.auc = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nauc_value &lt;- auc(roc_curve)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value, 4), \"\\n\\n\"))\n\n\nArea Under the Curve (AUC): 0.6872 \n\n\nPR curve and Area Under PR Curve (AUPR)\n\n\nCode\n## Load the ROCR library\n## install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n## --- 1. Create a 'prediction' object ---\n## 'prediction' takes all predictions and all true labels\n## We use 'pred_probs' and 'test_data$chd' from the CHD data split\npred_obj &lt;- prediction(pred_probs, test_data$chd)\n\n## --- 2. Create a 'performance' object for PR ---\n## \"prec\" is for precision, \"rec\" is for recall\nperf_pr &lt;- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n## --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc &lt;- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value &lt;- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n\n\nArea Under PR Curve (AUPR): 0.2826 \n\n\nCode\n## --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n## --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision &lt;- sum(test_data$chd == 1) / length(test_data$chd)\nabline(h = baseline_precision, col = \"blue\", lty = 2)\n\n## --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # &lt;-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html",
    "href": "unit5-factor/crbd.html",
    "title": "6  One-factor Design",
    "section": "",
    "text": "6.1 Completely Randomized Design",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-factor Design</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html#completely-randomized-design",
    "href": "unit5-factor/crbd.html#completely-randomized-design",
    "title": "6  One-factor Design",
    "section": "",
    "text": "6.1.1 Plasma Etching Experiment\nThis section analyzes data from a Completely Randomized Design (CRD). In a CRD, experimental units (in this case, the silicon wafers being etched) are assigned to treatments (the RF Power levels) completely at random. The primary goal is to determine if changing the RF Power level has a statistically significant effect on the mean etch rate.\n\n6.1.1.1 Data and Visualization\nWe begin by loading the data into a single, tidy data.frame. The response variable, rate, contains all the etch rate observations. The predictor variable, power, is a factor, which is R’s way of representing a categorical variable. This tells R to treat the different power levels as distinct groups.\n\n\nCode\n## Define the data vectors\nrate &lt;- c(575, 542, 530, 539, 570, 565, 593, 590, 579, 610,\n          600, 651, 610, 637, 629, 725, 700, 715, 685, 710)\npower_levels &lt;- c(160, 180, 200, 220)\n\n## Create the data frame\netching_df &lt;- data.frame(\n  rate = rate,\n  power = factor(rep(power_levels, each = 5))\n)\n\n## Display the first few rows\netching_df\n\n\n\n  \n\n\n\nGrouped Boxplots\n\n\nCode\nboxplot(rate~power, data=etching_df)\n\n\n\n\n\n\n\n\n\nUsing ggplot to visualize grouped data\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr) # Using dplyr for easier data manipulation\n\n## Calculate group means and their start/end indices\nmean_rates &lt;- etching_df %&gt;%\n  mutate(obs_index = row_number()) %&gt;%\n  group_by(power) %&gt;%\n  summarise(\n    mean_rate = mean(rate),\n    x_start = min(obs_index) - 0.5,\n    x_end = max(obs_index) + 0.5\n  )\n\nggplot(etching_df, aes(x = 1:nrow(etching_df), y = rate, color = power)) +\n  geom_point(size = 3, alpha = 0.7) + # Plot individual data points\n  geom_segment(\n    data = mean_rates, \n    aes(x = x_start, xend = x_end, y = mean_rate, yend = mean_rate),\n    linetype = \"dashed\", \n    size = 1.2\n  ) + # Add line segments for group means\n  labs(\n    title = \"Etch Rate Observations by RF Power Level\",\n    x = \"Observation Index\",\n    y = \"Etch Rate\",\n    color = \"RF Power (W)\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nIndex Plot of Etch Rate with Group-Specific Mean Lines\n\n\n\n\n\n\n6.1.1.2 Model Fitting with Sum-to-Zero Constraint\nWe fit a linear model using the lm() function to perform an Analysis of Variance (ANOVA). The model is specified as rate ~ power, and we now include the data = etching_df argument.\nTo get interpretable estimates for the treatment effects (\\(\\tau_i\\)), we use a sum-to-zero constraint (contr.sum), which forces the sum of the treatment effects to be zero (\\(\\sum \\tau_i = 0\\)).\n\n\nCode\nfit &lt;- lm(rate ~ power, data = etching_df, contrasts = list(power = contr.sum))\ncat (\"Model Matrix:\\n\")\n\n\nModel Matrix:\n\n\nCode\nmodel.matrix(fit)\n\n\n   (Intercept) power1 power2 power3\n1            1      1      0      0\n2            1      1      0      0\n3            1      1      0      0\n4            1      1      0      0\n5            1      1      0      0\n6            1      0      1      0\n7            1      0      1      0\n8            1      0      1      0\n9            1      0      1      0\n10           1      0      1      0\n11           1      0      0      1\n12           1      0      0      1\n13           1      0      0      1\n14           1      0      0      1\n15           1      0      0      1\n16           1     -1     -1     -1\n17           1     -1     -1     -1\n18           1     -1     -1     -1\n19           1     -1     -1     -1\n20           1     -1     -1     -1\nattr(,\"assign\")\n[1] 0 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$power\n    [,1] [,2] [,3]\n160    1    0    0\n180    0    1    0\n200    0    0    1\n220   -1   -1   -1\n\n\nCode\nsummary.fit &lt;- summary(fit)\ncat (\"Summary of lm fitting results:\\n\")\n\n\nSummary of lm fitting results:\n\n\nCode\nsummary.fit\n\n\n\nCall:\nlm(formula = rate ~ power, data = etching_df, contrasts = list(power = contr.sum))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  617.750      4.085 151.234  &lt; 2e-16 ***\npower1       -66.550      7.075  -9.406 6.39e-08 ***\npower2       -30.350      7.075  -4.290 0.000563 ***\npower3         7.650      7.075   1.081 0.295602    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9261,    Adjusted R-squared:  0.9122 \nF-statistic:  66.8 on 3 and 16 DF,  p-value: 2.883e-09\n\n\n\n\n6.1.1.3 Point Estimation of Parameters\nThe output of the model provides estimates for the overall mean (\\(\\hat{\\mu}\\)) and the treatment effects for the first k-1 levels (\\(\\hat{\\tau}_1, \\hat{\\tau}_2, \\hat{\\tau}_3\\)).\n\n\\(\\hat{\\mu}\\) (the Intercept) is the estimate of the grand mean etch rate across all power levels.\n\\(\\hat{\\tau}_i\\) is the estimated effect of the i-th power level, representing how much that level’s mean deviates from the grand mean.\n\nUsing the sum-to-zero constraint, we can manually calculate the effect for the final level, \\(\\hat{\\tau}_4\\).\n\n\nCode\n## Extract coefficients\nest &lt;- coef(fit)\ntau4.hat &lt;- -sum(est[-1])\ntaui.hat &lt;- c(est[-1], tau4.hat)\nprint(\"Estimated Treatment Effects (tau_i):\")\n\n\n[1] \"Estimated Treatment Effects (tau_i):\"\n\n\nCode\nprint(taui.hat)\n\n\npower1 power2 power3        \n-66.55 -30.35   7.65  89.25 \n\n\nCode\n## Estimates of treatment means (mu_i)\nmu.hat &lt;- est[1]\nmui.hat &lt;- mu.hat + taui.hat\nprint(\"Estimated Treatment Means (mu_i):\")\n\n\n[1] \"Estimated Treatment Means (mu_i):\"\n\n\nCode\nprint(mui.hat)\n\n\npower1 power2 power3        \n 551.2  587.4  625.4  707.0 \n\n\n\n\n6.1.1.4 ANOVA Table\nThe ANOVA table partitions the total variation into variation between treatment groups (power) and variation within treatment groups (random error). The p-value (Pr(&gt;F)) indicates if the treatment has a significant effect.\n\n\nCode\nanova(fit)\n\n\n\n  \n\n\n\n\n\n6.1.1.5 95% Confidence Intervals for Treatment Means\nA confidence interval provides a range of plausible values for the true mean etch rate at each power level.\n\n\nCode\n## Number of replicates\nn &lt;- 5 \n## Extract sqrt(MSE) and error df\nsqrt.MSE &lt;- summary.fit$sigma\nDF &lt;- fit$df.residual\n## Find t-value\nt.value &lt;- qt(0.975, DF)\n## Calculate CIs\nCI.lower &lt;- mui.hat - t.value * sqrt.MSE / sqrt(n)\nCI.upper &lt;- mui.hat + t.value * sqrt.MSE / sqrt(n)\n\n## Display CIs\ndata.frame(Power_Level = power_levels, Mean = mui.hat, Lower_CI = CI.lower, Upper_CI = CI.upper)\n\n\n\n  \n\n\n\nAlternatively, one can use a model without intercept\n\n\nCode\nfit_nointercpt &lt;- lm(rate ~ 0+power, data = etching_df)\nsummary(fit_nointercpt)\n\n\n\nCall:\nlm(formula = rate ~ 0 + power, data = etching_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \npower160  551.200      8.169   67.47   &lt;2e-16 ***\npower180  587.400      8.169   71.90   &lt;2e-16 ***\npower200  625.400      8.169   76.55   &lt;2e-16 ***\npower220  707.000      8.169   86.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.9991 \nF-statistic:  5768 on 4 and 16 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(fit_nointercpt)\n\n\n            2.5 %   97.5 %\npower160 533.8815 568.5185\npower180 570.0815 604.7185\npower200 608.0815 642.7185\npower220 689.6815 724.3185\n\n\n\n\n6.1.1.6 Comparison with Default “Treatment” Contrast\nFitting the model without specifying contrasts uses R’s default (“treatment” contrast), which sets \\(\\tau_1 = 0\\). The fundamental results (ANOVA, treatment means) remain unchanged.\n\n\nCode\nfit1 &lt;- lm(rate ~ power, data = etching_df)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = rate ~ power, data = etching_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -25.4  -13.0    2.8   13.2   25.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  551.200      8.169  67.471  &lt; 2e-16 ***\npower180      36.200     11.553   3.133  0.00642 ** \npower200      74.200     11.553   6.422 8.44e-06 ***\npower220     155.800     11.553  13.485 3.73e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.27 on 16 degrees of freedom\nMultiple R-squared:  0.9261,    Adjusted R-squared:  0.9122 \nF-statistic:  66.8 on 3 and 16 DF,  p-value: 2.883e-09\n\n\n\n\n6.1.1.7 Pairwise Comparisons\nSince our ANOVA result was significant, we perform post-hoc tests to determine exactly which pairs of power levels have different means.\n\n6.1.1.7.1 Tukey’s HSD Test\nTukey’s Honest Significant Difference (HSD) controls the family-wise error rate, adjusting p-values to account for multiple comparisons.\n\n\nCode\nfit.aov &lt;- aov(rate ~ power, data = etching_df)\nTukeyHSD(fit.aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = rate ~ power, data = etching_df)\n\n$power\n         diff        lwr       upr     p adj\n180-160  36.2   3.145624  69.25438 0.0294279\n200-160  74.2  41.145624 107.25438 0.0000455\n220-160 155.8 122.745624 188.85438 0.0000000\n200-180  38.0   4.945624  71.05438 0.0215995\n220-180 119.6  86.545624 152.65438 0.0000001\n220-200  81.6  48.545624 114.65438 0.0000146\n\n\n\n\n6.1.1.7.2 Fisher’s LSD Test\nThe Fisher’s Least Significant Difference (LSD) test does not control the family-wise error rate but is more powerful.\n\n\nCode\nwith(etching_df, pairwise.t.test(rate, power, p.adj = \"none\"))\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  rate and power \n\n    160     180     200    \n180 0.0064  -       -      \n200 8.4e-06 0.0046  -      \n220 3.7e-10 1.7e-08 2.7e-06\n\nP value adjustment method: none \n\n\n\n\n\n6.1.1.8 Checking Model Assumptions\nThe validity of our ANOVA results depends on three key assumptions about the model’s residuals. We use diagnostic plots to check them.\n\n\nCode\nr &lt;- rstudent(fit)\nfitted &lt;- fitted.values(fit)\n\n\n\n6.1.1.8.1 Normality of Residuals\nA Normal Q-Q plot is used to check if the residuals are normally distributed. The points should fall closely along the straight diagonal line.\n\n\nCode\nqqnorm(r)\nqqline(r)\n\n\n\n\n\nNormal Q-Q plot of standardized residuals.\n\n\n\n\n\n\n6.1.1.8.2 Independence of Residuals\nA plot of residuals versus run order helps check for independence. We look for random scatter around the zero line.\n\n\nCode\nplot(r, ylab = \"Standardized residuals\", xlab = \"Run order\",\n     main = \"Plot of residuals vs. run order\")\nabline(h = 0)\n\n\n\n\n\nStandardized residuals vs. run order.\n\n\n\n\n\n\n6.1.1.8.3 Constant Variance (Homoscedasticity)\nA plot of residuals versus fitted values helps check for constant variance. The spread of residuals should be roughly constant across all fitted values.\n\n\nCode\nplot(fitted, r, ylab = \"Standardized residuals\", \n     xlab = \"Fitted values\", main = \"Plot of residuals vs. fitted values\")\nabline(h = 0)\n\n\n\n\n\nStandardized residuals vs. fitted values.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-factor Design</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html#unbalanced-designs-with-nequal-sample-sizes",
    "href": "unit5-factor/crbd.html#unbalanced-designs-with-nequal-sample-sizes",
    "title": "6  One-factor Design",
    "section": "6.2 Unbalanced Designs with nequal Sample Sizes",
    "text": "6.2 Unbalanced Designs with nequal Sample Sizes\nThe ANOVA framework also handles unbalanced designs. We again start by creating a data frame.\n\n\nCode\n## Create the data frame\nbricks_df &lt;- data.frame(\n  density = c(21.8, 21.9, 21.7, 21.6, 21.7,\n              21.7, 21.4, 21.5, 21.4,\n              21.9, 21.8, 21.8, 21.6, 21.5,\n              21.9, 21.7, 21.8, 21.4),\n  temperature = factor(c(rep(100, 5), rep(125, 4), rep(150, 5), rep(175, 4)))\n)\n\nbricks_df\n\n\n\n  \n\n\n\nCode\n## Fit the model and get the ANOVA table\nfit2 &lt;- lm(density ~ temperature, data = bricks_df)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = density ~ temperature, data = bricks_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.300 -0.100  0.000  0.095  0.200 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    21.74000    0.07171 303.150   &lt;2e-16 ***\ntemperature125 -0.24000    0.10757  -2.231   0.0425 *  \ntemperature150 -0.02000    0.10142  -0.197   0.8465    \ntemperature175 -0.04000    0.10757  -0.372   0.7156    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1604 on 14 degrees of freedom\nMultiple R-squared:  0.3025,    Adjusted R-squared:  0.153 \nF-statistic: 2.024 on 3 and 14 DF,  p-value: 0.1569\n\n\nCode\nanova(fit2)\n\n\n\n  \n\n\n\nIn this case, the large p-value (0.133) indicates that there is no statistically significant evidence that firing temperature affects brick density.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-factor Design</span>"
    ]
  },
  {
    "objectID": "unit5-factor/crbd.html#randomized-complete-block-design",
    "href": "unit5-factor/crbd.html#randomized-complete-block-design",
    "title": "6  One-factor Design",
    "section": "6.3 Randomized Complete Block Design",
    "text": "6.3 Randomized Complete Block Design\n\n6.3.1 Vascular Graft Experiment\nThis section analyzes a Randomized Complete Block Design (RCBD), used to control for a known source of variability (here, “batches of resin,” treated as blocks).\n\n6.3.1.1 Data and Visulization\nWe structure the data in a data.frame to identify the response, treatment (pressure), and block (batch) for each observation.\n\n\nCode\n## Define data vectors\nstrength &lt;- c(90.3, 89.2, 98.2, 93.9, 87.4, 97.9,\n              92.5, 89.5, 90.6, 94.7, 87.0, 95.8,\n              85.5, 90.8, 89.6, 86.2, 88.0, 93.4,\n              82.5, 89.5, 85.6, 87.4, 78.9, 90.7)\npressure_levels &lt;- rep(c(8500, 8700, 8900, 9100), each = 6)\nbatch_levels &lt;- rep(1:6, 4)\n\n## Create the data frame\ngraft_df &lt;- data.frame(\n  strength = strength,\n  pressure = factor(pressure_levels),\n  batch = factor(batch_levels)\n)\n\n\ngraft_df\n\n\n\n  \n\n\n\nVisualize the Block and Treatment Effects\n\n\nCode\npar (mfrow = c(1,2))\n#boxplot\nplot(strength ~ batch, data=graft_df , main = \"Block\")\nplot(strength ~ pressure, data=graft_df , main = \"Pressure\")\n\n\n\n\n\n\n\n\n\nInteraction Plots\n\n\nCode\nggplot(graft_df, aes(x = pressure, y = strength, group = batch, color = batch)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  labs(\n    title = \"Interaction Plot: Batch and Pressure\",\n    x = \"Pressue\",\n    y = \"Strength\",\n    color = \"Batch\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nInteraction between Material Type and Temperature.\n\n\n\n\n\n#### Model Fitting and ANOVA\n\nThe model `strength ~ pressure + batch` partitions the total variance into treatment, block, and error components. Our primary interest is in the significance of the `pressure` factor.\n\n::: {.cell}\n\n```{.r .cell-code}\nrcbd.fit1 &lt;- aov(strength ~ pressure + batch, data = graft_df)\nanova(rcbd.fit1)\n\n\n  \n\n\n:::\nThe small p-value for pressure (0.0019) provides strong evidence that extrusion pressure significantly affects graft strength after accounting for batch differences.\n\n\n6.3.1.2 Model Adequacy Checks\nThe assumptions for an RCBD are the same as for a CRD. We perform the same diagnostic checks.\n\nCode\nrcbd.r1 &lt;- rstudent(rcbd.fit1)\nrcbd.fitted1 &lt;- fitted.values(rcbd.fit1)\n\nqqnorm(rcbd.r1, main = \"Normal Q-Q Plot\")\nqqline(rcbd.r1)\nplot(rcbd.fitted1, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Fitted values\", main = \"Residuals vs. Fitted\")\nabline(h = 0)\nplot(graft_df$pressure, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Extrusion pressure\", main = \"Residuals vs. Treatment\")\nabline(h = 0)\nplot(graft_df$batch, rcbd.r1, ylab = \"Standardized residuals\", \n     xlab = \"Batches of raw material\", main = \"Residuals vs. Block\")\nabline(h = 0)\n\n\n\n\n\n\n\nNormal Q-Q Plot\n\n\n\n\n\n\n\nResiduals vs. Fitted Values\n\n\n\n\n\n\n\n\n\nResiduals vs. Treatment (Pressure)\n\n\n\n\n\n\n\nResiduals vs. Block (Batch)\n\n\n\n\n\n\n\n6.3.1.3 Pairwise Comparisons\nAgain, since the treatment factor (pressure) is significant, we perform post-hoc tests.\n\n6.3.1.3.1 Tukey’s HSD Test\nTukey’s HSD compares all pairs of treatment levels while controlling the family-wise error rate.\n\n\nCode\nTukeyHSD(rcbd.fit1, which = \"pressure\")\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = strength ~ pressure + batch, data = graft_df)\n\n$pressure\n               diff        lwr       upr     p adj\n8700-8500 -1.133333  -5.637161  3.370495 0.8854831\n8900-8500 -3.900000  -8.403828  0.603828 0.1013084\n9100-8500 -7.050000 -11.553828 -2.546172 0.0020883\n8900-8700 -2.766667  -7.270495  1.737161 0.3245644\n9100-8700 -5.916667 -10.420495 -1.412839 0.0086667\n9100-8900 -3.150000  -7.653828  1.353828 0.2257674\n\n\n\n\n6.3.1.3.2 Fisher’s LSD Test\nThe LSD.test() function from the agricolae package correctly handles the error structure of an RCBD.\n\n\nCode\n## install.packages(\"agricolae\")\nlibrary(agricolae)\n\nout &lt;- LSD.test(rcbd.fit1, trt = \"pressure\", p.adj = \"none\", group = FALSE)\nprint(out$comparison)\n\n\n            difference pvalue signif.        LCL       UCL\n8500 - 8700   1.133333 0.4795         -2.1974047  4.464071\n8500 - 8900   3.900000 0.0247       *  0.5692620  7.230738\n8500 - 9100   7.050000 0.0004     ***  3.7192620 10.380738\n8700 - 8900   2.766667 0.0970       . -0.5640714  6.097405\n8700 - 9100   5.916667 0.0018      **  2.5859286  9.247405\n8900 - 9100   3.150000 0.0621       . -0.1807380  6.480738",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One-factor Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html",
    "href": "unit6-factorial/factorial.html",
    "title": "7  Two-Factor Factorial Design",
    "section": "",
    "text": "7.1 Battery Design Experiment\nThis analysis explores data from a two-factor factorial experiment designed to assess the lifespan of a battery. The experiment investigates two factors: material type (with 3 levels) and operating temperature (with 3 levels: 15°C, 70°C, and 125°C). The primary goal is to understand not only how each factor individually affects battery life but, more importantly, whether the effect of temperature depends on the material type used. This combined effect is known as an interaction.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#data-setup-and-preparation",
    "href": "unit6-factorial/factorial.html#data-setup-and-preparation",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.2 Data Setup and Preparation",
    "text": "7.2 Data Setup and Preparation\nFirst, we organize the raw data into a structured data.frame. This is a best practice in R that makes the data easier to manage and the code more readable. We create columns for the response variable life and the two factors, material and temperature, ensuring they are treated as categorical variables (factors) for the analysis.\n\n\nCode\n## Response variable: battery life\nlife &lt;- c(130,155,74,180,  34,40,80,75,   20,70,82,58,\n          150,188,159,126, 136,122,106,115, 25,70,58,45,\n          138,110,168,160, 174,120,150,139, 96,104,82,60)\n\n## Create the data frame\nbattery_df &lt;- data.frame(\n  life = life,\n  material = factor(rep(1:3, each = 12)),\n  temperature = factor(rep(rep(c(15, 70, 125), each = 4), 3))\n)\n\n## Preview the data\nbattery_df",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#exploratory-data-analysis-and-visualization",
    "href": "unit6-factorial/factorial.html#exploratory-data-analysis-and-visualization",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.3 Exploratory Data Analysis and Visualization",
    "text": "7.3 Exploratory Data Analysis and Visualization\nBefore fitting a formal model, we visualize the data to get an intuition for the relationships between the factors and the response.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#boxplots-of-main-effects",
    "href": "unit6-factorial/factorial.html#boxplots-of-main-effects",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.4 Boxplots of Main Effects",
    "text": "7.4 Boxplots of Main Effects\nBoxplots are excellent for examining the distribution of battery life for each level of our factors independently. This gives us a preliminary look at the main effects—the individual impact of material type and temperature.\n\nCode\nlibrary(ggplot2)\n\n## Boxplot for Material Type\nggplot(battery_df, aes(x = material, y = life, fill = material)) +\n  geom_boxplot() +\n  labs(title = \"Battery Life by Material Type\", x = \"Material Type\", y = \"Life (hours)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n## Boxplot for Temperature\nggplot(battery_df, aes(x = temperature, y = life, fill = temperature)) +\n  geom_boxplot() +\n  labs(title = \"Battery Life by Temperature\", x = \"Temperature (°C)\", y = \"Life (hours)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nDistribution of Battery Life by Material and Temperature.\n\n\n\n\n\n\n\nDistribution of Battery Life by Material and Temperature.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#interaction-plot",
    "href": "unit6-factorial/factorial.html#interaction-plot",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.5 Interaction Plot",
    "text": "7.5 Interaction Plot\nThe most crucial plot for a factorial experiment is the interaction plot. It displays the mean battery life for each combination of material and temperature. If the lines are parallel, it suggests there is no interaction. If the lines are not parallel (i.e., they cross or diverge), it indicates that the effect of temperature on battery life is different for each material type, signaling a likely interaction.\n\n\nCode\nggplot(battery_df, aes(x = temperature, y = life, group = material, color = material)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  labs(\n    title = \"Interaction Plot: Material Type and Temperature\",\n    x = \"Temperature (°C)\",\n    y = \"Average Battery Life (hours)\",\n    color = \"Material Type\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nInteraction between Material Type and Temperature.\n\n\n\n\nThe non-parallel lines in the plot strongly suggest that a significant interaction effect is present. Specifically, the performance of Material 3 drops less dramatically with increasing temperature compared to Materials 1 and 2.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#model-fitting-and-analysis-of-variance-anova",
    "href": "unit6-factorial/factorial.html#model-fitting-and-analysis-of-variance-anova",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.6 Model Fitting and Analysis of Variance (ANOVA)",
    "text": "7.6 Model Fitting and Analysis of Variance (ANOVA)\nWe now fit a linear model to formally test the significance of the main effects and the interaction term. The model life ~ material * temperature is shorthand for life ~ material + temperature + material:temperature. We use a sum-to-zero contrast (contr.sum) for balanced interpretation of the effects. The ANOVA table will tell us if the variation caused by our factors is statistically significant compared to the random variation in the data.\n\n\nCode\n## Fit the full factorial model\nbattery_fit &lt;- lm(life ~ material * temperature, \n                  data = battery_df,\n                  contrasts = list(material = contr.sum, temperature = contr.sum))\n\nsummary(battery_fit)\n\n\n\nCall:\nlm(formula = life ~ material * temperature, data = battery_df, \n    contrasts = list(material = contr.sum, temperature = contr.sum))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.750 -14.625   1.375  17.938  45.250 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             105.528      4.331  24.367  &lt; 2e-16 ***\nmaterial1               -22.361      6.125  -3.651  0.00111 ** \nmaterial2                 2.806      6.125   0.458  0.65057    \ntemperature1             39.306      6.125   6.418  7.1e-07 ***\ntemperature2              2.056      6.125   0.336  0.73975    \nmaterial1:temperature1   12.278      8.662   1.417  0.16778    \nmaterial2:temperature1    8.111      8.662   0.936  0.35735    \nmaterial1:temperature2  -27.972      8.662  -3.229  0.00325 ** \nmaterial2:temperature2    9.361      8.662   1.081  0.28936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.98 on 27 degrees of freedom\nMultiple R-squared:  0.7652,    Adjusted R-squared:  0.6956 \nF-statistic:    11 on 8 and 27 DF,  p-value: 9.426e-07\n\n\nCode\n## Generate the ANOVA table\nanova(battery_fit)\n\n\n\n  \n\n\n\nThe ANOVA table shows very small p-values (Pr(&gt;F)) for material, temperature, and, most importantly, the material:temperature interaction. This confirms our visual inspection: all effects are statistically significant. Because the interaction is significant, our interpretation should focus on the interaction itself rather than the main effects in isolation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#model-adequacy-checks",
    "href": "unit6-factorial/factorial.html#model-adequacy-checks",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.7 Model Adequacy Checks",
    "text": "7.7 Model Adequacy Checks\nThe validity of our ANOVA results depends on the model’s residuals meeting certain assumptions (normality, constant variance, independence). We check these with diagnostic plots.\n\nCode\n## Extract standardized residuals and fitted values\nbattery_fit_diag &lt;- data.frame(\n  residuals = rstandard(battery_fit),\n  fitted = fitted.values(battery_fit)\n)\n\n## Normal Q-Q Plot\np1 &lt;- ggplot(battery_fit_diag, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"Normal Q-Q Plot\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n## Residuals vs. Fitted Plot\np2 &lt;- ggplot(battery_fit_diag, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\", x = \"Fitted Values\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\np1 \np2\n\n\n\n\n\n\n\nDiagnostic plots for the battery life model.\n\n\n\n\n\n\n\nDiagnostic plots for the battery life model.\n\n\n\n\n\nThe Normal Q-Q plot shows the points falling roughly along the line, suggesting the normality assumption is met. The Residuals vs. Fitted plot shows a random scatter of points around the zero line, indicating that the variance is reasonably constant. The model assumptions appear to be satisfied.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#post-hoc-analysis-pairwise-comparisons",
    "href": "unit6-factorial/factorial.html#post-hoc-analysis-pairwise-comparisons",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.8 Post-Hoc Analysis: Pairwise Comparisons",
    "text": "7.8 Post-Hoc Analysis: Pairwise Comparisons\nSince the interaction is significant, we must compare the means of the nine specific treatment combinations (3 materials × 3 temperatures). Simply comparing the average effect of Material 1 vs. Material 2 would be misleading, as that difference depends on the temperature.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#tukeys-hsd-test",
    "href": "unit6-factorial/factorial.html#tukeys-hsd-test",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.9 Tukey’s HSD Test",
    "text": "7.9 Tukey’s HSD Test\nTukey’s Honest Significant Difference (HSD) test is a post-hoc test that compares all possible pairs of means while controlling the family-wise error rate. We apply it to an aov model object. The output for the material:temperature interaction shows which specific combinations are significantly different from one another.\n\n\nCode\n## Fit the model using aov() for Tukey's test\nbattery_aov &lt;- aov(life ~ material * temperature, data = battery_df)\n\n## Perform Tukey's HSD test\nTukeyHSD(battery_aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = life ~ material * temperature, data = battery_df)\n\n$material\n        diff       lwr      upr     p adj\n2-1 25.16667 -1.135677 51.46901 0.0627571\n3-1 41.91667 15.614323 68.21901 0.0014162\n3-2 16.75000 -9.552344 43.05234 0.2717815\n\n$temperature\n            diff        lwr       upr     p adj\n70-15  -37.25000  -63.55234 -10.94766 0.0043788\n125-15 -80.66667 -106.96901 -54.36432 0.0000001\n125-70 -43.41667  -69.71901 -17.11432 0.0009787\n\n$`material:temperature`\n               diff         lwr        upr     p adj\n2:15-1:15     21.00  -40.823184  82.823184 0.9616404\n3:15-1:15      9.25  -52.573184  71.073184 0.9998527\n1:70-1:15    -77.50 -139.323184 -15.676816 0.0065212\n2:70-1:15    -15.00  -76.823184  46.823184 0.9953182\n3:70-1:15     11.00  -50.823184  72.823184 0.9994703\n1:125-1:15   -77.25 -139.073184 -15.426816 0.0067471\n2:125-1:15   -85.25 -147.073184 -23.426816 0.0022351\n3:125-1:15   -49.25 -111.073184  12.573184 0.2016535\n3:15-2:15    -11.75  -73.573184  50.073184 0.9991463\n1:70-2:15    -98.50 -160.323184 -36.676816 0.0003449\n2:70-2:15    -36.00  -97.823184  25.823184 0.5819453\n3:70-2:15    -10.00  -71.823184  51.823184 0.9997369\n1:125-2:15   -98.25 -160.073184 -36.426816 0.0003574\n2:125-2:15  -106.25 -168.073184 -44.426816 0.0001152\n3:125-2:15   -70.25 -132.073184  -8.426816 0.0172076\n1:70-3:15    -86.75 -148.573184 -24.926816 0.0018119\n2:70-3:15    -24.25  -86.073184  37.573184 0.9165175\n3:70-3:15      1.75  -60.073184  63.573184 1.0000000\n1:125-3:15   -86.50 -148.323184 -24.676816 0.0018765\n2:125-3:15   -94.50 -156.323184 -32.676816 0.0006078\n3:125-3:15   -58.50 -120.323184   3.323184 0.0742711\n2:70-1:70     62.50    0.676816 124.323184 0.0460388\n3:70-1:70     88.50   26.676816 150.323184 0.0014173\n1:125-1:70     0.25  -61.573184  62.073184 1.0000000\n2:125-1:70    -7.75  -69.573184  54.073184 0.9999614\n3:125-1:70    28.25  -33.573184  90.073184 0.8281938\n3:70-2:70     26.00  -35.823184  87.823184 0.8822881\n1:125-2:70   -62.25 -124.073184  -0.426816 0.0474675\n2:125-2:70   -70.25 -132.073184  -8.426816 0.0172076\n3:125-2:70   -34.25  -96.073184  27.573184 0.6420441\n1:125-3:70   -88.25 -150.073184 -26.426816 0.0014679\n2:125-3:70   -96.25 -158.073184 -34.426816 0.0004744\n3:125-3:70   -60.25 -122.073184   1.573184 0.0604247\n2:125-1:125   -8.00  -69.823184  53.823184 0.9999508\n3:125-1:125   28.00  -33.823184  89.823184 0.8347331\n3:125-2:125   36.00  -25.823184  97.823184 0.5819453",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  },
  {
    "objectID": "unit6-factorial/factorial.html#fishers-lsd-method",
    "href": "unit6-factorial/factorial.html#fishers-lsd-method",
    "title": "7  Two-Factor Factorial Design",
    "section": "7.10 Fisher’s LSD Method",
    "text": "7.10 Fisher’s LSD Method\nThe Fisher’s Least Significant Difference (LSD) method is another option for pairwise comparisons. To test the interaction means, we must specify both factors in the trt argument.\n\n\nCode\nlibrary(agricolae)\n\n## Perform LSD test on the interaction term\nlsd_results &lt;- LSD.test(battery_aov, trt = c(\"material\", \"temperature\"),\n                        p.adj = \"none\", group = FALSE)\n\n## Print the comparison table\nprint(lsd_results$comparison)\n\n\n              difference pvalue signif.         LCL        UCL\n1:125 - 1:15      -77.25 0.0003     *** -114.950479 -39.549521\n1:125 - 1:70        0.25 0.9892          -37.450479  37.950479\n1:125 - 2:125       8.00 0.6667          -29.700479  45.700479\n1:125 - 2:15      -98.25 0.0000     *** -135.950479 -60.549521\n1:125 - 2:70      -62.25 0.0022      **  -99.950479 -24.549521\n1:125 - 3:125     -28.00 0.1392          -65.700479   9.700479\n1:125 - 3:15      -86.50 0.0001     *** -124.200479 -48.799521\n1:125 - 3:70      -88.25 0.0001     *** -125.950479 -50.549521\n1:15 - 1:70        77.50 0.0002     ***   39.799521 115.200479\n1:15 - 2:125       85.25 0.0001     ***   47.549521 122.950479\n1:15 - 2:15       -21.00 0.2631          -58.700479  16.700479\n1:15 - 2:70        15.00 0.4214          -22.700479  52.700479\n1:15 - 3:125       49.25 0.0124       *   11.549521  86.950479\n1:15 - 3:15        -9.25 0.6187          -46.950479  28.450479\n1:15 - 3:70       -11.00 0.5544          -48.700479  26.700479\n1:70 - 2:125        7.75 0.6765          -29.950479  45.450479\n1:70 - 2:15       -98.50 0.0000     *** -136.200479 -60.799521\n1:70 - 2:70       -62.50 0.0021      ** -100.200479 -24.799521\n1:70 - 3:125      -28.25 0.1358          -65.950479   9.450479\n1:70 - 3:15       -86.75 0.0001     *** -124.450479 -49.049521\n1:70 - 3:70       -88.50 0.0000     *** -126.200479 -50.799521\n2:125 - 2:15     -106.25 0.0000     *** -143.950479 -68.549521\n2:125 - 2:70      -70.25 0.0007     *** -107.950479 -32.549521\n2:125 - 3:125     -36.00 0.0605       .  -73.700479   1.700479\n2:125 - 3:15      -94.50 0.0000     *** -132.200479 -56.799521\n2:125 - 3:70      -96.25 0.0000     *** -133.950479 -58.549521\n2:15 - 2:70        36.00 0.0605       .   -1.700479  73.700479\n2:15 - 3:125       70.25 0.0007     ***   32.549521 107.950479\n2:15 - 3:15        11.75 0.5279          -25.950479  49.450479\n2:15 - 3:70        10.00 0.5907          -27.700479  47.700479\n2:70 - 3:125       34.25 0.0732       .   -3.450479  71.950479\n2:70 - 3:15       -24.25 0.1980          -61.950479  13.450479\n2:70 - 3:70       -26.00 0.1685          -63.700479  11.700479\n3:125 - 3:15      -58.50 0.0036      **  -96.200479 -20.799521\n3:125 - 3:70      -60.25 0.0029      **  -97.950479 -22.549521\n3:15 - 3:70        -1.75 0.9248          -39.450479  35.950479\n\n\nThe results from both Tukey’s HSD and Fisher’s LSD provide detailed p-values for comparing pairs of treatment combinations, allowing us to make specific conclusions, such as “at 125°C, Material 3 has a significantly longer life than Materials 1 and 2.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Two-Factor Factorial Design</span>"
    ]
  }
]