---
title: "Multiple Linear Regression"
author: ""
date: ""
editor: source
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
#| echo: false

options(sim_fps = 0.5)
options(sim_rebuild = TRUE)
```

## An Example: Wire Bond Strength Dataset

### Loading Data and Visualization

**Note:** You must change the file paths in the `read.csv()` functions below to match the location of the files on your computer (for example `C:\\Users\\<YourUsername>\\Documents` on Windows).

```{r}
## Read data. Change the path as necessary.
## Example: bond.data <- read.csv("wire-bond.csv")
bond.data <- read.csv("wire-bond.csv")

## This will now be automatically rendered as a paged table
bond.data
```

```{r, include=FALSE, eval=FALSE}
## Use set.seed() to make sure you get the exact same random data every time
set.seed(42)

## Define the number of data points to create
n <- 150

## Create the bond.data data frame
bond.data <- data.frame(
  length = runif(n, 10, 50),
  height = runif(n, 5, 25)
)

## Create 'strength' based on a linear relationship with the other variables
bond.data$strength <- (2.5 * bond.data$length) + 
                      (4.1 * bond.data$height) + 
                      rnorm(n, mean = 0, sd = 1)

head(bond.data)
```

**2D Visualization**

```{r}
#| fig-width: 6
#| fig-height: 2.5
par(mfrow = c(1, 3), mar = c(5, 4, 2, 1))

## 1) length vs strength
i1 <- which(!is.na(bond.data$length) & !is.na(bond.data$strength))
plot(bond.data$length[i1], bond.data$strength[i1],
     xlab = "Wire Length", ylab = "Pull strength", pch = 19)
text(bond.data$length[i1], bond.data$strength[i1],
     labels = i1, pos = 1, offset = 0.4, cex = 0.75)

## 2) height vs strength
i2 <- which(!is.na(bond.data$height) & !is.na(bond.data$strength))
plot(bond.data$height[i2], bond.data$strength[i2],
     xlab = "Die height", ylab = "Pull strength", pch = 19)
text(bond.data$height[i2], bond.data$strength[i2],
     labels = i2, pos = 1, offset = 0.4, cex = 0.75)

## 3) height vs length
i3 <- which(!is.na(bond.data$height) & !is.na(bond.data$length))
plot(bond.data$height[i3], bond.data$length[i3],
     xlab = "Die height", ylab = "Length", pch = 19)
text(bond.data$height[i3], bond.data$length[i3],
     labels = i3, pos = 1, offset = 0.4, cex = 0.75)
```

**3D Visualize**

```{r}
library(scatterplot3d)

par(mfrow = c(1,1))
s3d <- with(bond.data, scatterplot3d(
  x = length,
  y = height,
  z = strength,
  pch = 19,
  color = "steelblue",
  main = "3D Scatterplot: Strength vs. Length and Height",
  xlab = "Length",
  ylab = "Height",
  zlab = "Strength",
  angle = 60
))

fit <- lm(strength ~ length + height, data = bond.data)
s3d$plane3d(fit, lty.box = "solid")
```

### Model Fitting and Summary

We fit a multiple linear regression model with `strength` as the response variable and `length` and `height` as predictors.

```{r}
fit <- lm(strength ~ length + height, data = bond.data)
summary(fit)
```

The summary provides the ANOVA F-test for overall significance, $R^2$, adjusted $R^2$, and t-tests for individual coefficients.

### Confidence Intervals and Model Components

```{r}
## Confidence intervals
confint(fit)

## Fitted values and residuals
pred <- fitted.values(fit)
e <- resid(fit)
data.frame(y = bond.data$strength, y.hat = pred, e = e)

## Covariance matrix and standard errors
cov.mat <- vcov(fit)
cov.mat
data.frame(std.error = sqrt(diag(cov.mat)))
```

## RSS-based Inference: F-test, and adjusted $R^2$

**The General Linear Model**

The general linear model is:

$$y = X\beta + \epsilon$$

-   $y$: $n \times 1$ vector of responses
-   $X$: $n \times p$ design matrix (first column often ones)
-   $\beta$: $p \times 1$ parameter vector, where $p=k+1$
-   $\epsilon$: $n \times 1$ error vector

### RSS-Based Quantities

#### RSS-Based Quantities

| Source | Sum of Squares | $R^2$ | df | Mean Squares | $F$ | SS$_\mathrm{adj}$ | $\hat{\sigma}^2$ | $R^2_{\mathrm{adj}}$ |
|:-------|:-------|:-------|:------:|:-------|:-------|:-------|:-------|:-------|
| $x^\top\beta$ | $\mathrm{SSR} = \displaystyle \sum_{i=1}^n (\hat y_i - \bar y)^2$ | $\displaystyle \frac{\mathrm{SSR}}{\mathrm{SST}}$ | $k$ | $\displaystyle \mathrm{MSR} = \frac{\mathrm{SSR}}{k}$ | $\displaystyle \frac{\mathrm{MSR}}{\mathrm{MSE}}$ | $\mathrm{SSR}_{\mathrm{adj}}$ | $\displaystyle \hat{\sigma}^2_{x^\top\beta} = \frac{\mathrm{SSR}_{\mathrm{adj}}}{n-1}$ | $\displaystyle \frac{\mathrm{SSR}_{\mathrm{adj}}}{\mathrm{SST}} = 1 - \frac{\mathrm{MSE}}{\mathrm{MST}}$ |
| $\epsilon$ | $\mathrm{SSE} = \displaystyle \sum_{i=1}^n (y_i - \hat y_i)^2$ | — | $n-p$ | $\displaystyle \mathrm{MSE} = \frac{\mathrm{SSE}}{n-p}$ | — | $\mathrm{SSE}$ | $\displaystyle \hat{\sigma}^2_{\epsilon} = \mathrm{MSE}$ | — |
| $y$ | $\mathrm{SST} = \displaystyle \sum_{i=1}^n (y_i - \bar y)^2$ | — | $n-1$ | $\displaystyle \mathrm{MST} = \frac{\mathrm{SST}}{n-1}$ | — | $\mathrm{SST}$ | $\displaystyle \hat{\sigma}^2_{y} = \mathrm{MST}$ | — |

------------------------------------------------------------------------

**Interpretation of the** $\hat{\sigma}^2$ Column

The $\hat{\sigma}^2$ column highlights how each sum of squares corresponds to an estimated variance.\
This view makes the adjusted coefficient of determination clear:

$$
R^2_{\mathrm{adj}} 
= 1 - \frac{\hat{\sigma}^2_\epsilon}{\hat{\sigma}^2_y}
= \frac{\hat{\sigma}^2_{x^\top\beta}}{\hat{\sigma}^2_y}.
$$

Hence, the adjusted $R^2$ simply expresses the **proportion of total estimated variance** attributable to the fitted model $X\beta$ rather than the residual noise $\epsilon$.

### Remarks

#### Fundamental Identities

$$
\begin{aligned}
\mathrm{SST} &= \mathrm{SSR} + \mathrm{SSE}, \\
\mathrm{MST} &= \mathrm{MSE} + \frac{\mathrm{SSR}_{\mathrm{adj}}}{n-1}.
\end{aligned}
$$

where

$$
\mathrm{SSR}_{\mathrm{adj}} = (n-1)MST-(n-p+k)\mathrm{MSE} = \mathrm{SST}-\mathrm{SSE} - k\,\mathrm{MSE} = \mathrm{SSR} - k\,\mathrm{MSE}.
$$

------------------------------------------------------------------------

#### Difference of $\hat{\sigma}^2$ and Mean Squares

The quantity $\hat{\sigma}^2$ represents the **estimated variance** associated with each component of the model. MSE and MST are the estimated variances of the $\epsilon$ and $y$ itself. However, the MSR, although called **Mean Square for Regression (MSR)** is *NOT* an estimate of the variance or sample variance of $x^\top \beta$. The name of "mean" here is used to indicate a different thing. Its name “Mean Square” reflects that it is also an estimate estimate of noise variance $\sigma^2$ under $H_0\!:\,\beta = 0$:

$$
E[\mathrm{MSR} \mid H_0] = \sigma^2,
\qquad 
E[\mathrm{MSR} \mid H_1] > \sigma^2.
$$

Hence the F-statistic

$$
F = \frac{\mathrm{MSR}}{\mathrm{MSE}}
$$ is approximately equal to 1 subject to the variability as characterized with F-distribution with degree freedoms of $k$ and $n-p$. This test is to test whether any regression coefficients are not equal to 0.

------------------------------------------------------------------------

#### $\hat \sigma^2_{x^\top\beta}=\frac{\mathrm{SSR}_{\text{adj}}}{n-1}$

$\hat \sigma^2_{x^\top\beta}$ is an unbiased estimator of the variance of linear signal when $x$ is a regarded as a random variable. This can be seen from the following equations: $$
E[\mathrm{SSR}] = k\,\sigma^2 + \beta^\top X^\top (I - J/n)\,X\,\beta,
\qquad
E[\mathrm{MSE}] = \sigma^2.
$$ Hence, $$
\begin{aligned}
E[\mathrm{SSR}_{\mathrm{adj}}]
&= E[\mathrm{SSR}] - k\,E[\mathrm{MSE}] \\
&= \beta^\top X^\top (I - J/n)\,X\,\beta \\
&= \sum_{i=1}^n (\mu_i - \bar\mu)^2,
\end{aligned}
$$

where $$
\begin{aligned}
\mu_i &= x_i^\top \beta \\
\bar\mu &= \tfrac{1}{n}\sum_{i=1}^n \mu_i
\end{aligned}
$$ For fixed $X$, $\mathrm{SSR}_{\text{adj}}/(n-1)$ equals the **sample variance** of the true means $\{\mu_i\}$ over the observed design points. If the rows of $X$ are independently sampled with covariance matrix $\Sigma_X$ (the random-$X$ model), then

$$
\mathbb{E}_X\!\left[\frac{\mathrm{SSR}_{\text{adj}}}{n-1}\right]
= \beta^\top \Sigma_X \beta
= \mathrm{Var}(x^\top \beta),
$$

#### Connection to Rao-Blackwell Formula

The decomposition of $\hat{\sigma}^2$ is consistent with the **Rao–Blackwell formula** for total variance:

$$
\mathrm{Var}(y) = \mathrm{Var}\!\big(E[y \mid x]\big) + E\!\big(\mathrm{Var}[y \mid x]\big).
$$

Here,

-   $\mathrm{Var}\!\big(E[y \mid x]\big)$ corresponds to the **explained variation** due to the regression component $x^\top\beta$, and\
-   $E\!\big(\mathrm{Var}[y \mid x]\big)$ corresponds to the **residual variation** due to $\epsilon$.

### An Animation for Illustrating $R^2$ under $H_0$ and $H_1$

We simulate a dataset with $n=30$ observations and consider a sequence of nested models adding groups of predictors.

**Predictor Groups:**

1.  **Step 1 ($k=1$):** Add $x_1$. (Signal under $H_1$).
2.  **Step 2 ($k=6$):** Add $x_2, \dots, x_6$ (Noise).
3.  **Step 3 ($k=11$):** Add $x_7, \dots, x_{11}$ (Noise).
4.  **Step 4 ($k=20$):** Add $x_{12}, \dots, x_{20}$ (Noise).

```{r}
#| label: generate-all-animations
#| include: false
#| cache: true

# --- Setup ---
library(ggplot2)
library(patchwork)
require("latex2exp")
set.seed(123)

# knobs
n_frames <- 30
fps      <- 2
n        <- 30
sigma    <- 1
beta1    <- 1

# [FLEXIBLE] Define your steps here. 
# Ensure max(model_steps) < n - 1 to avoid division by zero in MSE/R2_adj
model_steps <- c(0, 1, 6, 11, 20) 

# Automatically determine the maximum number of predictors needed
max_k <- max(model_steps) 

# --- Generator Function ---
generate_animation <- function(scenario = "H0", filename) {
  
  # Ensure directory exists
  dir.create(dirname(filename), showWarnings = FALSE)
  
  # Temp directory for frames
  frame_dir <- file.path("frames_temp", scenario)
  dir.create(frame_dir, recursive = TRUE, showWarnings = FALSE)
  
  # --- 1. Theoretical Limits Setup ---
  
  # A. Y-axis Lower Limit for R^2_adj
  # Worst case: R^2 = 0 with max complexity p = max_k + 1
  max_p <- max_k + 1
  
  # Safety check: if p >= n, theoretical min is undefined (-Inf)
  if (max_p >= n) {
    y_min_limit <- -1 # fallback if user pushes limit
  } else {
    y_min_limit <- -0.2
  }

  # B. Theoretical Variance Expectations
  var_x <- 1 
  signal_var <- if(scenario == "H1") (beta1^2 * var_x) else 0
  
  theory_df <- data.frame(
    k = model_steps,
    expected_mse = ifelse(model_steps == 0, sigma^2 + signal_var, sigma^2)
  )

  # --- Frame Loop ---
  for (j in 1:n_frames) {
    
    # 2. Generate Data [DYNAMIC]
    X <- as.data.frame(replicate(max_k, rnorm(n)))
    names(X) <- paste0("x", 1:max_k)
    
    # Define Beta Vector [DYNAMIC]
    beta_vec <- rep(0, max_k)
    if (scenario == "H1") beta_vec[1] <- beta1 
    
    y <- as.numeric(as.matrix(X) %*% beta_vec + rnorm(n, sd = sigma))
    
    # 3. Fit Sequence of Models
    stats_list <- lapply(model_steps, function(k) {
      if (k == 0) {
        fit <- lm(y ~ 1)
      } else {
        k_safe <- min(k, ncol(X))
        form <- as.formula(paste("y ~", paste(names(X)[1:k_safe], collapse = " + ")))
        fit <- lm(form, data = X)
      }
      
      rss <- sum(residuals(fit)^2)
      p   <- k + 1 
      
      denom_mse <- if(n - p > 0) (n - p) else NA
      
      data.frame(
        k = k,
        p = p,
        MSE = if(!is.na(denom_mse)) rss / denom_mse else NA,
        MLE = rss / n,
        R2 = summary(fit)$r.squared,
        R2_adj = summary(fit)$adj.r.squared
      )
    })
    
    df <- do.call(rbind, stats_list)
    
    # 4. Plotting
    
    # Plot A: Variance Estimators
    g1 <- ggplot(df, aes(x = k)) +
      geom_step(data = theory_df, aes(y = expected_mse, linetype = "Theoretical"), 
                color = "gray40", size = 0.8, direction = "vh") +
      
      geom_line(aes(y = MLE, color = "Naive (MLE)"), size = 1, linetype = "solid") +
      geom_point(aes(y = MLE, color = "Naive (MLE)"), size = 2) +
      
      geom_line(aes(y = MSE, color = "Corrected (MSE)"), size = 1, linetype = "dashed") +
      geom_point(aes(y = MSE, color = "Corrected (MSE)"), size = 2, shape = 17) +
      
      scale_color_manual(values = c("Naive (MLE)" = "firebrick", "Corrected (MSE)" = "blue")) +
      scale_linetype_manual(values = c("Theoretical" = "dotted"), name=NULL) +
      
      scale_x_continuous(breaks = model_steps) +
      coord_cartesian(ylim = c(0, max(theory_df$expected_mse) + 1)) +
      labs(title = paste0("Scenario ", scenario, ": Estimators of Error Variance"),
           y = "Variance Estimate", x = NULL, color = NULL) +
      theme_minimal() + theme(legend.position = "top")

    # Plot B: Model Fit (Updated Layout)
    g2 <- ggplot(df, aes(x = k)) +
      geom_hline(yintercept = 0, color = "black", size = 0.2) +
      
      geom_line(aes(y = R2, color = "Naive (R^2)"), size = 1, linetype = "solid") +
      geom_point(aes(y = R2, color = "Naive (R^2)"), size = 2) +
      
      geom_line(aes(y = R2_adj, color = "Corrected (Adj R^2)"), size = 1, linetype = "dashed") +
      geom_point(aes(y = R2_adj, color = "Corrected (Adj R^2)"), size = 2, shape = 17) +
      
      scale_color_manual(values = c("Naive (R^2)" = "firebrick", "Corrected (Adj R^2)" = "blue")) +
      
      scale_x_continuous(breaks = model_steps) +
      scale_y_continuous(limits = c(y_min_limit, 1), labels = scales::percent) +
      labs(title = "Estimate of R-squared",
           y = "Value", x = "Number of Predictors (k)", color = NULL) +
      theme_minimal() + 
      # MOVE LEGEND TOP & ADD BOTTOM MARGIN
      theme(
        legend.position = "top",
        plot.margin = margin(t = 5, r = 5, b = 40, l = 5, unit = "pt")
      )

    final_plot <- g1 / g2
    ggsave(file.path(frame_dir, sprintf("f%03d.png", j)), final_plot, width = 7, height = 7, dpi = 100)
  }
  
  if (requireNamespace("av", quietly = TRUE)) {
    pngs <- list.files(frame_dir, "\\.png$", full.names = TRUE)
    av::av_encode_video(pngs, filename, framerate = fps, verbose = FALSE)
  }
}

# --- Execution ---
generate_animation("H0", "figs/rss-h0-v5.mp4")
generate_animation("H1", "figs/rss-h1-v5.mp4")
```

::: {.panel-tabset}

#### Null Hypothesis ($H_0$)

Under $H_0$, the true coefficient for $x_1$ is $\beta_1 = 0$. All predictors are noise.

```{r}
#| echo: false
#| fig.align: center

if (knitr::is_html_output()) {
  htmltools::tags$video(
    controls = "controls", width = "100%", 
    htmltools::tags$source(src = "figs/rss-h0-v5.mp4", type = "video/mp4")
  )
} else {
  plot(1, 1, type="n", axes=FALSE, xlab="", ylab="", main="Animation available in HTML version")
  text(1, 1, "View HTML for H0 Animation")
}
```

#### Alternative Hypothesis ($H_1$)

Under $H_1$, $x_1$ is a true predictor ($\beta_1 = 2$). The subsequent groups ($x_2 \dots x_{20}$) remain noise.

```{r}
#| echo: false
#| fig.align: center

if (knitr::is_html_output()) {
  htmltools::tags$video(
    controls = "controls", width = "100%", 
    htmltools::tags$source(src = "figs/rss-h1-v5.mp4", type = "video/mp4")
  )
} else {
  plot(1, 1, type="n", axes=FALSE, xlab="", ylab="", main="Animation available in HTML version")
  text(1, 1, "View HTML for H1 Animation")
}
```

:::

### Example:  Modelling Children Weight with Height and Age

```{r}
## Data: Weight, height and age of children
wgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)
hgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)
age <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)
child.data <- data.frame(wgt, hgt, age)
```

#### Problem 1: Height then Age

```{r}
fit_hgt_age <- lm(wgt ~ hgt + age, data = child.data)
summary(fit_hgt_age)

fit_hgt <- lm(wgt ~ hgt, data = child.data)
summary(fit_hgt)
anova(fit_hgt, fit_hgt_age)
anova(fit_hgt_age)
```

#### Problem 2: Age then Height

```{r}
fit_age <- lm(wgt ~ age, data = child.data)
summary(fit_age)

fit_age_hgt <- lm(wgt ~ age + hgt, data = child.data)
summary(fit_age_hgt)
anova(fit_age, fit_age_hgt)
anova(fit_age_hgt)
```

### Example:  Wire bond strength
```{r}
fit_len_hgt <-  lm(strength ~ length + height, data = bond.data)
fit_hgt_len <-  lm(strength ~ height+length, data = bond.data)
anova(fit_len_hgt)
anova(fit_hgt_len)
summary(fit_hgt_len)
summary(fit_len_hgt)

```

### Relationship between t-test and partial F-test

-   A t-test for a single coefficient is a special case of the partial F-test; the relationship is $F = t^2$ for 1 df in the numerator.
-   The p-value from t-test (output of *summary(lm())*) is the same as anova test for: $H_0: \beta_j = 0$ vs $H_1$: all covaraites have non-zero effects.

## Predictions for Mean Response and a Future Observation

### Confidence Interval for Mean Response

```{r}
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "confidence", level = 0.95)
```

### Prediction Interval for a New Observation

```{r}
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "prediction", level = 0.95)
```

## Model Diagnostics

### Residual Calculations

```{r}
residuals_df <- data.frame(
  hat_values = hatvalues(fit),
  ordinary_resid = resid(fit),
  standardized_resid = resid(fit) / sigma(fit),
  studentized_internal = rstandard(fit),
  studentized_external = rstudent(fit)
)
residuals_df
```

### Residual Plots

```{r}
#| fig-width: 6
#| fig-height: 4
n <- nrow(bond.data)
r <- rstudent(fit) 
y.hat <- fitted.values(fit)

par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
qqnorm(r, main = "Normal Q-Q Plot"); qqline(r)
plot(y.hat, r, xlab = "Fitted values", ylab = "Studentized Residuals"); abline(h = 0)
plot(1:n, r, xlab = "Observation Number", ylab = "Studentized Residuals"); abline(h = 0)
plot(bond.data$length, r, xlab = "Wire Length", ylab = "Studentized Residuals"); abline(h = 0)
plot(bond.data$height, r, xlab = "Die Height", ylab = "Studentized Residuals"); abline(h = 0)
```

## Influential Observations

```{r}
influence_df <- data.frame(dffits = dffits(fit),
                           cook.D = cooks.distance(fit),
                           dfbetas(fit))
influence_df
```

### Plotting with the `olsrr` Package

```{r}
## install.packages("olsrr") # Run once if needed
library(olsrr)

ols_plot_cooksd_chart(fit)
ols_plot_dffits(fit)
ols_plot_dfbetas(fit)
```

## Polynomial Regression

```{r}
y <- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)
x <- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)
fit_poly <- lm(y ~ x + I(x^2))
summary(fit_poly)
```

```{r}
plot(x, y, xlab = "Lot size, x", ylab = "Average cost per unit, y")
lines(x, predict(fit_poly, newdata = data.frame(x = x)), type = "l")
```

```{r}
fit1 <- lm(y ~ x)
anova(fit1, fit_poly)
```

## Handling Categorical Variables with Dummy Variables

Investigate the common observation that males tend to have higher blood pressure than females of similar age.

```{r}
#| echo: true
## Note: Update this path to your local file location
sbpdata <- read.csv("sbpdata.csv")
sbpdata
```

### Four Models Involving "sex"

#### Coincidence Model (Age Only)

```{r}
#| echo: true
## Ensure sex is a factor (labels will appear in the legend)
sbpdata$sex <- as.factor(sbpdata$sex)

## Fit (you already have this)
fit.age <- lm(sbp ~ age, data = sbpdata)

## Generate predictions over the observed age range
new_age <- seq(min(sbpdata$age, na.rm = TRUE),
               max(sbpdata$age, na.rm = TRUE),
               length.out = 200)
pred <- predict(fit.age, newdata = data.frame(age = new_age))

## Simple palette for the sex levels (works for 1–3 levels; expand if needed)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

## Scatter plot with colored points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

## Add predicted line
lines(new_age, pred, lwd = 2)

## Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, bty = "n", title = "Sex")

```

```{r}
#| echo: true
data.frame(model.matrix(fit.age)) 
print(anova(fit.age))
```

#### Additive Effect Model (Age + Sex)

```{r}
#| echo: true
## Parallelism: H0: beta3=0 (Sex has additive effect)
fit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)

## Ensure sex is a factor for labeling/colors
sbpdata$sex <- factor(sbpdata$sex)

## Fit (additive: parallelism)
fit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)

## X-range and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

## Scatter with colored points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

## Parallel fitted lines: one per sex (same slope, different intercepts)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.agePLUSsex, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

## Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

```{r}
#| echo: true
data.frame(model.matrix(fit.agePLUSsex))
print(anova(fit.age, fit.agePLUSsex))
```

#### Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)

```{r}
#| echo: true
## Make sure sex is a factor (for colors/legend)
sbpdata$sex <- factor(sbpdata$sex)

## Fit (interaction: different slopes by sex)
fit.age.TIMES.sex <- lm(sbp ~ age + sex + age:sex, data = sbpdata)

## Age grid and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

## Scatter: color points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

## Fitted lines: one per sex (different slopes allowed)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.age.TIMES.sex, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

## Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

**Model Matrix and ANOVA**

```{r}
#| echo: true
data.frame(model.matrix(fit.age.TIMES.sex))

summary(fit.age.TIMES.sex)

print(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))
```

#### Varying Slope, Equal Intercept Model (Age + Age:Sex)

```{r}
#| echo: true
## Make sure sex is a factor (for colors/legend)
sbpdata$sex <- factor(sbpdata$sex)

## Fit (interaction: different slopes by sex)
fit.equal.intercept <- lm(sbp ~ age + age:sex, data = sbpdata)


## Age grid and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

## Scatter: color points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

## Fitted lines: one per sex (different slopes allowed)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.equal.intercept, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

## Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

### Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables

```{r}
#| echo: true
#| message: true

fit.int <- lm(sbp ~ 1, data = sbpdata)
fit.sex <- lm(sbp ~ sex, data = sbpdata)

print(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))
print(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))
print(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))

summary(fit.age)
summary(fit.equal.intercept)
summary(fit.agePLUSsex)
summary(fit.age.TIMES.sex)


```

## Model Building

```{r}
library(olsrr)
## Note: Update this path to your local file location
wine <- read.csv("wine.csv")

model.wine <- lm(quality ~ ., data = wine)
```

### All Possible Regression

```{r}
ols_step_best_subset(model.wine)
```

### Automated Stepwise Procedures

```{r}
## Backward Elimination (alpha_out = 0.1)
ols_step_backward_p(model.wine, p_val = 0.1)

## Forward Selection (alpha_in = 0.1)
ols_step_forward_p(model.wine, p_val = 0.1)

## Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)
ols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)
```

## Multicollinearity

### A Simple Example

```{r}
y <- c(19, 20, 37, 39, 36, 38)
x1 <- c(4, 4, 7, 7, 7.1, 7.1)
x2 <- c(16, 16, 49, 49, 50.4, 50.4)
cor(data.frame(x1, x2))

fit_multi <- lm(y ~ x1 + x2)
summary(fit_multi)

fit1_multi <- lm(y ~ x1)
summary(fit1_multi)

ols_vif_tol(fit_multi)

```

### VIFs in the Wine Quality Data

```{r}
wine.x <- wine[, -ncol(wine)] # Assuming quality is the last column
cor(wine.x)

## VIF using olsrr (data frame output)
ols_vif_tol(model.wine)
```

### VIFs in the Children Height Data

```{r}
## Data: Weight, height and age of children
wgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)
hgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)
age <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)

fit_age_hgt <- lm(wgt ~ hgt + age, data = child.data)
ols_vif_tol(fit_age_hgt)
```
