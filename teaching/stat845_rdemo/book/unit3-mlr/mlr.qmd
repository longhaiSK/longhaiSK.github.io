---
title: "Multiple Linear Regression"
author:
  - name: "Longhai Li"
    affiliation: "University of Saskatchewan"
    url: "https://longhaisk.github.io/"
date: today
editor: source
editor_options: 
  chunk_output_type: console
---

# An Example: Wire Bond Strength Dataset

## Loading Data and Visualization

**Note:** You must change the file paths in the `read.csv()` functions below to match the location of the files on your computer (for example `C:\\Users\\<YourUsername>\\Documents` on Windows).

```{r}
# Read data. Change the path as necessary.
# Example: bond.data <- read.csv("wire-bond.csv")
bond.data <- read.csv("wire-bond.csv")

# This will now be automatically rendered as a paged table
bond.data
```

```{r, include=FALSE, eval=FALSE}
# Use set.seed() to make sure you get the exact same random data every time
set.seed(42)

# Define the number of data points to create
n <- 150

# Create the bond.data data frame
bond.data <- data.frame(
  length = runif(n, 10, 50),
  height = runif(n, 5, 25)
)

# Create 'strength' based on a linear relationship with the other variables
bond.data$strength <- (2.5 * bond.data$length) + 
                      (4.1 * bond.data$height) + 
                      rnorm(n, mean = 0, sd = 1)

head(bond.data)
```

**2D Visualization**

```{r}
par(mfrow = c(1, 3), mar = c(5, 4, 2, 1))

# 1) length vs strength
i1 <- which(!is.na(bond.data$length) & !is.na(bond.data$strength))
plot(bond.data$length[i1], bond.data$strength[i1],
     xlab = "Wire Length", ylab = "Pull strength", pch = 19)
text(bond.data$length[i1], bond.data$strength[i1],
     labels = i1, pos = 1, offset = 0.4, cex = 0.75)

# 2) height vs strength
i2 <- which(!is.na(bond.data$height) & !is.na(bond.data$strength))
plot(bond.data$height[i2], bond.data$strength[i2],
     xlab = "Die height", ylab = "Pull strength", pch = 19)
text(bond.data$height[i2], bond.data$strength[i2],
     labels = i2, pos = 1, offset = 0.4, cex = 0.75)

# 3) height vs length
i3 <- which(!is.na(bond.data$height) & !is.na(bond.data$length))
plot(bond.data$height[i3], bond.data$length[i3],
     xlab = "Die height", ylab = "Length", pch = 19)
text(bond.data$height[i3], bond.data$length[i3],
     labels = i3, pos = 1, offset = 0.4, cex = 0.75)
```

**3D Visualize**

```{r}
library(scatterplot3d)

par(mfrow = c(1,1))
s3d <- with(bond.data, scatterplot3d(
  x = length,
  y = height,
  z = strength,
  pch = 19,
  color = "steelblue",
  main = "3D Scatterplot: Strength vs. Length and Height",
  xlab = "Length",
  ylab = "Height",
  zlab = "Strength",
  angle = 60
))

fit <- lm(strength ~ length + height, data = bond.data)
s3d$plane3d(fit, lty.box = "solid")
```

## Model Fitting and Summary

We fit a multiple linear regression model with `strength` as the response variable and `length` and `height` as predictors.

```{r}
fit <- lm(strength ~ length + height, data = bond.data)
summary(fit)
```

The summary provides the ANOVA F-test for overall significance, $R^2$, adjusted $R^2$, and t-tests for individual coefficients.

## Confidence Intervals and Model Components

```{r}
# Confidence intervals
confint(fit)

# Fitted values and residuals
pred <- fitted.values(fit)
e <- resid(fit)
data.frame(y = bond.data$strength, y.hat = pred, e = e)

# Covariance matrix and standard errors
cov.mat <- vcov(fit)
cov.mat
data.frame(std.error = sqrt(diag(cov.mat)))
```

# RSS-based Inference: F-test, and adjusted $R^2$

**The General Linear Model**

The general linear model is:

$$y = X\beta + \epsilon$$

-   $y$: $n \times 1$ vector of responses
-   $X$: $n \times p$ design matrix (first column often ones)
-   $\beta$: $p \times 1$ parameter vector, where $p=k+1$
-   $\epsilon$: $n \times 1$ error vector

## RSS-Based Quantities

### RSS-Based Quantities

| Source | Sum of Squares | $R^2$ | df | Mean Squares | $F$ | SS$_\mathrm{adj}$ | $\hat{\sigma}^2$ | $R^2_{\mathrm{adj}}$ |
|:-------|:-------|:-------|:------:|:-------|:-------|:-------|:-------|:-------|
| $x^\top\beta$ | $\mathrm{SSR} = \displaystyle \sum_{i=1}^n (\hat y_i - \bar y)^2$ | $\displaystyle \frac{\mathrm{SSR}}{\mathrm{SST}}$ | $k$ | $\displaystyle \mathrm{MSR} = \frac{\mathrm{SSR}}{k}$ | $\displaystyle \frac{\mathrm{MSR}}{\mathrm{MSE}}$ | $\mathrm{SSR}_{\mathrm{adj}}$ | $\displaystyle \hat{\sigma}^2_{x^\top\beta} = \frac{\mathrm{SSR}_{\mathrm{adj}}}{n-1}$ | $\displaystyle \frac{\mathrm{SSR}_{\mathrm{adj}}}{\mathrm{SST}} = 1 - \frac{\mathrm{MSE}}{\mathrm{MST}}$ |
| $\epsilon$ | $\mathrm{SSE} = \displaystyle \sum_{i=1}^n (y_i - \hat y_i)^2$ | — | $n-p$ | $\displaystyle \mathrm{MSE} = \frac{\mathrm{SSE}}{n-p}$ | — | $\mathrm{SSE}$ | $\displaystyle \hat{\sigma}^2_{\epsilon} = \mathrm{MSE}$ | — |
| $y$ | $\mathrm{SST} = \displaystyle \sum_{i=1}^n (y_i - \bar y)^2$ | — | $n-1$ | $\displaystyle \mathrm{MST} = \frac{\mathrm{SST}}{n-1}$ | — | $\mathrm{SST}$ | $\displaystyle \hat{\sigma}^2_{y} = \mathrm{MST}$ | — |

------------------------------------------------------------------------

**Interpretation of the** $\hat{\sigma}^2$ Column

The $\hat{\sigma}^2$ column highlights how each sum of squares corresponds to an estimated variance.\
This view makes the adjusted coefficient of determination clear:

$$
R^2_{\mathrm{adj}} 
= 1 - \frac{\hat{\sigma}^2_\epsilon}{\hat{\sigma}^2_y}
= \frac{\hat{\sigma}^2_{x^\top\beta}}{\hat{\sigma}^2_y}.
$$

Hence, the adjusted $R^2$ simply expresses the **proportion of total estimated variance** attributable to the fitted model $X\beta$ rather than the residual noise $\epsilon$.

## Remarks

### Fundamental Identities

$$
\begin{aligned}
\mathrm{SST} &= \mathrm{SSR} + \mathrm{SSE}, \\
\mathrm{MST} &= \mathrm{MSE} + \frac{\mathrm{SSR}_{\mathrm{adj}}}{n-1}.
\end{aligned}
$$

where

$$
\mathrm{SSR}_{\mathrm{adj}} = (n-1)MST-(n-p+k)\mathrm{MSE} = \mathrm{SST}-\mathrm{SSE} - k\,\mathrm{MSE} = \mathrm{SSR} - k\,\mathrm{MSE}.
$$

------------------------------------------------------------------------

### Difference of $\hat{\sigma}^2$ and Mean Squares

The quantity $\hat{\sigma}^2$ represents the **estimated variance** associated with each component of the model. MSE and MST are the estimated variances of the $\epsilon$ and $y$ itself. However, the MSR, although called **Mean Square for Regression (MSR)** is *NOT* an estimate of the variance or sample variance of $x^\top \beta$. The name of "mean" here is used to indicate a different thing. Its name “Mean Square” reflects that it is also an estimate estimate of noise variance $\sigma^2$ under $H_0\!:\,\beta = 0$:

$$
E[\mathrm{MSR} \mid H_0] = \sigma^2,
\qquad 
E[\mathrm{MSR} \mid H_1] > \sigma^2.
$$

Hence the F-statistic

$$
F = \frac{\mathrm{MSR}}{\mathrm{MSE}}
$$ is approximately equal to 1 subject to the variability as characterized with F-distribution with degree freedoms of $k$ and $n-p$. This test is to test whether any regression coefficients are not equal to 0.

------------------------------------------------------------------------

### $\hat \sigma^2_{x^\top\beta}=\frac{\mathrm{SSR}_{\text{adj}}}{n-1}$

$\hat \sigma^2_{x^\top\beta}$ is an unbiased estimator of the variance of linear signal when $x$ is a regarded as a random variable. This can be seen from the following equations: $$
E[\mathrm{SSR}] = k\,\sigma^2 + \beta^\top X^\top (I - J/n)\,X\,\beta,
\qquad
E[\mathrm{MSE}] = \sigma^2.
$$ Hence, $$
\begin{aligned}
E[\mathrm{SSR}_{\mathrm{adj}}]
&= E[\mathrm{SSR}] - k\,E[\mathrm{MSE}] \\
&= \beta^\top X^\top (I - J/n)\,X\,\beta \\
&= \sum_{i=1}^n (\mu_i - \bar\mu)^2,
\end{aligned}
$$

where $$
\begin{aligned}
\mu_i &= x_i^\top \beta \\
\bar\mu &= \tfrac{1}{n}\sum_{i=1}^n \mu_i
\end{aligned}
$$ For fixed $X$, $\mathrm{SSR}_{\text{adj}}/(n-1)$ equals the **sample variance** of the true means $\{\mu_i\}$ over the observed design points. If the rows of $X$ are independently sampled with covariance matrix $\Sigma_X$ (the random-$X$ model), then

$$
\mathbb{E}_X\!\left[\frac{\mathrm{SSR}_{\text{adj}}}{n-1}\right]
= \beta^\top \Sigma_X \beta
= \mathrm{Var}(x^\top \beta),
$$

### Connection to Rao-Blackwell Formula

The decomposition of $\hat{\sigma}^2$ is consistent with the **Rao–Blackwell formula** for total variance:

$$
\mathrm{Var}(y) = \mathrm{Var}\!\big(E[y \mid x]\big) + E\!\big(\mathrm{Var}[y \mid x]\big).
$$

Here,

-   $\mathrm{Var}\!\big(E[y \mid x]\big)$ corresponds to the **explained variation** due to the regression component $x^\top\beta$, and\
-   $E\!\big(\mathrm{Var}[y \mid x]\big)$ corresponds to the **residual variation** due to $\epsilon$.

## A Simulation Study to Understand the Distributions of RSS

**Data Generating Model**

For $n=30$ and $p_{max}=20$, simulate with either $H_0:\beta=\mathbf 0$ or $H_1$ where only $\beta_1\neq 0$; $\epsilon_i\sim N(0,1)$.

**Sequence of Fitted Models**

| Model Name | \# of Predictors (k) | \# of Parameters (p) | R Formula |
|:----------------|:----------------:|:----------------:|:--------------------|
| Model 0 | 0 | 1 | `y ~ 1` |
| Model 1 | 2 | 3 | `y ~ x_1 + x_2` |
| ... | ... | ... | ... |
| Final Model | 20 | 21 | `y ~ x_1 + ... + x_20` |

```{r, include=FALSE}
require("latex2exp")
require("animation")
```

### When $H_0$ is true

```{r}
#| include: false
# --- Setup & params ---
library(ggplot2); library(patchwork)
dir.create("frames0", showWarnings = FALSE)

N_sim <- ifelse(interactive(), 1, 50)
n <- 30; p_max <- 20
beta <- rep(0, p_max); sigma <- 1
x_lim <- c(0, n); ks <- seq(0, p_max, by = 2)

# --- Generate frames ---
for (j in seq_len(N_sim)) {
  X <- as.data.frame(replicate(p_max, rnorm(n)))
  names(X) <- paste0("x", 1:p_max)
  y <- as.numeric(as.matrix(X) %*% beta + rnorm(n, sd = sigma))

  stats_list <- lapply(ks, function(k) {
    num_params <- k + 1
    fitsim <- if (k == 0) lm(y ~ 1) else lm(reformulate(paste0("x", 1:k), "y"), data = X)
    s <- summary(fitsim)
    f_val <- s$fstatistic
    data.frame(
      p = num_params,
      RSS = sum(residuals(fitsim)^2),
      R2 = ifelse(k == 0, 0, s$r.squared),
      R2_adj = ifelse(k == 0, 0, s$adj.r.squared),
      F_stat = ifelse(is.null(f_val), NA_real_, f_val[1])
    )
  })
  df <- do.call(rbind, stats_list)
  df <- rbind(df, data.frame(p = n, RSS = 0, R2 = 1, R2_adj = NA_real_, F_stat = NA_real_))

  g1 <- ggplot(df, aes(p, RSS)) +
    geom_line() + geom_point() +
    geom_point(data = subset(df, p == n), size = 2) +
    annotate("text", x = n, y = 0, label = "(n, 0)", vjust = -0.8, size = 3) +
    scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
    labs(title = "Residual Sum of Squares vs. Number of Parameters",
         x = "Number of parameters (p)", y = "Residual Sum of Squares (RSS)") +
    theme_minimal(base_size = 13)

  scale_factor <- max(df$F_stat, na.rm = TRUE); if (!is.finite(scale_factor) || scale_factor <= 0) scale_factor <- 1
  g2 <- ggplot(df, aes(p)) +
    geom_line(aes(y = F_stat, color = "F")) + geom_point(aes(y = F_stat, color = "F")) +
    geom_line(aes(y = R2 * scale_factor, color = "R2")) +
    geom_point(aes(y = R2 * scale_factor, color = "R2")) +
    geom_line(aes(y = R2_adj * scale_factor, color = "R2_adj"), linetype = "dashed") +
    geom_point(aes(y = R2_adj * scale_factor, color = "R2_adj")) +
    scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
    scale_y_continuous(
      name = "F-statistic",
      sec.axis = sec_axis(~ . / scale_factor,
                          name = expression(paste(R^2," / ",R[adj]^2)),
                          labels = scales::percent)
    ) +
    scale_color_manual(values = c(F = "firebrick", R2 = "blue", R2_adj = "cyan4"), name = "Metric") +
    labs(title = "Model Fit Statistics vs. Number of Parameters", x = "Number of parameters (p)") +
    theme_minimal(base_size = 13) + theme(legend.position = "top")

  ggsave(sprintf("frames0/f%03d.png", j), g1 / g2, width = 8, height = 8, dpi = 150)
}
library(av)
pngs <- list.files("frames0", "\\.png$", full.names = TRUE); pngs <- pngs[order(pngs)]
av_encode_video(pngs, "rss-h0.mp4", framerate = 1, verbose = FALSE)  # H.264 MP4
```

```{r results='asis'}
cat(sprintf('<video controls style="width:100%%;height:auto;"><source src="%s" type="video/mp4"></video>',"rss-h0.mp4"))

```

### When $H_1$ is true

```{r}
#| include: false
# --- Setup & params ---
library(ggplot2); library(patchwork)
dir.create("frames1", showWarnings = FALSE)

N_sim <- ifelse(interactive(), 1, 50)
n <- 30; p_max <- 20
beta <- rep(0, p_max); sigma <- 1
beta [1] <- 1
x_lim <- c(0, n); ks <- seq(0, p_max, by = 2)

# --- Generate frames ---
for (j in seq_len(N_sim)) {
  X <- as.data.frame(replicate(p_max, rnorm(n)))
  names(X) <- paste0("x", 1:p_max)
  y <- as.numeric(as.matrix(X) %*% beta + rnorm(n, sd = sigma))

  stats_list <- lapply(ks, function(k) {
    num_params <- k + 1
    fitsim <- if (k == 0) lm(y ~ 1) else lm(reformulate(paste0("x", 1:k), "y"), data = X)
    s <- summary(fitsim)
    f_val <- s$fstatistic
    data.frame(
      p = num_params,
      RSS = sum(residuals(fitsim)^2),
      R2 = ifelse(k == 0, 0, s$r.squared),
      R2_adj = ifelse(k == 0, 0, s$adj.r.squared),
      F_stat = ifelse(is.null(f_val), NA_real_, f_val[1])
    )
  })
  df <- do.call(rbind, stats_list)
  df <- rbind(df, data.frame(p = n, RSS = 0, R2 = 1, R2_adj = NA_real_, F_stat = NA_real_))

  g1 <- ggplot(df, aes(p, RSS)) +
    geom_line() + geom_point() +
    geom_point(data = subset(df, p == n), size = 2) +
    annotate("text", x = n, y = 0, label = "(n, 0)", vjust = -0.8, size = 3) +
    scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
    labs(title = "Residual Sum of Squares vs. Number of Parameters",
         x = "Number of parameters (p)", y = "Residual Sum of Squares (RSS)") +
    theme_minimal(base_size = 13)

  scale_factor <- max(df$F_stat, na.rm = TRUE); if (!is.finite(scale_factor) || scale_factor <= 0) scale_factor <- 1
  g2 <- ggplot(df, aes(p)) +
    geom_line(aes(y = F_stat, color = "F")) + geom_point(aes(y = F_stat, color = "F")) +
    geom_line(aes(y = R2 * scale_factor, color = "R2")) +
    geom_point(aes(y = R2 * scale_factor, color = "R2")) +
    geom_line(aes(y = R2_adj * scale_factor, color = "R2_adj"), linetype = "dashed") +
    geom_point(aes(y = R2_adj * scale_factor, color = "R2_adj")) +
    scale_x_continuous(limits = x_lim, breaks = pretty(x_lim)) +
    scale_y_continuous(
      name = "F-statistic",
      sec.axis = sec_axis(~ . / scale_factor,
                          name = expression(paste(R^2," / ",R[adj]^2)),
                          labels = scales::percent)
    ) +
    scale_color_manual(values = c(F = "firebrick", R2 = "blue", R2_adj = "cyan4"), name = "Metric") +
    labs(title = "Model Fit Statistics vs. Number of Parameters", x = "Number of parameters (p)") +
    theme_minimal(base_size = 13) + theme(legend.position = "top")

  ggsave(sprintf("frames1/f%03d.png", j), g1 / g2, width = 8, height = 8, dpi = 150)
}
library(av)
pngs <- list.files("frames1", "\\.png$", full.names = TRUE); pngs <- pngs[order(pngs)]
av_encode_video(pngs, "rss-h1.mp4", framerate = 1, verbose = FALSE)  # H.264 MP4
```

```{r results='asis'}
cat(sprintf('<video controls style="width:100%%;height:auto;"><source src="%s" type="video/mp4"></video>',"rss-h1.mp4"))

```

## Example

```{r}
# Data: Weight, height and age of children
wgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)
hgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)
age <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)
child.data <- data.frame(wgt, hgt, age)
```

### Problem 1: Height then Age

```{r}
fit_age_hgt <- lm(wgt ~ hgt + age, data = child.data)
summary(fit_age_hgt)

fit_hgt <- lm(wgt ~ hgt, data = child.data)
summary(fit_hgt)
anova(fit_hgt, fit_age_hgt)
```

### Problem 2: Age then Height

```{r}
fit_age <- lm(wgt ~ age, data = child.data)
summary(fit_age)

fit_age_hgt <- lm(wgt ~ age + hgt, data = child.data)
summary(fit_age_hgt)
anova(fit_age, fit_age_hgt)
```

## Relationship between t-test and partial F-test

-   A t-test for a single coefficient is a special case of the partial F-test; the relationship is $F = t^2$ for 1 df in the numerator.
-   The p-value from t-test (output of *summary(lm())*) is the same as anova test for: $H_0: \beta_j = 0$ vs $H_1$: all covaraites have non-zero effects.

# Predictions for Mean Response and a Future Observation

## Confidence Interval for Mean Response

```{r}
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "confidence", level = 0.95)
```

## Prediction Interval for a New Observation

```{r}
predict(fit, newdata = data.frame(length = 8, height = 275),
        interval = "prediction", level = 0.95)
```

# Model Diagnostics

## Residual Calculations

```{r}
residuals_df <- data.frame(
  hat_values = hatvalues(fit),
  ordinary_resid = resid(fit),
  standardized_resid = resid(fit) / sigma(fit),
  studentized_internal = rstandard(fit),
  studentized_external = rstudent(fit)
)
residuals_df
```

## Residual Plots

```{r}
n <- nrow(bond.data)
r <- rstudent(fit) 
y.hat <- fitted.values(fit)

par(mfrow = c(2, 3))
qqnorm(r, main = "Normal Q-Q Plot"); qqline(r)
plot(y.hat, r, xlab = "Fitted values", ylab = "Studentized Residuals"); abline(h = 0)
plot(1:n, r, xlab = "Observation Number", ylab = "Studentized Residuals"); abline(h = 0)
plot(bond.data$length, r, xlab = "Wire Length", ylab = "Studentized Residuals"); abline(h = 0)
plot(bond.data$height, r, xlab = "Die Height", ylab = "Studentized Residuals"); abline(h = 0)
```

# Influential Observations

```{r}
influence_df <- data.frame(dffits = dffits(fit),
                           cook.D = cooks.distance(fit),
                           dfbetas(fit))
influence_df
```

## Plotting with the `olsrr` Package

```{r}
# install.packages("olsrr") # Run once if needed
library(olsrr)

ols_plot_cooksd_chart(fit)
ols_plot_dffits(fit)
ols_plot_dfbetas(fit)
```

# Polynomial Regression

```{r}
y <- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)
x <- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)
fit_poly <- lm(y ~ x + I(x^2))
summary(fit_poly)
```

```{r}
plot(x, y, xlab = "Lot size, x", ylab = "Average cost per unit, y")
lines(x, predict(fit_poly, newdata = data.frame(x = x)), type = "l")
```

```{r}
fit1 <- lm(y ~ x)
anova(fit1, fit_poly)
```

# Handling Categorical Variables with Dummy Variables

Investigate the common observation that males tend to have higher blood pressure than females of similar age.

```{r}
#| echo: true
# Note: Update this path to your local file location
sbpdata <- read.csv("sbpdata.csv")
sbpdata
```

## Four Models Involving "sex"

### Coincidence Model (Age Only)

```{r}
#| echo: true
# Ensure sex is a factor (labels will appear in the legend)
sbpdata$sex <- as.factor(sbpdata$sex)

# Fit (you already have this)
fit.age <- lm(sbp ~ age, data = sbpdata)

# Generate predictions over the observed age range
new_age <- seq(min(sbpdata$age, na.rm = TRUE),
               max(sbpdata$age, na.rm = TRUE),
               length.out = 200)
pred <- predict(fit.age, newdata = data.frame(age = new_age))

# Simple palette for the sex levels (works for 1–3 levels; expand if needed)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

# Scatter plot with colored points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

# Add predicted line
lines(new_age, pred, lwd = 2)

# Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, bty = "n", title = "Sex")

```

```{r}
#| echo: true
data.frame(model.matrix(fit.age)) 
print(anova(fit.age))
```

### Additive Effect Model (Age + Sex)

```{r}
#| echo: true
# Parallelism: H0: beta3=0 (Sex has additive effect)
fit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)

# Ensure sex is a factor for labeling/colors
sbpdata$sex <- factor(sbpdata$sex)

# Fit (additive: parallelism)
fit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)

# X-range and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

# Scatter with colored points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

# Parallel fitted lines: one per sex (same slope, different intercepts)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.agePLUSsex, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

# Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

```{r}
#| echo: true
data.frame(model.matrix(fit.agePLUSsex))
print(anova(fit.age, fit.agePLUSsex))
```

### Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)

```{r}
#| echo: true
# Make sure sex is a factor (for colors/legend)
sbpdata$sex <- factor(sbpdata$sex)

# Fit (interaction: different slopes by sex)
fit.age.TIMES.sex <- lm(sbp ~ age + sex + age:sex, data = sbpdata)

# Age grid and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

# Scatter: color points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

# Fitted lines: one per sex (different slopes allowed)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.age.TIMES.sex, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

# Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

**Model Matrix and ANOVA**

```{r}
#| echo: true
data.frame(model.matrix(fit.age.TIMES.sex))

summary(fit.age.TIMES.sex)

print(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))
```

### Varying Slope, Equal Intercept Model (Age + Age:Sex)

```{r}
#| echo: true
# Make sure sex is a factor (for colors/legend)
sbpdata$sex <- factor(sbpdata$sex)

# Fit (interaction: different slopes by sex)
fit.equal.intercept <- lm(sbp ~ age + age:sex, data = sbpdata)


# Age grid and palette
ages <- seq(min(sbpdata$age, na.rm = TRUE),
            max(sbpdata$age, na.rm = TRUE),
            length.out = 200)
lev  <- levels(sbpdata$sex)
cols <- setNames(c("steelblue3", "tomato3", "darkorchid3")[seq_along(lev)], lev)

# Scatter: color points by sex
plot(sbp ~ age, data = sbpdata,
     col = cols[sbpdata$sex], pch = 16,
     xlab = "Age", ylab = "Systolic BP")

# Fitted lines: one per sex (different slopes allowed)
for (sx in lev) {
  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))
  yhat <- predict(fit.equal.intercept, newdata = nd)
  lines(ages, yhat, col = cols[sx], lwd = 2)
}

# Legend
legend("topleft", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = "n", title = "Sex")

```

## Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables

```{r}
#| echo: true
#| message: true

fit.int <- lm(sbp ~ 1, data = sbpdata)
fit.sex <- lm(sbp ~ sex, data = sbpdata)

print(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))
print(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))
print(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))

summary(fit.age)
summary(fit.equal.intercept)
summary(fit.agePLUSsex)
summary(fit.age.TIMES.sex)


```

# Model Building

```{r}
library(olsrr)
# Note: Update this path to your local file location
wine <- read.csv("wine.csv")

model.wine <- lm(quality ~ ., data = wine)
```

## All Possible Regression

```{r}
ols_step_best_subset(model.wine)
```

## Automated Stepwise Procedures

```{r}
# Backward Elimination (alpha_out = 0.1)
ols_step_backward_p(model.wine, p_val = 0.1)

# Forward Selection (alpha_in = 0.1)
ols_step_forward_p(model.wine, p_val = 0.1)

# Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)
ols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)
```

# Multicollinearity

## A Simple Example

```{r}
y <- c(19, 20, 37, 39, 36, 38)
x1 <- c(4, 4, 7, 7, 7.1, 7.1)
x2 <- c(16, 16, 49, 49, 50.4, 50.4)
cor(data.frame(x1, x2))

fit_multi <- lm(y ~ x1 + x2)
summary(fit_multi)

fit1_multi <- lm(y ~ x1)
summary(fit1_multi)

ols_vif_tol(fit_multi)

```

## VIFs in the Wine Quality Data

```{r}
wine.x <- wine[, -ncol(wine)] # Assuming quality is the last column
cor(wine.x)

# VIF using olsrr (data frame output)
ols_vif_tol(model.wine)
```

## VIFs in the Children Height Data

```{r}
# Data: Weight, height and age of children
wgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)
hgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)
age <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)

fit_age_hgt <- lm(wgt ~ hgt + age, data = child.data)
ols_vif_tol(fit_age_hgt)
```
