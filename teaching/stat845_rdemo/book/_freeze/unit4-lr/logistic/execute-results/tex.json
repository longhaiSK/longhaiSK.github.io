{
  "hash": "4b0cf720aa42ff24f0b7e1caab3f7fb2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression\"\nauthor:\n  - name: \"Longhai Li\"\n    url: \"https://longhaisk.github.io/\"\ndate: today\nfrom: markdown+tex_math_single_backslash+tex_math_dollars\nexecute:\n  echo: true\n  warning: false\n  message: false\n  cache: false\n  fig-width: 9\n  fig-height: 6\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n## Odds as a Function of Probability\n\nFor an event with probability \\(p\\), the odds is \n\\[\\mathrm{odds}(p)=\\frac{p}{1-p}\\] \nand the log-odds (logit) is \n\\[\\mathrm{logit}(p)=\\log\\left(\\frac{p}{1-p}\\right)\\]. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Plot odds(p) with a right-hand axis for log(odds(p)),\n## using different line colors for the two curves.\n## Defaults: p in [0.01, 0.99].\n## Args:\n##   p_min, p_max : endpoints for p-grid (0<p_min<p_max<1)\n##   n            : number of grid points\n##   annotate     : add reference lines/labels if TRUE\n##   odds_col     : color for odds(p)\n##   logit_col    : color for log(odds(p))\n##   lwd1, lwd2   : line widths for the two curves\n\n\nplot_odds <- function(p_min = 0.01, p_max = 0.99, n = 400,\n                      annotate = TRUE,\n                      odds_col = \"steelblue\",\n                      logit_col = \"firebrick\",\n                      lwd1 = 2, lwd2 = 2) {\n  stopifnot(p_min > 0, p_max < 1, p_min < p_max, n >= 10)\n  p <- seq(p_min, p_max, length.out = n)\n  odds <- p / (1 - p)\n  logit <- log(odds)\n\n  ## Left y-axis: odds(p)\n  plot(p, odds, type = \"l\", lwd = lwd1, col = odds_col,\n       xlab = \"Probability p\",\n       ylab = \"odds(p) = p / (1 - p)\")\n  if (annotate) {\n    abline(h = 1, v = 0.5, lty = 2)\n    text(0.52, 1.05, \"p = 0.5 â†’ odds = 1\", adj = 0)\n  }\n\n  ## Right y-axis: logit(p) = log(odds)\n  op <- par(new = TRUE)\n  on.exit(par(op), add = TRUE)\n  plot(p, logit, type = \"l\", lwd = lwd2, col = logit_col,\n       axes = FALSE, xlab = \"\", ylab = \"\")\n  axis(4)\n  mtext(\"log{odds(p)} = log{p/(1 - p)}\", side = 4, line = 3)\n\n  if (annotate) {\n    abline(v = 0.5, lty = 2)\n    # logit(0.5) = 0 reference (horizontal) on the right-axis scale\n    usr <- par(\"usr\")\n    segments(x0 = usr[1], y0 = 0, x1 = 0.5, y1 = 0, lty = 3)\n  }\n\n  legend(\"topleft\",\n         legend = c(\"odds(p)\", \"log{odds(p)}\"),\n         col = c(odds_col, logit_col),\n         lwd = c(lwd1, lwd2), bty = \"n\")\n\n  invisible(list(p = p, odds = odds, logit = logit))\n}\n\n## Example usage:\n## plot_odds()  # defaults: steelblue for odds, firebrick for log-odds (right axis)\nplot_odds(odds_col = \"#1f77b4\", logit_col = \"#d62728\", n = 600)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/plot-odds-colored-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nLogistic regression models **log-odds** linearly in predictors, which both keeps fitted probabilities in \\((0,1)\\) and turns multiplicative effects on odds into **additive** effects on the linear predictor.\n\n\n---\n\n## A Simulated Data \n\nWe simulate data from a logistic model where the **logit** is a linear function of $x$:\n\n$$\n\\operatorname{logit}{p(x)}\n=\n\\log\\left(\\frac{p(x)}{1-p(x)}\\right)\n=\n\\beta_0 + \\beta_1 x,\n$$\n\nso that\n\n$$\np(x)\n=\n\\operatorname{logit}^{-1}(\\beta_0+\\beta_1 x)\n=\n\\frac{1}{1+\\exp{-(\\beta_0+\\beta_1 x)}}.\n$$\n\nWe then display the observed $y_i$ (binary outcomes) and the true probability curve $p(x)$ in red.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n## -- Truth (edit as desired) --\nn     <- 200\nbeta0 <- 0\nbeta1 <-  4\n\n## -- Simulate --\nx   <- runif(n, -1, 1)             # predictor\neta <- beta0 + beta1 * x\np   <- plogis(eta)                 # true p(x)\ny   <- rbinom(n, size = 1, prob = p) # outcomes\n\nsim.data <- data.frame(x = x, y = y, p = p)\n```\n:::\n\n\n\n\n### Fit a logistic model to the simulated data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## -- Optional: fit a model to the simulated data --\nsim.fit <- glm(y ~ x, data = sim.data, family = binomial())\np_fit <- predict(sim.fit, newdata = data.frame(x = x), type = \"response\")\n\n## -- Plot: points for y_i (jittered), red line for true p(x) --\n## Define jitter amount\njit <- 0.05 \n## jitter to separate 0/1 visually\nyj <- jitter(sim.data$y, amount = jit) \n\nplot(sim.data$x, yj,\n     pch = 16, col = rgb(0, 0, 0, 0.45),\n     xlab = \"x\",\n     ylab = \"Observed y (points) & p(x) (curves)\",\n     ylim = c(-0.1, 1.1))\n\n## True probability curve (red)\nxg <- seq(min(x), max(x), length.out = 500)\nlines(xg, plogis(beta0 + beta1 * xg), col = \"red\", lwd = 2)\n\n## Optional: add fitted probability curve (dashed dark red)\nlines(xg, predict(sim.fit, newdata = data.frame(x = xg), type = \"response\"),\n      col = \"darkred\", lwd = 2, lty = 2)\n\nlegend(\"topleft\",\n       legend = c(\"y (jittered points)\", \"true p(x)\", \"fitted p(x)\"),\n       pch    = c(16, NA, NA),\n       lty    = c(NA, 1, 2),\n       col    = c(rgb(0,0,0,0.45), \"red\", \"darkred\"),\n       lwd    = c(NA, 2, 2),\n       bty    = \"n\")\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Example of Coronary Heart Disease Data\n\n### Load a dataset\nThis dataset is about a follow-up study to determine the development of coronary heart disease (CHD) over 9 years of follow-up of 609 white males from Evans County, Georgia.\n\n**Variable meanings (as provided):**\n\n* `chd`: 1 if a person has the disease, 0 otherwise.\n* `smk`: 1 if smoker, 0 if not.\n* `cat`: 1 if catecholamine level is high, 0 if low.\n* `sbp`: systolic blood pressure (continuous).\n* `age`: age in years (continuous).\n* `chl`: cholesterol level (continuous).\n* `ecg`: 1 if electrocardiogram is abnormal, 0 if normal.\n* `hpt`: 1 if high blood pressure, 0 if normal.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Adjust the path if needed. The default is your original V: drive path.\ndata_path <- \"evans.dat\"\n\n## Read data (expects a header row)\nCHD.data <- read.table(data_path, header = TRUE)\n\nCHD.data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id chd age cat chl dbp ecg sbp smk hpt\n1      21   0  56   0 270  80   0 138   0   0\n2      31   0  43   0 159  74   0 128   1   0\n3      51   1  56   1 201 112   1 164   1   1\n4      71   0  64   1 179 100   0 200   1   1\n5      74   0  49   0 243  82   0 145   1   0\n6      91   0  46   0 252  88   0 142   1   0\n7     111   1  52   0 179  80   1 128   1   0\n8     131   0  63   0 217  92   0 135   0   0\n9     141   0  42   0 176  76   0 114   1   0\n10    191   0  55   0 250 114   1 182   0   1\n11    201   0  74   0 293 100   0 166   0   1\n12    241   0  53   0 179  90   0 158   0   0\n13    251   0  58   0 201  86   0 142   1   0\n14    261   0  56   0 206  85   0 120   1   0\n15    271   0  69   0 225  84   0 168   0   1\n16    283   1  51   1 259 102   1 135   0   1\n17    291   0  43   0 193  78   0 118   1   0\n18    311   0  64   1 185 100   1 180   0   1\n19    312   0  44   0 150 108   0 160   0   1\n20    331   0  42   0 211  86   1 122   0   0\n21    351   0  57   0 216  88   0 130   0   0\n22    381   1  64   1 247  75   1 130   0   0\n23    401   0  49   0 200  82   0 130   0   0\n24    411   0  68   1 205  74   0 152   1   0\n25    431   0  41   0 225  98   0 135   1   1\n26    441   0  64   0 263  98   0 162   1   1\n27    451   0  41   0 205  80   0 120   0   0\n28    481   0  59   0 253  98   0 154   0   1\n29    501   0  50   0 282  90   0 142   1   0\n30    521   0  56   0 230  80   0 118   0   0\n31    541   0  57   1 203 112   0 182   0   1\n32    561   0  42   0 211  86   0 144   0   0\n33    571   0  59   0 234  84   0 164   1   1\n34    581   0  44   0 202  94   1 174   1   1\n35    611   0  52   0 162  78   0 134   1   0\n36    621   0  45   0 191  85   0 135   0   0\n37    641   0  41   0 220 110   0 178   0   1\n38    651   0  59   0 240  80   0 130   0   0\n39    671   0  52   0 189 110   0 168   0   1\n40    681   0  64   0 247 102   0 170   0   1\n41    731   0  46   0 181 122   1 176   1   1\n42    741   0  42   0 168  75   0 104   1   0\n43    751   0  54   0 187  86   0 146   1   0\n44    761   0  48   0 196  98   0 130   0   1\n45    811   0  45   0 155  70   0 142   1   0\n46    851   0  66   1 173 100   0 160   1   1\n47    861   0  41   0 138  70   0 115   1   0\n48    871   0  76   0 269  94   0 175   1   1\n49    881   1  49   0 266 102   0 152   1   1\n50    921   0  57   1 200 100   0 160   1   1\n51    941   0  51   0 188  84   0 124   1   0\n52    961   1  43   0 218 108   1 136   1   1\n53    971   0  43   0 212  80   1 108   1   0\n54    981   0  45   0 212 102   0 150   1   1\n55    991   0  45   0 180  80   0 122   1   0\n56   1061   1  46   1 166  76   1 162   0   1\n57   1071   0  40   0 257  84   0 130   0   0\n58   1081   0  48   0 243  82   1 154   1   0\n59   1091   0  64   1 179 100   1 148   1   1\n60   1111   0  70   0 167  64   0 112   1   0\n61   1151   0  52   0 178  84   1 112   1   0\n62   1171   0  55   0 178  94   0 152   0   0\n63   1181   0  49   0 211  68   0 114   1   0\n64   1191   1  56   0 171  85   0 125   1   0\n65   1201   1  66   1 205  80   0 150   1   0\n66   1221   0  48   0 229 130   0 195   1   1\n67   1231   0  47   0 238 120   1 160   1   1\n68   1471   0  54   1 195 112   0 174   1   1\n69   1501   0  44   0 162  82   0 120   0   0\n70   1561   0  51   0 240  84   1 126   1   0\n71   1691   0  43   0 177 102   1 138   1   1\n72   1701   0  68   0 252  88   1 112   1   0\n73   1741   0  49   0 217 105   0 148   0   1\n74   1751   0  55   0 263  84   0 114   0   0\n75   1761   0  51   0 229 100   0 162   1   1\n76   1791   0  50   0 245  96   0 144   0   1\n77   1811   0  65   0 177  74   0 122   0   0\n78   1821   0  42   0 203  78   0 134   1   0\n79   1851   0  57   0 194  75   1 114   0   0\n80   1881   0  42   0 288 110   0 142   0   1\n81   1891   0  53   0 217  70   0 120   1   0\n82   1901   0  57   1 163  94   0 184   0   1\n83   1911   0  61   0 180  84   0 136   0   0\n84   1951   0  53   0 209  98   0 142   1   1\n85   1961   0  45   0 200  80   0 135   0   0\n86   1971   0  44   0 194  80   0 120   1   0\n87   2241   0  63   0 227  90   1 135   0   0\n88   2252   0  42   0 158  92   0 135   1   0\n89   2273   0  73   1 183 120   1 220   0   1\n90   2281   0  47   0 253 110   0 140   1   1\n91   2311   0  56   0 198  88   0 122   1   0\n92   2371   1  41   0 228 132   0 162   1   1\n93   2381   0  58   0 217  86   0 140   0   0\n94   2391   0  55   0 163  70   0 110   1   0\n95   2401   0  46   0 212 124   0 184   1   1\n96   2461   0  57   0 144  95   0 130   0   1\n97   2481   0  44   0 134  74   0 114   1   0\n98   2501   0  52   1 183  96   0 158   1   1\n99   2511   0  56   0 212 108   0 144   0   1\n100  2531   0  64   0 214  82   0 128   1   0\n101  2541   0  54   0 249  92   0 120   1   0\n102  2571   0  52   0 180  78   1 104   1   0\n103  2591   0  42   0 212  92   0 125   1   0\n104  2611   0  46   0 167  82   0 120   1   0\n105  2621   0  46   0 273  94   0 152   0   0\n106  2631   0  42   0 210  96   0 134   1   1\n107  2641   0  54   1 173 110   0 170   1   1\n108  2671   0  43   0 256  72   0 114   1   0\n109  2681   0  53   0 234  80   0 122   0   0\n110  2691   1  40   0 221 100   0 140   1   1\n111  2711   0  46   0 261  86   0 128   1   0\n112  2731   0  43   0 299  80   0 116   0   0\n113  2851   0  43   0 192  75   0 115   1   0\n114  2861   0  47   0 185  80   1 146   1   0\n115  2871   0  44   0 283  70   0 108   1   0\n116  2881   0  49   0 176  92   0 134   1   0\n117  2891   1  56   1 331 110   0 190   1   1\n118  2901   1  56   0 203  82   0 120   1   0\n119  2911   0  64   1 217  92   0 166   1   1\n120  2921   0  54   0 164  72   0 122   1   0\n121  2931   0  54   0 256  98   0 148   0   1\n122  2991   0  51   0 184  98   0 170   0   1\n123  3001   0  49   0 165  80   0 114   1   0\n124  3011   0  47   0 189  92   0 145   0   0\n125  3031   0  58   0 221  88   0 140   1   0\n126  3061   0  70   1 126  66   1 164   1   1\n127  3601   0  42   0 169  80   1 122   1   0\n128  3611   0  59   0 266  92   0 138   0   0\n129  3621   0  57   1 153  92   0 148   1   0\n130  3651   0  76   1 211 114   1 228   1   1\n131  3661   0  43   0 113  76   0 114   1   0\n132  3701   0  46   0 200  85   0 145   1   0\n133  3721   0  75   1 172 114   1 162   1   1\n134  3751   0  42   0 131  84   0 130   0   0\n135  3761   0  64   0 214  84   0 120   0   0\n136  3771   0  63   1 236  94   1 190   0   1\n137  3791   0  54   0 213  90   0 142   0   0\n138  3811   0  66   0 226  90   0 166   0   1\n139  3813   0  44   0 200 110   0 160   1   1\n140  3841   0  72   0 188  78   0 130   0   0\n141  3861   0  50   0 268 102   0 138   0   1\n142  3871   0  59   1 195 114   1 208   0   1\n143  3881   1  59   0 216  95   0 140   1   1\n144  3891   0  53   0 182  92   0 130   1   0\n145  3901   0  48   0 178  95   0 135   1   1\n146  3911   0  40   0 191  76   0 152   1   0\n147  3941   0  61   0 255  80   0 120   0   0\n148  3951   0  42   0 225  80   0 126   1   0\n149  4161   0  42   0 166  90   0 145   0   0\n150  4191   0  49   0 278  84   0 126   1   0\n151  4202   0  40   0 235  72   0 116   0   0\n152  4221   0  51   0 251  86   0 128   1   0\n153  4242   0  44   0 217  90   0 146   0   0\n154  4261   0  44   0 181  94   0 144   1   0\n155  4271   0  47   0 208 108   0 178   0   1\n156  4291   0  51   0 182 112   0 182   0   1\n157  4301   0  69   0 228  75   0 115   1   0\n158  4321   0  58   1 170  88   1 152   1   0\n159  4331   0  74   1 147  80   1 200   0   1\n160  4341   0  48   0 190  78   0 114   1   0\n161  4381   0  64   0 205  98   1 140   0   1\n162  4401   0  53   0 216  78   0 124   1   0\n163  4411   0  71   0 170  90   0 140   1   0\n164  4421   0  47   0 127  74   0 110   1   0\n165  4451   0  56   0 235  92   0 128   1   0\n166  4461   0  40   0 200  72   0 118   0   0\n167  4491   0  46   0 283 100   0 148   1   1\n168  4531   0  68   1 157  94   0 162   0   1\n169  4551   1  54   0 206  76   1 142   0   0\n170  4581   0  54   0 197  88   0 125   1   0\n171  4591   0  45   0 163  75   0 115   1   0\n172  4601   0  66   0 176  60   1 124   0   0\n173  4641   0  58   0 211  88   0 146   1   0\n174  4681   0  49   0 161  75   0 115   0   0\n175  4711   0  51   0 244  90   0 128   0   0\n176  4731   0  44   0 172 100   0 138   0   1\n177  4751   0  61   1 166  86   0 156   1   0\n178  4771   0  48   0 184  76   0 116   1   0\n179  4781   0  63   0 143  92   0 122   1   0\n180  4791   0  54   0 196  84   0 138   1   0\n181  4801   0  52   0 189  88   0 142   1   0\n182  4811   0  45   0 227  98   1 140   1   1\n183  4821   0  62   0 236  94   0 160   0   1\n184  4831   0  41   0 240  86   0 144   0   0\n185  4851   0  41   0 256  90   0 145   1   0\n186  4861   0  61   0 200  84   0 148   1   0\n187  4871   0  42   0 199 104   0 166   1   1\n188  4901   0  42   0 161  88   0 124   0   0\n189  4911   0  72   0 211  80   1 104   0   0\n190  4951   0  43   0 180  64   0  92   0   0\n191  4961   1  72   0 200  86   1 138   0   0\n192  4971   0  51   0 206  80   0 132   1   0\n193  4981   0  58   0 254  94   0 152   1   0\n194  5011   0  41   0 215  90   0 142   1   0\n195  5061   0  71   1 162  98   1 184   1   1\n196  5071   1  63   0 145  96   0 162   1   1\n197  5091   0  44   0 220  90   1 130   1   0\n198  5101   0  45   0 298 108   0 170   1   1\n199  5111   0  54   0 300  94   0 148   1   0\n200  5131   1  52   1 306 108   0 178   1   1\n201  5141   0  55   0 302 134   1 206   1   1\n202  5181   1  41   0 158  80   0 140   1   0\n203  5191   0  54   0 194 130   1 170   1   1\n204  5211   0  64   1 229  94   1 156   1   0\n205  5251   0  61   0 259  82   0 118   0   0\n206  5281   0  40   0 214  94   0 130   0   0\n207  5301   0  51   0 168 106   0 156   1   1\n208  5361   0  51   0 265  90   0 158   1   0\n209  5391   0  75   0 225  80   0 125   0   0\n210  5421   1  40   0 219  80   0 115   1   0\n211  5451   1  63   0 202 110   0 160   0   1\n212  5461   0  42   1 217  94   1 138   0   0\n213  5471   1  64   0 231  85   0 120   1   0\n214  5521   1  50   0 215 114   0 170   1   1\n215  5601   0  49   0 146  98   1 145   1   1\n216  5621   0  48   0 198  75   0 120   1   0\n217  5631   0  58   0 206  92   0 154   0   0\n218  5641   0  46   0 227  98   0 168   1   1\n219  5671   0  46   0 214  92   1 166   1   1\n220  6341   0  42   0 225 100   1 162   1   1\n221  6351   0  57   0 193  86   0 124   0   0\n222  6371   0  50   0 186 102   0 160   0   1\n223  6391   0  46   0 147  85   0 122   1   0\n224  6411   0  45   0 205 100   0 166   0   1\n225  6421   0  57   1 196  98   1 196   1   1\n226  6441   0  46   0 195  96   0 138   0   1\n227  6451   0  45   1 153 108   1 212   1   1\n228  6461   0  58   1 172  96   1 168   0   1\n229  6482   0  42   0 293 110   0 176   1   1\n230  6491   0  53   0 274 106   0 158   1   1\n231  6501   0  55   0 221 106   0 162   0   1\n232  6511   0  53   0 197  70   0 112   0   0\n233  6531   0  69   1 194 100   1 150   0   1\n234  6551   0  58   0 204  74   0 122   1   0\n235  6561   0  46   0 203  84   0 114   1   0\n236  6591   0  62   0 293  90   1 142   1   0\n237  6631   0  61   0 197  72   1 110   0   0\n238  6641   0  49   0 195  82   0 138   1   0\n239  6651   0  48   0 184  96   0 144   1   1\n240  6661   1  55   0 209  85   0 130   1   0\n241  6681   0  52   1 209  98   1 170   0   1\n242  6691   0  61   0 214 100   0 158   0   1\n243  6721   0  68   1 130 106   1 200   0   1\n244  6731   0  55   0 196  70   0 125   0   0\n245  6741   0  52   1 237 126   1 224   0   1\n246  6751   0  43   0 185  85   1 140   1   0\n247  6761   1  47   0 248 104   1 132   1   1\n248  6781   0  57   0 252 106   0 166   0   1\n249  6791   0  55   0 198  96   0 144   1   1\n250  6801   0  71   0 176  62   0 138   0   0\n251  6811   0  74   1 193  98   1 202   0   1\n252  6821   1  65   0 185 105   0 156   0   1\n253  6831   0  65   0 241 102   1 146   0   1\n254  6871   0  44   0 231  70   0 108   0   0\n255  6881   0  40   0 157  78   1 122   0   0\n256  6891   0  45   0 152 106   1 148   1   1\n257  6911   0  50   0 237 102   0 156   1   1\n258  6921   0  64   1 175 110   1 142   0   1\n259  6931   1  56   0 195  94   1 150   0   0\n260  6941   0  62   1 151  88   0 165   0   1\n261  6961   0  44   0 205  80   0 128   1   0\n262  6981   0  73   0 190  75   0 115   0   0\n263  7001   0  46   0 239 100   0 160   1   1\n264  7021   0  51   0 232  80   0 120   0   0\n265  7031   0  59   1 170 100   0 180   1   1\n266  7051   1  67   1 319 104   0 182   0   1\n267  7091   0  54   0 225  86   0 122   0   0\n268  7101   0  49   0 252  90   0 128   1   0\n269  7121   0  46   0 224  84   0 130   1   0\n270  7131   0  42   1 229  90   1 145   0   0\n271  8641   0  68   0 195  76   1 116   1   0\n272  8651   0  43   0 230  85   1 135   1   0\n273  8671   0  56   1 186  98   1 154   0   1\n274  8682   0  68   1 192  94   0 154   1   0\n275  8711   0  46   0 184  78   0 110   1   0\n276  8721   1  64   1 233  94   0 140   1   0\n277  8731   0  54   0 175  96   0 156   1   1\n278  8751   0  48   0 188 106   0 148   1   1\n279  8771   0  41   0 232  82   0 126   1   0\n280  8811   0  65   1 178 106   1 194   0   1\n281  8841   0  41   0 187 108   0 154   0   1\n282  8851   1  42   0 207  86   1 128   1   0\n283  8971   0  66   0  94  86   0 134   0   0\n284  8981   0  44   0 211  90   0 142   1   0\n285  9011   0  42   0 275 100   1 150   1   1\n286  9021   0  51   0 165  85   0 130   1   0\n287  9031   0  56   0 282  94   0 134   1   0\n288  9051   1  64   1 239  94   0 162   1   1\n289  9061   0  44   0 256 106   0 162   1   1\n290  9071   1  55   0 175 108   0 160   1   1\n291  9091   0  55   0 306  82   0 160   1   1\n292  9101   1  67   0 188 102   1 168   0   1\n293  9191   1  56   1 221  78   1 154   1   0\n294  9201   1  63   1 213 156   1 256   0   1\n295  9261   1  67   0 250 100   0 158   0   1\n296  9471   0  48   0 268 120   0 172   1   1\n297  9601   1  45   0 263  86   0 132   0   0\n298  9631   0  49   0 150  98   1 120   1   1\n299  9651   1  70   1 251 108   1 174   1   1\n300  9671   0  45   0 180 102   0 156   1   1\n301  9681   0  48   0 336 110   0 174   1   1\n302  9711   1  42   0 210  70   0 124   1   0\n303  9721   0  69   1 179 110   0 175   1   1\n304  9731   0  44   0 177  75   0 120   0   0\n305  9751   0  48   0 227  92   0 158   1   0\n306  9791   0  46   0 195  72   0 120   1   0\n307  9801   0  52   0 227  76   0 116   1   0\n308  9811   0  73   0 250  84   0 154   0   0\n309  9831   0  67   0 218  96   1 148   0   1\n310  9841   0  63   0 229 100   0 168   1   1\n311  9871   0  45   1 197  80   1 134   0   0\n312  9881   0  46   0 190  86   0 122   1   0\n313  9891   0  68   1 189 104   1 202   1   1\n314  9901   0  49   0 185  80   0 120   1   0\n315  9911   1  63   0 194  90   0 190   1   1\n316  9931   0  59   0 192  66   0 134   0   0\n317  9941   0  67   1 261  80   1 160   1   1\n318  9951   0  49   0 174  78   1 108   0   0\n319  9961   0  65   1 189 114   1 168   1   1\n320  9981   0  44   0 248 100   0 145   1   1\n321 10011   0  45   0 214  94   0 122   0   0\n322 10041   0  47   0 275  76   0 114   1   0\n323 10051   0  46   0 259  92   0 130   1   0\n324 10071   0  52   0 230  68   0 100   0   0\n325 10091   0  60   0 206  84   0 138   1   0\n326 10121   0  45   0 275  95   0 125   1   1\n327 10151   1  67   1 237 100   1 170   1   1\n328 10181   0  60   0 289  80   1 118   0   0\n329 10201   0  65   1 176  82   0 200   1   1\n330 10221   0  72   1 232  80   1 210   1   1\n331 10231   1  71   0 184  90   0 160   1   1\n332 10241   0  55   0 283 108   1 178   1   1\n333 10271   0  54   0 214 110   0 170   1   1\n334 10401   0  52   1 161  76   0 162   1   1\n335 10402   0  48   0 232  98   0 154   1   1\n336 10921   0  66   0 228  72   0 120   1   0\n337 10951   0  52   1 206 120   1 206   0   1\n338 10971   0  64   0 218  80   0 110   1   0\n339 11011   0  42   0 262  92   0 142   1   0\n340 11081   0  52   0 227  66   0  98   0   0\n341 11101   0  51   0 215  60   0 100   0   0\n342 11141   0  54   0 146  70   0 115   0   0\n343 11151   0  51   0 268  85   0 140   1   0\n344 11161   0  60   0 211  94   0 166   0   1\n345 11221   0  48   0 213  90   1 145   1   0\n346 11281   0  73   0 249 108   0 206   1   1\n347 11291   0  50   0 218  92   0 130   1   0\n348 11321   0  45   0 221  92   0 128   0   0\n349 11341   1  56   1 228  92   0 152   1   0\n350 11351   1  46   0 240 104   0 142   1   1\n351 11361   1  76   1 279  96   0 136   1   1\n352 11391   0  52   0 186  70   0 118   0   0\n353 11441   0  54   0 160 110   1 200   1   1\n354 11461   0  53   1 222 104   1 154   0   1\n355 11481   0  43   0 211  65   0 112   1   0\n356 11491   0  46   0 195 132   1 230   1   1\n357 11501   0  63   0 290  90   0 150   0   0\n358 11511   0  44   0 220  95   0 138   0   1\n359 11531   0  42   0 161  80   0 124   1   0\n360 11553   0  74   1 212  98   0 164   1   1\n361 11611   0  53   0 182  86   0 136   1   0\n362 11651   0  56   1 223 110   1 208   1   1\n363 11661   0  47   0 290  92   0 136   1   0\n364 11711   0  43   0 249  90   1 162   1   1\n365 11721   0  51   0 174  92   0 124   1   0\n366 11731   0  63   1 204  92   1 190   1   1\n367 11781   0  49   0 245  62   0 124   1   0\n368 11791   0  57   1 216 114   0 174   1   1\n369 11811   0  43   0 245 120   1 145   0   1\n370 11831   0  58   0 151  98   0 138   1   1\n371 11851   0  49   1 178 102   0 166   1   1\n372 11891   0  47   0 227  88   0 132   1   0\n373 11911   0  45   0 253 104   0 152   1   1\n374 11941   1  65   1 222  88   1 162   0   1\n375 11971   0  51   0 258  94   1 178   1   1\n376 11981   0  49   0 182  84   1 124   1   0\n377 11991   0  51   0 184  96   0 150   1   1\n378 12051   1  67   0 357  90   0 129   0   0\n379 12111   0  47   0 193  90   0 135   1   0\n380 12121   0  50   0 198  82   1 136   1   0\n381 12141   0  48   0 263  76   0 102   0   0\n382 12151   0  48   0 254  74   0 124   0   0\n383 12181   0  64   0 248  74   0 126   1   0\n384 12221   0  43   0 197  84   0 122   1   0\n385 12231   0  41   0 282  98   0 132   0   1\n386 12241   0  48   0 238 106   0 144   1   1\n387 12251   0  50   0 156  74   0 122   1   0\n388 12271   0  46   0 234  70   0 120   1   0\n389 12281   0  44   0 203  82   0 110   1   0\n390 12291   1  65   0 200  90   0 160   1   1\n391 12293   0  44   0 209  84   0 132   1   0\n392 12311   0  40   0 245  94   0 142   0   0\n393 12351   0  56   0 124  86   0 142   0   0\n394 12371   0  56   1 199  86   1 154   1   0\n395 12381   1  47   0 148  85   1 145   1   0\n396 12391   0  48   0 246  92   0 122   1   0\n397 12401   0  46   0 233  96   0 138   0   1\n398 12431   0  48   0 265 100   1 142   1   1\n399 12461   0  50   0 207  86   1 142   1   0\n400 12471   0  69   0 227  72   1 108   1   0\n401 12481   0  45   0 205 130   1 182   1   1\n402 12641   0  57   0 189 102   1 128   1   1\n403 12681   1  69   0 191 102   0 164   1   1\n404 12741   0  45   0 171  91   0 145   1   0\n405 12742   0  52   0 178  91   1 145   1   0\n406 12751   0  63   0 229  94   0 148   1   0\n407 12761   0  61   1 169  90   0 140   1   0\n408 12801   0  48   0 238  88   0 134   1   0\n409 12831   1  45   0 216  94   0 138   1   0\n410 12861   0  66   1 178 110   0 198   0   1\n411 12891   0  54   0 173  92   0 162   0   1\n412 12901   0  45   0 173  64   0 120   1   0\n413 12911   1  66   0 180 104   1 162   1   1\n414 12921   0  53   0 168 110   0 154   1   1\n415 12941   0  40   0 277  80   0 120   0   0\n416 13021   0  55   0 181  78   0 132   1   0\n417 13041   0  48   0 272  98   1 156   1   1\n418 13051   0  49   0 307  88   0 130   0   0\n419 13101   0  61   0 203  94   1 146   0   0\n420 13111   0  41   0 212  90   0 120   1   0\n421 13121   0  43   0 248 118   0 142   1   1\n422 13131   0  47   0 208 110   0 160   1   1\n423 13321   0  46   0 218  86   0 126   1   0\n424 13351   0  63   1 163  76   0 175   1   1\n425 13391   0  62   0 261  88   0 130   1   0\n426 13421   0  72   0 224 100   1 190   0   1\n427 13431   0  50   0 292  80   1 128   1   0\n428 13451   0  46   0 202 100   1 172   1   1\n429 13461   0  44   0 145  72   0 114   1   0\n430 13471   0  46   1 183  88   1 162   1   1\n431 13481   0  47   0 188  88   0 126   1   0\n432 13511   0  51   1 209 106   0 180   1   1\n433 13521   0  46   0 217  84   0 144   1   0\n434 13531   0  47   0 180  78   0 126   1   0\n435 13541   0  44   0 190  90   0 140   0   0\n436 13551   0  55   0 211  80   0 115   1   0\n437 13571   0  56   0 204  76   0 124   1   0\n438 13591   0  54   0 185  98   0 170   0   1\n439 13611   1  50   0 206  70   0 108   1   0\n440 13641   0  59   0 265  96   0 150   1   1\n441 13651   0  47   0 246  80   0 130   0   0\n442 13661   0  65   1 171 102   0 166   0   1\n443 13662   0  41   0 211  91   0 145   0   0\n444 13671   0  47   0 139  96   1 192   1   1\n445 13691   0  49   0 155  84   0 124   0   0\n446 13721   0  50   0 229  90   0 134   0   0\n447 13731   0  56   1 148 110   0 168   1   1\n448 13751   0  50   0 198  86   0 134   1   0\n449 13761   0  55   1 186 120   0 172   1   1\n450 13771   0  52   0 211  70   0 112   1   0\n451 13811   0  57   0 210  80   0 120   1   0\n452 13841   1  47   1 212 122   1 220   1   1\n453 13861   0  59   0 227  70   0 122   0   0\n454 13901   0  47   0 232  90   0 142   0   0\n455 13911   0  42   0 176  88   0 122   1   0\n456 13931   0  56   0 166  86   0 126   0   0\n457 13941   1  43   0 268  88   1 132   1   0\n458 13951   0  55   0 178  80   0 104   1   0\n459 13961   0  49   1 147 134   1 300   1   1\n460 13971   0  71   1 164  94   0 174   0   1\n461 14041   0  71   1 187 114   1 172   0   1\n462 14071   0  69   1 165  96   1 140   0   1\n463 14691   0  47   0 250  88   1 122   1   0\n464 14701   0  44   0 199  70   0 120   1   0\n465 14711   1  65   1 233 116   1 180   1   1\n466 14731   0  65   0 182  74   0 124   0   0\n467 14741   0  40   0 210  94   0 128   0   0\n468 14751   0  47   0 235  86   0 128   1   0\n469 14761   0  54   0 172  92   0 144   1   0\n470 14771   0  64   1 198 100   0 178   0   1\n471 14781   0  59   1 212  90   1 198   0   1\n472 14801   0  72   0 285  90   1 150   1   0\n473 14811   0  52   0 194  82   0 132   0   0\n474 14861   0  56   0 237  70   0 106   0   0\n475 14871   0  56   0 153  66   1  96   0   0\n476 14881   0  54   0 219  92   0 152   1   0\n477 14901   0  60   1 188 114   1 210   1   1\n478 14911   0  63   0 276  95   1 145   0   1\n479 14931   0  73   0 203  68   0 130   1   0\n480 14941   1  49   0 228  98   0 140   1   1\n481 14981   0  57   0 199  80   0 134   0   0\n482 15001   0  53   1 162  90   0 158   1   0\n483 15011   0  43   0 221  94   0 125   1   0\n484 15021   0  68   0 150  78   0 145   0   0\n485 15031   0  53   0 140  80   0 120   1   0\n486 15041   0  50   0 162  65   1 110   0   0\n487 15061   0  49   0 171  86   1 125   1   0\n488 15091   0  62   0 206  94   0 144   0   0\n489 15111   0  52   0 226  76   0 130   1   0\n490 15121   0  57   1 113  94   1 146   1   0\n491 15141   0  45   0 197  78   0 118   0   0\n492 15161   0  50   0 180  88   0 132   1   0\n493 15171   0  50   1 180 118   1 214   1   1\n494 15191   0  53   0 196  86   0 144   1   0\n495 15201   0  51   0 211  98   0 135   0   1\n496 15211   0  58   1 164  96   1 155   1   1\n497 15221   0  64   0 218  85   0 154   0   0\n498 15251   1  49   0 191  76   0 132   1   0\n499 15271   0  55   0 189  64   0 110   0   0\n500 15311   0  50   0 156  82   0 114   1   0\n501 15321   0  50   0 223  80   0 130   1   0\n502 15361   0  57   0 165  76   0 132   1   0\n503 15401   0  55   1 200  94   1 188   1   1\n504 15421   0  48   1 162 135   1 250   1   1\n505 15431   0  56   1 207 110   0 172   1   1\n506 15441   0  72   0 262  84   0 172   1   1\n507 15511   1  67   1 236 106   1 200   0   1\n508 15541   0  70   1 192  90   1 162   0   1\n509 15562   0  57   1 203 100   0 170   1   1\n510 15611   0  67   1 200 160   1 224   0   1\n511 15641   0  62   0 280  86   0 124   1   0\n512 15651   0  72   1 229 140   1 270   1   1\n513 15661   0  70   0 290  84   0 138   0   0\n514 15671   0  65   0 222  88   0 146   0   0\n515 15691   0  58   0 259 100   1 154   0   1\n516 15711   0  64   0 205  80   0 140   0   0\n517 15761   0  44   0 276  74   0 112   1   0\n518 15791   0  55   0 171  68   0 110   0   0\n519 15831   0  71   0 287  90   0 130   0   0\n520 15851   1  72   0 174  78   1 192   1   1\n521 15882   0  71   0 277 110   1 200   0   1\n522 15891   0  54   0 192  85   0 130   0   0\n523 15911   0  51   0 196  90   0 128   1   0\n524 15921   0  68   0 203  74   0 138   1   0\n525 15931   0  47   0 271  85   0 145   1   0\n526 15941   0  49   1 169  85   1 145   0   0\n527 15951   0  48   0 201  98   0 150   1   1\n528 15981   0  74   0 244  94   0 164   0   1\n529 15991   0  49   0 161  92   0 120   0   0\n530 16321   1  53   0 192 106   0 164   1   1\n531 16431   0  46   0 192  86   1 116   1   0\n532 16441   0  59   0 230  84   0 158   1   0\n533 16461   0  50   0 312  98   1 138   0   1\n534 16481   1  69   1 230 100   1 170   0   1\n535 16501   1  75   1 233  90   1 222   1   1\n536 16531   0  42   0 207  72   0 106   1   0\n537 16541   0  50   0 317  90   1 138   0   0\n538 16571   0  44   0 213  84   0 118   1   0\n539 16581   0  44   0 220  98   0 140   0   1\n540 16591   0  42   0 225  95   0 140   0   1\n541 16622   0  42   0 288 104   0 150   1   1\n542 16691   0  44   0 168  94   1 134   1   0\n543 16701   0  57   1 182  96   1 138   1   1\n544 16711   1  68   1 242  84   0 128   1   0\n545 16752   0  69   0 258  82   0 145   1   0\n546 16761   0  74   1 172 100   0 190   1   1\n547 16841   0  56   1 239 140   1 220   1   1\n548 16871   1  58   1 209  94   1 140   1   0\n549 16891   0  46   0 181  84   0 124   0   0\n550 16911   0  60   1 199 100   0 162   0   1\n551 16931   0  62   0 217  90   0 144   1   0\n552 16971   0  74   0 200  78   0 118   1   0\n553 17071   0  44   0 268  80   0 126   1   0\n554 17111   0  54   0 202  86   0 134   0   0\n555 17121   0  49   0 224  86   1 134   1   0\n556 17131   0  46   0 302 102   0 160   1   1\n557 17151   0  45   0 239  90   0 128   0   0\n558 17161   0  57   0 205  88   0 140   0   0\n559 17171   0  56   1 192 170   1 270   0   1\n560 17181   0  42   0 282 114   0 170   1   1\n561 17191   0  52   0 232  94   0 144   1   0\n562 17211   0  49   0 229  92   0 162   1   1\n563 17231   0  51   0 336  86   0 130   1   0\n564 17251   0  40   0 146  84   0 125   0   0\n565 17271   0  43   0 224  72   0 115   0   0\n566 17291   0  45   0 228  76   1 136   1   0\n567 17361   0  63   0 211 108   0 144   1   1\n568 17401   0  52   0 212  76   0 118   0   0\n569 17481   1  67   1 243 118   1 220   1   1\n570 17961   0  72   1 208  94   1 174   1   1\n571 17991   0  54   0 284  98   0 146   1   1\n572 18061   0  52   0 190  88   1 130   1   0\n573 18071   0  49   0 264  92   0 162   0   1\n574 18101   0  42   0 288 108   0 146   0   1\n575 18121   0  41   0 181  94   0 136   1   0\n576 18131   1  56   1 283 100   0 188   1   1\n577 18141   0  46   0 217  66   0 120   1   0\n578 18151   1  52   0 250  80   0 132   1   0\n579 18161   0  44   0 209  70   0 116   1   0\n580 18171   1  43   0 189 106   0 154   1   1\n581 18201   0  73   0 190  78   0 138   0   0\n582 18401   0  54   0 223  82   0 122   1   0\n583 18411   0  46   0 241  84   0 120   1   0\n584 18421   0  44   0 214  96   1 142   1   1\n585 18441   0  43   0 207  86   0 122   0   0\n586 18481   0  46   0 186  86   0 130   0   0\n587 18491   1  74   1 212  70   1 144   1   0\n588 18511   0  54   1 211  94   1 152   0   0\n589 18521   0  63   0 223  86   0 158   1   0\n590 18551   0  58   1 206 108   1 192   0   1\n591 18581   0  50   0 194  92   0 134   1   0\n592 18631   0  71   0 193  82   0 115   1   0\n593 18661   0  52   0 213  90   0 140   1   0\n594 18681   0  63   0 318  82   1 126   0   0\n595 18711   0  66   0 216 104   1 154   0   1\n596 18731   0  60   0 211  66   0 128   1   0\n597 18752   0  47   0 219  88   0 128   1   0\n598 18771   0  57   0 322  98   0 144   1   1\n599 18801   0  66   1 239 100   1 184   1   1\n600 18841   0  66   1 195 104   0 158   1   1\n601 18871   0  47   0 243  78   0 118   1   0\n602 18921   0  60   0 223  92   1 122   1   0\n603 18971   0  48   0 174 102   0 160   1   1\n604 19003   0  61   1 163  86   1 144   1   0\n605 19011   0  64   0 225  90   0 160   1   1\n606 19061   1  46   0 252 122   0 158   1   1\n607 19091   0  49   0 261 102   0 166   1   1\n608 19121   0  51   0 184  88   0 118   1   0\n609 19161   0  64   0 206  82   0 152   0   0\n```\n\n\n:::\n\n```{.r .cell-code}\ncolnames(CHD.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"id\"  \"chd\" \"age\" \"cat\" \"chl\" \"dbp\" \"ecg\" \"sbp\" \"smk\" \"hpt\"\n```\n\n\n:::\n:::\n\n\n\n\n\n### Fit Logistic Regression Model for a Single Variable\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvars <- c(\"smk\", \"sbp\", \"age\", \"chl\")\n#jit  <- 0.01  # global jitter amount for y\n\n## par(mfrow = c(2, 2), mar = c(4, 4, 2, 4) + 0.1)  # extra right margin for axis(4)\n\nfor (v in vars) {\n  ## Univariate logistic regression using ORIGINAL variable name in the formula\n  fit <- glm(\n    formula = reformulate(v, response = \"chd\"),\n    data    = CHD.data,\n    family  = binomial()\n  )\n  print(summary(fit))\n\n  ## Base scatter of chd with small jitter (left axis: probability scale)\n  plot(\n    CHD.data[[v]],\n    jitter(CHD.data$chd, amount = jit),\n    pch  = 16, col = rgb(0, 0, 0, 0.45),\n    xlab = v, ylab = \"chd (jittered)\",\n    main = paste(\"chd vs\", v),\n    ylim = c(-0.1, 1.1)\n  )\n\n  ## Fitted Ï€(x) in red (left axis)\n  if (length(unique(CHD.data[[v]])) == 2) {\n    # binary predictor\n    xcat <- sort(unique(CHD.data[[v]]))\n    nd   <- setNames(data.frame(xcat), v)\n    pcat <- predict(fit, newdata = nd, type = \"response\")\n    points(xcat, pcat, pch = 19, col = \"red\")\n    lines(xcat, pcat, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{Ï€(x)} with fixed y-limits\n    logit_p <- log(pcat / (1 - pcat))\n    par(new = TRUE)\n    plot(\n      xcat, logit_p, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(CHD.data[[v]]), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n\n  } else {\n    # continuous predictor\n    xg <- seq(min(CHD.data[[v]]), max(CHD.data[[v]]), length.out = 400)\n    nd <- setNames(data.frame(xg), v)\n    pg <- predict(fit, newdata = nd, type = \"response\")\n    lines(xg, pg, col = \"red\", lwd = 2)\n\n    # Right-axis: logit{Ï€(x)} with fixed y-limits\n    logit_pg <- log(pg / (1 - pg))\n    par(new = TRUE)\n    plot(\n      xg, logit_pg, type = \"l\", lwd = 2, col = \"blue\",\n      axes = FALSE, xlab = \"\", ylab = \"\",\n      xlim = range(xg), ylim = c(-2.5, 0)\n    )\n    axis(4)\n    mtext(\"logit(p(x))\", side = 4, line = 3)\n    par(new = FALSE)\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.4898     0.2524  -9.865   <2e-16 ***\nsmk           0.6706     0.2919   2.297   0.0216 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 432.81  on 607  degrees of freedom\nAIC: 436.81\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/chd-univariate-2x2-jitter-fixedlogit-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.837912   0.629805  -6.094  1.1e-09 ***\nsbp          0.012154   0.004036   3.011   0.0026 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 430.06  on 607  degrees of freedom\nAIC: 434.06\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/chd-univariate-2x2-jitter-fixedlogit-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.47833    0.75610  -5.923 3.16e-09 ***\nage          0.04445    0.01315   3.381 0.000723 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 427.22  on 607  degrees of freedom\nAIC: 431.22\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/chd-univariate-2x2-jitter-fixedlogit-3.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = reformulate(v, response = \"chd\"), family = binomial(), \n    data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.538260   0.686879  -5.151 2.59e-07 ***\nchl          0.007004   0.003064   2.286   0.0223 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 433.42  on 607  degrees of freedom\nAIC: 437.42\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/chd-univariate-2x2-jitter-fixedlogit-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n### Fit Logistic Regression Model with all variables\n\nWe fit a logistic regression with a logit link:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1_chd <- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = CHD.data,\n  family = binomial(link = \"logit\")\n)\nsummary(fit1_chd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = chd ~ smk + cat + sbp + age + chl + ecg + hpt, \n    family = binomial(link = \"logit\"), data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.048892   1.345165  -4.497  6.9e-06 ***\nsmk          0.855951   0.306505   2.793  0.00523 ** \ncat          0.732763   0.376129   1.948  0.05139 .  \nsbp         -0.006995   0.006976  -1.003  0.31600    \nage          0.033956   0.015344   2.213  0.02690 *  \nchl          0.008970   0.003274   2.740  0.00615 ** \necg          0.417776   0.295553   1.414  0.15750    \nhpt          0.655498   0.359976   1.821  0.06861 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 399.35  on 601  degrees of freedom\nAIC: 415.35\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n\n\n**Notes for interpretation:**\n\n* Positive coefficients increase the log-odds of CHD; negative coefficients decrease it.\n* For indicator variables (e.g., `smk`), `exp(beta)` is the adjusted odds ratio comparing the group with value 1 versus 0, holding others fixed.\n* For continuous predictors (e.g., `sbp`, `age`), `exp(beta)` is the multiplicative change in the odds for a oneâ€‘unit increase. For a *d*-unit increase, the OR is `exp(d * beta)`.\n\n## Inference for Coefficients: Confidence Intervals and Covariance Matrix\n\nWe extract profileâ€‘likelihood CIs and the covariance matrix to confirm standard errors.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_95 <- confint(fit1_chd, level = 0.95)     # profile-likelihood CI\nvcov_mat <- vcov(fit1_chd)                    # covariance matrix of coefficients\nse_vec   <- sqrt(diag(vcov_mat))          # standard errors\n\nci_95\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   2.5 %       97.5 %\n(Intercept) -8.718003347 -3.427904298\nsmk          0.275699158  1.483333169\ncat         -0.006873216  1.471885644\nsbp         -0.021166144  0.006266328\nage          0.003687290  0.064005215\nchl          0.002533226  0.015404292\necg         -0.171584621  0.990632546\nhpt         -0.050184520  1.364993401\n```\n\n\n:::\n\n```{.r .cell-code}\nvcov_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             (Intercept)           smk           cat           sbp\n(Intercept)  1.809468553 -1.014526e-01  0.1391440386 -4.908229e-03\nsmk         -0.101452600  9.394560e-02 -0.0032961000 -1.653230e-04\ncat          0.139144039 -3.296100e-03  0.1414730484 -9.299960e-04\nsbp         -0.004908229 -1.653230e-04 -0.0009299960  4.866901e-05\nage         -0.011142995  7.738971e-04 -0.0017879998 -1.311191e-05\nchl         -0.002111134  3.161443e-05  0.0003146354 -1.821907e-06\necg          0.003442546  9.255483e-03 -0.0204455233 -2.982539e-04\nhpt          0.139817180  6.954592e-03 -0.0044220690 -1.486400e-03\n                      age           chl           ecg           hpt\n(Intercept) -1.114300e-02 -2.111134e-03  3.442546e-03  1.398172e-01\nsmk          7.738971e-04  3.161443e-05  9.255483e-03  6.954592e-03\ncat         -1.788000e-03  3.146354e-04 -2.044552e-02 -4.422069e-03\nsbp         -1.311191e-05 -1.821907e-06 -2.982539e-04 -1.486400e-03\nage          2.354442e-04 -1.480501e-06 -4.972374e-05  4.044434e-04\nchl         -1.480501e-06  1.071766e-05  5.040548e-05 -6.046197e-05\necg         -4.972374e-05  5.040548e-05  8.735130e-02  9.506863e-04\nhpt          4.044434e-04 -6.046197e-05  9.506863e-04  1.295828e-01\n```\n\n\n:::\n\n```{.r .cell-code}\nse_vec  # should match the SE column in summary(fit1_chd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)         smk         cat         sbp         age         chl \n1.345164879 0.306505459 0.376129032 0.006976318 0.015344190 0.003273784 \n        ecg         hpt \n0.295552531 0.359976081 \n```\n\n\n:::\n:::\n\n\n\n\n## Inference for Odds Ratios\n\n\n### Interpretation of Odds Ratios in Logistic Regression\n\nA multiple logistic regression model expresses the log-odds (logit) of an event as a linear function of predictors:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k.\n$$\n\nHere,\n\n* $p = \\Pr(Y = 1 \\mid x_1, x_2, \\ldots, x_k)$ is the probability of the event,\n* $\\beta_0$ is the intercept, and\n* each $\\beta_j$ represents the **change in the log-odds** of the event per one-unit increase in $x_j$, *holding all other variables constant*.\n\nExponentiating both sides gives the model in odds form:\n\n$$\n\\frac{p}{1-p}\n= \\exp(\\beta_0)\n\\times \\exp(\\beta_1 x_1)\n\\times \\exp(\\beta_2 x_2)\n\\times \\cdots\n\\times \\exp(\\beta_k x_k).\n$$\n\n**An R function for OR at two Profiles**\n\nThe or_from_predict R function is a utility designed to calculate the Odds Ratio (OR) and its 95% confidence interval (CI) between two specific covariate profiles (new1 and new0) for a given logistic regression model (fit). The calculation is performed on the link (logit) scale. For a logistic model $\\text{logit}(p) = \\eta = \\mathbf{X}\\boldsymbol{\\beta}$, the log-Odds Ratio (logOR) is the difference between the linear predictors ($\\eta_1, \\eta_0$) for the two profiles:\n$$\\widehat{\\text{logOR}} = \\eta_1 - \\eta_0 = (\\mathbf{x}_1^T - \\mathbf{x}_0^T) \\boldsymbol{\\beta} = \\mathbf{c}^T \\boldsymbol{\\beta}$${#eq-logor}\n\nHere, $\\mathbf{c} = \\mathbf{x}_1 - \\mathbf{x}_0$ is the linear contrast vector derived from the model matrices of the two profiles. The function estimates the variance of this contrast as $\\text{Var}(\\widehat{\\text{logOR}}) = \\mathbf{c}^T \\mathbf{V} \\mathbf{c}$, where $\\mathbf{V}$ is the model's variance-covariance matrix (vcov(fit)). The standard error $SE = \\sqrt{\\mathbf{c}^T \\mathbf{V} \\mathbf{c}}$ is used to compute the $100(1-\\alpha)\\%$ confidence interval for the logOR: \n$$\\widehat{\\text{logOR}} \\pm z_{1-\\alpha/2} \\times SE.$$ \nThese values (estimate and CI bounds) are then exponentiated to produce the final $\\widehat{\\text{OR}} = \\exp(\\widehat{\\text{logOR}})$ and its 95% CI. The function also prints two helpful summaries to the console: a data frame showing only the variables that differ between the new0 and new1 profiles, and a 2x3 table presenting the estimates and CIs for both the OR and the logOR.\n\n**The R function to find ORs**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Compute OR and 95% CI via predict() on the LINK scale\n## OR = exp( eta(new1) - eta(new0) ), where eta(.) = logit{Ï€(.)}\n## Compute OR via predict() contrast on the LINK scale, also:\n## (ii) print a 2-row data.frame of only variables that differ between new0 and new1\n## (iii) print a 2x3 table (rows: OR, logOR; cols: Estimate, CI_low, CI_up)\nor_from_predict <- function(fit, new1, new0, level = 0.95, digits = 4, tol = 1e-12) {\n  stopifnot(is.data.frame(new1), is.data.frame(new0))\n\n  ## --- REFACTORED SECTION START ---\n  ## ---- (ii) Two-row data.frame with only changed variables ----\n  \n  ## Helper function to find differing variables between two profiles\n  ## This is defined *inside* or_from_predict for encapsulation\n  get_changed_vars <- function(d0, d1, tolerance) {\n    common <- intersect(names(d0), names(d1))\n    diffv  <- vapply(common, function(nm) {\n      x0 <- d0[[nm]]; x1 <- d1[[nm]]\n      if (is.numeric(x0) && is.numeric(x1)) {\n        !isTRUE(all.equal(as.numeric(x0), as.numeric(x1), tolerance = tolerance))\n      } else {\n        !identical(x0, x1)\n      }\n    }, logical(1))\n    \n    keep <- common[diffv]\n    if (length(keep) == 0L) {\n      out <- data.frame(`_no_changes_` = \"no differences\")\n      rownames(out) <- c(\"new0\", \"new1\")\n      return(out)\n    }\n    out <- rbind(d0[keep], d1[keep])\n    rownames(out) <- c(\"new0\", \"new1\")\n    out\n  }\n  \n  ## Call the helper function\n  changes_df <- get_changed_vars(new0, new1, tol)\n  ## --- REFACTORED SECTION END ---\n\n\n  ## ---- Linear contrast for log-OR and its variance ----\n  \n  ## (i) Calculate logOR estimate using predict(type=\"link\")\n  ## eta(.) = logit{p(.)}\n  eta1 <- predict(fit, newdata = new1, type = \"link\")\n  eta0 <- predict(fit, newdata = new0, type = \"link\")\n  logOR_hat <- as.numeric(eta1 - eta0) # logOR = eta1 - eta0\n  \n  ## (ii) Calculate standard error using the contrast vector 'cvec'\n  X1 <- model.matrix(delete.response(terms(fit)), data = new1)\n  X0 <- model.matrix(delete.response(terms(fit)), data = new0)\n  cvec      <- as.numeric(X1 - X0)\n  V         <- vcov(fit)\n  se_logOR  <- sqrt(as.numeric(t(cvec) %*% V %*% cvec))\n\n  alpha  <- 1 - level\n  z      <- qnorm(1 - alpha / 2)\n  ci_log <- c(logOR_hat - z * se_logOR, logOR_hat + z * se_logOR)\n\n  ## ---- 2x3 table: rows OR and logOR; columns Estimate, CI_low, CI_up ----\n  res_tab <- data.frame(\n    Estimate = c(exp(logOR_hat),          logOR_hat),\n    CI_low   = c(exp(ci_log[1L]),         ci_log[1L]),\n    CI_up    = c(exp(ci_log[2L]),         ci_log[2L]),\n    row.names = c(\"OR\", \"logOR\")\n  )\n\n  ## ---- Print requested items ----\n  cat(\"\\nVariables that differ between new0 and new1:\\n\")\n  print(changes_df)\n  cat(\"\\nOdds Ratio summary:\\n\")\n  print(round(res_tab, digits = digits)) # Added rounding for neatness\n\n  ## ---- Return (invisibly) ----\n  invisible(list(\n    OR        = exp(logOR_hat),\n    CI_OR     = exp(ci_log),\n    logOR     = logOR_hat,\n    CI_logOR  = ci_log,\n    se_logOR  = se_logOR,\n    changes   = changes_df,\n    table     = res_tab\n  ))\n}\n\n## --- Example usage ---\n## Suppose 'fit1_chd' is your fitted model and 'CHD.data' is your data\n## base_prof <- as.data.frame(lapply(CHD.data, function(col) if (is.numeric(col)) mean(col) else col[1]))\n## new0 <- base_prof; new0$smk <- 0\n## new1 <- base_prof; new1$smk <- 1\n## or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n\n\nmean_profile <- function(data, vars_binary_as = c(0,1)) {\n  ## Build a single-row data.frame of typical values:\n  out <- lapply(data, function(col) {\n    if (is.numeric(col)) {\n      # If strictly 0/1, keep mean (works fine for GLM prediction),\n      # or switch to mode if you prefer.\n      if (all(col %in% c(0,1))) mean(col) else mean(col, na.rm = TRUE)\n    } else {\n      # Fallback to first level for factors/characters\n      if (is.factor(col)) levels(col)[1] else unique(col)[1]\n    }\n  })\n  as.data.frame(out)\n}\n```\n:::\n\n\n\n\n\n### Examples of Finding ORs and Their CIs for the CHD Dataset\n\n#### OR Smoking (`smk`) (1 vs 0)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### 1) Smoking OR: smk = 1 vs 0 (other vars at their means)\n## Example profiles at sample means (adjust as you like)\nbase_prof <- mean_profile(CHD.data)\nnew0 <- base_prof; new0$smk <- 0\nnew1 <- base_prof; new1$smk <- 1\n\nres_smk <- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariables that differ between new0 and new1:\n     smk\nnew0   0\nnew1   1\n\nOdds Ratio summary:\n      Estimate CI_low  CI_up\nOR      2.3536 1.2907 4.2917\nlogOR   0.8560 0.2552 1.4567\n```\n\n\n:::\n:::\n\n\n\n\n\n**How to read this:**\n\n* `OR_smk > 1` suggests higher odds of CHD among smokers (adjusted for other variables). If the 95% CI excludes 1, the association is statistically significant at the 5% level.\n\n#### OR for Systolic Blood Pressure (`sbp`): from 120 to 160\n\nWe compute the adjusted OR for a 40â€‘unit increase in `sbp` (from 120 to 160):\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### 2) SBP OR: 160 vs 120 (other vars at their means)\nnew0 <- base_prof; new0$sbp <- 120\nnew1 <- base_prof; new1$sbp <- 160\n\nres_sbp <- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariables that differ between new0 and new1:\n     sbp\nnew0 120\nnew1 160\n\nOdds Ratio summary:\n      Estimate  CI_low  CI_up\nOR      0.7559  0.4375 1.3062\nlogOR  -0.2798 -0.8267 0.2671\n```\n\n\n:::\n:::\n\n\n\n\n#### OR for Combined Effects of Two Variables: Smoking with an Age Difference\n\nSuppose we compare two groups that differ in **smoking status** and **age**:\n\n* **Group A:** `smk = 1`, `age = 50` (all other covariates equal)\n* **Group B:** `smk = 0`, `age = 30`\n\nThe logâ€‘odds contrast is ($A = \\beta_{smk} + (50-20)\\beta_{age}$), so the OR is ($\\exp(A)$).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew0 <- base_prof; new0$age <- 30; new0$smk <- 0\nnew1 <- base_prof; new1$age <- 50; new1$smk <- 1\n\nres_ageAsmk <- or_from_predict(fit1_chd, new1 = new1, new0 = new0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nVariables that differ between new0 and new1:\n     age smk\nnew0  30   0\nnew1  50   1\n\nOdds Ratio summary:\n      Estimate CI_low   CI_up\nOR      4.6417 1.8546 11.6168\nlogOR   1.5351 0.6177  2.4525\n```\n\n\n:::\n:::\n\n\n\n\n## Assessing Statistical Significance with Wilks' Theorem (Analogue of F-test for OLS)\n\n\nIn the context of logistic regression, Wilks' theorem provides the basis for the Likelihood Ratio Test (LRT) used to assess the significance of predictor variables. The theorem states that when comparing a full model ($M_1$) to a nested null model ($M_0$), the test statistic, $\\Lambda$, asymptotically follows a chi-squared ($\\chi^2$) distribution under the null hypothesis (i.e., that the simpler model $M_0$ is correct).\n\nThe statistic $\\Lambda$ is calculated as the difference in the maximized log-likelihoods:\n$$\\Lambda = -2(\\log L_0 - \\log L_1)$${#eq-LRT}\nwhere $\\log L_0$ and $\\log L_1$ are the log-likelihoods of the null and full models, respectively. In logistic regression, this is equivalent to the difference in the deviances: $\\Lambda = \\text{Deviance}_0 - \\text{Deviance}_1$. This test statistic $\\Lambda$ represents the reduction in deviance (a measure of badness-of-fit) achieved by adding the extra predictors to the model.\n\nThe following R code chunk generates a conceptual plot of this relationship:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| label: plot-lrt-concept\n#| echo: false\n#| fig-cap: \"Conceptual plot of Deviance versus Number of Parameters, illustrating the Likelihood Ratio Test statistic (Î›) and Residual Deviance. Î› is the drop from the Null Model to the Full Model. The Saturated Model represents perfect fit (Deviance = 0).\"\n\nlibrary(ggplot2)\n\n## 1. Create conceptual data for the plot\n## These are just for illustration\nn_obs <- 60 # Number of observations\np0 <- 1     # Parameters in null model (intercept)\np1 <- 21    # Parameters in full model (e.g., intercept + 7 predictors)\npsat <- n_obs # Parameters in saturated model (1 per observation)\n\nD0 <- 41 # Null deviance\nD1 <- 20 # Full model deviance (residual deviance of M1)\nD_sat <- 0 # Saturated model deviance\n\n## Data frame for the three points\nplot_data <- data.frame(\n  model = c(\"M_0 (Null)\", \"M_1 (Full)\", \"M_Sat (Saturated)\"),\n  params = c(p0, p1, psat),\n  deviance = c(D0, D1, D_sat),\n  ## Add custom justification and nudges for labels\n  hjust_val = c(0.5, 0.5, 1.1), # Right-align the last label\n  nudge_x_val = c(0, 0, 0) \n)\n\n## 2. Create the ggplot\nggplot(plot_data, aes(x = params, y = deviance)) +\n  ## Draw dashed guide lines for D0 and D1\n  geom_segment(aes(x = p0, y = D0, xend = p1, yend = D0), linetype = \"dashed\", color = \"grey70\") +\n  geom_segment(aes(x = p1, y = D1, xend = psat, yend = D1), linetype = \"dashed\", color = \"grey70\") +\n  \n  ## Connect the points with lines\n  geom_line(color = \"black\", linetype = \"solid\", linewidth = 0.5) +\n  \n  ## Draw the main points\n  geom_point(size = 4, aes(color = model)) +\n  \n  ## --- MODIFIED LABEL PLACEMENT ---\n  ## Label the points using custom nudge/justification\n  geom_text(\n    aes(label = model, hjust = hjust_val, nudge_x = nudge_x_val), \n    nudge_y = 2.5,  # Use a much smaller vertical nudge\n    size = 4\n  ) +\n  \n  ## --- ADDED BACK D0 and D1 ANNOTATIONS ---\n  ## D0 (Null Deviance)\n  geom_segment(\n    aes(x = p0 - 2, y = D0, xend = p0 - 2, yend = D_sat), # Nudged left\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"darkgreen\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = p0 - 3, y = D0 / 2, # Nudged left\n    label = \"D[0]\", parse = TRUE,\n    color = \"darkgreen\", hjust = 0.5, size = 5\n  ) +\n  \n  ## D1 (Residual Deviance of Full Model)\n  geom_segment(\n    aes(x = psat + 2, y = D1, xend = psat + 2, yend = D_sat), # Nudged right\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"darkblue\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = psat + 3, y = D1 / 2, # Nudged right\n    label = \"D[1]\", parse = TRUE,\n    color = \"darkblue\", hjust = 0.5, size = 5\n  ) +\n  \n  ## LRT statistic Î› = D0 - D1\n  geom_segment(\n    aes(x = p1 + 2, y = D0, xend = p1 + 2, yend = D1), # Nudged right\n    arrow = arrow(ends = \"both\", length = unit(0.1, \"inches\")),\n    color = \"red\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    x = p1 + 3, y = D1 + (D0 - D1) / 2, # Nudged right\n    label = \"Lambda == D[0] - D[1]\",\n    parse = TRUE,\n    color = \"red\", hjust = 0, size = 5\n  ) +\n  \n  ## Customize axes to show the symbolic labels\n  scale_x_continuous(\n    breaks = c(p0, p1, psat),\n    labels = c(expression(p[0]), expression(p[1]), expression(n)),\n    expand = expansion(mult = 0.1) # Add some padding\n  ) +\n  scale_y_continuous(\n    breaks = c(D_sat, D1, D0),\n    labels = c(expression(0), expression(D[1]), expression(D[0]))\n  ) +\n  \n  ## Labels and Title\n  labs(\n    title = \"Relationship between Deviance and Model Complexity\",\n    x = \"Number of Parameters\",\n    y = \"Model Deviance\"\n  ) +\n  \n  ## Clean theme\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"none\" # Remove legend, as points are labeled\n  ) +\n  scale_color_manual(values = c(\"M_0 (Null)\" = \"blue\", \"M_1 (Full)\" = \"blue\", \"M_Sat (Saturated)\" = \"red\"))\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nAs the diagram illustrates, the null model ($M_0$) has fewer parameters ($p_0$) and a higher deviance ($D_0$, or worse fit), while the full model ($M_1$) has more parameters ($p_1$) and a lower deviance ($D_1$). The Likelihood Ratio Test statistic $D$ is the magnitude of this drop in deviance.\n\nFor assessing the overall significance of a regression model (`fit1_chd`), this involves comparing it to its corresponding intercept-only (null) model. The degrees of freedom for the $\\chi^2$ test is the difference in the number of parameters, $df = p_1 - p_0$, which equals the number of predictors in the full model.\n\nHere is an R code chunk demonstrating how to compute this p-value directly from a `glm` fit object, assuming it is named `fit1_chd`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate the Likelihood Ratio Test statistic (D) and degrees of freedom (df)\n## by comparing the model's deviance to the null (intercept-only) deviance,\n## both of which are stored in the 'fit1_chd' object.\nsummary(fit1_chd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = chd ~ smk + cat + sbp + age + chl + ecg + hpt, \n    family = binomial(link = \"logit\"), data = CHD.data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.048892   1.345165  -4.497  6.9e-06 ***\nsmk          0.855951   0.306505   2.793  0.00523 ** \ncat          0.732763   0.376129   1.948  0.05139 .  \nsbp         -0.006995   0.006976  -1.003  0.31600    \nage          0.033956   0.015344   2.213  0.02690 *  \nchl          0.008970   0.003274   2.740  0.00615 ** \necg          0.417776   0.295553   1.414  0.15750    \nhpt          0.655498   0.359976   1.821  0.06861 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 438.56  on 608  degrees of freedom\nResidual deviance: 399.35  on 601  degrees of freedom\nAIC: 415.35\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\nlrt_statistic <- fit1_chd$null.deviance - fit1_chd$deviance\nlrt_df <- fit1_chd$df.null - fit1_chd$df.residual\n\n## Compute the p-value from the chi-squared distribution\n## We use lower.tail = FALSE to get P(ChiSq > D)\np_value <- pchisq(lrt_statistic, lrt_df, lower.tail = FALSE)\n\n## Create and print the result in an ANOVA-like table\n## Row 1: Null model\n## Row 2: Full model (fit1_chd), showing the test against the null\nlrt_table <- data.frame(\n  \"Resid. Df\" = c(fit1_chd$df.null, fit1_chd$df.residual),\n  \"Resid. Dev\" = c(round(fit1_chd$null.deviance, 4), round(fit1_chd$deviance, 4)),\n  \"Test Df\" = c(NA, lrt_df),\n  \"Test Statistic (D)\" = c(NA, round(lrt_statistic, 4)),\n  \"p-value\" = c(NA, format.pval(p_value, digits = 4)),\n  row.names = c(\"Null Model\", \"Full Model (fit1_chd)\"),\n  check.names = FALSE # Prevent R from changing 'p-value' to 'p.value'\n)\n\ncat(\"Likelihood Ratio Test for Model Significance:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood Ratio Test for Model Significance:\n```\n\n\n:::\n\n```{.r .cell-code}\nlrt_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      Resid. Df Resid. Dev Test Df Test Statistic (D)   p-value\nNull Model                  608   438.5583      NA                 NA      <NA>\nFull Model (fit1_chd)       601   399.3539       7            39.2044 1.787e-06\n```\n\n\n:::\n:::\n\n\n\n\n**Using built-in anova() function**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit0_chd <- glm (chd~1, data = CHD.data, family = binomial())\nanova(fit0_chd, fit1_chd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: chd ~ 1\nModel 2: chd ~ smk + cat + sbp + age + chl + ecg + hpt\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       608     438.56                          \n2       601     399.35  7   39.204 1.787e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit1_chd, test=\"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: chd\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                   608     438.56              \nsmk   1   5.7453       607     432.81 0.0165324 *  \ncat   1  14.3716       606     418.44 0.0001501 ***\nsbp   1   0.7574       605     417.68 0.3841353    \nage   1   5.2821       604     412.40 0.0215455 *  \nchl   1   7.8619       603     404.54 0.0050489 ** \necg   1   1.8701       602     402.67 0.1714609    \nhpt   1   3.3159       601     399.35 0.0686113 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Assessing Predictive Effect-Size (Anologue to $R^2_\\mathrm{adj}$)\n\nWhile the LRT assesses overall model significance (in-sample fit), it's also crucial to evaluate how well the model predicts new, unseen data (out-of-sample performance). A common method is to split the data into a training set (e.g., 2/3 of the data) and a test set (e.g., 1/3). The model is fit using only the training data and then used to make predictions for the test data. We can then compare these predictions to the actual outcomes in the test set.\n\n### Understanding the Confusion Matrix and Metrics\n\nTo evaluate a model's predictive performance, we classify its probabilistic predictions using a threshold (typically 0.5) and compare them to the true outcomes in a **Confusion Matrix**:\n\n|                 | **Predicted: 0** | **Predicted: 1** |\n| :-------------- | :-------------------- | :-------------------- |\n| **Actual: 0** | True Negative (TN)    | False Positive (FP)   |\n| **Actual: 1** | False Negative (FN)   | True Positive (TP)    |\n\nFrom this matrix, we derive several key performance metrics:\n\n* **Misclassification Error Rate (ER):** The proportion of all predictions that were incorrect.\n    $$\n    \\text{Error Rate} = \\frac{FP + FN}{TP + TN + FP + FN}\n    $$\n    \n\n* **Precision (Positive Predictive Value):** Answers: \"Of all the times the model predicted positive, how often was it correct?\" This is crucial when the cost of a **False Positive** is high.\n    $$\n    \\text{Precision} = \\frac{TP}{TP + FP}\n    $$\n    \n \n* **Recall (Sensitivity or True Positive Rate):** Answers: \"Of all the actual positive cases, how many did the model find?\" This is crucial when the cost of a **False Negative** is high.\n    $$\n    \\text{Recall (TPR)} = \\frac{TP}{TP + FN}\n    $$\n    \n\n* **ROC Curve and AUC:** An **ROC (Receiver Operating Characteristic) Curve** is a graph that shows a model's diagnostic ability across *all possible classification thresholds*. It plots the **True Positive Rate (Recall)** on the y-axis against the **False Positive Rate** (FPR = $\\frac{FP}{FP + TN}$) on the x-axis.\n    * **Interpretation:** The curve shows the trade-off between sensitivity (finding all the positives) and specificity (not mislabeling negatives). A random \"no-skill\" classifier is represented by a diagonal line from (0,0) to (1,1). A perfect classifier would hug the **top-left corner** (TPR = 1, FPR = 0).\n    * **AUC (Area Under the Curve):** The AUC summarizes the entire curve into a single number from 0 to 1. An AUC of 0.5 corresponds to a random guess, while an AUC of 1.0 represents a perfect model.\n\n* **Precision-Recall (PR) Curve:** A **PR Curve** plots **Precision** (y-axis) against **Recall** (x-axis) at all possible thresholds.\n    * **Interpretation:** This curve shows the trade-off between how *reliable* a positive prediction is (Precision) and how *complete* the model is at finding all positives (Recall).\n    * **When to Use:** The PR curve is particularly informative when the dataset is **imbalanced** (i.e., one class, like \"fraud\" or \"disease,\" is much rarer than the other). Unlike the ROC curve, the PR curve's baseline (the \"no-skill\" line) is a horizontal line at the proportion of positive cases, which makes it easier to see if the model is performing significantly better than chance in a low-positive-rate scenario. A perfect classifier would hug the **top-right corner** (Precision = 1, Recall = 1).\n\n### Illustration with the Simulated Dataset\n\nThis section applies the train/test split and model evaluation workflow to the `sim.data` created in the previous step.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load the pROC library for AUC calculation\n## install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n## --- 1. Split the data ---\n## We use 'sim.data' which has 200 rows\nset.seed(123) # for reproducibility\nn_sim <- nrow(sim.data)\ntrain_size_sim <- floor(2/3 * n_sim)\ntrain_indices_sim <- sample(1:n_sim, size = train_size_sim)\ntrain_data_sim <- sim.data[train_indices_sim, ]\ntest_data_sim  <- sim.data[-train_indices_sim, ]\n\n## --- 2. Refit the model on the training data ---\n## We fit the model y ~ x on the training data\nfit_train_sim <- glm(\n  y ~ x,\n  data = train_data_sim,\n  family = binomial(link = \"logit\")\n)\n\n## --- 3. Make predictions on the test data ---\n## Note: The true probabilities 'p' are also in test_data_sim\n## We predict from the *fitted* model\npred_probs_sim <- predict(fit_train_sim, newdata = test_data_sim, type = \"response\")\n```\n:::\n\n\n\n\n**Plotting the Predictive Probabilities with True Labels**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## --- 5. Plot sorted predicted probabilities ---\n\n## Create a data frame for plotting\nplot_data_sim <- data.frame(\n  Prob = pred_probs_sim,\n  Actual = as.factor(test_data_sim$y),\n  TrueProb = test_data_sim$p # Include true probs for comparison\n)\n\n## Sort by predicted probability\nplot_data_sim <- plot_data_sim[order(plot_data_sim$Prob), ]\nplot_data_sim$Rank <- 1:nrow(plot_data_sim)\n\n## Create the plot\nplot(\n  plot_data_sim$Rank,\n  plot_data_sim$Prob,\n  pch = ifelse(plot_data_sim$Actual == 0, 1, 4),\n  col = ifelse(plot_data_sim$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class (Simulated Data)\",\n  ylim = c(0, 1)\n)\nabline(h = 0.5, lty = 2, col = \"black\")\nabline(h = 0.1, lty = 3, col = \"grey\")\n\n## Add the true probability curve (sorted by predicted prob)\n## This shows how well the fitted model's predictions align with the true probs\n#lines(plot_data_sim$Rank, plot_data_sim$TrueProb[order(plot_data_sim$Prob)], col = \"darkgreen\", lwd = 2)\n\n\n## Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  lty = c(NA, NA),\n  lwd = c(NA, NA),\n  col = c(\"blue\", \"red\")\n)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/plot-sorted-probabilities-sim-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**Confusion Matrix with threshold=0.5**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## --- 4. Assess accuracy ---\n\n## 4a. Misclassification Error Rate (using 0.5 threshold)\nthreshold <- 0.5\npred_class_sim <- ifelse(pred_probs_sim > threshold, 1, 0)\nconf_matrix_sim <- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n## --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix (Counts, threshold = 0.5):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(conf_matrix_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual  0  1\n     0 26  5\n     1  9 27\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n```\n\n\n:::\n\n```{.r .cell-code}\n## margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual     0     1\n     0 0.839 0.161\n     1 0.250 0.750\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n```\n\n\n:::\n\n```{.r .cell-code}\n## margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual     0     1\n     0 0.743 0.156\n     1 0.257 0.844\n```\n\n\n:::\n\n```{.r .cell-code}\n## --- MODIFIED LINES END ---\n\n\n## Check if matrix has 2x2 dimensions, otherwise metrics will fail\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN <- conf_matrix_sim[1, 1]\n  FP <- conf_matrix_sim[1, 2]\n  FN <- conf_matrix_sim[2, 1]\n  TP <- conf_matrix_sim[2, 2]\n\n  ## Calculate metrics\n  error_rate <- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall <- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR <- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision <- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMisclassification Error Rate: 0.209 \nTrue Positive Rate (Recall): 0.75 \nFalse Positive Rate: 0.1613 \nPrecision: 0.8438 \n```\n\n\n:::\n:::\n\n\n\n\n**Confusion Matrix with threshold=0.1**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.1\npred_class_sim <- ifelse(pred_probs_sim > threshold, 1, 0)\nconf_matrix_sim <- table(Actual = test_data_sim$y, Predicted = pred_class_sim)\n\n## --- MODIFIED LINES START ---\ncat(\"Confusion Matrix (Counts, threshold = 0.1):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix (Counts, threshold = 0.1):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(conf_matrix_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual  0  1\n     0 15 16\n     1  1 35\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRow Proportions (Given Actual, % Predicted -- Relates to TPR/FPR):\n```\n\n\n:::\n\n```{.r .cell-code}\n## margin = 1 calculates proportions across rows\nprint(round(prop.table(conf_matrix_sim, margin = 1), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual     0     1\n     0 0.484 0.516\n     1 0.028 0.972\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nColumn Proportions (Given Predicted, % Actual -- Relates to Precision):\n```\n\n\n:::\n\n```{.r .cell-code}\n## margin = 2 calculates proportions across columns\nprint(round(prop.table(conf_matrix_sim, margin = 2), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Predicted\nActual     0     1\n     0 0.938 0.314\n     1 0.062 0.686\n```\n\n\n:::\n\n```{.r .cell-code}\n## --- MODIFIED LINES END ---\n\n\n## Check if matrix has 2x2 dimensions\nif (all(dim(conf_matrix_sim) == c(2, 2))) {\n  TN <- conf_matrix_sim[1, 1]\n  FP <- conf_matrix_sim[1, 2]\n  FN <- conf_matrix_sim[2, 1]\n  TP <- conf_matrix_sim[2, 2]\n\n  ## Calculate metrics\n  error_rate <- (FP + FN) / (TP + TN + FP + FN)\n  TPR_Recall <- TP / (TP + FN) # True Positive Rate (Recall / Sensitivity)\n  FPR <- FP / (FP + TN)      # False Positive Rate (1 - Specificity)\n  Precision <- TP / (TP + FP)  # Positive Predictive Value\n\n  cat(paste(\"\\nMisclassification Error Rate:\", round(error_rate, 4), \"\\n\"))\n  cat(paste(\"True Positive Rate (Recall):\", round(TPR_Recall, 4), \"\\n\"))\n  cat(paste(\"False Positive Rate:\", round(FPR, 4), \"\\n\"))\n  cat(paste(\"Precision:\", round(Precision, 4), \"\\n\"))\n} else {\n  cat(\"\\nCannot calculate full metrics: model predicted only one class.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMisclassification Error Rate: 0.2537 \nTrue Positive Rate (Recall): 0.9722 \nFalse Positive Rate: 0.5161 \nPrecision: 0.6863 \n```\n\n\n:::\n:::\n\n\n\n\n**ROC curve and Area Under the ROC (AUC)**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 4b. Area Under the Curve (AUC)\nroc_curve_sim <- roc(test_data_sim$y, pred_probs_sim, quiet = TRUE)\n\n## Plot the ROC curve\nplot(roc_curve_sim, main = \"ROC Curve (Simulated Test Data)\", print.auc = TRUE)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nauc_value_sim <- auc(roc_curve_sim)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value_sim, 4), \"\\n\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea Under the Curve (AUC): 0.8996 \n```\n\n\n:::\n:::\n\n\n\n\n**PR curve and Area Under PR Curve (AUPR)**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load the ROCR library\n## install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n## --- 1. Create a 'prediction' object ---\n## 'prediction' takes all predictions and all true labels\npred_obj <- prediction(pred_probs_sim, test_data_sim$y)\n\n## --- 2. Create a 'performance' object for PR ---\n## \"prec\" is for precision, \"rec\" is for recall\nperf_pr <- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n## --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc <- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value <- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea Under PR Curve (AUPR): 0.9157 \n```\n\n\n:::\n\n```{.r .cell-code}\n## --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Simulated Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n## --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision_sim <- sum(test_data_sim$y == 1) / length(test_data_sim$y)\nabline(h = baseline_precision_sim, col = \"blue\", lty = 2)\n\n## --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # <-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision_sim, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/pr-curve-rocr-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n### Application to the CHD Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load the pROC library for AUC calculation\n## install.packages(\"pROC\") # Uncomment to install if needed\nlibrary(pROC)\n\n## --- 1. Split the data ---\nset.seed(123) # for reproducibility\nn <- nrow(CHD.data)\ntrain_size <- floor(2/3 * n)\ntrain_indices <- sample(1:n, size = train_size)\ntrain_data <- CHD.data[train_indices, ]\ntest_data  <- CHD.data[-train_indices, ]\n\n## --- 2. Refit the model on the training data ---\nfit_train <- glm(\n  chd ~ smk + cat + sbp + age + chl + ecg + hpt,\n  data = train_data,\n  family = binomial(link = \"logit\")\n)\n\n## --- 3. Make predictions on the test data ---\npred_probs <- predict(fit_train, newdata = test_data, type = \"response\")\n```\n:::\n\n\n\n\n**Plot Predictive Probabilities**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## --- 5. Plot sorted predicted probabilities ---\n\n## Create a data frame for plotting\nplot_data <- data.frame(\n  Prob = pred_probs,\n  Actual = as.factor(test_data$chd)\n)\n\n## Sort by predicted probability\nplot_data <- plot_data[order(plot_data$Prob), ]\nplot_data$Rank <- 1:nrow(plot_data)\n\n## Create the plot\n## We use 'pch' (plot character) to set different symbols\n## 'pch = 1' is 'o' (default)\n## 'pch = 4' is 'x'\nplot(\n  plot_data$Rank,\n  plot_data$Prob,\n  pch = ifelse(plot_data$Actual == 0, 1, 4),\n  col = ifelse(plot_data$Actual == 0, \"blue\", \"red\"),\n  xlab = \"Index (Sorted by Predicted Probability)\",\n  ylab = \"Log-odds of Predicted Probability\",\n  main = \"Predicted Probabilities vs. Actual Class\",\n  ylim = c(0,1)\n)\nabline(h=0.5)\nabline(h=0.1, col=\"grey\")\n\n## Add a legend\nlegend(\n  \"topleft\",\n  legend = c(\"Actual 0 (o)\", \"Actual 1 (x)\"),\n  pch = c(1, 4),\n  col = c(\"blue\", \"red\")\n)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/plot-sorted-probabilities-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**ROC curve and Area Under the ROC (AUC)**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 4b. Area Under the Curve (AUC)\nroc_curve <- roc(test_data$chd, pred_probs, quiet = TRUE)\n\n## Plot the ROC curve\nplot(roc_curve, main = \"ROC Curve (Test Data)\", print.auc = TRUE)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nauc_value <- auc(roc_curve)\ncat(paste(\"Area Under the Curve (AUC):\", round(auc_value, 4), \"\\n\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea Under the Curve (AUC): 0.6872 \n```\n\n\n:::\n:::\n\n\n\n\n**PR curve and Area Under PR Curve (AUPR)**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load the ROCR library\n## install.packages(\"ROCR\") # Uncomment to install if needed\nlibrary(ROCR)\n\n## --- 1. Create a 'prediction' object ---\n## 'prediction' takes all predictions and all true labels\n## We use 'pred_probs' and 'test_data$chd' from the CHD data split\npred_obj <- prediction(pred_probs, test_data$chd)\n\n## --- 2. Create a 'performance' object for PR ---\n## \"prec\" is for precision, \"rec\" is for recall\nperf_pr <- performance(pred_obj, measure = \"prec\", x.measure = \"rec\")\n\n## --- 3. Calculate Area Under the PR Curve (AUPR) ---\nperf_auc <- performance(pred_obj, measure = \"aucpr\") # \"aucpr\" = Area Under PR Curve\naupr_value <- perf_auc@y.values[[1]]\ncat(paste(\"Area Under PR Curve (AUPR):\", round(aupr_value, 4), \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea Under PR Curve (AUPR): 0.2826 \n```\n\n\n:::\n\n```{.r .cell-code}\n## --- 4. Plot the performance object ---\nplot(perf_pr, \n     main = \"Precision-Recall Curve (Test Data)\", \n     xlim = c(0, 1), \n     ylim = c(0, 1),\n     col = \"black\")\n\n## --- 5. Calculate and add the 'no-skill' baseline ---\nbaseline_precision <- sum(test_data$chd == 1) / length(test_data$chd)\nabline(h = baseline_precision, col = \"blue\", lty = 2)\n\n## --- 6. Add a legend with AUPR ---\nlegend(\"bottomleft\", \n       legend = c(\n           paste(\"Model (AUPR =\", round(aupr_value, 4), \")\"),  # <-- MODIFIED LINE\n           paste(\"Baseline (\", round(baseline_precision, 3), \")\")\n       ), \n       col = c(\"black\", \"blue\"), \n       lty = c(1, 2), \n       bty = \"n\") # bty=\"n\" removes the box\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-pdf/pr-curve-rocr-chd-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "logistic_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}