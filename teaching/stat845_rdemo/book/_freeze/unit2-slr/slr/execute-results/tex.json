{
  "hash": "3258ba1df62eef4a3c4c3da548ffcb6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simple Linear Regression\"\nsubtitle: \"A Simulation Illustration with R\"\nauthor:\n  - name: \"Longhai Li\"\n    affiliation: \"University of Saskatchewan\"\n    url: \"https://longhaisk.github.io/\"\ndate: today\nformat: \n  html\neditor: source\neditor_options:\n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(\"knitr\")\nknitr::opts_chunk$set(\n  comment = \"#\",\n  fig.width = 8,\n  fig.height = 6,\n  cache = TRUE\n)\nset.seed(47)\n\noptions(sim_rebuild=FALSE)\n```\n:::\n\n\n\n\n\n\n\n## Overview of Simple Linear Regression\n\nTo make the simple linear regression model concrete, let’s first visualize a simulated dataset that follows\n$$\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\qquad\n\\varepsilon_i \\sim \\mathcal N(0, \\sigma^2).\n$$\n\nHere, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ represents random noise.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\nn <- 40\nbeta0 <- 2; beta1 <- 1.5; sigma <- 2\nx <- runif(n, 0, 10)\ny <- beta0 + beta1 * x + rnorm(n, 0, sigma)\ndat <- data.frame(x, y)\n\nfit <- lm(y ~ x, data = dat)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"Predictor X\", ylab = \"Response Y\",\n     main = \"Simulated Data with Fitted Linear Regression Line\")\nabline(fit, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Observed data\", \"Fitted line\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"steelblue\", \"red\"), bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThe scatterplot shows data points scattered around a line — the red line is the fitted regression model.\n\n---\n\n### Least Squares Estimation\n\n**Goal:** Find $\\hat\\beta_0$ and $\\hat\\beta_1$ that minimize\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2.\n$$\n\n**Solutions:**\n$$\n\\hat\\beta_1 = \\frac{\\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2}\n= \\frac{S_{xy}}{S_{xx}},\n\\qquad\n\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\,\\bar x.\n$$\n\nHere\n$$\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y),\n\\qquad\nS_{xx} = \\sum_i (x_i - \\bar x)^2.\n$$\n\n**Shortcut (computational) formulas:**\n$$\nS_{xy} = \\sum_i x_i y_i - n\\,\\bar x\\,\\bar y,\n\\qquad\nS_{xx} = \\sum_i x_i^2 - n\\,\\bar x^2.\n$$\n\n**Interpretation:**  \n- $\\hat\\beta_1$ measures the estimated change in $Y$ for each unit increase in $X$.  \n- $\\hat\\beta_0$ represents the fitted value of $Y$ when $X=0$.\n\n---\n\n### Residual and Sum of Squares Definitions\n\nLet $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i$ and $e_i = y_i - \\hat y_i$.\n\n| Symbol  | Definition | Computing Formula (in terms of $S_{xx}, S_{xy}$, etc.) |\n|:--------|:------------|:------------------------------------------------------|\n| **SST** | Total Sum of Squares  | $\\displaystyle \\sum_i (y_i - \\bar y)^2 = S_{yy} = \\sum_i y_i^2 - n\\,\\bar y^2$ |\n| **SSR** | Regression Sum of Squares | $\\displaystyle \\sum_i (\\hat y_i - \\bar y)^2 = \\hat\\beta_1^2 S_{xx} = \\frac{S_{xy}^2}{S_{xx}}$ |\n| **SSE** | Error (Residual) Sum of Squares | $\\displaystyle \\sum_i (y_i - \\hat y_i)^2 = S_{yy} - \\frac{S_{xy}^2}{S_{xx}}$ |\n\n**Identity:**\n$$\n\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}.\n$$\n\nHere,\n$$\nS_{xx} = \\sum_i (x_i - \\bar x)^2 = \\sum_i x_i^2 - n\\bar x^2, \\qquad\nS_{yy} = \\sum_i (y_i - \\bar y)^2 = \\sum_i y_i^2 - n\\bar y^2, \\qquad\nS_{xy} = \\sum_i (x_i - \\bar x)(y_i - \\bar y) = \\sum_i x_i y_i - n\\bar x \\bar y.\n$$\n\n---\n\n### Coefficient of Determination ($R^2$)\n\nMeasures the proportion of total variation in $Y$ explained by $X$:\n$$\nR^2 = \\frac{\\mathrm{SSR}}{\\mathrm{SST}}\n= 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}.\n$$\n\n**Interpretation:**\n\n* $R^2 = 1$ means perfect linear fit;\n* $R^2 = 0$ means the model explains none of the variation.\n\n---\n\n\n### F-test for Overall Significance\n\nTests whether $X$ is linearly related to $Y$.\n\n**Hypotheses:**\n$$\nH_0: \\beta_1 = 0\n\\quad \\text{vs.} \\quad\nH_A: \\beta_1 \\ne 0.\n$$\n\n**Test Statistic:**\n$$\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n= \\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\n\\sim F_{1,n-2}\\quad (H_0).\n$$\n\n\n**p-value approach for observe $F^{\\mathrm{obs}}$:**  \n\nGiven the observed statistic \\(F^{\\text{obs}}\\) with \\((1,\\,n-2)\\) df,\n$$\np-\\text{value} \\;=\\; \\Pr\\!\\big(F_{1,\\,n-2} \\ge F^{\\text{obs}}\\big)\n\\;=\\; \\mathrm{pf}\\!\\big(F^{\\text{obs}},\\,1,\\,n-2,\\ \\text{lower.tail}= \\mathrm{FALSE}\\big).\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## -- Inputs (provide these from your analysis context) -------------------------\n## n   <- ...   # sample size\n## SSR <- ...   # regression sum of squares\n## SSE <- ...   # error sum of squares\nn   <- 20\nSSR <- 5\nSSE <- 40\n\n\n\n\ndf1  <- 1\ndf2  <- n - 2\nFobs <- (SSR/df1) / (SSE/df2)         # observed F\npval <- pf(Fobs, df1 = df1, df2 = df2, lower.tail = FALSE)\npval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1509505\n```\n\n\n:::\n\n```{.r .cell-code}\n## -- Plot F density and shade the p-value tail (with proper annotations) -------\nxmax <- max(qf(0.995, df1, df2), Fobs * 1.2)  # extra space for labels\npeak <- max(df(seq(0, xmax, length.out = 500), df1, df2))\n\n## Density curve\ncurve(df(x, df1, df2), from = 0, to = xmax,\n      xlab = \"F\", ylab = \"Density\",\n      main = sprintf(\"F(%d, %d) density  |  observed F = %.3f\", df1, df2, Fobs))\n\n## Shade right tail (p-value region)\nxs <- seq(Fobs, xmax, length.out = 300)\nys <- df(xs, df1, df2)\npolygon(c(Fobs, xs, xmax), c(0, ys, 0),\n        col = rgb(0, 0, 0, 0.18), border = NA)\n\n## Vertical line at Fobs (optional visual aid)\nabline(v = Fobs, lwd = 2)\n\n## ---- Annotation for F^obs pointing to the x-axis value (Fobs, 0) -------------\nx_txt_F <- Fobs + 0.06 * xmax\ny_txt_F <- 0.45 * peak\narrows(x0 = x_txt_F, y0 = y_txt_F, x1 = Fobs, y1 = 0,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_F, y_txt_F,\n     labels = bquote(F^{obs} == .(format(Fobs, digits = 3))),\n     pos = 4)\n\n## ---- Annotation for p-value pointing into the shaded tail --------------------\nx_tip_p <- (Fobs + xmax) / 1.7\ny_tip_p <- df(x_tip_p, df1, df2)\nx_txt_p <- Fobs + 0.08 * xmax\ny_txt_p <- 0.80 * peak\narrows(x0 = x_txt_p, y0 = y_txt_p, x1 = x_tip_p, y1 = y_tip_p,\n       length = 0.08, lwd = 1.5)\ntext(x_txt_p, y_txt_p,\n     labels = bquote(p == .(format(pval, digits = 4, scientific = TRUE))),\n     pos = 4)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n---\n\n### t-test for the Slope $\\beta_1$\n\nEquivalent to the $F$-test in simple regression since $t^2 = F$.\n\n**Formula:**\n$$\nt = \\frac{\\hat\\beta_1}{\\operatorname{SE}(\\hat\\beta_1)},\n\\qquad\n\\operatorname{SE}(\\hat\\beta_1) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_i (x_i - \\bar x)^2}},\n\\qquad\n\\hat\\sigma^2 = \\frac{\\mathrm{SSE}}{n-2}.\n$$\n\n**Distribution:**\n$$\nt \\sim t_{n-2}\\quad (H_0:\\beta_1=0).\n$$\n\n---\n\n### Prediction for a New Case $x_0$\n\n**Predicted mean response:**\n$$\n\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0.\n$$\n\n**95% Confidence interval for mean response:**\n$$\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n$$\n\n**95% Prediction interval for a new observation:**\n$$\n\\hat y(x_0) \\pm t_{1-\\alpha/2,,n-2},\n\\hat\\sigma,\\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{\\sum_i (x_i - \\bar x)^2}}.\n$$\n\n---\n\n**Summary Cheat Sheet**\n\n| Concept       | Key Formula                                                               |\n| :------------ | :------------------------------------------------------------------------ |\n| Model         | $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$                             |\n| LS Estimates  | $\\hat\\beta_1 = S_{xy}/S_{xx}$, $\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x$ |\n| Decomposition | $\\mathrm{SST} = \\mathrm{SSR} + \\mathrm{SSE}$                              |\n| $R^2$         | $R^2 = 1 - \\mathrm{SSE}/\\mathrm{SST}$                                     |\n| $F$-test      | $F = (\\mathrm{SSR}/1)/(\\mathrm{SSE}/(n-2))$                               |\n| $t$-test      | $t = \\hat\\beta_1 / \\operatorname{SE}(\\hat\\beta_1)$                        |\n| Prediction    | $\\hat y(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0$                             |\n\n\n\n\n\n\n## Example 1: Vehicle Insurance Premium (warm-up)\n\nWe examine premiums $y$ vs. driving amount $x$. The scatterplot hints at a **downward** trend.\n\n### Input data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nissu <- data.frame(\n  driving = c(5, 2, 12, 9, 15, 6, 25, 16),\n  premium = c(64, 87, 50, 71, 44, 56, 42, 60)\n)\n\ny <- issu$premium\nx <- issu$driving\nxbar <- mean(x); ybar <- mean(y); n <- length(y)\n\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Vehicle Insurance: Premium vs. Driving\")\nabline(h = ybar, lty = 3)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**Narrative.** The horizontal line at $\\bar y$ represents the intercept-only model. Any fitted line that tilts away from this must earn its keep by reducing residual variation enough to offset the loss of one degree of freedom.\n\n### Estimating regression coefficients\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.issu <- lm(y ~ x)\nplot(x, y, xlab = \"Driving\", ylab = \"Premium\",\n     main = \"Fitted Simple Linear Regression\")\nabline(fit.issu, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nThe slope estimate $\\hat\\beta_1$ captures the **marginal change in premium per unit of driving** (units of $y$ per unit of $x$). Inference on $\\beta_1$ tells us whether the pattern rises above noise.\n\n### Residuals and fitted values (geometry picture)\n\nLet $\\hat y_i=\\hat\\beta_0+\\hat\\beta_1 x_i$ and $\\tilde y_i=\\bar y$. Residuals are $e_i=y_i-\\hat y_i$ (model) and $y_i-\\bar y$ (null). Visualizing all three clarifies the ANOVA identity.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta0 <- coef(fit.issu)[1]\nbeta1 <- coef(fit.issu)[2]\nfitted1 <- beta0 + beta1 * x\nfitted0 <- rep(ybar, n)\nresidual1 <- y - fitted1\nresidual0 <- y - fitted0\n\ndata.frame(y, fitted0, residual0, fitted1, residual1,\n           diff.fitted = fitted1 - fitted0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   y fitted0 residual0  fitted1  residual1 diff.fitted\n1 64   59.25      4.75 68.92243  -4.922425    9.672425\n2 87   59.25     27.75 73.56519  13.434811   14.315189\n3 50   59.25     -9.25 58.08931  -8.089309   -1.160691\n4 71   59.25     11.75 62.73207   8.267927    3.482073\n5 44   59.25    -15.25 53.44654  -9.446545   -5.803455\n6 56   59.25     -3.25 67.37484 -11.374837    8.124837\n7 42   59.25    -17.25 37.97066   4.029335  -21.279335\n8 60   59.25      0.75 51.89896   8.101043   -7.351043\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-7-1.pdf)\n:::\n:::\n\n\n\n\n### SST, SSR, SSE and their meanings\n\n* $\\text{SST}=\\sum (y_i-\\bar y)^2$ quantifies **total** variability around the mean.\n* $\\text{SSR}=\\sum (\\hat y_i-\\bar y)^2$ is the part **explained by $x$**.\n* $\\text{SSE}=\\sum (y_i-\\hat y_i)^2$ is the **leftover** (unexplained) variability.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSST <- sum((y - fitted0)^2); SST\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1557.5\n```\n\n\n:::\n\n```{.r .cell-code}\nSSE <- sum((y - fitted1)^2); SSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 639.0065\n```\n\n\n:::\n\n```{.r .cell-code}\nSSR <- SST - SSE; SSR\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 918.4935\n```\n\n\n:::\n:::\n\n\n\n\nDirect check: $\\text{SSR}=\\sum(\\hat y_i-\\bar y)^2$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((fitted1 - fitted0)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 918.4935\n```\n\n\n:::\n:::\n\n\n\n\n### Visual ANOVA on an RSS plot\n\nWe place the **residual sum of squares** against model dimension to show the trade-off between fit and df.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Recompute cleanly\nSST <- sum((y - mean(y))^2)\nSSE <- sum(resid(fit.issu)^2)\nSSR <- SST - SSE\ndf_SSR <- 1\ndf_SSE <- n - 2\n\npar(mar = c(6, 4, 4, 2) + 0.1)\nplot(c(1, 2, n), c(SST, SSE, 0), type = \"b\", pch = 19,\n     xlab = \"Number of Parameters in Model\",\n     ylab = \"Residual Sum of Squares (RSS)\",\n     main = \"ANOVA Geometry on RSS vs. Model Size\",\n     xlim = c(0, 14), ylim = c(-400, SST * 1.1), xaxt = \"n\")\naxis(1, at = c(1, 2, n), labels = c(\"1 (Intercept)\", \"2 (+Slope)\", paste(n, \"(Saturated)\")))\nabline(h = seq(0, 2000, by = 100), lty = 3, col = \"grey\")\n\npar(xpd = TRUE)\narrows(9, 0, 9, SSE, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, SSE/2, \"SSE\", col = \"blue\", pos = 4, font = 2, cex = 1.2)\n\narrows(9, SSE, 9, SST, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(9, (SST + SSE)/2, \"SSR\", col = \"red\", pos = 4, font = 2, cex = 1.2)\n\narrows(2, -200, n, -200, col = \"blue\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext((2 + n)/2, -250, paste(\"df_SSE =\", df_SSE), col = \"blue\", font = 2)\n\narrows(1, -200, 2, -200, col = \"red\", code = 3, angle = 90, length = 0.1, lwd = 2)\ntext(1.5, -250, paste(\"df_SSR =\", df_SSR), col = \"red\", font = 2)\npar(xpd = FALSE)\n\nf_value <- (SSR/df_SSR) / (SSE/df_SSE)\np_value <- pf(f_value, df1 = df_SSR, df2 = df_SSE, lower.tail = FALSE)\nlegend(\"topright\",\n       legend = c(sprintf(\"F-statistic: %.2f\", f_value),\n                  sprintf(\"p-value: %.3f\", p_value)),\n       title = \"ANOVA Results\", bty = \"o\", cex = 0.9)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### $R^2$, $F$ and a compact ANOVA table\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2 <- SSR / SST; R2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5897229\n```\n\n\n:::\n\n```{.r .cell-code}\nf  <- (SSR/1) / (SSE/(n-2)); f\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.624264\n```\n\n\n:::\n\n```{.r .cell-code}\npvf <- pf(f, df1 = 1, df2 = n-2, lower.tail = FALSE); pvf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0260588\n```\n\n\n:::\n\n```{.r .cell-code}\nFtable <- data.frame(\n  Source = c(\"Regression\", \"Error\"),\n  df     = c(1, n - 2),\n  SS     = c(SSR, SSE),\n  MS     = c(SSR/1, SSE/(n-2)),\n  F      = c(f, NA),\n  pvalue = c(pvf, NA),\n  R2part = c(SSR, SSE) / SST\n)\nFtable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Source df       SS       MS        F    pvalue    R2part\n1 Regression  1 918.4935 918.4935 8.624264 0.0260588 0.5897229\n2      Error  6 639.0065 106.5011       NA        NA 0.4102771\n```\n\n\n:::\n:::\n\n\n\n\nA call to `anova()` reproduces the same test:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit.issu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nx          1 918.49  918.49  8.6243 0.02606 *\nResiduals  6 639.01  106.50                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Sampling distributions via animation\n\nUnder $H_0:\\beta_1=0$, $F$ follows $F_{1,n-2}$. Under $H_A$, the distribution shifts right (noncentral $F$).\n\n#### Null world ($H_0$ true)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Simulation under H0: animated GIF (HTML) and static PNG (PDF).](figs/sim_h0.png){#fig-simulation fig-align='center' width=5.33in}\n:::\n:::\n\n\n\n#### Alternative world ($H_1$ true)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Simulation under HA (slope = -2): animated GIF for HTML, static PNG for PDF.](figs/sim_HA.png){#fig-simulation-HA fig-align='center' width=5.33in}\n:::\n:::\n\n\n\n\n---\n\n## Example 2: Oxygen Purity Data\n\nWe model oxygen purity $y$ as a function of hydrocarbon level $x$ and report both **mean response** and **prediction** uncertainty.\n\n### Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(0.99, 1.02, 1.15, 1.29, 1.46, 1.36, 0.87, 1.23, 1.55, 1.40, 1.19,\n       1.15, 0.98, 1.01, 1.11, 1.20, 1.26, 1.32, 1.43, 0.95)\ny <- c(90.01, 89.05, 91.43, 93.74, 96.73, 94.45, 87.59, 91.77, 99.42, 93.65,\n       93.54, 92.52, 90.56, 89.54, 89.85, 90.39, 93.25, 93.41, 94.98, 87.33)\nn <- length(x); n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20\n```\n\n\n:::\n\n```{.r .cell-code}\npurity.data <- data.frame(x = x, y = y)\nhead(purity.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     x     y\n1 0.99 90.01\n2 1.02 89.05\n3 1.15 91.43\n4 1.29 93.74\n5 1.46 96.73\n6 1.36 94.45\n```\n\n\n:::\n:::\n\n\n\n\n### Fit and quick summary\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ x, data = purity.data)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = purity.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83029 -0.73334  0.04497  0.69969  1.96809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   74.283      1.593   46.62  < 2e-16 ***\nx             14.947      1.317   11.35 1.23e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.087 on 18 degrees of freedom\nMultiple R-squared:  0.8774,\tAdjusted R-squared:  0.8706 \nF-statistic: 128.9 on 1 and 18 DF,  p-value: 1.227e-09\n```\n\n\n:::\n:::\n\n\n\n\n**Interpretation.** The slope’s sign gives the direction of association; its $t$ test (or $F$ with 1 df) assesses evidence for a trend. Look at $\\hat\\sigma$ for noise scale and $R^2$ for variance explained.\n\n### Scatter with fitted line\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Oxygen Purity vs Hydrocarbon Level\")\nabline(fit, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/plot-fit-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Coefficient CIs and ANOVA\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(fit, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %   97.5 %\n(Intercept) 70.93555 77.63108\nx           12.18107 17.71389\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nx          1 152.13 152.127  128.86 1.227e-09 ***\nResiduals 18  21.25   1.181                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Mean-response and prediction bands\n\nThe **mean-response CI** narrows near $\\bar x$ and widens at the extremes; the **prediction band** is wider by the irreducible noise term.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- seq(min(purity.data$x), max(purity.data$x), length = 50)\nnewdata <- data.frame(x = x0)\n\nest.mean <- predict(fit, newdata = newdata, interval = \"confidence\", level = 0.95)\npred.new <- predict(fit, newdata = newdata, interval = \"prediction\", level = 0.95)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(purity.data$x, purity.data$y,\n     xlab = \"Hydrocarbon level (x)\", ylab = \"Purity (y)\",\n     main = \"Regression Line with Confidence and Prediction Bands\")\nabline(fit)\nmatlines(x0, est.mean[, 2:3], col = \"blue\", lty = 2, lwd = 2)\nmatlines(x0, pred.new[, 2:3], col = \"red\",  lty = 3, lwd = 2)\nlegend(\"topleft\", c(\"Confidence Bands (mean)\", \"Prediction Bands (new y)\"),\n       col = c(\"blue\", \"red\"), lty = 2:3, bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/plot-bands-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Residual diagnostics (assumptions check)\n\nWe look for **no pattern** in residuals vs. fits and **approximate straightness** in the Q–Q plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- fitted.values(fit)\ne <- resid(fit)\nd <- e / summary(fit)$sigma\n\npar(mfrow = c(2,2))\nplot(purity.data$x, purity.data$y, xlab = \"x\", ylab = \"y\"); abline(fit)\nqqnorm(d, main = \"Normal Q–Q\"); qqline(d)\nplot(pred, d, xlab = \"Fitted\", ylab = \"Std. residuals\", main = \"Residuals vs Fits\"); abline(h = 0, lty = 2)\nplot(1:n, d, xlab = \"Order\", ylab = \"Std. residuals\", main = \"Residuals vs Order\"); abline(h = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/residual-analysis-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\n```\n:::\n\n\n\n\n---\n\n## Correlation analysis (for comparison, not causation)\n\nCorrelation summarizes linear association without fitting a line or making model assumptions.\n\n### Data and scatter\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstrength <- c(9.95,24.45,31.75,35.00,25.02,16.86,14.38,9.60,24.35,\n              27.50,17.08,37.00,41.95,11.66,21.65,17.89,69.00,10.30,\n              34.93,46.59,44.88,54.12,56.63,22.13,21.15)\nlength <- c(2,8,11,10,8,4,2,2,9,8,4,11,12,2,4,4,20,1,10,\n            15,15,16,17,6,5)\nplot(length, strength, xlab = \"Length\", ylab = \"Strength\",\n     main = \"Strength vs Length (scatter)\")\n```\n\n::: {.cell-output-display}\n![](slr_files/figure-pdf/correlation-data-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Pearson correlation and test\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(strength, length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9818118\n```\n\n\n:::\n\n```{.r .cell-code}\ncor.test(strength, length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  strength and length\nt = 24.801, df = 23, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9585414 0.9920735\nsample estimates:\n      cor \n0.9818118 \n```\n\n\n:::\n:::\n\n\n\n\n**Note.** A large $|r|$ and small $p$ indicate linear association; regression further quantifies the slope and supports prediction, with diagnostics to check assumptions.\n\n---\n\n## What to report (checklist)\n\n* Estimated line $\\hat y=\\hat\\beta_0+\\hat\\beta_1 x$ with units.\n* $t$/$F$ test for slope, $p$-value and CI for $\\beta_1$.\n* $R^2$ and $\\hat\\sigma$ (RMSE) for fit quality.\n* Mean-response and prediction intervals at substantively relevant $x_0$.\n* Residual diagnostics and any remedies (transformations, robust methods) if needed.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}