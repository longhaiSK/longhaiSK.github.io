{
  "hash": "f28051d5850d92165d17abce469a6e60",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiple Linear Regression\"\nauthor:\n  - name: \"Longhai Li\"\n    affiliation: \"University of Saskatchewan\"\n    url: \"https://longhaisk.github.io/\"\ndate: today\neditor: source\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n## An Example: Wire Bond Strength Dataset\n\n### Loading Data and Visualization\n\n**Note:** You must change the file paths in the `read.csv()` functions below to match the location of the files on your computer (for example `C:\\\\Users\\\\<YourUsername>\\\\Documents` on Windows).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Read data. Change the path as necessary.\n## Example: bond.data <- read.csv(\"wire-bond.csv\")\nbond.data <- read.csv(\"wire-bond.csv\")\n\n## This will now be automatically rendered as a paged table\nbond.data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   strength length height\n1      9.95      2     50\n2     24.45      8    110\n3     31.75     11    120\n4     35.00     10    550\n5     25.02      8    295\n6     16.86      4    200\n7     14.38      2    375\n8      9.60      2     52\n9     24.35      9    100\n10    27.50      8    300\n11    17.08      4    412\n12    37.00     11    400\n13    41.95     12    500\n14    11.66      2    360\n15    21.65      4    205\n16    17.89      4    400\n17    69.00     20    600\n18    10.30      1    585\n19    34.93     10    540\n20    46.59     15    250\n21    44.88     15    290\n22    54.12     16    510\n23    56.63     17    590\n24    22.13      6    100\n25    21.15      5    400\n```\n\n\n:::\n:::\n\n\n\n\n\n\n**2D Visualization**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3), mar = c(5, 4, 2, 1))\n\n## 1) length vs strength\ni1 <- which(!is.na(bond.data$length) & !is.na(bond.data$strength))\nplot(bond.data$length[i1], bond.data$strength[i1],\n     xlab = \"Wire Length\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$length[i1], bond.data$strength[i1],\n     labels = i1, pos = 1, offset = 0.4, cex = 0.75)\n\n## 2) height vs strength\ni2 <- which(!is.na(bond.data$height) & !is.na(bond.data$strength))\nplot(bond.data$height[i2], bond.data$strength[i2],\n     xlab = \"Die height\", ylab = \"Pull strength\", pch = 19)\ntext(bond.data$height[i2], bond.data$strength[i2],\n     labels = i2, pos = 1, offset = 0.4, cex = 0.75)\n\n## 3) height vs length\ni3 <- which(!is.na(bond.data$height) & !is.na(bond.data$length))\nplot(bond.data$height[i3], bond.data$length[i3],\n     xlab = \"Die height\", ylab = \"Length\", pch = 19)\ntext(bond.data$height[i3], bond.data$length[i3],\n     labels = i3, pos = 1, offset = 0.4, cex = 0.75)\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**3D Visualize**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(scatterplot3d)\n\npar(mfrow = c(1,1))\ns3d <- with(bond.data, scatterplot3d(\n  x = length,\n  y = height,\n  z = strength,\n  pch = 19,\n  color = \"steelblue\",\n  main = \"3D Scatterplot: Strength vs. Length and Height\",\n  xlab = \"Length\",\n  ylab = \"Height\",\n  zlab = \"Strength\",\n  angle = 60\n))\n\nfit <- lm(strength ~ length + height, data = bond.data)\ns3d$plane3d(fit, lty.box = \"solid\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Model Fitting and Summary\n\nWe fit a multiple linear regression model with `strength` as the response variable and `length` and `height` as predictors.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(strength ~ length + height, data = bond.data)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = strength ~ length + height, data = bond.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.865 -1.542 -0.362  1.196  5.841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.263791   1.060066   2.136 0.044099 *  \nlength      2.744270   0.093524  29.343  < 2e-16 ***\nheight      0.012528   0.002798   4.477 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.288 on 22 degrees of freedom\nMultiple R-squared:  0.9811,\tAdjusted R-squared:  0.9794 \nF-statistic: 572.2 on 2 and 22 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nThe summary provides the ANOVA F-test for overall significance, $R^2$, adjusted $R^2$, and t-tests for individual coefficients.\n\n### Confidence Intervals and Model Components\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Confidence intervals\nconfint(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %     97.5 %\n(Intercept) 0.065348613 4.46223426\nlength      2.550313061 2.93822623\nheight      0.006724246 0.01833138\n```\n\n\n:::\n\n```{.r .cell-code}\n## Fitted values and residuals\npred <- fitted.values(fit)\ne <- resid(fit)\ndata.frame(y = bond.data$strength, y.hat = pred, e = e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       y     y.hat           e\n1   9.95  8.378721  1.57127871\n2  24.45 25.596008 -1.14600783\n3  31.75 33.954095 -2.20409488\n4  35.00 36.596784 -1.59678413\n5  25.02 27.913653 -2.89365294\n6  16.86 15.746432  1.11356772\n7  14.38 12.450260  1.92974001\n8   9.60  8.403777  1.19622309\n9  24.35 28.214999 -3.86499936\n10 27.50 27.976292 -0.47629200\n11 17.08 18.402328 -1.32232830\n12 37.00 37.461882 -0.46188206\n13 41.95 41.458933  0.49106715\n14 11.66 12.262343 -0.60234282\n15 21.65 15.809071  5.84092866\n16 17.89 18.251995 -0.36199456\n17 69.00 64.665871  4.33412887\n18 10.30 12.336831 -2.03683074\n19 34.93 36.471506 -1.54150602\n20 46.59 46.559789  0.03021107\n21 44.88 47.060901 -2.18090138\n22 54.12 52.561290  1.55871047\n23 56.63 56.307784  0.32221591\n24 22.13 19.982190  2.14780957\n25 21.15 20.996264  0.15373580\n```\n\n\n:::\n\n```{.r .cell-code}\n## Covariance matrix and standard errors\ncov.mat <- vcov(fit)\ncov.mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             (Intercept)        length        height\n(Intercept)  1.123740429 -3.921612e-02 -1.781991e-03\nlength      -0.039216122  8.746709e-03 -9.903775e-05\nheight      -0.001781991 -9.903775e-05  7.831149e-06\n```\n\n\n:::\n\n```{.r .cell-code}\ndata.frame(std.error = sqrt(diag(cov.mat)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              std.error\n(Intercept) 1.060066238\nlength      0.093523844\nheight      0.002798419\n```\n\n\n:::\n:::\n\n\n\n\n## RSS-based Inference: F-test, and adjusted $R^2$\n\n**The General Linear Model**\n\nThe general linear model is:\n\n$$y = X\\beta + \\epsilon$$\n\n-   $y$: $n \\times 1$ vector of responses\n-   $X$: $n \\times p$ design matrix (first column often ones)\n-   $\\beta$: $p \\times 1$ parameter vector, where $p=k+1$\n-   $\\epsilon$: $n \\times 1$ error vector\n\n### RSS-Based Quantities\n\n#### RSS-Based Quantities\n\n| Source | Sum of Squares | $R^2$ | df | Mean Squares | $F$ | SS$_\\mathrm{adj}$ | $\\hat{\\sigma}^2$ | $R^2_{\\mathrm{adj}}$ |\n|:-------|:-------|:-------|:------:|:-------|:-------|:-------|:-------|:-------|\n| $x^\\top\\beta$ | $\\mathrm{SSR} = \\displaystyle \\sum_{i=1}^n (\\hat y_i - \\bar y)^2$ | $\\displaystyle \\frac{\\mathrm{SSR}}{\\mathrm{SST}}$ | $k$ | $\\displaystyle \\mathrm{MSR} = \\frac{\\mathrm{SSR}}{k}$ | $\\displaystyle \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}$ | $\\mathrm{SSR}_{\\mathrm{adj}}$ | $\\displaystyle \\hat{\\sigma}^2_{x^\\top\\beta} = \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}$ | $\\displaystyle \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{\\mathrm{SST}} = 1 - \\frac{\\mathrm{MSE}}{\\mathrm{MST}}$ |\n| $\\epsilon$ | $\\mathrm{SSE} = \\displaystyle \\sum_{i=1}^n (y_i - \\hat y_i)^2$ | — | $n-p$ | $\\displaystyle \\mathrm{MSE} = \\frac{\\mathrm{SSE}}{n-p}$ | — | $\\mathrm{SSE}$ | $\\displaystyle \\hat{\\sigma}^2_{\\epsilon} = \\mathrm{MSE}$ | — |\n| $y$ | $\\mathrm{SST} = \\displaystyle \\sum_{i=1}^n (y_i - \\bar y)^2$ | — | $n-1$ | $\\displaystyle \\mathrm{MST} = \\frac{\\mathrm{SST}}{n-1}$ | — | $\\mathrm{SST}$ | $\\displaystyle \\hat{\\sigma}^2_{y} = \\mathrm{MST}$ | — |\n\n------------------------------------------------------------------------\n\n**Interpretation of the** $\\hat{\\sigma}^2$ Column\n\nThe $\\hat{\\sigma}^2$ column highlights how each sum of squares corresponds to an estimated variance.\\\nThis view makes the adjusted coefficient of determination clear:\n\n$$\nR^2_{\\mathrm{adj}} \n= 1 - \\frac{\\hat{\\sigma}^2_\\epsilon}{\\hat{\\sigma}^2_y}\n= \\frac{\\hat{\\sigma}^2_{x^\\top\\beta}}{\\hat{\\sigma}^2_y}.\n$$\n\nHence, the adjusted $R^2$ simply expresses the **proportion of total estimated variance** attributable to the fitted model $X\\beta$ rather than the residual noise $\\epsilon$.\n\n### Remarks\n\n#### Fundamental Identities\n\n$$\n\\begin{aligned}\n\\mathrm{SST} &= \\mathrm{SSR} + \\mathrm{SSE}, \\\\\n\\mathrm{MST} &= \\mathrm{MSE} + \\frac{\\mathrm{SSR}_{\\mathrm{adj}}}{n-1}.\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\mathrm{SSR}_{\\mathrm{adj}} = (n-1)MST-(n-p+k)\\mathrm{MSE} = \\mathrm{SST}-\\mathrm{SSE} - k\\,\\mathrm{MSE} = \\mathrm{SSR} - k\\,\\mathrm{MSE}.\n$$\n\n------------------------------------------------------------------------\n\n#### Difference of $\\hat{\\sigma}^2$ and Mean Squares\n\nThe quantity $\\hat{\\sigma}^2$ represents the **estimated variance** associated with each component of the model. MSE and MST are the estimated variances of the $\\epsilon$ and $y$ itself. However, the MSR, although called **Mean Square for Regression (MSR)** is *NOT* an estimate of the variance or sample variance of $x^\\top \\beta$. The name of \"mean\" here is used to indicate a different thing. Its name “Mean Square” reflects that it is also an estimate estimate of noise variance $\\sigma^2$ under $H_0\\!:\\,\\beta = 0$:\n\n$$\nE[\\mathrm{MSR} \\mid H_0] = \\sigma^2,\n\\qquad \nE[\\mathrm{MSR} \\mid H_1] > \\sigma^2.\n$$\n\nHence the F-statistic\n\n$$\nF = \\frac{\\mathrm{MSR}}{\\mathrm{MSE}}\n$$ is approximately equal to 1 subject to the variability as characterized with F-distribution with degree freedoms of $k$ and $n-p$. This test is to test whether any regression coefficients are not equal to 0.\n\n------------------------------------------------------------------------\n\n#### $\\hat \\sigma^2_{x^\\top\\beta}=\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}$\n\n$\\hat \\sigma^2_{x^\\top\\beta}$ is an unbiased estimator of the variance of linear signal when $x$ is a regarded as a random variable. This can be seen from the following equations: $$\nE[\\mathrm{SSR}] = k\\,\\sigma^2 + \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta,\n\\qquad\nE[\\mathrm{MSE}] = \\sigma^2.\n$$ Hence, $$\n\\begin{aligned}\nE[\\mathrm{SSR}_{\\mathrm{adj}}]\n&= E[\\mathrm{SSR}] - k\\,E[\\mathrm{MSE}] \\\\\n&= \\beta^\\top X^\\top (I - J/n)\\,X\\,\\beta \\\\\n&= \\sum_{i=1}^n (\\mu_i - \\bar\\mu)^2,\n\\end{aligned}\n$$\n\nwhere $$\n\\begin{aligned}\n\\mu_i &= x_i^\\top \\beta \\\\\n\\bar\\mu &= \\tfrac{1}{n}\\sum_{i=1}^n \\mu_i\n\\end{aligned}\n$$ For fixed $X$, $\\mathrm{SSR}_{\\text{adj}}/(n-1)$ equals the **sample variance** of the true means $\\{\\mu_i\\}$ over the observed design points. If the rows of $X$ are independently sampled with covariance matrix $\\Sigma_X$ (the random-$X$ model), then\n\n$$\n\\mathbb{E}_X\\!\\left[\\frac{\\mathrm{SSR}_{\\text{adj}}}{n-1}\\right]\n= \\beta^\\top \\Sigma_X \\beta\n= \\mathrm{Var}(x^\\top \\beta),\n$$\n\n#### Connection to Rao-Blackwell Formula\n\nThe decomposition of $\\hat{\\sigma}^2$ is consistent with the **Rao–Blackwell formula** for total variance:\n\n$$\n\\mathrm{Var}(y) = \\mathrm{Var}\\!\\big(E[y \\mid x]\\big) + E\\!\\big(\\mathrm{Var}[y \\mid x]\\big).\n$$\n\nHere,\n\n-   $\\mathrm{Var}\\!\\big(E[y \\mid x]\\big)$ corresponds to the **explained variation** due to the regression component $x^\\top\\beta$, and\\\n-   $E\\!\\big(\\mathrm{Var}[y \\mid x]\\big)$ corresponds to the **residual variation** due to $\\epsilon$.\n\n### A Simulation Study to Understand the Distributions of RSS\n\n**Data Generating Model**\n\nFor $n=30$ and $p_{max}=20$, simulate with either $H_0:\\beta=\\mathbf 0$ or $H_1$ where only $\\beta_1\\neq 0$; $\\epsilon_i\\sim N(0,1)$.\n\n**Sequence of Fitted Models**\n\n| Model Name | \\# of Predictors (k) | \\# of Parameters (p) | R Formula |\n|:----------------|:----------------:|:----------------:|:--------------------|\n| Model 0 | 0 | 1 | `y ~ 1` |\n| Model 1 | 2 | 3 | `y ~ x_1 + x_2` |\n| ... | ... | ... | ... |\n| Final Model | 20 | 21 | `y ~ x_1 + ... + x_20` |\n\n\n\n\n\n\n\n\n\n#### When $H_0$ is true\n\n\n\n\n\n\n\n\n{{< video rss-h0.mp4 >}}\n\n\n\n\n\n\n#### When $H_1$ is true\n\n\n\n\n\n\n{{< video rss-h1.mp4 >}}\n\n\n\n\n\n\n### Example:  Modelling Children Weight with Height and Age\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data: Weight, height and age of children\nwgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\nchild.data <- data.frame(wgt, hgt, age)\n```\n:::\n\n\n\n\n#### Problem 1: Height then Age\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_hgt_age <- lm(wgt ~ hgt + age, data = child.data)\nsummary(fit_hgt_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = wgt ~ hgt + age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nhgt           0.7220     0.2608   2.768   0.0218 *\nage           2.0501     0.9372   2.187   0.0565 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78,\tAdjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_hgt <- lm(wgt ~ hgt, data = child.data)\nsummary(fit_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = wgt ~ hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8736 -3.8973 -0.4402  2.2624 11.8375 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   6.1898    12.8487   0.482  0.64035   \nhgt           1.0722     0.2417   4.436  0.00126 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.471 on 10 degrees of freedom\nMultiple R-squared:  0.663,\tAdjusted R-squared:  0.6293 \nF-statistic: 19.67 on 1 and 10 DF,  p-value: 0.001263\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_hgt, fit_hgt_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: wgt ~ hgt\nModel 2: wgt ~ hgt + age\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     10 299.33                              \n2      9 195.43  1     103.9 4.7849 0.05649 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_hgt_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: wgt\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nhgt        1 588.92  588.92 27.1216 0.0005582 ***\nage        1 103.90  103.90  4.7849 0.0564853 .  \nResiduals  9 195.43   21.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### Problem 2: Age then Height\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_age <- lm(wgt ~ age, data = child.data)\nsummary(fit_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = wgt ~ age, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.000  -3.911   1.143   4.071  10.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  30.5714     8.6137   3.549  0.00528 **\nage           3.6429     0.9551   3.814  0.00341 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.015 on 10 degrees of freedom\nMultiple R-squared:  0.5926,\tAdjusted R-squared:  0.5519 \nF-statistic: 14.55 on 1 and 10 DF,  p-value: 0.003407\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_age_hgt <- lm(wgt ~ age + hgt, data = child.data)\nsummary(fit_age_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = wgt ~ age + hgt, data = child.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8708 -1.7004  0.3454  1.4642 10.2336 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   6.5530    10.9448   0.599   0.5641  \nage           2.0501     0.9372   2.187   0.0565 .\nhgt           0.7220     0.2608   2.768   0.0218 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 9 degrees of freedom\nMultiple R-squared:   0.78,\tAdjusted R-squared:  0.7311 \nF-statistic: 15.95 on 2 and 9 DF,  p-value: 0.001099\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_age, fit_age_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: wgt ~ age\nModel 2: wgt ~ age + hgt\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     10 361.86                              \n2      9 195.43  1    166.43 7.6646 0.02181 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_age_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: wgt\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nage        1 526.39  526.39 24.2419 0.0008205 ***\nhgt        1 166.43  166.43  7.6646 0.0218070 *  \nResiduals  9 195.43   21.71                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Example:  Wire bond strength\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_len_hgt <-  lm(strength ~ length + height, data = bond.data)\nfit_hgt_len <-  lm(strength ~ height+length, data = bond.data)\nanova(fit_len_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq  F value    Pr(>F)    \nlength     1 5885.9  5885.9 1124.293 < 2.2e-16 ***\nheight     1  104.9   104.9   20.041 0.0001883 ***\nResiduals 22  115.2     5.2                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(fit_hgt_len)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: strength\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nheight     1 1483.2  1483.2  283.32 4.731e-14 ***\nlength     1 4507.5  4507.5  861.01 < 2.2e-16 ***\nResiduals 22  115.2     5.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_hgt_len)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = strength ~ height + length, data = bond.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.865 -1.542 -0.362  1.196  5.841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.263791   1.060066   2.136 0.044099 *  \nheight      0.012528   0.002798   4.477 0.000188 ***\nlength      2.744270   0.093524  29.343  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.288 on 22 degrees of freedom\nMultiple R-squared:  0.9811,\tAdjusted R-squared:  0.9794 \nF-statistic: 572.2 on 2 and 22 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_len_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = strength ~ length + height, data = bond.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.865 -1.542 -0.362  1.196  5.841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.263791   1.060066   2.136 0.044099 *  \nlength      2.744270   0.093524  29.343  < 2e-16 ***\nheight      0.012528   0.002798   4.477 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.288 on 22 degrees of freedom\nMultiple R-squared:  0.9811,\tAdjusted R-squared:  0.9794 \nF-statistic: 572.2 on 2 and 22 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n### Relationship between t-test and partial F-test\n\n-   A t-test for a single coefficient is a special case of the partial F-test; the relationship is $F = t^2$ for 1 df in the numerator.\n-   The p-value from t-test (output of *summary(lm())*) is the same as anova test for: $H_0: \\beta_j = 0$ vs $H_1$: all covaraites have non-zero effects.\n\n## Predictions for Mean Response and a Future Observation\n\n### Confidence Interval for Mean Response\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"confidence\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      fit      lwr      upr\n1 27.6631 26.66324 28.66296\n```\n\n\n:::\n:::\n\n\n\n\n### Prediction Interval for a New Observation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, newdata = data.frame(length = 8, height = 275),\n        interval = \"prediction\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      fit      lwr      upr\n1 27.6631 22.81378 32.51241\n```\n\n\n:::\n:::\n\n\n\n\n## Model Diagnostics\n\n### Residual Calculations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals_df <- data.frame(\n  hat_values = hatvalues(fit),\n  ordinary_resid = resid(fit),\n  standardized_resid = resid(fit) / sigma(fit),\n  studentized_internal = rstandard(fit),\n  studentized_external = rstudent(fit)\n)\nresiduals_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   hat_values ordinary_resid standardized_resid studentized_internal\n1  0.15728923     1.57127871         0.68673363           0.74808172\n2  0.11164598    -1.14600783        -0.50086730          -0.53140990\n3  0.14191905    -2.20409488        -0.96330846          -1.03992315\n4  0.10188923    -1.59678413        -0.69788088          -0.73640435\n5  0.04178381    -2.89365294        -1.26468257          -1.29196212\n6  0.07486842     1.11356772         0.48668921           0.50599936\n7  0.11806106     1.92974001         0.84340057           0.89807919\n8  0.15608149     1.19622309         0.52281407           0.56911105\n9  0.12797685    -3.86499936        -1.68921340          -1.80892479\n10 0.04131672    -0.47629200        -0.20816532          -0.21260369\n11 0.09253979    -1.32232830        -0.57792886          -0.60668127\n12 0.05256700    -0.46188206        -0.20186740          -0.20739197\n13 0.08202675     0.49106715         0.21462286           0.22400668\n14 0.11291577    -0.60234282        -0.26325633          -0.27950939\n15 0.07373697     5.84092866         2.55280118           2.65246601\n16 0.08794942    -0.36199456        -0.15821117          -0.16566382\n17 0.25934228     4.33412887         1.89424832           2.20104100\n18 0.29287870    -2.03683074        -0.89020500          -1.05862725\n19 0.09617553    -1.54150602        -0.67372136          -0.70866056\n20 0.14726101     0.03021107         0.01320387           0.01429859\n21 0.12963943    -2.18090138        -0.95317165          -1.02169558\n22 0.13580052     1.55871047         0.68124063           0.73281364\n23 0.18237610     0.32221591         0.14082575           0.15574183\n24 0.10908869     2.14780957         0.93870874           0.99452024\n25 0.07287021     0.15373580         0.06719084           0.06978142\n   studentized_external\n1            0.74035927\n2           -0.52255660\n3           -1.04194550\n4           -0.72850799\n5           -1.31305171\n6            0.49726770\n7            0.89397096\n8            0.56016499\n9           -1.91552083\n10          -0.20792931\n11          -0.59775404\n12          -0.20282206\n13           0.21910643\n14          -0.27356920\n15           3.14216850\n16          -0.16195600\n17           2.43521394\n18          -1.06168251\n19          -0.70040768\n20           0.01396991\n21          -1.02276424\n22           0.72486668\n23           0.15224503\n24           0.99426154\n25           0.06818458\n```\n\n\n:::\n:::\n\n\n\n\n### Residual Plots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- nrow(bond.data)\nr <- rstudent(fit) \ny.hat <- fitted.values(fit)\n\npar(mfrow = c(2, 3))\nqqnorm(r, main = \"Normal Q-Q Plot\"); qqline(r)\nplot(y.hat, r, xlab = \"Fitted values\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(1:n, r, xlab = \"Observation Number\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$length, r, xlab = \"Wire Length\", ylab = \"Studentized Residuals\"); abline(h = 0)\nplot(bond.data$height, r, xlab = \"Die Height\", ylab = \"Studentized Residuals\"); abline(h = 0)\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Influential Observations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfluence_df <- data.frame(dffits = dffits(fit),\n                           cook.D = cooks.distance(fit),\n                           dfbetas(fit))\ninfluence_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         dffits       cook.D  X.Intercept.       length       height\n1   0.319854702 3.481748e-02  0.3179493921 -0.100534181 -0.200085326\n2  -0.185251548 1.183028e-02 -0.1403477437 -0.051464370  0.148315370\n3  -0.423741713 5.962023e-02 -0.2219151046 -0.237135616  0.339340552\n4  -0.245376811 2.050736e-02  0.0787635526  0.022343842 -0.184260891\n5  -0.274191565 2.426179e-02 -0.1572410603 -0.009662357  0.055328303\n6   0.141461363 6.906752e-03  0.1301249135 -0.058073567 -0.049408295\n7   0.327082639 3.598953e-02  0.1479853099 -0.261848970  0.142220906\n8   0.240902557 1.996750e-02  0.2394962591 -0.076575387 -0.149787102\n9  -0.733818383 1.600749e-01 -0.5011686139 -0.283749099  0.605559181\n10 -0.043165927 6.493384e-04 -0.0241138520 -0.001038287  0.007460760\n11 -0.190885522 1.251125e-02 -0.0602003847  0.132053762 -0.102733745\n12 -0.047774650 7.954763e-04  0.0017554145 -0.016926531 -0.008495572\n13  0.065496465 1.494604e-03 -0.0224255162  0.017340091  0.033756253\n14 -0.097602753 3.314830e-03 -0.0483552961  0.077880727 -0.038066705\n15  0.886553487 1.866936e-01  0.8097463528 -0.374290156 -0.292048214\n16 -0.050292599 8.821617e-04 -0.0177647675  0.034746758 -0.025275682\n17  1.441003392 5.654455e-01 -0.8513738015  1.008880052  0.413618783\n18 -0.683268805 1.547244e-01 -0.0218935465  0.521608456 -0.532432956\n19 -0.228476293 1.781295e-02  0.0700729860  0.018004228 -0.167581999\n20  0.005805362 1.176892e-05  0.0005613509  0.004752581 -0.003094588\n21 -0.394724862 5.182743e-02 -0.0084618169 -0.324109965  0.170622396\n22  0.287343813 2.812893e-02 -0.1326208213  0.183002776  0.076391058\n23  0.071903545 1.803448e-03 -0.0412553376  0.040164093  0.030365108\n24  0.347915085 4.036930e-02  0.3084561584  0.016541769 -0.262089402\n25  0.019115733 1.275757e-04  0.0062730782 -0.011614674  0.009459029\n```\n\n\n:::\n:::\n\n\n\n\n### Plotting with the `olsrr` Package\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## install.packages(\"olsrr\") # Run once if needed\nlibrary(olsrr)\n\nols_plot_cooksd_chart(fit)\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nols_plot_dffits(fit)\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-19-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nols_plot_dfbetas(fit)\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-19-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Polynomial Regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(1.81, 1.70, 1.65, 1.55, 1.48, 1.40, 1.30, 1.26, 1.24, 1.21, 1.20, 1.18)\nx <- c(20, 25, 30, 35, 40, 50, 60, 65, 70, 75, 80, 90)\nfit_poly <- lm(y ~ x + I(x^2))\nsummary(fit_poly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0174763 -0.0065087  0.0001297  0.0071482  0.0151887 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.198e+00  2.255e-02   97.48 6.38e-15 ***\nx           -2.252e-02  9.424e-04  -23.90 1.88e-09 ***\nI(x^2)       1.251e-04  8.658e-06   14.45 1.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01219 on 9 degrees of freedom\nMultiple R-squared:  0.9975,\tAdjusted R-squared:  0.9969 \nF-statistic:  1767 on 2 and 9 DF,  p-value: 2.096e-12\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, xlab = \"Lot size, x\", ylab = \"Average cost per unit, y\")\nlines(x, predict(fit_poly, newdata = data.frame(x = x)), type = \"l\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- lm(y ~ x)\nanova(fit1, fit_poly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df      RSS Df Sum of Sq      F    Pr(>F)    \n1     10 0.032340                                  \n2      9 0.001337  1  0.031002 208.67 1.564e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Handling Categorical Variables with Dummy Variables\n\nInvestigate the common observation that males tend to have higher blood pressure than females of similar age.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Note: Update this path to your local file location\nsbpdata <- read.csv(\"sbpdata.csv\")\nsbpdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sex sbp age\n1    0 144  39\n2    0 138  45\n3    0 145  47\n4    0 162  65\n5    0 142  46\n6    0 170  67\n7    0 124  42\n8    0 158  67\n9    0 154  56\n10   0 162  64\n11   0 150  56\n12   0 140  59\n13   0 110  34\n14   0 128  42\n15   0 130  48\n16   0 135  45\n17   0 114  17\n18   0 116  20\n19   0 124  19\n20   0 136  36\n21   0 142  50\n22   0 120  39\n23   0 120  21\n24   0 160  44\n25   0 158  53\n26   0 144  63\n27   0 130  29\n28   0 125  25\n29   0 175  69\n30   1 158  41\n31   1 185  60\n32   1 152  41\n33   1 159  47\n34   1 176  66\n35   1 156  47\n36   1 184  68\n37   1 138  43\n38   1 172  68\n39   1 168  57\n40   1 176  65\n41   1 164  57\n42   1 154  61\n43   1 124  36\n44   1 142  44\n45   1 144  50\n46   1 149  47\n47   1 128  19\n48   1 130  22\n49   1 138  21\n50   1 150  38\n51   1 156  52\n52   1 134  41\n53   1 134  18\n54   1 174  51\n55   1 174  55\n56   1 158  65\n57   1 144  33\n58   1 139  23\n59   1 180  70\n60   1 165  56\n61   1 172  62\n62   1 160  51\n63   1 157  48\n64   1 170  59\n65   1 153  40\n66   1 148  35\n67   1 140  33\n68   1 132  26\n69   1 169  61\n```\n\n\n:::\n:::\n\n\n\n\n### Four Models Involving \"sex\"\n\n#### Coincidence Model (Age Only)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Ensure sex is a factor (labels will appear in the legend)\nsbpdata$sex <- as.factor(sbpdata$sex)\n\n## Fit (you already have this)\nfit.age <- lm(sbp ~ age, data = sbpdata)\n\n## Generate predictions over the observed age range\nnew_age <- seq(min(sbpdata$age, na.rm = TRUE),\n               max(sbpdata$age, na.rm = TRUE),\n               length.out = 200)\npred <- predict(fit.age, newdata = data.frame(age = new_age))\n\n## Simple palette for the sex levels (works for 1–3 levels; expand if needed)\nlev  <- levels(sbpdata$sex)\ncols <- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter plot with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Add predicted line\nlines(new_age, pred, lwd = 2)\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, bty = \"n\", title = \"Sex\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-24-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(model.matrix(fit.age)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   X.Intercept. age\n1             1  39\n2             1  45\n3             1  47\n4             1  65\n5             1  46\n6             1  67\n7             1  42\n8             1  67\n9             1  56\n10            1  64\n11            1  56\n12            1  59\n13            1  34\n14            1  42\n15            1  48\n16            1  45\n17            1  17\n18            1  20\n19            1  19\n20            1  36\n21            1  50\n22            1  39\n23            1  21\n24            1  44\n25            1  53\n26            1  63\n27            1  29\n28            1  25\n29            1  69\n30            1  41\n31            1  60\n32            1  41\n33            1  47\n34            1  66\n35            1  47\n36            1  68\n37            1  43\n38            1  68\n39            1  57\n40            1  65\n41            1  57\n42            1  61\n43            1  36\n44            1  44\n45            1  50\n46            1  47\n47            1  19\n48            1  22\n49            1  21\n50            1  38\n51            1  52\n52            1  41\n53            1  18\n54            1  51\n55            1  55\n56            1  65\n57            1  33\n58            1  23\n59            1  70\n60            1  56\n61            1  62\n62            1  51\n63            1  48\n64            1  59\n65            1  40\n66            1  35\n67            1  33\n68            1  26\n69            1  61\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit.age))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: sbp\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nage        1 14951.3 14951.3  121.27 < 2.2e-16 ***\nResiduals 67  8260.5   123.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### Additive Effect Model (Age + Sex)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Parallelism: H0: beta3=0 (Sex has additive effect)\nfit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)\n\n## Ensure sex is a factor for labeling/colors\nsbpdata$sex <- factor(sbpdata$sex)\n\n## Fit (additive: parallelism)\nfit.agePLUSsex <- lm(sbp ~ age + sex, data = sbpdata)\n\n## X-range and palette\nages <- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  <- levels(sbpdata$sex)\ncols <- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter with colored points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Parallel fitted lines: one per sex (same slope, different intercepts)\nfor (sx in lev) {\n  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat <- predict(fit.agePLUSsex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-26-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(model.matrix(fit.agePLUSsex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   X.Intercept. age sex1\n1             1  39    0\n2             1  45    0\n3             1  47    0\n4             1  65    0\n5             1  46    0\n6             1  67    0\n7             1  42    0\n8             1  67    0\n9             1  56    0\n10            1  64    0\n11            1  56    0\n12            1  59    0\n13            1  34    0\n14            1  42    0\n15            1  48    0\n16            1  45    0\n17            1  17    0\n18            1  20    0\n19            1  19    0\n20            1  36    0\n21            1  50    0\n22            1  39    0\n23            1  21    0\n24            1  44    0\n25            1  53    0\n26            1  63    0\n27            1  29    0\n28            1  25    0\n29            1  69    0\n30            1  41    1\n31            1  60    1\n32            1  41    1\n33            1  47    1\n34            1  66    1\n35            1  47    1\n36            1  68    1\n37            1  43    1\n38            1  68    1\n39            1  57    1\n40            1  65    1\n41            1  57    1\n42            1  61    1\n43            1  36    1\n44            1  44    1\n45            1  50    1\n46            1  47    1\n47            1  19    1\n48            1  22    1\n49            1  21    1\n50            1  38    1\n51            1  52    1\n52            1  41    1\n53            1  18    1\n54            1  51    1\n55            1  55    1\n56            1  65    1\n57            1  33    1\n58            1  23    1\n59            1  70    1\n60            1  56    1\n61            1  62    1\n62            1  51    1\n63            1  48    1\n64            1  59    1\n65            1  40    1\n66            1  35    1\n67            1  33    1\n68            1  26    1\n69            1  61    1\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit.age, fit.agePLUSsex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     67 8260.5                                  \n2     66 5202.0  1    3058.5 38.805 3.701e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### Varying Intercept and Varying Slope Model (Age + Sex + Age:Sex)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex <- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.age.TIMES.sex <- lm(sbp ~ age + sex + age:sex, data = sbpdata)\n\n## Age grid and palette\nages <- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  <- levels(sbpdata$sex)\ncols <- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat <- predict(fit.age.TIMES.sex, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**Model Matrix and ANOVA**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(model.matrix(fit.age.TIMES.sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   X.Intercept. age sex1 age.sex1\n1             1  39    0        0\n2             1  45    0        0\n3             1  47    0        0\n4             1  65    0        0\n5             1  46    0        0\n6             1  67    0        0\n7             1  42    0        0\n8             1  67    0        0\n9             1  56    0        0\n10            1  64    0        0\n11            1  56    0        0\n12            1  59    0        0\n13            1  34    0        0\n14            1  42    0        0\n15            1  48    0        0\n16            1  45    0        0\n17            1  17    0        0\n18            1  20    0        0\n19            1  19    0        0\n20            1  36    0        0\n21            1  50    0        0\n22            1  39    0        0\n23            1  21    0        0\n24            1  44    0        0\n25            1  53    0        0\n26            1  63    0        0\n27            1  29    0        0\n28            1  25    0        0\n29            1  69    0        0\n30            1  41    1       41\n31            1  60    1       60\n32            1  41    1       41\n33            1  47    1       47\n34            1  66    1       66\n35            1  47    1       47\n36            1  68    1       68\n37            1  43    1       43\n38            1  68    1       68\n39            1  57    1       57\n40            1  65    1       65\n41            1  57    1       57\n42            1  61    1       61\n43            1  36    1       36\n44            1  44    1       44\n45            1  50    1       50\n46            1  47    1       47\n47            1  19    1       19\n48            1  22    1       22\n49            1  21    1       21\n50            1  38    1       38\n51            1  52    1       52\n52            1  41    1       41\n53            1  18    1       18\n54            1  51    1       51\n55            1  55    1       55\n56            1  65    1       65\n57            1  33    1       33\n58            1  23    1       23\n59            1  70    1       70\n60            1  56    1       56\n61            1  62    1       62\n62            1  51    1       51\n63            1  48    1       48\n64            1  59    1       59\n65            1  40    1       40\n66            1  35    1       35\n67            1  33    1       33\n68            1  26    1       26\n69            1  61    1       61\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit.age.TIMES.sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 97.07708    5.17046  18.775  < 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,\tAdjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit.age,fit.agePLUSsex,fit.age.TIMES.sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age + sex\nModel 3: sbp ~ age + sex + age:sex\n  Res.Df    RSS Df Sum of Sq       F    Pr(>F)    \n1     67 8260.5                                   \n2     66 5202.0  1   3058.52 38.2210 4.692e-08 ***\n3     65 5201.4  1      0.55  0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### Varying Slope, Equal Intercept Model (Age + Age:Sex)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Make sure sex is a factor (for colors/legend)\nsbpdata$sex <- factor(sbpdata$sex)\n\n## Fit (interaction: different slopes by sex)\nfit.equal.intercept <- lm(sbp ~ age + age:sex, data = sbpdata)\n\n\n## Age grid and palette\nages <- seq(min(sbpdata$age, na.rm = TRUE),\n            max(sbpdata$age, na.rm = TRUE),\n            length.out = 200)\nlev  <- levels(sbpdata$sex)\ncols <- setNames(c(\"steelblue3\", \"tomato3\", \"darkorchid3\")[seq_along(lev)], lev)\n\n## Scatter: color points by sex\nplot(sbp ~ age, data = sbpdata,\n     col = cols[sbpdata$sex], pch = 16,\n     xlab = \"Age\", ylab = \"Systolic BP\")\n\n## Fitted lines: one per sex (different slopes allowed)\nfor (sx in lev) {\n  nd <- data.frame(age = ages, sex = factor(sx, levels = lev))\n  yhat <- predict(fit.equal.intercept, newdata = nd)\n  lines(ages, yhat, col = cols[sx], lwd = 2)\n}\n\n## Legend\nlegend(\"topleft\", legend = lev, col = cols[lev], pch = 16, lwd = 2, bty = \"n\", title = \"Sex\")\n```\n\n::: {.cell-output-display}\n![](mlr_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Orders of Terms Matters in ANOVA and Warnings in Interpreting t-test Tables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.int <- lm(sbp ~ 1, data = sbpdata)\nfit.sex <- lm(sbp ~ sex, data = sbpdata)\n\nprint(anova(fit.int,fit.age,fit.agePLUSsex, fit.age.TIMES.sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 < 2.2e-16 ***\n3     66  5202.0  1    3058.5  38.2210 4.692e-08 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit.int,fit.age,fit.equal.intercept, fit.age.TIMES.sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ age\nModel 3: sbp ~ age + age:sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n1     68 23211.8                                    \n2     67  8260.5  1   14951.3 186.8390 < 2.2e-16 ***\n3     66  5474.9  1    2785.6  34.8107 1.437e-07 ***\n4     65  5201.4  1     273.4   3.4171   0.06907 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(fit.int,fit.sex,fit.agePLUSsex, fit.age.TIMES.sex))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: sbp ~ 1\nModel 2: sbp ~ sex\nModel 3: sbp ~ age + sex\nModel 4: sbp ~ age + sex + age:sex\n  Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n1     68 23211.8                                    \n2     67 19282.5  1    3929.2  49.1017 1.684e-09 ***\n3     66  5202.0  1   14080.6 175.9583 < 2.2e-16 ***\n4     65  5201.4  1       0.5   0.0069    0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit.age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sbp ~ age, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.782  -7.632   1.968   8.201  22.651 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 103.34905    4.33190   23.86   <2e-16 ***\nage           0.98333    0.08929   11.01   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 67 degrees of freedom\nMultiple R-squared:  0.6441,\tAdjusted R-squared:  0.6388 \nF-statistic: 121.3 on 1 and 67 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit.equal.intercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sbp ~ age + age:sex, data = sbpdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.6338  -4.3067   0.9922   4.9819  20.2753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 104.12501    3.55578  29.283  < 2e-16 ***\nage           0.80908    0.07918  10.219 3.14e-15 ***\nage:sex1      0.26705    0.04608   5.795 2.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.108 on 66 degrees of freedom\nMultiple R-squared:  0.7641,\tAdjusted R-squared:  0.757 \nF-statistic: 106.9 on 2 and 66 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit.agePLUSsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sbp ~ age + sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.705  -3.299   1.248   4.325  21.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 96.77353    3.62085  26.727  < 2e-16 ***\nage          0.95606    0.07153  13.366  < 2e-16 ***\nsex1        13.51345    2.16932   6.229  3.7e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.878 on 66 degrees of freedom\nMultiple R-squared:  0.7759,\tAdjusted R-squared:  0.7691 \nF-statistic: 114.2 on 2 and 66 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit.age.TIMES.sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sbp ~ age + sex + age:sex, data = sbpdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.647  -3.410   1.254   4.314  21.153 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 97.07708    5.17046  18.775  < 2e-16 ***\nage          0.94932    0.10864   8.738 1.43e-12 ***\nsex1        12.96144    7.01172   1.849   0.0691 .  \nage:sex1     0.01203    0.14519   0.083   0.9342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.946 on 65 degrees of freedom\nMultiple R-squared:  0.7759,\tAdjusted R-squared:  0.7656 \nF-statistic: 75.02 on 3 and 65 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Model Building\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(olsrr)\n## Note: Update this path to your local file location\nwine <- read.csv(\"wine.csv\")\n\nmodel.wine <- lm(quality ~ ., data = wine)\n```\n:::\n\n\n\n\n### All Possible Regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_step_best_subset(model.wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Best Subsets Regression             \n-------------------------------------------------\nModel Index    Predictors\n-------------------------------------------------\n     1         flavor                             \n     2         flavor oakiness                    \n     3         aroma flavor oakiness              \n     4         clarity aroma flavor oakiness      \n     5         clarity aroma body flavor oakiness \n-------------------------------------------------\n\n                                                  Subsets Regression Summary                                                   \n-------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                         \nModel    R-Square    R-Square    R-Square     C(p)       AIC        SBIC        SBC        MSEP       FPE       HSP       APC  \n-------------------------------------------------------------------------------------------------------------------------------\n  1        0.6242      0.6137      0.5868    9.0436    130.0214    21.6859    134.9341    61.4102    1.7010    0.0462    0.4176 \n  2        0.6611      0.6417      0.6058    6.8132    128.0901    20.1242    134.6404    57.0033    1.6171    0.0441    0.3970 \n  3        0.7038      0.6776      0.6379    3.9278    124.9781    18.0702    133.1661    51.3383    1.4906    0.0409    0.3659 \n  4        0.7147      0.6801      0.6102    4.6747    125.5480    19.2854    135.3736    50.9872    1.5143    0.0418    0.3717 \n  5        0.7206      0.6769       0.587    6.0000    126.7552    21.0956    138.2183    51.5452    1.5649    0.0436    0.3842 \n-------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n```\n\n\n:::\n:::\n\n\n\n\n### Automated Stepwise Procedures\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Backward Elimination (alpha_out = 0.1)\nols_step_backward_p(model.wine, p_val = 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Full Model    126.755    138.218    21.096    0.72060    0.67694 \n 1      body          125.548    135.374    19.285    0.71471    0.68013 \n 2      clarity       124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n----------------------------------------------------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\n## Forward Selection (alpha_in = 0.1)\nols_step_forward_p(model.wine, p_val = 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor        130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness      128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma         124.978    133.166    18.070    0.70377    0.67763 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\n## Stepwise Regression (alpha_in = 0.1, alpha_out = 0.1)\nols_step_both_p(model.wine, p_enter = 0.1, p_remove = 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable          AIC        SBC       SBIC       R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model      165.209    168.484    55.141    0.00000    0.00000 \n 1      flavor (+)      130.021    134.934    21.686    0.62417    0.61373 \n 2      oakiness (+)    128.090    134.640    20.124    0.66111    0.64175 \n 3      aroma (+)       124.978    133.166    18.070    0.70377    0.67763 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.839       RMSE                 1.098 \nR-Squared               0.704       MSE                  1.207 \nAdj. R-Squared          0.678       Coef. Var            9.338 \nPred R-Squared          0.638       AIC                124.978 \nMAE                     0.868       SBC                133.166 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n               Sum of                                              \n              Squares        DF    Mean Square      F         Sig. \n-------------------------------------------------------------------\nRegression    108.935         3         36.312    26.925    0.0000 \nResidual       45.853        34          1.349                     \nTotal         154.788        37                                    \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.467         1.333                  4.852    0.000     3.759     9.176 \n     flavor     1.200         0.275        0.603     4.364    0.000     0.641     1.758 \n   oakiness    -0.602         0.264       -0.217    -2.278    0.029    -1.140    -0.065 \n      aroma     0.580         0.262        0.307     2.213    0.034     0.047     1.113 \n----------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n## Multicollinearity\n\n### A Simple Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- c(19, 20, 37, 39, 36, 38)\nx1 <- c(4, 4, 7, 7, 7.1, 7.1)\nx2 <- c(16, 16, 49, 49, 50.4, 50.4)\ncor(data.frame(x1, x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x1        x2\nx1 1.0000000 0.9999713\nx2 0.9999713 1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_multi <- lm(y ~ x1 + x2)\nsummary(fit_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n   1    2    3    4    5    6 \n-0.5  0.5 -1.0  1.0 -1.0  1.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -156.056    117.158  -1.332    0.275\nx1            65.444     45.890   1.426    0.249\nx2            -5.389      4.152  -1.298    0.285\n\nResidual standard error: 1.225 on 3 degrees of freedom\nMultiple R-squared:  0.9897,\tAdjusted R-squared:  0.9829 \nF-statistic: 144.3 on 2 and 3 DF,  p-value: 0.001043\n```\n\n\n:::\n\n```{.r .cell-code}\nfit1_multi <- lm(y ~ x1)\nsummary(fit1_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.5260  0.4740 -0.1925  1.8075 -1.7814  0.2186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.0293     2.3332  -1.727    0.159    \nx1            5.8888     0.3762  15.654 9.73e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.325 on 4 degrees of freedom\nMultiple R-squared:  0.9839,\tAdjusted R-squared:  0.9799 \nF-statistic: 245.1 on 1 and 4 DF,  p-value: 9.725e-05\n```\n\n\n:::\n\n```{.r .cell-code}\nols_vif_tol(fit_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Variables    Tolerance      VIF\n1        x1 5.738191e-05 17427.09\n2        x2 5.738191e-05 17427.09\n```\n\n\n:::\n:::\n\n\n\n\n### VIFs in the Wine Quality Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine.x <- wine[, -ncol(wine)] # Assuming quality is the last column\ncor(wine.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             clarity     aroma       body      flavor  oakiness\nclarity   1.00000000 0.0619021 -0.3083783 -0.08515993 0.1832147\naroma     0.06190210 1.0000000  0.5489102  0.73656121 0.2016444\nbody     -0.30837826 0.5489102  1.0000000  0.64665917 0.1521059\nflavor   -0.08515993 0.7365612  0.6466592  1.00000000 0.1797605\noakiness  0.18321471 0.2016444  0.1521059  0.17976051 1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\n## VIF using olsrr (data frame output)\nols_vif_tol(model.wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Variables Tolerance      VIF\n1   clarity 0.7896462 1.266390\n2     aroma 0.4199665 2.381143\n3      body 0.4862649 2.056492\n4    flavor 0.3728175 2.682277\n5  oakiness 0.9118005 1.096731\n```\n\n\n:::\n:::\n\n\n\n\n### VIFs in the Children Height Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data: Weight, height and age of children\nwgt <- c(64, 71, 53, 67, 55, 58, 77, 57, 56, 51, 76, 68)\nhgt <- c(57, 59, 49, 62, 51, 50, 55, 48, 42, 42, 61, 57)\nage <- c(8, 10, 6, 11, 8, 7, 10, 9, 10, 6, 12, 9)\n\nfit_age_hgt <- lm(wgt ~ hgt + age, data = child.data)\nols_vif_tol(fit_age_hgt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Variables Tolerance      VIF\n1       hgt 0.6232021 1.604616\n2       age 0.6232021 1.604616\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}