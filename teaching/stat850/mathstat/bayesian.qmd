# Bayesian Methods

## Fundamental Elements of Bayesian Inference

The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes' Theorem (or Law).

::: {#thm-bayes}
### Posterior Distribution

Suppose we have a parameter $\theta$ with a prior distribution denoted by $\pi(\theta)$. If we observe data $x$ drawn from a distribution with probability density function (pdf) $f(x; \theta)$, then the **posterior density** of $\theta$ given the data $x$ is defined as:

$$
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{\int_{\Theta} \pi(\theta) f(x;\theta) d\theta}
$$

In this equation:

* $\pi(\theta)$ is the **prior**.
* $f(x;\theta)$ is the **likelihood**.
* The denominator is the marginal distribution of $x$, often represented as a normalizing constant $c(x)$ which is free of $\theta$.

Thus, we can state the proportional relationship:

$$
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
$$
:::

::: {#exm-binomial-beta}
### Binomial-Beta Conjugacy

Consider an experiment where $x|\theta \sim \text{Bin}(n, \theta)$. The likelihood function is:

$$
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
$$

Suppose we choose a Beta distribution as the prior for $\theta$, such that $\theta \sim \text{Beta}(a, b)$. The prior density is:

$$
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
$$

where $B(a,b)$ is the Beta function defined as $\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta$.

To find the posterior, we multiply the prior and the likelihood:

$$
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
$$

Combining terms with the same base:

$$
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
$$

We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:

$$
\theta|x \sim \text{Beta}(a+x, b+n-x)
$$

**Properties of the Posterior:**

* The posterior mean is:
    $$E(\theta|x) = \frac{a+x}{a+b+n}$$
    As $n \to \infty$, this approximates the maximum likelihood estimate $\frac{x}{n}$.

* The posterior variance is:
    $$\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}$$
    For large $n$, this approximates $\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}$.
:::

::: {#exm-normal-normal}
### Normal-Normal Conjugacy (Known Variance)

Let $X_1, X_2, \dots, X_n$ be independent and identically distributed (i.i.d.) variables such that $X_i \sim N(\mu, \sigma^2)$, where $\sigma^2$ is known.

We assign a Normal prior to the mean $\mu$: $\mu \sim N(\mu_0, \sigma_0^2)$.

To find the posterior $\pi(\mu|x_1, \dots, x_n)$, let $x = (x_1, \dots, x_n)$. The posterior is proportional to:

$$
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
$$

$$
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
$$

Expanding the squares and collecting terms involving $\mu$ (completing the square), we find that the posterior is also a Normal distribution $\mu|x \sim N(\mu_1, \sigma_1^2)$.

**Posterior Precision:**

It is often more convenient to work with **precision** (the inverse of variance). Let:

* $\tau_0 = 1/\sigma_0^2$ (Prior precision)
* $\tau = 1/\sigma^2$ (Data precision)
* $\tau_1 = 1/\sigma_1^2$ (Posterior precision)

The relationship is additive:

$$
\tau_1 = \tau_0 + n\tau
$$

$$
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
$$

The posterior mean $\mu_1$ is a weighted average of the prior mean and the sample mean:

$$
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
$$

So, the posterior distribution is:

$$
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
$$
:::


::: {#exm-discrete-posterior}
### Discrete Posterior Calculation

Consider the following table where we calculate the posterior probabilities for a discrete parameter space.

Let the parameter $\theta$ take values $\{1, 2, 3\}$ with prior probabilities $\pi(\theta)$. Let the data $x$ take values $\{0, 1, 2, \dots\}$.

Given:

* Prior $\pi(\theta)$: $\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3$.
* Likelihood $\pi(x|\theta)$:
    * If $\theta=1$, $x \sim \text{Uniform on } \{0, 1\}$ (Prob = 1/2).
    * If $\theta=2$, $x \sim \text{Uniform on } \{0, 1, 2\}$ (Prob = 1/3).
    * If $\theta=3$, $x \sim \text{Uniform on } \{0, 1, 2, 3\}$ (Prob = 1/4).

Suppose we observe $x=2$. We calculate the posterior $\pi(\theta|x=2)$:

$$
\pi(\theta|x) = \frac{\pi(\theta)\pi(x|\theta)}{\sum \pi(\theta)\pi(x|\theta)}
$$

* For $\theta=1$: $\pi(x=2|1) = 0$. Product = 0.
* For $\theta=2$: $\pi(x=2|2) = 1/3$. Product = $1/3 \times 1/3 = 1/9$.
* For $\theta=3$: $\pi(x=2|3) = 1/4$. Product = $1/3 \times 1/4 = 1/12$.

The marginal sum is $0 + 1/9 + 1/12 = 7/36$.
The posterior probabilities are:

* $\pi(1|x=2) = 0$
* $\pi(2|x=2) = (1/9) / (7/36) = 4/7$
* $\pi(3|x=2) = (1/12) / (7/36) = 3/7$
:::

::: {#exm-Normal-with-Unknown-Mean-and-Variance}
### Normal with Unknown Mean and Variance

Consider $X_1, \dots, X_n \sim N(\mu, 1/\tau)$, where both $\mu$ and the precision $\tau$ are unknown.

We use a **Normal-Gamma** conjugate prior:

1.  $\tau \sim \text{Gamma}(\alpha, \beta)$
    $$\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}$$
2.  $\mu|\tau \sim N(\nu, 1/(k\tau))$
    $$\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}$$

The joint prior is:

$$
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
$$

Multiplying by the likelihood leads to a posterior of the same form (Conjugate), with updated parameters:

* $\alpha' = \alpha + n/2$
* $k' = k + n$
* $\nu' = \frac{k\nu + n\bar{x}}{k+n}$
* $\beta' = \beta + \frac{1}{2} \frac{nk}{n+k}(\bar{x}-\nu)^2 + \frac{1}{2}\sum (x_i - \bar{x})^2$
:::

## Decision Theory and Bayes Rules

The general form of Bayes rule is derived by minimizing risk.

::: {#def-risk}
### Risk Function and Bayes Risk

* **Risk Function:** $R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx$
* **Bayes Risk:** The expected risk with respect to the prior.
    $$r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta$$
:::

Minimizing the Bayes Risk is equivalent to minimizing the expected loss for each $x$. The quantity to minimize is the **Posterior Expected Loss**:

$$
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
$$

### Common Loss Functions and Estimators

::: {#ex-squared-loss}
### Squared Error Loss (Point Estimate)
    
$$L(\theta, a) = (\theta - a)^2$$
    
To find the optimal estimator $d(x)$, we minimize $E_{\theta|x}[(\theta - d(x))^2]$. Taking the derivative with respect to $d$ and setting to 0:
    
$$-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)$$
    
**Result:** The Bayes rule is the **posterior mean**.
:::

::: {#ex-absolute-loss}
### Absolute Error Loss
    
$$L(\theta, d) = |\theta - d|$$
    
Minimizing $E_{\theta|x}[|\theta - d|]$ requires solving:
    
$$\int_{-\infty}^{d} \pi(\theta|x) d\theta = \int_{d}^{\infty} \pi(\theta|x) d\theta = \frac{1}{2}$$
    
**Result:** The Bayes rule is the **posterior median**.
:::

::: {#ex-hypothesis-testing}
### Hypothesis Testing (0-1 Loss)
    
Testing $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$.
    
$$L(\theta, a) = \begin{cases} 1 & \text{if error} \\ 0 & \text{if correct} \end{cases}$$
    
The Bayes rule selects the hypothesis with the higher posterior probability.
    
$$d(x) = 1 \iff P(\theta \in \Theta_1 | x) \ge P(\theta \in \Theta_0 | x)$$
:::

::: {#def-hpd}
### Highest Posterior Density (HPD) Interval
    
In interval estimation, we prescribe a set $A = (d-\delta, d+\delta)$ and minimize the loss associated with $\theta$ falling outside this interval.
    
The Bayes rule $d(x)$ is the center of the interval with the highest probability coverage. This leads to the **Highest Posterior Density (HPD)** interval.
    
In practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the **Equal-Tailed Interval**, where we cut off $\alpha/2$ probability from each tail.
:::

## Minimax Estimation

A decision rule $d(x)$ is **minimax** if it minimizes the maximum possible risk: $\sup_\theta R(\theta, d)$.

::: {#thm-minimax-constant}
### Constant Risk Theorem
If a Bayes rule $d^\pi$ has constant risk (i.e., $R(\theta, d^\pi) = c$ for all $\theta$), then $d^\pi$ is a minimax estimator.
:::

::: {#ex-binomial-minimax}
### Binomial Minimax Estimator

Let $X \sim \text{Bin}(n, \theta)$ and $\theta \sim \text{Beta}(a, b)$.
The squared error loss is $L(\theta, d) = (\theta - d)^2$.
The Bayes estimator is the posterior mean:
$$d(x) = \frac{a+x}{a+b+n}$$

We calculate the risk $R(\theta, d)$:

$$
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
$$

Let $c = a+b+n$.
$$R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]$$

Using the bias-variance decomposition and knowing $E(x) = n\theta$ and $E(x^2) = (n\theta)^2 + n\theta(1-\theta)$, we expand the risk function. To make the risk constant (independent of $\theta$), we set the coefficients of $\theta$ and $\theta^2$ to zero.

Solving the resulting system of equations yields:
$$a = b = \frac{\sqrt{n}}{2}$$

Thus, the minimax estimator is:
$$d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}$$

This differs from the standard MLE $\hat{p} = x/n$ and the uniform prior Bayes estimator ($a=b=1$).
:::

## Stein Estimation and Shrinkage

Consider estimating a multivariate mean vector $\mu = (\mu_1, \dots, \mu_p)$ given independent observations $X_i \sim N(\mu_i, 1)$ for $i=1, \dots, p$.

The standard estimator is the MLE: $d^0(X) = X$.
The loss function is the sum of squared errors: $L(\mu, d) = ||\mu - d||^2 = \sum (\mu_i - d_i)^2$.

::: {#thm-stein-inadmissibility}
### Stein's Result

When $p \ge 3$, the estimator $d^0(X)$ is **inadmissible**. There exists an estimator that strictly dominates it (has lower risk everywhere).
:::

Consider the class of shrinkage estimators:
$$d^a(X) = \left( 1 - \frac{a}{||X||^2} \right) X$$
where $X = (X_1, \dots, X_p)^T$.

When $a > 0$, this estimator "shrinks" the data vector toward the origin $(0, \dots, 0)$.

::: {#lem-stein}
### Stein's Lemma

If $X \sim N(\mu, 1)$, then for a differentiable function $h$:
$$E[(X-\mu)h(X)] = E[h'(X)]$$
:::

::: {.proof}
Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator $d^a$.

$$R(\mu, d^a) = E || \mu - d^a(X) ||^2$$

After expanding and applying Stein's Lemma, the risk becomes:
$$R(\mu, d^a) = p - [2a(p-2) - a^2] E \left( \frac{1}{||X||^2} \right)$$

For $d^a$ to possess lower risk than $d^0$ (where risk = $p$), we need the term in the brackets to be positive:
$$2a(p-2) - a^2 > 0 \implies 0 < a < 2(p-2)$$

The optimal choice (minimizing risk) is $a = p-2$.
This yields the **James-Stein Estimator**:
$$\delta^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$$
:::

## Empirical Bayes

The James-Stein estimator can be motivated via an Empirical Bayes approach.

**Model:**

1.  $X_i | \mu_i \sim N(\mu_i, 1)$
2.  Prior: $\mu_i \sim N(0, \tau^2)$

The posterior mean for $\mu_i$ (if $\tau^2$ were known) is:
$$E(\mu_i|x_i) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i$$

The marginal distribution of $X_i$ is $N(0, 1+\tau^2)$.
Consequently, $S = \sum X_i^2 \sim (1+\tau^2) \chi^2_p$.

We can estimate the unknown shrinkage factor $B = \frac{1}{1+\tau^2}$ using the data.
Since $E[ \frac{p-2}{S} ] = \frac{1}{1+\tau^2}$, we replace the theoretical shrinkage factor with its unbiased estimate:
$$\hat{B} = \frac{p-2}{||X||^2}$$

This recovers the James-Stein rule:
$$\delta^{EB}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$$

::: {#ex-baseball}
### Baseball Example (Efron & Morris)

We illustrate Stein estimation using baseball batting averages.
Let $y_i$ be the number of hits for player $i$ in their first $n=45$ at-bats.
Let $\hat{p}_i = y_i/n$ be the observed average.

To apply the Normal model, we use a variance-stabilizing transformation:
$$X_i = \sqrt{n} \arcsin(2\hat{p}_i - 1)$$
Under this transformation, $X_i \approx N(\mu_i, 1)$.

Using the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value $\mu_0$).
Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages $\hat{p}_i$.
:::

## Predictive Distributions

A key feature of Bayesian analysis is the predictive distribution for a future observation $x^*$.

$$f(x^*|x) = \int f(x^*|\theta) \pi(\theta|x) d\theta$$

::: {#ex-predictive-normal}
### Normal-Normal Predictive Distribution

If $x_1, \dots, x_n \sim N(\mu, \sigma^2)$ (with $\sigma^2$ known) and $\mu \sim N(\mu_0, \sigma_0^2)$, the predictive distribution for a new observation $x^*$ is:

$$x^*|x \sim N(\mu_1, \sigma^2 + \sigma_1^2)$$

where $\mu_1$ and $\sigma_1^2$ are the posterior mean and variance of $\mu$. The predictive variance includes both the inherent sampling uncertainty ($\sigma^2$) and the uncertainty about the parameter ($\sigma_1^2$).
:::

## Hierarchical Modeling and MCMC

When analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).

**Hierarchical Structure:**

1.  **Data:** $X_i | \mu_i \sim f(x_i|\mu_i)$
2.  **Parameters:** $\mu_i | \theta \sim \pi(\mu_i|\theta)$
3.  **Hyperparameters:** $\theta \sim \pi(\theta)$

**Gibbs Sampling:**
To estimate the posterior $f(\mu, \theta | x)$, we sample iteratively from the **full conditional distributions**:

1.  Sample $\mu_i$ from $f(\mu_i | x, \theta)$.
2.  Sample $\theta$ from $f(\theta | \mu, x)$.

::: {#ex-hierarchical-baseball}
### Baseball Example with Hierarchical Model

* $Y_i \sim \text{Bin}(n_i, p_i)$
* Logit transform: $\mu_i = \text{logit}(p_i)$
* $\mu_i \sim N(\theta, \tau^2)$
* Priors on $\theta$ and $\tau^2$.

Since the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use **Metropolis-Hastings** steps within the Gibbs sampler.

**Algorithm:**

1.  Initialize parameters $\mu^{(0)}, \theta^{(0)}, \tau^{(0)}$.
2.  Propose new values based on a candidate distribution.
3.  Accept or reject based on the acceptance probability ratio (Likelihood $\times$ Prior ratio).
4.  Repeat until convergence.

The marginal posterior density for a specific parameter (e.g., $f(\mu_j|x)$) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.
:::
