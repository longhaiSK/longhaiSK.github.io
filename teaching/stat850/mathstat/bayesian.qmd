# Bayesian Methods

## Fundamental Elements of Bayesian Inference

The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes' Theorem (or Law).

::: {#thm-bayes}
### Posterior Distribution

Suppose we have a parameter $\theta$ with a prior distribution denoted by $\pi(\theta)$. If we observe data $x$ drawn from a distribution with probability density function (pdf) $f(x; \theta)$, then the **posterior density** of $\theta$ given the data $x$ is defined as:

$$
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{\int_{\Theta} \pi(\theta) f(x;\theta) d\theta}
$$

In this equation:

* $\pi(\theta)$ is the **prior**.
* $f(x;\theta)$ is the **likelihood**.
* The denominator is the marginal distribution of $x$, often represented as a normalizing constant $c(x)$ which is free of $\theta$.

Thus, we can state the proportional relationship:

$$
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
$$
:::



::: {#exm-binomial-beta}
### Binomial-beta Conjugacy

Consider an experiment where $x|\theta \sim \text{Bin}(n, \theta)$. The likelihood function is:

$$
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
$$

Suppose we choose a Beta distribution as the prior for $\theta$, such that $\theta \sim \text{Beta}(a, b)$. The prior density is:

$$
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
$$

where $B(a,b)$ is the Beta function defined as $\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta$.

To find the posterior, we multiply the prior and the likelihood:

$$
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
$$

Combining terms with the same base:

$$
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
$$

We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:

$$
\theta|x \sim \text{Beta}(a+x, b+n-x)
$$

**Properties of the Posterior:**

* The posterior mean is:
    $$E(\theta|x) = \frac{a+x}{a+b+n}$$
    As $n \to \infty$, this approximates the maximum likelihood estimate $\frac{x}{n}$.

* The posterior variance is:
    $$\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}$$
    For large $n$, this approximates $\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}$.

**Numerical Illustration:**

Suppose we are estimating a probability $\theta$.

* **Prior:** $\theta \sim \text{Beta}(2, 2)$ (Mean = 0.5).
* **Data:** 10 trials, 8 successes ($n=10, x=8$).
* **Posterior:** $\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)$ (Mean $\approx$ 0.71).

The plot below shows the prior (dashed) and posterior (solid) densities.

```{r}
#| label: fig-beta-conjugacy
#| fig-cap: "Prior vs Posterior for Beta-Binomial Example"
#| echo: true

theta <- seq(0, 1, length.out = 200)

# Prior: Beta(2, 2)
prior <- dbeta(theta, shape1 = 2, shape2 = 2)

# Posterior: Beta(10, 4)
posterior <- dbeta(theta, shape1 = 10, shape2 = 4)

plot(theta, posterior, type = 'l', lwd = 2, col = "blue",
     xlab = expression(theta), ylab = "Density",
     main = "Beta Prior vs Posterior", ylim = c(0, max(c(prior, posterior))))
lines(theta, prior, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior Beta(2,2)", "Posterior Beta(10,4)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-normal-normal}
### Normal-normal Conjugacy (known Variance)

Let $X_1, X_2, \dots, X_n$ be independent and identically distributed (i.i.d.) variables such that $X_i \sim N(\mu, \sigma^2)$, where $\sigma^2$ is known.

We assign a Normal prior to the mean $\mu$: $\mu \sim N(\mu_0, \sigma_0^2)$.

To find the posterior $\pi(\mu|x_1, \dots, x_n)$, let $x = (x_1, \dots, x_n)$. The posterior is proportional to:

$$
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
$$

$$
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
$$



**Posterior Precision:**

It is often more convenient to work with **precision** (the inverse of variance). Let:

* $\tau_0 = 1/\sigma_0^2$ (Prior precision)
* $\tau = 1/\sigma^2$ (Data precision)
* $\tau_1 = 1/\sigma_1^2$ (Posterior precision)

The relationship is additive:

$$
\tau_1 = \tau_0 + n\tau
$$

$$
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
$$

The posterior mean $\mu_1$ is a weighted average of the prior mean and the sample mean:

$$
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
$$

So, the posterior distribution is:

$$
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
$$

**Numerical Illustration:**

Suppose we estimate a mean height $\mu$.

* **Known Variance:** $\sigma^2 = 100$ ($\tau = 0.01$).
* **Prior:** $\mu \sim N(175, 25)$ (Precision $\tau_0 = 0.04$).
* **Data:** $n=10, \bar{x}=180$. (Total data precision $n\tau = 0.1$).
* **Posterior:**
  * Precision $\tau_1 = 0.04 + 0.1 = 0.14$.
  * Variance $\sigma_1^2 \approx 7.14$.
  * Mean $\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6$.

The plot below illustrates the prior (dashed) and posterior (solid) normal densities.

```{r}
#| label: fig-normal-conjugacy
#| fig-cap: "Prior vs Posterior for Normal-Normal Example"
#| echo: true

mu_vals <- seq(150, 200, length.out = 200)

# Prior: N(175, 25) -> SD = 5
prior_norm <- dnorm(mu_vals, mean = 175, sd = 5)

# Posterior: N(178.6, 7.14) -> SD = Sqrt(7.14) Approx 2.67
posterior_norm <- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))

plot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Density",
     main = "Normal Prior vs Posterior",
     ylim = c(0, max(c(prior_norm, posterior_norm))))
lines(mu_vals, prior_norm, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior N(175, 25)", "Posterior N(178.6, 7.14)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-discrete-posterior}
### Discrete Posterior Calculation

Consider the following table where we calculate the posterior probabilities for a discrete parameter space.

Let the parameter $\theta$ take values $\{1, 2, 3\}$ with prior probabilities $\pi(\theta)$. Let the data $x$ take values $\{0, 1, 2, \dots\}$.

Given:

* Prior $\pi(\theta)$: $\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3$.
* Likelihood $\pi(x|\theta)$:
    * If $\theta=1$, $x \sim \text{Uniform on } \{0, 1\}$ (Prob = 1/2).
    * If $\theta=2$, $x \sim \text{Uniform on } \{0, 1, 2\}$ (Prob = 1/3).
    * If $\theta=3$, $x \sim \text{Uniform on } \{0, 1, 2, 3\}$ (Prob = 1/4).

Suppose we observe $x=2$. The calculation of the posterior probabilities is summarized in the table below:

| | $\theta=1$ | $\theta=2$ | $\theta=3$ | Sum |
|:---|:---:|:---:|:---:|:---:|
| **Prior** $\pi(\theta)$ | $1/3$ | $1/3$ | $1/3$ | $1$ |
| **Likelihood** $\pi(x=2|\theta)$ | $0$ | $1/3$ | $1/4$ | - |
| **Product** $\pi(\theta)\pi(x|\theta)$ | $0$ | $1/9$ | $1/12$ | $7/36$ |
| **Posterior** $\pi(\theta|x)$ | $0$ | $4/7$ | $3/7$ | $1$ |

The marginal sum (evidence) is calculated as $0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36$. The posterior values are obtained by dividing the product row by this sum.
:::

::: {#exm-Normal-with-Unknown-Mean-and-Variance}
### Normal with Unknown Mean and Variance

Consider $X_1, \dots, X_n \sim N(\mu, 1/\tau)$, where both $\mu$ and the precision $\tau$ are unknown.

We use a **Normal-Gamma** conjugate prior:

1.  $\tau \sim \text{Gamma}(\alpha, \beta)$
    $$\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}$$

2.  $\mu|\tau \sim N(\nu, 1/(k\tau))$
    $$\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}$$

The joint prior is the product of the conditional and the marginal:
$$
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
$$

**Derivation of the Posterior:**

First, we write the likelihood in terms of the sufficient statistics $\bar{x}$ and $S_{xx} = \sum (x_i - \bar{x})^2$:
$$
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
$$

Multiplying the prior by the likelihood gives the joint posterior:
$$
\begin{aligned}
\pi(\mu, \tau | x) &\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
$$

Next, we complete the square for the terms involving $\mu$ inside the brackets. It can be shown that:
$$
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
$$

Substituting this back into the joint density and grouping terms that do not depend on $\mu$:
$$
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
$$

**Results:**

By inspecting the factored equation above, we identify the updated parameters:

* **Marginal Posterior of $\tau$:**
    The first part corresponds to a Gamma kernel $\tau^{\alpha' - 1} e^{-\beta'\tau}$.
    $$\tau|x \sim \text{Gamma}(\alpha', \beta')$$
    where $\alpha' = \alpha + n/2$ and $\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2$.

* **Conditional Posterior of $\mu$:**
    The second part corresponds to a Normal kernel with precision $k'\tau$.
    $$\mu|\tau, x \sim N(\nu', 1/(k'\tau))$$
    where $k' = k + n$ and $\nu' = \frac{k\nu + n\bar{x}}{k+n}$.
:::

## Decision Theory and Bayes Rules

The general form of Bayes rule is derived by minimizing risk.

::: {#def-risk}
### Risk Function and Bayes Risk

* **Risk Function:** $R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx$
* **Bayes Risk:** The expected risk with respect to the prior.
    $$r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta$$
:::

::: {#thm-bayes-rule-minimization}
### Minimization of Bayes Risk

Minimizing the Bayes risk $r(\pi, d)$ is equivalent to minimizing the posterior expected loss for each observed $x$. That is, the Bayes rule $d(x)$ satisfies:
$$
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
$$
:::

::: {.proof}
We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function $R(\theta, d)$:

$$
\begin{aligned}
r(\pi, d) &= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
$$

Assuming the conditions for Fubini's Theorem are met, we switch the order of integration:

$$
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
$$

Recall that the joint density can be factored as $f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)$, where $m(x)$ is the marginal density of the data. Substituting this into the inner integral:

$$
\begin{aligned}
r(\pi, d) &= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
$$

Since the marginal density $m(x)$ is non-negative, minimizing the total integral $r(\pi, d)$ with respect to the decision rule $d(\cdot)$ is equivalent to minimizing the term inside the brackets for every $x$ (specifically where $m(x) > 0$).

The term inside the brackets is the **Posterior Expected Loss**:

$$
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
$$

Therefore, to minimize the Bayes risk, one must choose $d(x)$ to minimize the posterior expected loss for each $x$.
:::

### Common Loss Functions and Estimators

::: {#ex-squared-loss}
### Squared Error Loss (point Estimate)
    
$$L(\theta, a) = (\theta - a)^2$$
    
To find the optimal estimator $d(x)$, we minimize $E_{\theta|x}[(\theta - d(x))^2]$. Taking the derivative with respect to $d$ and setting to 0:
    
$$-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)$$
    
**Result:** The Bayes rule is the **posterior mean**.
:::

::: {#ex-absolute-loss}
### Absolute Error Loss
    
$$L(\theta, d) = |\theta - d|$$
    
Minimizing $E_{\theta|x}[|\theta - d|]$ requires solving:
    
$$\int_{-\infty}^{d} \pi(\theta|x) d\theta = \int_{d}^{\infty} \pi(\theta|x) d\theta = \frac{1}{2}$$
    
**Result:** The Bayes rule is the **posterior median**.
:::

::: {#ex-hypothesis-testing}
### Hypothesis Testing (0-1 Loss)
    
Testing $H_0: \theta \in \Theta_0$ vs $H_1: \theta \in \Theta_1$.
    
$$L(\theta, a) = \begin{cases} 1 & \text{if error} \\ 0 & \text{if correct} \end{cases}$$
    
The Bayes rule selects the hypothesis with the higher posterior probability.
    
$$d(x) = 1 \iff P(\theta \in \Theta_1 | x) \ge P(\theta \in \Theta_0 | x)$$
:::

::: {#def-hpd}
### Highest Posterior Density (HPD) Interval
    
In interval estimation, we prescribe a set $A = (d-\delta, d+\delta)$ and minimize the loss associated with $\theta$ falling outside this interval.
    
The Bayes rule $d(x)$ is the center of the interval with the highest probability coverage. This leads to the **Highest Posterior Density (HPD)** interval.
    
In practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the **Equal-Tailed Interval**, where we cut off $\alpha/2$ probability from each tail.
:::

## Minimax Estimation

A decision rule $d(x)$ is **minimax** if it minimizes the maximum possible risk: $\sup_\theta R(\theta, d)$.

::: {#thm-minimax-constant}
### Constant Risk Theorem
If a Bayes rule $d^\pi$ has constant risk (i.e., $R(\theta, d^\pi) = c$ for all $\theta$), then $d^\pi$ is a minimax estimator.
:::

::: {#ex-binomial-minimax}
### Binomial Minimax Estimator

Let $X \sim \text{Bin}(n, \theta)$ and $\theta \sim \text{Beta}(a, b)$.
The squared error loss is $L(\theta, d) = (\theta - d)^2$.
The Bayes estimator is the posterior mean:
$$d(x) = \frac{a+x}{a+b+n}$$

We calculate the risk $R(\theta, d)$:

$$
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
$$

Let $c = a+b+n$.
$$R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]$$

Using the bias-variance decomposition and knowing $E(x) = n\theta$ and $E(x^2) = (n\theta)^2 + n\theta(1-\theta)$, we expand the risk function. To make the risk constant (independent of $\theta$), we set the coefficients of $\theta$ and $\theta^2$ to zero.

Solving the resulting system of equations yields:
$$a = b = \frac{\sqrt{n}}{2}$$

Thus, the minimax estimator is:
$$d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}$$

This differs from the standard MLE $\hat{p} = x/n$ and the uniform prior Bayes estimator ($a=b=1$).
:::

## Stein Estimation and Shrinkage

Consider estimating a multivariate mean vector $\mu = (\mu_1, \dots, \mu_p)$ given independent observations $X_i \sim N(\mu_i, 1)$ for $i=1, \dots, p$.

The standard estimator is the MLE: $d^0(X) = X$.
The loss function is the sum of squared errors: $L(\mu, d) = ||\mu - d||^2 = \sum (\mu_i - d_i)^2$.

::: {#thm-stein-inadmissibility}
### Stein's Result

When $p \ge 3$, the estimator $d^0(X)$ is **inadmissible**. There exists an estimator that strictly dominates it (has lower risk everywhere).
:::

Consider the class of shrinkage estimators:
$$d^a(X) = \left( 1 - \frac{a}{||X||^2} \right) X$$
where $X = (X_1, \dots, X_p)^T$.

When $a > 0$, this estimator "shrinks" the data vector toward the origin $(0, \dots, 0)$.

::: {#lem-stein}
### Stein's Lemma

If $X \sim N(\mu, 1)$, then for a differentiable function $h$:
$$E[(X-\mu)h(X)] = E[h'(X)]$$
:::

::: {.proof}
Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator $d^a$.

$$R(\mu, d^a) = E || \mu - d^a(X) ||^2$$

After expanding and applying Stein's Lemma, the risk becomes:
$$R(\mu, d^a) = p - [2a(p-2) - a^2] E \left( \frac{1}{||X||^2} \right)$$

For $d^a$ to possess lower risk than $d^0$ (where risk = $p$), we need the term in the brackets to be positive:
$$2a(p-2) - a^2 > 0 \implies 0 < a < 2(p-2)$$

The optimal choice (minimizing risk) is $a = p-2$.
This yields the **James-Stein Estimator**:
$$\delta^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$$
:::

## Empirical Bayes

The James-Stein estimator can be motivated via an Empirical Bayes approach.

**Model:**

1.  $X_i | \mu_i \sim N(\mu_i, 1)$
2.  Prior: $\mu_i \sim N(0, \tau^2)$

The posterior mean for $\mu_i$ (if $\tau^2$ were known) is:
$$E(\mu_i|x_i) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i$$

The marginal distribution of $X_i$ is $N(0, 1+\tau^2)$.
Consequently, $S = \sum X_i^2 \sim (1+\tau^2) \chi^2_p$.

We can estimate the unknown shrinkage factor $B = \frac{1}{1+\tau^2}$ using the data.
Since $E[ \frac{p-2}{S} ] = \frac{1}{1+\tau^2}$, we replace the theoretical shrinkage factor with its unbiased estimate:
$$\hat{B} = \frac{p-2}{||X||^2}$$

This recovers the James-Stein rule:
$$\delta^{EB}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$$

::: {#ex-baseball}
### Baseball Example (efron & Morris)

We illustrate Stein estimation using baseball batting averages.
Let $y_i$ be the number of hits for player $i$ in their first $n=45$ at-bats.
Let $\hat{p}_i = y_i/n$ be the observed average.

To apply the Normal model, we use a variance-stabilizing transformation:
$$X_i = \sqrt{n} \arcsin(2\hat{p}_i - 1)$$
Under this transformation, $X_i \approx N(\mu_i, 1)$.

Using the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value $\mu_0$).
Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages $\hat{p}_i$.
:::

## Predictive Distributions

A key feature of Bayesian analysis is the predictive distribution for a future observation $x^*$.

$$f(x^*|x) = \int f(x^*|\theta) \pi(\theta|x) d\theta$$

::: {#ex-predictive-normal}
### Normal-normal Predictive Distribution

If $x_1, \dots, x_n \sim N(\mu, \sigma^2)$ (with $\sigma^2$ known) and $\mu \sim N(\mu_0, \sigma_0^2)$, the predictive distribution for a new observation $x^*$ is:

$$x^*|x \sim N(\mu_1, \sigma^2 + \sigma_1^2)$$

where $\mu_1$ and $\sigma_1^2$ are the posterior mean and variance of $\mu$. The predictive variance includes both the inherent sampling uncertainty ($\sigma^2$) and the uncertainty about the parameter ($\sigma_1^2$).
:::

## Hierarchical Modeling and MCMC

When analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).

**Hierarchical Structure:**

1.  **Data:** $X_i | \mu_i \sim f(x_i|\mu_i)$
2.  **Parameters:** $\mu_i | \theta \sim \pi(\mu_i|\theta)$
3.  **Hyperparameters:** $\theta \sim \pi(\theta)$

**Gibbs Sampling:**
To estimate the posterior $f(\mu, \theta | x)$, we sample iteratively from the **full conditional distributions**:

1.  Sample $\mu_i$ from $f(\mu_i | x, \theta)$.
2.  Sample $\theta$ from $f(\theta | \mu, x)$.

::: {#ex-hierarchical-baseball}
### Baseball Example with Hierarchical Model

* $Y_i \sim \text{Bin}(n_i, p_i)$
* Logit transform: $\mu_i = \text{logit}(p_i)$
* $\mu_i \sim N(\theta, \tau^2)$
* Priors on $\theta$ and $\tau^2$.

Since the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use **Metropolis-Hastings** steps within the Gibbs sampler.

**Algorithm:**

1.  Initialize parameters $\mu^{(0)}, \theta^{(0)}, \tau^{(0)}$.
2.  Propose new values based on a candidate distribution.
3.  Accept or reject based on the acceptance probability ratio (Likelihood $\times$ Prior ratio).
4.  Repeat until convergence.

The marginal posterior density for a specific parameter (e.g., $f(\mu_j|x)$) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.
:::
