---
title: "Bayesian Inference"
format: 
  html: default
  pdf: default
---

## Fundamental Elements of Bayesian Inference

The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes' Theorem (or Law).

::: {#def-posterior}
### Posterior Distribution

Suppose we have a parameter $\theta$ with a prior distribution denoted by $\pi(\theta)$. If we observe data $x$ drawn from a distribution with probability density function (pdf) $f(x; \theta)$, then the **posterior density** of $\theta$ given the data $x$ is defined as:

$$
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{m(x)}
$$

where $m(x)$ is the **marginal distribution** (or marginal likelihood) of the data, calculated as:
$$
m(x) = \int_{\Theta} \pi(\theta) f(x;\theta) d\theta
$$

In this context, $m(x)$ acts as a normalizing constant. Since it depends only on the data $x$ and not on the parameter $\theta$, it ensures that the posterior density integrates to 1 but does not influence the **shape** of the posterior distribution.

Thus, we often state the proportional relationship:

$$
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
$$
:::



::: {#exm-binomial-beta}
### Binomial-beta Conjugacy

Consider an experiment where $x|\theta \sim \text{Bin}(n, \theta)$. The likelihood function is:

$$
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
$$

Suppose we choose a Beta distribution as the prior for $\theta$, such that $\theta \sim \text{Beta}(a, b)$. The prior density is:

$$
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
$$

where $B(a,b)$ is the Beta function defined as $\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta$.

To find the posterior, we multiply the prior and the likelihood:

$$
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
$$

Combining terms with the same base:

$$
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
$$

We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:

$$
\theta|x \sim \text{Beta}(a+x, b+n-x)
$$

**Properties of the Posterior:**

* The posterior mean is:
    $$E(\theta|x) = \frac{a+x}{a+b+n}$$
    As $n \to \infty$, this approximates the maximum likelihood estimate $\frac{x}{n}$.

* The posterior variance is:
    $$\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}$$
    For large $n$, this approximates $\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}$.

**Numerical Illustration:**

Suppose we are estimating a probability $\theta$.

* **Prior:** $\theta \sim \text{Beta}(2, 2)$ (Mean = 0.5).
* **Data:** 10 trials, 8 successes ($n=10, x=8$).
* **Posterior:** $\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)$ (Mean $\approx$ 0.71).

The plot below shows the prior (dashed) and posterior (solid) densities.

```{r}
#| label: fig-beta-conjugacy
#| fig-cap: "Prior vs Posterior for Beta-Binomial Example"
#| echo: false

theta <- seq(0, 1, length.out = 200)

# Prior: Beta(2, 2)
prior <- dbeta(theta, shape1 = 2, shape2 = 2)

# Posterior: Beta(10, 4)
posterior <- dbeta(theta, shape1 = 10, shape2 = 4)

plot(theta, posterior, type = 'l', lwd = 2, col = "blue",
     xlab = expression(theta), ylab = "Density",
     main = "Beta Prior vs Posterior", ylim = c(0, max(c(prior, posterior))))
lines(theta, prior, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior Beta(2,2)", "Posterior Beta(10,4)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-normal-normal}
### Normal-normal Conjugacy (known Variance)

Let $X_1, X_2, \dots, X_n$ be independent and identically distributed (i.i.d.) variables such that $X_i \sim N(\mu, \sigma^2)$, where $\sigma^2$ is known.

We assign a Normal prior to the mean $\mu$: $\mu \sim N(\mu_0, \sigma_0^2)$.

To find the posterior $\pi(\mu|x_1, \dots, x_n)$, let $x = (x_1, \dots, x_n)$. The posterior is proportional to:

$$
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
$$

$$
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
$$



**Posterior Precision:**

It is often more convenient to work with **precision** (the inverse of variance). Let:

* $\tau_0 = 1/\sigma_0^2$ (Prior precision)
* $\tau = 1/\sigma^2$ (Data precision)
* $\tau_1 = 1/\sigma_1^2$ (Posterior precision)

The relationship is additive:

$$
\tau_1 = \tau_0 + n\tau
$$

$$
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
$$

The posterior mean $\mu_1$ is a weighted average of the prior mean and the sample mean:

$$
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
$$

So, the posterior distribution is:

$$
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
$$

**Numerical Illustration:**

Suppose we estimate a mean height $\mu$.

* **Known Variance:** $\sigma^2 = 100$ ($\tau = 0.01$).
* **Prior:** $\mu \sim N(175, 25)$ (Precision $\tau_0 = 0.04$).
* **Data:** $n=10, \bar{x}=180$. (Total data precision $n\tau = 0.1$).
* **Posterior:**
  * Precision $\tau_1 = 0.04 + 0.1 = 0.14$.
  * Variance $\sigma_1^2 \approx 7.14$.
  * Mean $\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6$.

The plot below illustrates the prior (dashed) and posterior (solid) normal densities.

```{r}
#| label: fig-normal-conjugacy
#| fig-cap: "Prior vs Posterior for Normal-Normal Example"
#| echo: false

mu_vals <- seq(150, 200, length.out = 200)

# Prior: N(175, 25) -> SD = 5
prior_norm <- dnorm(mu_vals, mean = 175, sd = 5)

# Posterior: N(178.6, 7.14) -> SD = Sqrt(7.14) Approx 2.67
posterior_norm <- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))

plot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Density",
     main = "Normal Prior vs Posterior",
     ylim = c(0, max(c(prior_norm, posterior_norm))))
lines(mu_vals, prior_norm, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior N(175, 25)", "Posterior N(178.6, 7.14)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-discrete-posterior}
### Discrete Posterior Calculation

Consider the following table where we calculate the posterior probabilities for a discrete parameter space.

Let the parameter $\theta$ take values $\{1, 2, 3\}$ with prior probabilities $\pi(\theta)$. Let the data $x$ take values $\{0, 1, 2, \dots\}$.

Given:

* Prior $\pi(\theta)$: $\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3$.
* Likelihood $\pi(x|\theta)$:
    * If $\theta=1$, $x \sim \text{Uniform on } \{0, 1\}$ (Prob = 1/2).
    * If $\theta=2$, $x \sim \text{Uniform on } \{0, 1, 2\}$ (Prob = 1/3).
    * If $\theta=3$, $x \sim \text{Uniform on } \{0, 1, 2, 3\}$ (Prob = 1/4).

Suppose we observe $x=2$. The calculation of the posterior probabilities is summarized in the table below:

| | $\theta=1$ | $\theta=2$ | $\theta=3$ | Sum |
|:---|:---:|:---:|:---:|:---:|
| **Prior** $\pi(\theta)$ | $1/3$ | $1/3$ | $1/3$ | $1$ |
| **Likelihood** $\pi(x=2|\theta)$ | $0$ | $1/3$ | $1/4$ | - |
| **Product** $\pi(\theta)\pi(x|\theta)$ | $0$ | $1/9$ | $1/12$ | $7/36$ |
| **Posterior** $\pi(\theta|x)$ | $0$ | $4/7$ | $3/7$ | $1$ |

The marginal sum (evidence) is calculated as $0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36$. The posterior values are obtained by dividing the product row by this sum.
:::

::: {#exm-Normal-with-Unknown-Mean-and-Variance}
### Normal with Unknown Mean and Variance

Consider $X_1, \dots, X_n \sim N(\mu, 1/\tau)$, where both $\mu$ and the precision $\tau$ are unknown.

We use a **Normal-Gamma** conjugate prior:

1.  $\tau \sim \text{Gamma}(\alpha, \beta)$
    $$\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}$$

2.  $\mu|\tau \sim N(\nu, 1/(k\tau))$
    $$\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}$$

The joint prior is the product of the conditional and the marginal:
$$
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
$$

**Derivation of the Posterior:**

First, we write the likelihood in terms of the sufficient statistics $\bar{x}$ and $S_{xx} = \sum (x_i - \bar{x})^2$:
$$
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
$$

Multiplying the prior by the likelihood gives the joint posterior:
$$
\begin{aligned}
\pi(\mu, \tau | x) &\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
$$

Next, we complete the square for the terms involving $\mu$ inside the brackets. It can be shown that:
$$
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
$$

Substituting this back into the joint density and grouping terms that do not depend on $\mu$:
$$
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
$$

**Results:**

By inspecting the factored equation above, we identify the updated parameters:

* **Marginal Posterior of $\tau$:**
    The first part corresponds to a Gamma kernel $\tau^{\alpha' - 1} e^{-\beta'\tau}$.
    $$\tau|x \sim \text{Gamma}(\alpha', \beta')$$
    where $\alpha' = \alpha + n/2$ and $\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2$.

* **Conditional Posterior of $\mu$:**
    The second part corresponds to a Normal kernel with precision $k'\tau$.
    $$\mu|\tau, x \sim N(\nu', 1/(k'\tau))$$
    where $k' = k + n$ and $\nu' = \frac{k\nu + n\bar{x}}{k+n}$.
:::

## Bayes Rules

The general form of Bayes rule is derived by minimizing risk.

::: {#def-risk}
### Risk Function and Bayes Risk

* **Risk Function:** $R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx$
* **Bayes Risk:** The expected risk with respect to the prior.
    $$r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta$$
:::

::: {#thm-bayes-rule-minimization}
### Minimization of Bayes Risk

Minimizing the Bayes risk $r(\pi, d)$ is equivalent to minimizing the posterior expected loss for each observed $x$. That is, the Bayes rule $d(x)$ satisfies:
$$
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
$$
:::

::: {.proof}
We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function $R(\theta, d)$:

$$
\begin{aligned}
r(\pi, d) &= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
$$

Assuming the conditions for Fubini's Theorem are met, we switch the order of integration:

$$
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
$$

Recall that the joint density can be factored as $f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)$, where $m(x)$ is the marginal density of the data. Substituting this into the inner integral:

$$
\begin{aligned}
r(\pi, d) &= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
$$

Since the marginal density $m(x)$ is non-negative, minimizing the total integral $r(\pi, d)$ with respect to the decision rule $d(\cdot)$ is equivalent to minimizing the term inside the brackets for every $x$ (specifically where $m(x) > 0$).

The term inside the brackets is the **Posterior Expected Loss**:

$$
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
$$


:::



:::{.callout-important}
Therefore, to minimize the Bayes risk, one just need to  choose $d(x)$ to minimize the posterior expected loss for each $x$.
:::

The following diagram summarizes the general workflow for deriving a Bayes estimator:

```{tikz fig-bayes-workflow}
%| fig-cap: "Workflow for Finding the Bayes Rule"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 80% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{shapes.geometric, arrows}"

\begin{tikzpicture}[node distance=2cm, auto]
    % Styles
    \tikzstyle{block} = [rectangle, draw, fill=blue!10, text width=8em, text centered, rounded corners, minimum height=3em]
    \tikzstyle{input} = [trapezium, trapezium left angle=70, trapezium right angle=110, draw, fill=green!10, text width=6em, text centered, minimum height=3em]
    \tikzstyle{line} = [draw, -latex, thick]

    % Nodes
    \node [input] (inputs) {Prior $\pi(\theta)$ \\ Likelihood $f(x|\theta)$ \\ Data $x$};
    \node [block, below of=inputs, node distance=3cm] (posterior) {Compute Posterior \\ $\pi(\theta|x)$};
    \node [input, right of=posterior, node distance=5cm] (loss) {Loss Function \\ $L(\theta, d)$};
    \node [block, below of=posterior, node distance=3cm] (exp_loss) {Compute Posterior Expected Loss \\ $E_{\theta|x}[L(\theta, d)]$};
    \node [block, below of=exp_loss, node distance=3cm] (minimize) {Minimize w.r.t $d$ \\ $\arg\min_d E[L]$};
    \node [block, below of=minimize, node distance=3cm, fill=red!10] (bayesrule) {Bayes Rule \\ $d(x)$};

    % Paths
    \path [line] (inputs) -- (posterior);
    \path [line] (posterior) -- (exp_loss);
    \path [line] (loss) |- (exp_loss);
    \path [line] (exp_loss) -- (minimize);
    \path [line] (minimize) -- (bayesrule);

\end{tikzpicture}
```


### Common Loss Functions and Bayes Estimators

#### Squared Error Loss (point Estimate)

$$L(\theta, a) = (\theta - a)^2$$

To find the optimal estimator $d(x)$, we minimize the posterior expected loss $E_{\theta|x}[(\theta - d(x))^2]$. Taking the derivative with respect to $d$ and setting it to 0:

$$-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)$$

**Result:** The Bayes rule under squared error loss is the **posterior mean**.

#### Absolute Error Loss

$$L(\theta, d) = |\theta - d|$$

To find the Bayes rule, we minimize the posterior expected loss:

$$
\psi(d) = E_{\theta|x} [ |\theta - d| ] = \int_{-\infty}^{\infty} |\theta - d| \, dF(\theta|x)
$$

where $F(\theta|x)$ is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point $d$:

$$
\psi(d) = \int_{-\infty}^{d} (d - \theta) \, dF(\theta|x) + \int_{d}^{\infty} (\theta - d) \, dF(\theta|x)
$$

We find the minimum by analyzing the rate of change of $\psi(d)$ with respect to $d$. Differentiating (or taking the subgradient for non-differentiable points):

$$
\frac{d}{dd} \psi(d) = \int_{-\infty}^{d} 1 \, dF(\theta|x) - \int_{d}^{\infty} 1 \, dF(\theta|x) = P(\theta \le d|x) - P(\theta > d|x)
$$

Setting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:

$$
P(\theta \le d|x) = P(\theta > d|x)
$$

Since the total probability is 1, this condition simplifies to finding $d$ such that the cumulative probability is $1/2$.

**General Case (Discrete or Mixed Distributions)**

In cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to $F(d) = 0.5$ may not exist. To generalize, the Bayes rule is defined as any **median** $m$ of the posterior distribution.

A median is formally defined as any value $m$ that satisfies the following two conditions simultaneously:

* $P(\theta \le m|x) \ge \frac{1}{2}$
* $P(\theta \ge m|x) \ge \frac{1}{2}$

**Result:** The Bayes rule under absolute error loss is the **posterior median**.

#### Weighted Absolute Error Loss (min-normalization)

$$L(\theta, d) = \frac{|\theta - d|}{\min(\theta, 1-\theta)}$$

This loss function penalizes errors extremely heavily when the true parameter $\theta$ is near the boundaries (0 or 1). Because the denominator approaches zero at the boundaries, the "cost" of an error becomes infinite, forcing the estimator to be very cautious (conservative) if the posterior has significant mass near 0 or 1.

To find the Bayes rule, we minimize the posterior expected loss. Let $\pi(\theta|x)$ denote the posterior density.

$$
\psi(d) = E_{\theta|x} \left[ \frac{|\theta - d|}{\min(\theta, 1-\theta)} \right] = \int \frac{|\theta - d|}{\min(\theta, 1-\theta)} \pi(\theta|x) \, d\theta
$$

Let $w(\theta) = \frac{1}{\min(\theta, 1-\theta)}$. We can view this integral as an expectation with respect to a **weighted posterior density** $\pi^*(\theta|x)$:

$$
\pi^*(\theta|x) \propto w(\theta) \pi(\theta|x) = \frac{\pi(\theta|x)}{\min(\theta, 1-\theta)}
$$

**Result:** The Bayes rule is the **median** of the weighted posterior distribution $\pi^*(\theta|x)$.

::: {#alg-weighted-median}
##### Importance Sampling for Weighted Median

**Goal:** Estimate the median of $\pi^*(\theta|x) \propto w(\theta)\pi(\theta|x)$ using samples from $\pi(\theta|x)$.

1.  **Sample:** Generate $M$ independent draws $\theta_1, \dots, \theta_M$ from the standard posterior $\pi(\theta|x)$.

2.  **Weight:** For each $i = 1, \dots, M$, compute the importance weight:
    $$ W_i = w(\theta_i) = \frac{1}{\min(\theta_i, 1-\theta_i)} $$

3.  **Sort:** Reorder the samples such that $\theta_{(1)} \le \theta_{(2)} \le \dots \le \theta_{(M)}$.
    Permute the weights $W_{(1)}, \dots, W_{(M)}$ to match this ordering.

4.  **Accumulate:** Compute the cumulative weights:
    $$ S_k = \sum_{j=1}^k W_{(j)} \quad \text{for } k=1, \dots, M $$

5.  **Select:** Find the smallest index $k^*$ such that the cumulative weight exceeds half the total weight:
    $$ k^* = \min \{ k : S_k \ge 0.5 \times S_M \} $$

6.  **Output:** Return the estimator $\hat{\delta} = \theta_{(k^*)}$.
:::

**Numerical Example: Beta(2, 10)**

We compare the "exact" weighted median (found by numerical integration) with the Monte Carlo estimate for a skewed distribution.

```{r}
#| echo: true


# 1. Setup Parameters
alpha <- 2
beta <- 10
# Standard Posterior Density: Dbeta(x, 2, 10)

# 2. Define the Weight Function (using MIN)
w <- function(theta) { 1 / pmin(theta, 1 - theta) }

# 3. Calculate "theoretical" Weighted Median (via Numerical Integration)
# Unnormalized Weighted Density
weighted_dens_unnorm <- function(theta) {
  w(theta) * dbeta(theta, alpha, beta)
}

# Find Normalization Constant C
C <- integrate(weighted_dens_unnorm, 0, 1)$value

# Define CDF of the Weighted Distribution
weighted_cdf <- function(q) {
  integrate(weighted_dens_unnorm, 0, q)$value / C
}

# Find Root of Cdf(m) - 0.5 = 0
# Note: Expanded Search Interval Because Median Might Shift Near Boundaries
theo_median <- uniroot(function(x) weighted_cdf(x) - 0.5, c(0.001, 0.999))$root


# 4. Calculate Monte Carlo Weighted Median
set.seed(123)
M <- 100000
theta_samples <- rbeta(M, alpha, beta)

# Compute Weights
weights <- w(theta_samples)

# Sort Samples and Weights
ord <- order(theta_samples)
sorted_theta <- theta_samples[ord]
sorted_weights <- weights[ord]

# Find Index Where Cumulative Weight Crosses 50%
cum_weights <- cumsum(sorted_weights)
total_weight <- sum(sorted_weights)
idx <- which(cum_weights >= 0.5 * total_weight)[1]
mc_median <- sorted_theta[idx]
# 5. Comparison Table
comparison_df <- data.frame(
  "Theoretical Weighted" = theo_median,
  "Monte Carlo Weighted" = mc_median,
  "Standard Posterior"   = qbeta(0.5, alpha, beta),
  check.names = FALSE
)

knitr::kable(comparison_df, digits = 3, align = "c")

```

#### Hypothesis Testing (0-1 Loss)

Consider the hypothesis test $H_0: \theta \in \Theta_0$ versus $H_1: \theta \in \Theta_1$. We define the decision space as $\mathcal{A} = \{0, 1\}$, where $a=0$ means accepting $H_0$ and $a=1$ means rejecting $H_0$ (accepting $H_1$).

**Case 1: 0-1 Loss**

The standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one:

| State of Nature ($\theta$) | Action $a=0$ (Accept $H_0$) | Action $a=1$ (Reject $H_0$) |
| :--- | :---: | :---: |
| **$\theta \in \Theta_0$ ($H_0$ True)** | $0$ (Correct) | $1$ (Type I Error) |
| **$\theta \in \Theta_1$ ($H_1$ True)** | $1$ (Type II Error) | $0$ (Correct) |

: Standard 0-1 Loss Function {#tbl-loss-std}

To find the Bayes rule, we minimize the **posterior expected loss** for a given $x$, denoted as $E_{\theta|x}[L(\theta, a)]$.

* **Expected Loss for choosing $a=0$ (Accept $H_0$):**
    $$
    E_{\theta|x}[L(\theta, 0)] = 0 \cdot P(\theta \in \Theta_0|x) + 1 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_1|x)
    $$

* **Expected Loss for choosing $a=1$ (Reject $H_0$):**
    $$
    E_{\theta|x}[L(\theta, 1)] = 1 \cdot P(\theta \in \Theta_0|x) + 0 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_0|x)
    $$

The Bayes rule selects the action with the smaller expected loss. Thus, we choose $a=1$ if:
$$
P(\theta \in \Theta_0|x) \le P(\theta \in \Theta_1|x)
$$
This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.

**Case 2: General Loss (Asymmetric Costs)**

In many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let $c_1$ be the cost of a Type I error and $c_2$ be the cost of a Type II error. Usually, we normalize one cost to 1.

| State of Nature ($\theta$) | Action $a=0$ (Accept $H_0$) | Action $a=1$ (Reject $H_0$) |
| :--- | :---: | :---: |
| **$\theta \in \Theta_0$ ($H_0$ True)** | $0$ | $c$ (Type I Error) |
| **$\theta \in \Theta_1$ ($H_1$ True)** | $1$ (Type II Error) | $0$ |

: Loss Function with Type I Error Cost $c$ {#tbl-loss-cost}

We again calculate the posterior expected loss:

* **Expected Loss for $a=0$:**
    $$E[L(\theta, 0)|x] = 0 \cdot P(\Theta_0|x) + 1 \cdot P(\Theta_1|x) = P(\Theta_1|x)$$

* **Expected Loss for $a=1$:**
    $$E[L(\theta, 1)|x] = c \cdot P(\Theta_0|x) + 0 \cdot P(\Theta_1|x) = c P(\Theta_0|x)$$

We reject $H_0$ ($a=1$) if the expected loss of doing so is lower:
$$
c P(\Theta_0|x) \le P(\Theta_1|x)
$$

Since $P(\Theta_1|x) = 1 - P(\Theta_0|x)$, we can rewrite this condition as:
$$
c P(\Theta_0|x) \le 1 - P(\Theta_0|x) \implies (1+c) P(\Theta_0|x) \le 1
$$
$$
P(\Theta_0|x) \le \frac{1}{1+c}
$$

**Result:** With asymmetric costs, we accept $H_1$ only if the posterior probability of the null hypothesis is sufficiently small (below the threshold $\frac{1}{1+c}$). If the cost of false rejection $c$ is high, we require stronger evidence against $H_0$.

#### Classification Prediction (categorical Parameter)

In classification problems, the parameter of interest is a discrete class label $\theta$ (often denoted as $y$) taking values in a set of categories $\{1, 2, \dots, K\}$. The goal is to predict the true class label based on observed features $x$.

We typically employ the **0-1 loss function**, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:

$$L(\theta, \hat{\theta}) = \begin{cases} 0 & \text{if } \hat{\theta} = \theta \ (\text{Correct Classification}) \\ 1 & \text{if } \hat{\theta} \neq \theta \ (\text{Misclassification}) \end{cases}$$

To find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.

$$
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta} L(\theta, \hat{\theta}) \pi(\theta|x)
$$

Since the loss is 1 only when the predicted class $\hat{\theta}$ differs from the true class $\theta$, this sum simplifies to:

$$
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta \neq \hat{\theta}} 1 \cdot \pi(\theta|x) = P(\theta \neq \hat{\theta} | x) = 1 - P(\theta = \hat{\theta} | x)
$$

Minimizing the misclassification rate $1 - P(\theta = \hat{\theta} | x)$ is mathematically equivalent to maximizing the probability of being correct, $P(\theta = \hat{\theta} | x)$.

**Result:** The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the **Maximum A Posteriori (MAP)** estimator, in the context of machine learning and pattern recognition, this decision rule is known as the **Bayes Optimal Classifier**.

$$
\hat{\theta}_{\text{Bayes}}(x) = \underset{k \in \{1, \dots, K\}}{\arg\max} \ P(\theta = k | x)
$$



#### Interval Estimation and Highest Posterior Density (HPD)

We can motivate the choice of a Credible Interval by defining a specific loss function for interval estimation. Suppose we seek an estimate $d$ and specify a tolerance $\delta > 0$. We define the loss function as:

$$
L(\theta, d) = \begin{cases} 
0 & \text{if } |\theta - d| \le \delta \\
1 & \text{if } |\theta - d| > \delta
\end{cases}
$$

The **Expected Posterior Loss** is the posterior probability that $\theta$ lies outside the interval $(d - \delta, d + \delta)$. Therefore, minimizing the loss is equivalent to finding the interval of fixed length $2\delta$ that **maximizes the posterior probability**:

$$ P(d - \delta \le \theta \le d + \delta \mid x) $$

In practice, we typically reverse this formulation: instead of fixing the length $2\delta$, we fix the coverage probability $1-\alpha$ (e.g., 0.95) and seek the interval with the **shortest possible length**. This results in the **Highest Posterior Density (HPD)** interval, defined as the region where the posterior density exceeds a certain threshold $c$:

$$ C_{HPD} = \{ \theta : \pi(\theta \mid x) \ge c \} $$

This HPD interval is optimal because it includes the "most likely" values of $\theta$ and, for a unimodal distribution, provides the narrowest interval for a given confidence level.


**Comparison with Equal-Tailed Intervals:**

* **Equal-Tailed Interval:** We simply cut off $\alpha/2$ probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.
* **HPD Interval:** This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.

The plot below illustrates a skewed posterior distribution (Gamma). Notice how the **HPD Interval (Blue)** is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the **Equal-Tailed Interval (Red)**.

```{r}
#| label: fig-hpd-vs-equal
#| fig-cap: "Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution"
#| echo: false

## Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)
x_vals <- seq(0, 15, length.out = 1000)
y_vals <- dgamma(x_vals, shape = 2, rate = 0.5)

## Target Coverage
alpha <- 0.10
target_prob <- 1 - alpha

## 1. Equal-tailed Interval (quantiles)
eq_lower <- qgamma(alpha/2, shape = 2, rate = 0.5)
eq_upper <- qgamma(1 - alpha/2, shape = 2, rate = 0.5)

## 2. HPD Interval (density Threshold Optimization)
## We Look for a Density Threshold K Such That the Area Above K Is 0.90
find_hpd <- function(dist_vals, density_vals, probability) {
  ## Sort density values
  ord <- order(density_vals, decreasing = TRUE)
  sorted_dens <- density_vals[ord]
  sorted_dist <- dist_vals[ord]
  
  ## Accumulate probability (approximation)
  dx <- diff(dist_vals)[1]
  cum_prob <- cumsum(sorted_dens * dx)
  
  ## Find cutoff index
  cutoff_idx <- which(cum_prob >= probability)[1]
  
  ## Get the subset of x values
  hpd_set <- sorted_dist[1:cutoff_idx]
  return(c(min(hpd_set), max(hpd_set)))
}

hpd_bounds <- find_hpd(x_vals, y_vals, target_prob)
hpd_lower <- hpd_bounds[1]
hpd_upper <- hpd_bounds[2]

## Plotting
plot(x_vals, y_vals, type = 'l', lwd = 2, col = "black",
     main = "90% Credible Intervals (Skewed Posterior)",
     xlab = expression(theta), ylab = "Density",
     ylim = c(0, max(y_vals) * 1.2))

## Shade HPD
polygon(c(x_vals[x_vals >= hpd_lower & x_vals <= hpd_upper], hpd_upper, hpd_lower),
        c(y_vals[x_vals >= hpd_lower & x_vals <= hpd_upper], 0, 0),
        col = rgb(0, 0, 1, 0.2), border = NA)

## Draw Equal-tailed Lines (red)
abline(v = c(eq_lower, eq_upper), col = "red", lwd = 2, lty = 2)
## Draw HPD Lines (blue)
abline(v = c(hpd_lower, hpd_upper), col = "blue", lwd = 2, lty = 1)

legend("topright", 
       legend = c("Posterior Density", 
                  paste0("Equal-Tailed (Len: ", round(eq_upper - eq_lower, 2), ")"), 
                  paste0("HPD (Len: ", round(hpd_upper - hpd_lower, 2), ")")),
       col = c("black", "red", "blue"), 
       lty = c(1, 2, 1), lwd = 2,
       fill = c(NA, NA, rgb(0, 0, 1, 0.2)), border = NA)

```


### Finding Minimax Rules with Bayes Rules

@thm-minimax-constant states that if a Bayes estimator $\delta^\pi$ (derived from a prior $\pi$) yields a constant risk $R(\theta, \delta^\pi) = c$ across the entire parameter space $\Theta$, then that estimator is necessarily minimax. 


This result is a cornerstone of decision theory because it provides a sufficient condition for minimaxity. While the minimax criterion focuses on the "worst-case scenario" by minimizing the maximum possible risk, the Bayes criterion focuses on the "average-case scenario" relative to a prior. When the risk is constant, these two perspectives align: the average risk equals the maximum risk, and no other estimator can achieve a lower maximum without also having a lower Bayes risk, which would contradict the optimality of the Bayes rule.


::: {#exm-binomial-minimax}
#### Binomial Minimax Estimator

Let $X \sim \text{Bin}(n, \theta)$ and $\theta \sim \text{Beta}(a, b)$.
The squared error loss is $L(\theta, d) = (\theta - d)^2$.
The Bayes estimator is the posterior mean:
$$d(x) = \frac{a+x}{a+b+n}$$

We calculate the risk $R(\theta, d)$:

$$
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
$$

Let $c = a+b+n$.
$$R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]$$

Using the bias-variance decomposition and knowing $E(x) = n\theta$ and $E(x^2) = (n\theta)^2 + n\theta(1-\theta)$, we expand the risk function. To make the risk constant (independent of $\theta$), we set the coefficients of $\theta$ and $\theta^2$ to zero.

Solving the resulting system of equations yields:
$$a = b = \frac{\sqrt{n}}{2}$$

Thus, the minimax estimator is:
$$d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}$$

This differs from the standard MLE $\hat{p} = x/n$ and the uniform prior Bayes estimator ($a=b=1$).
:::

According to @thm-limits, let $\{\delta_n\}$ be a sequence of Bayes rules with respect to priors $\{\pi_n\}$, and let $r(\pi_n, \delta_n)$ be the associated Bayes risks. If there exists a rule $\delta_0$ such that
$$
\sup_{\theta} R(\theta, \delta_0) \le \lim_{n \to \infty} r(\pi_n, \delta_n)
$$
then $\delta_0$ is a minimax estimator.


::: {#exm-exponential-minimax}
#### Exponential Minimax Estimation

Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$ with mean $\theta$. We consider the **Scale-Invariant Loss Function**:

$$
L(\theta, \delta) = \left( \frac{\delta}{\theta} - 1 \right)^2
$$

We propose the estimator $\delta_0(X) = \frac{\sum X_i}{n+1}$. Consider a sequence of priors $\pi_k \sim \text{Inverse-Gamma}(\alpha_k, \beta_k)$ for $\theta$, where the density is given by $\pi(\theta) \propto \theta^{-\alpha_k - 1} e^{-\beta_k / \theta}$.

We will show that $\delta_0$ is a minimax estimator by deriving the Bayes estimator for this sequence and comparing the risks.

**Posterior Analysis and Moments**

Let $T = \sum X_i$. The likelihood is $L(\theta|X) = \theta^{-n} e^{-T/\theta}$. Multiplying by the prior $\pi_k(\theta)$, the posterior density is:

$$
\pi(\theta | X) \propto \theta^{-(n + \alpha_k) - 1} e^{-(T + \beta_k)/\theta}
$$

This is an **Inverse-Gamma** distribution with parameters $\alpha^* = n + \alpha_k$ and $\beta^* = T + \beta_k$. Using the properties of the Inverse-Gamma distribution, the required posterior moments are:

$$
E[\theta^{-1} | X] = \frac{n + \alpha_k}{T + \beta_k}, \quad E[\theta^{-2} | X] = \frac{(n + \alpha_k)(n + \alpha_k + 1)}{(T + \beta_k)^2}
$$

The posterior expected loss is $E[ L(\theta, \delta) | X ] = \delta^2 E[\theta^{-2} | X] - 2\delta E[\theta^{-1} | X] + 1$.

**Derivation of the Bayes Estimator**

Minimizing the posterior expected loss with respect to $\delta$ yields $\delta_{\pi_k} = \frac{E[\theta^{-1} | X]}{E[\theta^{-2} | X]}$. Substituting the moments derived above:

$$
\delta_{\pi_k}(X) = \frac{ \frac{n + \alpha_k}{T + \beta_k} }{ \frac{(n + \alpha_k)(n + \alpha_k + 1)}{(T + \beta_k)^2} } = \frac{T + \beta_k}{n + \alpha_k + 1}
$$

**Bayes Risk Limit**

The Bayes risk $r(\pi_k)$ is the expected value of the minimum posterior loss. Substituting $\delta_{\pi_k}$ back into the loss equation:

$$
r(\pi_k | X) = 1 - \frac{(E[\theta^{-1}|X])^2}{E[\theta^{-2}|X]} = 1 - \frac{n + \alpha_k}{n + \alpha_k + 1} = \frac{1}{n + \alpha_k + 1}
$$

Since this does not depend on $X$, the integrated Bayes risk is $r(\pi_k) = \frac{1}{n + \alpha_k + 1}$. Taking the limit as the prior parameters approach zero ($\alpha_k \to 0$):

$$
\lim_{k \to \infty} r(\pi_k) = \frac{1}{n+1}
$$

**Minimax Verification**

Finally, we calculate the frequentist risk for the candidate estimator $\delta_0(X) = \frac{T}{n+1}$. Let $Y = T/\theta \sim \text{Gamma}(n, 1)$.

$$
R(\theta, \delta_0) = E \left[ \left( \frac{Y}{n+1} - 1 \right)^2 \right] = \text{Var}\left(\frac{Y}{n+1}\right) + \left(E\left[\frac{Y}{n+1}\right]-1\right)^2
$$

With $E[Y]=n$ and $\text{Var}(Y)=n$, the risk is:

$$
R(\theta, \delta_0) = \frac{n}{(n+1)^2} + \left( \frac{n}{n+1} - 1 \right)^2 = \frac{1}{n+1}
$$

Since the risk of $\delta_0$ is constant and equals the limit of the Bayes risks ($\frac{1}{n+1}$), $\delta_0$ is a **minimax estimator**.
:::

## Stein's Paradox and the James-stein Estimator

In high-dimensional estimation ($p \ge 3$), the Maximum Likelihood Estimator (MLE) is inadmissible under squared error loss. The **James-Stein Estimator** dominates the MLE, meaning it achieves lower risk for all values of $\theta$.



Consider the setting:

* Data: $X \sim N_p(\theta, I)$
* Prior: $\theta \sim N_p(0, \tau^2 I)$
* Estimator: $d^{JS}(x) = \left( 1 - \frac{p-2}{||x||^2} \right) x$


::: {#thm-mle-inadmissible}
### Inadmissibility of the MLE in High Dimensions (stein's Phenomenon)

Let $X \sim N_p(\theta, I)$ be a $p$-dimensional random vector with $p \ge 3$. Under the squared error loss function $L(\theta, d) = ||\theta - d||^2$, the standard Maximum Likelihood Estimator $d^0(X) = X$ is **inadmissible**.
:::

::: {.proof}
To show that $d^0(X) = X$ is inadmissible, we must find another estimator that dominates it (i.e., has equal or lower risk for all $\theta$, and strictly lower risk for at least one $\theta$).

First, consider the risk of the standard estimator $d^0$. Since $X_i \sim N(\theta_i, 1)$ are independent:

$$
R(\theta, d^0) = E_\theta [ ||X - \theta||^2 ] = \sum_{i=1}^p E [ (X_i - \theta_i)^2 ] = \sum_{i=1}^p \text{Var}(X_i) = p
$$

Now consider the James-Stein estimator $d^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$. As established in the derivation of the Bayes Risk in @thm-js-bayes-risk, the frequentist risk function of $d^{JS}$ is:

$$
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
$$

Since the random variable $||X||^2$ is non-negative and not identically infinity, the expectation $E_\theta [ 1/||X||^2 ]$ is strictly positive for all $\theta$. Therefore:

$$
R(\theta, d^{JS}) < p = R(\theta, d^0) \quad \text{for all } \theta \in \mathbb{R}^p
$$

Because $d^{JS}$ achieves a strictly lower risk than $d^0$ everywhere in the parameter space, $d^0$ is dominated by $d^{JS}$ and is thus inadmissible.
:::

### Practical Application: One-way ANOVA and "Borrowing Strength"

::: {#exm-anova-js}

Consider a One-Way ANOVA setting where we wish to estimate the means of $p$ different independent groups (e.g., the true batting averages of $p=10$ baseball players, or the efficacy of $p=5$ different hospital treatments).

* **Model:** Let $X_i \sim N(\theta_i, \sigma^2)$ be the observed sample mean for group $i$, for $i = 1, \dots, p$.
* **Goal:** Estimate the vector of true means $\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)$ simultaneously. The loss is the sum of squared errors: $L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}) = \sum (\theta_i - \hat{\theta}_i)^2$.

**The MLE Approach (Total Separation):**
The standard estimator is $\hat{\theta}_i^{\text{MLE}} = X_i$. This estimates each group entirely independently, using only data from that specific group. If a specific player has a lucky streak, their estimate is very high; if they are unlucky, it is very low.

**The James-Stein Approach (Shrinkage / Pooling):**
In this context, the James-Stein estimator (specifically the variation shrinking toward the grand mean $\bar{X}$) is:
$$
\hat{\theta}_i^{JS} = \bar{X} + \left( 1 - \frac{(p-3)\sigma^2}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X})
$$

**Why is this better?**
Even though the groups might be physically independent (e.g., distinct hospitals), the James-Stein estimator **"borrows strength"** from the ensemble.

1.  **Noise Reduction:** Extreme observations $X_i$ are likely to contain more positive noise than signal. Shrinking them toward the global average $\bar{X}$ reduces this variance.
2.  **Stein's Paradox:** While $\hat{\theta}_i^{JS}$ introduces bias (estimates are pulled toward the center), the reduction in variance is so significant that the **Total Risk** (sum of squared errors over all groups) is strictly lower than that of the MLE, provided $p \ge 3$.

Thus, estimating the groups *together* yields a more accurate global picture than estimating them *separately*, even if the groups are independent.
:::

### Why Is This Paradoxical?

The result that $d^{JS}$ dominates $d^0$ is called **Stein's Paradox** because it defies intuition in several ways:

1.  **Independence Irrelevance:** The result holds even if the components $X_i$ are completely unrelated (e.g., $X_1$ is the price of tea in China, $X_2$ is the temperature in Saskatoon, and $X_3$ is the weight of a local cat). It seems absurd that combining unrelated data improves the estimate of each, but the combined risk is indeed lower.
2.  **No "Free Lunch":** The James-Stein estimator does not improve every individual component $\theta_i$ simultaneously for every realization. Instead, it minimizes the **total** risk $\sum E(\hat{\theta}_i - \theta_i)^2$. It sacrifices accuracy on outliers (by biasing them) to gain significant stability on the bulk of the data.
3.  **Destruction of Symmetry:** The MLE is invariant under translation and rotation. The James-Stein estimator breaks this symmetry by shrinking toward an arbitrary point (usually the origin or the grand mean), yet it yields a better objective performance.

### What We Learned

1.  **Bias-Variance Tradeoff:** This is the most famous example where introducing **bias** (shrinkage) leads to a massive reduction in **variance**, thereby reducing the overall Mean Squared Error (MSE). Unbiasedness is not always a virtue in estimation.
2.  **Inadmissibility in High Dimensions:** Intuitions formed in 1D or 2D (where MLE is admissible) fail in higher dimensions ($p \ge 3$). The volume of space grows so fast that "standard" diffuse priors or MLEs become inefficient.
3.  **Hierarchical Modeling:** Stein's result provides the theoretical foundation for **Hierarchical Bayesian Models**. When we assume parameters come from a common distribution (e.g., $\theta_i \sim N(\mu, \tau^2)$), we naturally derive shrinkage estimators that "borrow strength" across groups, formalized as Empirical Bayes or fully Bayesian methods.

## Empirical Bayes Rules

The James-Stein estimator provides a natural entry point into the concept of **Empirical Bayes (EB)**. While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.

### The General Empirical Bayes Framework

In a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to "learn" the prior.

The workflow typically follows these steps:

1.  **Hierarchical Model:**
    We assume the data $X$ comes from a distribution $f(x|\theta)$, and the parameter $\theta$ comes from a prior $\pi(\theta|\eta)$ controlled by hyperparameters $\eta$.

2.  **Marginal Likelihood (Evidence):**
    We integrate out the parameter $\theta$ to obtain the marginal distribution of the data given the hyperparameters:
    $$m(x|\eta) = \int f(x|\theta) \pi(\theta|\eta) d\theta$$

3.  **Estimation of Hyperparameters:**
    Instead of fixing $\eta$, we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments:
    $$\hat{\eta} = \underset{\eta}{\arg\max} \ m(x|\eta)$$

4.  **Posterior Inference:**
    We proceed with standard Bayesian inference, but we substitute the estimated estimate $\hat{\eta}$ into the posterior:
    $$\pi(\theta|x, \hat{\eta}) \propto f(x|\theta) \pi(\theta|\hat{\eta})$$

**Discussion:**

* **"Borrowing Strength":** EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.
* **The Critique:** A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate $\theta$) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a "hyperprior" on $\eta$ and integrate it out.

### Deriving James-stein as Empirical Bayes


We can derive the Bayes Risk $r(\pi, d^{JS})$ of this estimator using two equivalent methods: minimizing the expected frequentist risk, or minimizing the expected posterior loss.

::: {#thm-js-bayes-risk}
### Bayes Risk of James-stein Estimator

For $p \ge 3$, the Bayes risk of the James-Stein estimator $d^{JS}$ with respect to the prior $\theta \sim N(0, \tau^2 I)$ is:

$$
r(\pi, d^{JS}) = \frac{p\tau^2 + 2}{\tau^2 + 1}
$$
:::

::: {.proof}
**Method 1: Integration over the Prior (Frequentist Risk approach)** 

The Bayes risk is defined as $r(\pi, d) = E_\pi [ R(\theta, d) ]$.

First, recall the frequentist risk of the James-Stein estimator for a fixed $\theta$. Using Stein's Lemma, the risk is given by:
$$
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
$$

To find the Bayes risk, we take the expectation of this risk with respect to the prior $\pi(\theta)$:
$$
r(\pi, d^{JS}) = \int R(\theta, d^{JS}) \pi(\theta) d\theta = p - (p-2)^2 E_\pi \left[ E_\theta \left( \frac{1}{||X||^2} \right) \right]
$$

By the law of iterated expectations, $E_\pi [ E_\theta (\cdot) ]$ is equivalent to the expectation with respect to the marginal distribution of $X$, denoted as $m(x)$.
Under the conjugate prior, the marginal distribution is $X \sim N(0, (1+\tau^2)I)$.

Consequently, the quantity $\frac{||X||^2}{1+\tau^2}$ follows a Chi-squared distribution with $p$ degrees of freedom ($\chi^2_p$). The expectation of the inverse chi-square is:
$$
E \left[ \frac{1}{||X||^2} \right] = \frac{1}{1+\tau^2} E \left[ \frac{1}{\chi^2_p} \right] = \frac{1}{1+\tau^2} \cdot \frac{1}{p-2}
$$

Substituting this back into the risk equation:
$$
\begin{aligned}
r(\pi, d^{JS}) &= p - (p-2)^2 \cdot \frac{1}{(p-2)(1+\tau^2)} \\
&= p - \frac{p-2}{1+\tau^2} \\
&= \frac{p(1+\tau^2) - (p-2)}{1+\tau^2} \\
&= \frac{p\tau^2 + p - p + 2}{1+\tau^2} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\end{aligned}
$$
:::

::: {.proof}
**Method 2: Integration over the Marginal (Posterior Loss approach)** 

Alternatively, we can compute the Bayes risk by first finding the posterior expected loss for a given $x$, and then averaging over the marginal distribution of $x$:
$$
r(\pi, d) = E_m [ E_{\theta|x} [ L(\theta, d(x)) ] ]
$$

**Step 1: Posterior Expected Loss**

The posterior distribution of $\theta$ given $x$ is:
$$
\theta | x \sim N \left( \frac{\tau^2}{1+\tau^2}x, \frac{\tau^2}{1+\tau^2}I \right)
$$

The expected squared error loss can be decomposed into the variance (trace) and the squared bias:
$$
E_{\theta|x} [ ||\theta - d^{JS}(x)||^2 ] = \text{tr}(\text{Var}(\theta|x)) + || E[\theta|x] - d^{JS}(x) ||^2
$$

* **Trace term:**
    $$\text{tr} \left( \frac{\tau^2}{1+\tau^2} I_p \right) = \frac{p\tau^2}{1+\tau^2}$$

* **Squared Bias term:**
    Let $B = \frac{1}{1+\tau^2}$. Then $E[\theta|x] = (1-B)x$.
    The estimator is $d^{JS}(x) = (1 - \frac{p-2}{||x||^2})x$.
    The difference is:
    $$
    E[\theta|x] - d^{JS}(x) = \left( (1-B) - \left( 1 - \frac{p-2}{||x||^2} \right) \right) x = \left( \frac{p-2}{||x||^2} - B \right) x
    $$
    Squaring the norm gives:
    $$
    \left( \frac{p-2}{||x||^2} - B \right)^2 ||x||^2 = \frac{(p-2)^2}{||x||^2} - 2B(p-2) + B^2 ||x||^2
    $$

**Step 2: Expectation with respect to Marginal $X$** 

We now take the expectation $E_m[\cdot]$ of the posterior loss. Recall $X \sim N(0, (1+\tau^2)I)$, so $E[||X||^2] = p(1+\tau^2)$ and $E[1/||X||^2] = \frac{1}{(p-2)(1+\tau^2)}$.

* **Expectation of Trace term:** Constant, remains $\frac{p\tau^2}{1+\tau^2}$.
* **Expectation of Bias term:**
    $$
    \begin{aligned}
    E_m \left[ \frac{(p-2)^2}{||X||^2} - \frac{2(p-2)}{1+\tau^2} + \frac{||X||^2}{(1+\tau^2)^2} \right] &= (p-2)^2 \frac{1}{(p-2)(1+\tau^2)} - \frac{2(p-2)}{1+\tau^2} + \frac{p(1+\tau^2)}{(1+\tau^2)^2} \\
    &= \frac{p-2}{1+\tau^2} - \frac{2p-4}{1+\tau^2} + \frac{p}{1+\tau^2} \\
    &= \frac{p - 2 - 2p + 4 + p}{1+\tau^2} \\
    &= \frac{2}{1+\tau^2}
    \end{aligned}
    $$

**Step 3: Combine Terms** 

$$
r(\pi, d^{JS}) = \underbrace{\frac{p\tau^2}{1+\tau^2}}_{\text{Variance Part}} + \underbrace{\frac{2}{1+\tau^2}}_{\text{Bias Part}} = \frac{p\tau^2 + 2}{\tau^2 + 1}
$$

Both methods yield the same result.
:::

We can derive the James-Stein rule explicitly using this framework.

**Model:**

1.  Likelihood: $X_i | \mu_i \sim N(\mu_i, 1)$ for $i=1, \dots, p$.
2.  Prior: $\mu_i \sim N(0, \tau^2)$. Here, $\tau^2$ is the unknown hyperparameter.

**Step 1: The Ideal Bayes Estimator**

If we knew $\tau^2$, the posterior distribution of $\mu_i$ would be Normal with mean:
$$E(\mu_i|x_i, \tau^2) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i$$
We define the shrinkage factor $B = \frac{1}{1+\tau^2}$.

**Step 2: Marginal Estimation**

Since $\mu_i$ and $X_i-\mu_i$ are independent normals, the marginal distribution of the data is:
$$X_i \sim N(0, 1+\tau^2)$$
Consequently, the sum of squares $S = ||X||^2 = \sum X_i^2$ follows a scaled Chi-squared distribution:
$$S \sim (1+\tau^2) \chi^2_p$$

**Step 3: Estimating the Shrinkage Factor**

We need to estimate $B = \frac{1}{1+\tau^2}$. Note that the expected value of an inverse Chi-square variable is $E[1/\chi^2_p] = \frac{1}{p-2}$. Therefore:
$$E \left[ \frac{p-2}{S} \right] = \frac{p-2}{1+\tau^2} E\left[\frac{1}{\chi^2_p}\right] = \frac{p-2}{1+\tau^2} \cdot \frac{1}{p-2} = \frac{1}{1+\tau^2} = B$$

Thus, $\hat{B} = \frac{p-2}{||X||^2}$ is an unbiased estimator of the optimal shrinkage factor.

**Step 4: The Empirical Bayes Rule**

Plugging $\hat{B}$ into the ideal Bayes estimator recovers the James-Stein rule:
$$\delta^{EB}(X) = \left( 1 - \hat{B} \right) X = \left( 1 - \frac{p-2}{||X||^2} \right) X$$



## Hierarchical Modeling via MCMC

In complex Bayesian settings where the posterior distribution cannot be derived analytically, we utilize hierarchical structures to represent levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate the resulting distributions.


### Hierarchical Model Structure

A hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:

$$
\begin{aligned}
\text{Level 1 (Data Likelihood):} & \quad X_i | \mu_i, \sigma^2 \sim f(x_i | \mu_i, \sigma^2) \\
\text{Level 2 (Parameters):} & \quad \mu_i | \theta, \tau^2 \sim \pi(\mu_i | \theta, \tau^2) \\
\text{Level 3 (Hyperparameters):} & \quad \theta, \tau^2 \sim \pi(\theta, \tau^2)
\end{aligned}
$$

The goal is to compute the joint posterior distribution of all unobserved parameters given the data $X = \{X_1, \dots, X_n\}$:

$$
p(\boldsymbol{\mu}, \theta, \tau^2 | X) \propto \left[ \prod_{i=1}^n f(x_i | \mu_i, \sigma^2) \pi(\mu_i | \theta, \tau^2) \right] \pi(\theta, \tau^2)
$$


### Graphical Model Representation (tree Structure)

The following tree diagram illustrates the conditional dependencies. Note that the parameters $\mu_i$ are conditionally independent given the hyperparameter $\theta$, which facilitates "borrowing strength" across groups.

```{tikz fig-hierarchical-tree}
%| fig-cap: "Hierarchical Tree Structure"
%| echo: false
%| fig-align: center
%| out-extra: 'style="width: 80% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{arrows, positioning, shapes}"

\begin{tikzpicture}[
    node distance=1.5cm and 1.5cm,
    obs/.style={circle, draw=black, fill=gray!30, thick, minimum size=0.8cm},
    latent/.style={circle, draw=black, thick, minimum size=0.8cm},
    every node/.style={align=center}
]

% Top Level: Hyperparameter
\node[latent] (theta) {$\theta, \tau^2$};

% Middle Level: Parameters (mu)
\node[latent] (mu2) [below=of theta] {$\mu_2$};
\node[latent] (mu1) [left=of mu2] {$\mu_1$};
\node (dots) [right=of mu2] {$\dots$};
\node[latent] (mun) [right=of dots] {$\mu_n$};

% Bottom Level: Data (X)
\node[obs] (x1) [below=of mu1] {$X_1$};
\node[obs] (x2) [below=of mu2] {$X_2$};
\node[obs] (xn) [below=of mun] {$X_n$};

% Edges
\draw[-latex, thick] (theta) -- (mu1);
\draw[-latex, thick] (theta) -- (mu2);
\draw[-latex, thick] (theta) -- (mun);

\draw[-latex, thick] (mu1) -- (x1);
\draw[-latex, thick] (mu2) -- (x2);
\draw[-latex, thick] (mun) -- (xn);

\end{tikzpicture}
```


### MCMC Estimation

In hierarchical models, the joint posterior distribution $p(\boldsymbol{\mu}, \theta | X)$ often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use **Markov Chain Monte Carlo (MCMC)** to draw sequence of samples $\{\boldsymbol{\mu}^{(t)}, \theta^{(t)}\}$ that converge to the target posterior distribution.

#### Gibbs Sampling Algorithm
::: {#alg-gibbs-sampling}

Gibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the **full conditional distributions**. To sample from a target distribution $p(\theta_1, \theta_2, \dots, \theta_k)$, the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:

$$
\begin{aligned}
\theta_1^{(t+1)} &\sim p(\theta_1 | \theta_2^{(t)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
\theta_2^{(t+1)} &\sim p(\theta_2 | \theta_1^{(t+1)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
&\vdots \\
\theta_k^{(t+1)} &\sim p(\theta_k | \theta_1^{(t+1)}, \theta_2^{(t+1)}, \dots, \theta_{k-1}^{(t+1)})
\end{aligned}
$$
:::



::: {#exm-hierarchical-gibbs}
#### Gibbs Sampling for Groups of Normal Data
**The Model**

To apply the general Gibbs sampling framework $\theta_1, \theta_2, \dots, \theta_k$ to our specific hierarchical model, we identify the variables as follows:

* **Data Observations ($X_i$):** These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school $i$). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.

* **Group-Level Parameters ($\theta_1 = \mu_i$):** These represent the latent means for each specific group or cluster. In the update step, $\mu_i$ acts as the first block of variables. It is updated by "compromising" between the local data $X_i$ and the global characteristic $\theta$.

* **Global Hyperparameter ($\theta_2 = \theta$):** This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all $\mu_i$ values, effectively "pooling" information from all groups to estimate the overall population center.

**Gibbs Update in Hierarchical Models**

In the hierarchical tree structure provided earlier, let our parameter vector be $(\mu_i, \theta)$. The "orthogonality" of the updates becomes clear when we derive the full conditionals for a Gaussian case:

* **Case $\theta_1 = \mu_i$:** Sample $\mu_i^{(t+1)}$ from $p(\mu_i | X_i, \theta^{(t)})$. This is a normal distribution with:
$$
\mu_i^{(t+1)} \sim N\left( \frac{\tau^2 X_i + \sigma^2 \theta^{(t)}}{\sigma^2 + \tau^2}, \frac{\sigma^2 \tau^2}{\sigma^2 + \tau^2} \right)
$$

* **Case $\theta_2 = \theta$:** Sample $\theta^{(t+1)}$ from $p(\theta | \boldsymbol{\mu}^{(t+1)})$. Assuming a flat prior $\pi(\theta) \propto 1$:
$$
\theta^{(t+1)} \sim N\left( \frac{1}{n} \sum_{i=1}^n \mu_i^{(t+1)}, \frac{\tau^2}{n} \right)
$$
:::


**Visual Characteristic:** Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.


#### Metropolis-hastings (MH) Sampling

When the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step $t$:

* **Propose:** Draw a candidate state $\theta^*$ from a proposal distribution $q(\theta^* | \theta^{(t)})$.
* **Accept/Reject:** Calculate the acceptance probability:
$$
\alpha = \min \left( 1, \frac{p(\theta^* | X) q(\theta^{(t)} | \theta^*)}{p(\theta^{(t)} | X) q(\theta^* | \theta^{(t)})} \right)
$$
* Set $\theta^{(t+1)} = \theta^*$ with probability $\alpha$; otherwise, set $\theta^{(t+1)} = \theta^{(t)}$.

**Visual Characteristic:** MH sampling moves in arbitrary directions and can "stay put" if a proposal is rejected, exploring the space via a random walk.

```{r}
#| label: fig-mcmc-comparison
#| fig-cap: "Comparison of Sampling Paths"
#| echo: false
#| fig-height: 5
#| fig-width: 10

set.seed(123)
rho <- 0.8
log_target <- function(x, y) { -0.5 * (x^2 - 2*rho*x*y + y^2) / (1 - rho^2) }

# Gibbs Path (step-wise Update)
gx <- -2; gy <- -2
gx_path <- gx; gy_path <- gy
for(i in 1:25) {
  gx <- rnorm(1, rho * gy, sqrt(1 - rho^2))
  gx_path <- c(gx_path, gx, gx); gy_path <- c(gy_path, gy, gy) # Horizontal move
  gy <- rnorm(1, rho * gx, sqrt(1 - rho^2))
  gx_path <- c(gx_path, gx); gy_path <- c(gy_path, gy) # Vertical move
}

# MH Path (random Walk)
mx <- numeric(50); my <- numeric(50)
mx[1] <- -2; my[1] <- -2
for(i in 2:50) {
  px <- mx[i-1] + rnorm(1, 0, 0.4); py <- my[i-1] + rnorm(1, 0, 0.4)
  acc <- exp(log_target(px, py) - log_target(mx[i-1], my[i-1]))
  if(runif(1) < acc) { mx[i] <- px; my[i] <- py } else { mx[i] <- mx[i-1]; my[i] <- my[i-1] }
}

par(mfrow = c(1, 2))
t_seq <- seq(-3, 3, length=50); z <- outer(t_seq, t_seq, function(x,y) exp(log_target(x,y)))
plot(gx_path, gy_path, type="l", col="blue", main="Gibbs (Orthogonal Steps)", xlab=expression(theta[1]), ylab=expression(theta[2]))
contour(t_seq, t_seq, z, add=TRUE, col="gray")
plot(mx, my, type="l", col="red", main="Metropolis-Hastings (Random Walk)", xlab=expression(theta[1]), ylab=expression(theta[2]))
contour(t_seq, t_seq, z, add=TRUE, col="gray")
```



## Case Study: 1998 Major League Baseball Home Run Race

In 1998, the baseball world was captivated by Mark McGwire and Sammy Sosa as they chased Roger Maris' 1961 record of 61 home runs in a single season. While McGwire and Sosa finished with 70 and 66 home runs respectively, we consider whether such performance could have been predicted using pre-season exhibition data.

For a set of $i = 1, \dots, 17$ players (including McGwire and Sosa), we observe their batting records in pre-season exhibition matches. Our goal is to estimate each player's home run "strike rate" for the competitive season.

### Transforming Data

We utilize the pre-season home runs ($y_i$) and at-bats ($n_i$) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance $\sigma^2 = 1$.

$$
x_i = \sqrt{n_i} \arcsin\left( 2 \frac{y_i}{n_i} - 1 \right)
$$

The goal is to estimate the latent parameter $\mu_i$ for each player and compare it to the "true" regular season performance.

### True Season Parameter ($\mu_i$ or $p_i^{season}$)

To validate our estimates, we define the "true" parameter value $\mu_i$ using the player's performance over the full competitive season. Let $Y_i$ be the total home runs and $N_i$ be the total at-bats in the regular season. The true transformed rate is calculated as:

$$
\mu_i^{\text{season}} = \sqrt{n_i} \arcsin\left( 2 \frac{Y_i}{N_i} - 1 \right)
$$

Note that while we use the season-long probability ($Y_i/N_i$), we scale it by the pre-season sample size ($\sqrt{n_i}$). This ensures that $\mu_i^{\text{season}}$ is on the same scale as our observations $x_i$, allowing for direct comparison of the estimation error.

```{r}
#| label: setup-data
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(brms)
library(dplyr)
library(tidyr)

# 1. Input Raw Data
ni <- c(58, 59, 74, 84, 69, 63, 60, 54, 53, 60, 66, 66, 72, 64, 42, 38, 58)
yi <- c(7, 9, 4, 7, 3, 6, 2, 10, 2, 2, 4, 3, 2, 5, 3, 2, 6)
Ni <- c(509, 643, 633, 645, 606, 555, 619, 609, 552, 540, 561, 440, 585, 531, 454, 504, 244)
Yi <- c(70, 66, 56, 46, 45, 44, 43, 40, 37, 34, 32, 30, 29, 28, 23, 21, 15)

# 2. Calculate Derived Values
p_pre   <- yi / ni                        # Pre-season Probability
x       <- sqrt(ni) * asin(2 * p_pre - 1) # Transformed Pre-season (x_i)

p_season <- Yi / Ni                       # Season Probability
true_mu  <- sqrt(ni) * asin(2 * p_season - 1) # Transformed Season (mu_i)

# 3. Create Main Data Frame
baseball_data <- data.frame(
  Player = 1:17,
  Pre_HR = yi,
  Pre_AtBats = ni,
  p_pre = round(p_pre, 3),
  x = x,
  sei = 1, # Known standard error for transformed data
  Season_HR = Yi,
  Season_AtBats = Ni,
  p_season = p_season,
  true_mu = true_mu
)

# 4. Display the Data
knitr::kable(baseball_data, 
             col.names = c("Player", "$y_i$", "$n_i$", "$p_i^{\\text{pre}}$", "$x_i$", "SE", 
                           "$Y_i$", "$N_i$", "$p_i^{\\text{seas}}$", "$\\mu_i$"),
             align = "c",
             digits = 3,
             caption = "1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data")
```

In this analysis, we model the home run strike rates of 17 Major League Baseball players using pre-season exhibition data from 1998. We apply five statistical methods ranging from simple independent estimation to advanced Bayesian decision theory.

### Methods for Estimating $\mu_i$ (transformed Scale)

#### Method 1: Simple Estimation (MLE)

The Maximum Likelihood Estimator (MLE) assumes each player's performance is independent. It relies solely on the observed pre-season data.

$$ \hat{\mu}_i^{MLE} = X_i $$

```{r}
#| label: method-mle
#| echo: false

# Simple Estimate Is Just the Data Itself
mu_mle <- baseball_data$x

# MSE Calculation (transformed Scale)
mse_mle <- mean((mu_mle - baseball_data$true_mu)^2)
```

#### Method 2: Empirical Bayes (james-stein)

The James-Stein estimator introduces a global mean $\bar{X}$ and shrinks individual estimates toward it. This assumes the players come from a common population distribution.

$$ \hat{\mu}_i^{JS} = \bar{X} + \left( 1 - \frac{k-3}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X}) $$

where $k=17$ is the number of players.

```{r}
#| label: method-js
#| echo: false

theta_hat <- mean(baseball_data$x)
S <- sum((baseball_data$x - theta_hat)^2)
shrinkage_factor <- 1 - (14 / S)

mu_js <- theta_hat + shrinkage_factor * (baseball_data$x - theta_hat)

# MSE Calculation (transformed Scale)
mse_js <- mean((mu_js - baseball_data$true_mu)^2)
```

#### Method 3: Fully Bayesian MCMC (brms)

We use a hierarchical Bayesian model where parameters are treated as random variables. We implement this using `brms`.

$$
\begin{aligned}
X_i &\sim N(\mu_i, 1) \\
\mu_i &\sim N(\theta, \tau^2) \\
\theta &\sim N(0, 10) \\
\tau &\sim \text{Cauchy}(0, 2)
\end{aligned}
$$

```{r}
#| label: method-brms
#| echo: false
#| results: hide
#| message: false
#| warning: false

# Fit Random Intercept Model: X | Se(1) ~ 1 + (1|player)
fit_brms <- brm(
  formula = x | se(sei, sigma = TRUE) ~ 1 + (1 | Player),
  data = baseball_data,
  prior = c(
    prior(normal(0, 10), class = "Intercept"),
    prior(cauchy(0, 2), class = "sd")
  ),
  chains = 2, iter = 4000, warmup = 1000, seed = 123,
  refresh = 0
)

# Extract Point Estimates (posterior Means)
post_means <- fitted(fit_brms)[, "Estimate"]
mu_brms <- post_means

# MSE Calculation (transformed Scale)
mse_brms <- mean((mu_brms - baseball_data$true_mu)^2)
```

### Comparison of Estimates of $\mu_i$

**Full Comparison of Estimates (Transformed Scale)**

The following table presents the transformed data ($x_i$) and the true season parameter ($\mu_i$) alongside the estimates from the three methods. The rows are sorted by $x_i$ to visualize how the shrinkage methods (James-Stein and Bayesian) pull the estimates away from the extremes and toward the population mean compared to the raw MLE.

```{r}
#| label: tbl-estimates-sorted
#| echo: false

# 1. Compile All Estimates into a Single Data Frame
df_estimates <- data.frame(
  Player = 1:17,
  ni = baseball_data$Pre_AtBats,       
  x_i = baseball_data$x,               # MLE Estimate (Raw Data)
  mu_js = mu_js,                       # James-Stein Estimate
  mu_bayes = mu_brms,                  # Fully Bayesian Estimate
  mu_true = baseball_data$true_mu      # True Season Parameter
)

# 2. Sort by X_i (ascending)
df_sorted <- df_estimates[order(df_estimates$x_i), ]

# 3. Display the Table
df_display_mu <- df_sorted
df_display_mu[, 3:6] <- round(df_display_mu[, 3:6], 3)

knitr::kable(df_display_mu[, c("Player", "x_i", "mu_js", "mu_bayes", "mu_true")],
             row.names = FALSE,
             col.names = c("Player", "$x_i$ (MLE)", "$\\hat{\\mu}_{JS}$", 
                           "$\\hat{\\mu}_{Bayes}$", "$\\mu_{true}$"),
             align = "c",
             caption = "Comparison of Estimates (Sorted by Pre-season $x_i$)")
```

**Plots of Errors (Sorted by $x_i$)**

This plot displays the Squared Error for each player. The x-axis represents the players sorted from lowest pre-season performance to highest.

```{r}
#| label: fig-error-index-sorted
#| fig-cap: "Squared Error by Sorted Player Index (Transformed Scale)"
#| echo: false
#| fig-height: 5

# Calculate Squared Errors Using the SORTED Dataframe
err_mle  <- (df_sorted$x_i - df_sorted$mu_true)^2
err_js   <- (df_sorted$mu_js - df_sorted$mu_true)^2
err_brms <- (df_sorted$mu_bayes - df_sorted$mu_true)^2

# Determine Y-axis Range
y_max <- max(c(err_mle, err_js, err_brms))

# Plot MLE Errors (baseline)
plot(1:17, err_mle, type = "b", pch = 1, col = "black", lty = 2,
     xlab = "Player Index (Sorted by Pre-season Performance)", 
     ylab = expression(Squared~Error~~(hat(mu) - mu[true])^2),
     main = "Estimation Error Comparison (Sorted)",
     ylim = c(0, y_max))

# Add James-stein Errors
lines(1:17, err_js, type = "b", pch = 19, col = "blue")

# Add Bayesian (brms) Errors
lines(1:17, err_brms, type = "b", pch = 17, col = "red")

# Add Grid and Legend
grid()
legend("topleft", 
       title = "Mean Squared Error",
       legend = c(paste0("MLE: ", round(mse_mle, 3)), 
                  paste0("JS: ", round(mse_js, 3)), 
                  paste0("Bayes: ", round(mse_brms, 3))),
       col = c("black", "blue", "red"), 
       pch = c(1, 19, 17), 
       lty = c(2, 1, 1))
```

### Methods for Estimating $p_i$ Directly

#### Method 1-3: Converting $\hat \mu_i$ Back to $p_i$

The first three methods (MLE, James-Stein, and Normal-Normal Bayes) estimated the parameter $\mu_i$ on the transformed scale. To obtain the probability estimates $\hat{p}_i$, we apply the inverse of the variance-stabilizing transformation:

$$ \hat{p}_i = \frac{1}{2} \left( \sin\left( \frac{\hat{\mu}_i}{\sqrt{n_i}} \right) + 1 \right) $$

where $\hat{\mu}_i$ corresponds to the estimate derived from Method 1, 2, or 3, and $n_i$ is the number of pre-season at-bats for player $i$.

#### Method 4: Hierarchical Logistic Regression (logit-normal)

In this fourth method, we model the probability $p_i$ directly using a hierarchical structure on the log-odds scale, rather than transforming the data.

We assume the count $y_i$ follows a Binomial distribution. The log-odds (logit) of the success rate $p_i$ are drawn from a common Normal distribution with unknown mean $\mu_0$ and standard deviation $\tau_0$.

$$
\begin{aligned}
y_i | p_i &\sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) &\sim N(\mu_0, \tau_0^2) \\
\mu_0 &\sim N(0, 10) \\
\tau_0 &\sim \text{Cauchy}(0, 2)
\end{aligned}
$$

We implement this in `brms` using the `binomial` family with a logit link. The individual point estimate $\hat{p}_i$ is the **posterior mean** of $p_i$. Note that because the inverse-logit function is non-linear, the posterior mean of $p_i$ is not simply the inverse-logit of the posterior mean of the random effect; `brms` handles this integration automatically via the `fitted()` function.

```{r}
#| label: method-logit-normal
#| echo: false
#| results: hide
#| message: false
#| warning: false

# 1. Fit Hierarchical Logistic Regression
# Formula: Y | Trials(n) ~ 1 + (1 | Player)
# This Estimates a Global Intercept (mu_0) and Random Intercepts for Each Player (logit(p_i))
fit_logit <- brm(
  formula = Pre_HR | trials(Pre_AtBats) ~ 1 + (1 | Player),
  data = baseball_data,
  family = binomial(link = "logit"),
  prior = c(
    prior(normal(0, 5), class = "Intercept"),
    prior(cauchy(0, 2), class = "sd")
  ),
  chains = 2, iter = 4000, warmup = 1000, seed = 123,
  refresh = 0
)

# 2. Extract Posterior Means of P_i
# Fitted() Returns the Posterior Expectations of the Response (expected Count).
fitted_counts <- fitted(fit_logit) 
p_hat_logit <- fitted_counts[, "Estimate"] / baseball_data$Pre_AtBats
```

#### Method 5: Optimal Bayes Estimator (weighted Median)

While the posterior mean (Method 4) minimizes the Mean Squared Error (MSE), it is not necessarily optimal for the **Relative Standardized Error** metric we defined earlier:
$$L(p, \hat{p}) = \frac{|p - \hat{p}|}{\min(p, 1-p)}$$

This is a form of weighted absolute error loss, where the weight is $w(p) = \frac{1}{\min(p, 1-p)}$. Theoretical derivation shows that the estimator minimizing the expected posterior loss for this function is the **Weighted Posterior Median**.

We compute this by extracting the full posterior samples from the Logit-Normal model (Method 4) and calculating the weighted median for each player.

```{r}
#| label: method-optimal
#| echo: false

# 1. Extract Posterior Samples (n_samples X 17 Players)
# Posterior_epred Gives Samples of the Expected Count (N * P)
post_counts <- posterior_epred(fit_logit) 

# Convert to Probability Scale by Dividing by Trials
p_samples <- sweep(post_counts, 2, baseball_data$Pre_AtBats, "/")

# 2. Define Function for Weighted Median
# Finds the Value 'q' Such That Sum(weights Where X <= Q) >= 0.5 * Total_weight
get_weighted_median <- function(samples) {
  # Calculate weights based on the loss function denominator
  # Avoid division by exact zero (unlikely but safer)
  denom <- pmin(samples, 1 - samples)
  denom[denom < 1e-6] <- 1e-6 
  weights <- 1 / denom
  
  # Normalize weights
  weights_norm <- weights / sum(weights)
  
  # Sort samples and weights
  ord <- order(samples)
  samp_sorted <- samples[ord]
  w_sorted <- weights_norm[ord]
  
  # Find cutoff
  cum_w <- cumsum(w_sorted)
  idx <- which(cum_w >= 0.5)[1]
  
  return(samp_sorted[idx])
}

# 3. Apply to All Players
p_hat_optimal <- apply(p_samples, 2, get_weighted_median)
```

#### Comparison of All Five Estimates (probability Scale)

We now compare all five methods: MLE, James-Stein (transformed), Bayes Normal-Normal (transformed), Hierarchical Logit-Normal (Posterior Mean), and Optimal Bayes (Weighted Median).

**1. MSE Comparison**

```{r}
#| label: compare-five-methods
#| echo: false
#| warning: false
#| message: false

# 1. Prepare Estimates from Previous Steps
inv_trans <- function(mu, n) { 0.5 * (sin(mu / sqrt(n)) + 1) }

# Convert Transformed Estimates Back to Probability Scale
p_mle    <- inv_trans(baseball_data$x, baseball_data$Pre_AtBats)
p_js     <- inv_trans(mu_js, baseball_data$Pre_AtBats)
p_normal <- inv_trans(mu_brms, baseball_data$Pre_AtBats) 
# P_hat_logit (method 4) and P_hat_optimal (method 5) Are Already Calculated

# 2. Combine into Dataframe
df_compare <- data.frame(
  Player = baseball_data$Player,
  x_i    = baseball_data$x, # For sorting
  p_true = baseball_data$p_season,
  p_mle  = p_mle,
  p_js   = p_js,
  p_norm = p_normal,
  p_logit = p_hat_logit,
  p_opt   = p_hat_optimal
)

# Sort by Initial Performance
df_compare_sorted <- df_compare[order(df_compare$x_i), ]

# 3. Calculate MSE
mse_p_mle   <- mean((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)
mse_p_js    <- mean((df_compare_sorted$p_js - df_compare_sorted$p_true)^2)
mse_p_norm  <- mean((df_compare_sorted$p_norm - df_compare_sorted$p_true)^2)
mse_p_logit <- mean((df_compare_sorted$p_logit - df_compare_sorted$p_true)^2)
mse_p_opt   <- mean((df_compare_sorted$p_opt - df_compare_sorted$p_true)^2)

# 4. Plot MSE
y_max <- max((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)

plot(1:17, (df_compare_sorted$p_mle - df_compare_sorted$p_true)^2, 
     type = "b", pch = 1, col = "black", lty = 2,
     xlab = "Player Index (Sorted by Pre-season)",
     ylab = "Squared Error",
     main = "Squared Error by Method",
     ylim = c(0, y_max))

lines(1:17, (df_compare_sorted$p_js - df_compare_sorted$p_true)^2, type = "b", pch = 19, col = "blue")
lines(1:17, (df_compare_sorted$p_norm - df_compare_sorted$p_true)^2, type = "b", pch = 17, col = "red")
lines(1:17, (df_compare_sorted$p_logit - df_compare_sorted$p_true)^2, type = "b", pch = 15, col = "darkgreen")
lines(1:17, (df_compare_sorted$p_opt - df_compare_sorted$p_true)^2, type = "b", pch = 18, col = "purple")

grid()
legend("topleft",
       legend = c(paste0("MLE [MSE: ", round(mse_p_mle, 4), "]"),
                  paste0("JS [MSE: ", round(mse_p_js, 4), "]"),
                  paste0("Normal-Bayes [MSE: ", round(mse_p_norm, 4), "]"),
                  paste0("Logit-Normal [MSE: ", round(mse_p_logit, 4), "]"),
                  paste0("Optimal-Bayes [MSE: ", round(mse_p_opt, 4), "]")),
       col = c("black", "blue", "red", "darkgreen", "purple"),
       pch = c(1, 19, 17, 15, 18),
       lty = c(2, 1, 1, 1, 1),
       cex = 0.75,
       bg = "white")
```

**2. Relative Standardized Error**

We also evaluate the methods using the relative error metric that penalizes deviations based on the rarity of the event:
$$ \text{Metric}_i = \frac{|p_i^{\text{true}} - \hat{p}_i|}{\min(p_i^{\text{true}}, 1 - p_i^{\text{true}})} $$

```{r}
#| label: fig-error-relative-final
#| fig-cap: "Relative Error Assessment: Five Methods"
#| echo: false
#| fig-height: 6

# 1. Define Metric
calc_metric <- function(p_hat, p_true) {
  denom <- pmin(p_true, 1 - p_true)
  abs(p_hat - p_true) / denom
}

# 2. Calculate Metric
rel_mle   <- calc_metric(df_compare_sorted$p_mle, df_compare_sorted$p_true)
rel_js    <- calc_metric(df_compare_sorted$p_js, df_compare_sorted$p_true)
rel_norm  <- calc_metric(df_compare_sorted$p_norm, df_compare_sorted$p_true)
rel_logit <- calc_metric(df_compare_sorted$p_logit, df_compare_sorted$p_true)
rel_opt   <- calc_metric(df_compare_sorted$p_opt, df_compare_sorted$p_true)

# 3. Sum of Errors
sum_rel_mle   <- sum(rel_mle)
sum_rel_js    <- sum(rel_js)
sum_rel_norm  <- sum(rel_norm)
sum_rel_logit <- sum(rel_logit)
sum_rel_opt   <- sum(rel_opt)

# 4. Plot
y_max_rel <- max(c(rel_mle, rel_js, rel_norm, rel_logit, rel_opt)) * 1.1

plot(1:17, rel_mle, type = "b", pch = 1, col = "black", lty = 2,
     xlab = "Player Index (Sorted by Pre-season)", 
     ylab = "Relative Standardized Error",
     main = "Assessment of Estimation Methods",
     ylim = c(0, y_max_rel))

lines(1:17, rel_js, type = "b", pch = 19, col = "blue")
lines(1:17, rel_norm, type = "b", pch = 17, col = "red")
lines(1:17, rel_logit, type = "b", pch = 15, col = "darkgreen")
lines(1:17, rel_opt, type = "b", pch = 18, col = "purple")

grid()

legend("topleft", 
       title = "Method [Sum Relative Error]",
       legend = c(paste0("MLE [", round(sum_rel_mle, 3), "]"), 
                  paste0("James-Stein [", round(sum_rel_js, 3), "]"), 
                  paste0("Normal-Bayes [", round(sum_rel_norm, 3), "]"),
                  paste0("Logit-Normal [", round(sum_rel_logit, 3), "]"),
                  paste0("Optimal-Bayes [", round(sum_rel_opt, 3), "]")),
       col = c("black", "blue", "red", "darkgreen", "purple"), 
       pch = c(1, 19, 17, 15, 18), 
       lty = c(2, 1, 1, 1, 1),
       cex = 0.75, 
       bg = "white")
```

## Bayesian Predictive Distributions



A key feature of Bayesian analysis is the ability to make inference about future observations, rather than just the model parameters. The **posterior predictive distribution** describes the probability of observing a new data point $y^*$ given the observed data $y$.


::: {#def-posterior-predictive}
### Posterior Predictive Distribution
Let $f(y^*|\theta)$ be the sampling distribution of a future observation $y^*$ given parameter $\theta$, and let $\pi(\theta|y)$ be the posterior distribution of $\theta$ given observed data $y$. The posterior predictive density is obtained by marginalizing over the parameter $\theta$:

$$
f(y^*|y) = \int_\Theta f(y^*|\theta) \pi(\theta|y) \, d\theta
$$
:::


This distribution incorporates two distinct sources of uncertainty:

* **Sampling Uncertainty (Aleatoric):** The inherent variability of the data generation process, represented by the variance in $f(y^*|\theta)$.
* **Parameter Uncertainty (Epistemic):** The uncertainty regarding the true value of $\theta$, represented by the variance in the posterior $\pi(\theta|y)$.

As sample size $n \to \infty$, the parameter uncertainty vanishes (the posterior approaches a point mass), and the predictive distribution converges to the true data-generating distribution.


::: {#exm-predictive-normal}
### Normal-normal Predictive Distribution
Consider a case where the data $y_1, \dots, y_n$ are independent and normally distributed with unknown mean $\mu$ and known variance $\sigma^2$:

$$
Y_i | \mu \sim N(\mu, \sigma^2)
$$

Assume a conjugate prior for the mean: $\mu \sim N(\mu_0, \sigma_0^2)$. The posterior distribution is $\mu|y \sim N(\mu_n, \sigma_n^2)$, where $\mu_n$ and $\sigma_n^2$ are the updated posterior hyperparameters.

The predictive distribution for a new observation $y^*$ is derived as:

$$
\begin{aligned}
f(y^*|y) &= \int_{-\infty}^{\infty} f(y^*|\mu) \pi(\mu|y) \, d\mu \\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y^*-\mu)^2}{2\sigma^2}} \times \frac{1}{\sqrt{2\pi\sigma_n^2}} e^{-\frac{(\mu-\mu_n)^2}{2\sigma_n^2}} \, d\mu
\end{aligned}
$$

This convolution of two Gaussians results in a new Gaussian distribution:

$$
y^* | y \sim N(\mu_n, \sigma^2 + \sigma_n^2)
$$

Here, the total predictive variance is the sum of the data variance ($\sigma^2$) and the posterior uncertainty about the mean ($\sigma_n^2$).
:::

