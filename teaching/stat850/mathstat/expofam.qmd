---
title: "Exponential Families"
engine: knitr
format: 
  pdf: default
---


## Exponential Families

::: {#def-exponential-family}

### Exponential Family
A family of probability density functions (or probability mass functions) $f(x|\theta)$ is said to be an **Exponential Family** if it can be written in the form:

$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

where:

* $\theta = (\theta_1, \dots, \theta_d)$ is the parameter vector.

* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.

* By the Factorization Theorem, the vector $T(x) = (\tau_1(x), \dots, \tau_k(x))$ constitutes a **sufficient statistic** for $\theta$.

:::

### Examples of Exponential Families

::: {#exm-exponential-dist}

### Exponential Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\theta} e^{-x_i/\theta} = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

Here we identify:

* $C(\theta) = \theta^{-n}$

* $h(\mathbf{x}) = 1$

* $\pi_1(\theta) = -\frac{1}{\theta}$

* $\tau_1(\mathbf{x}) = \sum_{i=1}^n x_i$.

:::

::: {#exm-gamma-dist}

### Gamma Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\Gamma(\alpha)\beta^\alpha} x_i^{\alpha-1} e^{-x_i/\beta}
$$

$$
= [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Rewriting in the canonical form:

$$
= [\Gamma(\alpha)]^{-n} \beta^{-n\alpha} \exp\left\{ (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Here:

* $\pi_1(\theta) = \alpha - 1$, $\tau_1(\mathbf{x}) = \sum \log x_i$

* $\pi_2(\theta) = -\frac{1}{\beta}$, $\tau_2(\mathbf{x}) = \sum x_i$.

:::

::: {#exm-beta-dist}

### Beta Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\theta = (a, b)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}
$$

$$
= [B(a, b)]^{-n} \exp\left\{ (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) \right\}
$$

This is an exponential family with $k=2$.

:::

::: {#exm-normal-dist}

### Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$ with $\theta = (\mu, \sigma^2)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{(x_i - \mu)^2}{2\sigma^2} \right\}
$$

$$
= (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{n\mu^2}{2\sigma^2} \right\}
$$

$$
= \left[ (2\pi)^{-n/2} (\sigma^2)^{-n/2} e^{-\frac{n\mu^2}{2\sigma^2}} \right] \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\}
$$

Here $d=2$ and $k=2$.

:::

### Examples of Non-exponential Families

A model is **not** in the exponential family if the support depends on the parameter.

::: {#exm-uniform-dist}

### Uniform Distribution
Let $X \sim U(0, \theta)$.
$$
f(x|\theta) = \frac{1}{\theta} I(0 < x < \theta)
$$

This cannot be written in the required form because the indicator function $I(0 < x < \theta)$ cannot be factorized into separate functions of $x$ and $\theta$ inside an exponential.

:::

::: {#exm-cauchy-dist}

### Cauchy Distribution
Let $X \sim \text{Cauchy}(\theta)$.
$$
f(x|\theta) = \frac{1}{\pi [1 + (x-\theta)^2]}
$$

This involves $\log(1 + (x-\theta)^2)$ in the exponent, which cannot be separated into sums of products $\pi_i(\theta)\tau_i(x)$.

:::

## Regular Families

In the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.

::: {#def-regular-family}

### Regular Family
A family of probability density functions $f(x|\theta)$ is said to be a **Regular Family** (or "sufficiently well-behaved") if the domain of $x$ for which $f(x|\theta) > 0$ (the support) does not depend on the parameter $\theta$.

This condition is necessary to satisfy the identity that allows differentiation under the integral sign:

$$
\frac{\partial}{\partial \theta} \int f(x|\theta) dx = \int \frac{\partial}{\partial \theta} f(x|\theta) dx
$$

:::

### Why Regularity Matters {.unnumbered}

If a family is regular, we can differentiate the identity $\int f(x|\theta) dx = 1$ with respect to $\theta$. This operation yields the fundamental properties of the **Score Function** $\frac{\partial}{\partial \theta} \log f(X|\theta)$:

1.  **First Moment Identity:** The expected value of the score function is zero.

$$E_\theta \left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right] = 0$$

2.  **Second Moment Identity:** The Fisher Information (variance of the score) is equal to the negative expected Hessian.

$$E_\theta \left[ \frac{\partial^2 \log f(X|\theta)}{\partial \theta_j \partial \theta_l} \right] = -E_\theta \left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \frac{\partial \log f(X|\theta)}{\partial \theta_l} \right]$$

### An Example of Non-regular Distribution  {.unnumbered}
The **Uniform Distribution** $U(0, \theta)$ is a classic example of a **non-regular** family. Because the support $(0, \theta)$ depends on $\theta$, the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.

## Moments of Sufficient Statistics of Exponential Families

### Means of Sufficient Statistics (General Case)

::: {#thm-moments-exp-family}

### Means of Sufficient Statistics (General Case)
For a random variable $X$ belonging to an exponential family with density $f(x|\theta) = C(\theta) h(x) \exp\{\sum_{i=1}^k \pi_i(\theta) \tau_i(x)\}$, the moments of the sufficient statistics $\tau_i(X)$ satisfy the system of equations:
$$
\frac{1}{C(\theta)} \frac{\partial C(\theta)}{\partial \theta_j} + \sum_{i=1}^k \frac{\partial \pi_i(\theta)}{\partial \theta_j} E[\tau_i(X)] = 0 \quad \text{for } j=1, \dots, d
$$

:::

::: {.proof}
For regular families, we can interchange differentiation and integration. Since $\int f(x|\theta) dx = 1$, we have:
$$
\frac{\partial}{\partial \theta} \int f(x|\theta) dx = 0 \implies \int \frac{\partial}{\partial \theta} f(x|\theta) dx = 0
$$

Using the identity $\frac{\partial f}{\partial \theta} = f(x|\theta) \frac{\partial \log f}{\partial \theta}$, we derive the fundamental moment property:

$$
E\left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right] = \int \frac{\partial \log f(x|\theta)}{\partial \theta_j} f(x|\theta) dx = 0
$$

For the exponential family, the log-likelihood is given by:

$$
\log f(x|\theta) = \log C(\theta) + \log h(x) + \sum_{i=1}^k \pi_i(\theta) \tau_i(x)
$$

Taking the derivative with respect to $\theta_j$:

$$
\frac{\partial \log f(x|\theta)}{\partial \theta_j} = \frac{1}{C(\theta)} \frac{\partial C(\theta)}{\partial \theta_j} + \sum_{i=1}^k \frac{\partial \pi_i(\theta)}{\partial \theta_j} \tau_i(x)
$$

Taking expectations and applying the condition $E[\frac{\partial}{\partial \theta} \log f(X|\theta)] = 0$ yields the theorem statement.

:::

::: {#exm-normal-moments}

### Moments of Normal Sufficient Statistics
Consider the Normal distribution $N(\mu, \sigma^2)$ where $\theta = (\mu, \sigma^2)$. The density is:
$$
f(\mathbf{x}|\theta) = (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{n\mu^2}{2\sigma^2} \right\} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\}
$$

Here we identify the components :

* $C(\mu, \sigma^2) = (2\pi)^{-n/2} (\sigma^2)^{-n/2} \exp\left(-\frac{n\mu^2}{2\sigma^2}\right)$

* $\pi_1 = -\frac{1}{2\sigma^2}, \quad \tau_1(\mathbf{x}) = \sum x_i^2$

* $\pi_2 = \frac{\mu}{\sigma^2}, \quad \tau_2(\mathbf{x}) = \sum x_i$

We apply the theorem by differentiating with respect to $\mu$ and $\sigma^2$.

1. Differentiate with respect to $\mu$:

   $$
      \frac{\partial \log C}{\partial \mu} = -\frac{n\mu}{\sigma^2}
   $$

      

   $$
      \frac{\partial \pi_1}{\partial \mu} = 0, \quad \frac{\partial \pi_2}{\partial \mu} = \frac{1}{\sigma^2}
   $$

      The theorem equation becomes:

   $$
      -\frac{n\mu}{\sigma^2} + 0 \cdot E[\sum X_i^2] + \frac{1}{\sigma^2} E[\sum X_i] = 0
   $$

      


   $$
      \implies E[\sum_{i=1}^n X_i] = n\mu
   $$

1. Differentiate with respect to $\sigma^2$:

   $$
      \frac{\partial \log C}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{n\mu^2}{2(\sigma^2)^2}
   $$

      

   

   $$
      \frac{\partial \pi_1}{\partial \sigma^2} = \frac{1}{2(\sigma^2)^2}, \quad \frac{\partial \pi_2}{\partial \sigma^2} = -\frac{\mu}{(\sigma^2)^2}
   $$

      The theorem equation becomes:

   $$
      \left( -\frac{n}{2\sigma^2} + \frac{n\mu^2}{2(\sigma^2)^2} \right) + \frac{1}{2(\sigma^2)^2} E[\sum X_i^2] - \frac{\mu}{(\sigma^2)^2} E[\sum X_i] = 0
   $$

      Multiplying by $2(\sigma^2)^2$:

   $$
      -n\sigma^2 + n\mu^2 + E[\sum X_i^2] - 2\mu(n\mu) = 0
   $$

      


   $$
      E[\sum X_i^2] = n\sigma^2 + n\mu^2
   $$

   This recovers the standard second moment $E[X^2] = \sigma^2 + \mu^2$.

:::

::: {#exm-gamma-moments}

### Moments of Gamma Sufficient Statistics
Consider the Gamma distribution $\text{Gamma}(\alpha, \beta)$ with $\theta = (\alpha, \beta)$. The density is :
$$
f(\mathbf{x}|\theta) = [\Gamma(\alpha)]^{-n} \beta^{-n\alpha} \exp\left\{ (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Here we identify:

* $\log C(\alpha, \beta) = -n \log \Gamma(\alpha) - n\alpha \log \beta$

* $\pi_1 = \alpha - 1, \quad \tau_1(\mathbf{x}) = \sum \log x_i$

* $\pi_2 = -\frac{1}{\beta}, \quad \tau_2(\mathbf{x}) = \sum x_i$

1. Differentiate with respect to $\beta$:

   $$
      \frac{\partial \log C}{\partial \beta} = -\frac{n\alpha}{\beta}
   $$


   $$
      \frac{\partial \pi_1}{\partial \beta} = 0, \quad \frac{\partial \pi_2}{\partial \beta} = \frac{1}{\beta^2}
   $$

      The theorem yields:

   $$
      -\frac{n\alpha}{\beta} + \frac{1}{\beta^2} E[\sum X_i] = 0 \implies E[\sum X_i] = n\alpha\beta
   $$

1. Differentiate with respect to $\alpha$:

   $$
      \frac{\partial \log C}{\partial \alpha} = -n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)} - n \log \beta = -n \psi(\alpha) - n \log \beta
   $$

      where $\psi(\alpha)$ is the digamma function.

   $$
      \frac{\partial \pi_1}{\partial \alpha} = 1, \quad \frac{\partial \pi_2}{\partial \alpha} = 0
   $$

      The theorem yields:

   $$
      (-n \psi(\alpha) - n \log \beta) + 1 \cdot E[\sum \log X_i] = 0
   $$

   $$
   \implies E[\sum_{i=1}^n \log X_i] = n(\psi(\alpha) + \log \beta)
   $$

:::


### Natural Parameterization

::: {#def-natural-parameterization}

### Natural Parameterization
Suppose an exponential family is given by:
$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

We define the **natural parameters** $\eta_i$ as $\eta_i = \pi_i(\theta)$. Let $\eta = (\eta_1, \dots, \eta_k)$. The density becomes:

$$
f(x|\eta) = C^*(\eta) h(x) \exp\left\{ \sum_{i=1}^k \eta_i \tau_i(x) \right\}
$$

:::

::: {#def-natural-space}

### Natural Parameter Space
The natural parameter space $\mathcal{H}$ is defined as:
$$
\mathcal{H} = \{ \eta = (\pi_1(\theta), \dots, \pi_k(\theta)) : \int h(x) e^{\sum \eta_i \tau_i(x)} dx < \infty \}
$$

where the condition ensures $C(\theta)$ is finite.

:::

:::{#def-curvedexpfam}

### Full vs. Curved Exponential Families

Let $d$ be the dimension of $\theta$ and $k$ be the dimension of the sufficient statistic vector $\tau(x)$.

* If $d = k$, we say $f(x|\theta)$ is a **Full Exponential Family**.

* If $d < k$, we say $f(x|\theta)$ is a **Curved Exponential Family**.

:::

::: {#exm-natural-normal}

### Natural Parameterization of Normal Distribution
Consider the full Normal family $N(\mu, \sigma^2)$ where both parameters are unknown ($d=2$). We previously identified the sufficient statistics $T_1(x) = \sum x_i$ and $T_2(x) = \sum x_i^2$ ($k=2$). Since $d=k$, this is a **Full Exponential Family**.

The natural parameters $\eta = (\eta_1, \eta_2)$ are defined by the mapping:

$$
\eta_1 = \frac{\mu}{\sigma^2}, \quad \eta_2 = -\frac{1}{2\sigma^2}
$$

The density can be rewritten purely in terms of $\eta$:

$$
f(\mathbf{x}|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n x_i + \eta_2 \sum_{i=1}^n x_i^2 - A(\eta) \right\}
$$
where the log-partition function $A(\eta)$ absorbs the normalizing constants.

:::

::: {#exm-natural-gamma}

### Natural Parameterization of Gamma Distribution
Consider the Gamma family $\text{Gamma}(\alpha, \beta)$ ($d=2$). We identified the sufficient statistics $T_1(x) = \sum \log x_i$ and $T_2(x) = \sum x_i$ ($k=2$). Since $d=k$, this is also a **Full Exponential Family**.

The natural parameters $\eta = (\eta_1, \eta_2)$ are derived from the canonical form :

$$
\eta_1 = \alpha - 1, \quad \eta_2 = -\frac{1}{\beta}
$$

The density in natural parameterization is:

$$
f(\mathbf{x}|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n \log x_i + \eta_2 \sum_{i=1}^n x_i - A(\eta) \right\}
$$

:::

::: {#exm-curved-normal-natural}

### Curved Exponential Family (Natural Parameterization)
Consider the $N(\theta, \theta^2)$ distribution ($d=1$). The density is:
$$
f(x|\theta) \propto \exp\left\{ -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i \right\}
$$

To express this in the natural parameterization, we define $\eta = (\eta_1, \eta_2)$ as:

$$
\eta_1 = -\frac{1}{2\theta^2}, \quad \eta_2 = \frac{1}{\theta}
$$

The density becomes:

$$
f(x|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n x_i^2 + \eta_2 \sum_{i=1}^n x_i - A(\eta) \right\}
$$

However, the natural parameters $\eta_1$ and $\eta_2$ are not independent. They satisfy the constraint:

$$
\eta_1 = -\frac{1}{2} \eta_2^2
$$

Because the parameter space $\mathcal{H}$ forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a **Curved Exponential Family**.

:::

### Mean and Covariance of  Natural Exponential Families


::: {#thm-natural-covariance}

### Mean and Covariance of Natural Exponential Families
If the exponential family is in its **canonical form** (natural parameterization) where $\pi_i(\theta) = \theta_i$ for each $i$, then the mean and covariance of the sufficient statistics are given by:
$$
E_\theta[\tau_i(X)] = - \frac{\partial}{\partial \theta_i} \log C(\theta)
$$

$$
\text{Cov}_\theta(\tau_i(X), \tau_j(X)) = - \frac{\partial^2}{\partial \theta_i \partial \theta_j} \log C(\theta)
$$

:::

::: {.proof}
We use the second-order regularity condition for the score function:
$$
E_\theta\left[ \frac{\partial^2 \log f(X|\theta)}{\partial \theta_i \partial \theta_j} \right] = - E_\theta\left[ \frac{\partial \log f(X|\theta)}{\partial \theta_i} \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right]
$$

In the case of a natural exponential family, $\pi_k(\theta) = \theta_k$. The derivative of the log-likelihood simplifies to:

$$
\frac{\partial \log f}{\partial \theta_i} = \frac{\partial \log C(\theta)}{\partial \theta_i} + \tau_i(x)
$$

Differentiating again with respect to $\theta_j$:

$$
\frac{\partial^2 \log f}{\partial \theta_i \partial \theta_j} = \frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j}
$$

Since this second derivative is non-random (it does not depend on $x$), its expectation is simply itself.

Now consider the right-hand side of the regularity condition. From the first moment property, we know $E[\tau_i(X)] = - \frac{\partial \log C}{\partial \theta_i}$. Thus, the score function is:

$$
\frac{\partial \log f}{\partial \theta_i} = \tau_i(X) - E[\tau_i(X)]
$$

Substituting these into the identity:

$$
\frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j} = - E\left[ (\tau_i(X) - E[\tau_i(X)]) (\tau_j(X) - E[\tau_j(X)]) \right]
$$

$$
\frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j} = - \text{Cov}_\theta(\tau_i(X), \tau_j(X))
$$

Multiplying by $-1$ gives the result.

:::



## Distributions of Sufficient Statistics of Exponential Families

::: {#lem-joint-distribution}

### Joint Distribution of Sufficient Statistics
If $X$ has a distribution in the exponential family $f(x|\theta) = C(\theta) h(x) \exp\{\sum \pi_i(\theta) \tau_i(x)\}$, then the joint distribution of the sufficient statistics $T = (\tau_1(X), \dots, \tau_k(X))$ is also in the exponential family with the same natural parameters.

:::

::: {.proof}
Let $X$ be discrete. The probability mass function of $T$ is:
$$
P(T_1 = y_1, \dots, T_k = y_k | \theta) = \sum_{\{x : \tau(x) = y\}} P(X=x|\theta)
$$

$$
= \sum_{\{x : \tau(x) = y\}} C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

Since the exponential term and $C(\theta)$ depend only on $y$ and $\theta$, they can be pulled out of the sum:

$$
= C(\theta) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\} \left( \sum_{\{x : \tau(x) = y\}} h(x) \right)
$$

Defining $h^*(y) = \sum_{\{x : \tau(x) = y\}} h(x)$, we get:

$$
f_T(y|\theta) = C(\theta) h^*(y) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

which is of the exponential family form .

:::

::: {#lem-marginal-distribution}

### Marginal Distribution Lemma for Exponetial Families (MDL)
Let $S$ be a subset of indices $\{1, \dots, k\}$. If $\pi_i(\theta)$ are constant for all $i \notin S$, then the marginal distribution of the statistics $T_S = \{ \tau_j(X) : j \in S \}$ is of the exponential family form with natural parameters $\pi_j(\theta)$ for $j \in S$.

:::

::: {.proof}
Let $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):
$$
P(T_S = y_S | \theta) = \sum_{y_{S^c}} C(\theta) h^*(y) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j + \sum_{l \in S^c} \pi_l(\theta) y_l \right\}
$$

Since $\pi_l(\theta)$ is constant for $l \in S^c$, the term $\exp\{\sum_{l \in S^c} \pi_l y_l\}$ does not depend on $\theta$. We can group it with $h^*(y)$:

$$
= C(\theta) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j \right\} \sum_{y_{S^c}} h^*(y) \exp\left\{ \sum_{l \in S^c} \pi_l y_l \right\}
$$

The sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form.

:::


::: {#exm-bernoulli-mdl-single}

### Binomial Distribution from Bernoulli Trials

Consider $n$ independent coin flips $X_1, \dots, X_n \sim \text{Bernoulli}(p)$. We wish to find the distribution of the sufficient statistic $T = \sum_{i=1}^n X_i$.

1.  **Exponential Family Form**
    The joint probability mass function for a sequence $x = (x_1, \dots, x_n)$ is:
    $$
    P(X=x) = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} = (1-p)^n \exp\left\{ \log\left(\frac{p}{1-p}\right) \sum_{i=1}^n x_i \right\}
    $$
    We identify the components:
    * **Natural Parameter:** $\eta = \log\left(\frac{p}{1-p}\right)$
    * **Statistic:** $T(x) = \sum x_i$
    * **Base Measure:** $h(x) = 1$ (Every specific binary sequence has weight 1).

2.  **Applying MDL**
    The Lemma states the marginal distribution of $T$ has the form:
    $$
    f_T(t) \propto h^*(t) \exp(\eta t)
    $$
    where $h^*(t)$ is the "volume" of the sample space mapping to $T=t$.

3.  **Calculating the Base Measure $h^*(t)$**
    Since $h(x)=1$, we simply count how many sequences $x$ satisfy $\sum x_i = t$. This is a standard combinatorial counting problem:
    $$
    h^*(t) = \binom{n}{t}
    $$

4.  **Result**
    Substituting back:
    $$
    f_T(t) \propto \binom{n}{t} \exp\left\{ t \log\left(\frac{p}{1-p}\right) \right\} = \binom{n}{t} \left(\frac{p}{1-p}\right)^t
    $$
    Multiplying by the constant $(1-p)^n$ we set aside:
    $$
    f_T(t) = \binom{n}{t} p^t (1-p)^{n-t}
    $$
    This matches the **Binomial$(n, p)$** distribution.

:::

::: {#exm-exponential-sum-mdl}

### Gamma Distribution from Exponential Sum

Consider $n$ independent variables $X_1, \dots, X_n \sim \text{Exp}(\lambda)$ with pdf $f(x) = \lambda e^{-\lambda x}$. We want to find the distribution of $T = \sum_{i=1}^n X_i$.

1.  **Exponential Family Form**
    The joint density is:
    $$
    f(x_1, \dots, x_n) = \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^n \cdot \mathbf{1}_{\{x_i > 0\}} \cdot \exp\left\{ -\lambda \sum_{i=1}^n x_i \right\}
    $$
    * **Natural Parameter:** $\eta = -\lambda$
    * **Statistic:** $T(x) = \sum x_i$
    * **Base Measure:** $h(x) = 1$ (on the positive orthant).

2.  **Applying MDL**
    The marginal density of $T$ is:
    $$
    f_T(t) \propto h^*(t) \exp(-\lambda t)
    $$
    Here, $h^*(t)$ represents the geometric surface area of the simplex defined by $x_1 + \dots + x_n = t$ with $x_i > 0$.

3.  **Calculating the Jacobian/Base Measure $h^*(t)$**
    Instead of calculating the surface area directly, it is often easier to calculate the cumulative volume $V(t)$ of the region $\sum x_i \le t$ and take the derivative.
    The volume of the standard simplex scaled by $t$ is known to be:
    $$
    V(t) = \int_{0 < \sum x_i \le t} 1 \, dx = \frac{t^n}{n!}
    $$
    The "surface area" $h^*(t)$ is the rate of change of this volume with respect to $t$:
    $$
    h^*(t) = \frac{d}{dt} V(t) = \frac{d}{dt} \left( \frac{t^n}{n!} \right) = \frac{n t^{n-1}}{n!} = \frac{t^{n-1}}{(n-1)!}
    $$

4.  **Result**
    Substituting $h^*(t)$ back into the MDL form:
    $$
    f_T(t) \propto \frac{t^{n-1}}{(n-1)!} e^{-\lambda t}
    $$
    We recognize the kernel $t^{n-1} e^{-\lambda t}$ as that of a **Gamma$(n, \lambda)$** distribution (also known as the Erlang distribution). The factor $1/(n-1)!$ matches the $\Gamma(n)$ term in the denominator.

:::



::: {#exm-bernoulli-failure}

### Failure of MDL: The Coupled Bernoulli

Suppose we try to be clever and rewrite the Bernoulli joint density treating Heads ($T_1$) and Tails ($T_2$) as distinct sufficient statistics:

$$
f(x|p) = \exp\left\{ (\log p) \sum x_i + (\log(1-p)) (n - \sum x_i) \right\}
$$

Let's define separate natural parameters to make it look like a 2-parameter exponential family:

  * $\eta_1 = \log p$
  * $\eta_2 = \log (1-p)$
  * $T_1 = \sum x_i$ (Heads)
  * $T_2 = n - \sum x_i$ (Tails)

The density looks like:
$$
f(t_1, t_2) \propto h(t_1, t_2) \exp( \eta_1 T_1 + \eta_2 T_2 )
$$

**Attempting to apply the Lemma:**
If we blindly followed the Lemma to find the marginal distribution of Heads ($T_1$), we would say: "Treat $\eta_2$ as constant, ignore the $T_2$ part, and focus on $T_1$."

$$
\text{Hypothetical Marginal}(t_1) \stackrel{?}{\propto} h^*(t_1) \exp( \eta_1 t_1 ) = \binom{n}{t_1} p^{t_1}
$$

**The Result:** $f(t_1) \propto \binom{n}{t_1} p^{t_1}$.
This is **WRONG**. It is missing the $(1-p)^{n-t_1}$ term. It suggests the probability of heads grows infinitely with $p$ without being penalized by the probability of tails shrinking.

**Why it Failed:**
The Marginal Distribution Lemma requires the natural parameters $\eta_1$ and $\eta_2$ to be **variationally independent** (the parameter space must contain a rectangle).

1.  **Parameter Constraint:** $\eta_1$ and $\eta_2$ are coupled by the constraint $e^{\eta_1} + e^{\eta_2} = 1$ (since $p + (1-p) = 1$). You cannot vary $\eta_1$ (change $p$) while holding $\eta_2$ constant.
   2.  **Statistic Constraint:** $T_1$ and $T_2$ are perfectly collinear ($T_1 + T_2 = n$). The support of $(T_1, T_2)$ is a line segment, not a 2D grid. The base measure $h(t_1, t_2)$ does not factor because $h(t_1, t_2)$ is zero everywhere except on that line.

:::

::: {#exm-normal-marginal-comparison}

### Marginal Distributions of Normal Sufficient Statistics
Consider the Normal model $N(\mu, \sigma^2)$ with sufficient statistics $T_1 = \sum X_i^2$ and $T_2 = \sum X_i$. The natural parameters are $\pi_1 = -\frac{1}{2\sigma^2}$ and $\pi_2 = \frac{\mu}{\sigma^2}$.

1. Marginal of $T_2$ (fixing $\sigma^2$)
   Suppose $\sigma^2$ is known (constant).

   * **Lemma Condition:** $\pi_1 = -1/(2\sigma^2)$ is constant. The condition holds.
   * **Implied Form by MDL:** The Marginal Distribution Lemma claims that $T_2$ follows an exponential family with natural parameter $\pi_2 = \frac{\mu}{\sigma^2}$:
   $$
   f_{T_2}(t_2|\theta) \propto h^*(t_2) \exp\left\{ \frac{\mu}{\sigma^2} t_2 \right\}
   $$

   * **Comparison with Known Distribution:**
      We know $T_2 = \sum X_i \sim N(n\mu, n\sigma^2)$. The density is:
      $$
      f(t_2|\mu) = \frac{1}{\sqrt{2\pi n \sigma^2}} \exp\left\{ -\frac{(t_2 - n\mu)^2}{2n\sigma^2} \right\}
      $$

      Expanding the square $-\frac{1}{2n\sigma^2}(t_2^2 - 2n\mu t_2 + n^2\mu^2)$ and regrouping terms:

      $$
      f(t_2|\mu) = \underbrace{ \exp\left\{ -\frac{n\mu^2}{2\sigma^2} \right\} }_{C(\mu)} \underbrace{ \frac{1}{\sqrt{2\pi n \sigma^2}} \exp\left\{ -\frac{t_2^2}{2n\sigma^2} \right\} }_{h(t_2)} \exp\left\{ \frac{\mu}{\sigma^2} t_2 \right\}
      $$
      
      **Result:** The Lemma correctly identifies the form, with the natural parameter $\frac{\mu}{\sigma^2}$.

2. Marginal of $T_1$ (fixing $\mu$)
   Suppose $\mu$ is known (constant). We investigate whether the marginal distribution of $T_1 = \sum X_i^2$ remains in the exponential family by checking the behavior of the remaining natural parameter $\pi_2 = \frac{\mu}{\sigma^2}$ with respect to the free parameter $\sigma^2$.

   **Case A ($\mu = 0$):**

   In this case, $\pi_2 = 0$, which is trivially constant with respect to $\sigma^2$. The condition of the Marginal Distribution Lemma is satisfied.

   * **Implied Form by MDL:** The lemma claims that $T_1$ must follow an exponential family form with natural parameter $\pi_1 = -\frac{1}{2\sigma^2}$:
   $$
   f_{T_1}(t_1|\sigma^2) \propto h^*(t_1) \exp\left\{ -\frac{1}{2\sigma^2} t_1 \right\}
   $$

   * **Verification:** We know that for $\mu=0$, the scaled statistic $T_1 / \sigma^2$ follows a central Chi-squared distribution with $n$ degrees of freedom ($\chi^2_n$). The density is proportional to:
   $$
   f(t_1) \propto t_1^{n/2 - 1} \exp\left\{ -\frac{t_1}{2\sigma^2} \right\}
   $$
   This matches the form claimed by the MDL perfectly, with natural parameter $-1/(2\sigma^2)$ and base measure $h^*(t_1) = t_1^{n/2-1}$.

   **Case B ($\mu \neq 0$):**

   In this case, $\pi_2 = \frac{\mu}{\sigma^2}$ is a function of $\sigma^2$. As $\sigma^2$ changes, $\pi_2$ changes. The condition of the Lemma "$\pi_i(\theta)$ are constant for all $i \notin S$" **fails**. Therefore, the structure claimed by the lemma—a simple exponential family with parameter $\pi_1$—is not guaranteed.

   * **Verification:** We know that for $\mu \neq 0$, the statistic $T_1/\sigma^2$ follows a **Non-central Chi-squared** distribution $\chi'^2_n(\lambda)$ with non-centrality parameter $\lambda = \frac{\sum \mu^2}{\sigma^2} = \frac{n\mu^2}{\sigma^2}$.

   * The density of a non-central Chi-squared involves an infinite mixture of central densities:
     $$
     f(t_1) = \sum_{k=0}^\infty P(K=k) f_{\chi^2_{n+2k}}(t_1)
     $$
     where the weights $P(K=k)$ depend on $\lambda$ (and thus $\sigma^2$).

   * **Result:** Because the parameter $\sigma^2$ appears inside the Poisson weights of the infinite sum, the density **cannot** be factored into the simple form $C(\sigma^2)h(t_1)\exp(\pi_1 t_1)$. This confirms that when the orthogonality condition is violated, the marginal distribution of a sufficient statistic leaves the simple exponential family.

:::

::: {#exm-variance-distribution-mdl}

### Distribution of Sample Variance $S^2$ via Marginal Distribution Lemma

Consider the Normal model $X_i \sim N(\mu, \sigma^2)$. We wish to find the marginal distribution of the sufficient statistic $S^2 = \frac{1}{n-1}\sum (X_i - \bar{X})^2$.

1. **Jacobian of the Transformation**
   When transforming the joint density from the $n$ data points $(X_1, \dots, X_n)$ to the sufficient statistics $(\bar{X}, S^2)$, we must account for the change in volume (the Jacobian). 
   
   The vector of residuals $(X_1 - \bar{X}, \dots, X_n - \bar{X})$ lies on an $(n-1)$-dimensional sphere with squared radius proportional to $s^2$. The surface area of this sphere scales as $(s^2)^{\frac{n-1}{2}-1}$. Thus, the volume element transforms as:
   
   $$
   \prod dx_i \propto (s^2)^{\frac{n-3}{2}} d s^2 d\bar{x}
   $$

2. **Decomposition of the Joint Density**
   Incorporating this Jacobian into the exponential family form, the joint density of $(\bar{X}, S^2)$ is:

   $$
   \begin{aligned}
   f(s^2, \bar{x}) &\propto \underbrace{(s^2)^{\frac{n-3}{2}}}_{\text{Jacobian } h(s^2, \bar{x})} \cdot \exp\left\{ -\frac{1}{2\sigma^2}\sum x_i^2 + \frac{\mu}{\sigma^2}\sum x_i \right\} \\
   &\propto (s^2)^{\frac{n-3}{2}} \cdot \exp\left\{ -\frac{1}{2\sigma^2}\left[ (n-1)s^2 + n\bar{x}^2 \right] + \frac{n\mu}{\sigma^2}\bar{x} \right\}
   \end{aligned}
   $$

   Grouping the terms by variable:

   $$
   f(s^2, \bar{x}) \propto \underbrace{\left[ (s^2)^{\frac{n-3}{2}} \exp\left\{ -\frac{n-1}{2\sigma^2} s^2 \right\} \right]}_{\text{Terms involving } s^2} \cdot \underbrace{\left[ \exp\left\{ -\frac{n}{2\sigma^2}\bar{x}^2 + \frac{n\mu}{\sigma^2}\bar{x} \right\} \right]}_{\text{Terms involving } \bar{x}}
   $$

3. **Marginal Integration**
   To find the marginal distribution of $S^2$, we integrate out $\bar{x}$. Since the density factors completely (orthogonality), the integral over $\bar{x}$ contributes only a multiplicative constant $C(\mu, \sigma^2)$ and does not depend on $s^2$.
   
   The marginal density is simply the $s^2$ component retained from the joint density:

   $$
   f_{S^2}(s^2) \propto (s^2)^{\frac{n-3}{2}} \exp\left\{ -\frac{n-1}{2\sigma^2} s^2 \right\}
   $$

4. **Identification of Degrees of Freedom**
   We match this result against the kernel of a standard Gamma distribution (or Chi-squared type), $f(y) \propto y^{k-1} e^{-\beta y}$.
   
   * **Shape Parameter ($k$):**
       $$
       k - 1 = \frac{n-3}{2} \implies k = \frac{n-1}{2}
       $$
       Since the degrees of freedom $\nu$ is defined as $2k$, we have $\nu = n-1$.
       
   * **Rate Parameter ($\beta$):**
       $$
       \beta = \frac{n-1}{2\sigma^2}
       $$
   
   This confirms that $S^2 \sim \text{Gamma}(\frac{n-1}{2}, \frac{n-1}{2\sigma^2})$, or equivalently, $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$.

:::

### Moment Generating Functions for Exponential Families

