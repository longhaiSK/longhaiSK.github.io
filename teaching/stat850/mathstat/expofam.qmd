---
title: "Exponential Families"
engine: knitr
format: 
  html: default
  pdf: default
---

## Sufficient Statistics

::: {#def-sufficiency-likelihood}

### Sufficient Statistic (Likelihood Principle)
A statistic $T(\mathbf{X})$ is **sufficient** for $\theta$ if the likelihood function $L(\theta; \mathbf{x})$ depends on the sample data $\mathbf{x}$ **only through** the value of the statistic $T(\mathbf{x})$.

Formally, this means that for any two sample points $\mathbf{x}$ and $\mathbf{y}$ such that $T(\mathbf{x}) = T(\mathbf{y})$, the ratio of their likelihoods is constant with respect to $\theta$:

$$
\frac{L(\theta; \mathbf{x})}{L(\theta; \mathbf{y})} = k(\mathbf{x}, \mathbf{y}) \quad (\text{independent of } \theta)
$$

This implies that $L(\theta; \mathbf{x}) \propto L(\theta; T(\mathbf{x}))$.

:::

::: {#thm-factorization}

### Factorization Theorem
Let $f(\mathbf{x}|\theta)$ denote the joint probability density function (or probability mass function) of a sample $\mathbf{X}$. A statistic $T(\mathbf{X})$ is **sufficient** for $\theta$ if and only if the density can be factored as:

$$
f(\mathbf{x}|\theta) = g(T(\mathbf{x})|\theta) h(\mathbf{x})
$$

where:

* $g(T(\mathbf{x})|\theta)$ depends on the data $\mathbf{x}$ only through the statistic $T(\mathbf{x})$ and the parameter $\theta$.

* $h(\mathbf{x})$ does not depend on $\theta$.

:::

::: {#exm-normal-sufficient}

### Sufficient Statistics for Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. The joint density is given by:

$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{ -\frac{(x_i - \mu)^2}{2\sigma^2} \right\}
$$

Expanding the exponent:

$$
f(\mathbf{x}|\theta) = (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} \left( \sum_{i=1}^n x_i^2 - 2\mu \sum_{i=1}^n x_i + n\mu^2 \right) \right\}
$$

We can rearrange this into the factorization form $g(T(\mathbf{x})|\theta) h(\mathbf{x})$:

$$
f(\mathbf{x}|\theta) = \underbrace{ (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{n\mu^2}{2\sigma^2} \right\} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\} }_{g(T(\mathbf{x})|\theta)} \cdot \underbrace{ 1 }_{h(\mathbf{x})}
$$

Here, the function $g$ depends on the data only through the pair of statistics:

$$
T(\mathbf{x}) = \left( \sum_{i=1}^n X_i, \sum_{i=1}^n X_i^2 \right)
$$

Thus, $T(\mathbf{X}) = (\sum X_i, \sum X_i^2)$ is sufficient for $\theta = (\mu, \sigma^2)$.

**Invariance Property:**
It is a general property that any one-to-one (invertible) function of a sufficient statistic is also a sufficient statistic. We can define the sample mean and sample variance as:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i, \quad S^2 = \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - \frac{1}{n} \left(\sum_{i=1}^n X_i\right)^2 \right)
$$

Since the mapping from $(\sum X_i, \sum X_i^2)$ to $(\bar{X}, S^2)$ is invertible (provided $n \ge 2$), the statistic $T^*(\mathbf{X}) = (\bar{X}, S^2)$ is also sufficient for $\theta$.

:::

::: {#rem-sufficient-likelihood}

### Sufficient Statistic as Parameter of Likelihood
There is a dual relationship between the sufficient statistic and the parameter $\theta$. Conventionally, we view $f(x|\theta)$ as a function of $x$ parameterized by $\theta$.

However, in Bayesian inference or likelihood theory, we often view the likelihood $L(\theta; x)$ as a function of $\theta$ determined by the observed data $x$. The Factorization Theorem implies:

$$
L(\theta; \mathbf{x}) \propto g(T(\mathbf{x})|\theta)
$$

This suggests that $T(\mathbf{x})$ completely determines the shape of the likelihood function. In this specific sense, the sufficient statistic $T(\mathbf{x})$ acts as the **"parameter"** of the likelihood function itself.

For the exponential family that we will discuss below, this duality is explicit:

$$
\log L(\theta; \mathbf{x}) = \text{const} + \sum_{i=1}^k \eta_i(\theta) T_i(\mathbf{x}) - n A(\theta)
$$

Here, $T_i(\mathbf{x})$ serves as the coefficient (or parameter) for the function $\eta_i(\theta)$.

:::

## Exponential Families

::: {#def-exponential-family}

### Exponential Family
A family of probability density functions (or probability mass functions) $f(x|\theta)$ is said to be an **Exponential Family** if it can be written in the form:

$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

where:

* $\theta = (\theta_1, \dots, \theta_d)$ is the parameter vector.

* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.

* By the Factorization Theorem, the vector $T(x) = (\tau_1(x), \dots, \tau_k(x))$ constitutes a **sufficient statistic** for $\theta$.

:::

### Examples of Exponential Families

::: {#exm-exponential-dist}

### Exponential Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\theta} e^{-x_i/\theta} = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

Here we identify:

* $C(\theta) = \theta^{-n}$

* $h(\mathbf{x}) = 1$

* $\pi_1(\theta) = -\frac{1}{\theta}$

* $\tau_1(\mathbf{x}) = \sum_{i=1}^n x_i$.

:::

::: {#exm-gamma-dist}

### Gamma Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\Gamma(\alpha)\beta^\alpha} x_i^{\alpha-1} e^{-x_i/\beta}
$$

$$
= [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Rewriting in the canonical form:

$$
= [\Gamma(\alpha)]^{-n} \beta^{-n\alpha} \exp\left\{ (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Here:

* $\pi_1(\theta) = \alpha - 1$, $\tau_1(\mathbf{x}) = \sum \log x_i$

* $\pi_2(\theta) = -\frac{1}{\beta}$, $\tau_2(\mathbf{x}) = \sum x_i$.

:::

::: {#exm-beta-dist}

### Beta Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\theta = (a, b)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}
$$

$$
= [B(a, b)]^{-n} \exp\left\{ (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) \right\}
$$

This is an exponential family with $k=2$.

:::

::: {#exm-normal-dist}

### Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$ with $\theta = (\mu, \sigma^2)$.
$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{(x_i - \mu)^2}{2\sigma^2} \right\}
$$

$$
= (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{n\mu^2}{2\sigma^2} \right\}
$$

$$
= \left[ (2\pi)^{-n/2} (\sigma^2)^{-n/2} e^{-\frac{n\mu^2}{2\sigma^2}} \right] \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\}
$$

Here $d=2$ and $k=2$.

:::

### Examples of Non-exponential Families

A model is **not** in the exponential family if the support depends on the parameter.

::: {#exm-uniform-dist}

### Uniform Distribution
Let $X \sim U(0, \theta)$.
$$
f(x|\theta) = \frac{1}{\theta} I(0 < x < \theta)
$$

This cannot be written in the required form because the indicator function $I(0 < x < \theta)$ cannot be factorized into separate functions of $x$ and $\theta$ inside an exponential.

:::

::: {#exm-cauchy-dist}

### Cauchy Distribution
Let $X \sim \text{Cauchy}(\theta)$.
$$
f(x|\theta) = \frac{1}{\pi [1 + (x-\theta)^2]}
$$

This involves $\log(1 + (x-\theta)^2)$ in the exponent, which cannot be separated into sums of products $\pi_i(\theta)\tau_i(x)$.

:::

## Regular Families

In the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.

::: {#def-regular-family}

### Regular Family
A family of probability density functions $f(x|\theta)$ is said to be a **Regular Family** (or "sufficiently well-behaved") if the domain of $x$ for which $f(x|\theta) > 0$ (the support) does not depend on the parameter $\theta$.

This condition is necessary to satisfy the identity that allows differentiation under the integral sign:

$$
\frac{\partial}{\partial \theta} \int f(x|\theta) dx = \int \frac{\partial}{\partial \theta} f(x|\theta) dx
$$

:::

### Why Regularity Matters {.unnumbered}

If a family is regular, we can differentiate the identity $\int f(x|\theta) dx = 1$ with respect to $\theta$. This operation yields the fundamental properties of the **Score Function** $\frac{\partial}{\partial \theta} \log f(X|\theta)$:

1.  **First Moment Identity:** The expected value of the score function is zero.

$$E_\theta \left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right] = 0$$

2.  **Second Moment Identity:** The Fisher Information (variance of the score) is equal to the negative expected Hessian.

$$E_\theta \left[ \frac{\partial^2 \log f(X|\theta)}{\partial \theta_j \partial \theta_l} \right] = -E_\theta \left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \frac{\partial \log f(X|\theta)}{\partial \theta_l} \right]$$

### An Example of Non-regular Distribution  {.unnumbered}
The **Uniform Distribution** $U(0, \theta)$ is a classic example of a **non-regular** family. Because the support $(0, \theta)$ depends on $\theta$, the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.

## Moments of Sufficient Statistics of Exponential Families

### Means of Sufficient Statistics (General Case)

::: {#thm-moments-exp-family}

### Means of Sufficient Statistics (General Case)
For a random variable $X$ belonging to an exponential family with density $f(x|\theta) = C(\theta) h(x) \exp\{\sum_{i=1}^k \pi_i(\theta) \tau_i(x)\}$, the moments of the sufficient statistics $\tau_i(X)$ satisfy the system of equations:
$$
\frac{1}{C(\theta)} \frac{\partial C(\theta)}{\partial \theta_j} + \sum_{i=1}^k \frac{\partial \pi_i(\theta)}{\partial \theta_j} E[\tau_i(X)] = 0 \quad \text{for } j=1, \dots, d
$$

:::

::: {.proof}
For regular families, we can interchange differentiation and integration. Since $\int f(x|\theta) dx = 1$, we have:
$$
\frac{\partial}{\partial \theta} \int f(x|\theta) dx = 0 \implies \int \frac{\partial}{\partial \theta} f(x|\theta) dx = 0
$$

Using the identity $\frac{\partial f}{\partial \theta} = f(x|\theta) \frac{\partial \log f}{\partial \theta}$, we derive the fundamental moment property:

$$
E\left[ \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right] = \int \frac{\partial \log f(x|\theta)}{\partial \theta_j} f(x|\theta) dx = 0
$$

For the exponential family, the log-likelihood is given by:

$$
\log f(x|\theta) = \log C(\theta) + \log h(x) + \sum_{i=1}^k \pi_i(\theta) \tau_i(x)
$$

Taking the derivative with respect to $\theta_j$:

$$
\frac{\partial \log f(x|\theta)}{\partial \theta_j} = \frac{1}{C(\theta)} \frac{\partial C(\theta)}{\partial \theta_j} + \sum_{i=1}^k \frac{\partial \pi_i(\theta)}{\partial \theta_j} \tau_i(x)
$$

Taking expectations and applying the condition $E[\frac{\partial}{\partial \theta} \log f(X|\theta)] = 0$ yields the theorem statement.

:::

::: {#exm-normal-moments}

### Moments of Normal Sufficient Statistics
Consider the Normal distribution $N(\mu, \sigma^2)$ where $\theta = (\mu, \sigma^2)$. The density is:
$$
f(\mathbf{x}|\theta) = (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{n\mu^2}{2\sigma^2} \right\} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\}
$$

Here we identify the components :

* $C(\mu, \sigma^2) = (2\pi)^{-n/2} (\sigma^2)^{-n/2} \exp\left(-\frac{n\mu^2}{2\sigma^2}\right)$

* $\pi_1 = -\frac{1}{2\sigma^2}, \quad \tau_1(\mathbf{x}) = \sum x_i^2$

* $\pi_2 = \frac{\mu}{\sigma^2}, \quad \tau_2(\mathbf{x}) = \sum x_i$

We apply the theorem by differentiating with respect to $\mu$ and $\sigma^2$.

1. Differentiate with respect to $\mu$:

   $$
      \frac{\partial \log C}{\partial \mu} = -\frac{n\mu}{\sigma^2}
   $$

      

   $$
      \frac{\partial \pi_1}{\partial \mu} = 0, \quad \frac{\partial \pi_2}{\partial \mu} = \frac{1}{\sigma^2}
   $$

      The theorem equation becomes:

   $$
      -\frac{n\mu}{\sigma^2} + 0 \cdot E[\sum X_i^2] + \frac{1}{\sigma^2} E[\sum X_i] = 0
   $$

      


   $$
      \implies E[\sum_{i=1}^n X_i] = n\mu
   $$

1. Differentiate with respect to $\sigma^2$:

   $$
      \frac{\partial \log C}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{n\mu^2}{2(\sigma^2)^2}
   $$

      

   

   $$
      \frac{\partial \pi_1}{\partial \sigma^2} = \frac{1}{2(\sigma^2)^2}, \quad \frac{\partial \pi_2}{\partial \sigma^2} = -\frac{\mu}{(\sigma^2)^2}
   $$

      The theorem equation becomes:

   $$
      \left( -\frac{n}{2\sigma^2} + \frac{n\mu^2}{2(\sigma^2)^2} \right) + \frac{1}{2(\sigma^2)^2} E[\sum X_i^2] - \frac{\mu}{(\sigma^2)^2} E[\sum X_i] = 0
   $$

      Multiplying by $2(\sigma^2)^2$:

   $$
      -n\sigma^2 + n\mu^2 + E[\sum X_i^2] - 2\mu(n\mu) = 0
   $$

      


   $$
      E[\sum X_i^2] = n\sigma^2 + n\mu^2
   $$

   This recovers the standard second moment $E[X^2] = \sigma^2 + \mu^2$.

:::

::: {#exm-gamma-moments}

### Moments of Gamma Sufficient Statistics
Consider the Gamma distribution $\text{Gamma}(\alpha, \beta)$ with $\theta = (\alpha, \beta)$. The density is :
$$
f(\mathbf{x}|\theta) = [\Gamma(\alpha)]^{-n} \beta^{-n\alpha} \exp\left\{ (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Here we identify:

* $\log C(\alpha, \beta) = -n \log \Gamma(\alpha) - n\alpha \log \beta$

* $\pi_1 = \alpha - 1, \quad \tau_1(\mathbf{x}) = \sum \log x_i$

* $\pi_2 = -\frac{1}{\beta}, \quad \tau_2(\mathbf{x}) = \sum x_i$

1. Differentiate with respect to $\beta$:

   $$
      \frac{\partial \log C}{\partial \beta} = -\frac{n\alpha}{\beta}
   $$


   $$
      \frac{\partial \pi_1}{\partial \beta} = 0, \quad \frac{\partial \pi_2}{\partial \beta} = \frac{1}{\beta^2}
   $$

      The theorem yields:

   $$
      -\frac{n\alpha}{\beta} + \frac{1}{\beta^2} E[\sum X_i] = 0 \implies E[\sum X_i] = n\alpha\beta
   $$

1. Differentiate with respect to $\alpha$:

   $$
      \frac{\partial \log C}{\partial \alpha} = -n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)} - n \log \beta = -n \psi(\alpha) - n \log \beta
   $$

      where $\psi(\alpha)$ is the digamma function.

   $$
      \frac{\partial \pi_1}{\partial \alpha} = 1, \quad \frac{\partial \pi_2}{\partial \alpha} = 0
   $$

      The theorem yields:

   $$
      (-n \psi(\alpha) - n \log \beta) + 1 \cdot E[\sum \log X_i] = 0
   $$

   $$
   \implies E[\sum_{i=1}^n \log X_i] = n(\psi(\alpha) + \log \beta)
   $$

:::


### Natural Parameterization

::: {#def-natural-parameterization}

### Natural Parameterization
Suppose an exponential family is given by:
$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

We define the **natural parameters** $\eta_i$ as $\eta_i = \pi_i(\theta)$. Let $\eta = (\eta_1, \dots, \eta_k)$. The density becomes:

$$
f(x|\eta) = C^*(\eta) h(x) \exp\left\{ \sum_{i=1}^k \eta_i \tau_i(x) \right\}
$$

:::

::: {#def-natural-space}

### Natural Parameter Space
The natural parameter space $\mathcal{H}$ is defined as:
$$
\mathcal{H} = \{ \eta = (\pi_1(\theta), \dots, \pi_k(\theta)) : \int h(x) e^{\sum \eta_i \tau_i(x)} dx < \infty \}
$$

where the condition ensures $C(\theta)$ is finite.

:::

:::{#def-curvedexpfam}

### Full vs. Curved Exponential Families

Let $d$ be the dimension of $\theta$ and $k$ be the dimension of the sufficient statistic vector $\tau(x)$.

* If $d = k$, we say $f(x|\theta)$ is a **Full Exponential Family**.

* If $d < k$, we say $f(x|\theta)$ is a **Curved Exponential Family**.

:::

::: {#exm-natural-normal}

### Natural Parameterization of Normal Distribution
Consider the full Normal family $N(\mu, \sigma^2)$ where both parameters are unknown ($d=2$). We previously identified the sufficient statistics $T_1(x) = \sum x_i$ and $T_2(x) = \sum x_i^2$ ($k=2$). Since $d=k$, this is a **Full Exponential Family**.

The natural parameters $\eta = (\eta_1, \eta_2)$ are defined by the mapping:

$$
\eta_1 = \frac{\mu}{\sigma^2}, \quad \eta_2 = -\frac{1}{2\sigma^2}
$$

The density can be rewritten purely in terms of $\eta$:

$$
f(\mathbf{x}|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n x_i + \eta_2 \sum_{i=1}^n x_i^2 - A(\eta) \right\}
$$
where the log-partition function $A(\eta)$ absorbs the normalizing constants.

:::

::: {#exm-natural-gamma}

### Natural Parameterization of Gamma Distribution
Consider the Gamma family $\text{Gamma}(\alpha, \beta)$ ($d=2$). We identified the sufficient statistics $T_1(x) = \sum \log x_i$ and $T_2(x) = \sum x_i$ ($k=2$). Since $d=k$, this is also a **Full Exponential Family**.

The natural parameters $\eta = (\eta_1, \eta_2)$ are derived from the canonical form :

$$
\eta_1 = \alpha - 1, \quad \eta_2 = -\frac{1}{\beta}
$$

The density in natural parameterization is:

$$
f(\mathbf{x}|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n \log x_i + \eta_2 \sum_{i=1}^n x_i - A(\eta) \right\}
$$

:::

::: {#exm-curved-normal-natural}

### Curved Exponential Family (Natural Parameterization)
Consider the $N(\theta, \theta^2)$ distribution ($d=1$). The density is:
$$
f(x|\theta) \propto \exp\left\{ -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i \right\}
$$

To express this in the natural parameterization, we define $\eta = (\eta_1, \eta_2)$ as:

$$
\eta_1 = -\frac{1}{2\theta^2}, \quad \eta_2 = \frac{1}{\theta}
$$

The density becomes:

$$
f(x|\eta) \propto \exp\left\{ \eta_1 \sum_{i=1}^n x_i^2 + \eta_2 \sum_{i=1}^n x_i - A(\eta) \right\}
$$

However, the natural parameters $\eta_1$ and $\eta_2$ are not independent. They satisfy the constraint:

$$
\eta_1 = -\frac{1}{2} \eta_2^2
$$

Because the parameter space $\mathcal{H}$ forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a **Curved Exponential Family**.

:::

### Mean and Covariance of  Natural Exponential Families


::: {#thm-natural-covariance}

### Mean and Covariance of Natural Exponential Families
If the exponential family is in its **canonical form** (natural parameterization) where $\pi_i(\theta) = \theta_i$ for each $i$, then the mean and covariance of the sufficient statistics are given by:
$$
E_\theta[\tau_i(X)] = - \frac{\partial}{\partial \theta_i} \log C(\theta)
$$

$$
\text{Cov}_\theta(\tau_i(X), \tau_j(X)) = - \frac{\partial^2}{\partial \theta_i \partial \theta_j} \log C(\theta)
$$

:::

::: {.proof}
We use the second-order regularity condition for the score function:
$$
E_\theta\left[ \frac{\partial^2 \log f(X|\theta)}{\partial \theta_i \partial \theta_j} \right] = - E_\theta\left[ \frac{\partial \log f(X|\theta)}{\partial \theta_i} \frac{\partial \log f(X|\theta)}{\partial \theta_j} \right]
$$

In the case of a natural exponential family, $\pi_k(\theta) = \theta_k$. The derivative of the log-likelihood simplifies to:

$$
\frac{\partial \log f}{\partial \theta_i} = \frac{\partial \log C(\theta)}{\partial \theta_i} + \tau_i(x)
$$

Differentiating again with respect to $\theta_j$:

$$
\frac{\partial^2 \log f}{\partial \theta_i \partial \theta_j} = \frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j}
$$

Since this second derivative is non-random (it does not depend on $x$), its expectation is simply itself.

Now consider the right-hand side of the regularity condition. From the first moment property, we know $E[\tau_i(X)] = - \frac{\partial \log C}{\partial \theta_i}$. Thus, the score function is:

$$
\frac{\partial \log f}{\partial \theta_i} = \tau_i(X) - E[\tau_i(X)]
$$

Substituting these into the identity:

$$
\frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j} = - E\left[ (\tau_i(X) - E[\tau_i(X)]) (\tau_j(X) - E[\tau_j(X)]) \right]
$$

$$
\frac{\partial^2 \log C(\theta)}{\partial \theta_i \partial \theta_j} = - \text{Cov}_\theta(\tau_i(X), \tau_j(X))
$$

Multiplying by $-1$ gives the result.

:::



## Distributions of Sufficient Statistics

::: {#lem-joint-distribution}

### Joint Distribution of Sufficient Statistics
If $X$ has a distribution in the exponential family $f(x|\theta) = C(\theta) h(x) \exp\{\sum \pi_i(\theta) \tau_i(x)\}$, then the joint distribution of the sufficient statistics $T = (\tau_1(X), \dots, \tau_k(X))$ is also in the exponential family with the same natural parameters.

:::

::: {.proof}
Let $X$ be discrete. The probability mass function of $T$ is:
$$
P(T_1 = y_1, \dots, T_k = y_k | \theta) = \sum_{\{x : \tau(x) = y\}} P(X=x|\theta)
$$

$$
= \sum_{\{x : \tau(x) = y\}} C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

Since the exponential term and $C(\theta)$ depend only on $y$ and $\theta$, they can be pulled out of the sum:

$$
= C(\theta) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\} \left( \sum_{\{x : \tau(x) = y\}} h(x) \right)
$$

Defining $h^*(y) = \sum_{\{x : \tau(x) = y\}} h(x)$, we get:

$$
f_T(y|\theta) = C(\theta) h^*(y) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

which is of the exponential family form .

:::

::: {#lem-marginal-distribution}

### Marginal Distribution Lemma for Exponetial Families (MDL)
Let $S$ be a subset of indices $\{1, \dots, k\}$. If $\pi_i(\theta)$ are constant for all $i \notin S$, then the marginal distribution of the statistics $T_S = \{ \tau_j(X) : j \in S \}$ is of the exponential family form with natural parameters $\pi_j(\theta)$ for $j \in S$.

:::

::: {.proof}
Let $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):
$$
P(T_S = y_S | \theta) = \sum_{y_{S^c}} C(\theta) h^*(y) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j + \sum_{l \in S^c} \pi_l(\theta) y_l \right\}
$$

Since $\pi_l(\theta)$ is constant for $l \in S^c$, the term $\exp\{\sum_{l \in S^c} \pi_l y_l\}$ does not depend on $\theta$. We can group it with $h^*(y)$:

$$
= C(\theta) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j \right\} \sum_{y_{S^c}} h^*(y) \exp\left\{ \sum_{l \in S^c} \pi_l y_l \right\}
$$

The sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form.

:::

::: {#exm-normal-marginal-comparison}

### Marginal Distributions of Normal Sufficient Statistics
Consider the Normal model $N(\mu, \sigma^2)$ with sufficient statistics $T_1 = \sum X_i^2$ and $T_2 = \sum X_i$. The natural parameters are $\pi_1 = -\frac{1}{2\sigma^2}$ and $\pi_2 = \frac{\mu}{\sigma^2}$.

1. Marginal of $T_2$ (fixing $\sigma^2$)
   Suppose $\sigma^2$ is known (constant).

   * **Lemma Condition:** $\pi_1 = -1/(2\sigma^2)$ is constant. The condition holds.
   * **Implied Form by MDL:** The Marginal Distribution Lemma claims that $T_2$ follows an exponential family with natural parameter $\pi_2 = \frac{\mu}{\sigma^2}$:
   $$
   f_{T_2}(t_2|\theta) \propto h^*(t_2) \exp\left\{ \frac{\mu}{\sigma^2} t_2 \right\}
   $$

   * **Comparison with Known Distribution:**
      We know $T_2 = \sum X_i \sim N(n\mu, n\sigma^2)$. The density is:
      $$
      f(t_2|\mu) = \frac{1}{\sqrt{2\pi n \sigma^2}} \exp\left\{ -\frac{(t_2 - n\mu)^2}{2n\sigma^2} \right\}
      $$

      Expanding the square $-\frac{1}{2n\sigma^2}(t_2^2 - 2n\mu t_2 + n^2\mu^2)$ and regrouping terms:

      $$
      f(t_2|\mu) = \underbrace{ \exp\left\{ -\frac{n\mu^2}{2\sigma^2} \right\} }_{C(\mu)} \underbrace{ \frac{1}{\sqrt{2\pi n \sigma^2}} \exp\left\{ -\frac{t_2^2}{2n\sigma^2} \right\} }_{h(t_2)} \exp\left\{ \frac{\mu}{\sigma^2} t_2 \right\}
      $$
      
      **Result:** The Lemma correctly identifies the form, with the natural parameter $\frac{\mu}{\sigma^2}$.

2. Marginal of $T_1$ (fixing $\mu$)
   Suppose $\mu$ is known (constant). We investigate whether the marginal distribution of $T_1 = \sum X_i^2$ remains in the exponential family by checking the behavior of the remaining natural parameter $\pi_2 = \frac{\mu}{\sigma^2}$ with respect to the free parameter $\sigma^2$.

   **Case A ($\mu = 0$):**

   In this case, $\pi_2 = 0$, which is trivially constant with respect to $\sigma^2$. The condition of the Marginal Distribution Lemma is satisfied.

   * **Implied Form by MDL:** The lemma claims that $T_1$ must follow an exponential family form with natural parameter $\pi_1 = -\frac{1}{2\sigma^2}$:
   $$
   f_{T_1}(t_1|\sigma^2) \propto h^*(t_1) \exp\left\{ -\frac{1}{2\sigma^2} t_1 \right\}
   $$

   * **Verification:** We know that for $\mu=0$, the scaled statistic $T_1 / \sigma^2$ follows a central Chi-squared distribution with $n$ degrees of freedom ($\chi^2_n$). The density is proportional to:
   $$
   f(t_1) \propto t_1^{n/2 - 1} \exp\left\{ -\frac{t_1}{2\sigma^2} \right\}
   $$
   This matches the form claimed by the MDL perfectly, with natural parameter $-1/(2\sigma^2)$ and base measure $h^*(t_1) = t_1^{n/2-1}$.

   **Case B ($\mu \neq 0$):**

   In this case, $\pi_2 = \frac{\mu}{\sigma^2}$ is a function of $\sigma^2$. As $\sigma^2$ changes, $\pi_2$ changes. The condition of the Lemma "$\pi_i(\theta)$ are constant for all $i \notin S$" **fails**. Therefore, the structure claimed by the lemma—a simple exponential family with parameter $\pi_1$—is not guaranteed.

   * **Verification:** We know that for $\mu \neq 0$, the statistic $T_1/\sigma^2$ follows a **Non-central Chi-squared** distribution $\chi'^2_n(\lambda)$ with non-centrality parameter $\lambda = \frac{\sum \mu^2}{\sigma^2} = \frac{n\mu^2}{\sigma^2}$.

   * The density of a non-central Chi-squared involves an infinite mixture of central densities:
     $$
     f(t_1) = \sum_{k=0}^\infty P(K=k) f_{\chi^2_{n+2k}}(t_1)
     $$
     where the weights $P(K=k)$ depend on $\lambda$ (and thus $\sigma^2$).

   * **Result:** Because the parameter $\sigma^2$ appears inside the Poisson weights of the infinite sum, the density **cannot** be factored into the simple form $C(\sigma^2)h(t_1)\exp(\pi_1 t_1)$. This confirms that when the orthogonality condition is violated, the marginal distribution of a sufficient statistic leaves the simple exponential family.

:::
