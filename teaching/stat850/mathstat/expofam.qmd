---
title: "Exponential Families"
engine: knitr
format: 
  html: default
  pdf: default
---
# Exponential Families

::: {#def-exponential-family}
### Exponential Family
A family of probability density functions (or probability mass functions) $f(x|\theta)$ is said to be an **Exponential Family** if it can be written in the form:

$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

where:

* [cite_start]$\theta = (\theta_1, \dots, \theta_d)$ is the parameter vector[cite: 20].

* $k$ is the number of terms in the exponent. [cite_start]Note that $d$ may be less than $k$[cite: 22].
:::

::: {#exm-exponential-dist}
### Exponential Distribution
[cite_start]Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter[cite: 23].

$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\theta} e^{-x_i/\theta} = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

Here we identify:

* $C(\theta) = \theta^{-n}$

* $h(\mathbf{x}) = 1$

* $\pi_1(\theta) = -\frac{1}{\theta}$

* [cite_start]$\tau_1(\mathbf{x}) = \sum_{i=1}^n x_i$[cite: 26, 27].
:::

::: {#exm-gamma-dist}
### Gamma Distribution
[cite_start]Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$[cite: 30].

$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\Gamma(\alpha)\beta^\alpha} x_i^{\alpha-1} e^{-x_i/\beta}
$$

$$
= [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Rewriting in the canonical form:

$$
= [\Gamma(\alpha)]^{-n} \beta^{-n\alpha} \exp\left\{ (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

Here:

* $\pi_1(\theta) = \alpha - 1$, $\tau_1(\mathbf{x}) = \sum \log x_i$

* [cite_start]$\pi_2(\theta) = -\frac{1}{\beta}$, $\tau_2(\mathbf{x}) = \sum x_i$[cite: 31, 33].
:::

::: {#exm-beta-dist}
### Beta Distribution
[cite_start]Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\theta = (a, b)$[cite: 35].

$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}
$$

$$
= [B(a, b)]^{-n} \exp\left\{ (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) \right\}
$$

[cite_start]This is an exponential family with $k=2$[cite: 37, 38].
:::

::: {#exm-normal-dist}
### Normal Distribution
[cite_start]Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$ with $\theta = (\mu, \sigma^2)$[cite: 46].

$$
f(\mathbf{x}|\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{(x_i - \mu)^2}{2\sigma^2} \right\}
$$

$$
= (2\pi\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{n\mu^2}{2\sigma^2} \right\}
$$

$$
= \left[ (2\pi)^{-n/2} (\sigma^2)^{-n/2} e^{-\frac{n\mu^2}{2\sigma^2}} \right] \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i \right\}
$$

[cite_start]Here $d=2$ and $k=2$[cite: 49].
:::

## Counter-Examples

[cite_start]A model is **not** in the exponential family if the support depends on the parameter[cite: 57].

::: {#exm-uniform-dist}
### Uniform Distribution
[cite_start]Let $X \sim U(0, \theta)$[cite: 76].

$$
f(x|\theta) = \frac{1}{\theta} I(0 < x < \theta)
$$

[cite_start]This cannot be written in the required form because the indicator function $I(0 < x < \theta)$ cannot be factorized into separate functions of $x$ and $\theta$ inside an exponential[cite: 77].
:::

::: {#exm-cauchy-dist}
### Cauchy Distribution
[cite_start]Let $X \sim \text{Cauchy}(\theta)$[cite: 61].

$$
f(x|\theta) = \frac{1}{\pi [1 + (x-\theta)^2]}
$$

[cite_start]This involves $\log(1 + (x-\theta)^2)$ in the exponent, which cannot be separated into sums of products $\pi_i(\theta)\tau_i(x)$[cite: 63, 64].
:::

## Properties of Exponential Families

For regular families, we can interchange differentiation and integration. Since $\int f(x|\theta) dx = 1$, we have:

$$
\frac{\partial}{\partial \theta} \int f(x|\theta) dx = 0 \implies \int \frac{\partial}{\partial \theta} f(x|\theta) dx = 0
$$

[cite_start]Using the identity $\frac{\partial f}{\partial \theta} = f(x|\theta) \frac{\partial \log f}{\partial \theta}$, we derive the fundamental moment property[cite: 78]:

$$
E\left[ \frac{\partial}{\partial \theta} \log f(X|\theta) \right] = 0
$$

[cite_start]Differentiating a second time yields[cite: 81]:

$$
E\left[ \frac{\partial^2}{\partial \theta_i \partial \theta_j} \log f(X|\theta) \right] = - E\left[ \frac{\partial \log f}{\partial \theta_i} \frac{\partial \log f}{\partial \theta_j} \right]
$$

For the exponential family $f(x|\theta) = C(\theta) h(x) \exp\{\sum \pi_i(\theta) \tau_i(x)\}$, the log-likelihood is:

$$
\log f(x|\theta) = \log C(\theta) + \log h(x) + \sum_{i=1}^k \pi_i(\theta) \tau_i(x)
$$

[cite_start]Taking the expectation of the derivative with respect to $\theta_j$ and setting it to zero gives[cite: 86]:

$$
\frac{1}{C(\theta)} \frac{\partial C(\theta)}{\partial \theta_j} + \sum_{i=1}^k \frac{\partial \pi_i(\theta)}{\partial \theta_j} E[\tau_i(X)] = 0
$$

[cite_start]This system of equations allows us to find the moments $E[\tau_i(X)]$[cite: 91].

## Natural Parameterization

::: {#def-natural-parameterization}
### Natural Parameterization
Suppose an exponential family is given by:

$$
f(x|\theta) = C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) \tau_i(x) \right\}
$$

[cite_start]We define the **natural parameters** $\eta_i$ as $\eta_i = \pi_i(\theta)$[cite: 105]. Let $\eta = (\eta_1, \dots, \eta_k)$. The density becomes:

$$
f(x|\eta) = C^*(\eta) h(x) \exp\left\{ \sum_{i=1}^k \eta_i \tau_i(x) \right\}
$$
:::

::: {#def-natural-space}
### Natural Parameter Space
The natural parameter space $\mathcal{H}$ is defined as:

$$
\mathcal{H} = \{ \eta = (\pi_1(\theta), \dots, \pi_k(\theta)) : \int h(x) e^{\sum \eta_i \tau_i(x)} dx < \infty \}
$$

[cite_start]where the condition ensures $C(\theta)$ is finite[cite: 123].
:::

### Full vs. Curved Exponential Families

[cite_start]Let $d$ be the dimension of $\theta$ and $k$ be the dimension of the sufficient statistic vector $\tau(x)$[cite: 124, 126].

* [cite_start]If $d = k$, we say $f(x|\theta)$ is a **Full Exponential Family**[cite: 127].

* [cite_start]If $d < k$, we say $f(x|\theta)$ is a **Curved Exponential Family**[cite: 133].

::: {#exm-curved-normal}
### Curved Normal Distribution
Consider $N(\theta, \theta^2)$ where $\mu = \theta$ and $\sigma^2 = \theta^2$. [cite_start]This implies a relationship between the parameters[cite: 113, 145].

$$
f(x|\theta) \propto \exp\left\{ -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i \right\}
$$

Here we have two sufficient statistics ($\sum x_i^2, \sum x_i$) but only one parameter $\theta$ ($d=1, k=2$). The natural parameters are $\eta_1 = -1/(2\theta^2)$ and $\eta_2 = 1/\theta$, which satisfies $\eta_1 = -\eta_2^2 / 2$. [cite_start]This constraint forms a curve in the parameter space[cite: 139].
:::

### Moments in Natural Parameterization

[cite_start]For a natural exponential family, differentiating the log-partition function generates cumulants[cite: 165]:

$$
E[\tau_j(X)] = - \frac{\partial}{\partial \eta_j} \log C^*(\eta)
$$

$$
\text{Cov}(\tau_i(X), \tau_j(X)) = - \frac{\partial^2}{\partial \eta_i \partial \eta_j} \log C^*(\eta)
$$

## Lemmas on Sufficient Statistics

::: {#lem-joint-distribution}
### Joint Distribution of Sufficient Statistics
[cite_start]If $X$ has a distribution in the exponential family $f(x|\theta) = C(\theta) h(x) \exp\{\sum \pi_i(\theta) \tau_i(x)\}$, then the joint distribution of the sufficient statistics $T = (\tau_1(X), \dots, \tau_k(X))$ is also in the exponential family with the same natural parameters[cite: 171, 172].
:::

::: {.proof}
Let $X$ be discrete. The probability mass function of $T$ is:

$$
P(T_1 = y_1, \dots, T_k = y_k | \theta) = \sum_{\{x : \tau(x) = y\}} P(X=x|\theta)
$$

$$
= \sum_{\{x : \tau(x) = y\}} C(\theta) h(x) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

Since the exponential term and $C(\theta)$ depend only on $y$ and $\theta$, they can be pulled out of the sum:

$$
= C(\theta) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\} \left( \sum_{\{x : \tau(x) = y\}} h(x) \right)
$$

Defining $h^*(y) = \sum_{\{x : \tau(x) = y\}} h(x)$, we get:

$$
f_T(y|\theta) = C(\theta) h^*(y) \exp\left\{ \sum_{i=1}^k \pi_i(\theta) y_i \right\}
$$

[cite_start]which is of the exponential family form [cite: 185-199].
:::

::: {#lem-marginal-distribution}
### Marginal Distribution
Let $S$ be a subset of indices $\{1, \dots, k\}$. [cite_start]If $\pi_i(\theta)$ are constant for all $i \notin S$, then the marginal distribution of the statistics $T_S = \{ \tau_j(X) : j \in S \}$ is of the exponential family form with natural parameters $\pi_j(\theta)$ for $j \in S$[cite: 200, 207].
:::

::: {.proof}
Let $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):

$$
P(T_S = y_S | \theta) = \sum_{y_{S^c}} C(\theta) h^*(y) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j + \sum_{l \in S^c} \pi_l(\theta) y_l \right\}
$$

Since $\pi_l(\theta)$ is constant for $l \in S^c$, the term $\exp\{\sum_{l \in S^c} \pi_l y_l\}$ does not depend on $\theta$. We can group it with $h^*(y)$:

$$
= C(\theta) \exp\left\{ \sum_{j \in S} \pi_j(\theta) y_j \right\} \sum_{y_{S^c}} h^*(y) \exp\left\{ \sum_{l \in S^c} \pi_l y_l \right\}
$$

[cite_start]The sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form [cite: 216-236].
:::