---
title: "Exponential Families"
engine: knitr
format: 
  html: default
  pdf: default
---

## Exponential Families

::: {#def-exponential-family}

### Exponential Family
A family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\ell(\theta; x) = \log f(x|\theta)$, can be expressed as the sum of three distinct terms:

$$
\ell(\theta; x) = \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)
$$

Exponentiating this yields the density form:

$$
f(x|\theta) = h(x) \exp\left\{ \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) \right\}
$$

where:

* $\theta = (\theta_1, \dots, \theta_d)$ is the vector of model parameters.
* $\eta_i(\theta)$ are the **natural parameter functions**.
* $T(x) = (T_1(x), \dots, T_k(x))$ constitutes the vector of **sufficient statistics** for $\theta$.
* $A(\theta)$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.
* $h(x)$ is the base measure.

:::

### Examples of Exponential Families

::: {#exm-exponential-dist}

### Exponential Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter.
$$
f(\mathbf{x}|\theta) = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = -\frac{1}{\theta} \sum_{i=1}^n x_i - n \log \theta
$$

Identifying the components:

* $\eta_1(\theta) = -\frac{1}{\theta}$
* $T_1(\mathbf{x}) = \sum_{i=1}^n x_i$
* $A(\theta) = n \log \theta$
* $\log h(\mathbf{x}) = 0$

:::

::: {#exm-gamma-dist}

### Gamma Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$. The density is:
$$
f(\mathbf{x}|\theta) = [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i - \left[ n \log \Gamma(\alpha) + n\alpha \log \beta \right]
$$

Identifying the components:

* $\eta_1(\theta) = \alpha - 1$, $\quad T_1(\mathbf{x}) = \sum \log x_i$
* $\eta_2(\theta) = -\frac{1}{\beta}$, $\quad T_2(\mathbf{x}) = \sum x_i$
* $A(\theta) = n \log \Gamma(\alpha) + n\alpha \log \beta$

:::

::: {#exm-beta-dist}

### Beta Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\theta = (a, b)$.
$$
\ell(\theta; \mathbf{x}) = (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) - n \log B(a, b)
$$

This is an exponential family with $k=2$.

* $\eta_1 = a-1$, $T_1 = \sum \log x_i$
* $\eta_2 = b-1$, $T_2 = \sum \log(1-x_i)$
* $A(\theta) = n \log B(a, b)$

:::

::: {#exm-normal-dist}

### Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
$$

Identifying the components:

* $\eta_1 = \frac{\mu}{\sigma^2}$, $T_1 = \sum x_i$
* $\eta_2 = -\frac{1}{2\sigma^2}$, $T_2 = \sum x_i^2$
* $A(\theta) = \frac{n\mu^2}{2\sigma^2} + n \log \sigma + \frac{n}{2} \log(2\pi)$

:::

### Examples of Non-exponential Families

A model is **not** in the exponential family if the support depends on the parameter.

::: {#exm-uniform-dist}

### Uniform Distribution
Let $X \sim U(0, \theta)$.
$$
\ell(\theta; x) = -\log \theta + \log I(0 < x < \theta)
$$

The term $\log I(0 < x < \theta)$ couples $x$ and $\theta$ in a way that cannot be separated into a sum $\sum \eta_i(\theta) T_i(x)$.

:::

## Regular Families

In the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.

::: {#def-regular-family}

### Regular Family
A family of probability density functions is said to be a **Regular Family** if the support $\{x : f(x|\theta) > 0\}$ does not depend on the parameter $\theta$.

This condition allows for the interchange of differentiation and integration:
$$
\frac{\partial}{\partial \theta} \int \exp\{\ell(\theta; x)\} dx = \int \frac{\partial}{\partial \theta} \exp\{\ell(\theta; x)\} dx
$$

:::

### Why Regularity Matters {.unnumbered}

If a family is regular, we can differentiate the identity $\int f(x|\theta) dx = 1$ with respect to $\theta$. This yields the fundamental properties of the **Score Function** $\nabla \ell(\theta; X)$:

1.  **First Moment Identity:** The expected value of the score is zero.
    $$E_\theta \left[ \frac{\partial \ell(\theta; X)}{\partial \theta_j} \right] = 0$$

2.  **Second Moment Identity:** The Fisher Information is the negative expected Hessian.
    $$E_\theta \left[ \frac{\partial^2 \ell(\theta; X)}{\partial \theta_j \partial \theta_k} \right] = -E_\theta \left[ \frac{\partial \ell(\theta; X)}{\partial \theta_j} \frac{\partial \ell(\theta; X)}{\partial \theta_k} \right]$$

## Moments of Sufficient Statistics

### Means of Sufficient Statistics (General Case)

::: {#thm-moments-exp-family}

### Means via the Score Function
For a regular exponential family with log-likelihood $\ell(\theta; x) = \sum \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)$, the expectation of the sufficient statistics can be found by setting the expected score to zero:

$$
E_\theta \left[ \frac{\partial \ell(\theta; X)}{\partial \theta_j} \right] = 0
$$

Substituting the specific form of $\ell(\theta; X)$:

$$
\sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} E[T_i(X)] = \frac{\partial A(\theta)}{\partial \theta_j} \quad \text{for } j=1, \dots, d
$$

:::

::: {.proof}
The log-likelihood is:
$$
\ell(\theta; x) = \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)
$$

Differentiating with respect to $\theta_j$:
$$
\frac{\partial \ell}{\partial \theta_j} = \sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} T_i(x) - \frac{\partial A(\theta)}{\partial \theta_j}
$$

Taking the expectation and using the regularity condition $E[\frac{\partial \ell}{\partial \theta_j}] = 0$:
$$
E\left[ \sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} T_i(X) - \frac{\partial A(\theta)}{\partial \theta_j} \right] = 0
$$

$$
\sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} E[T_i(X)] = \frac{\partial A(\theta)}{\partial \theta_j}
$$

:::

### Natural Parameterization

::: {#def-natural-parameterization}

### Natural Parameterization
If the parameters are chosen such that $\eta_i(\theta) = \theta_i$ (implying $d=k$), the family is in **Canonical Form** (or Natural Parameterization). The log-likelihood simplifies to:

$$
\ell(\eta; x) = \sum_{i=1}^k \eta_i T_i(x) - A(\eta) + \log h(x)
$$

In this case, the moment generating properties become direct derivatives of the log-partition function $A(\eta)$.

**Mean:**
$$
E[T_i(X)] = \frac{\partial A(\eta)}{\partial \eta_i}
$$

**Variance/Covariance:**
$$
\text{Cov}(T_i(X), T_j(X)) = \frac{\partial^2 A(\eta)}{\partial \eta_i \partial \eta_j}
$$

:::

:::{#def-curvedexpfam}

### Full vs. Curved Exponential Families

* **Full Exponential Family:** When the natural parameters $\eta$ can vary independently in an open set of $\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).
* **Curved Exponential Family:** When the dimension of the parameter vector $\theta$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\eta(\theta)$ to lie on a non-linear curve or surface within the natural parameter space.

:::

::: {#exm-curved-normal-natural}

### Curved Exponential Family Example
Consider the $N(\theta, \theta^2)$ distribution ($d=1$). The log-likelihood is:
$$
\ell(\theta; x) = -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i - n \log \theta - \text{const}
$$

Here:

* $\eta_1(\theta) = -\frac{1}{2\theta^2}$, $T_1 = \sum x_i^2$
* $\eta_2(\theta) = \frac{1}{\theta}$, $T_2 = \sum x_i$

Since $d=1$ but $k=2$, and $\eta_1 = -\frac{1}{2}\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.

:::

::: {#exm-bernoulli-moments}

### Moments of the Binomial Distribution

Consider $n$ independent coin flips $X_1, \dots, X_n \sim \text{Bernoulli}(p)$. We find the mean and variance of $T = \sum X_i$.

1. **Log-Likelihood Form**
   $$
   \ell(\theta; x) = \log\left(\frac{p}{1-p}\right) \sum x_i + n \log(1-p)
   $$

   * Natural Parameter: $\eta = \log\left(\frac{p}{1-p}\right)$.
   * Log-Partition Function: $A(\eta) = -n \log(1-p) = n \log(1+e^\eta)$.

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = n \frac{e^\eta}{1+e^\eta} = np
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = n \frac{e^\eta}{(1+e^\eta)^2} = np(1-p)
   $$

:::

::: {#exm-exponential-moments}

### Moments of the Gamma Sufficient Statistic

Consider $X_i \sim \text{Exp}(\lambda)$. We find the moments of $T = \sum X_i$.

1. **Log-Likelihood Form**
   $$
   \ell(\lambda; x) = -\lambda \sum x_i + n \log \lambda
   $$

   * Natural Parameter: $\eta = -\lambda$.
   * Log-Partition Function: $A(\eta) = -n \log \lambda = -n \log(-\eta)$.

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = -n \frac{1}{-\eta}(-1) = -\frac{n}{\eta} = \frac{n}{\lambda}
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = \frac{n}{\eta^2} = \frac{n}{\lambda^2}
   $$

:::

::: {#exm-normal-sufficient-moments}

### Moments of Normal Sufficient Statistics

Consider $X_i \sim N(\mu, \sigma^2)$.

1. **Log-Likelihood Form**
   $$
   \ell(\theta; x) = \frac{\mu}{\sigma^2} \sum x_i - \frac{1}{2\sigma^2} \sum x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(\sigma^2) \right]
   $$
   
   Natural Parameters: $\eta_1 = \frac{\mu}{\sigma^2}, \eta_2 = -\frac{1}{2\sigma^2}$.
   
   Log-Partition Function (in terms of $\eta$):
   $$
   A(\eta) = -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2)
   $$

2. **First Moments (Means)**
   $$
   E[T_1] = E\left[\sum X_i\right] = \frac{\partial A}{\partial \eta_1} = -\frac{n\eta_1}{2\eta_2} = n\mu
   $$
   $$
   E[T_2] = E\left[\sum X_i^2\right] = \frac{\partial A}{\partial \eta_2} = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2\eta_2} = n(\mu^2 + \sigma^2)
   $$

3. **Second Moment (Covariance)**
   $$
   \text{Cov}(T_1, T_2) = \frac{\partial^2 A}{\partial \eta_1 \partial \eta_2} = \frac{n\eta_1}{2\eta_2^2} = 2n\mu\sigma^2
   $$

:::