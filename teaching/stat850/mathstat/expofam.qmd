---
title: "Exponential Families"
engine: knitr
format: 
  html: default
  pdf: default
---

## Regular Families, Score Vector, and Fisher Information

In the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.

::: {#def-regular-family}


### Regular Family
A family of probability density functions is said to be a **Regular Family** if the support $\{x : f(x|\theta) > 0\}$ does not depend on the parameter $\theta$.

This condition allows for the interchange of differentiation and integration:
$$
\frac{\partial}{\partial \theta} \int \exp\{\ell(\theta; x)\} dx = \int \frac{\partial}{\partial \theta} \exp\{\ell(\theta; x)\} dx
$$

:::

### Definition of Score and Fisher Information

Before stating the theorem, we define the following notations for the score and information in the context of a parameter vector $\theta = (\theta_1, \dots, \theta_p)^T \in \mathbb{R}^p$:

:::{#def-score-fisher}

1.  **Score Vector ($U$):** The gradient of the log-likelihood. It is a random column vector of dimension $p \times 1$.
    $$
    U(\theta; X) = \nabla \ell(\theta; X) = \frac{\partial \ell(\theta; X)}{\partial \theta} = 
    \begin{bmatrix}
    \frac{\partial \ell(\theta; X)}{\partial \theta_1} \\[6pt]
    \frac{\partial \ell(\theta; X)}{\partial \theta_2} \\[6pt]
    \vdots \\[6pt]
    \frac{\partial \ell(\theta; X)}{\partial \theta_p}
    \end{bmatrix}
    $$

2.  **Observed Information Matrix ($J$):** The negative Hessian of the log-likelihood. It is a symmetric random matrix of dimension $p \times p$, measuring the curvature of the log-likelihood surface.
    $$
    J(\theta; X) = - \nabla^2 \ell(\theta; X) = - \frac{\partial^2 \ell(\theta; X)}{\partial \theta \partial \theta^T} = -
    \begin{bmatrix}
    \frac{\partial^2 \ell}{\partial \theta_1^2} & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_p} \\[8pt]
    \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_p} \\[8pt]
    \vdots & \vdots & \ddots & \vdots \\[8pt]
    \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_p^2}
    \end{bmatrix}
    $$

3.  **(Expected) Fisher Information Matrix ($I$):** The covariance matrix of the score vector. It is a deterministic $p \times p$ matrix (for a fixed $\theta$).
    $$
    I(\theta) = E_\theta \left[ J(\theta; X) \right]
    $$

:::

::: {#thm-score-identities}
## Bartlett's Identities: Mean and Covariance of Score Vector

Let $\{f(x|\theta) : \theta \in \Theta\}$ be a regular family of probability density functions. The following identities hold relating the moments of the score vector $U(\theta; X)$ and the observed information matrix $J(\theta; X)$:

1.  **First Moment Identity:** The expected score is zero.
    $$
    E_\theta [ U(\theta; X) ] = 0
    $$

2.  **Second Moment Identity:** The expected observed information equals the covariance of the score vector (Fisher Information).
    $$
    \text{Cov}_\theta \left( U(\theta; X) \right) = E_\theta [ J(\theta; X) ] = I(\theta)
    $$

:::

:::{#rem-score-moments}
The only assumption in the theorem above is that the families are regular. Therefore, we do not need to assume the log-likelihood $\ell(\theta)$ is "well-behaved" (e.g., approximately quadratic or indepedence within $X$) for these two identities to hold.
:::

::: {.proof}
**1. Proof of the First Moment Identity**

We start with the fundamental property that a density function integrates to 1 over the sample space of $X$:
$$
\int f(x|\theta) \, dx = 1
$$
Differentiating both sides with respect to the parameter vector $\theta$:
$$
\nabla_\theta \int f(x|\theta) \, dx = 0
$$
Assuming regularity allows us to interchange differentiation and integration:
$$
\int \nabla_\theta f(x|\theta) \, dx = 0
$$
Using the identity $\nabla_\theta f(x|\theta) = f(x|\theta) \nabla_\theta \log f(x|\theta) = f(x|\theta) U(\theta; x)$:
$$
\int U(\theta; x) f(x|\theta) \, dx = 0
$$
This is precisely the definition of the expectation:
$$
E_\theta [ U(\theta; X) ] = 0
$$

**2. Proof of the Second Moment Identity**

We differentiate the result of the First Moment Identity ($E[U(\theta; X)]=0$) with respect to $\theta^T$.
$$
\nabla_{\theta^T} \int U(\theta; x) f(x|\theta) \, dx = 0
$$
Applying the product rule inside the integral:
$$
\int \left[ \left( \nabla_{\theta^T} U(\theta; x) \right) f(x|\theta) + U(\theta; x) \left( \nabla_{\theta^T} f(x|\theta) \right) \right] dx = 0
$$
We analyze the two terms in the bracket:

* **Term 1:** $\nabla_{\theta^T} U(\theta; x)$ is the Jacobian of the score, which is the Hessian of the log-likelihood, $\nabla^2 \ell(\theta; x)$. By definition, this is $-J(\theta; x)$.
* **Term 2:** We use the identity $\nabla_{\theta^T} f(x|\theta) = f(x|\theta) (\nabla_{\theta} \log f(x|\theta))^T = f(x|\theta) U(\theta; x)^T$.

Substituting these back into the integral:
$$
\int \left[ -J(\theta; x) f(x|\theta) + U(\theta; x) U(\theta; x)^T f(x|\theta) \right] dx = 0
$$
This simplifies to expectations:
$$
-E_\theta [ J(\theta; X) ] + E_\theta [ U(\theta; X) U(\theta; X)^T ] = 0
$$
Rearranging gives:
$$
E_\theta [ J(\theta; X) ] = E_\theta [ U(\theta; X) U(\theta; X)^T ]
$$
Finally, recall the definition of the covariance matrix for a random vector with zero mean. Since $E_\theta[U(\theta; X)] = 0$, we have:
$$
\text{Cov}_\theta(U(\theta; X)) = E_\theta[U(\theta; X)U(\theta; X)^T] - E_\theta[U(\theta; X)]E_\theta[U(\theta; X)]^T = E_\theta[U(\theta; X)U(\theta; X)^T]
$$
Therefore, we conclude:
$$
\text{Cov}_\theta(U(\theta; X))=E_\theta [ J(\theta; X) ] = I(\theta)
$$
:::


## Exponential Families

::: {#def-exponential-family}

### Exponential Family
A family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\ell(\theta; x) = \log f(x|\theta)$, can be expressed as the sum of three distinct terms:

$$
\ell(\theta; x) = \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)
$$

Exponentiating this yields the density form:

$$
f(x|\theta) = h(x) \exp\left\{ \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) \right\}
$$

where:

* $\theta = (\theta_1, \dots, \theta_d)$ is the vector of model parameters.
* $\eta_i(\theta)$ are the **natural parameter functions**.
* $T(x) = (T_1(x), \dots, T_k(x))$ constitutes the vector of **sufficient statistics** for $\theta$.
* $A(\theta)$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.
* $h(x)$ is the base measure.

:::

### Examples of Exponential Families

::: {#exm-exponential-dist}

### Exponential Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter.
$$
f(\mathbf{x}|\theta) = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = -\frac{1}{\theta} \sum_{i=1}^n x_i - n \log \theta
$$

Identifying the components:

* $\eta_1(\theta) = -\frac{1}{\theta}$
* $T_1(\mathbf{x}) = \sum_{i=1}^n x_i$
* $A(\theta) = n \log \theta$
* $\log h(\mathbf{x}) = 0$

:::

::: {#exm-gamma-dist}

### Gamma Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$. The density is:
$$
f(\mathbf{x}|\theta) = [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i - \left[ n \log \Gamma(\alpha) + n\alpha \log \beta \right]
$$

Identifying the components:

* $\eta_1(\theta) = \alpha - 1$, $\quad T_1(\mathbf{x}) = \sum \log x_i$
* $\eta_2(\theta) = -\frac{1}{\beta}$, $\quad T_2(\mathbf{x}) = \sum x_i$
* $A(\theta) = n \log \Gamma(\alpha) + n\alpha \log \beta$

:::

::: {#exm-beta-dist}

### Beta Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\theta = (a, b)$.
$$
\ell(\theta; \mathbf{x}) = (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) - n \log B(a, b)
$$

This is an exponential family with $k=2$.

* $\eta_1 = a-1$, $T_1 = \sum \log x_i$
* $\eta_2 = b-1$, $T_2 = \sum \log(1-x_i)$
* $A(\theta) = n \log B(a, b)$

:::

::: {#exm-normal-dist}

### Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
$$

Identifying the components:

* $\eta_1 = \frac{\mu}{\sigma^2}$, $T_1 = \sum x_i$
* $\eta_2 = -\frac{1}{2\sigma^2}$, $T_2 = \sum x_i^2$
* $A(\theta) = \frac{n\mu^2}{2\sigma^2} + n \log \sigma + \frac{n}{2} \log(2\pi)$

:::

### Examples of Non-exponential Families

A model is **not** in the exponential family if the support depends on the parameter.

::: {#exm-uniform-dist}

### Uniform Distribution
Let $X \sim U(0, \theta)$.
$$
\ell(\theta; x) = -\log \theta + \log I(0 < x < \theta)
$$

The term $\log I(0 < x < \theta)$ couples $x$ and $\theta$ in a way that cannot be separated into a sum $\sum \eta_i(\theta) T_i(x)$.

:::


## Moments of Sufficient Statistics of Exponential Families

### Means of Sufficient Statistics (General Case)

::: {#thm-moments-exp-family}

### Means via the Score Function
For a regular exponential family with log-likelihood $\ell(\theta; x) = \sum \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)$, the expectation of the sufficient statistics can be found by setting the expected score to zero:

$$
E_\theta \left[ \frac{\partial \ell(\theta; X)}{\partial \theta_j} \right] = 0
$$

Substituting the specific form of $\ell(\theta; X)$:

$$
\sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} E[T_i(X)] = \frac{\partial A(\theta)}{\partial \theta_j} \quad \text{for } j=1, \dots, d
$$

:::

::: {.proof}
The log-likelihood is:
$$
\ell(\theta; x) = \sum_{i=1}^k \eta_i(\theta) T_i(x) - A(\theta) + \log h(x)
$$

Differentiating with respect to $\theta_j$:
$$
\frac{\partial \ell}{\partial \theta_j} = \sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} T_i(x) - \frac{\partial A(\theta)}{\partial \theta_j}
$$

Taking the expectation and using the regularity condition $E[\frac{\partial \ell}{\partial \theta_j}] = 0$:
$$
E\left[ \sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} T_i(X) - \frac{\partial A(\theta)}{\partial \theta_j} \right] = 0
$$

$$
\sum_{i=1}^k \frac{\partial \eta_i(\theta)}{\partial \theta_j} E[T_i(X)] = \frac{\partial A(\theta)}{\partial \theta_j}
$$

:::

### Natural Parameterization

::: {#def-natural-parameterization}
### Natural Parameterization (Canonical Form)

If the parameterization is chosen such that the natural parameters are the components of the parameter vector itself (i.e., $\eta(\theta) = \theta$), the exponential family is said to be in **Canonical Form** or **Natural Parameterization**.

The log-likelihood for the natural parameter vector $\eta = (\eta_1, \dots, \eta_k)^T$ simplifies to:
$$
\ell(\eta; x) = \sum_{i=1}^k \eta_i T_i(x) - A(\eta) + \log h(x)
$$
or in vector notation:
$$
\ell(\eta; x) = \eta^T T(x) - A(\eta) + \log h(x)
$$
where $A(\eta)$ is the log-partition function.
:::


:::{#def-curvedexpfam}

### Full vs. Curved Exponential Families

* **Full Exponential Family:** When the natural parameters $\eta$ can vary independently in an open set of $\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).
* **Curved Exponential Family:** When the dimension of the parameter vector $\theta$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\eta(\theta)$ to lie on a non-linear curve or surface within the natural parameter space.

:::

::: {#exm-curved-normal-natural}

### Curved Exponential Family Example
Consider the $N(\theta, \theta^2)$ distribution ($d=1$). The log-likelihood is:
$$
\ell(\theta; x) = -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i - n \log \theta - \text{const}
$$

Here:

* $\eta_1(\theta) = -\frac{1}{2\theta^2}$, $T_1 = \sum x_i^2$
* $\eta_2(\theta) = \frac{1}{\theta}$, $T_2 = \sum x_i$

Since $d=1$ but $k=2$, and $\eta_1 = -\frac{1}{2}\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.

:::

### Mean and Variance of Sufficient Statitics of Exponential Families

::: {#thm-cumulant-generating}
### Mean and Variance of Sufficient Statitics

For an exponential family in canonical form, the log-partition function $A(\eta)$ acts as the **Cumulant Generating Function** for the sufficient statistic vector $T(X)$. The derivatives of $A(\eta)$ yield the moments of $T(X)$ as follows:

1.  **Mean (First Derivative):**
    $$
    E[T_i(X)] = \frac{\partial A(\eta)}{\partial \eta_i}
    $$
    In vector form: $E[T(X)] = \nabla A(\eta)$.

2.  **Covariance (Second Derivative):**
    $$
    \text{Cov}(T_i(X), T_j(X)) = \frac{\partial^2 A(\eta)}{\partial \eta_i \partial \eta_j}
    $$
    In matrix form: $\text{Var}(T(X)) = \nabla^2 A(\eta)$.


:::

::: {.proof}
**Derivation**

These results follow directly from Bartlett's Identities (Theorem @thm-score-identities) applied to the canonical log-likelihood:
$$
\ell(\eta; x) = \eta^T T(x) - A(\eta) + \log h(x)
$$

**For the Mean:**
The score function (gradient of $\ell$) is:
$$
U(\eta) = \nabla_\eta \ell(\eta; x) = T(x) - \nabla A(\eta)
$$
By the First Moment Identity, $E[U(\eta)] = 0$:
$$
E[T(X) - \nabla A(\eta)] = 0 \implies E[T(X)] = \nabla A(\eta)
$$

**For the Covariance:**
The observed information (negative Hessian of $\ell$) is:
$$
J(\eta) = -\nabla^2_\eta \ell(\eta; x) = -\nabla_\eta (T(x) - \nabla A(\eta)) = \nabla^2 A(\eta)
$$
Note that $T(x)$ is constant with respect to $\eta$, so its derivative vanishes.
By the Second Moment Identity, $I(\eta) = E[J(\eta)] = \text{Cov}(U(\eta))$.
Since $U(\eta) = T(X) - \text{constant}$, $\text{Cov}(U(\eta)) = \text{Cov}(T(X))$.
Therefore:
$$
\text{Cov}(T(X)) = E[\nabla^2 A(\eta)] = \nabla^2 A(\eta)
$$
:::

::: {#exm-bernoulli-moments}

### Moments of the Binomial Distribution

Consider $n$ independent coin flips $X_1, \dots, X_n \sim \text{Bernoulli}(p)$. We find the mean and variance of $T = \sum X_i$.

1. **Log-Likelihood Form**
   $$
   \ell(\theta; x) = \log\left(\frac{p}{1-p}\right) \sum x_i + n \log(1-p)
   $$

   * Natural Parameter: $\eta = \log\left(\frac{p}{1-p}\right)$.
   * Log-Partition Function: $A(\eta) = -n \log(1-p) = n \log(1+e^\eta)$.

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = n \frac{e^\eta}{1+e^\eta} = np
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = n \frac{e^\eta}{(1+e^\eta)^2} = np(1-p)
   $$

:::

::: {#exm-exponential-moments}

### Moments of the Gamma Sufficient Statistic

Consider $X_i \sim \text{Exp}(\lambda)$. We find the moments of $T = \sum X_i$.

1. **Log-Likelihood Form**
   $$
   \ell(\lambda; x) = -\lambda \sum x_i + n \log \lambda
   $$

   * Natural Parameter: $\eta = -\lambda$.
   * Log-Partition Function: $A(\eta) = -n \log \lambda = -n \log(-\eta)$.

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = -n \frac{1}{-\eta}(-1) = -\frac{n}{\eta} = \frac{n}{\lambda}
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = \frac{n}{\eta^2} = \frac{n}{\lambda^2}
   $$

:::

::: {#exm-normal-sufficient-moments}

### Moments of Normal Sufficient Statistics

Consider $X_i \sim N(\mu, \sigma^2)$.

1. **Log-Likelihood Form**
   $$
   \ell(\theta; x) = \frac{\mu}{\sigma^2} \sum x_i - \frac{1}{2\sigma^2} \sum x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(\sigma^2) \right]
   $$
   
   Natural Parameters: $\eta_1 = \frac{\mu}{\sigma^2}, \eta_2 = -\frac{1}{2\sigma^2}$.
   
   Log-Partition Function (in terms of $\eta$):
   $$
   A(\eta) = -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2)
   $$

2. **First Moments (Means)**
   $$
   E[T_1] = E\left[\sum X_i\right] = \frac{\partial A}{\partial \eta_1} = -\frac{n\eta_1}{2\eta_2} = n\mu
   $$
   $$
   E[T_2] = E\left[\sum X_i^2\right] = \frac{\partial A}{\partial \eta_2} = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2\eta_2} = n(\mu^2 + \sigma^2)
   $$

3. **Second Moment (Covariance)**
   $$
   \text{Cov}(T_1, T_2) = \frac{\partial^2 A}{\partial \eta_1 \partial \eta_2} = \frac{n\eta_1}{2\eta_2^2} = 2n\mu\sigma^2
   $$

:::