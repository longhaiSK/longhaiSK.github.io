<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Likelihood Theory – Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mle.html" rel="next">
<link href="./sufficiency.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0de45b8b5f35a93596f1d788d60c4c11.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-4ae8bba48d879873ce5a6b72ff9b51ba.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/bookstyles.css">
<script src="resources/num_eq.js"></script>
<script src="resources/merge-toc.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./loglike.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Likelihood Theory</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introstatinf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sufficiency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sufficient Statistic</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./loglike.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Likelihood Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./umvue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Minimum Variance Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Likelihood-based Hypothesis Testing</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#definitions-and-notations" id="toc-definitions-and-notations" class="nav-link active" data-scroll-target="#definitions-and-notations"><span class="header-section-number">5.1</span> Definitions and Notations</a>
  <ul>
  <li><a href="#regular-family" id="toc-regular-family" class="nav-link" data-scroll-target="#regular-family"><span class="header-section-number">5.1.1</span> Regular Family</a></li>
  <li><a href="#score-and-fisher-information" id="toc-score-and-fisher-information" class="nav-link" data-scroll-target="#score-and-fisher-information"><span class="header-section-number">5.1.2</span> Score and Fisher Information</a></li>
  </ul></li>
  <li><a href="#mean-and-covariance-of-score-vector" id="toc-mean-and-covariance-of-score-vector" class="nav-link" data-scroll-target="#mean-and-covariance-of-score-vector"><span class="header-section-number">5.2</span> Mean and Covariance of Score Vector</a></li>
  <li><a href="#cramer-rao-lower-bound" id="toc-cramer-rao-lower-bound" class="nav-link" data-scroll-target="#cramer-rao-lower-bound"><span class="header-section-number">5.3</span> Cramer-Rao Lower Bound</a></li>
  <li><a href="#multivariate-cramer-rao-lower-bound" id="toc-multivariate-cramer-rao-lower-bound" class="nav-link" data-scroll-target="#multivariate-cramer-rao-lower-bound"><span class="header-section-number">5.4</span> Multivariate Cramer-Rao Lower Bound</a>
  <ul>
  <li><a href="#example-with-exponential-likelihood" id="toc-example-with-exponential-likelihood" class="nav-link" data-scroll-target="#example-with-exponential-likelihood"><span class="header-section-number">5.4.1</span> Example with Exponential Likelihood</a></li>
  </ul></li>
  <li><a href="#exponential-families" id="toc-exponential-families" class="nav-link" data-scroll-target="#exponential-families"><span class="header-section-number">5.5</span> Exponential Families</a>
  <ul>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">5.5.1</span> Examples</a>
  <ul class="collapse">
  <li><a href="#exponential-distribution" id="toc-exponential-distribution" class="nav-link" data-scroll-target="#exponential-distribution">Exponential Distribution</a></li>
  <li><a href="#gamma-distribution" id="toc-gamma-distribution" class="nav-link" data-scroll-target="#gamma-distribution">Gamma Distribution</a></li>
  <li><a href="#beta-distribution" id="toc-beta-distribution" class="nav-link" data-scroll-target="#beta-distribution">Beta Distribution</a></li>
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution">Normal Distribution</a></li>
  </ul></li>
  <li><a href="#examples-of-non-exponential-families" id="toc-examples-of-non-exponential-families" class="nav-link" data-scroll-target="#examples-of-non-exponential-families"><span class="header-section-number">5.5.2</span> Examples of Non-exponential Families</a>
  <ul class="collapse">
  <li><a href="#uniform-distribution" id="toc-uniform-distribution" class="nav-link" data-scroll-target="#uniform-distribution">Uniform Distribution</a></li>
  </ul></li>
  <li><a href="#moments-of-sufficient-statistics-of-exponential-families" id="toc-moments-of-sufficient-statistics-of-exponential-families" class="nav-link" data-scroll-target="#moments-of-sufficient-statistics-of-exponential-families"><span class="header-section-number">5.5.3</span> Moments of Sufficient Statistics of Exponential Families</a>
  <ul class="collapse">
  <li><a href="#means-of-sufficient-statistics-general-case" id="toc-means-of-sufficient-statistics-general-case" class="nav-link" data-scroll-target="#means-of-sufficient-statistics-general-case"><span class="header-section-number">5.5.3.1</span> Means of Sufficient Statistics (General Case)</a></li>
  <li><a href="#natural-parameterization" id="toc-natural-parameterization" class="nav-link" data-scroll-target="#natural-parameterization"><span class="header-section-number">5.5.3.2</span> Natural Parameterization</a></li>
  <li><a href="#mean-and-variance-of-sufficient-statistics" id="toc-mean-and-variance-of-sufficient-statistics" class="nav-link" data-scroll-target="#mean-and-variance-of-sufficient-statistics"><span class="header-section-number">5.5.3.3</span> Mean and Variance of Sufficient Statistics</a></li>
  <li><a href="#examples-1" id="toc-examples-1" class="nav-link" data-scroll-target="#examples-1"><span class="header-section-number">5.5.3.4</span> Examples</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Likelihood Theory</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="definitions-and-notations" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="definitions-and-notations"><span class="header-section-number">5.1</span> Definitions and Notations</h2>
<section id="regular-family" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="regular-family"><span class="header-section-number">5.1.1</span> Regular Family</h3>
<div id="def-regular-family" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Regular Families)</strong></span> A family of probability density functions is said to be a <strong>Regular Family</strong> if the support <span class="math inline">\(\{\mathbf{x} : f(\mathbf{x}|\boldsymbol{\theta}) &gt; 0\}\)</span> does not depend on the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>This condition allows for the interchange of differentiation and integration: <span class="math display">\[
\nabla_{\boldsymbol{\theta}} \int \exp\{\ell(\boldsymbol{\theta}; \mathbf{x})\} d\mathbf{x} = \int \nabla_{\boldsymbol{\theta}} \exp\{\ell(\boldsymbol{\theta}; \mathbf{x})\} d\mathbf{x}
\]</span></p>
</div>
</section>
<section id="score-and-fisher-information" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="score-and-fisher-information"><span class="header-section-number">5.1.2</span> Score and Fisher Information</h3>
<p>Before stating the theorem, we define the following notations for the score and information in the context of a parameter vector <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)^T \in \mathbb{R}^p\)</span>:</p>
<div id="def-score-fisher" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Score Vector (<span class="math inline">\(\mathbf{U}\)</span>):</strong> The gradient of the log-likelihood. It is a random column vector of dimension <span class="math inline">\(p \times 1\)</span>. <span class="math display">\[
\mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) = \nabla \ell(\boldsymbol{\theta}; \mathbf{X}) = \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \boldsymbol{\theta}} =
\begin{bmatrix}
\frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_1} \\[6pt]
\frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_2} \\[6pt]
\vdots \\[6pt]
\frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_p}
\end{bmatrix}
\]</span></p></li>
<li><p><strong>Observed Information Matrix (<span class="math inline">\(\mathbf{J}\)</span>):</strong> The negative Hessian of the log-likelihood. It is a symmetric random matrix of dimension <span class="math inline">\(p \times p\)</span>, measuring the curvature of the log-likelihood surface. <span class="math display">\[
\mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) = - \nabla^2 \ell(\boldsymbol{\theta}; \mathbf{X}) = - \frac{\partial^2 \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^T} = -
\begin{bmatrix}
\frac{\partial^2 \ell}{\partial \theta_1^2} &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} &amp; \cdots &amp; \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_p} \\[8pt]
\frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_2^2} &amp; \cdots &amp; \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_p} \\[8pt]
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[8pt]
\frac{\partial^2 \ell}{\partial \theta_p \partial \theta_1} &amp; \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_2} &amp; \cdots &amp; \frac{\partial^2 \ell}{\partial \theta_p^2}
\end{bmatrix}
\]</span></p></li>
<li><p><strong>(Expected) Fisher Information Matrix (<span class="math inline">\(\mathbf{I}\)</span>):</strong> The covariance matrix of the score vector. It is a deterministic <span class="math inline">\(p \times p\)</span> matrix (for a fixed <span class="math inline">\(\boldsymbol{\theta}\)</span>). <span class="math display">\[
\mathbf{I}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}} \left[ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) \right]
\]</span></p></li>
</ol>
</div>
</section>
</section>
<section id="mean-and-covariance-of-score-vector" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="mean-and-covariance-of-score-vector"><span class="header-section-number">5.2</span> Mean and Covariance of Score Vector</h2>
<div id="thm-score-identities" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (Bartlett’s Identities: Mean and Covariance of Score Vector)</strong></span> Let <span class="math inline">\(\{f(\mathbf{x}|\boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}\)</span> be a regular family of probability density functions. The following identities hold relating the moments of the score vector <span class="math inline">\(\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})\)</span> and the observed information matrix <span class="math inline">\(\mathbf{J}(\boldsymbol{\theta}; \mathbf{X})\)</span>:</p>
<ol type="1">
<li><p><strong>First Moment Identity:</strong> The expected score is zero vector. <span class="math display">\[
E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{0}
\]</span></p></li>
<li><p><strong>Second Moment Identity:</strong> The expected observed information equals the covariance of the score vector (Fisher Information). <span class="math display">\[
\text{Cov}_{\boldsymbol{\theta}} \left( \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \right) = E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{I}(\boldsymbol{\theta})
\]</span></p></li>
</ol>
</div>
<div id="rem-score-moments" class="proof remark">
<p><span class="proof-title"><em>Remark 5.1</em>. </span>The only assumption in the theorem above is that the families are regular. Therefore, we do not need to assume the log-likelihood <span class="math inline">\(\ell(\boldsymbol{\theta})\)</span> is “well-behaved” (e.g., approximately quadratic or independence within <span class="math inline">\(\mathbf{X}\)</span>) for these two identities to hold.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li>Proof of the First Moment Identity</li>
</ol>
<p>We start with the fundamental property that a density function integrates to 1 over the sample space of <span class="math inline">\(\mathbf{X}\)</span>: <span class="math display">\[
\int f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = 1
\]</span> Differentiating both sides with respect to the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>: <span class="math display">\[
\nabla_{\boldsymbol{\theta}} \int f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
\]</span> Assuming regularity allows us to interchange differentiation and integration: <span class="math display">\[
\int \nabla_{\boldsymbol{\theta}} f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
\]</span> Using the identity <span class="math inline">\(\nabla_{\boldsymbol{\theta}} f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) \nabla_{\boldsymbol{\theta}} \log f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})\)</span>: <span class="math display">\[
\int \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
\]</span> This is precisely the definition of the expectation: <span class="math display">\[
E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{0}
\]</span></p>
<ol start="2" type="1">
<li>Proof of the Second Moment Identity</li>
</ol>
<p>We differentiate the result of the First Moment Identity (<span class="math inline">\(E[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]=\mathbf{0}\)</span>) with respect to <span class="math inline">\(\boldsymbol{\theta}^T\)</span>. <span class="math display">\[
\nabla_{\boldsymbol{\theta}^T} \int \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
\]</span> Applying the product rule inside the integral (remembering <span class="math inline">\(\mathbf{U}\)</span> is a vector): <span class="math display">\[
\int \left[ \left( \nabla_{\boldsymbol{\theta}^T} \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \right) f(\mathbf{x}|\boldsymbol{\theta}) + \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \left( \nabla_{\boldsymbol{\theta}^T} f(\mathbf{x}|\boldsymbol{\theta}) \right) \right] d\mathbf{x} = \mathbf{0}
\]</span> We analyze the two terms in the bracket:</p>
<ul>
<li><strong>Term 1:</strong> <span class="math inline">\(\nabla_{\boldsymbol{\theta}^T} \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})\)</span> is the Jacobian of the score, which is the Hessian of the log-likelihood, <span class="math inline">\(\nabla^2 \ell(\boldsymbol{\theta}; \mathbf{x})\)</span>. By definition, this is <span class="math inline">\(-\mathbf{J}(\boldsymbol{\theta}; \mathbf{x})\)</span>.</li>
<li><strong>Term 2:</strong> We use the identity <span class="math inline">\(\nabla_{\boldsymbol{\theta}^T} f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) (\nabla_{\boldsymbol{\theta}} \log f(\mathbf{x}|\boldsymbol{\theta}))^T = f(\mathbf{x}|\boldsymbol{\theta}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})^T\)</span>.</li>
</ul>
<p>Substituting these back into the integral: <span class="math display">\[
\int \left[ -\mathbf{J}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) + \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})^T f(\mathbf{x}|\boldsymbol{\theta}) \right] d\mathbf{x} = \mathbf{0}
\]</span> This simplifies to expectations: <span class="math display">\[
-E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] + E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T ] = \mathbf{0}
\]</span> Rearranging gives: <span class="math display">\[
E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T ]
\]</span> Finally, recall the definition of the covariance matrix for a random vector with zero mean. Since <span class="math inline">\(E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})] = \mathbf{0}\)</span>, we have: <span class="math display">\[
\text{Cov}_{\boldsymbol{\theta}}(\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})) = E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T] - E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]^T = E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T]
\]</span> Therefore, we conclude: <span class="math display">\[
\text{Cov}_{\boldsymbol{\theta}}(\mathbf{U}(\boldsymbol{\theta}; \mathbf{X}))=E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{I}(\boldsymbol{\theta})
\]</span></p>
</div>
</section>
<section id="cramer-rao-lower-bound" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="cramer-rao-lower-bound"><span class="header-section-number">5.3</span> Cramer-Rao Lower Bound</h2>
<p>In estimation theory, we often wish to know the limit of how well a parameter can be estimated. The following theorem provides a lower bound on the variance of any estimator.</p>
<div id="thm-crlb" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2 (Cramer-Rao Lower Bound for Scalar Estimator)</strong></span> Let <span class="math inline">\(X\)</span> be a random variable with probability density function (or probability mass function) <span class="math inline">\(f(x|\theta)\)</span>, where <span class="math inline">\(\theta \in \Theta\)</span> is a scalar unknown parameter. Let <span class="math inline">\(T(X)\)</span> be any estimator with finite variance, and let <span class="math inline">\(m(\theta) = E_\theta[T(X)]\)</span> denote its expectation.</p>
<p>Assume the following <strong>regularity conditions</strong> hold:</p>
<ol type="1">
<li><p>The support of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\mathcal{X} = \{x : f(x|\theta) &gt; 0\}\)</span>, does not depend on <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The differentiation with respect to <span class="math inline">\(\theta\)</span> and integration (or summation) with respect to <span class="math inline">\(x\)</span> can be interchanged.</p></li>
</ol>
<p>Then, the variance of <span class="math inline">\(T(X)\)</span> satisfies:</p>
<p><span class="math display">\[
\text{Var}_\theta(T(X)) \ge \frac{[m'(\theta)]^2}{I(\theta)}
\]</span></p>
<p>where <span class="math inline">\(I(\theta) = E_\theta \left[ \left( \frac{\partial}{\partial \theta} \log f(X|\theta) \right)^2 \right]\)</span> is the scalar Fisher Information.</p>
<p><strong>Particular Case:</strong> If <span class="math inline">\(T(X)\)</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\(\theta\)</span> (i.e., <span class="math inline">\(m(\theta) = \theta\)</span> and <span class="math inline">\(m'(\theta)=1\)</span>), then:</p>
<p><span class="math display">\[
\text{Var}_\theta(T(X)) \ge \frac{1}{I(\theta)}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(U = \frac{\partial}{\partial \theta} \log f(X|\theta)\)</span> be the scalar Score function. From the properties of the Score function under the stated regularity conditions, we know that the score has mean zero and variance equal to the Fisher Information:</p>
<p><span class="math display">\[
E_\theta[U] = 0 \quad \text{and} \quad \text{Var}_\theta(U) = I(\theta)
\]</span></p>
<p>Consider the covariance between the estimator <span class="math inline">\(T(X)\)</span> and the Score <span class="math inline">\(U\)</span>. By the Cauchy-Schwarz inequality (applied to covariance), we have:</p>
<p><span class="math display">\[
[\text{Cov}_\theta(T, U)]^2 \le \text{Var}_\theta(T) \text{Var}_\theta(U)
\]</span></p>
<p>We now evaluate the covariance term explicitly. By definition:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cov}_\theta(T, U) &amp;= E_\theta[T(X) U] - E_\theta[T]E_\theta[U] \\
&amp;= E_\theta\left[ T(X) \frac{\partial}{\partial \theta} \log f(X|\theta) \right] - m(\theta) \cdot 0 \\
&amp;= \int T(x) \left( \frac{\partial \log f(x|\theta)}{\partial \theta} \right) f(x|\theta) \, dx \\
&amp;= \int T(x) \left( \frac{1}{f(x|\theta)} \frac{\partial f(x|\theta)}{\partial \theta} \right) f(x|\theta) \, dx \\
&amp;= \int T(x) \frac{\partial f(x|\theta)}{\partial \theta} \, dx
\end{aligned}
\]</span></p>
<p>Invoking the regularity condition that allows the interchange of derivative and integral, we move the derivative outside the integral:</p>
<p><span class="math display">\[
\text{Cov}_\theta(T, U) = \frac{\partial}{\partial \theta} \int T(x) f(x|\theta) \, dx = \frac{\partial}{\partial \theta} E_\theta[T(X)] = m'(\theta)
\]</span></p>
<p>Substituting this result and <span class="math inline">\(\text{Var}_\theta(U) = I(\theta)\)</span> back into the covariance inequality:</p>
<p><span class="math display">\[
[m'(\theta)]^2 \le \text{Var}_\theta(T) \cdot I(\theta)
\]</span></p>
<p>Rearranging the terms yields the desired lower bound:</p>
<p><span class="math display">\[
\text{Var}_\theta(T(X)) \ge \frac{[m'(\theta)]^2}{I(\theta)}
\]</span></p>
</div>
</section>
<section id="multivariate-cramer-rao-lower-bound" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="multivariate-cramer-rao-lower-bound"><span class="header-section-number">5.4</span> Multivariate Cramer-Rao Lower Bound</h2>
<div id="thm-multivariate-crlb" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3 (Multivariate Cramer-Rao Lower Bound)</strong></span> Let <span class="math inline">\(\mathbf{X}\)</span> be a random vector with density <span class="math inline">\(f(\mathbf{x}|\boldsymbol{\theta})\)</span>, where <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^p\)</span> is a vector of unknown parameters. Let <span class="math inline">\(\mathbf{T}(\mathbf{X}) \in \mathbb{R}^k\)</span> be any estimator with finite covariance matrix, and let <span class="math inline">\(\mathbf{m}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}}[\mathbf{T}(\mathbf{X})]\)</span> denote its expectation vector.</p>
<p>Let <span class="math inline">\(\mathbf{I}(\boldsymbol{\theta})\)</span> be the <span class="math inline">\(p \times p\)</span> Fisher Information Matrix:</p>
<p><span class="math display">\[
\mathbf{I}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}} \left[ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^\top \right]
\]</span></p>
<p>Let <span class="math inline">\(\mathbf{D}(\boldsymbol{\theta}) = \frac{\partial \mathbf{m}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\)</span> be the <span class="math inline">\(k \times p\)</span> Jacobian matrix of the expectation, where <span class="math inline">\(D_{ij} = \frac{\partial m_i}{\partial \theta_j}\)</span>.</p>
<p>Under standard regularity conditions, the covariance matrix of <span class="math inline">\(\mathbf{T}\)</span> satisfies the inequality:</p>
<p><span class="math display">\[
\text{Var}_{\boldsymbol{\theta}}(\mathbf{T}) \succeq \mathbf{D}(\boldsymbol{\theta}) [\mathbf{I}(\boldsymbol{\theta})]^{-1} \mathbf{D}(\boldsymbol{\theta})^\top
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{A} \succeq \mathbf{B}\)</span> means that the matrix <span class="math inline">\(\mathbf{A} - \mathbf{B}\)</span> is positive semi-definite (i.e., for any vector <span class="math inline">\(\mathbf{v}\)</span>, <span class="math inline">\(\mathbf{v}^\top (\mathbf{A} - \mathbf{B}) \mathbf{v} \ge 0\)</span>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf{U} = \nabla_{\boldsymbol{\theta}} \log f(\mathbf{X}|\boldsymbol{\theta})\)</span> be the <span class="math inline">\(p \times 1\)</span> Score vector. We know that <span class="math inline">\(E[\mathbf{U}] = \mathbf{0}\)</span> and <span class="math inline">\(\text{Var}(\mathbf{U}) = \mathbf{I}(\boldsymbol{\theta})\)</span>.</p>
<p>Consider the covariance between the estimator <span class="math inline">\(\mathbf{T}\)</span> and the Score <span class="math inline">\(\mathbf{U}\)</span>. By an argument similar to the scalar case (interchanging derivative and integral), we find:</p>
<p><span class="math display">\[
\text{Cov}(\mathbf{T}, \mathbf{U}) = E[\mathbf{T} \mathbf{U}^\top] = \mathbf{D}(\boldsymbol{\theta})
\]</span></p>
<p>Now, define the block vector <span class="math inline">\(\mathbf{Z} = \begin{pmatrix} \mathbf{T} \\ \mathbf{U} \end{pmatrix}\)</span>. The covariance matrix of <span class="math inline">\(\mathbf{Z}\)</span> is necessarily positive semi-definite:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Z}) = \begin{pmatrix} \text{Var}(\mathbf{T}) &amp; \text{Cov}(\mathbf{T}, \mathbf{U}) \\ \text{Cov}(\mathbf{U}, \mathbf{T}) &amp; \text{Var}(\mathbf{U}) \end{pmatrix} = \begin{pmatrix} \Sigma_{\mathbf{T}} &amp; \mathbf{D} \\ \mathbf{D}^\top &amp; \mathbf{I} \end{pmatrix} \succeq 0
\]</span></p>
<p>For this block matrix to be positive semi-definite, the Schur complement of the block <span class="math inline">\(\mathbf{I}\)</span> must be positive semi-definite (assuming <span class="math inline">\(\mathbf{I}\)</span> is positive definite/invertible):</p>
<p><span class="math display">\[
\Sigma_{\mathbf{T}} - \mathbf{D} \mathbf{I}^{-1} \mathbf{D}^\top \succeq 0
\]</span></p>
<p>Thus, <span class="math inline">\(\text{Var}(\mathbf{T}) \succeq \mathbf{D} \mathbf{I}^{-1} \mathbf{D}^\top\)</span>.</p>
</div>
<div id="cor-scalar-crlb" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5.1 (Corollary: Scalar Estimator)</strong></span> Consider the case where <span class="math inline">\(T(\mathbf{X})\)</span> is a scalar estimator (<span class="math inline">\(k=1\)</span>) for a scalar parameter <span class="math inline">\(\theta\)</span> (<span class="math inline">\(p=1\)</span>).</p>
<ol type="1">
<li><p><strong>Matrices to Scalars:</strong> The covariance matrix <span class="math inline">\(\text{Var}(\mathbf{T})\)</span> becomes the scalar variance <span class="math inline">\(\text{Var}(T)\)</span>. The Fisher Information matrix <span class="math inline">\(\mathbf{I}(\boldsymbol{\theta})\)</span> becomes the scalar <span class="math inline">\(I(\theta)\)</span>.</p></li>
<li><p><strong>Jacobian to Derivative:</strong> The Jacobian matrix <span class="math inline">\(\mathbf{D}(\boldsymbol{\theta})\)</span> reduces to the scalar derivative <span class="math inline">\(m'(\theta)\)</span>.</p></li>
</ol>
<p>Substituting these into the multivariate bound:</p>
<p><span class="math display">\[
\text{Var}(T) - m'(\theta) [I(\theta)]^{-1} m'(\theta) \ge 0
\]</span></p>
<p><span class="math display">\[
\text{Var}(T) \ge \frac{[m'(\theta)]^2}{I(\theta)}
\]</span></p>
</div>
<div id="rem-crlb-generality" class="proof remark">
<p><span class="proof-title"><em>Remark 5.2</em> (Generality of the Lower Bound). </span>The power of the Cramer-Rao Lower Bound lies in its independence from the specific method of estimation. It relies solely on the properties of the underlying probability model (specifically, the curvature of the log-likelihood function) and the bias of the estimator. Consequently, it provides a universal benchmark for precision:</p>
<ol type="1">
<li><p><strong>Fundamental Limit</strong> It represents the limit of “extractable information” about <span class="math inline">\(\boldsymbol{\theta}\)</span> contained in the data <span class="math inline">\(\mathbf{X}\)</span>. No matter how clever the estimation algorithm is (e.g., Method of Moments, Bayes estimators, etc.), the variance cannot be reduced beyond this intrinsic bound determined by the Fisher Information.</p></li>
<li><p><strong>Efficiency Standard</strong> It allows us to define the concept of an <em>efficient estimator</em>. Any unbiased estimator that attains this lower bound is the Uniformly Minimum Variance Unbiased Estimator (UMVUE).</p></li>
<li><p><strong>Asymptotic Justification</strong> While finite-sample estimators may not always achieve this bound, the Maximum Likelihood Estimator (MLE) is asymptotically efficient. This means that as the sample size <span class="math inline">\(n \to \infty\)</span>, the variance of the MLE approaches the CRLB, justifying the popularity of likelihood-based inference.</p></li>
</ol>
</div>
<section id="example-with-exponential-likelihood" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="example-with-exponential-likelihood"><span class="header-section-number">5.4.1</span> Example with Exponential Likelihood</h3>
<div id="exm-exponential-crlb" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1</strong></span> Let <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Exp}(\theta)\)</span>, where the density is <span class="math inline">\(f(x|\theta) = \frac{1}{\theta} e^{-x/\theta}\)</span>. We illustrate the likelihood identities and the efficiency of the sample mean.</p>
<ol type="1">
<li><p>The Score Function (<span class="math inline">\(U\)</span>) The log-likelihood function is: <span class="math display">\[
\ell(\theta; \mathbf{x}) = \sum_{i=1}^n \left( -\log \theta - \frac{x_i}{\theta} \right) = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^n x_i
\]</span> The Score function is the first derivative with respect to <span class="math inline">\(\theta\)</span>: <span class="math display">\[
U(\theta; \mathbf{x}) = \frac{\partial \ell}{\partial \theta} = -\frac{n}{\theta} + \frac{\sum x_i}{\theta^2}
\]</span> <em>Check First Moment:</em> <span class="math inline">\(E[U] = -\frac{n}{\theta} + \frac{1}{\theta^2} \sum E[X_i] = -\frac{n}{\theta} + \frac{n\theta}{\theta^2} = 0\)</span>. (Verified)</p></li>
<li><p>Fisher Information (<span class="math inline">\(I(\theta)\)</span>) We calculate the information using two different definitions to verify Bartlett’s identity.</p></li>
</ol>
<ul>
<li><p><strong>Method A: Negative Expected Hessian</strong> <span class="math display">\[
  U'(\theta) = \frac{\partial U}{\partial \theta} = \frac{n}{\theta^2} - \frac{2\sum x_i}{\theta^3}
  \]</span> <span class="math display">\[
  I(\theta) = -E[U'(\theta)] = -\left( \frac{n}{\theta^2} - \frac{2 n \theta}{\theta^3} \right) = -\left( \frac{n}{\theta^2} - \frac{2n}{\theta^2} \right) = \frac{n}{\theta^2}
  \]</span></p></li>
<li><p><strong>Method B: Variance of the Score</strong> <span class="math display">\[
  \text{Var}(U) = \text{Var}\left( -\frac{n}{\theta} + \frac{\sum X_i}{\theta^2} \right) = \frac{1}{\theta^4} \text{Var}\left( \sum X_i \right)
  \]</span> Since <span class="math inline">\(X_i\)</span> are independent with <span class="math inline">\(\text{Var}(X_i) = \theta^2\)</span>: <span class="math display">\[
  \text{Var}(U) = \frac{1}{\theta^4} (n \theta^2) = \frac{n}{\theta^2}
  \]</span></p></li>
</ul>
<p><strong>Result:</strong> <span class="math inline">\(\text{Var}(U) = -E[U'] = I(\theta)\)</span>. (Identity Verified)</p>
<ol start="3" type="1">
<li>Cramer-Rao Lower Bound (CRLB) Consider the estimator <span class="math inline">\(T(\mathbf{X}) = \bar{X}\)</span>.</li>
</ol>
<ul>
<li><p><strong>Expectation:</strong> <span class="math inline">\(m(\theta) = E[\bar{X}] = \theta\)</span>. Thus, <span class="math inline">\(T\)</span> is unbiased and <span class="math inline">\(m'(\theta) = 1\)</span>.</p></li>
<li><p><strong>Actual Variance:</strong> <span class="math display">\[
  \text{Var}(T(\mathbf{X})) = \text{Var}(\bar{X}) = \frac{\text{Var}(X)}{n} = \frac{\theta^2}{n}
  \]</span></p></li>
<li><p><strong>Theoretical Lower Bound:</strong> <span class="math display">\[
  \text{CRLB} = \frac{[m'(\theta)]^2}{I(\theta)} = \frac{1^2}{n/\theta^2} = \frac{\theta^2}{n}
  \]</span></p></li>
</ul>
<p><strong>Conclusion:</strong> <span class="math display">\[
\text{Var}(T(\mathbf{X})) = \frac{\theta^2}{n} \ge \frac{\theta^2}{n}
\]</span> The variance of <span class="math inline">\(T(\mathbf{X})\)</span> achieves the lower bound exactly. Therefore, <span class="math inline">\(\bar{X}\)</span> is an <strong>efficient estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</div>
</section>
</section>
<section id="exponential-families" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="exponential-families"><span class="header-section-number">5.5</span> Exponential Families</h2>
<div id="def-exponential-family" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (Exponential Family)</strong></span> A family of probability density functions (or probability mass functions) is said to be an <strong>Exponential Family</strong> if the log-likelihood function, denoted by <span class="math inline">\(\ell(\boldsymbol{\theta}; \mathbf{x}) = \log f(\mathbf{x}|\boldsymbol{\theta})\)</span>, can be expressed as the sum of three distinct terms:</p>
<p><span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})
\]</span></p>
<p>Exponentiating this yields the density form:</p>
<p><span class="math display">\[
f(\mathbf{x}|\boldsymbol{\theta}) = h(\mathbf{x}) \exp\left\{ \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) \right\}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)\)</span> is the vector of model parameters.</li>
<li><span class="math inline">\(\eta_i(\boldsymbol{\theta})\)</span> are the <strong>natural parameter functions</strong>.</li>
<li><span class="math inline">\(\mathbf{T}(\mathbf{x}) = (T_1(\mathbf{x}), \dots, T_k(\mathbf{x}))\)</span> constitutes the vector of <strong>sufficient statistics</strong> for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</li>
<li><span class="math inline">\(A(\boldsymbol{\theta})\)</span> is the <strong>log-partition function</strong> (or cumulant function), which ensures the density integrates to 1.</li>
<li><span class="math inline">\(h(\mathbf{x})\)</span> is the base measure.</li>
</ul>
</div>
<section id="examples" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="examples"><span class="header-section-number">5.5.1</span> Examples</h3>
<section id="exponential-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exponential-distribution">Exponential Distribution</h4>
<div id="exm-exponential-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 (Exponential Distribution)</strong></span> Let <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the scale parameter. <span class="math display">\[
f(\mathbf{x}|\theta) = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
\]</span></p>
<p>The log-likelihood is: <span class="math display">\[
\ell(\theta; \mathbf{x}) = -\frac{1}{\theta} \sum_{i=1}^n x_i - n \log \theta
\]</span></p>
<p>Identifying the components:</p>
<ul>
<li><span class="math inline">\(\eta_1(\theta) = -\frac{1}{\theta}\)</span></li>
<li><span class="math inline">\(T_1(\mathbf{x}) = \sum_{i=1}^n x_i\)</span></li>
<li><span class="math inline">\(A(\theta) = n \log \theta\)</span></li>
<li><span class="math inline">\(\log h(\mathbf{x}) = 0\)</span></li>
</ul>
</div>
</section>
<section id="gamma-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="gamma-distribution">Gamma Distribution</h4>
<div id="exm-gamma-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (Gamma Distribution)</strong></span> Let <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)\)</span>. The density is: <span class="math display">\[
f(\mathbf{x}|\boldsymbol{\theta}) = [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
\]</span></p>
<p>The log-likelihood is: <span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i - \left[ n \log \Gamma(\alpha) + n\alpha \log \beta \right]
\]</span></p>
<p>Identifying the components:</p>
<ul>
<li><span class="math inline">\(\eta_1(\boldsymbol{\theta}) = \alpha - 1\)</span>, <span class="math inline">\(\quad T_1(\mathbf{x}) = \sum \log x_i\)</span></li>
<li><span class="math inline">\(\eta_2(\boldsymbol{\theta}) = -\frac{1}{\beta}\)</span>, <span class="math inline">\(\quad T_2(\mathbf{x}) = \sum x_i\)</span></li>
<li><span class="math inline">\(A(\boldsymbol{\theta}) = n \log \Gamma(\alpha) + n\alpha \log \beta\)</span></li>
</ul>
</div>
</section>
<section id="beta-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="beta-distribution">Beta Distribution</h4>
<div id="exm-beta-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 (Beta Distribution)</strong></span> Let <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)\)</span> with <span class="math inline">\(\boldsymbol{\theta} = (a, b)\)</span>. <span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) - n \log B(a, b)
\]</span></p>
<p>This is an exponential family with <span class="math inline">\(k=2\)</span>.</p>
<ul>
<li><span class="math inline">\(\eta_1 = a-1\)</span>, <span class="math inline">\(T_1 = \sum \log x_i\)</span></li>
<li><span class="math inline">\(\eta_2 = b-1\)</span>, <span class="math inline">\(T_2 = \sum \log(1-x_i)\)</span></li>
<li><span class="math inline">\(A(\boldsymbol{\theta}) = n \log B(a, b)\)</span></li>
</ul>
</div>
</section>
<section id="normal-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="normal-distribution">Normal Distribution</h4>
<div id="exm-normal-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 (Normal Distribution)</strong></span> Let <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>. The log-likelihood is: <span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
\]</span></p>
<p>Identifying the components:</p>
<ul>
<li><span class="math inline">\(\eta_1 = \frac{\mu}{\sigma^2}\)</span>, <span class="math inline">\(T_1 = \sum x_i\)</span></li>
<li><span class="math inline">\(\eta_2 = -\frac{1}{2\sigma^2}\)</span>, <span class="math inline">\(T_2 = \sum x_i^2\)</span></li>
<li><span class="math inline">\(A(\boldsymbol{\theta}) = \frac{n\mu^2}{2\sigma^2} + n \log \sigma + \frac{n}{2} \log(2\pi)\)</span></li>
</ul>
</div>
</section>
</section>
<section id="examples-of-non-exponential-families" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="examples-of-non-exponential-families"><span class="header-section-number">5.5.2</span> Examples of Non-exponential Families</h3>
<p>A model is <strong>not</strong> in the exponential family if the support depends on the parameter.</p>
<section id="uniform-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h4>
<div id="exm-uniform-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6 (Uniform Distribution)</strong></span> Let <span class="math inline">\(X \sim U(0, \theta)\)</span>. <span class="math display">\[
\ell(\theta; x) = -\log \theta + \log I(0 &lt; x &lt; \theta)
\]</span></p>
<p>The term <span class="math inline">\(\log I(0 &lt; x &lt; \theta)\)</span> couples <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta\)</span> in a way that cannot be separated into a sum <span class="math inline">\(\sum \eta_i(\theta) T_i(x)\)</span>.</p>
</div>
</section>
</section>
<section id="moments-of-sufficient-statistics-of-exponential-families" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="moments-of-sufficient-statistics-of-exponential-families"><span class="header-section-number">5.5.3</span> Moments of Sufficient Statistics of Exponential Families</h3>
<section id="means-of-sufficient-statistics-general-case" class="level4" data-number="5.5.3.1">
<h4 data-number="5.5.3.1" class="anchored" data-anchor-id="means-of-sufficient-statistics-general-case"><span class="header-section-number">5.5.3.1</span> Means of Sufficient Statistics (General Case)</h4>
<div id="thm-moments-exp-family" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.4 (Means via the Score Function)</strong></span> For a regular exponential family with log-likelihood <span class="math inline">\(\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})\)</span>, the expectation of the sufficient statistics can be found by setting the expected score to zero:</p>
<p><span class="math display">\[
E_{\boldsymbol{\theta}} \left[ \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_j} \right] = 0
\]</span></p>
<p>Substituting the specific form of <span class="math inline">\(\ell(\boldsymbol{\theta}; \mathbf{X})\)</span>:</p>
<p><span class="math display">\[
\sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} E[T_i(\mathbf{X})] = \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j} \quad \text{for } j=1, \dots, d
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The log-likelihood is: <span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})
\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(\theta_j\)</span>: <span class="math display">\[
\frac{\partial \ell}{\partial \theta_j} = \sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} T_i(\mathbf{x}) - \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j}
\]</span></p>
<p>Taking the expectation and using the regularity condition <span class="math inline">\(E[\frac{\partial \ell}{\partial \theta_j}] = 0\)</span>: <span class="math display">\[
E\left[ \sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} T_i(\mathbf{X}) - \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j} \right] = 0
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} E[T_i(\mathbf{X})] = \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j}
\]</span></p>
</div>
</section>
<section id="natural-parameterization" class="level4" data-number="5.5.3.2">
<h4 data-number="5.5.3.2" class="anchored" data-anchor-id="natural-parameterization"><span class="header-section-number">5.5.3.2</span> Natural Parameterization</h4>
<div id="def-natural-parameterization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 (Natural Parameterization (Canonical Form))</strong></span> If the parameterization is chosen such that the natural parameters are the components of the parameter vector itself (i.e., <span class="math inline">\(\boldsymbol{\eta}(\boldsymbol{\theta}) = \boldsymbol{\theta}\)</span>), the exponential family is said to be in <strong>Canonical Form</strong> or <strong>Natural Parameterization</strong>.</p>
<p>The log-likelihood for the natural parameter vector <span class="math inline">\(\boldsymbol{\eta} = (\eta_1, \dots, \eta_k)^T\)</span> simplifies to: <span class="math display">\[
\ell(\boldsymbol{\eta}; \mathbf{x}) = \sum_{i=1}^k \eta_i T_i(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
\]</span> or in vector notation: <span class="math display">\[
\ell(\boldsymbol{\eta}; \mathbf{x}) = \boldsymbol{\eta}^T \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
\]</span> where <span class="math inline">\(A(\boldsymbol{\eta})\)</span> is the log-partition function.</p>
</div>
<div id="def-curvedexpfam" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 (Full vs.&nbsp;Curved Exponential Families)</strong></span> &nbsp;</p>
<ul>
<li><strong>Full Exponential Family:</strong> When the natural parameters <span class="math inline">\(\boldsymbol{\eta}\)</span> can vary independently in an open set of <span class="math inline">\(\mathbb{R}^k\)</span> (i.e., <span class="math inline">\(d=k\)</span> and the mapping is a bijection).</li>
<li><strong>Curved Exponential Family:</strong> When the dimension of the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> is smaller than the number of sufficient statistics (<span class="math inline">\(d &lt; k\)</span>), forcing the natural parameters <span class="math inline">\(\boldsymbol{\eta}(\boldsymbol{\theta})\)</span> to lie on a non-linear curve or surface within the natural parameter space.</li>
</ul>
</div>
<div id="exm-curved-normal-natural" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (Curved Exponential Family Example)</strong></span> Consider the <span class="math inline">\(N(\theta, \theta^2)\)</span> distribution (<span class="math inline">\(d=1\)</span>). The log-likelihood is: <span class="math display">\[
\ell(\theta; \mathbf{x}) = -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i - n \log \theta - \text{const}
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(\eta_1(\theta) = -\frac{1}{2\theta^2}\)</span>, <span class="math inline">\(T_1 = \sum x_i^2\)</span></li>
<li><span class="math inline">\(\eta_2(\theta) = \frac{1}{\theta}\)</span>, <span class="math inline">\(T_2 = \sum x_i\)</span></li>
</ul>
<p>Since <span class="math inline">\(d=1\)</span> but <span class="math inline">\(k=2\)</span>, and <span class="math inline">\(\eta_1 = -\frac{1}{2}\eta_2^2\)</span>, the parameters are constrained to a parabola. This is a <strong>Curved Exponential Family</strong>.</p>
</div>
</section>
<section id="mean-and-variance-of-sufficient-statistics" class="level4" data-number="5.5.3.3">
<h4 data-number="5.5.3.3" class="anchored" data-anchor-id="mean-and-variance-of-sufficient-statistics"><span class="header-section-number">5.5.3.3</span> Mean and Variance of Sufficient Statistics</h4>
<div id="thm-cumulant-generating" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.5 (Mean and Variance of Sufficient Statistics)</strong></span> For an exponential family in canonical form, the log-partition function <span class="math inline">\(A(\boldsymbol{\eta})\)</span> acts as the <strong>Cumulant Generating Function</strong> for the sufficient statistic vector <span class="math inline">\(\mathbf{T}(\mathbf{X})\)</span>. The derivatives of <span class="math inline">\(A(\boldsymbol{\eta})\)</span> yield the moments of <span class="math inline">\(\mathbf{T}(\mathbf{X})\)</span> as follows:</p>
<ol type="1">
<li><p><strong>Mean (First Derivative):</strong> <span class="math display">\[
E[\mathbf{T}(\mathbf{X})] = \nabla A(\boldsymbol{\eta})
\]</span></p></li>
<li><p><strong>Covariance (Second Derivative):</strong> <span class="math display">\[
\text{Var}(\mathbf{T}(\mathbf{X})) = \nabla^2 A(\boldsymbol{\eta})
\]</span></p></li>
</ol>
<p><strong>Link to Fisher Information:</strong> In the canonical parameterization, the observed information matrix is constant (non-stochastic) and equals the Hessian of <span class="math inline">\(A(\boldsymbol{\eta})\)</span>. Therefore, the covariance of the sufficient statistics is exactly the Fisher Information Matrix:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{T}(\mathbf{X})) = \mathbf{I}(\boldsymbol{\eta})
\]</span></p>
<p>This implies that <span class="math inline">\(\mathbf{T}(\mathbf{X})\)</span> is an efficient estimator for the mean parameter <span class="math inline">\(\mathbf{m}(\boldsymbol{\eta}) = E[\mathbf{T}(\mathbf{X})]\)</span>, as it achieves the Cramer-Rao Lower Bound with equality (identity link).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Derivation</strong></p>
<p>These results follow directly from Bartlett’s Identities (Theorem <a href="#thm-score-identities" class="quarto-xref">Theorem&nbsp;<span>5.1</span></a>) applied to the canonical log-likelihood: <span class="math display">\[
\ell(\boldsymbol{\eta}; \mathbf{x}) = \boldsymbol{\eta}^T \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
\]</span></p>
<p><strong>For the Mean:</strong> The score function (gradient of <span class="math inline">\(\ell\)</span>) is: <span class="math display">\[
\mathbf{U}(\boldsymbol{\eta}) = \nabla_{\boldsymbol{\eta}} \ell(\boldsymbol{\eta}; \mathbf{x}) = \mathbf{T}(\mathbf{x}) - \nabla A(\boldsymbol{\eta})
\]</span> By the First Moment Identity, <span class="math inline">\(E[\mathbf{U}(\boldsymbol{\eta})] = \mathbf{0}\)</span>: <span class="math display">\[
E[\mathbf{T}(\mathbf{X}) - \nabla A(\boldsymbol{\eta})] = \mathbf{0} \implies E[\mathbf{T}(\mathbf{X})] = \nabla A(\boldsymbol{\eta})
\]</span></p>
<p><strong>For the Covariance:</strong> The observed information (negative Hessian of <span class="math inline">\(\ell\)</span>) is: <span class="math display">\[
\mathbf{J}(\boldsymbol{\eta}) = -\nabla^2_{\boldsymbol{\eta}} \ell(\boldsymbol{\eta}; \mathbf{x}) = -\nabla_{\boldsymbol{\eta}} (\mathbf{T}(\mathbf{x}) - \nabla A(\boldsymbol{\eta})) = \nabla^2 A(\boldsymbol{\eta})
\]</span> Note that <span class="math inline">\(\mathbf{T}(\mathbf{x})\)</span> is constant with respect to <span class="math inline">\(\boldsymbol{\eta}\)</span>, so its derivative vanishes. By the Second Moment Identity, <span class="math inline">\(\mathbf{I}(\boldsymbol{\eta}) = E[\mathbf{J}(\boldsymbol{\eta})] = \text{Cov}(\mathbf{U}(\boldsymbol{\eta}))\)</span>. Since <span class="math inline">\(\mathbf{U}(\boldsymbol{\eta}) = \mathbf{T}(\mathbf{X}) - \text{constant}\)</span>, <span class="math inline">\(\text{Cov}(\mathbf{U}(\boldsymbol{\eta})) = \text{Cov}(\mathbf{T}(\mathbf{X}))\)</span>. Therefore: <span class="math display">\[
\text{Cov}(\mathbf{T}(\mathbf{X})) = E[\nabla^2 A(\boldsymbol{\eta})] = \nabla^2 A(\boldsymbol{\eta})
\]</span></p>
</div>
</section>
<section id="examples-1" class="level4" data-number="5.5.3.4">
<h4 data-number="5.5.3.4" class="anchored" data-anchor-id="examples-1"><span class="header-section-number">5.5.3.4</span> Examples</h4>
<section id="moments-of-the-binomial-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="moments-of-the-binomial-distribution">Moments of the Binomial Distribution</h5>
<div id="exm-bernoulli-moments" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 (Moments of the Binomial Distribution)</strong></span> Consider <span class="math inline">\(n\)</span> independent coin flips <span class="math inline">\(X_1, \dots, X_n \sim \text{Bernoulli}(p)\)</span>. We find the mean and variance of <span class="math inline">\(T = \sum X_i\)</span>.</p>
<ol type="1">
<li><p><strong>Log-Likelihood Form</strong></p>
<p>The standard log-likelihood is: <span class="math display">\[
\ell(p; \mathbf{x}) = \log\left(\frac{p}{1-p}\right) \sum x_i + n \log(1-p)
\]</span></p>
<ul>
<li>Natural Parameter: <span class="math inline">\(\eta = \log\left(\frac{p}{1-p}\right) \implies p = \frac{e^\eta}{1+e^\eta}\)</span>.</li>
<li>Log-Partition Function: <span class="math inline">\(A(\eta) = -n \log(1-p) = n \log(1+e^\eta)\)</span>.</li>
</ul>
<p><strong>Canonical Log-Likelihood <span class="math inline">\(\ell(\eta)\)</span>:</strong> <span class="math display">\[
\ell(\eta; \mathbf{x}) = \eta \left(\sum x_i\right) - n \log(1+e^\eta)
\]</span></p></li>
<li><p><strong>Calculating Moments</strong> <span class="math display">\[
E[T] = \frac{\partial A}{\partial \eta} = n \frac{e^\eta}{1+e^\eta} = np
\]</span></p>
<p><span class="math display">\[
\text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = n \frac{e^\eta (1+e^\eta) - e^\eta(e^\eta)}{(1+e^\eta)^2} = n \frac{e^\eta}{(1+e^\eta)^2} = np(1-p)
\]</span></p></li>
</ol>
</div>
</section>
<section id="moments-of-the-gamma-sufficient-statistic" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="moments-of-the-gamma-sufficient-statistic">Moments of the Gamma Sufficient Statistic</h5>
<div id="exm-exponential-moments" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9 (Moments of the Gamma Sufficient Statistic)</strong></span> Consider <span class="math inline">\(X_i \sim \text{Exp}(\lambda)\)</span>. We find the moments of <span class="math inline">\(T = \sum X_i\)</span>.</p>
<ol type="1">
<li><p><strong>Log-Likelihood Form</strong></p>
<p>The standard log-likelihood is: <span class="math display">\[
\ell(\lambda; \mathbf{x}) = -\lambda \sum x_i + n \log \lambda
\]</span></p>
<ul>
<li>Natural Parameter: <span class="math inline">\(\eta = -\lambda\)</span>.</li>
<li>Log-Partition Function: <span class="math inline">\(A(\eta) = -n \log \lambda = -n \log(-\eta)\)</span>.</li>
</ul>
<p><strong>Canonical Log-Likelihood <span class="math inline">\(\ell(\eta)\)</span>:</strong> <span class="math display">\[
\ell(\eta; \mathbf{x}) = \eta \left(\sum x_i\right) - \left[ -n \log(-\eta) \right] = \eta \sum x_i + n \log(-\eta)
\]</span></p></li>
<li><p><strong>Calculating Moments</strong> <span class="math display">\[
E[T] = \frac{\partial A}{\partial \eta} = -n \frac{1}{-\eta}(-1) = -\frac{n}{\eta} = \frac{n}{\lambda}
\]</span></p>
<p><span class="math display">\[
\text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = \frac{\partial}{\partial \eta} \left(-\frac{n}{\eta}\right) = \frac{n}{\eta^2} = \frac{n}{\lambda^2}
\]</span></p></li>
</ol>
</div>
</section>
<section id="moments-of-normal-sufficient-statistics" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="moments-of-normal-sufficient-statistics">Moments of Normal Sufficient Statistics</h5>
<div id="exm-normal-sufficient-moments" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10 (Moments of Normal Sufficient Statistics)</strong></span> Consider <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>.</p>
<ol type="1">
<li><p><strong>Log-Likelihood Form</strong></p>
<p>The standard log-likelihood is: <span class="math display">\[
\ell(\boldsymbol{\theta}; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum x_i - \frac{1}{2\sigma^2} \sum x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
\]</span></p>
<ul>
<li>Natural Parameters: <span class="math inline">\(\eta_1 = \frac{\mu}{\sigma^2}, \quad \eta_2 = -\frac{1}{2\sigma^2}\)</span>.</li>
<li>Log-Partition Function (in terms of <span class="math inline">\(\boldsymbol{\eta}\)</span>): Using <span class="math inline">\(\sigma^2 = -\frac{1}{2\eta_2}\)</span> and <span class="math inline">\(\mu = -\frac{\eta_1}{2\eta_2}\)</span>: <span class="math display">\[
A(\boldsymbol{\eta}) = -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2) + \frac{n}{2}\log(2\pi)
\]</span></li>
</ul>
<p><strong>Canonical Log-Likelihood <span class="math inline">\(\ell(\boldsymbol{\eta})\)</span>:</strong> <span class="math display">\[
\ell(\boldsymbol{\eta}; \mathbf{x}) = \eta_1 \left(\sum x_i\right) + \eta_2 \left(\sum x_i^2\right) - \left[ -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2) \right]
\]</span></p></li>
<li><p><strong>First Moments (Means)</strong> <span class="math display">\[
E[T_1] = E\left[\sum X_i\right] = \frac{\partial A}{\partial \eta_1} = -\frac{2n\eta_1}{4\eta_2} = -\frac{n\eta_1}{2\eta_2} = n\mu
\]</span> <span class="math display">\[
E[T_2] = E\left[\sum X_i^2\right] = \frac{\partial A}{\partial \eta_2} = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2(-2\eta_2)}(-2) = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2\eta_2}
\]</span> Subbing back <span class="math inline">\(\mu, \sigma\)</span>: <span class="math display">\[
= n\mu^2 + n\sigma^2 = n(\mu^2 + \sigma^2)
\]</span></p></li>
<li><p><strong>Second Moment (Covariance)</strong> <span class="math display">\[
\text{Cov}(T_1, T_2) = \frac{\partial^2 A}{\partial \eta_1 \partial \eta_2} = \frac{\partial}{\partial \eta_2} \left( -\frac{n\eta_1}{2\eta_2} \right) = \frac{n\eta_1}{2\eta_2^2} = 2n\mu\sigma^2
\]</span></p></li>
<li><p><strong>Independence of <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span></strong> We verify that <span class="math inline">\(\text{Cov}(\bar{X}, S^2) = 0\)</span>.</p>
<p>Express <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> in terms of <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>: <span class="math display">\[
\bar{X} = \frac{1}{n} T_1
\]</span> <span class="math display">\[
S^2 = \frac{1}{n-1} \left( \sum X_i^2 - n\bar{X}^2 \right) = \frac{1}{n-1} \left( T_2 - \frac{1}{n} T_1^2 \right)
\]</span></p>
<p>Now compute the covariance (ignoring constants <span class="math inline">\(\frac{1}{n(n-1)}\)</span> for now): <span class="math display">\[
\text{Cov}\left(T_1, T_2 - \frac{1}{n}T_1^2\right) = \text{Cov}(T_1, T_2) - \frac{1}{n} \text{Cov}(T_1, T_1^2)
\]</span></p>
<p>We need <span class="math inline">\(\text{Cov}(T_1, T_1^2)\)</span>. Since <span class="math inline">\(T_1 = \sum X_i \sim N(n\mu, n\sigma^2)\)</span>, we use the property of the normal distribution that for <span class="math inline">\(Y \sim N(\theta, \tau^2)\)</span>, <span class="math inline">\(\text{Cov}(Y, Y^2) = 2\theta \tau^2\)</span>. Here <span class="math inline">\(\theta = n\mu\)</span> and <span class="math inline">\(\tau^2 = n\sigma^2\)</span>: <span class="math display">\[
\text{Cov}(T_1, T_1^2) = 2(n\mu)(n\sigma^2) = 2n^2\mu\sigma^2
\]</span></p>
<p>Substituting this back into the expression: <span class="math display">\[
\text{Cov}\left(T_1, T_2 - \frac{1}{n}T_1^2\right) = \underbrace{2n\mu\sigma^2}_{\text{From Part 3}} - \frac{1}{n} (2n^2\mu\sigma^2) = 2n\mu\sigma^2 - 2n\mu\sigma^2 = 0
\]</span></p>
<p>Since <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> are uncorrelated and derived from normally distributed data, they are <strong>independent</strong>.</p></li>
</ol>
</div>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./sufficiency.html" class="pagination-link" aria-label="Sufficient Statistic">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sufficient Statistic</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mle.html" class="pagination-link" aria-label="Maximum Likelihood Estimation">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>