<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Bayesian Methods – Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./decision.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0de45b8b5f35a93596f1d788d60c4c11.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-9f849cbbbd34ccda4fce30280e8b4082.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/bookstyles.css">
<script src="resources/num_eq.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bayesian.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introstatinf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#fundamental-elements-of-bayesian-inference" id="toc-fundamental-elements-of-bayesian-inference" class="nav-link active" data-scroll-target="#fundamental-elements-of-bayesian-inference"><span class="header-section-number">3.1</span> Fundamental Elements of Bayesian Inference</a></li>
  <li><a href="#decision-theory-and-bayes-rules" id="toc-decision-theory-and-bayes-rules" class="nav-link" data-scroll-target="#decision-theory-and-bayes-rules"><span class="header-section-number">3.2</span> Decision Theory and Bayes Rules</a>
  <ul>
  <li><a href="#common-loss-functions-and-estimators" id="toc-common-loss-functions-and-estimators" class="nav-link" data-scroll-target="#common-loss-functions-and-estimators"><span class="header-section-number">3.2.1</span> Common Loss Functions and Estimators</a></li>
  <li><a href="#squared-error-loss-point-estimate" id="toc-squared-error-loss-point-estimate" class="nav-link" data-scroll-target="#squared-error-loss-point-estimate"><span class="header-section-number">3.2.2</span> Squared Error Loss (point Estimate)</a></li>
  <li><a href="#absolute-error-loss" id="toc-absolute-error-loss" class="nav-link" data-scroll-target="#absolute-error-loss"><span class="header-section-number">3.2.3</span> Absolute Error Loss</a></li>
  <li><a href="#hypothesis-testing-0-1-loss" id="toc-hypothesis-testing-0-1-loss" class="nav-link" data-scroll-target="#hypothesis-testing-0-1-loss"><span class="header-section-number">3.2.4</span> Hypothesis Testing (0-1 Loss)</a></li>
  </ul></li>
  <li><a href="#minimax-estimation" id="toc-minimax-estimation" class="nav-link" data-scroll-target="#minimax-estimation"><span class="header-section-number">3.3</span> Minimax Estimation</a>
  <ul>
  <li><a href="#binomial-minimax-estimator" id="toc-binomial-minimax-estimator" class="nav-link" data-scroll-target="#binomial-minimax-estimator"><span class="header-section-number">3.3.1</span> Binomial Minimax Estimator</a></li>
  </ul></li>
  <li><a href="#stein-estimation-and-shrinkage" id="toc-stein-estimation-and-shrinkage" class="nav-link" data-scroll-target="#stein-estimation-and-shrinkage"><span class="header-section-number">3.4</span> Stein Estimation and Shrinkage</a></li>
  <li><a href="#empirical-bayes" id="toc-empirical-bayes" class="nav-link" data-scroll-target="#empirical-bayes"><span class="header-section-number">3.5</span> Empirical Bayes</a>
  <ul>
  <li><a href="#baseball-example-efron-morris" id="toc-baseball-example-efron-morris" class="nav-link" data-scroll-target="#baseball-example-efron-morris"><span class="header-section-number">3.5.1</span> Baseball Example (efron &amp; Morris)</a></li>
  </ul></li>
  <li><a href="#predictive-distributions" id="toc-predictive-distributions" class="nav-link" data-scroll-target="#predictive-distributions"><span class="header-section-number">3.6</span> Predictive Distributions</a>
  <ul>
  <li><a href="#normal-normal-predictive-distribution" id="toc-normal-normal-predictive-distribution" class="nav-link" data-scroll-target="#normal-normal-predictive-distribution"><span class="header-section-number">3.6.1</span> Normal-normal Predictive Distribution</a></li>
  </ul></li>
  <li><a href="#hierarchical-modeling-and-mcmc" id="toc-hierarchical-modeling-and-mcmc" class="nav-link" data-scroll-target="#hierarchical-modeling-and-mcmc"><span class="header-section-number">3.7</span> Hierarchical Modeling and MCMC</a>
  <ul>
  <li><a href="#baseball-example-with-hierarchical-model" id="toc-baseball-example-with-hierarchical-model" class="nav-link" data-scroll-target="#baseball-example-with-hierarchical-model"><span class="header-section-number">3.7.1</span> Baseball Example with Hierarchical Model</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="fundamental-elements-of-bayesian-inference" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="fundamental-elements-of-bayesian-inference"><span class="header-section-number">3.1</span> Fundamental Elements of Bayesian Inference</h2>
<p>The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).</p>
<div id="thm-bayes" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Posterior Distribution)</strong></span> Suppose we have a parameter <span class="math inline">\(\theta\)</span> with a prior distribution denoted by <span class="math inline">\(\pi(\theta)\)</span>. If we observe data <span class="math inline">\(x\)</span> drawn from a distribution with probability density function (pdf) <span class="math inline">\(f(x; \theta)\)</span>, then the <strong>posterior density</strong> of <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(x\)</span> is defined as:</p>
<p><span class="math display">\[
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{\int_{\Theta} \pi(\theta) f(x;\theta) d\theta}
\]</span></p>
<p>In this equation:</p>
<ul>
<li><span class="math inline">\(\pi(\theta)\)</span> is the <strong>prior</strong>.</li>
<li><span class="math inline">\(f(x;\theta)\)</span> is the <strong>likelihood</strong>.</li>
<li>The denominator is the marginal distribution of <span class="math inline">\(x\)</span>, often represented as a normalizing constant <span class="math inline">\(c(x)\)</span> which is free of <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Thus, we can state the proportional relationship:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
\]</span></p>
</div>
<div id="exm-binomial-beta" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Binomial-beta Conjugacy)</strong></span> Consider an experiment where <span class="math inline">\(x|\theta \sim \text{Bin}(n, \theta)\)</span>. The likelihood function is:</p>
<p><span class="math display">\[
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
\]</span></p>
<p>Suppose we choose a Beta distribution as the prior for <span class="math inline">\(\theta\)</span>, such that <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>. The prior density is:</p>
<p><span class="math display">\[
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
\]</span></p>
<p>where <span class="math inline">\(B(a,b)\)</span> is the Beta function defined as <span class="math inline">\(\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta\)</span>.</p>
<p>To find the posterior, we multiply the prior and the likelihood:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
\]</span></p>
<p>Combining terms with the same base:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
\]</span></p>
<p>We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:</p>
<p><span class="math display">\[
\theta|x \sim \text{Beta}(a+x, b+n-x)
\]</span></p>
<p><strong>Properties of the Posterior:</strong></p>
<ul>
<li><p>The posterior mean is: <span class="math display">\[E(\theta|x) = \frac{a+x}{a+b+n}\]</span> As <span class="math inline">\(n \to \infty\)</span>, this approximates the maximum likelihood estimate <span class="math inline">\(\frac{x}{n}\)</span>.</p></li>
<li><p>The posterior variance is: <span class="math display">\[\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\]</span> For large <span class="math inline">\(n\)</span>, this approximates <span class="math inline">\(\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}\)</span>.</p></li>
</ul>
<p><strong>Numerical Illustration:</strong></p>
<p>Suppose we are estimating a probability <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><strong>Prior:</strong> <span class="math inline">\(\theta \sim \text{Beta}(2, 2)\)</span> (Mean = 0.5).</li>
<li><strong>Data:</strong> 10 trials, 8 successes (<span class="math inline">\(n=10, x=8\)</span>).</li>
<li><strong>Posterior:</strong> <span class="math inline">\(\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)\)</span> (Mean <span class="math inline">\(\approx\)</span> 0.71).</li>
</ul>
<p>The plot below shows the prior (dashed) and posterior (solid) densities.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior: Beta(2, 2)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(theta, <span class="at">shape1 =</span> <span class="dv">2</span>, <span class="at">shape2 =</span> <span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior: Beta(10, 4)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(theta, <span class="at">shape1 =</span> <span class="dv">10</span>, <span class="at">shape2 =</span> <span class="dv">4</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta, posterior, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(theta), <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Beta Prior vs Posterior"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fu">c</span>(prior, posterior))))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(theta, prior, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Prior Beta(2,2)"</span>, <span class="st">"Posterior Beta(10,4)"</span>),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-beta-conjugacy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-beta-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-beta-conjugacy-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beta-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Prior vs Posterior for Beta-Binomial Example
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-normal-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Normal-normal Conjugacy (known Variance))</strong></span> Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed (i.i.d.) variables such that <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known.</p>
<p>We assign a Normal prior to the mean <span class="math inline">\(\mu\)</span>: <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span>.</p>
<p>To find the posterior <span class="math inline">\(\pi(\mu|x_1, \dots, x_n)\)</span>, let <span class="math inline">\(x = (x_1, \dots, x_n)\)</span>. The posterior is proportional to:</p>
<p><span class="math display">\[
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
\]</span></p>
<p><span class="math display">\[
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
\]</span></p>
<p><strong>Posterior Precision:</strong></p>
<p>It is often more convenient to work with <strong>precision</strong> (the inverse of variance). Let:</p>
<ul>
<li><span class="math inline">\(\tau_0 = 1/\sigma_0^2\)</span> (Prior precision)</li>
<li><span class="math inline">\(\tau = 1/\sigma^2\)</span> (Data precision)</li>
<li><span class="math inline">\(\tau_1 = 1/\sigma_1^2\)</span> (Posterior precision)</li>
</ul>
<p>The relationship is additive:</p>
<p><span class="math display">\[
\tau_1 = \tau_0 + n\tau
\]</span></p>
<p><span class="math display">\[
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
\]</span></p>
<p>The posterior mean <span class="math inline">\(\mu_1\)</span> is a weighted average of the prior mean and the sample mean:</p>
<p><span class="math display">\[
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
\]</span></p>
<p>So, the posterior distribution is:</p>
<p><span class="math display">\[
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
\]</span></p>
<p><strong>Numerical Illustration:</strong></p>
<p>Suppose we estimate a mean height <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li><strong>Known Variance:</strong> <span class="math inline">\(\sigma^2 = 100\)</span> (<span class="math inline">\(\tau = 0.01\)</span>).</li>
<li><strong>Prior:</strong> <span class="math inline">\(\mu \sim N(175, 25)\)</span> (Precision <span class="math inline">\(\tau_0 = 0.04\)</span>).</li>
<li><strong>Data:</strong> <span class="math inline">\(n=10, \bar{x}=180\)</span>. (Total data precision <span class="math inline">\(n\tau = 0.1\)</span>).</li>
<li><strong>Posterior:</strong>
<ul>
<li>Precision <span class="math inline">\(\tau_1 = 0.04 + 0.1 = 0.14\)</span>.</li>
<li>Variance <span class="math inline">\(\sigma_1^2 \approx 7.14\)</span>.</li>
<li>Mean <span class="math inline">\(\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6\)</span>.</li>
</ul></li>
</ul>
<p>The plot below illustrates the prior (dashed) and posterior (solid) normal densities.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mu_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">150</span>, <span class="dv">200</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior: N(175, 25) -&gt; SD = 5</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>prior_norm <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_vals, <span class="at">mean =</span> <span class="dv">175</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior: N(178.6, 7.14) -&gt; SD = Sqrt(7.14) Approx 2.67</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>posterior_norm <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_vals, <span class="at">mean =</span> <span class="fl">178.6</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fl">7.14</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu_vals, posterior_norm, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(mu), <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Normal Prior vs Posterior"</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fu">c</span>(prior_norm, posterior_norm))))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(mu_vals, prior_norm, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Prior N(175, 25)"</span>, <span class="st">"Posterior N(178.6, 7.14)"</span>),</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-normal-conjugacy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-normal-conjugacy-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Prior vs Posterior for Normal-Normal Example
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-discrete-posterior" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Discrete Posterior Calculation)</strong></span> Consider the following table where we calculate the posterior probabilities for a discrete parameter space.</p>
<p>Let the parameter <span class="math inline">\(\theta\)</span> take values <span class="math inline">\(\{1, 2, 3\}\)</span> with prior probabilities <span class="math inline">\(\pi(\theta)\)</span>. Let the data <span class="math inline">\(x\)</span> take values <span class="math inline">\(\{0, 1, 2, \dots\}\)</span>.</p>
<p>Given:</p>
<ul>
<li>Prior <span class="math inline">\(\pi(\theta)\)</span>: <span class="math inline">\(\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3\)</span>.</li>
<li>Likelihood <span class="math inline">\(\pi(x|\theta)\)</span>:
<ul>
<li>If <span class="math inline">\(\theta=1\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1\}\)</span> (Prob = 1/2).</li>
<li>If <span class="math inline">\(\theta=2\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1, 2\}\)</span> (Prob = 1/3).</li>
<li>If <span class="math inline">\(\theta=3\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1, 2, 3\}\)</span> (Prob = 1/4).</li>
</ul></li>
</ul>
<p>Suppose we observe <span class="math inline">\(x=2\)</span>. The calculation of the posterior probabilities is summarized in the table below:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span class="math inline">\(\theta=1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\theta=2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\theta=3\)</span></th>
<th style="text-align: center;">Sum</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Prior</strong> <span class="math inline">\(\pi(\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Likelihood</strong> <span class="math inline">\(\pi(x=2|\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/4\)</span></td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Product</strong> <span class="math inline">\(\pi(\theta)\pi(x|\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/9\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/12\)</span></td>
<td style="text-align: center;"><span class="math inline">\(7/36\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Posterior</strong> <span class="math inline">\(\pi(\theta|x)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4/7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3/7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>The marginal sum (evidence) is calculated as <span class="math inline">\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\)</span>. The posterior values are obtained by dividing the product row by this sum.</p>
</div>
<div id="exm-Normal-with-Unknown-Mean-and-Variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (Normal with Unknown Mean and Variance)</strong></span> Consider <span class="math inline">\(X_1, \dots, X_n \sim N(\mu, 1/\tau)\)</span>, where both <span class="math inline">\(\mu\)</span> and the precision <span class="math inline">\(\tau\)</span> are unknown.</p>
<p>We use a <strong>Normal-Gamma</strong> conjugate prior:</p>
<ol type="1">
<li><p><span class="math inline">\(\tau \sim \text{Gamma}(\alpha, \beta)\)</span> <span class="math display">\[\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}\]</span></p></li>
<li><p><span class="math inline">\(\mu|\tau \sim N(\nu, 1/(k\tau))\)</span> <span class="math display">\[\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}\]</span></p></li>
</ol>
<p>The joint prior is the product of the conditional and the marginal: <span class="math display">\[
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
\]</span></p>
<p><strong>Derivation of the Posterior:</strong></p>
<p>First, we write the likelihood in terms of the sufficient statistics <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(S_{xx} = \sum (x_i - \bar{x})^2\)</span>: <span class="math display">\[
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
\]</span></p>
<p>Multiplying the prior by the likelihood gives the joint posterior: <span class="math display">\[
\begin{aligned}
\pi(\mu, \tau | x) &amp;\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&amp;\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
\]</span></p>
<p>Next, we complete the square for the terms involving <span class="math inline">\(\mu\)</span> inside the brackets. It can be shown that: <span class="math display">\[
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
\]</span></p>
<p>Substituting this back into the joint density and grouping terms that do not depend on <span class="math inline">\(\mu\)</span>: <span class="math display">\[
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
\]</span></p>
<p><strong>Results:</strong></p>
<p>By inspecting the factored equation above, we identify the updated parameters:</p>
<ul>
<li><p><strong>Marginal Posterior of <span class="math inline">\(\tau\)</span>:</strong> The first part corresponds to a Gamma kernel <span class="math inline">\(\tau^{\alpha' - 1} e^{-\beta'\tau}\)</span>. <span class="math display">\[\tau|x \sim \text{Gamma}(\alpha', \beta')\]</span> where <span class="math inline">\(\alpha' = \alpha + n/2\)</span> and <span class="math inline">\(\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2\)</span>.</p></li>
<li><p><strong>Conditional Posterior of <span class="math inline">\(\mu\)</span>:</strong> The second part corresponds to a Normal kernel with precision <span class="math inline">\(k'\tau\)</span>. <span class="math display">\[\mu|\tau, x \sim N(\nu', 1/(k'\tau))\]</span> where <span class="math inline">\(k' = k + n\)</span> and <span class="math inline">\(\nu' = \frac{k\nu + n\bar{x}}{k+n}\)</span>.</p></li>
</ul>
</div>
</section>
<section id="decision-theory-and-bayes-rules" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="decision-theory-and-bayes-rules"><span class="header-section-number">3.2</span> Decision Theory and Bayes Rules</h2>
<p>The general form of Bayes rule is derived by minimizing risk.</p>
<div id="def-risk" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Risk Function and Bayes Risk)</strong></span> &nbsp;</p>
<ul>
<li><strong>Risk Function:</strong> <span class="math inline">\(R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx\)</span></li>
<li><strong>Bayes Risk:</strong> The expected risk with respect to the prior. <span class="math display">\[r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta\]</span></li>
</ul>
</div>
<div id="thm-bayes-rule-minimization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Minimization of Bayes Risk)</strong></span> Minimizing the Bayes risk <span class="math inline">\(r(\pi, d)\)</span> is equivalent to minimizing the posterior expected loss for each observed <span class="math inline">\(x\)</span>. That is, the Bayes rule <span class="math inline">\(d(x)\)</span> satisfies: <span class="math display">\[
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function <span class="math inline">\(R(\theta, d)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
r(\pi, d) &amp;= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&amp;= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
\]</span></p>
<p>Assuming the conditions for Fubini’s Theorem are met, we switch the order of integration:</p>
<p><span class="math display">\[
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
\]</span></p>
<p>Recall that the joint density can be factored as <span class="math inline">\(f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)\)</span>, where <span class="math inline">\(m(x)\)</span> is the marginal density of the data. Substituting this into the inner integral:</p>
<p><span class="math display">\[
\begin{aligned}
r(\pi, d) &amp;= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&amp;= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
\]</span></p>
<p>Since the marginal density <span class="math inline">\(m(x)\)</span> is non-negative, minimizing the total integral <span class="math inline">\(r(\pi, d)\)</span> with respect to the decision rule <span class="math inline">\(d(\cdot)\)</span> is equivalent to minimizing the term inside the brackets for every <span class="math inline">\(x\)</span> (specifically where <span class="math inline">\(m(x) &gt; 0\)</span>).</p>
<p>The term inside the brackets is the <strong>Posterior Expected Loss</strong>:</p>
<p><span class="math display">\[
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
\]</span></p>
<p>Therefore, to minimize the Bayes risk, one must choose <span class="math inline">\(d(x)\)</span> to minimize the posterior expected loss for each <span class="math inline">\(x\)</span>.</p>
</div>
<section id="common-loss-functions-and-estimators" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="common-loss-functions-and-estimators"><span class="header-section-number">3.2.1</span> Common Loss Functions and Estimators</h3>
</section>
<div id="ex-squared-loss">
<section id="squared-error-loss-point-estimate" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="squared-error-loss-point-estimate"><span class="header-section-number">3.2.2</span> Squared Error Loss (point Estimate)</h3>
<p><span class="math display">\[L(\theta, a) = (\theta - a)^2\]</span></p>
<p>To find the optimal estimator <span class="math inline">\(d(x)\)</span>, we minimize <span class="math inline">\(E_{\theta|x}[(\theta - d(x))^2]\)</span>. Taking the derivative with respect to <span class="math inline">\(d\)</span> and setting to 0:</p>
<p><span class="math display">\[-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)\]</span></p>
<p><strong>Result:</strong> The Bayes rule is the <strong>posterior mean</strong>.</p>
</section>
</div>
<div id="ex-absolute-loss">
<section id="absolute-error-loss" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="absolute-error-loss"><span class="header-section-number">3.2.3</span> Absolute Error Loss</h3>
<p><span class="math display">\[L(\theta, d) = |\theta - d|\]</span></p>
<p>Minimizing <span class="math inline">\(E_{\theta|x}[|\theta - d|]\)</span> requires solving:</p>
<p><span class="math display">\[\int_{-\infty}^{d} \pi(\theta|x) d\theta = \int_{d}^{\infty} \pi(\theta|x) d\theta = \frac{1}{2}\]</span></p>
<p><strong>Result:</strong> The Bayes rule is the <strong>posterior median</strong>.</p>
</section>
</div>
<div id="ex-hypothesis-testing">
<section id="hypothesis-testing-0-1-loss" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="hypothesis-testing-0-1-loss"><span class="header-section-number">3.2.4</span> Hypothesis Testing (0-1 Loss)</h3>
<p>Testing <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> vs <span class="math inline">\(H_1: \theta \in \Theta_1\)</span>.</p>
<p><span class="math display">\[L(\theta, a) = \begin{cases} 1 &amp; \text{if error} \\ 0 &amp; \text{if correct} \end{cases}\]</span></p>
<p>The Bayes rule selects the hypothesis with the higher posterior probability.</p>
<p><span class="math display">\[d(x) = 1 \iff P(\theta \in \Theta_1 | x) \ge P(\theta \in \Theta_0 | x)\]</span></p>
</section>
</div>
<div id="def-hpd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Highest Posterior Density (HPD) Interval)</strong></span> In interval estimation, we prescribe a set <span class="math inline">\(A = (d-\delta, d+\delta)\)</span> and minimize the loss associated with <span class="math inline">\(\theta\)</span> falling outside this interval.</p>
<p>The Bayes rule <span class="math inline">\(d(x)\)</span> is the center of the interval with the highest probability coverage. This leads to the <strong>Highest Posterior Density (HPD)</strong> interval.</p>
<p>In practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the <strong>Equal-Tailed Interval</strong>, where we cut off <span class="math inline">\(\alpha/2\)</span> probability from each tail.</p>
</div>
</section>
<section id="minimax-estimation" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="minimax-estimation"><span class="header-section-number">3.3</span> Minimax Estimation</h2>
<p>A decision rule <span class="math inline">\(d(x)\)</span> is <strong>minimax</strong> if it minimizes the maximum possible risk: <span class="math inline">\(\sup_\theta R(\theta, d)\)</span>.</p>
<div id="thm-minimax-constant" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Constant Risk Theorem)</strong></span> If a Bayes rule <span class="math inline">\(d^\pi\)</span> has constant risk (i.e., <span class="math inline">\(R(\theta, d^\pi) = c\)</span> for all <span class="math inline">\(\theta\)</span>), then <span class="math inline">\(d^\pi\)</span> is a minimax estimator.</p>
</div>
<div id="ex-binomial-minimax">
<section id="binomial-minimax-estimator" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="binomial-minimax-estimator"><span class="header-section-number">3.3.1</span> Binomial Minimax Estimator</h3>
<p>Let <span class="math inline">\(X \sim \text{Bin}(n, \theta)\)</span> and <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>. The squared error loss is <span class="math inline">\(L(\theta, d) = (\theta - d)^2\)</span>. The Bayes estimator is the posterior mean: <span class="math display">\[d(x) = \frac{a+x}{a+b+n}\]</span></p>
<p>We calculate the risk <span class="math inline">\(R(\theta, d)\)</span>:</p>
<p><span class="math display">\[
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
\]</span></p>
<p>Let <span class="math inline">\(c = a+b+n\)</span>. <span class="math display">\[R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]\]</span></p>
<p>Using the bias-variance decomposition and knowing <span class="math inline">\(E(x) = n\theta\)</span> and <span class="math inline">\(E(x^2) = (n\theta)^2 + n\theta(1-\theta)\)</span>, we expand the risk function. To make the risk constant (independent of <span class="math inline">\(\theta\)</span>), we set the coefficients of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta^2\)</span> to zero.</p>
<p>Solving the resulting system of equations yields: <span class="math display">\[a = b = \frac{\sqrt{n}}{2}\]</span></p>
<p>Thus, the minimax estimator is: <span class="math display">\[d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}\]</span></p>
<p>This differs from the standard MLE <span class="math inline">\(\hat{p} = x/n\)</span> and the uniform prior Bayes estimator (<span class="math inline">\(a=b=1\)</span>).</p>
</section>
</div>
</section>
<section id="stein-estimation-and-shrinkage" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="stein-estimation-and-shrinkage"><span class="header-section-number">3.4</span> Stein Estimation and Shrinkage</h2>
<p>Consider estimating a multivariate mean vector <span class="math inline">\(\mu = (\mu_1, \dots, \mu_p)\)</span> given independent observations <span class="math inline">\(X_i \sim N(\mu_i, 1)\)</span> for <span class="math inline">\(i=1, \dots, p\)</span>.</p>
<p>The standard estimator is the MLE: <span class="math inline">\(d^0(X) = X\)</span>. The loss function is the sum of squared errors: <span class="math inline">\(L(\mu, d) = ||\mu - d||^2 = \sum (\mu_i - d_i)^2\)</span>.</p>
<div id="thm-stein-inadmissibility" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 (Stein’s Result)</strong></span> When <span class="math inline">\(p \ge 3\)</span>, the estimator <span class="math inline">\(d^0(X)\)</span> is <strong>inadmissible</strong>. There exists an estimator that strictly dominates it (has lower risk everywhere).</p>
</div>
<p>Consider the class of shrinkage estimators: <span class="math display">\[d^a(X) = \left( 1 - \frac{a}{||X||^2} \right) X\]</span> where <span class="math inline">\(X = (X_1, \dots, X_p)^T\)</span>.</p>
<p>When <span class="math inline">\(a &gt; 0\)</span>, this estimator “shrinks” the data vector toward the origin <span class="math inline">\((0, \dots, 0)\)</span>.</p>
<div id="lem-stein" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3.1 (Stein’s Lemma)</strong></span> If <span class="math inline">\(X \sim N(\mu, 1)\)</span>, then for a differentiable function <span class="math inline">\(h\)</span>: <span class="math display">\[E[(X-\mu)h(X)] = E[h'(X)]\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator <span class="math inline">\(d^a\)</span>.</p>
<p><span class="math display">\[R(\mu, d^a) = E || \mu - d^a(X) ||^2\]</span></p>
<p>After expanding and applying Stein’s Lemma, the risk becomes: <span class="math display">\[R(\mu, d^a) = p - [2a(p-2) - a^2] E \left( \frac{1}{||X||^2} \right)\]</span></p>
<p>For <span class="math inline">\(d^a\)</span> to possess lower risk than <span class="math inline">\(d^0\)</span> (where risk = <span class="math inline">\(p\)</span>), we need the term in the brackets to be positive: <span class="math display">\[2a(p-2) - a^2 &gt; 0 \implies 0 &lt; a &lt; 2(p-2)\]</span></p>
<p>The optimal choice (minimizing risk) is <span class="math inline">\(a = p-2\)</span>. This yields the <strong>James-Stein Estimator</strong>: <span class="math display">\[\delta^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X\]</span></p>
</div>
</section>
<section id="empirical-bayes" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="empirical-bayes"><span class="header-section-number">3.5</span> Empirical Bayes</h2>
<p>The James-Stein estimator can be motivated via an Empirical Bayes approach.</p>
<p><strong>Model:</strong></p>
<ol type="1">
<li><span class="math inline">\(X_i | \mu_i \sim N(\mu_i, 1)\)</span></li>
<li>Prior: <span class="math inline">\(\mu_i \sim N(0, \tau^2)\)</span></li>
</ol>
<p>The posterior mean for <span class="math inline">\(\mu_i\)</span> (if <span class="math inline">\(\tau^2\)</span> were known) is: <span class="math display">\[E(\mu_i|x_i) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i\]</span></p>
<p>The marginal distribution of <span class="math inline">\(X_i\)</span> is <span class="math inline">\(N(0, 1+\tau^2)\)</span>. Consequently, <span class="math inline">\(S = \sum X_i^2 \sim (1+\tau^2) \chi^2_p\)</span>.</p>
<p>We can estimate the unknown shrinkage factor <span class="math inline">\(B = \frac{1}{1+\tau^2}\)</span> using the data. Since <span class="math inline">\(E[ \frac{p-2}{S} ] = \frac{1}{1+\tau^2}\)</span>, we replace the theoretical shrinkage factor with its unbiased estimate: <span class="math display">\[\hat{B} = \frac{p-2}{||X||^2}\]</span></p>
<p>This recovers the James-Stein rule: <span class="math display">\[\delta^{EB}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X\]</span></p>
<div id="ex-baseball">
<section id="baseball-example-efron-morris" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="baseball-example-efron-morris"><span class="header-section-number">3.5.1</span> Baseball Example (efron &amp; Morris)</h3>
<p>We illustrate Stein estimation using baseball batting averages. Let <span class="math inline">\(y_i\)</span> be the number of hits for player <span class="math inline">\(i\)</span> in their first <span class="math inline">\(n=45\)</span> at-bats. Let <span class="math inline">\(\hat{p}_i = y_i/n\)</span> be the observed average.</p>
<p>To apply the Normal model, we use a variance-stabilizing transformation: <span class="math display">\[X_i = \sqrt{n} \arcsin(2\hat{p}_i - 1)\]</span> Under this transformation, <span class="math inline">\(X_i \approx N(\mu_i, 1)\)</span>.</p>
<p>Using the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value <span class="math inline">\(\mu_0\)</span>). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages <span class="math inline">\(\hat{p}_i\)</span>.</p>
</section>
</div>
</section>
<section id="predictive-distributions" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="predictive-distributions"><span class="header-section-number">3.6</span> Predictive Distributions</h2>
<p>A key feature of Bayesian analysis is the predictive distribution for a future observation <span class="math inline">\(x^*\)</span>.</p>
<p><span class="math display">\[f(x^*|x) = \int f(x^*|\theta) \pi(\theta|x) d\theta\]</span></p>
<div id="ex-predictive-normal">
<section id="normal-normal-predictive-distribution" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="normal-normal-predictive-distribution"><span class="header-section-number">3.6.1</span> Normal-normal Predictive Distribution</h3>
<p>If <span class="math inline">\(x_1, \dots, x_n \sim N(\mu, \sigma^2)\)</span> (with <span class="math inline">\(\sigma^2\)</span> known) and <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span>, the predictive distribution for a new observation <span class="math inline">\(x^*\)</span> is:</p>
<p><span class="math display">\[x^*|x \sim N(\mu_1, \sigma^2 + \sigma_1^2)\]</span></p>
<p>where <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\sigma_1^2\)</span> are the posterior mean and variance of <span class="math inline">\(\mu\)</span>. The predictive variance includes both the inherent sampling uncertainty (<span class="math inline">\(\sigma^2\)</span>) and the uncertainty about the parameter (<span class="math inline">\(\sigma_1^2\)</span>).</p>
</section>
</div>
</section>
<section id="hierarchical-modeling-and-mcmc" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="hierarchical-modeling-and-mcmc"><span class="header-section-number">3.7</span> Hierarchical Modeling and MCMC</h2>
<p>When analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).</p>
<p><strong>Hierarchical Structure:</strong></p>
<ol type="1">
<li><strong>Data:</strong> <span class="math inline">\(X_i | \mu_i \sim f(x_i|\mu_i)\)</span></li>
<li><strong>Parameters:</strong> <span class="math inline">\(\mu_i | \theta \sim \pi(\mu_i|\theta)\)</span></li>
<li><strong>Hyperparameters:</strong> <span class="math inline">\(\theta \sim \pi(\theta)\)</span></li>
</ol>
<p><strong>Gibbs Sampling:</strong> To estimate the posterior <span class="math inline">\(f(\mu, \theta | x)\)</span>, we sample iteratively from the <strong>full conditional distributions</strong>:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\mu_i\)</span> from <span class="math inline">\(f(\mu_i | x, \theta)\)</span>.</li>
<li>Sample <span class="math inline">\(\theta\)</span> from <span class="math inline">\(f(\theta | \mu, x)\)</span>.</li>
</ol>
<div id="ex-hierarchical-baseball">
<section id="baseball-example-with-hierarchical-model" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="baseball-example-with-hierarchical-model"><span class="header-section-number">3.7.1</span> Baseball Example with Hierarchical Model</h3>
<ul>
<li><span class="math inline">\(Y_i \sim \text{Bin}(n_i, p_i)\)</span></li>
<li>Logit transform: <span class="math inline">\(\mu_i = \text{logit}(p_i)\)</span></li>
<li><span class="math inline">\(\mu_i \sim N(\theta, \tau^2)\)</span></li>
<li>Priors on <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\tau^2\)</span>.</li>
</ul>
<p>Since the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use <strong>Metropolis-Hastings</strong> steps within the Gibbs sampler.</p>
<p><strong>Algorithm:</strong></p>
<ol type="1">
<li>Initialize parameters <span class="math inline">\(\mu^{(0)}, \theta^{(0)}, \tau^{(0)}\)</span>.</li>
<li>Propose new values based on a candidate distribution.</li>
<li>Accept or reject based on the acceptance probability ratio (Likelihood <span class="math inline">\(\times\)</span> Prior ratio).</li>
<li>Repeat until convergence.</li>
</ol>
<p>The marginal posterior density for a specific parameter (e.g., <span class="math inline">\(f(\mu_j|x)\)</span>) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.</p>
</section>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./decision.html" class="pagination-link" aria-label="Decision Theory">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Decision Theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>