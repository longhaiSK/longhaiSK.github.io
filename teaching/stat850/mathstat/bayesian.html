<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Bayesian Methods – Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./decision.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1b3e43c72e8be34557c75123b0b69e0d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d1855ce4d3ca2472244e2456266329f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/bookstyles.css">
<script src="resources/num_eq.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bayesian.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Inference</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introstatinf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#fundamental-elements-of-bayesian-inference" id="toc-fundamental-elements-of-bayesian-inference" class="nav-link active" data-scroll-target="#fundamental-elements-of-bayesian-inference"><span class="header-section-number">3.1</span> Fundamental Elements of Bayesian Inference</a></li>
  <li><a href="#bayes-rules" id="toc-bayes-rules" class="nav-link" data-scroll-target="#bayes-rules"><span class="header-section-number">3.2</span> Bayes Rules</a>
  <ul>
  <li><a href="#common-loss-functions-and-bayes-estimators" id="toc-common-loss-functions-and-bayes-estimators" class="nav-link" data-scroll-target="#common-loss-functions-and-bayes-estimators"><span class="header-section-number">3.2.1</span> Common Loss Functions and Bayes Estimators</a>
  <ul class="collapse">
  <li><a href="#squared-error-loss-point-estimate" id="toc-squared-error-loss-point-estimate" class="nav-link" data-scroll-target="#squared-error-loss-point-estimate"><span class="header-section-number">3.2.1.1</span> Squared Error Loss (point Estimate)</a></li>
  <li><a href="#absolute-error-loss" id="toc-absolute-error-loss" class="nav-link" data-scroll-target="#absolute-error-loss"><span class="header-section-number">3.2.1.2</span> Absolute Error Loss</a></li>
  <li><a href="#hypothesis-testing-0-1-loss" id="toc-hypothesis-testing-0-1-loss" class="nav-link" data-scroll-target="#hypothesis-testing-0-1-loss"><span class="header-section-number">3.2.1.3</span> Hypothesis Testing (0-1 Loss)</a></li>
  <li><a href="#classification-prediction-categorical-parameter" id="toc-classification-prediction-categorical-parameter" class="nav-link" data-scroll-target="#classification-prediction-categorical-parameter"><span class="header-section-number">3.2.1.4</span> Classification Prediction (categorical Parameter)</a></li>
  <li><a href="#interval-estimation-and-highest-posterior-density-hpd" id="toc-interval-estimation-and-highest-posterior-density-hpd" class="nav-link" data-scroll-target="#interval-estimation-and-highest-posterior-density-hpd"><span class="header-section-number">3.2.1.5</span> Interval Estimation and Highest Posterior Density (HPD)</a></li>
  </ul></li>
  <li><a href="#constant-risk-bayes-estimator-is-minimax" id="toc-constant-risk-bayes-estimator-is-minimax" class="nav-link" data-scroll-target="#constant-risk-bayes-estimator-is-minimax"><span class="header-section-number">3.2.2</span> Constant Risk Bayes Estimator Is Minimax</a></li>
  </ul></li>
  <li><a href="#steins-paradox-and-the-james-stein-estimator" id="toc-steins-paradox-and-the-james-stein-estimator" class="nav-link" data-scroll-target="#steins-paradox-and-the-james-stein-estimator"><span class="header-section-number">3.3</span> Stein’s Paradox and the James-stein Estimator</a>
  <ul>
  <li><a href="#practical-application-one-way-anova-and-borrowing-strength" id="toc-practical-application-one-way-anova-and-borrowing-strength" class="nav-link" data-scroll-target="#practical-application-one-way-anova-and-borrowing-strength"><span class="header-section-number">3.3.1</span> Practical Application: One-way ANOVA and “borrowing Strength”</a></li>
  <li><a href="#why-is-this-paradoxical" id="toc-why-is-this-paradoxical" class="nav-link" data-scroll-target="#why-is-this-paradoxical"><span class="header-section-number">3.3.2</span> Why is this Paradoxical?</a></li>
  <li><a href="#what-we-learned" id="toc-what-we-learned" class="nav-link" data-scroll-target="#what-we-learned"><span class="header-section-number">3.3.3</span> What We Learned</a></li>
  </ul></li>
  <li><a href="#empirical-bayes-rules" id="toc-empirical-bayes-rules" class="nav-link" data-scroll-target="#empirical-bayes-rules"><span class="header-section-number">3.4</span> Empirical Bayes Rules</a>
  <ul>
  <li><a href="#the-general-empirical-bayes-framework" id="toc-the-general-empirical-bayes-framework" class="nav-link" data-scroll-target="#the-general-empirical-bayes-framework"><span class="header-section-number">3.4.1</span> The General Empirical Bayes Framework</a></li>
  <li><a href="#deriving-james-stein-as-empirical-bayes" id="toc-deriving-james-stein-as-empirical-bayes" class="nav-link" data-scroll-target="#deriving-james-stein-as-empirical-bayes"><span class="header-section-number">3.4.2</span> Deriving James-Stein as Empirical Bayes</a></li>
  </ul></li>
  <li><a href="#hierarchical-modeling-via-mcmc" id="toc-hierarchical-modeling-via-mcmc" class="nav-link" data-scroll-target="#hierarchical-modeling-via-mcmc"><span class="header-section-number">3.5</span> Hierarchical Modeling via MCMC</a>
  <ul>
  <li><a href="#hierarchical-model-structure" id="toc-hierarchical-model-structure" class="nav-link" data-scroll-target="#hierarchical-model-structure"><span class="header-section-number">3.5.1</span> Hierarchical Model Structure</a></li>
  <li><a href="#graphical-model-representation-tree-structure" id="toc-graphical-model-representation-tree-structure" class="nav-link" data-scroll-target="#graphical-model-representation-tree-structure"><span class="header-section-number">3.5.2</span> Graphical Model Representation (Tree Structure)</a></li>
  <li><a href="#mcmc-estimation" id="toc-mcmc-estimation" class="nav-link" data-scroll-target="#mcmc-estimation"><span class="header-section-number">3.5.3</span> MCMC Estimation</a>
  <ul class="collapse">
  <li><a href="#gibbs-sampling-algorithm" id="toc-gibbs-sampling-algorithm" class="nav-link" data-scroll-target="#gibbs-sampling-algorithm"><span class="header-section-number">3.5.3.1</span> Gibbs Sampling Algorithm</a></li>
  <li><a href="#metropolis-hastings-mh-sampling" id="toc-metropolis-hastings-mh-sampling" class="nav-link" data-scroll-target="#metropolis-hastings-mh-sampling"><span class="header-section-number">3.5.3.2</span> Metropolis-Hastings (MH) Sampling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#case-study-1998-major-league-baseball-home-run-race" id="toc-case-study-1998-major-league-baseball-home-run-race" class="nav-link" data-scroll-target="#case-study-1998-major-league-baseball-home-run-race"><span class="header-section-number">3.6</span> Case Study: 1998 Major League Baseball Home Run Race</a>
  <ul>
  <li><a href="#transforming-data" id="toc-transforming-data" class="nav-link" data-scroll-target="#transforming-data"><span class="header-section-number">3.6.1</span> Transforming Data</a></li>
  <li><a href="#true-season-parameter-mu_i-or-p_iseason" id="toc-true-season-parameter-mu_i-or-p_iseason" class="nav-link" data-scroll-target="#true-season-parameter-mu_i-or-p_iseason"><span class="header-section-number">3.6.2</span> True Season Parameter (<span class="math inline">\(\mu_i\)</span> or <span class="math inline">\(p_i^{Season}\)</span>)</a></li>
  <li><a href="#methods-for-estimating-mu_i-transformed-scale" id="toc-methods-for-estimating-mu_i-transformed-scale" class="nav-link" data-scroll-target="#methods-for-estimating-mu_i-transformed-scale"><span class="header-section-number">3.6.3</span> Methods for Estimating <span class="math inline">\(\mu_i\)</span> (Transformed Scale)</a>
  <ul class="collapse">
  <li><a href="#method-1-simple-estimation-mle" id="toc-method-1-simple-estimation-mle" class="nav-link" data-scroll-target="#method-1-simple-estimation-mle"><span class="header-section-number">3.6.3.1</span> Method 1: Simple Estimation (MLE)</a></li>
  <li><a href="#method-2-empirical-bayes-james-stein" id="toc-method-2-empirical-bayes-james-stein" class="nav-link" data-scroll-target="#method-2-empirical-bayes-james-stein"><span class="header-section-number">3.6.3.2</span> Method 2: Empirical Bayes (James-Stein)</a></li>
  <li><a href="#method-3-fully-bayesian-mcmc-brms" id="toc-method-3-fully-bayesian-mcmc-brms" class="nav-link" data-scroll-target="#method-3-fully-bayesian-mcmc-brms"><span class="header-section-number">3.6.3.3</span> Method 3: Fully Bayesian MCMC (brms)</a></li>
  </ul></li>
  <li><a href="#comparison-of-estimates-of-mu_i" id="toc-comparison-of-estimates-of-mu_i" class="nav-link" data-scroll-target="#comparison-of-estimates-of-mu_i"><span class="header-section-number">3.6.4</span> Comparison of Estimates of <span class="math inline">\(\mu_i\)</span></a></li>
  <li><a href="#methods-for-estimating-p_i-directly" id="toc-methods-for-estimating-p_i-directly" class="nav-link" data-scroll-target="#methods-for-estimating-p_i-directly"><span class="header-section-number">3.6.5</span> Methods for Estimating <span class="math inline">\(p_i\)</span> directly</a>
  <ul class="collapse">
  <li><a href="#method-1-3-converting-hat-mu_i-back-to-p_i" id="toc-method-1-3-converting-hat-mu_i-back-to-p_i" class="nav-link" data-scroll-target="#method-1-3-converting-hat-mu_i-back-to-p_i"><span class="header-section-number">3.6.5.1</span> Method 1-3: Converting <span class="math inline">\(\hat \mu_i\)</span> back to <span class="math inline">\(p_i\)</span></a></li>
  <li><a href="#method-4-hierarchical-logistic-regression-logit-normal" id="toc-method-4-hierarchical-logistic-regression-logit-normal" class="nav-link" data-scroll-target="#method-4-hierarchical-logistic-regression-logit-normal"><span class="header-section-number">3.6.5.2</span> Method 4: Hierarchical Logistic Regression (Logit-Normal)</a></li>
  <li><a href="#method-5-optimal-bayes-estimator-weighted-median" id="toc-method-5-optimal-bayes-estimator-weighted-median" class="nav-link" data-scroll-target="#method-5-optimal-bayes-estimator-weighted-median"><span class="header-section-number">3.6.5.3</span> Method 5: Optimal Bayes Estimator (Weighted Median)</a></li>
  <li><a href="#comparison-of-all-five-estimates-probability-scale" id="toc-comparison-of-all-five-estimates-probability-scale" class="nav-link" data-scroll-target="#comparison-of-all-five-estimates-probability-scale"><span class="header-section-number">3.6.5.4</span> Comparison of All Five Estimates (Probability Scale)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bayesian-predictive-distributions" id="toc-bayesian-predictive-distributions" class="nav-link" data-scroll-target="#bayesian-predictive-distributions"><span class="header-section-number">3.7</span> Bayesian Predictive Distributions</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Methods</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="fundamental-elements-of-bayesian-inference" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="fundamental-elements-of-bayesian-inference"><span class="header-section-number">3.1</span> Fundamental Elements of Bayesian Inference</h2>
<p>The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).</p>
<div id="def-posterior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Posterior Distribution)</strong></span> Suppose we have a parameter <span class="math inline">\(\theta\)</span> with a prior distribution denoted by <span class="math inline">\(\pi(\theta)\)</span>. If we observe data <span class="math inline">\(x\)</span> drawn from a distribution with probability density function (pdf) <span class="math inline">\(f(x; \theta)\)</span>, then the <strong>posterior density</strong> of <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(x\)</span> is defined as:</p>
<p><span class="math display">\[
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{m(x)}
\]</span></p>
<p>where <span class="math inline">\(m(x)\)</span> is the <strong>marginal distribution</strong> (or marginal likelihood) of the data, calculated as: <span class="math display">\[
m(x) = \int_{\Theta} \pi(\theta) f(x;\theta) d\theta
\]</span></p>
<p>In this context, <span class="math inline">\(m(x)\)</span> acts as a normalizing constant. Since it depends only on the data <span class="math inline">\(x\)</span> and not on the parameter <span class="math inline">\(\theta\)</span>, it ensures that the posterior density integrates to 1 but does not influence the <strong>shape</strong> of the posterior distribution.</p>
<p>Thus, we often state the proportional relationship:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
\]</span></p>
</div>
<div id="exm-binomial-beta" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Binomial-beta Conjugacy)</strong></span> Consider an experiment where <span class="math inline">\(x|\theta \sim \text{Bin}(n, \theta)\)</span>. The likelihood function is:</p>
<p><span class="math display">\[
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
\]</span></p>
<p>Suppose we choose a Beta distribution as the prior for <span class="math inline">\(\theta\)</span>, such that <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>. The prior density is:</p>
<p><span class="math display">\[
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
\]</span></p>
<p>where <span class="math inline">\(B(a,b)\)</span> is the Beta function defined as <span class="math inline">\(\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta\)</span>.</p>
<p>To find the posterior, we multiply the prior and the likelihood:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
\]</span></p>
<p>Combining terms with the same base:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
\]</span></p>
<p>We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:</p>
<p><span class="math display">\[
\theta|x \sim \text{Beta}(a+x, b+n-x)
\]</span></p>
<p><strong>Properties of the Posterior:</strong></p>
<ul>
<li><p>The posterior mean is: <span class="math display">\[E(\theta|x) = \frac{a+x}{a+b+n}\]</span> As <span class="math inline">\(n \to \infty\)</span>, this approximates the maximum likelihood estimate <span class="math inline">\(\frac{x}{n}\)</span>.</p></li>
<li><p>The posterior variance is: <span class="math display">\[\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\]</span> For large <span class="math inline">\(n\)</span>, this approximates <span class="math inline">\(\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}\)</span>.</p></li>
</ul>
<p><strong>Numerical Illustration:</strong></p>
<p>Suppose we are estimating a probability <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><strong>Prior:</strong> <span class="math inline">\(\theta \sim \text{Beta}(2, 2)\)</span> (Mean = 0.5).</li>
<li><strong>Data:</strong> 10 trials, 8 successes (<span class="math inline">\(n=10, x=8\)</span>).</li>
<li><strong>Posterior:</strong> <span class="math inline">\(\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)\)</span> (Mean <span class="math inline">\(\approx\)</span> 0.71).</li>
</ul>
<p>The plot below shows the prior (dashed) and posterior (solid) densities.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior: Beta(2, 2)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(theta, <span class="at">shape1 =</span> <span class="dv">2</span>, <span class="at">shape2 =</span> <span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior: Beta(10, 4)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(theta, <span class="at">shape1 =</span> <span class="dv">10</span>, <span class="at">shape2 =</span> <span class="dv">4</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta, posterior, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(theta), <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Beta Prior vs Posterior"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fu">c</span>(prior, posterior))))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(theta, prior, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Prior Beta(2,2)"</span>, <span class="st">"Posterior Beta(10,4)"</span>),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-beta-conjugacy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-beta-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-beta-conjugacy-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beta-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Prior vs Posterior for Beta-Binomial Example
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-normal-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Normal-normal Conjugacy (known Variance))</strong></span> Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed (i.i.d.) variables such that <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known.</p>
<p>We assign a Normal prior to the mean <span class="math inline">\(\mu\)</span>: <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span>.</p>
<p>To find the posterior <span class="math inline">\(\pi(\mu|x_1, \dots, x_n)\)</span>, let <span class="math inline">\(x = (x_1, \dots, x_n)\)</span>. The posterior is proportional to:</p>
<p><span class="math display">\[
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
\]</span></p>
<p><span class="math display">\[
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
\]</span></p>
<p><strong>Posterior Precision:</strong></p>
<p>It is often more convenient to work with <strong>precision</strong> (the inverse of variance). Let:</p>
<ul>
<li><span class="math inline">\(\tau_0 = 1/\sigma_0^2\)</span> (Prior precision)</li>
<li><span class="math inline">\(\tau = 1/\sigma^2\)</span> (Data precision)</li>
<li><span class="math inline">\(\tau_1 = 1/\sigma_1^2\)</span> (Posterior precision)</li>
</ul>
<p>The relationship is additive:</p>
<p><span class="math display">\[
\tau_1 = \tau_0 + n\tau
\]</span></p>
<p><span class="math display">\[
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
\]</span></p>
<p>The posterior mean <span class="math inline">\(\mu_1\)</span> is a weighted average of the prior mean and the sample mean:</p>
<p><span class="math display">\[
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
\]</span></p>
<p>So, the posterior distribution is:</p>
<p><span class="math display">\[
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
\]</span></p>
<p><strong>Numerical Illustration:</strong></p>
<p>Suppose we estimate a mean height <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li><strong>Known Variance:</strong> <span class="math inline">\(\sigma^2 = 100\)</span> (<span class="math inline">\(\tau = 0.01\)</span>).</li>
<li><strong>Prior:</strong> <span class="math inline">\(\mu \sim N(175, 25)\)</span> (Precision <span class="math inline">\(\tau_0 = 0.04\)</span>).</li>
<li><strong>Data:</strong> <span class="math inline">\(n=10, \bar{x}=180\)</span>. (Total data precision <span class="math inline">\(n\tau = 0.1\)</span>).</li>
<li><strong>Posterior:</strong>
<ul>
<li>Precision <span class="math inline">\(\tau_1 = 0.04 + 0.1 = 0.14\)</span>.</li>
<li>Variance <span class="math inline">\(\sigma_1^2 \approx 7.14\)</span>.</li>
<li>Mean <span class="math inline">\(\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6\)</span>.</li>
</ul></li>
</ul>
<p>The plot below illustrates the prior (dashed) and posterior (solid) normal densities.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mu_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">150</span>, <span class="dv">200</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior: N(175, 25) -&gt; SD = 5</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>prior_norm <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_vals, <span class="at">mean =</span> <span class="dv">175</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior: N(178.6, 7.14) -&gt; SD = Sqrt(7.14) Approx 2.67</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>posterior_norm <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_vals, <span class="at">mean =</span> <span class="fl">178.6</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fl">7.14</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu_vals, posterior_norm, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(mu), <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Normal Prior vs Posterior"</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(<span class="fu">c</span>(prior_norm, posterior_norm))))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(mu_vals, prior_norm, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Prior N(175, 25)"</span>, <span class="st">"Posterior N(178.6, 7.14)"</span>),</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-normal-conjugacy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-normal-conjugacy-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-conjugacy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Prior vs Posterior for Normal-Normal Example
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-discrete-posterior" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Discrete Posterior Calculation)</strong></span> Consider the following table where we calculate the posterior probabilities for a discrete parameter space.</p>
<p>Let the parameter <span class="math inline">\(\theta\)</span> take values <span class="math inline">\(\{1, 2, 3\}\)</span> with prior probabilities <span class="math inline">\(\pi(\theta)\)</span>. Let the data <span class="math inline">\(x\)</span> take values <span class="math inline">\(\{0, 1, 2, \dots\}\)</span>.</p>
<p>Given:</p>
<ul>
<li>Prior <span class="math inline">\(\pi(\theta)\)</span>: <span class="math inline">\(\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3\)</span>.</li>
<li>Likelihood <span class="math inline">\(\pi(x|\theta)\)</span>:
<ul>
<li>If <span class="math inline">\(\theta=1\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1\}\)</span> (Prob = 1/2).</li>
<li>If <span class="math inline">\(\theta=2\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1, 2\}\)</span> (Prob = 1/3).</li>
<li>If <span class="math inline">\(\theta=3\)</span>, <span class="math inline">\(x \sim \text{Uniform on } \{0, 1, 2, 3\}\)</span> (Prob = 1/4).</li>
</ul></li>
</ul>
<p>Suppose we observe <span class="math inline">\(x=2\)</span>. The calculation of the posterior probabilities is summarized in the table below:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><span class="math inline">\(\theta=1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\theta=2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\theta=3\)</span></th>
<th style="text-align: center;">Sum</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Prior</strong> <span class="math inline">\(\pi(\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Likelihood</strong> <span class="math inline">\(\pi(x=2|\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/4\)</span></td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Product</strong> <span class="math inline">\(\pi(\theta)\pi(x|\theta)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/9\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1/12\)</span></td>
<td style="text-align: center;"><span class="math inline">\(7/36\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Posterior</strong> <span class="math inline">\(\pi(\theta|x)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4/7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3/7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>The marginal sum (evidence) is calculated as <span class="math inline">\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\)</span>. The posterior values are obtained by dividing the product row by this sum.</p>
</div>
<div id="exm-Normal-with-Unknown-Mean-and-Variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (Normal with Unknown Mean and Variance)</strong></span> Consider <span class="math inline">\(X_1, \dots, X_n \sim N(\mu, 1/\tau)\)</span>, where both <span class="math inline">\(\mu\)</span> and the precision <span class="math inline">\(\tau\)</span> are unknown.</p>
<p>We use a <strong>Normal-Gamma</strong> conjugate prior:</p>
<ol type="1">
<li><p><span class="math inline">\(\tau \sim \text{Gamma}(\alpha, \beta)\)</span> <span class="math display">\[\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}\]</span></p></li>
<li><p><span class="math inline">\(\mu|\tau \sim N(\nu, 1/(k\tau))\)</span> <span class="math display">\[\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}\]</span></p></li>
</ol>
<p>The joint prior is the product of the conditional and the marginal: <span class="math display">\[
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
\]</span></p>
<p><strong>Derivation of the Posterior:</strong></p>
<p>First, we write the likelihood in terms of the sufficient statistics <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(S_{xx} = \sum (x_i - \bar{x})^2\)</span>: <span class="math display">\[
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
\]</span></p>
<p>Multiplying the prior by the likelihood gives the joint posterior: <span class="math display">\[
\begin{aligned}
\pi(\mu, \tau | x) &amp;\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&amp;\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
\]</span></p>
<p>Next, we complete the square for the terms involving <span class="math inline">\(\mu\)</span> inside the brackets. It can be shown that: <span class="math display">\[
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
\]</span></p>
<p>Substituting this back into the joint density and grouping terms that do not depend on <span class="math inline">\(\mu\)</span>: <span class="math display">\[
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
\]</span></p>
<p><strong>Results:</strong></p>
<p>By inspecting the factored equation above, we identify the updated parameters:</p>
<ul>
<li><p><strong>Marginal Posterior of <span class="math inline">\(\tau\)</span>:</strong> The first part corresponds to a Gamma kernel <span class="math inline">\(\tau^{\alpha' - 1} e^{-\beta'\tau}\)</span>. <span class="math display">\[\tau|x \sim \text{Gamma}(\alpha', \beta')\]</span> where <span class="math inline">\(\alpha' = \alpha + n/2\)</span> and <span class="math inline">\(\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2\)</span>.</p></li>
<li><p><strong>Conditional Posterior of <span class="math inline">\(\mu\)</span>:</strong> The second part corresponds to a Normal kernel with precision <span class="math inline">\(k'\tau\)</span>. <span class="math display">\[\mu|\tau, x \sim N(\nu', 1/(k'\tau))\]</span> where <span class="math inline">\(k' = k + n\)</span> and <span class="math inline">\(\nu' = \frac{k\nu + n\bar{x}}{k+n}\)</span>.</p></li>
</ul>
</div>
</section>
<section id="bayes-rules" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="bayes-rules"><span class="header-section-number">3.2</span> Bayes Rules</h2>
<p>The general form of Bayes rule is derived by minimizing risk.</p>
<div id="def-risk" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Risk Function and Bayes Risk)</strong></span> &nbsp;</p>
<ul>
<li><strong>Risk Function:</strong> <span class="math inline">\(R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx\)</span></li>
<li><strong>Bayes Risk:</strong> The expected risk with respect to the prior. <span class="math display">\[r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta\]</span></li>
</ul>
</div>
<div id="thm-bayes-rule-minimization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Minimization of Bayes Risk)</strong></span> Minimizing the Bayes risk <span class="math inline">\(r(\pi, d)\)</span> is equivalent to minimizing the posterior expected loss for each observed <span class="math inline">\(x\)</span>. That is, the Bayes rule <span class="math inline">\(d(x)\)</span> satisfies: <span class="math display">\[
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function <span class="math inline">\(R(\theta, d)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
r(\pi, d) &amp;= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&amp;= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
\]</span></p>
<p>Assuming the conditions for Fubini’s Theorem are met, we switch the order of integration:</p>
<p><span class="math display">\[
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
\]</span></p>
<p>Recall that the joint density can be factored as <span class="math inline">\(f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)\)</span>, where <span class="math inline">\(m(x)\)</span> is the marginal density of the data. Substituting this into the inner integral:</p>
<p><span class="math display">\[
\begin{aligned}
r(\pi, d) &amp;= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&amp;= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
\]</span></p>
<p>Since the marginal density <span class="math inline">\(m(x)\)</span> is non-negative, minimizing the total integral <span class="math inline">\(r(\pi, d)\)</span> with respect to the decision rule <span class="math inline">\(d(\cdot)\)</span> is equivalent to minimizing the term inside the brackets for every <span class="math inline">\(x\)</span> (specifically where <span class="math inline">\(m(x) &gt; 0\)</span>).</p>
<p>The term inside the brackets is the <strong>Posterior Expected Loss</strong>:</p>
<p><span class="math display">\[
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
\]</span></p>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Therefore, to minimize the Bayes risk, one just need to choose <span class="math inline">\(d(x)\)</span> to minimize the posterior expected loss for each <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<p>The following diagram summarizes the general workflow for deriving a Bayes estimator:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bayes-workflow" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 80% !important;" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayes-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-bayes-workflow-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayes-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Workflow for Finding the Bayes Rule
</figcaption>
</figure>
</div>
</div>
</div>
<section id="common-loss-functions-and-bayes-estimators" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="common-loss-functions-and-bayes-estimators"><span class="header-section-number">3.2.1</span> Common Loss Functions and Bayes Estimators</h3>
<section id="squared-error-loss-point-estimate" class="level4" data-number="3.2.1.1">
<h4 data-number="3.2.1.1" class="anchored" data-anchor-id="squared-error-loss-point-estimate"><span class="header-section-number">3.2.1.1</span> Squared Error Loss (point Estimate)</h4>
<p><span class="math display">\[L(\theta, a) = (\theta - a)^2\]</span></p>
<p>To find the optimal estimator <span class="math inline">\(d(x)\)</span>, we minimize the posterior expected loss <span class="math inline">\(E_{\theta|x}[(\theta - d(x))^2]\)</span>. Taking the derivative with respect to <span class="math inline">\(d\)</span> and setting it to 0:</p>
<p><span class="math display">\[-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)\]</span></p>
<p><strong>Result:</strong> The Bayes rule under squared error loss is the <strong>posterior mean</strong>.</p>
</section>
<section id="absolute-error-loss" class="level4" data-number="3.2.1.2">
<h4 data-number="3.2.1.2" class="anchored" data-anchor-id="absolute-error-loss"><span class="header-section-number">3.2.1.2</span> Absolute Error Loss</h4>
<p><span class="math display">\[L(\theta, d) = |\theta - d|\]</span></p>
<p>To find the Bayes rule, we minimize the posterior expected loss:</p>
<p><span class="math display">\[
\psi(d) = E_{\theta|x} [ |\theta - d| ] = \int_{-\infty}^{\infty} |\theta - d| \, dF(\theta|x)
\]</span></p>
<p>where <span class="math inline">\(F(\theta|x)\)</span> is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point <span class="math inline">\(d\)</span>:</p>
<p><span class="math display">\[
\psi(d) = \int_{-\infty}^{d} (d - \theta) \, dF(\theta|x) + \int_{d}^{\infty} (\theta - d) \, dF(\theta|x)
\]</span></p>
<p>We find the minimum by analyzing the rate of change of <span class="math inline">\(\psi(d)\)</span> with respect to <span class="math inline">\(d\)</span>. Differentiating (or taking the subgradient for non-differentiable points):</p>
<p><span class="math display">\[
\frac{d}{dd} \psi(d) = \int_{-\infty}^{d} 1 \, dF(\theta|x) - \int_{d}^{\infty} 1 \, dF(\theta|x) = P(\theta \le d|x) - P(\theta &gt; d|x)
\]</span></p>
<p>Setting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:</p>
<p><span class="math display">\[
P(\theta \le d|x) = P(\theta &gt; d|x)
\]</span></p>
<p>Since the total probability is 1, this condition simplifies to finding <span class="math inline">\(d\)</span> such that the cumulative probability is <span class="math inline">\(1/2\)</span>.</p>
<p><strong>General Case (Discrete or Mixed Distributions)</strong></p>
<p>In cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to <span class="math inline">\(F(d) = 0.5\)</span> may not exist. To generalize, the Bayes rule is defined as any <strong>median</strong> <span class="math inline">\(m\)</span> of the posterior distribution.</p>
<p>A median is formally defined as any value <span class="math inline">\(m\)</span> that satisfies the following two conditions simultaneously:</p>
<ul>
<li><span class="math inline">\(P(\theta \le m|x) \ge \frac{1}{2}\)</span></li>
<li><span class="math inline">\(P(\theta \ge m|x) \ge \frac{1}{2}\)</span></li>
</ul>
<p><strong>Result:</strong> The Bayes rule under absolute error loss is the <strong>posterior median</strong>.</p>
</section>
<section id="hypothesis-testing-0-1-loss" class="level4" data-number="3.2.1.3">
<h4 data-number="3.2.1.3" class="anchored" data-anchor-id="hypothesis-testing-0-1-loss"><span class="header-section-number">3.2.1.3</span> Hypothesis Testing (0-1 Loss)</h4>
<p>Consider the hypothesis test <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> versus <span class="math inline">\(H_1: \theta \in \Theta_1\)</span>. We define the decision space as <span class="math inline">\(\mathcal{A} = \{0, 1\}\)</span>, where <span class="math inline">\(a=0\)</span> means accepting <span class="math inline">\(H_0\)</span> and <span class="math inline">\(a=1\)</span> means rejecting <span class="math inline">\(H_0\)</span> (accepting <span class="math inline">\(H_1\)</span>).</p>
<p><strong>Case 1: 0-1 Loss</strong></p>
<p>The standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one: <span class="math display">\[L(\theta, a) = \begin{cases} 0 &amp; \text{if } \theta \in \Theta_0, a=0 \ (\text{Correct } H_0) \\ 1 &amp; \text{if } \theta \in \Theta_0, a=1 \ (\text{Type I Error}) \\ 1 &amp; \text{if } \theta \in \Theta_1, a=0 \ (\text{Type II Error}) \\ 0 &amp; \text{if } \theta \in \Theta_1, a=1 \ (\text{Correct } H_1) \end{cases}\]</span></p>
<p>To find the Bayes rule, we minimize the <strong>posterior expected loss</strong> for a given <span class="math inline">\(x\)</span>, denoted as <span class="math inline">\(E_{\theta|x}[L(\theta, a)]\)</span>.</p>
<ul>
<li><p><strong>Expected Loss for choosing <span class="math inline">\(a=0\)</span> (Accept <span class="math inline">\(H_0\)</span>):</strong> <span class="math display">\[
  E_{\theta|x}[L(\theta, 0)] = 0 \cdot P(\theta \in \Theta_0|x) + 1 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_1|x)
  \]</span></p></li>
<li><p><strong>Expected Loss for choosing <span class="math inline">\(a=1\)</span> (Reject <span class="math inline">\(H_0\)</span>):</strong> <span class="math display">\[
  E_{\theta|x}[L(\theta, 1)] = 1 \cdot P(\theta \in \Theta_0|x) + 0 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_0|x)
  \]</span></p></li>
</ul>
<p>The Bayes rule selects the action with the smaller expected loss. Thus, we choose <span class="math inline">\(a=1\)</span> if: <span class="math display">\[
P(\theta \in \Theta_0|x) \le P(\theta \in \Theta_1|x)
\]</span> This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.</p>
<p><strong>Case 2: General Loss (Asymmetric Costs)</strong></p>
<p>In many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let <span class="math inline">\(c_1\)</span> be the cost of a Type I error and <span class="math inline">\(c_2\)</span> be the cost of a Type II error. Usually, we normalize one cost to 1.</p>
<p>Suppose the loss function is: <span class="math display">\[L(\theta, a) = \begin{cases} 0 &amp; \text{if } \theta \in \Theta_0, a=0 \\ c &amp; \text{if } \theta \in \Theta_0, a=1 \ (\text{Cost of Type I Error}) \\ 1 &amp; \text{if } \theta \in \Theta_1, a=0 \ (\text{Cost of Type II Error}) \\ 0 &amp; \text{if } \theta \in \Theta_1, a=1 \end{cases}\]</span></p>
<p>We again calculate the posterior expected loss:</p>
<ul>
<li><p><strong>Expected Loss for <span class="math inline">\(a=0\)</span>:</strong> <span class="math display">\[E[L(\theta, 0)|x] = 0 \cdot P(\Theta_0|x) + 1 \cdot P(\Theta_1|x) = P(\Theta_1|x)\]</span></p></li>
<li><p><strong>Expected Loss for <span class="math inline">\(a=1\)</span>:</strong> <span class="math display">\[E[L(\theta, 1)|x] = c \cdot P(\Theta_0|x) + 0 \cdot P(\Theta_1|x) = c P(\Theta_0|x)\]</span></p></li>
</ul>
<p>We reject <span class="math inline">\(H_0\)</span> (<span class="math inline">\(a=1\)</span>) if the expected loss of doing so is lower: <span class="math display">\[
c P(\Theta_0|x) \le P(\Theta_1|x)
\]</span></p>
<p>Since <span class="math inline">\(P(\Theta_1|x) = 1 - P(\Theta_0|x)\)</span>, we can rewrite this condition as: <span class="math display">\[
c P(\Theta_0|x) \le 1 - P(\Theta_0|x) \implies (1+c) P(\Theta_0|x) \le 1
\]</span> <span class="math display">\[
P(\Theta_0|x) \le \frac{1}{1+c}
\]</span></p>
<p><strong>Result:</strong> With asymmetric costs, we accept <span class="math inline">\(H_1\)</span> only if the posterior probability of the null hypothesis is sufficiently small (below the threshold <span class="math inline">\(\frac{1}{1+c}\)</span>). If the cost of false rejection <span class="math inline">\(c\)</span> is high, we require stronger evidence against <span class="math inline">\(H_0\)</span>.</p>
</section>
<section id="classification-prediction-categorical-parameter" class="level4" data-number="3.2.1.4">
<h4 data-number="3.2.1.4" class="anchored" data-anchor-id="classification-prediction-categorical-parameter"><span class="header-section-number">3.2.1.4</span> Classification Prediction (categorical Parameter)</h4>
<p>In classification problems, the parameter of interest is a discrete class label <span class="math inline">\(\theta\)</span> (often denoted as <span class="math inline">\(y\)</span>) taking values in a set of categories <span class="math inline">\(\{1, 2, \dots, K\}\)</span>. The goal is to predict the true class label based on observed features <span class="math inline">\(x\)</span>.</p>
<p>We typically employ the <strong>0-1 loss function</strong>, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:</p>
<p><span class="math display">\[L(\theta, \hat{\theta}) = \begin{cases} 0 &amp; \text{if } \hat{\theta} = \theta \ (\text{Correct Classification}) \\ 1 &amp; \text{if } \hat{\theta} \neq \theta \ (\text{Misclassification}) \end{cases}\]</span></p>
<p>To find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.</p>
<p><span class="math display">\[
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta} L(\theta, \hat{\theta}) \pi(\theta|x)
\]</span></p>
<p>Since the loss is 1 only when the predicted class <span class="math inline">\(\hat{\theta}\)</span> differs from the true class <span class="math inline">\(\theta\)</span>, this sum simplifies to:</p>
<p><span class="math display">\[
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta \neq \hat{\theta}} 1 \cdot \pi(\theta|x) = P(\theta \neq \hat{\theta} | x) = 1 - P(\theta = \hat{\theta} | x)
\]</span></p>
<p>Minimizing the misclassification rate <span class="math inline">\(1 - P(\theta = \hat{\theta} | x)\)</span> is mathematically equivalent to maximizing the probability of being correct, <span class="math inline">\(P(\theta = \hat{\theta} | x)\)</span>.</p>
<p><strong>Result:</strong> The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the <strong>Maximum A Posteriori (MAP)</strong> estimator, in the context of machine learning and pattern recognition, this decision rule is known as the <strong>Bayes Optimal Classifier</strong>.</p>
<p><span class="math display">\[
\hat{\theta}_{\text{Bayes}}(x) = \underset{k \in \{1, \dots, K\}}{\arg\max} \ P(\theta = k | x)
\]</span></p>
</section>
<section id="interval-estimation-and-highest-posterior-density-hpd" class="level4" data-number="3.2.1.5">
<h4 data-number="3.2.1.5" class="anchored" data-anchor-id="interval-estimation-and-highest-posterior-density-hpd"><span class="header-section-number">3.2.1.5</span> Interval Estimation and Highest Posterior Density (HPD)</h4>
<p>We can motivate the choice of a Credible Interval by defining a specific loss function for interval estimation. Suppose we seek an estimate <span class="math inline">\(d\)</span> and specify a tolerance <span class="math inline">\(\delta &gt; 0\)</span>. We define the loss function as:</p>
<p><span class="math display">\[
L(\theta, d) = \begin{cases}
0 &amp; \text{if } |\theta - d| \le \delta \\
1 &amp; \text{if } |\theta - d| &gt; \delta
\end{cases}
\]</span></p>
<p>The <strong>Expected Posterior Loss</strong> is the posterior probability that <span class="math inline">\(\theta\)</span> lies outside the interval <span class="math inline">\((d - \delta, d + \delta)\)</span>. Therefore, minimizing the loss is equivalent to finding the interval of fixed length <span class="math inline">\(2\delta\)</span> that <strong>maximizes the posterior probability</strong>:</p>
<p><span class="math display">\[ P(d - \delta \le \theta \le d + \delta \mid x) \]</span></p>
<p>In practice, we typically reverse this formulation: instead of fixing the length <span class="math inline">\(2\delta\)</span>, we fix the coverage probability <span class="math inline">\(1-\alpha\)</span> (e.g., 0.95) and seek the interval with the <strong>shortest possible length</strong>. This results in the <strong>Highest Posterior Density (HPD)</strong> interval, defined as the region where the posterior density exceeds a certain threshold <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[ C_{HPD} = \{ \theta : \pi(\theta \mid x) \ge c \} \]</span></p>
<p>This HPD interval is optimal because it includes the “most likely” values of <span class="math inline">\(\theta\)</span> and, for a unimodal distribution, provides the narrowest interval for a given confidence level.</p>
<p><strong>Comparison with Equal-Tailed Intervals:</strong></p>
<ul>
<li><strong>Equal-Tailed Interval:</strong> We simply cut off <span class="math inline">\(\alpha/2\)</span> probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.</li>
<li><strong>HPD Interval:</strong> This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.</li>
</ul>
<p>The plot below illustrates a skewed posterior distribution (Gamma). Notice how the <strong>HPD Interval (Blue)</strong> is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the <strong>Equal-Tailed Interval (Red)</strong>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">15</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y_vals <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(x_vals, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">rate =</span> <span class="fl">0.5</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Target Coverage</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.10</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>target_prob <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> alpha</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. Equal-tailed Interval (quantiles)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>eq_lower <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(alpha<span class="sc">/</span><span class="dv">2</span>, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">rate =</span> <span class="fl">0.5</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>eq_upper <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="dv">1</span> <span class="sc">-</span> alpha<span class="sc">/</span><span class="dv">2</span>, <span class="at">shape =</span> <span class="dv">2</span>, <span class="at">rate =</span> <span class="fl">0.5</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. HPD Interval (density Threshold Optimization)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="do">## We Look for a Density Threshold K Such That the Area Above K Is 0.90</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>find_hpd <span class="ot">&lt;-</span> <span class="cf">function</span>(dist_vals, density_vals, probability) {</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Sort density values</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  ord <span class="ot">&lt;-</span> <span class="fu">order</span>(density_vals, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  sorted_dens <span class="ot">&lt;-</span> density_vals[ord]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  sorted_dist <span class="ot">&lt;-</span> dist_vals[ord]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Accumulate probability (approximation)</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>  dx <span class="ot">&lt;-</span> <span class="fu">diff</span>(dist_vals)[<span class="dv">1</span>]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>  cum_prob <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(sorted_dens <span class="sc">*</span> dx)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Find cutoff index</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>  cutoff_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(cum_prob <span class="sc">&gt;=</span> probability)[<span class="dv">1</span>]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Get the subset of x values</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>  hpd_set <span class="ot">&lt;-</span> sorted_dist[<span class="dv">1</span><span class="sc">:</span>cutoff_idx]</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">min</span>(hpd_set), <span class="fu">max</span>(hpd_set)))</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>hpd_bounds <span class="ot">&lt;-</span> <span class="fu">find_hpd</span>(x_vals, y_vals, target_prob)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>hpd_lower <span class="ot">&lt;-</span> hpd_bounds[<span class="dv">1</span>]</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>hpd_upper <span class="ot">&lt;-</span> hpd_bounds[<span class="dv">2</span>]</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="do">## Plotting</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_vals, y_vals, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"black"</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"90% Credible Intervals (Skewed Posterior)"</span>,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(theta), <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(y_vals) <span class="sc">*</span> <span class="fl">1.2</span>))</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="do">## Shade HPD</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(x_vals[x_vals <span class="sc">&gt;=</span> hpd_lower <span class="sc">&amp;</span> x_vals <span class="sc">&lt;=</span> hpd_upper], hpd_upper, hpd_lower),</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(y_vals[x_vals <span class="sc">&gt;=</span> hpd_lower <span class="sc">&amp;</span> x_vals <span class="sc">&lt;=</span> hpd_upper], <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.2</span>), <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="do">## Draw Equal-tailed Lines (red)</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">c</span>(eq_lower, eq_upper), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="do">## Draw HPD Lines (blue)</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">c</span>(hpd_lower, hpd_upper), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Posterior Density"</span>, </span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Equal-Tailed (Len: "</span>, <span class="fu">round</span>(eq_upper <span class="sc">-</span> eq_lower, <span class="dv">2</span>), <span class="st">")"</span>), </span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"HPD (Len: "</span>, <span class="fu">round</span>(hpd_upper <span class="sc">-</span> hpd_lower, <span class="dv">2</span>), <span class="st">")"</span>)),</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span>), </span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.2</span>)), <span class="at">border =</span> <span class="cn">NA</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-hpd-vs-equal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hpd-vs-equal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-hpd-vs-equal-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hpd-vs-equal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="constant-risk-bayes-estimator-is-minimax" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="constant-risk-bayes-estimator-is-minimax"><span class="header-section-number">3.2.2</span> Constant Risk Bayes Estimator Is Minimax</h3>
<p>A decision rule <span class="math inline">\(d(x)\)</span> is <strong>minimax</strong> if it minimizes the maximum possible risk: <span class="math inline">\(\sup_\theta R(\theta, d)\)</span>.</p>
<div id="thm-minimax-constant" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Constant Risk Bayes Estimator Is Minimax)</strong></span> Let <span class="math inline">\(\delta^\pi\)</span> be a Bayes estimator with respect to a prior <span class="math inline">\(\pi\)</span>. If the risk function of <span class="math inline">\(\delta^\pi\)</span> is constant on the parameter space <span class="math inline">\(\Theta\)</span>, such that <span class="math inline">\(R(\theta, \delta^\pi) = c\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span>, then <span class="math inline">\(\delta^\pi\)</span> is a minimax estimator.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\delta^\pi\)</span> be the Bayes estimator with constant risk <span class="math inline">\(c\)</span>. First, we compute its Bayes risk <span class="math inline">\(r(\pi, \delta^\pi)\)</span>. Since the risk is constant:</p>
<p><span class="math display">\[
r(\pi, \delta^\pi) = \int_\Theta R(\theta, \delta^\pi) \pi(\theta) d\theta = \int_\Theta c \, \pi(\theta) d\theta = c
\]</span></p>
<p>Now, let <span class="math inline">\(\delta'\)</span> be any arbitrary estimator. By the definition of a Bayes estimator, <span class="math inline">\(\delta^\pi\)</span> minimizes the Bayes risk among all estimators:</p>
<p><span class="math display">\[
r(\pi, \delta^\pi) \le r(\pi, \delta')
\]</span></p>
<p>Next, we observe that the Bayes risk of <span class="math inline">\(\delta'\)</span> is the expectation of its risk function with respect to the prior <span class="math inline">\(\pi\)</span>. An average cannot exceed the maximum value of the function being averaged (the supremum):</p>
<p><span class="math display">\[
r(\pi, \delta') = \int_\Theta R(\theta, \delta') \pi(\theta) d\theta \le \sup_{\theta \in \Theta} R(\theta, \delta') \cdot \int_\Theta \pi(\theta) d\theta = \sup_{\theta \in \Theta} R(\theta, \delta')
\]</span></p>
<p>Combining these inequalities, we have:</p>
<p><span class="math display">\[
\sup_{\theta \in \Theta} R(\theta, \delta^\pi) = c = r(\pi, \delta^\pi) \le r(\pi, \delta') \le \sup_{\theta \in \Theta} R(\theta, \delta')
\]</span></p>
<p>Since <span class="math inline">\(\sup_\theta R(\theta, \delta^\pi) \le \sup_\theta R(\theta, \delta')\)</span> holds for any estimator <span class="math inline">\(\delta'\)</span>, <span class="math inline">\(\delta^\pi\)</span> minimizes the maximum risk. Therefore, it is minimax.</p>
</div>
<p>The plot below visualizes the logic of the proof. The red line represents the <strong>Constant Risk Bayes Estimator</strong> (<span class="math inline">\(\delta^\pi\)</span>), which has a constant height <span class="math inline">\(c\)</span>. The blue curve represents an <strong>Arbitrary Estimator</strong> (<span class="math inline">\(\delta'\)</span>).</p>
<p>Because <span class="math inline">\(\delta^\pi\)</span> minimizes the <em>weighted average</em> risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk (<span class="math inline">\(\sup R\)</span>) strictly greater than or equal to <span class="math inline">\(c\)</span>. Thus, the constant risk estimator has the lowest possible maximum.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define Parameter Space Theta</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Constant Risk Bayes Estimator (risk = C)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>c_val <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>risk_bayes <span class="ot">&lt;-</span> <span class="fu">rep</span>(c_val, <span class="fu">length</span>(theta))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Arbitrary Alternative Estimator</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># This Function Is Chosen Such That It Dips Below C but Rises Above It Elsewhere</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>risk_alt <span class="ot">&lt;-</span> c_val <span class="sc">+</span> <span class="fl">0.2</span> <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> theta) <span class="sc">-</span> <span class="fl">0.05</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta, risk_alt, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ylab =</span> <span class="st">"Risk R(theta, d)"</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(theta),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Geometry of the Minimax Theorem"</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Constant Risk Line</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(theta, risk_bayes, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark the Maximum of the Alternative</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>max_alt <span class="ot">&lt;-</span> <span class="fu">max</span>(risk_alt)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>max_theta <span class="ot">&lt;-</span> theta[<span class="fu">which.max</span>(risk_alt)]</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(max_theta, max_alt, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(max_theta, max_alt, <span class="at">labels =</span> <span class="fu">expression</span>(sup<span class="sc">~</span><span class="fu">R</span>(theta, delta<span class="sc">^</span><span class="st">"'"</span>)), <span class="at">pos =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Label the Constant Risk</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.1</span>, c_val, <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">R</span>(theta, delta<span class="sc">^</span>pi) <span class="sc">==</span> c), <span class="at">pos =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Legend</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Arbitrary Estimator"</span>, <span class="st">"Constant Risk Bayes Est."</span>),</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-minimax-proof" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-minimax-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-minimax-proof-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-minimax-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-binomial-minimax" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 (Binomial Minimax Estimator)</strong></span> Let <span class="math inline">\(X \sim \text{Bin}(n, \theta)\)</span> and <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>. The squared error loss is <span class="math inline">\(L(\theta, d) = (\theta - d)^2\)</span>. The Bayes estimator is the posterior mean: <span class="math display">\[d(x) = \frac{a+x}{a+b+n}\]</span></p>
<p>We calculate the risk <span class="math inline">\(R(\theta, d)\)</span>:</p>
<p><span class="math display">\[
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
\]</span></p>
<p>Let <span class="math inline">\(c = a+b+n\)</span>. <span class="math display">\[R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]\]</span></p>
<p>Using the bias-variance decomposition and knowing <span class="math inline">\(E(x) = n\theta\)</span> and <span class="math inline">\(E(x^2) = (n\theta)^2 + n\theta(1-\theta)\)</span>, we expand the risk function. To make the risk constant (independent of <span class="math inline">\(\theta\)</span>), we set the coefficients of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta^2\)</span> to zero.</p>
<p>Solving the resulting system of equations yields: <span class="math display">\[a = b = \frac{\sqrt{n}}{2}\]</span></p>
<p>Thus, the minimax estimator is: <span class="math display">\[d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}\]</span></p>
<p>This differs from the standard MLE <span class="math inline">\(\hat{p} = x/n\)</span> and the uniform prior Bayes estimator (<span class="math inline">\(a=b=1\)</span>).</p>
</div>
</section>
</section>
<section id="steins-paradox-and-the-james-stein-estimator" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="steins-paradox-and-the-james-stein-estimator"><span class="header-section-number">3.3</span> Stein’s Paradox and the James-stein Estimator</h2>
<p>In high-dimensional estimation (<span class="math inline">\(p \ge 3\)</span>), the Maximum Likelihood Estimator (MLE) is inadmissible under squared error loss. The <strong>James-Stein Estimator</strong> dominates the MLE, meaning it achieves lower risk for all values of <span class="math inline">\(\theta\)</span>.</p>
<p>Consider the setting:</p>
<ul>
<li>Data: <span class="math inline">\(X \sim N_p(\theta, I)\)</span></li>
<li>Prior: <span class="math inline">\(\theta \sim N_p(0, \tau^2 I)\)</span></li>
<li>Estimator: <span class="math inline">\(d^{JS}(x) = \left( 1 - \frac{p-2}{||x||^2} \right) x\)</span></li>
</ul>
<p>We can derive the Bayes Risk <span class="math inline">\(r(\pi, d^{JS})\)</span> of this estimator using two equivalent methods: minimizing the expected frequentist risk, or minimizing the expected posterior loss.</p>
<div id="thm-js-bayes-risk" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Bayes Risk of James-stein Estimator)</strong></span> For <span class="math inline">\(p \ge 3\)</span>, the Bayes risk of the James-Stein estimator <span class="math inline">\(d^{JS}\)</span> with respect to the prior <span class="math inline">\(\theta \sim N(0, \tau^2 I)\)</span> is:</p>
<p><span class="math display">\[
r(\pi, d^{JS}) = \frac{p\tau^2 + 2}{\tau^2 + 1}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Method 1: Integration over the Prior (Frequentist Risk approach)</strong></p>
<p>The Bayes risk is defined as <span class="math inline">\(r(\pi, d) = E_\pi [ R(\theta, d) ]\)</span>.</p>
<p>First, recall the frequentist risk of the James-Stein estimator for a fixed <span class="math inline">\(\theta\)</span>. Using Stein’s Lemma, the risk is given by: <span class="math display">\[
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
\]</span></p>
<p>To find the Bayes risk, we take the expectation of this risk with respect to the prior <span class="math inline">\(\pi(\theta)\)</span>: <span class="math display">\[
r(\pi, d^{JS}) = \int R(\theta, d^{JS}) \pi(\theta) d\theta = p - (p-2)^2 E_\pi \left[ E_\theta \left( \frac{1}{||X||^2} \right) \right]
\]</span></p>
<p>By the law of iterated expectations, <span class="math inline">\(E_\pi [ E_\theta (\cdot) ]\)</span> is equivalent to the expectation with respect to the marginal distribution of <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(m(x)\)</span>. Under the conjugate prior, the marginal distribution is <span class="math inline">\(X \sim N(0, (1+\tau^2)I)\)</span>.</p>
<p>Consequently, the quantity <span class="math inline">\(\frac{||X||^2}{1+\tau^2}\)</span> follows a Chi-squared distribution with <span class="math inline">\(p\)</span> degrees of freedom (<span class="math inline">\(\chi^2_p\)</span>). The expectation of the inverse chi-square is: <span class="math display">\[
E \left[ \frac{1}{||X||^2} \right] = \frac{1}{1+\tau^2} E \left[ \frac{1}{\chi^2_p} \right] = \frac{1}{1+\tau^2} \cdot \frac{1}{p-2}
\]</span></p>
<p>Substituting this back into the risk equation: <span class="math display">\[
\begin{aligned}
r(\pi, d^{JS}) &amp;= p - (p-2)^2 \cdot \frac{1}{(p-2)(1+\tau^2)} \\
&amp;= p - \frac{p-2}{1+\tau^2} \\
&amp;= \frac{p(1+\tau^2) - (p-2)}{1+\tau^2} \\
&amp;= \frac{p\tau^2 + p - p + 2}{1+\tau^2} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\end{aligned}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Method 2: Integration over the Marginal (Posterior Loss approach)</strong></p>
<p>Alternatively, we can compute the Bayes risk by first finding the posterior expected loss for a given <span class="math inline">\(x\)</span>, and then averaging over the marginal distribution of <span class="math inline">\(x\)</span>: <span class="math display">\[
r(\pi, d) = E_m [ E_{\theta|x} [ L(\theta, d(x)) ] ]
\]</span></p>
<p><strong>Step 1: Posterior Expected Loss</strong></p>
<p>The posterior distribution of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(x\)</span> is: <span class="math display">\[
\theta | x \sim N \left( \frac{\tau^2}{1+\tau^2}x, \frac{\tau^2}{1+\tau^2}I \right)
\]</span></p>
<p>The expected squared error loss can be decomposed into the variance (trace) and the squared bias: <span class="math display">\[
E_{\theta|x} [ ||\theta - d^{JS}(x)||^2 ] = \text{tr}(\text{Var}(\theta|x)) + || E[\theta|x] - d^{JS}(x) ||^2
\]</span></p>
<ul>
<li><p><strong>Trace term:</strong> <span class="math display">\[\text{tr} \left( \frac{\tau^2}{1+\tau^2} I_p \right) = \frac{p\tau^2}{1+\tau^2}\]</span></p></li>
<li><p><strong>Squared Bias term:</strong> Let <span class="math inline">\(B = \frac{1}{1+\tau^2}\)</span>. Then <span class="math inline">\(E[\theta|x] = (1-B)x\)</span>. The estimator is <span class="math inline">\(d^{JS}(x) = (1 - \frac{p-2}{||x||^2})x\)</span>. The difference is: <span class="math display">\[
  E[\theta|x] - d^{JS}(x) = \left( (1-B) - \left( 1 - \frac{p-2}{||x||^2} \right) \right) x = \left( \frac{p-2}{||x||^2} - B \right) x
  \]</span> Squaring the norm gives: <span class="math display">\[
  \left( \frac{p-2}{||x||^2} - B \right)^2 ||x||^2 = \frac{(p-2)^2}{||x||^2} - 2B(p-2) + B^2 ||x||^2
  \]</span></p></li>
</ul>
<p><strong>Step 2: Expectation with respect to Marginal <span class="math inline">\(X\)</span></strong></p>
<p>We now take the expectation <span class="math inline">\(E_m[\cdot]\)</span> of the posterior loss. Recall <span class="math inline">\(X \sim N(0, (1+\tau^2)I)\)</span>, so <span class="math inline">\(E[||X||^2] = p(1+\tau^2)\)</span> and <span class="math inline">\(E[1/||X||^2] = \frac{1}{(p-2)(1+\tau^2)}\)</span>.</p>
<ul>
<li><strong>Expectation of Trace term:</strong> Constant, remains <span class="math inline">\(\frac{p\tau^2}{1+\tau^2}\)</span>.</li>
<li><strong>Expectation of Bias term:</strong> <span class="math display">\[
  \begin{aligned}
  E_m \left[ \frac{(p-2)^2}{||X||^2} - \frac{2(p-2)}{1+\tau^2} + \frac{||X||^2}{(1+\tau^2)^2} \right] &amp;= (p-2)^2 \frac{1}{(p-2)(1+\tau^2)} - \frac{2(p-2)}{1+\tau^2} + \frac{p(1+\tau^2)}{(1+\tau^2)^2} \\
  &amp;= \frac{p-2}{1+\tau^2} - \frac{2p-4}{1+\tau^2} + \frac{p}{1+\tau^2} \\
  &amp;= \frac{p - 2 - 2p + 4 + p}{1+\tau^2} \\
  &amp;= \frac{2}{1+\tau^2}
  \end{aligned}
  \]</span></li>
</ul>
<p><strong>Step 3: Combine Terms</strong></p>
<p><span class="math display">\[
r(\pi, d^{JS}) = \underbrace{\frac{p\tau^2}{1+\tau^2}}_{\text{Variance Part}} + \underbrace{\frac{2}{1+\tau^2}}_{\text{Bias Part}} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\]</span></p>
<p>Both methods yield the same result.</p>
</div>
<div id="thm-mle-inadmissible" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 (Inadmissibility of the MLE in High Dimensions (stein’s Phenomenon))</strong></span> Let <span class="math inline">\(X \sim N_p(\theta, I)\)</span> be a <span class="math inline">\(p\)</span>-dimensional random vector with <span class="math inline">\(p \ge 3\)</span>. Under the squared error loss function <span class="math inline">\(L(\theta, d) = ||\theta - d||^2\)</span>, the standard Maximum Likelihood Estimator <span class="math inline">\(d^0(X) = X\)</span> is <strong>inadmissible</strong>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To show that <span class="math inline">\(d^0(X) = X\)</span> is inadmissible, we must find another estimator that dominates it (i.e., has equal or lower risk for all <span class="math inline">\(\theta\)</span>, and strictly lower risk for at least one <span class="math inline">\(\theta\)</span>).</p>
<p>First, consider the risk of the standard estimator <span class="math inline">\(d^0\)</span>. Since <span class="math inline">\(X_i \sim N(\theta_i, 1)\)</span> are independent:</p>
<p><span class="math display">\[
R(\theta, d^0) = E_\theta [ ||X - \theta||^2 ] = \sum_{i=1}^p E [ (X_i - \theta_i)^2 ] = \sum_{i=1}^p \text{Var}(X_i) = p
\]</span></p>
<p>Now consider the James-Stein estimator <span class="math inline">\(d^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X\)</span>. As established in the derivation of the Bayes Risk in <a href="#thm-js-bayes-risk" class="quarto-xref">Theorem&nbsp;<span>3.3</span></a>, the frequentist risk function of <span class="math inline">\(d^{JS}\)</span> is:</p>
<p><span class="math display">\[
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
\]</span></p>
<p>Since the random variable <span class="math inline">\(||X||^2\)</span> is non-negative and not identically infinity, the expectation <span class="math inline">\(E_\theta [ 1/||X||^2 ]\)</span> is strictly positive for all <span class="math inline">\(\theta\)</span>. Therefore:</p>
<p><span class="math display">\[
R(\theta, d^{JS}) &lt; p = R(\theta, d^0) \quad \text{for all } \theta \in \mathbb{R}^p
\]</span></p>
<p>Because <span class="math inline">\(d^{JS}\)</span> achieves a strictly lower risk than <span class="math inline">\(d^0\)</span> everywhere in the parameter space, <span class="math inline">\(d^0\)</span> is dominated by <span class="math inline">\(d^{JS}\)</span> and is thus inadmissible.</p>
</div>
<section id="practical-application-one-way-anova-and-borrowing-strength" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="practical-application-one-way-anova-and-borrowing-strength"><span class="header-section-number">3.3.1</span> Practical Application: One-way ANOVA and “borrowing Strength”</h3>
<div id="exm-anova-js" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6</strong></span> Consider a One-Way ANOVA setting where we wish to estimate the means of <span class="math inline">\(p\)</span> different independent groups (e.g., the true batting averages of <span class="math inline">\(p=10\)</span> baseball players, or the efficacy of <span class="math inline">\(p=5\)</span> different hospital treatments).</p>
<ul>
<li><strong>Model:</strong> Let <span class="math inline">\(X_i \sim N(\theta_i, \sigma^2)\)</span> be the observed sample mean for group <span class="math inline">\(i\)</span>, for <span class="math inline">\(i = 1, \dots, p\)</span>.</li>
<li><strong>Goal:</strong> Estimate the vector of true means <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)\)</span> simultaneously. The loss is the sum of squared errors: <span class="math inline">\(L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}) = \sum (\theta_i - \hat{\theta}_i)^2\)</span>.</li>
</ul>
<p><strong>The MLE Approach (Total Separation):</strong> The standard estimator is <span class="math inline">\(\hat{\theta}_i^{\text{MLE}} = X_i\)</span>. This estimates each group entirely independently, using only data from that specific group. If a specific player has a lucky streak, their estimate is very high; if they are unlucky, it is very low.</p>
<p><strong>The James-Stein Approach (Shrinkage / Pooling):</strong> In this context, the James-Stein estimator (specifically the variation shrinking toward the grand mean <span class="math inline">\(\bar{X}\)</span>) is: <span class="math display">\[
\hat{\theta}_i^{JS} = \bar{X} + \left( 1 - \frac{(p-3)\sigma^2}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X})
\]</span></p>
<p><strong>Why is this better?</strong> Even though the groups might be physically independent (e.g., distinct hospitals), the James-Stein estimator <strong>“borrows strength”</strong> from the ensemble.</p>
<ol type="1">
<li><strong>Noise Reduction:</strong> Extreme observations <span class="math inline">\(X_i\)</span> are likely to contain more positive noise than signal. Shrinking them toward the global average <span class="math inline">\(\bar{X}\)</span> reduces this variance.</li>
<li><strong>Stein’s Paradox:</strong> While <span class="math inline">\(\hat{\theta}_i^{JS}\)</span> introduces bias (estimates are pulled toward the center), the reduction in variance is so significant that the <strong>Total Risk</strong> (sum of squared errors over all groups) is strictly lower than that of the MLE, provided <span class="math inline">\(p \ge 3\)</span>.</li>
</ol>
<p>Thus, estimating the groups <em>together</em> yields a more accurate global picture than estimating them <em>separately</em>, even if the groups are independent.</p>
</div>
</section>
<section id="why-is-this-paradoxical" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="why-is-this-paradoxical"><span class="header-section-number">3.3.2</span> Why is this Paradoxical?</h3>
<p>The result that <span class="math inline">\(d^{JS}\)</span> dominates <span class="math inline">\(d^0\)</span> is called <strong>Stein’s Paradox</strong> because it defies intuition in several ways:</p>
<ol type="1">
<li><strong>Independence Irrelevance:</strong> The result holds even if the components <span class="math inline">\(X_i\)</span> are completely unrelated (e.g., <span class="math inline">\(X_1\)</span> is the price of tea in China, <span class="math inline">\(X_2\)</span> is the temperature in Saskatoon, and <span class="math inline">\(X_3\)</span> is the weight of a local cat). It seems absurd that combining unrelated data improves the estimate of each, but the combined risk is indeed lower.</li>
<li><strong>No “Free Lunch”:</strong> The James-Stein estimator does not improve every individual component <span class="math inline">\(\theta_i\)</span> simultaneously for every realization. Instead, it minimizes the <strong>total</strong> risk <span class="math inline">\(\sum E(\hat{\theta}_i - \theta_i)^2\)</span>. It sacrifices accuracy on outliers (by biasing them) to gain significant stability on the bulk of the data.</li>
<li><strong>Destruction of Symmetry:</strong> The MLE is invariant under translation and rotation. The James-Stein estimator breaks this symmetry by shrinking toward an arbitrary point (usually the origin or the grand mean), yet it yields a better objective performance.</li>
</ol>
</section>
<section id="what-we-learned" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="what-we-learned"><span class="header-section-number">3.3.3</span> What We Learned</h3>
<ol type="1">
<li><strong>Bias-Variance Tradeoff:</strong> This is the most famous example where introducing <strong>bias</strong> (shrinkage) leads to a massive reduction in <strong>variance</strong>, thereby reducing the overall Mean Squared Error (MSE). Unbiasedness is not always a virtue in estimation.</li>
<li><strong>Inadmissibility in High Dimensions:</strong> Intuitions formed in 1D or 2D (where MLE is admissible) fail in higher dimensions (<span class="math inline">\(p \ge 3\)</span>). The volume of space grows so fast that “standard” diffuse priors or MLEs become inefficient.</li>
<li><strong>Hierarchical Modeling:</strong> Stein’s result provides the theoretical foundation for <strong>Hierarchical Bayesian Models</strong>. When we assume parameters come from a common distribution (e.g., <span class="math inline">\(\theta_i \sim N(\mu, \tau^2)\)</span>), we naturally derive shrinkage estimators that “borrow strength” across groups, formalized as Empirical Bayes or fully Bayesian methods.</li>
</ol>
</section>
</section>
<section id="empirical-bayes-rules" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="empirical-bayes-rules"><span class="header-section-number">3.4</span> Empirical Bayes Rules</h2>
<p>The James-Stein estimator provides a natural entry point into the concept of <strong>Empirical Bayes (EB)</strong>. While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.</p>
<section id="the-general-empirical-bayes-framework" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="the-general-empirical-bayes-framework"><span class="header-section-number">3.4.1</span> The General Empirical Bayes Framework</h3>
<p>In a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to “learn” the prior.</p>
<p>The workflow typically follows these steps:</p>
<ol type="1">
<li><p><strong>Hierarchical Model:</strong> We assume the data <span class="math inline">\(X\)</span> comes from a distribution <span class="math inline">\(f(x|\theta)\)</span>, and the parameter <span class="math inline">\(\theta\)</span> comes from a prior <span class="math inline">\(\pi(\theta|\eta)\)</span> controlled by hyperparameters <span class="math inline">\(\eta\)</span>.</p></li>
<li><p><strong>Marginal Likelihood (Evidence):</strong> We integrate out the parameter <span class="math inline">\(\theta\)</span> to obtain the marginal distribution of the data given the hyperparameters: <span class="math display">\[m(x|\eta) = \int f(x|\theta) \pi(\theta|\eta) d\theta\]</span></p></li>
<li><p><strong>Estimation of Hyperparameters:</strong> Instead of fixing <span class="math inline">\(\eta\)</span>, we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments: <span class="math display">\[\hat{\eta} = \underset{\eta}{\arg\max} \ m(x|\eta)\]</span></p></li>
<li><p><strong>Posterior Inference:</strong> We proceed with standard Bayesian inference, but we substitute the estimated estimate <span class="math inline">\(\hat{\eta}\)</span> into the posterior: <span class="math display">\[\pi(\theta|x, \hat{\eta}) \propto f(x|\theta) \pi(\theta|\hat{\eta})\]</span></p></li>
</ol>
<p><strong>Discussion:</strong></p>
<ul>
<li><strong>“Borrowing Strength”:</strong> EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.</li>
<li><strong>The Critique:</strong> A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate <span class="math inline">\(\theta\)</span>) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a “hyperprior” on <span class="math inline">\(\eta\)</span> and integrate it out.</li>
</ul>
</section>
<section id="deriving-james-stein-as-empirical-bayes" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="deriving-james-stein-as-empirical-bayes"><span class="header-section-number">3.4.2</span> Deriving James-Stein as Empirical Bayes</h3>
<p>We can derive the James-Stein rule explicitly using this framework.</p>
<p><strong>Model:</strong></p>
<ol type="1">
<li>Likelihood: <span class="math inline">\(X_i | \mu_i \sim N(\mu_i, 1)\)</span> for <span class="math inline">\(i=1, \dots, p\)</span>.</li>
<li>Prior: <span class="math inline">\(\mu_i \sim N(0, \tau^2)\)</span>. Here, <span class="math inline">\(\tau^2\)</span> is the unknown hyperparameter.</li>
</ol>
<p><strong>Step 1: The Ideal Bayes Estimator</strong></p>
<p>If we knew <span class="math inline">\(\tau^2\)</span>, the posterior distribution of <span class="math inline">\(\mu_i\)</span> would be Normal with mean: <span class="math display">\[E(\mu_i|x_i, \tau^2) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i\]</span> We define the shrinkage factor <span class="math inline">\(B = \frac{1}{1+\tau^2}\)</span>.</p>
<p><strong>Step 2: Marginal Estimation</strong></p>
<p>Since <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(X_i-\mu_i\)</span> are independent normals, the marginal distribution of the data is: <span class="math display">\[X_i \sim N(0, 1+\tau^2)\]</span> Consequently, the sum of squares <span class="math inline">\(S = ||X||^2 = \sum X_i^2\)</span> follows a scaled Chi-squared distribution: <span class="math display">\[S \sim (1+\tau^2) \chi^2_p\]</span></p>
<p><strong>Step 3: Estimating the Shrinkage Factor</strong></p>
<p>We need to estimate <span class="math inline">\(B = \frac{1}{1+\tau^2}\)</span>. Note that the expected value of an inverse Chi-square variable is <span class="math inline">\(E[1/\chi^2_p] = \frac{1}{p-2}\)</span>. Therefore: <span class="math display">\[E \left[ \frac{p-2}{S} \right] = \frac{p-2}{1+\tau^2} E\left[\frac{1}{\chi^2_p}\right] = \frac{p-2}{1+\tau^2} \cdot \frac{1}{p-2} = \frac{1}{1+\tau^2} = B\]</span></p>
<p>Thus, <span class="math inline">\(\hat{B} = \frac{p-2}{||X||^2}\)</span> is an unbiased estimator of the optimal shrinkage factor.</p>
<p><strong>Step 4: The Empirical Bayes Rule</strong></p>
<p>Plugging <span class="math inline">\(\hat{B}\)</span> into the ideal Bayes estimator recovers the James-Stein rule: <span class="math display">\[\delta^{EB}(X) = \left( 1 - \hat{B} \right) X = \left( 1 - \frac{p-2}{||X||^2} \right) X\]</span></p>
</section>
</section>
<section id="hierarchical-modeling-via-mcmc" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="hierarchical-modeling-via-mcmc"><span class="header-section-number">3.5</span> Hierarchical Modeling via MCMC</h2>
<p>In complex Bayesian settings where the posterior distribution cannot be derived analytically, we utilize hierarchical structures to represent levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate the resulting distributions.</p>
<section id="hierarchical-model-structure" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="hierarchical-model-structure"><span class="header-section-number">3.5.1</span> Hierarchical Model Structure</h3>
<p>A hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Level 1 (Data Likelihood):} &amp; \quad X_i | \mu_i, \sigma^2 \sim f(x_i | \mu_i, \sigma^2) \\
\text{Level 2 (Parameters):} &amp; \quad \mu_i | \theta, \tau^2 \sim \pi(\mu_i | \theta, \tau^2) \\
\text{Level 3 (Hyperparameters):} &amp; \quad \theta, \tau^2 \sim \pi(\theta, \tau^2)
\end{aligned}
\]</span></p>
<p>The goal is to compute the joint posterior distribution of all unobserved parameters given the data <span class="math inline">\(X = \{X_1, \dots, X_n\}\)</span>:</p>
<p><span class="math display">\[
p(\boldsymbol{\mu}, \theta, \tau^2 | X) \propto \left[ \prod_{i=1}^n f(x_i | \mu_i, \sigma^2) \pi(\mu_i | \theta, \tau^2) \right] \pi(\theta, \tau^2)
\]</span></p>
</section>
<section id="graphical-model-representation-tree-structure" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="graphical-model-representation-tree-structure"><span class="header-section-number">3.5.2</span> Graphical Model Representation (Tree Structure)</h3>
<p>The following tree diagram illustrates the conditional dependencies. Note that the parameters <span class="math inline">\(\mu_i\)</span> are conditionally independent given the hyperparameter <span class="math inline">\(\theta\)</span>, which facilitates “borrowing strength” across groups.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hierarchical-tree" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 80% !important;" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hierarchical-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-hierarchical-tree-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hierarchical-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Hierarchical Tree Structure
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="mcmc-estimation" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="mcmc-estimation"><span class="header-section-number">3.5.3</span> MCMC Estimation</h3>
<p>In hierarchical models, the joint posterior distribution <span class="math inline">\(p(\boldsymbol{\mu}, \theta | X)\)</span> often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use <strong>Markov Chain Monte Carlo (MCMC)</strong> to draw sequence of samples <span class="math inline">\(\{\boldsymbol{\mu}^{(t)}, \theta^{(t)}\}\)</span> that converge to the target posterior distribution.</p>
<section id="gibbs-sampling-algorithm" class="level4" data-number="3.5.3.1">
<h4 data-number="3.5.3.1" class="anchored" data-anchor-id="gibbs-sampling-algorithm"><span class="header-section-number">3.5.3.1</span> Gibbs Sampling Algorithm</h4>
<div id="alg-gibbs-sampling">
<p>Gibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the <strong>full conditional distributions</strong>. To sample from a target distribution <span class="math inline">\(p(\theta_1, \theta_2, \dots, \theta_k)\)</span>, the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:</p>
<p><span class="math display">\[
\begin{aligned}
\theta_1^{(t+1)} &amp;\sim p(\theta_1 | \theta_2^{(t)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
\theta_2^{(t+1)} &amp;\sim p(\theta_2 | \theta_1^{(t+1)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
&amp;\vdots \\
\theta_k^{(t+1)} &amp;\sim p(\theta_k | \theta_1^{(t+1)}, \theta_2^{(t+1)}, \dots, \theta_{k-1}^{(t+1)})
\end{aligned}
\]</span></p>
</div>
<div id="exm-hierarchical-gibbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7 (Gibbs Sampling for Groups of Normal Data)</strong></span> <strong>The Model</strong></p>
<p>To apply the general Gibbs sampling framework <span class="math inline">\(\theta_1, \theta_2, \dots, \theta_k\)</span> to our specific hierarchical model, we identify the variables as follows:</p>
<ul>
<li><p><strong>Data Observations (<span class="math inline">\(X_i\)</span>):</strong> These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school <span class="math inline">\(i\)</span>). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.</p></li>
<li><p><strong>Group-Level Parameters (<span class="math inline">\(\theta_1 = \mu_i\)</span>):</strong> These represent the latent means for each specific group or cluster. In the update step, <span class="math inline">\(\mu_i\)</span> acts as the first block of variables. It is updated by “compromising” between the local data <span class="math inline">\(X_i\)</span> and the global characteristic <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Global Hyperparameter (<span class="math inline">\(\theta_2 = \theta\)</span>):</strong> This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all <span class="math inline">\(\mu_i\)</span> values, effectively “pooling” information from all groups to estimate the overall population center.</p></li>
</ul>
<p><strong>Gibbs Update in Hierarchical Models</strong></p>
<p>In the hierarchical tree structure provided earlier, let our parameter vector be <span class="math inline">\((\mu_i, \theta)\)</span>. The “orthogonality” of the updates becomes clear when we derive the full conditionals for a Gaussian case:</p>
<ul>
<li><p><strong>Case <span class="math inline">\(\theta_1 = \mu_i\)</span>:</strong> Sample <span class="math inline">\(\mu_i^{(t+1)}\)</span> from <span class="math inline">\(p(\mu_i | X_i, \theta^{(t)})\)</span>. This is a normal distribution with: <span class="math display">\[
\mu_i^{(t+1)} \sim N\left( \frac{\tau^2 X_i + \sigma^2 \theta^{(t)}}{\sigma^2 + \tau^2}, \frac{\sigma^2 \tau^2}{\sigma^2 + \tau^2} \right)
\]</span></p></li>
<li><p><strong>Case <span class="math inline">\(\theta_2 = \theta\)</span>:</strong> Sample <span class="math inline">\(\theta^{(t+1)}\)</span> from <span class="math inline">\(p(\theta | \boldsymbol{\mu}^{(t+1)})\)</span>. Assuming a flat prior <span class="math inline">\(\pi(\theta) \propto 1\)</span>: <span class="math display">\[
\theta^{(t+1)} \sim N\left( \frac{1}{n} \sum_{i=1}^n \mu_i^{(t+1)}, \frac{\tau^2}{n} \right)
\]</span></p></li>
</ul>
</div>
<p><strong>Visual Characteristic:</strong> Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.</p>
</section>
<section id="metropolis-hastings-mh-sampling" class="level4" data-number="3.5.3.2">
<h4 data-number="3.5.3.2" class="anchored" data-anchor-id="metropolis-hastings-mh-sampling"><span class="header-section-number">3.5.3.2</span> Metropolis-Hastings (MH) Sampling</h4>
<p>When the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step <span class="math inline">\(t\)</span>:</p>
<ul>
<li><strong>Propose:</strong> Draw a candidate state <span class="math inline">\(\theta^*\)</span> from a proposal distribution <span class="math inline">\(q(\theta^* | \theta^{(t)})\)</span>.</li>
<li><strong>Accept/Reject:</strong> Calculate the acceptance probability: <span class="math display">\[
\alpha = \min \left( 1, \frac{p(\theta^* | X) q(\theta^{(t)} | \theta^*)}{p(\theta^{(t)} | X) q(\theta^* | \theta^{(t)})} \right)
\]</span></li>
<li>Set <span class="math inline">\(\theta^{(t+1)} = \theta^*\)</span> with probability <span class="math inline">\(\alpha\)</span>; otherwise, set <span class="math inline">\(\theta^{(t+1)} = \theta^{(t)}\)</span>.</li>
</ul>
<p><strong>Visual Characteristic:</strong> MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>log_target <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) { <span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> (x<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>rho<span class="sc">*</span>x<span class="sc">*</span>y <span class="sc">+</span> y<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> rho<span class="sc">^</span><span class="dv">2</span>) }</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs Path (Step-wise update)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>gx <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span>; gy <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>gx_path <span class="ot">&lt;-</span> gx; gy_path <span class="ot">&lt;-</span> gy</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>) {</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  gx <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, rho <span class="sc">*</span> gy, <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> rho<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  gx_path <span class="ot">&lt;-</span> <span class="fu">c</span>(gx_path, gx, gx); gy_path <span class="ot">&lt;-</span> <span class="fu">c</span>(gy_path, gy, gy) <span class="co"># Horizontal move</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  gy <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, rho <span class="sc">*</span> gx, <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> rho<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  gx_path <span class="ot">&lt;-</span> <span class="fu">c</span>(gx_path, gx); gy_path <span class="ot">&lt;-</span> <span class="fu">c</span>(gy_path, gy) <span class="co"># Vertical move</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># MH Path (Random Walk)</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>mx <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">50</span>); my <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">50</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>mx[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span>; my[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  px <span class="ot">&lt;-</span> mx[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.4</span>); py <span class="ot">&lt;-</span> my[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.4</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  acc <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">log_target</span>(px, py) <span class="sc">-</span> <span class="fu">log_target</span>(mx[i<span class="dv">-1</span>], my[i<span class="dv">-1</span>]))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> acc) { mx[i] <span class="ot">&lt;-</span> px; my[i] <span class="ot">&lt;-</span> py } <span class="cf">else</span> { mx[i] <span class="ot">&lt;-</span> mx[i<span class="dv">-1</span>]; my[i] <span class="ot">&lt;-</span> my[i<span class="dv">-1</span>] }</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>t_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length=</span><span class="dv">50</span>); z <span class="ot">&lt;-</span> <span class="fu">outer</span>(t_seq, t_seq, <span class="cf">function</span>(x,y) <span class="fu">exp</span>(<span class="fu">log_target</span>(x,y)))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gx_path, gy_path, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">main=</span><span class="st">"Gibbs (Orthogonal Steps)"</span>, <span class="at">xlab=</span><span class="fu">expression</span>(theta[<span class="dv">1</span>]), <span class="at">ylab=</span><span class="fu">expression</span>(theta[<span class="dv">2</span>]))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(t_seq, t_seq, z, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"gray"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mx, my, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"red"</span>, <span class="at">main=</span><span class="st">"Metropolis-Hastings (Random Walk)"</span>, <span class="at">xlab=</span><span class="fu">expression</span>(theta[<span class="dv">1</span>]), <span class="at">ylab=</span><span class="fu">expression</span>(theta[<span class="dv">2</span>]))</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(t_seq, t_seq, z, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"gray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-mcmc-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mcmc-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-mcmc-comparison-1.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mcmc-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Comparison of Sampling Paths
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="case-study-1998-major-league-baseball-home-run-race" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="case-study-1998-major-league-baseball-home-run-race"><span class="header-section-number">3.6</span> Case Study: 1998 Major League Baseball Home Run Race</h2>
<p>In 1998, the baseball world was captivated by Mark McGwire and Sammy Sosa as they chased Roger Maris’ 1961 record of 61 home runs in a single season. While McGwire and Sosa finished with 70 and 66 home runs respectively, we consider whether such performance could have been predicted using pre-season exhibition data.</p>
<p>For a set of <span class="math inline">\(i = 1, \dots, 17\)</span> players (including McGwire and Sosa), we observe their batting records in pre-season exhibition matches. Our goal is to estimate each player’s home run “strike rate” for the competitive season.</p>
<section id="transforming-data" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="transforming-data"><span class="header-section-number">3.6.1</span> Transforming Data</h3>
<p>We utilize the pre-season home runs (<span class="math inline">\(y_i\)</span>) and at-bats (<span class="math inline">\(n_i\)</span>) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
<p><span class="math display">\[
x_i = \sqrt{n_i} \arcsin\left( 2 \frac{y_i}{n_i} - 1 \right)
\]</span></p>
<p>The goal is to estimate the latent parameter <span class="math inline">\(\mu_i\)</span> for each player and compare it to the “true” regular season performance.</p>
</section>
<section id="true-season-parameter-mu_i-or-p_iseason" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="true-season-parameter-mu_i-or-p_iseason"><span class="header-section-number">3.6.2</span> True Season Parameter (<span class="math inline">\(\mu_i\)</span> or <span class="math inline">\(p_i^{Season}\)</span>)</h3>
<p>To validate our estimates, we define the “true” parameter value <span class="math inline">\(\mu_i\)</span> using the player’s performance over the full competitive season. Let <span class="math inline">\(Y_i\)</span> be the total home runs and <span class="math inline">\(N_i\)</span> be the total at-bats in the regular season. The true transformed rate is calculated as:</p>
<p><span class="math display">\[
\mu_i^{\text{season}} = \sqrt{n_i} \arcsin\left( 2 \frac{Y_i}{N_i} - 1 \right)
\]</span></p>
<p>Note that while we use the season-long probability (<span class="math inline">\(Y_i/N_i\)</span>), we scale it by the pre-season sample size (<span class="math inline">\(\sqrt{n_i}\)</span>). This ensures that <span class="math inline">\(\mu_i^{\text{season}}\)</span> is on the same scale as our observations <span class="math inline">\(x_i\)</span>, allowing for direct comparison of the estimation error.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Input Raw Data</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ni <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">58</span>, <span class="dv">59</span>, <span class="dv">74</span>, <span class="dv">84</span>, <span class="dv">69</span>, <span class="dv">63</span>, <span class="dv">60</span>, <span class="dv">54</span>, <span class="dv">53</span>, <span class="dv">60</span>, <span class="dv">66</span>, <span class="dv">66</span>, <span class="dv">72</span>, <span class="dv">64</span>, <span class="dv">42</span>, <span class="dv">38</span>, <span class="dv">58</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>yi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>Ni <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">509</span>, <span class="dv">643</span>, <span class="dv">633</span>, <span class="dv">645</span>, <span class="dv">606</span>, <span class="dv">555</span>, <span class="dv">619</span>, <span class="dv">609</span>, <span class="dv">552</span>, <span class="dv">540</span>, <span class="dv">561</span>, <span class="dv">440</span>, <span class="dv">585</span>, <span class="dv">531</span>, <span class="dv">454</span>, <span class="dv">504</span>, <span class="dv">244</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>Yi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">70</span>, <span class="dv">66</span>, <span class="dv">56</span>, <span class="dv">46</span>, <span class="dv">45</span>, <span class="dv">44</span>, <span class="dv">43</span>, <span class="dv">40</span>, <span class="dv">37</span>, <span class="dv">34</span>, <span class="dv">32</span>, <span class="dv">30</span>, <span class="dv">29</span>, <span class="dv">28</span>, <span class="dv">23</span>, <span class="dv">21</span>, <span class="dv">15</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate Derived Values</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>p_pre   <span class="ot">&lt;-</span> yi <span class="sc">/</span> ni                        <span class="co"># Pre-season Probability</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>x       <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(ni) <span class="sc">*</span> <span class="fu">asin</span>(<span class="dv">2</span> <span class="sc">*</span> p_pre <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Transformed Pre-season (x_i)</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>p_season <span class="ot">&lt;-</span> Yi <span class="sc">/</span> Ni                       <span class="co"># Season Probability</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>true_mu  <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(ni) <span class="sc">*</span> <span class="fu">asin</span>(<span class="dv">2</span> <span class="sc">*</span> p_season <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Transformed Season (mu_i)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create Main Data Frame</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>baseball_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">Player =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">Pre_HR =</span> yi,</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">Pre_AtBats =</span> ni,</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_pre =</span> <span class="fu">round</span>(p_pre, <span class="dv">3</span>),</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> x,</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">sei =</span> <span class="dv">1</span>, <span class="co"># Known standard error for transformed data</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">Season_HR =</span> Yi,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">Season_AtBats =</span> Ni,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_season =</span> p_season,</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">true_mu =</span> true_mu</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Display the Data</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(baseball_data, </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>             <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Player"</span>, <span class="st">"$y_i$"</span>, <span class="st">"$n_i$"</span>, <span class="st">"$p_i^{</span><span class="sc">\\</span><span class="st">text{pre}}$"</span>, <span class="st">"$x_i$"</span>, <span class="st">"SE"</span>, </span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"$Y_i$"</span>, <span class="st">"$N_i$"</span>, <span class="st">"$p_i^{</span><span class="sc">\\</span><span class="st">text{seas}}$"</span>, <span class="st">"$</span><span class="sc">\\</span><span class="st">mu_i$"</span>),</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>             <span class="at">align =</span> <span class="st">"c"</span>,</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>             <span class="at">digits =</span> <span class="dv">3</span>,</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 20%">
<col style="width: 9%">
<col style="width: 4%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 21%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Player</th>
<th style="text-align: center;"><span class="math inline">\(y_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(n_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p_i^{\text{pre}}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x_i\)</span></th>
<th style="text-align: center;">SE</th>
<th style="text-align: center;"><span class="math inline">\(Y_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(N_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p_i^{\text{seas}}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mu_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">-6.559</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">509</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">-6.176</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">-5.901</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">643</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">-7.055</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">-9.476</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">633</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">-8.317</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">-9.029</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">645</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">-9.441</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">-9.558</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">606</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">-8.463</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">-7.488</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">555</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">-7.937</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">-9.323</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">619</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">-8.035</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">-5.005</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">609</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">-7.734</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">-8.589</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">552</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">-7.622</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">-9.323</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">-8.238</td>
</tr>
<tr class="odd">
<td style="text-align: center;">11</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">-8.720</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">561</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">-8.843</td>
</tr>
<tr class="even">
<td style="text-align: center;">12</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">-9.270</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">-8.469</td>
</tr>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">0.028</td>
<td style="text-align: center;">-10.487</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">585</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">-9.518</td>
</tr>
<tr class="even">
<td style="text-align: center;">14</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">-8.034</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">531</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">-8.859</td>
</tr>
<tr class="odd">
<td style="text-align: center;">15</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">-6.673</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">454</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">-7.237</td>
</tr>
<tr class="even">
<td style="text-align: center;">16</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">-6.829</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">504</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">-7.149</td>
</tr>
<tr class="odd">
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">-6.975</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">244</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">-8.146</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In this analysis, we model the home run strike rates of 17 Major League Baseball players using pre-season exhibition data from 1998. We apply five statistical methods ranging from simple independent estimation to advanced Bayesian decision theory.</p>
</section>
<section id="methods-for-estimating-mu_i-transformed-scale" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="methods-for-estimating-mu_i-transformed-scale"><span class="header-section-number">3.6.3</span> Methods for Estimating <span class="math inline">\(\mu_i\)</span> (Transformed Scale)</h3>
<section id="method-1-simple-estimation-mle" class="level4" data-number="3.6.3.1">
<h4 data-number="3.6.3.1" class="anchored" data-anchor-id="method-1-simple-estimation-mle"><span class="header-section-number">3.6.3.1</span> Method 1: Simple Estimation (MLE)</h4>
<p>The Maximum Likelihood Estimator (MLE) assumes each player’s performance is independent. It relies solely on the observed pre-season data.</p>
<p><span class="math display">\[ \hat{\mu}_i^{MLE} = X_i \]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple Estimate is just the data itself</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>mu_mle <span class="ot">&lt;-</span> baseball_data<span class="sc">$</span>x</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE Calculation (Transformed Scale)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>mse_mle <span class="ot">&lt;-</span> <span class="fu">mean</span>((mu_mle <span class="sc">-</span> baseball_data<span class="sc">$</span>true_mu)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="method-2-empirical-bayes-james-stein" class="level4" data-number="3.6.3.2">
<h4 data-number="3.6.3.2" class="anchored" data-anchor-id="method-2-empirical-bayes-james-stein"><span class="header-section-number">3.6.3.2</span> Method 2: Empirical Bayes (James-Stein)</h4>
<p>The James-Stein estimator introduces a global mean <span class="math inline">\(\bar{X}\)</span> and shrinks individual estimates toward it. This assumes the players come from a common population distribution.</p>
<p><span class="math display">\[ \hat{\mu}_i^{JS} = \bar{X} + \left( 1 - \frac{k-3}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X}) \]</span></p>
<p>where <span class="math inline">\(k=17\)</span> is the number of players.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>theta_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(baseball_data<span class="sc">$</span>x)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">sum</span>((baseball_data<span class="sc">$</span>x <span class="sc">-</span> theta_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>shrinkage_factor <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (<span class="dv">14</span> <span class="sc">/</span> S)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>mu_js <span class="ot">&lt;-</span> theta_hat <span class="sc">+</span> shrinkage_factor <span class="sc">*</span> (baseball_data<span class="sc">$</span>x <span class="sc">-</span> theta_hat)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE Calculation (Transformed Scale)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>mse_js <span class="ot">&lt;-</span> <span class="fu">mean</span>((mu_js <span class="sc">-</span> baseball_data<span class="sc">$</span>true_mu)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="method-3-fully-bayesian-mcmc-brms" class="level4" data-number="3.6.3.3">
<h4 data-number="3.6.3.3" class="anchored" data-anchor-id="method-3-fully-bayesian-mcmc-brms"><span class="header-section-number">3.6.3.3</span> Method 3: Fully Bayesian MCMC (brms)</h4>
<p>We use a hierarchical Bayesian model where parameters are treated as random variables. We implement this using <code>brms</code>.</p>
<p><span class="math display">\[
\begin{aligned}
X_i &amp;\sim N(\mu_i, 1) \\
\mu_i &amp;\sim N(\theta, \tau^2) \\
\theta &amp;\sim N(0, 10) \\
\tau &amp;\sim \text{Cauchy}(0, 2)
\end{aligned}
\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Random Intercept Model: x | se(1) ~ 1 + (1|Player)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>fit_brms <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> x <span class="sc">|</span> <span class="fu">se</span>(sei, <span class="at">sigma =</span> <span class="cn">TRUE</span>) <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Player),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> baseball_data,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">prior =</span> <span class="fu">c</span>(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="at">class =</span> <span class="st">"Intercept"</span>),</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prior</span>(<span class="fu">cauchy</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="at">class =</span> <span class="st">"sd"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">iter =</span> <span class="dv">4000</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">seed =</span> <span class="dv">123</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract Point Estimates (Posterior Means)</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>post_means <span class="ot">&lt;-</span> <span class="fu">fitted</span>(fit_brms)[, <span class="st">"Estimate"</span>]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>mu_brms <span class="ot">&lt;-</span> post_means</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE Calculation (Transformed Scale)</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>mse_brms <span class="ot">&lt;-</span> <span class="fu">mean</span>((mu_brms <span class="sc">-</span> baseball_data<span class="sc">$</span>true_mu)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="comparison-of-estimates-of-mu_i" class="level3" data-number="3.6.4">
<h3 data-number="3.6.4" class="anchored" data-anchor-id="comparison-of-estimates-of-mu_i"><span class="header-section-number">3.6.4</span> Comparison of Estimates of <span class="math inline">\(\mu_i\)</span></h3>
<p><strong>Full Comparison of Estimates (Transformed Scale)</strong></p>
<p>The following table presents the transformed data (<span class="math inline">\(x_i\)</span>) and the true season parameter (<span class="math inline">\(\mu_i\)</span>) alongside the estimates from the three methods. The rows are sorted by <span class="math inline">\(x_i\)</span> to visualize how the shrinkage methods (James-Stein and Bayesian) pull the estimates away from the extremes and toward the population mean compared to the raw MLE.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Compile all estimates into a single data frame</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df_estimates <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Player =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">ni =</span> baseball_data<span class="sc">$</span>Pre_AtBats,       </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_i =</span> baseball_data<span class="sc">$</span>x,               <span class="co"># MLE Estimate (Raw Data)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu_js =</span> mu_js,                       <span class="co"># James-Stein Estimate</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu_bayes =</span> mu_brms,                  <span class="co"># Fully Bayesian Estimate</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu_true =</span> baseball_data<span class="sc">$</span>true_mu      <span class="co"># True Season Parameter</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Sort by x_i (ascending)</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>df_sorted <span class="ot">&lt;-</span> df_estimates[<span class="fu">order</span>(df_estimates<span class="sc">$</span>x_i), ]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Display the table</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>df_display_mu <span class="ot">&lt;-</span> df_sorted</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>df_display_mu[, <span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>] <span class="ot">&lt;-</span> <span class="fu">round</span>(df_display_mu[, <span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="dv">3</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(df_display_mu[, <span class="fu">c</span>(<span class="st">"Player"</span>, <span class="st">"x_i"</span>, <span class="st">"mu_js"</span>, <span class="st">"mu_bayes"</span>, <span class="st">"mu_true"</span>)],</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">row.names =</span> <span class="cn">FALSE</span>,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Player"</span>, <span class="st">"$x_i$ (MLE)"</span>, <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">mu}_{JS}$"</span>, </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">mu}_{Bayes}$"</span>, <span class="st">"$</span><span class="sc">\\</span><span class="st">mu_{true}$"</span>),</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">align =</span> <span class="st">"c"</span>,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Comparison of Estimates (Sorted by Pre-season $x_i$)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-estimates-sorted" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-estimates-sorted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Comparison of Estimates (Sorted by Pre-season <span class="math inline">\(x_i\)</span>)
</figcaption>
<div aria-describedby="tbl-estimates-sorted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<caption>Comparison of Estimates (Sorted by Pre-season <span class="math inline">\(x_i\)</span>)</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 17%">
<col style="width: 24%">
<col style="width: 28%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Player</th>
<th style="text-align: center;"><span class="math inline">\(x_i\)</span> (MLE)</th>
<th style="text-align: center;"><span class="math inline">\(\hat{\mu}_{JS}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\hat{\mu}_{Bayes}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mu_{true}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">-10.487</td>
<td style="text-align: center;">-9.589</td>
<td style="text-align: center;">-8.746</td>
<td style="text-align: center;">-9.518</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">-9.558</td>
<td style="text-align: center;">-9.006</td>
<td style="text-align: center;">-8.478</td>
<td style="text-align: center;">-8.463</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">-9.476</td>
<td style="text-align: center;">-8.954</td>
<td style="text-align: center;">-8.470</td>
<td style="text-align: center;">-8.317</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">-9.323</td>
<td style="text-align: center;">-8.858</td>
<td style="text-align: center;">-8.412</td>
<td style="text-align: center;">-8.035</td>
</tr>
<tr class="odd">
<td style="text-align: center;">10</td>
<td style="text-align: center;">-9.323</td>
<td style="text-align: center;">-8.858</td>
<td style="text-align: center;">-8.415</td>
<td style="text-align: center;">-8.238</td>
</tr>
<tr class="even">
<td style="text-align: center;">12</td>
<td style="text-align: center;">-9.270</td>
<td style="text-align: center;">-8.825</td>
<td style="text-align: center;">-8.412</td>
<td style="text-align: center;">-8.469</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">-9.029</td>
<td style="text-align: center;">-8.673</td>
<td style="text-align: center;">-8.331</td>
<td style="text-align: center;">-9.441</td>
</tr>
<tr class="even">
<td style="text-align: center;">11</td>
<td style="text-align: center;">-8.720</td>
<td style="text-align: center;">-8.479</td>
<td style="text-align: center;">-8.260</td>
<td style="text-align: center;">-8.843</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">-8.589</td>
<td style="text-align: center;">-8.397</td>
<td style="text-align: center;">-8.206</td>
<td style="text-align: center;">-7.622</td>
</tr>
<tr class="even">
<td style="text-align: center;">14</td>
<td style="text-align: center;">-8.034</td>
<td style="text-align: center;">-8.048</td>
<td style="text-align: center;">-8.054</td>
<td style="text-align: center;">-8.859</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">-7.488</td>
<td style="text-align: center;">-7.705</td>
<td style="text-align: center;">-7.897</td>
<td style="text-align: center;">-7.937</td>
</tr>
<tr class="even">
<td style="text-align: center;">17</td>
<td style="text-align: center;">-6.975</td>
<td style="text-align: center;">-7.384</td>
<td style="text-align: center;">-7.754</td>
<td style="text-align: center;">-8.146</td>
</tr>
<tr class="odd">
<td style="text-align: center;">16</td>
<td style="text-align: center;">-6.829</td>
<td style="text-align: center;">-7.292</td>
<td style="text-align: center;">-7.714</td>
<td style="text-align: center;">-7.149</td>
</tr>
<tr class="even">
<td style="text-align: center;">15</td>
<td style="text-align: center;">-6.673</td>
<td style="text-align: center;">-7.194</td>
<td style="text-align: center;">-7.663</td>
<td style="text-align: center;">-7.237</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">-6.559</td>
<td style="text-align: center;">-7.122</td>
<td style="text-align: center;">-7.628</td>
<td style="text-align: center;">-6.176</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">-5.901</td>
<td style="text-align: center;">-6.709</td>
<td style="text-align: center;">-7.441</td>
<td style="text-align: center;">-7.055</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: center;">-5.005</td>
<td style="text-align: center;">-6.146</td>
<td style="text-align: center;">-7.186</td>
<td style="text-align: center;">-7.734</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p><strong>Plots of Errors (Sorted by <span class="math inline">\(x_i\)</span>)</strong></p>
<p>This plot displays the Squared Error for each player. The x-axis represents the players sorted from lowest pre-season performance to highest.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Squared Errors using the SORTED dataframe</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>err_mle  <span class="ot">&lt;-</span> (df_sorted<span class="sc">$</span>x_i <span class="sc">-</span> df_sorted<span class="sc">$</span>mu_true)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>err_js   <span class="ot">&lt;-</span> (df_sorted<span class="sc">$</span>mu_js <span class="sc">-</span> df_sorted<span class="sc">$</span>mu_true)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>err_brms <span class="ot">&lt;-</span> (df_sorted<span class="sc">$</span>mu_bayes <span class="sc">-</span> df_sorted<span class="sc">$</span>mu_true)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine Y-axis range</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>y_max <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">c</span>(err_mle, err_js, err_brms))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot MLE Errors (Baseline)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, err_mle, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Player Index (Sorted by Pre-season Performance)"</span>, </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(Squared<span class="sc">~</span>Error<span class="sc">~</span><span class="er">~</span>(<span class="fu">hat</span>(mu) <span class="sc">-</span> mu[true])<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Estimation Error Comparison (Sorted)"</span>,</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, y_max))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Add James-Stein Errors</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, err_js, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Bayesian (brms) Errors</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, err_brms, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Grid and Legend</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">"MLE: "</span>, <span class="fu">round</span>(mse_mle, <span class="dv">3</span>)), </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"JS: "</span>, <span class="fu">round</span>(mse_js, <span class="dv">3</span>)), </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Bayes: "</span>, <span class="fu">round</span>(mse_brms, <span class="dv">3</span>))),</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dv">17</span>), </span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-error-index-sorted" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-error-index-sorted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-error-index-sorted-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-error-index-sorted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Squared Error by Sorted Player Index (Transformed Scale)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="methods-for-estimating-p_i-directly" class="level3" data-number="3.6.5">
<h3 data-number="3.6.5" class="anchored" data-anchor-id="methods-for-estimating-p_i-directly"><span class="header-section-number">3.6.5</span> Methods for Estimating <span class="math inline">\(p_i\)</span> directly</h3>
<section id="method-1-3-converting-hat-mu_i-back-to-p_i" class="level4" data-number="3.6.5.1">
<h4 data-number="3.6.5.1" class="anchored" data-anchor-id="method-1-3-converting-hat-mu_i-back-to-p_i"><span class="header-section-number">3.6.5.1</span> Method 1-3: Converting <span class="math inline">\(\hat \mu_i\)</span> back to <span class="math inline">\(p_i\)</span></h4>
<p>The first three methods (MLE, James-Stein, and Normal-Normal Bayes) estimated the parameter <span class="math inline">\(\mu_i\)</span> on the transformed scale. To obtain the probability estimates <span class="math inline">\(\hat{p}_i\)</span>, we apply the inverse of the variance-stabilizing transformation:</p>
<p><span class="math display">\[ \hat{p}_i = \frac{1}{2} \left( \sin\left( \frac{\hat{\mu}_i}{\sqrt{n_i}} \right) + 1 \right) \]</span></p>
<p>where <span class="math inline">\(\hat{\mu}_i\)</span> corresponds to the estimate derived from Method 1, 2, or 3, and <span class="math inline">\(n_i\)</span> is the number of pre-season at-bats for player <span class="math inline">\(i\)</span>.</p>
</section>
<section id="method-4-hierarchical-logistic-regression-logit-normal" class="level4" data-number="3.6.5.2">
<h4 data-number="3.6.5.2" class="anchored" data-anchor-id="method-4-hierarchical-logistic-regression-logit-normal"><span class="header-section-number">3.6.5.2</span> Method 4: Hierarchical Logistic Regression (Logit-Normal)</h4>
<p>In this fourth method, we model the probability <span class="math inline">\(p_i\)</span> directly using a hierarchical structure on the log-odds scale, rather than transforming the data.</p>
<p>We assume the count <span class="math inline">\(y_i\)</span> follows a Binomial distribution. The log-odds (logit) of the success rate <span class="math inline">\(p_i\)</span> are drawn from a common Normal distribution with unknown mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\tau_0\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
y_i | p_i &amp;\sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) &amp;\sim N(\mu_0, \tau_0^2) \\
\mu_0 &amp;\sim N(0, 10) \\
\tau_0 &amp;\sim \text{Cauchy}(0, 2)
\end{aligned}
\]</span></p>
<p>We implement this in <code>brms</code> using the <code>binomial</code> family with a logit link. The individual point estimate <span class="math inline">\(\hat{p}_i\)</span> is the <strong>posterior mean</strong> of <span class="math inline">\(p_i\)</span>. Note that because the inverse-logit function is non-linear, the posterior mean of <span class="math inline">\(p_i\)</span> is not simply the inverse-logit of the posterior mean of the random effect; <code>brms</code> handles this integration automatically via the <code>fitted()</code> function.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Fit Hierarchical Logistic Regression</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Formula: y | trials(n) ~ 1 + (1 | Player)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># This estimates a global intercept (mu_0) and random intercepts for each player (logit(p_i))</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>fit_logit <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> Pre_HR <span class="sc">|</span> <span class="fu">trials</span>(Pre_AtBats) <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Player),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> baseball_data,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">"logit"</span>),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">prior =</span> <span class="fu">c</span>(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prior</span>(<span class="fu">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">class =</span> <span class="st">"Intercept"</span>),</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prior</span>(<span class="fu">cauchy</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="at">class =</span> <span class="st">"sd"</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">iter =</span> <span class="dv">4000</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">seed =</span> <span class="dv">123</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Extract Posterior Means of p_i</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># fitted() returns the posterior expectations of the response (Expected Count). </span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>fitted_counts <span class="ot">&lt;-</span> <span class="fu">fitted</span>(fit_logit) </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>p_hat_logit <span class="ot">&lt;-</span> fitted_counts[, <span class="st">"Estimate"</span>] <span class="sc">/</span> baseball_data<span class="sc">$</span>Pre_AtBats</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="method-5-optimal-bayes-estimator-weighted-median" class="level4" data-number="3.6.5.3">
<h4 data-number="3.6.5.3" class="anchored" data-anchor-id="method-5-optimal-bayes-estimator-weighted-median"><span class="header-section-number">3.6.5.3</span> Method 5: Optimal Bayes Estimator (Weighted Median)</h4>
<p>While the posterior mean (Method 4) minimizes the Mean Squared Error (MSE), it is not necessarily optimal for the <strong>Relative Standardized Error</strong> metric we defined earlier: <span class="math display">\[L(p, \hat{p}) = \frac{|p - \hat{p}|}{\min(p, 1-p)}\]</span></p>
<p>This is a form of weighted absolute error loss, where the weight is <span class="math inline">\(w(p) = \frac{1}{\min(p, 1-p)}\)</span>. Theoretical derivation shows that the estimator minimizing the expected posterior loss for this function is the <strong>Weighted Posterior Median</strong>.</p>
<p>We compute this by extracting the full posterior samples from the Logit-Normal model (Method 4) and calculating the weighted median for each player.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Extract Posterior Samples (N_samples x 17 players)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># posterior_epred gives samples of the expected count (N * p)</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>post_counts <span class="ot">&lt;-</span> <span class="fu">posterior_epred</span>(fit_logit) </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to probability scale by dividing by trials</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>p_samples <span class="ot">&lt;-</span> <span class="fu">sweep</span>(post_counts, <span class="dv">2</span>, baseball_data<span class="sc">$</span>Pre_AtBats, <span class="st">"/"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define Function for Weighted Median</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Finds the value 'q' such that sum(weights where x &lt;= q) &gt;= 0.5 * total_weight</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>get_weighted_median <span class="ot">&lt;-</span> <span class="cf">function</span>(samples) {</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate weights based on the loss function denominator</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Avoid division by exact zero (unlikely but safer)</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  denom <span class="ot">&lt;-</span> <span class="fu">pmin</span>(samples, <span class="dv">1</span> <span class="sc">-</span> samples)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  denom[denom <span class="sc">&lt;</span> <span class="fl">1e-6</span>] <span class="ot">&lt;-</span> <span class="fl">1e-6</span> </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> denom</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Normalize weights</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>  weights_norm <span class="ot">&lt;-</span> weights <span class="sc">/</span> <span class="fu">sum</span>(weights)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sort samples and weights</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  ord <span class="ot">&lt;-</span> <span class="fu">order</span>(samples)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>  samp_sorted <span class="ot">&lt;-</span> samples[ord]</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>  w_sorted <span class="ot">&lt;-</span> weights_norm[ord]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find cutoff</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>  cum_w <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(w_sorted)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">which</span>(cum_w <span class="sc">&gt;=</span> <span class="fl">0.5</span>)[<span class="dv">1</span>]</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(samp_sorted[idx])</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Apply to all players</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>p_hat_optimal <span class="ot">&lt;-</span> <span class="fu">apply</span>(p_samples, <span class="dv">2</span>, get_weighted_median)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="comparison-of-all-five-estimates-probability-scale" class="level4" data-number="3.6.5.4">
<h4 data-number="3.6.5.4" class="anchored" data-anchor-id="comparison-of-all-five-estimates-probability-scale"><span class="header-section-number">3.6.5.4</span> Comparison of All Five Estimates (Probability Scale)</h4>
<p>We now compare all five methods: MLE, James-Stein (transformed), Bayes Normal-Normal (transformed), Hierarchical Logit-Normal (Posterior Mean), and Optimal Bayes (Weighted Median).</p>
<p><strong>1. MSE Comparison</strong></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Prepare Estimates from Previous Steps</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>inv_trans <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, n) { <span class="fl">0.5</span> <span class="sc">*</span> (<span class="fu">sin</span>(mu <span class="sc">/</span> <span class="fu">sqrt</span>(n)) <span class="sc">+</span> <span class="dv">1</span>) }</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert transformed estimates back to probability scale</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>p_mle    <span class="ot">&lt;-</span> <span class="fu">inv_trans</span>(baseball_data<span class="sc">$</span>x, baseball_data<span class="sc">$</span>Pre_AtBats)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>p_js     <span class="ot">&lt;-</span> <span class="fu">inv_trans</span>(mu_js, baseball_data<span class="sc">$</span>Pre_AtBats)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>p_normal <span class="ot">&lt;-</span> <span class="fu">inv_trans</span>(mu_brms, baseball_data<span class="sc">$</span>Pre_AtBats) </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># p_hat_logit (Method 4) and p_hat_optimal (Method 5) are already calculated</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Combine into DataFrame</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>df_compare <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">Player =</span> baseball_data<span class="sc">$</span>Player,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_i    =</span> baseball_data<span class="sc">$</span>x, <span class="co"># For sorting</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_true =</span> baseball_data<span class="sc">$</span>p_season,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_mle  =</span> p_mle,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_js   =</span> p_js,</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_norm =</span> p_normal,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_logit =</span> p_hat_logit,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_opt   =</span> p_hat_optimal</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by initial performance</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>df_compare_sorted <span class="ot">&lt;-</span> df_compare[<span class="fu">order</span>(df_compare<span class="sc">$</span>x_i), ]</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Calculate MSE</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>mse_p_mle   <span class="ot">&lt;-</span> <span class="fu">mean</span>((df_compare_sorted<span class="sc">$</span>p_mle <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>mse_p_js    <span class="ot">&lt;-</span> <span class="fu">mean</span>((df_compare_sorted<span class="sc">$</span>p_js <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>mse_p_norm  <span class="ot">&lt;-</span> <span class="fu">mean</span>((df_compare_sorted<span class="sc">$</span>p_norm <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>mse_p_logit <span class="ot">&lt;-</span> <span class="fu">mean</span>((df_compare_sorted<span class="sc">$</span>p_logit <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>mse_p_opt   <span class="ot">&lt;-</span> <span class="fu">mean</span>((df_compare_sorted<span class="sc">$</span>p_opt <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Plot MSE</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>y_max <span class="ot">&lt;-</span> <span class="fu">max</span>((df_compare_sorted<span class="sc">$</span>p_mle <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, (df_compare_sorted<span class="sc">$</span>p_mle <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>, </span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>,</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Player Index (Sorted by Pre-season)"</span>,</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Squared Error"</span>,</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Squared Error by Method"</span>,</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, y_max))</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, (df_compare_sorted<span class="sc">$</span>p_js <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, (df_compare_sorted<span class="sc">$</span>p_norm <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, (df_compare_sorted<span class="sc">$</span>p_logit <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, (df_compare_sorted<span class="sc">$</span>p_opt <span class="sc">-</span> df_compare_sorted<span class="sc">$</span>p_true)<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">col =</span> <span class="st">"purple"</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>,</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">"MLE [MSE: "</span>, <span class="fu">round</span>(mse_p_mle, <span class="dv">4</span>), <span class="st">"]"</span>),</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"JS [MSE: "</span>, <span class="fu">round</span>(mse_p_js, <span class="dv">4</span>), <span class="st">"]"</span>),</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Normal-Bayes [MSE: "</span>, <span class="fu">round</span>(mse_p_norm, <span class="dv">4</span>), <span class="st">"]"</span>),</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Logit-Normal [MSE: "</span>, <span class="fu">round</span>(mse_p_logit, <span class="dv">4</span>), <span class="st">"]"</span>),</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Optimal-Bayes [MSE: "</span>, <span class="fu">round</span>(mse_p_opt, <span class="dv">4</span>), <span class="st">"]"</span>)),</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"darkgreen"</span>, <span class="st">"purple"</span>),</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dv">17</span>, <span class="dv">15</span>, <span class="dv">18</span>),</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex =</span> <span class="fl">0.75</span>,</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>       <span class="at">bg =</span> <span class="st">"white"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="bayesian_files/figure-html/compare-five-methods-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p><strong>2. Relative Standardized Error</strong></p>
<p>We also evaluate the methods using the relative error metric that penalizes deviations based on the rarity of the event: <span class="math display">\[ \text{Metric}_i = \frac{|p_i^{\text{true}} - \hat{p}_i|}{\min(p_i^{\text{true}}, 1 - p_i^{\text{true}})} \]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define Metric</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>calc_metric <span class="ot">&lt;-</span> <span class="cf">function</span>(p_hat, p_true) {</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  denom <span class="ot">&lt;-</span> <span class="fu">pmin</span>(p_true, <span class="dv">1</span> <span class="sc">-</span> p_true)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abs</span>(p_hat <span class="sc">-</span> p_true) <span class="sc">/</span> denom</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate Metric</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>rel_mle   <span class="ot">&lt;-</span> <span class="fu">calc_metric</span>(df_compare_sorted<span class="sc">$</span>p_mle, df_compare_sorted<span class="sc">$</span>p_true)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>rel_js    <span class="ot">&lt;-</span> <span class="fu">calc_metric</span>(df_compare_sorted<span class="sc">$</span>p_js, df_compare_sorted<span class="sc">$</span>p_true)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>rel_norm  <span class="ot">&lt;-</span> <span class="fu">calc_metric</span>(df_compare_sorted<span class="sc">$</span>p_norm, df_compare_sorted<span class="sc">$</span>p_true)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>rel_logit <span class="ot">&lt;-</span> <span class="fu">calc_metric</span>(df_compare_sorted<span class="sc">$</span>p_logit, df_compare_sorted<span class="sc">$</span>p_true)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>rel_opt   <span class="ot">&lt;-</span> <span class="fu">calc_metric</span>(df_compare_sorted<span class="sc">$</span>p_opt, df_compare_sorted<span class="sc">$</span>p_true)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Sum of Errors</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>sum_rel_mle   <span class="ot">&lt;-</span> <span class="fu">sum</span>(rel_mle)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>sum_rel_js    <span class="ot">&lt;-</span> <span class="fu">sum</span>(rel_js)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>sum_rel_norm  <span class="ot">&lt;-</span> <span class="fu">sum</span>(rel_norm)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>sum_rel_logit <span class="ot">&lt;-</span> <span class="fu">sum</span>(rel_logit)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>sum_rel_opt   <span class="ot">&lt;-</span> <span class="fu">sum</span>(rel_opt)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Plot</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>y_max_rel <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">c</span>(rel_mle, rel_js, rel_norm, rel_logit, rel_opt)) <span class="sc">*</span> <span class="fl">1.1</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, rel_mle, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>,</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Player Index (Sorted by Pre-season)"</span>, </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Relative Standardized Error"</span>,</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Assessment of Estimation Methods"</span>,</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, y_max_rel))</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, rel_js, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, rel_norm, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, rel_logit, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">17</span>, rel_opt, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">col =</span> <span class="st">"purple"</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"Method [Sum Relative Error]"</span>,</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">"MLE ["</span>, <span class="fu">round</span>(sum_rel_mle, <span class="dv">3</span>), <span class="st">"]"</span>), </span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"James-Stein ["</span>, <span class="fu">round</span>(sum_rel_js, <span class="dv">3</span>), <span class="st">"]"</span>), </span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Normal-Bayes ["</span>, <span class="fu">round</span>(sum_rel_norm, <span class="dv">3</span>), <span class="st">"]"</span>),</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Logit-Normal ["</span>, <span class="fu">round</span>(sum_rel_logit, <span class="dv">3</span>), <span class="st">"]"</span>),</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">paste0</span>(<span class="st">"Optimal-Bayes ["</span>, <span class="fu">round</span>(sum_rel_opt, <span class="dv">3</span>), <span class="st">"]"</span>)),</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"darkgreen"</span>, <span class="st">"purple"</span>), </span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dv">17</span>, <span class="dv">15</span>, <span class="dv">18</span>), </span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex =</span> <span class="fl">0.75</span>, </span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">bg =</span> <span class="st">"white"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-error-relative-final" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-error-relative-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesian_files/figure-html/fig-error-relative-final-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-error-relative-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: Relative Error Assessment: Five Methods
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="bayesian-predictive-distributions" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="bayesian-predictive-distributions"><span class="header-section-number">3.7</span> Bayesian Predictive Distributions</h2>
<p>A key feature of Bayesian analysis is the ability to make inference about future observations, rather than just the model parameters. The <strong>posterior predictive distribution</strong> describes the probability of observing a new data point <span class="math inline">\(y^*\)</span> given the observed data <span class="math inline">\(y\)</span>.</p>
<div id="def-posterior-predictive" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (Posterior Predictive Distribution)</strong></span> Let <span class="math inline">\(f(y^*|\theta)\)</span> be the sampling distribution of a future observation <span class="math inline">\(y^*\)</span> given parameter <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(\pi(\theta|y)\)</span> be the posterior distribution of <span class="math inline">\(\theta\)</span> given observed data <span class="math inline">\(y\)</span>. The posterior predictive density is obtained by marginalizing over the parameter <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
f(y^*|y) = \int_\Theta f(y^*|\theta) \pi(\theta|y) \, d\theta
\]</span></p>
</div>
<p>This distribution incorporates two distinct sources of uncertainty:</p>
<ul>
<li><strong>Sampling Uncertainty (Aleatoric):</strong> The inherent variability of the data generation process, represented by the variance in <span class="math inline">\(f(y^*|\theta)\)</span>.</li>
<li><strong>Parameter Uncertainty (Epistemic):</strong> The uncertainty regarding the true value of <span class="math inline">\(\theta\)</span>, represented by the variance in the posterior <span class="math inline">\(\pi(\theta|y)\)</span>.</li>
</ul>
<p>As sample size <span class="math inline">\(n \to \infty\)</span>, the parameter uncertainty vanishes (the posterior approaches a point mass), and the predictive distribution converges to the true data-generating distribution.</p>
<div id="exm-predictive-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8 (Normal-Normal Predictive Distribution)</strong></span> Consider a case where the data <span class="math inline">\(y_1, \dots, y_n\)</span> are independent and normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
Y_i | \mu \sim N(\mu, \sigma^2)
\]</span></p>
<p>Assume a conjugate prior for the mean: <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span>. The posterior distribution is <span class="math inline">\(\mu|y \sim N(\mu_n, \sigma_n^2)\)</span>, where <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(\sigma_n^2\)</span> are the updated posterior hyperparameters.</p>
<p>The predictive distribution for a new observation <span class="math inline">\(y^*\)</span> is derived as:</p>
<p><span class="math display">\[
\begin{aligned}
f(y^*|y) &amp;= \int_{-\infty}^{\infty} f(y^*|\mu) \pi(\mu|y) \, d\mu \\
&amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y^*-\mu)^2}{2\sigma^2}} \times \frac{1}{\sqrt{2\pi\sigma_n^2}} e^{-\frac{(\mu-\mu_n)^2}{2\sigma_n^2}} \, d\mu
\end{aligned}
\]</span></p>
<p>This convolution of two Gaussians results in a new Gaussian distribution:</p>
<p><span class="math display">\[
y^* | y \sim N(\mu_n, \sigma^2 + \sigma_n^2)
\]</span></p>
<p>Here, the total predictive variance is the sum of the data variance (<span class="math inline">\(\sigma^2\)</span>) and the posterior uncertainty about the mean (<span class="math inline">\(\sigma_n^2\)</span>).</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./decision.html" class="pagination-link" aria-label="Decision Theory">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Decision Theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>