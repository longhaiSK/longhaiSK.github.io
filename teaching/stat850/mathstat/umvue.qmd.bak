# Uniformly Minimum Variance Estimators

## Sufficiency and Completeness

[cite_start]In the context of statistical inference, we consider the likelihood function $L(\theta; x) = f(x, \theta)$, where $\theta$ is the variable and $x$ is fixed[cite: 1, 6, 7, 8].

::: {#exm-normal-likelihood}
### Normal Distribution Likelihood
Suppose $X_{1}, \dots, X_{n} | \mu, \sigma^{2} \sim N(\mu, \sigma^{2})$. [cite_start]The data is represented as $x = (x_1, \dots, x_n)$ and the parameter vector is $\theta = (\mu, \sigma^{2})$[cite: 9, 10]. The likelihood function is given by:

$$ 
L(\theta; x) = f(x | \theta) = (2\pi)^{-\frac{n}{2}} (\sigma^{2})^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2\sigma^{2}} \sum (x_i - \mu)^2 \right\} 
$$

[cite_start]This can be rewritten to show it is determined by $\bar{x}$ and $s^2$[cite: 11, 12, 13, 14, 15].
:::

::: {#def-sufficient-statistic}
### Sufficient Statistic
[cite_start]A statistic $T = T(x)$ is sufficient for $\theta$ if one of the following conditions is met[cite: 28, 29, 30]:

* Factorization Criterion:
  [cite_start]The joint density can be factored as $f(x | \theta) = h(x) g(T(x), \theta)$, where $h(x)$ is independent of $\theta$[cite: 31, 33, 34].

* Likelihood Ratio Theorem:
  [cite_start]For any pair of data sets $x$ and $x'$ such that $T(x) = T(x')$, the ratio $\frac{f(x, \theta_2)}{f(x, \theta_1)} = \frac{f(x', \theta_2)}{f(x', \theta_1)}$ for all $\theta_1, \theta_2$[cite: 35, 38, 39, 40, 42].

* Conditional Distribution:
  [cite_start]The conditional distribution of the data given the statistic, $f(x | T(x), \theta)$, is independent of $\theta$[cite: 43, 45, 46].
:::


## Minimal Sufficient Statistics

::: {#def-minimal-sufficient}
### Minimal Sufficient Statistics (MSS)
A sufficient statistic $T(X)$ is minimal sufficient if $T(X)$ is a function of any other sufficient statistic $S$. [cite_start]That is, for any sufficient statistic $S$, there exists a function $g$ such that $T(x) = g(S)$[cite: 107, 108, 111, 115, 118].
:::

::: {#thm-mss-ratio}
### Theorem 6.1: Ratio Characterization of MSS
[cite_start]$T(x)$ is minimal sufficient if and only if for any pair $x$ and $x'$, $T(x) = T(x')$ is equivalent to the ratio $\frac{f(x, \theta_2)}{f(x, \theta_1)}$ being equal to $\frac{f(x', \theta_2)}{f(x', \theta_1)}$ for all $\theta_1, \theta_2$[cite: 138, 141, 144, 204, 205].
:::

## Completeness

::: {#def-complete-statistic}
### Complete Statistic
A statistic $T$ is said to be complete if for every real-valued function $g$, $E[g(T) | [cite_start]\theta] = 0$ for all $\theta$ implies $P(g(T) = 0 | \theta) = 1$ for all $\theta$[cite: 206, 224, 227, 230].
:::

[cite_start]If a statistic is complete, then an unbiased estimator of $\theta$ based on that statistic is unique[cite: 235, 236, 238, 254].

::: {#thm-exponential-family-completeness}
### Lemma 6.3: Exponential Family Completeness
[cite_start]If $T = (T_1, \dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in its parameter space, then $T$ is complete[cite: 331, 332, 334, 337, 340, 341].
:::

::: {.proof}
[cite_start]The density of $T$ follows the form $f(t_1, \dots, t_k | \theta) = C(\theta) h(t) \exp(\sum \theta_i t_i)$[cite: 342, 347]. Suppose $E[g(T) | \theta] = 0$:

$$ 
\int g(t) C(\theta) h(t) \exp\left( \sum \theta_i t_i \right) dt = 0 
$$

This integral represents a Laplace transform. [cite_start]Since the transform is zero over an open set, the function $g(t)h(t)$ must be zero almost everywhere, implying $g(T) = 0$ almost surely[cite: 363, 367, 368, 371, 372].
:::

## Uniformly Minimum Variance Unbiased Estimators (UMVUE)

::: {#def-umvue}
### UMVUE
[cite_start]A statistic $T(x)$ is a Uniformly Minimum Variance Unbiased Estimator for $\tau(\theta)$ if[cite: 382, 387]:

* It is unbiased: $E[T(x) | [cite_start]\theta] = \tau(\theta)$ for all $\theta$[cite: 389, 390].

* [cite_start]It has minimum variance: $Var(T(x) | \theta) \leq Var(d(x) | \theta)$ for all $\theta$, where $d(x)$ is any other unbiased estimator of $\tau(\theta)$[cite: 391, 393].
:::

::: {#thm-rao-blackwell}
### Theorem 6.3: Rao-Blackwell Theorem
Let $d_1(x)$ be an unbiased estimator of $\theta$ and $T$ be a sufficient statistic. Let $g(T) = E[d_1(x) | [cite_start]T]$[cite: 397, 398, 401, 402, 403]. Then:

1. [cite_start]$g(T)$ is a statistic (independent of $\theta$)[cite: 404, 407, 408].

2. $E[g(T) | [cite_start]\theta] = \theta$[cite: 409].

3. [cite_start]$Var(g(T) | \theta) \leq Var(d_1(x) | \theta)$[cite: 410, 411].
:::

::: {.proof}
Since $T$ is sufficient, the conditional distribution of $X$ given $T$ does not depend on $\theta$, making $E[d_1(x) | [cite_start]T]$ a valid statistic[cite: 413, 416, 417]. By the law of total expectation, $E[E[d_1(x) | [cite_start]T]] = E[d_1(x)] = \theta$[cite: 418, 420]. [cite_start]By Jensen's Inequality or the law of total variance, the variance of the conditional expectation is less than or equal to the total variance[cite: 426, 428].
:::

### Lehmann-Scheff√© Theorem

If $T$ is a sufficient and complete statistic, then $g(T) = E[d(X) | [cite_start]T]$ is the unique UMVUE[cite: 373, 375, 376, 430, 432].

### Methods for finding UMVUE

1. [cite_start]Find a complete sufficient statistic $T$ for $\theta$[cite: 445, 446, 447].

2. Find any unbiased estimator $d(x)$ such that $E[d(x) | [cite_start]\theta] = \theta$[cite: 449, 452, 454].

3. Calculate $g(T) = E[d(x) | T]$. [cite_start]This result is the UMVUE[cite: 455, 456].

::: {#exm-poisson-umvue}
### UMVUE for Poisson Mean
Let $X_1, \dots, X_n \sim \text{Poisson}(\lambda)$.

1. [cite_start]$T = \sum X_i$ is a complete sufficient statistic[cite: 479, 482].

2. Let $d(X) = X_1$. [cite_start]Since $E[X_1] = \lambda$, it is an unbiased estimator[cite: 484, 485].

3. Find $E[X_1 | T]$. Since $X_1 | [cite_start]\sum X_i = k \sim \text{Binomial}(k, 1/n)$[cite: 489, 494], we have:

$$ 
E[X_1 | T] = \frac{T}{n} = \bar{X} 
$$

[cite_start]Thus, $\bar{X}$ is the UMVUE for $\lambda$[cite: 495, 496].
:::