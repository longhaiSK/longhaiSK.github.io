% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage[top=25mm,left=20mm,right=20mm]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\let\oldrule\rule
\renewcommand{\rule}[2]{\ifdim#1=0.5\linewidth\hrule\else\oldrule{#1}{#2}\fi}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistical Inference},
  pdfauthor={Longhai Li},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Statistical Inference}
\author{Longhai Li}
\date{2026-01-15}
\begin{document}
\maketitle


\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a concise course about statistical inference.

\section*{Key Features}\label{key-features}
\addcontentsline{toc}{section}{Key Features}

\markright{Key Features}

\begin{itemize}
\tightlist
\item
  Use simulation and graphs to illustrate the concepts in probability
  theory and statistical inference
\item
  Rigourous derivation of the key theorems in statistical inference
\end{itemize}

\section*{Audience}\label{audience}
\addcontentsline{toc}{section}{Audience}

\markright{Audience}

This course requires a strong command of multivariate calculus,
alongside a rigorous foundation in intermediate probability theory
including asymptotic theorey for probability. Students should also
possess prior exposure to applied statistical methods and familiar with
basic statistical concepts such as p-value and confidence internal.

\bookmarksetup{startatroot}

\chapter{Introduction to Statistical
Inference}\label{introduction-to-statistical-inference}

\section{Population Model (Data
Model)}\label{population-model-data-model}

We begin with observations (units) \(X_1, X_2, \dots, X_n\). These may
be vectors. We regard these observations as a realization of random
variables.

\begin{definition}[Population
Distribution]\protect\hypertarget{def-population-distribution}{}\label{def-population-distribution}

We assume that \(X_1, X_2, \dots, X_n \sim f(x)\). The function \(f(x)\)
is called the \textbf{population distribution}.

\end{definition}

\subsection*{Assumptions and Scope}\label{assumptions-and-scope}
\addcontentsline{toc}{subsection}{Assumptions and Scope}

For simplicity, we often assume the data are Independent and Identically
Distributed (i.i.d.). The assumption of identical distribution can be
relaxed to regression settings in which the distributions of \(x_i\)'s
are independent but dependent on covariate \(x_i\).

In \textbf{Parametric Statistics}, we assume \(f(x)\) is of a known
analytic form but involves unknown parameters.

\begin{example}[Parametric Model:
Normal]\protect\hypertarget{exm-normal-distribution}{}\label{exm-normal-distribution}

Consider the Normal distribution:
\[f(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
Here, the parameter space is
\(\Theta = \{ (\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma \in [0, +\infty) \}\).
The goal is to learn aspects of the unknown \(\theta\) from observations
\(X_1, \dots, X_n\).

\end{example}

\begin{example}[Parametric Model:
Bernoulli]\protect\hypertarget{exm-bernoulli-distribution}{}\label{exm-bernoulli-distribution}

Consider a sequence of binary outcomes (e.g., Success/Failure) where
each \(X_i \in \{0, 1\}\). We assume
\(X_i \sim \text{Bernoulli}(\theta)\). The probability mass function is:
\[f(x; \theta) = \theta^x (1-\theta)^{1-x}\] Here, the parameter space
is \(\Theta = [0, 1]\), where \(\theta\) represents the probability of
success.

\end{example}

\section{Probabilistic Model vs.~Statistical
Inference}\label{probabilistic-model-vs.-statistical-inference}

There is a fundamental distinction between probability and statistics
regarding the parameter \(\theta\). We can visualize this using a
``shooting target'' analogy:

\begin{itemize}
\item
  \textbf{\(\theta\) (The Center):} The true, unknown bullseye location.
\item
  \textbf{\(x\) (The Shots):} The observed holes on the target board.
\item
  \textbf{Probability (Deductive):} The center \(\theta\) is
  \textbf{known}. We predict where the shots \(x\) will land.
\item
  \textbf{Statistics (Inductive):} The shots \(x\) are \textbf{observed}
  on the board. The center \(\theta\) is unknown. We hypothesize
  different potential centers to see which one best explains the shots.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-prob-vs-stat-base-1.pdf}}

}

\caption{\label{fig-prob-vs-stat-base}Probability vs Statistics. Left:
Probability---The model is fixed (Blue center/contours), generating
random data. Right: Statistics---Data is fixed (Black points); we test
two hypothesized models: H1 (Green) centered at the sample mean (Good
Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).}

\end{figure}%

\section{A Motivating Example: The Lady Tasting
Tea}\label{a-motivating-example-the-lady-tasting-tea}

To illustrate the concepts of statistical inference, we consider the
famous experiment described by R.A. Fisher.

A lady claims she can distinguish whether milk was poured into the cup
before or after the tea. To test this claim, we prepare \(n\) cups of
tea.

\begin{itemize}
\tightlist
\item
  \textbf{Random Variable:} Let \(X_i=1\) if she identifies the cup
  correctly, and \(0\) otherwise.
\item
  \textbf{Parameter:} Let \(\theta\) be the probability that she
  correctly identifies a cup.
\item
  \textbf{The Data:} Suppose we observe that she identifies
  \textbf{70\%} of cups correctly (\(\bar{x} = 0.7\)), which is a
  summary of the observed vector of \(x_i\), for example,
\end{itemize}

\[x=(0,1,1,0, 1,1,0,1,1,1)\]

\subsection{Small Sample (n=10)}

We observe \textbf{7 out of 10} correct (\(k=7\)). \[\bar{x} = 0.7\]

\subsection{Large Sample (n=40)}

We observe \textbf{28 out of 40} correct (\(k=28\)). \[\bar{x} = 0.7\]

\section{Questions to Answer in Statistical
Inference}\label{questions-to-answer-in-statistical-inference}

Using this example, we identify the four main types of statistical
inference.

\paragraph*{Point Estimation}\label{point-estimation}
\addcontentsline{toc}{paragraph}{Point Estimation}

We want to use a single number to capture the parameter:
\(\hat{\theta} = \theta(X_1, \dots, X_n)\).

\begin{itemize}
\tightlist
\item
  \emph{Tea Example:} Our best guess for her success rate is
  \(\hat{\theta} = 0.7\).
\end{itemize}

\paragraph*{Hypothesis Testing}\label{hypothesis-testing}
\addcontentsline{toc}{paragraph}{Hypothesis Testing}

We want to test a theory about the parameter: \(H_0\) vs \(H_1\).

\begin{itemize}
\tightlist
\item
  \emph{Tea Example:} Is she just guessing? We test
  \(H_0: \theta = 0.5\) vs \(H_1: \theta > 0.5\).
\end{itemize}

\paragraph*{Model Assessment}\label{model-assessment}
\addcontentsline{toc}{paragraph}{Model Assessment}

We want to test a theory about the parameter: \(H_0\) vs \(H_1\).

\begin{itemize}
\tightlist
\item
  \emph{Example:} Can we use a reduced model? What level of complexity
  of \(f(x; \theta)\) is necessary?
\end{itemize}

\paragraph*{Interval Estimation}\label{interval-estimation}
\addcontentsline{toc}{paragraph}{Interval Estimation}

We want to construct an interval likely to contain the parameter:
\(\theta \in (L, U)\).

\begin{itemize}
\tightlist
\item
  \emph{Tea Example:} We might say her true skill \(\theta\) is likely
  between \(0.45\) and \(0.95\).
\end{itemize}

\paragraph*{Prediction}\label{prediction}
\addcontentsline{toc}{paragraph}{Prediction}

We want to predict a new observation \(Y_{n+1}\) given previous data.

\begin{itemize}
\tightlist
\item
  \emph{Tea Example:} If we give her an \((n+1)\)-th cup, what is the
  probability she identifies it correctly?
\end{itemize}

\section{The Likelihood Function}\label{the-likelihood-function}

The bridge between probability and statistics is the Likelihood
Function.

\begin{definition}[Likelihood
Function]\protect\hypertarget{def-likelihood}{}\label{def-likelihood}

Let \(f(x_1, \dots, x_n; \theta)\) be the joint probability density (or
mass) function of the data given the parameter \(\theta\). When we view
this function as a function of \(\theta\) for fixed observed data
\(x_1, \dots, x_n\), we call it the \textbf{likelihood function},
denoted \(L(\theta)\). \[L(\theta) = f(x_1, \dots, x_n; \theta)\]

\end{definition}

\subsection*{Example: Lady Tasting Tea}\label{example-lady-tasting-tea}
\addcontentsline{toc}{subsection}{Example: Lady Tasting Tea}

For our Tea Tasting data, the likelihood is proportional to the Binomial
probability: \[L(\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}\]

\subsection{n=10 (k=7)}

Here, \(L(\theta) = \binom{10}{7} \theta^{7} (1-\theta)^{3}\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(\theta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Calculation \(\binom{10}{7} \theta^{7} (1-\theta)^{3}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(L(\theta)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.0 & 120 \(\times 0^{7} \times 1^{3}\) & 0.0000 \\
0.2 & 120 \(\times 0.2^{7} \times 0.8^{3}\) & 0.0008 \\
0.4 & 120 \(\times 0.4^{7} \times 0.6^{3}\) & 0.0425 \\
0.6 & 120 \(\times 0.6^{7} \times 0.4^{3}\) & 0.2150 \\
0.7 & 120 \(\times 0.7^{7} \times 0.3^{3}\) & \textbf{0.2668} (Max) \\
0.8 & 120 \(\times 0.8^{7} \times 0.2^{3}\) & 0.2013 \\
1.0 & 120 \(\times 1^{7} \times 0^{3}\) & 0.0000 \\
\end{longtable}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-likelihood-tea-small-1.pdf}}

}

\caption{\label{fig-likelihood-tea-small}Likelihood Function (n= 10 )}

\end{figure}%

\subsubsection{n=40 (k=28)}\label{n40-k28}

Here, \(L(\theta) = \binom{40}{28} \theta^{28} (1-\theta)^{12}\). Notice
how the likelihood becomes \textbf{narrower} (more peaked) with more
data, even though the peak remains at 0.7.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(\theta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Calculation \(\binom{40}{28} \theta^{28} (1-\theta)^{12}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(L(\theta)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.0 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0^{28} \times 1^{12}\) & 0.0000 \\
0.2 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0.2^{28} \times 0.8^{12}\) & 0.0000 \\
0.4 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0.4^{28} \times 0.6^{12}\) & 0.0001 \\
0.6 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0.6^{28} \times 0.4^{12}\) & 0.0576 \\
0.7 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0.7^{28} \times 0.3^{12}\) & \textbf{0.1366} (Max) \\
0.8 & \ensuremath{5.5868535\times 10^{9}}
\(\times 0.8^{28} \times 0.2^{12}\) & 0.0443 \\
1.0 & \ensuremath{5.5868535\times 10^{9}}
\(\times 1^{28} \times 0^{12}\) & 0.0000 \\
\end{longtable}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-likelihood-tea-large-1.pdf}}

}

\caption{\label{fig-likelihood-tea-large}Likelihood Function (n= 40 )}

\end{figure}%

\subsubsection*{Questions}\label{questions}
\addcontentsline{toc}{subsubsection}{Questions}

\begin{itemize}
\tightlist
\item
  Is an estimator like \(\bar x\), which is called Maximum Likelihood
  Estimator (MLE), a good estimator in general?
\item
  What do you discover from actually observing the two likelihood
  unctions of different sample size \(n\)?
\item
  Is the likelihood function central to all inference problems?
\item
  What are the essential `parameters' of the likelihood function?
\end{itemize}

There are two primary frameworks for ``How'' to perform these
inferences.

\section{Frequentist Inference}\label{frequentist-inference}

\begin{itemize}
\tightlist
\item
  \textbf{Concept:} \(\theta\) is unknown but fixed; Data \(X\) is
  random.
\item
  \textbf{Sampling Distribution:} We analyze how \(\hat{\theta}\)
  behaves under hypothetical repeated sampling.
\end{itemize}

\subsection*{Example: Frequentist Test of Lady Tasting
Tea}\label{example-frequentist-test-of-lady-tasting-tea}
\addcontentsline{toc}{subsection}{Example: Frequentist Test of Lady
Tasting Tea}

We test \(H_0: \theta=0.5\) (Guessing) vs \(H_1: \theta > 0.5\) (Skill).
We analyze the behavior of \(\bar{X}\) assuming \(H_0\) is true. The
rejection region (one-sided) is shaded red.

\subsection{n=10 (k=7)}

We calculate the P-value: Probability of observing \(\ge 7\) correct out
of 10, assuming \(\theta=0.5\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-freq-sampling-tea-small-1.pdf}}

}

\caption{\label{fig-freq-sampling-tea-small}Sampling Distribution (n= 10
)}

\end{figure}%

\subsection{n=40 (k=28)}

We calculate the P-value: Probability of observing \(\ge 28\) correct
out of 40. With a larger sample size, the same proportion (0.7) provides
\textbf{stronger evidence} against the null.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-freq-sampling-tea-large-1.pdf}}

}

\caption{\label{fig-freq-sampling-tea-large}Sampling Distribution (n= 40
)}

\end{figure}%

\subsection{Questions to Answer}\label{questions-to-answer}

In this course, we will answer several challenging questions related to
general parametric models in the Frequentist framework.

\begin{itemize}
\tightlist
\item
  \textbf{MLE:} Can we use the Maximum Likelihood Estimator (MLE)
  \(\hat{\theta}\) for general models even no closed-form solution
  exists? Is MLE a good method?
\item
  \textbf{Sampling Distributions:} What is the distribution of
  \(\hat{\theta}_{\text{MLE}}\)? What's its mean and standard deviation?
\item
  \textbf{Confidence Intervals:} How to construct CI with
  \(\hat{\theta}\)?
\item
  \textbf{Hypothesis Testing:} How do we derive powerful tests from the
  likelihood function? How to assess goodness-of-fit of parametric
  models with their likelhiood information?
\end{itemize}

\section{Bayesian Inference}\label{bayesian-inference}

\begin{itemize}
\tightlist
\item
  \textbf{Concept:} \(\theta\) is regarded as a random variable.
\item
  \textbf{Posterior:} Posterior \(\propto\) Likelihood \(\times\) Prior.
\end{itemize}

\subsection*{Example: Bayesian Analysis of the Lady Tasting
Tea}\label{example-bayesian-analysis-of-the-lady-tasting-tea}
\addcontentsline{toc}{subsection}{Example: Bayesian Analysis of the Lady
Tasting Tea}

Prior: \(\text{Beta}(1,1)\) (Uniform).

\subsection{n=10 (k=7)}

Posterior: \(\text{Beta}(1+7, 1+3) = \text{Beta}(8, 4)\)

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-bayes-tea-small-1.pdf}}

}

\caption{\label{fig-bayes-tea-small}Bayesian Update (n= 10 )}

\end{figure}%

\subsection{n=40 (k=28)}

Posterior: \(\text{Beta}(1+28, 1+12) = \text{Beta}(29, 13)\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{introstatinf_files/figure-pdf/fig-bayes-tea-large-1.pdf}}

}

\caption{\label{fig-bayes-tea-large}Bayesian Update (n= 40 )}

\end{figure}%

\subsection{Questions to Answer}\label{questions-to-answer-1}

We will also tackle the specific technical challenges involved in
Bayesian analysis.

\begin{itemize}
\tightlist
\item
  \textbf{Posterior Derivation:} How do we derive the posterior
  distribution \(f(\theta|x)\) for various likelihoods and priors?
\item
  \textbf{Comparing with Other methods:} Are Bayesain methods good or
  not or general inference?
\item
  \textbf{Computation:} When the posterior cannot be derived
  analytically, how do we use computational techniques like Markov Chain
  Monte Carlo (MCMC) to sample from it?
\item
  \textbf{Summarization:} How do we construct Credible Intervals (e.g.,
  Highest Posterior Density regions) from posterior samples?
\item
  \textbf{Prediction:} How do we solve the integral required to compute
  the posterior predictive distribution for future data?
\item
  \textbf{Prior:} How to choose our prior? What's its effect on our
  inference?
\item
  \textbf{Model Comparison and Assessment:} How to assess a Bayesian
  model?
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Decision Theory}\label{decision-theory}

\section{Formulation of Decision
Theory}\label{formulation-of-decision-theory}

In decision theory, we formalize the process of making decisions under
uncertainty using the following components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Parameter Space (\(\Theta\)):} The set of all possible states
  of nature or values that the parameter can take. \(\theta \in \Theta\)
  (e.g., mean, variance).
\item
  \textbf{Sample Space (\(\mathcal{X}\)):} The space where the data
  \(X\) lies. Example: \(X = (X_1, X_2, \dots, X_n)\) where
  \(X_i \in \mathbb{R}\). So \(\mathcal{X} \in \mathbb{R}^n\).
\item
  \textbf{Family of Probability Distributions:}
  \(\{P_\theta(x) : \theta \in \Theta\}\). This describes how likely we
  are to see the data \(X\) given a specific parameter \(\theta\).

  \begin{itemize}
  \tightlist
  \item
    If \(X\) is continuous: \(P_\theta(x) = f(x, \theta)\) (Probability
    Density Function).
  \item
    If \(X\) is discrete: \(P_\theta(x) = f(x, \theta)\) (Probability
    Mass Function).
  \end{itemize}
\item
  \textbf{Action Space (\(\mathcal{A}\)):} The set of all actions or
  decisions available to the experimenter.
\item
  \textbf{Loss Function:}
  \(L: \Theta \times \mathcal{A} \rightarrow \mathbb{R}\).
  \(L(\theta, a)\) specifies the loss incurred if the true parameter is
  \(\theta\) and we take action \(a\). Generally,
  \(L(\theta, a) \ge 0\).
\end{enumerate}

\section{Decision Rules and Risk
Functions}\label{decision-rules-and-risk-functions}

\subsection{Decision Rule}\label{decision-rule}

A decision rule is a function
\(d: \mathcal{X} \rightarrow \mathcal{A}\). It dictates the action
\(d(x)\) we take when we observe data \(x\).

\subsection{Risk Function}\label{risk-function}

The risk function is the expected loss for a given decision rule \(d\)
as a function of the parameter \(\theta\).

\[R(\theta, d) = E_\theta[L(\theta, d(X))]\]

\section{Examples of Decision
Problems}\label{examples-of-decision-problems}

\subsection{Example 1: Hypothesis
Testing}\label{example-1-hypothesis-testing}

We want to test \(H_0\) vs \(H_1\).

\begin{itemize}
\tightlist
\item
  \textbf{Action Space:} \(\mathcal{A} = \{0, 1\}\) (0=``Accept
  \(H_0\)'', 1=``Reject \(H_0\)'').
\item
  \textbf{Loss Function (0-1 Loss):} 0 if correct, 1 if wrong.
\item
  \textbf{Risk Function:}

  \begin{itemize}
  \tightlist
  \item
    If \(\theta \in H_0\): \(R(\theta, d) = P(\text{Type I Error})\).
  \item
    If \(\theta \in H_1\): \(R(\theta, d) = P(\text{Type II Error})\).
  \end{itemize}
\end{itemize}

\subsection{Example 2: Point
Estimation}\label{example-2-point-estimation}

We want to estimate a parameter \(\theta\).

\begin{itemize}
\tightlist
\item
  \textbf{Action Space:} \(\mathcal{A} = \Theta\).
\item
  \textbf{Loss Function (Squared Error):}
  \(L(\theta, a) = (\theta - a)^2\).
\item
  \textbf{Risk Function (MSE):}
  \(R(\theta, d) = \text{Var}(\bar{x}) + \text{Bias}^2\).
\end{itemize}

\subsection{Example 3: Interval
Estimation}\label{example-3-interval-estimation}

We want to estimate a range for the parameter.

\begin{itemize}
\tightlist
\item
  \textbf{Action Space:}
  \(\mathcal{A} = \{(l, u) : l \in \mathbb{R}, u \in \mathbb{R}, l \le u\}\).
\end{itemize}

\subsection{Example 4: The Duchess and the Emerald
Necklace}\label{sec-necklace}

\textbf{Scenario:} You are the Duchess of Omnium. You have two
necklaces: a priceless \textbf{Real} one and a valueless
\textbf{Imitation}. They are indistinguishable to you. One is in the
\textbf{Left Drawer (Box 1)}, the other is in the \textbf{Right Drawer
(Box 2)}.

\textbf{The Data (Great Aunt):} You consult your Great Aunt. She
inspects the Left Drawer first, then the Right.

\begin{itemize}
\tightlist
\item
  If the \textbf{Real} necklace is in the \textbf{Left} (\(\theta=1\)):
  She identifies it correctly. (Infallible).
\item
  If the \textbf{Real} necklace is in the \textbf{Right} (\(\theta=2\)):
  She sees the fake first, gets confused, and guesses randomly
  (\(50/50\)).
\end{itemize}

\subsubsection{Formulation}\label{formulation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Parameter Space:} \(\Theta = \{1, 2\}\) (1=Real Left, 2=Real
  Right).
\item
  \textbf{Action Space:} \(\mathcal{A} = \{1, 2\}\) (1=Wear Left, 2=Wear
  Right).
\item
  \textbf{Loss Function:} 0 if correct, 1 if wrong.
\end{enumerate}

\subsubsection{Risk Calculation for Deterministic
Rules}\label{risk-calculation-for-deterministic-rules}

We consider four deterministic rules \(d(X)\). We calculate the risk
(\(R_1\) for \(\theta=1\) and \(R_2\) for \(\theta=2\)) for each.

\textbf{Rule \(d_1\) (Always Left)}

\begin{longtable}[]{@{}llccc@{}}
\toprule\noalign{}
State & Component & \(X=1\) & \(X=2\) & Risk (Sum) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(\theta=1\)} & Loss \(L(1, d)\) & 0 & 0 & \\
& Prob \(P(X \mid \theta=1)\) & 1 & 0 & \textbf{\(R_1 = 0\)} \\
\textbf{\(\theta=2\)} & Loss \(L(2, d)\) & 1 & 1 & \\
& Prob \(P(X \mid \theta=2)\) & 0.5 & 0.5 & \textbf{\(R_2 = 1\)} \\
\end{longtable}

\textbf{Rule \(d_2\) (Always Right)}

\begin{longtable}[]{@{}llccc@{}}
\toprule\noalign{}
State & Component & \(X=1\) & \(X=2\) & Risk (Sum) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(\theta=1\)} & Loss \(L(1, d)\) & 1 & 1 & \\
& Prob \(P(X \mid \theta=1)\) & 1 & 0 & \textbf{\(R_1 = 1\)} \\
\textbf{\(\theta=2\)} & Loss \(L(2, d)\) & 0 & 0 & \\
& Prob \(P(X \mid \theta=2)\) & 0.5 & 0.5 & \textbf{\(R_2 = 0\)} \\
\end{longtable}

\textbf{Rule \(d_3\) (Follow Aunt)}

\begin{longtable}[]{@{}llccc@{}}
\toprule\noalign{}
State & Component & \(X=1\) & \(X=2\) & Risk (Sum) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(\theta=1\)} & Loss \(L(1, d)\) & 0 & 1 & \\
& Prob \(P(X \mid \theta=1)\) & 1 & 0 & \textbf{\(R_1 = 0\)} \\
\textbf{\(\theta=2\)} & Loss \(L(2, d)\) & 1 & 0 & \\
& Prob \(P(X \mid \theta=2)\) & 0.5 & 0.5 & \textbf{\(R_2 = 0.5\)} \\
\end{longtable}

\textbf{Rule \(d_4\) (Do Opposite)}

\begin{longtable}[]{@{}llccc@{}}
\toprule\noalign{}
State & Component & \(X=1\) & \(X=2\) & Risk (Sum) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(\theta=1\)} & Loss \(L(1, d)\) & 1 & 0 & \\
& Prob \(P(X \mid \theta=1)\) & 1 & 0 & \textbf{\(R_1 = 1\)} \\
\textbf{\(\theta=2\)} & Loss \(L(2, d)\) & 0 & 1 & \\
& Prob \(P(X \mid \theta=2)\) & 0.5 & 0.5 & \textbf{\(R_2 = 0.5\)} \\
\end{longtable}

\section{Principles for Choosing a Decision
Rule}\label{principles-for-choosing-a-decision-rule}

Since no single rule minimizes risk for all \(\theta\), we rely on
several principles to order and select decision rules.

\subsection{Admissibility}\label{admissibility}

A decision rule \(d\) is \textbf{admissible} if it is not ``dominated''
by any other rule.

\begin{itemize}
\tightlist
\item
  \textbf{Domination:} A rule \(d\) dominates \(d'\) if
  \(R(\theta, d) \le R(\theta, d')\) for all \(\theta\), with strict
  inequality for at least one \(\theta\).
\item
  \textbf{Inadmissibility:} If a rule is dominated, it is inadmissible
  and can be discarded (we can do better or equal in every possible
  state).
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-admissibility-1.pdf}}

}

\caption{\label{fig-admissibility}Illustration of Domination: Rule A
(Red) is inadmissible because Rule B (Blue) has lower risk for all
values of theta.}

\end{figure}%

\subsection{Minimax Principle}\label{minimax-principle}

The Minimax principle is a conservative approach that guards against the
worst-case scenario. It selects the rule that minimizes the maximum
risk. \[ \min_{d} \left[ \sup_{\theta} R(\theta, d) \right] \]

In the plot below, while Rule B has lower risk in the center, it has a
very high maximum risk. Rule A is ``flatter'' and has a lower maximum
value, making it the \textbf{Minimax} choice.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-minimax-curve-1.pdf}}

}

\caption{\label{fig-minimax-curve}Illustration of Minimax: Rule A has a
lower peak risk than Rule B, making Rule A the Minimax choice.}

\end{figure}%

\subsection{Bayes Decision Rules}\label{bayes-decision-rules}

The Bayes principle incorporates prior knowledge. If we assign a
probability distribution (prior) \(\pi(\theta)\) to the parameter, we
can calculate the \textbf{Bayes Risk}, which is the weighted average of
the risk function. We choose the rule that minimizes this average.
\[ r(\pi, d) = E_\pi [R(\theta, d)] = \int_\Theta R(\theta, d) \pi(\theta) d\theta \]

\section{Risk Set for Finite Parameter
Space}\label{risk-set-for-finite-parameter-space}

For finite parameter spaces (e.g., \(\Theta = \{1, 2\}\)), we can
visualize the problem in 2D space where the axes are
\(R_1 = R(\theta_1)\) and \(R_2 = R(\theta_2)\).

\subsection{\texorpdfstring{The Risk Set
(\(S\))}{The Risk Set (S)}}\label{the-risk-set-s}

The set of all possible risk vectors is called the Risk Set \(S\).

\begin{itemize}
\tightlist
\item
  \textbf{Deterministic Rules:} These are the vertices of the set.
\item
  \textbf{Randomized Rules:} By choosing rule \(d_i\) with probability
  \(p\) and \(d_j\) with probability \(1-p\), we can achieve any risk on
  the line segment connecting them.
\item
  \textbf{Convexity:} The Risk Set is the \textbf{convex hull} of the
  deterministic rules.
\end{itemize}

\subsection{Visualizing Admissibility}\label{visualizing-admissibility}

The admissible rules lie on the \textbf{lower-left boundary} of the set.
Any point to the ``north-east'' of another point is dominated
(inadmissible).

\subsection{Visualizing Minimax}\label{visualizing-minimax}

The Minimax rule is found by intersecting the Risk Set with the line
\(y=x\) (\(R_1 = R_2\)).

\begin{itemize}
\tightlist
\item
  We look for the point in \(S\) that touches the \(45^\circ\) line at
  the lowest value.
\item
  If the set is entirely below the line, we minimize \(R_2\). If
  entirely above, we minimize \(R_1\).
\end{itemize}

\subsection{Visualizing Bayes Rules}\label{visualizing-bayes-rules}

A Bayes rule minimizes \(\pi_1 R_1 + \pi_2 R_2 = k\). This equation
represents a line with slope \(m = -\pi_1 / \pi_2\).

\begin{itemize}
\tightlist
\item
  To find the Bayes rule, we find the \textbf{tangent line} to the Risk
  Set \(S\) with slope \(-\pi_1 / \pi_2\).
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-generic-geometry-1.pdf}}

}

\caption{\label{fig-generic-geometry}Geometric Interpretation: The gray
polygon is the Risk Set S. The blue boundary represents admissible
rules. The red point is the Minimax rule. The green line represents a
Bayes rule for a specific prior.}

\end{figure}%

\section{Revisiting the Necklace Example: Geometric
Solution}\label{revisiting-the-necklace-example-geometric-solution}

We now apply the geometric interpretation to the Necklace problem using
the risks calculated in Section~\ref{sec-necklace}.

\begin{itemize}
\tightlist
\item
  \(d_1\): \((0, 1)\)
\item
  \(d_2\): \((1, 0)\)
\item
  \(d_3\): \((0, 0.5)\)
\item
  \(d_4\): \((1, 0.5)\)
\end{itemize}

\subsection{Analysis}\label{analysis}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Admissibility:}

  \begin{itemize}
  \tightlist
  \item
    \(d_4\) has risk \((1, 0.5)\). \(d_3\) has risk \((0, 0.5)\). Since
    \(0 < 1\), \(d_3\) strictly dominates \(d_4\). Thus \(d_4\) is
    \textbf{inadmissible}.
  \item
    The efficient frontier connects \(d_3\) and \(d_2\).
  \end{itemize}
\item
  \textbf{Minimax Solution:} The Minimax rule lies on the segment
  connecting \(d_3 (0, 0.5)\) and \(d_2 (1, 0)\).

  \begin{itemize}
  \tightlist
  \item
    Let the randomized rule be \(\delta^* = p d_3 + (1-p) d_2\).
  \item
    \(R(\delta^*) = p \begin{pmatrix} 0 \\ 0.5 \end{pmatrix} + (1-p) \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 1-p \\ 0.5p \end{pmatrix}\).
  \item
    Set \(R_1 = R_2\):
    \(1-p = 0.5p \Rightarrow 1 = 1.5p \Rightarrow p = 2/3\).
  \item
    \textbf{Result:} The Minimax rule is to choose \(d_3\) with
    probability \(2/3\) and \(d_2\) with probability \(1/3\).
  \end{itemize}
\end{enumerate}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-necklace-geometry-1.pdf}}

}

\caption{\label{fig-necklace-geometry}Necklace Problem Solution. The
Minimax rule (red diamond) is the specific randomized combination of d3
and d2 that equalizes the risk.}

\end{figure}%

\section{Theorems Relating Minimax and Bayes
Rules}\label{theorems-relating-minimax-and-bayes-rules}

In practice, finding a Minimax rule directly is mathematically
difficult. A standard strategy is to ``guess'' a Least Favorable Prior
\(\pi\)---defined as the prior distribution that maximizes the minimum
Bayes risk (i.e., the prior against which it is hardest to
defend)---find the corresponding Bayes rule, and then check if it
satisfies specific conditions to confirm it is Minimax.

\subsection{Constant Risk Bayes Rule Is Minimax (Proof by
Contradiction)}\label{constant-risk-bayes-rule-is-minimax-proof-by-contradiction}

\begin{theorem}[Constant Risk Bayes Rule Is
Minimax]\protect\hypertarget{thm-minimax-constant}{}\label{thm-minimax-constant}

Let \(\delta^\pi\) be a Bayes estimator with respect to a prior \(\pi\).
If the risk function of \(\delta^\pi\) is constant on the parameter
space \(\Theta\), such that \(R(\theta, \delta^\pi) = c\) for all
\(\theta \in \Theta\), then \(\delta^\pi\) is a minimax estimator.

\end{theorem}

\begin{proof}
Assume, for the sake of contradiction, that \(\delta^\pi\) is
\textbf{not} a minimax estimator.

By definition, if \(\delta^\pi\) is not minimax, there must exist some
other estimator \(\delta'\) that has a strictly smaller maximum risk.
That is:

\[
\sup_{\theta \in \Theta} R(\theta, \delta') < \sup_{\theta \in \Theta} R(\theta, \delta^\pi)
\]

Since we are given that \(R(\theta, \delta^\pi) = c\) for all
\(\theta \in \Theta\), its supremum is simply \(c\). Therefore, our
assumption implies:

\[
\sup_{\theta \in \Theta} R(\theta, \delta') < c
\]

Now, consider the Bayes risk of \(\delta'\) with respect to the prior
\(\pi\). The Bayes risk is the weighted average of the risk function:

\[
r(\pi, \delta') = \int_\Theta R(\theta, \delta') \pi(\theta) d\theta
\]

Since \(R(\theta, \delta') \le \sup_{\theta} R(\theta, \delta')\) for
all \(\theta\), and we assumed this supremum is strictly less than
\(c\), it follows that:

\[
r(\pi, \delta') \le \sup_{\theta \in \Theta} R(\theta, \delta') < c
\]

However, we know that \(c\) is the Bayes risk of \(\delta^\pi\):

\[
r(\pi, \delta^\pi) = \int_\Theta c \, \pi(\theta) d\theta = c
\]

Substituting this into our inequality, we get:

\[
r(\pi, \delta') < r(\pi, \delta^\pi)
\]

This result contradicts the fact that \(\delta^\pi\) is a \textbf{Bayes
estimator}. By definition, a Bayes estimator must minimize the Bayes
risk, meaning \(r(\pi, \delta^\pi) \le r(\pi, \delta)\) for any
estimator \(\delta\).

Because our assumption that \(\delta^\pi\) is not minimax leads to a
contradiction of the Bayes optimality of \(\delta^\pi\), the assumption
must be false. Thus, \(\delta^\pi\) must be minimax.
\end{proof}

The plot below visualizes this logic. If an estimator \(\delta'\) (Blue)
were to be ``better'' in a minimax sense than \(\delta^\pi\) (Red), its
entire curve would have to stay below the maximum value \(c\). However,
if it stays below \(c\) everywhere, its average (Bayes risk) would
necessarily be lower than \(c\), which is impossible if \(\delta^\pi\)
is the Bayes estimator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define Parameter Space Theta}
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{200}\NormalTok{)}

\CommentTok{\# 1. Constant Risk Bayes Estimator (risk = C)}
\NormalTok{c\_val }\OtherTok{\textless{}{-}} \FloatTok{0.6}
\NormalTok{risk\_bayes }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(c\_val, }\FunctionTok{length}\NormalTok{(theta))}

\CommentTok{\# 2. An estimator that would contradict Bayes optimality }
\CommentTok{\# (Always below the constant risk line)}
\NormalTok{risk\_contradiction }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{+} \FloatTok{0.05} \SpecialCharTok{*} \FunctionTok{cos}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ theta)}

\CommentTok{\# Plotting}
\FunctionTok{plot}\NormalTok{(theta, risk\_bayes, }\AttributeTok{type =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ylab =} \StringTok{"Risk R(theta, d)"}\NormalTok{, }\AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(theta),}
     \AttributeTok{main =} \StringTok{"Proof by Contradiction Geometry"}\NormalTok{)}

\CommentTok{\# Add the "Better" Estimator (which is impossible)}
\FunctionTok{lines}\NormalTok{(theta, risk\_contradiction, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Shaded area showing the "Impossible" Bayes Risk improvement}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(theta, }\FunctionTok{rev}\NormalTok{(theta)), }\FunctionTok{c}\NormalTok{(risk\_contradiction, }\FunctionTok{rev}\NormalTok{(risk\_bayes)), }
        \AttributeTok{col =} \FunctionTok{rgb}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}

\CommentTok{\# Add Legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Constant Risk Bayes (c)"}\NormalTok{, }\StringTok{"Hypothetical \textquotesingle{}Better\textquotesingle{} Est."}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-minimax-contradiction-1.pdf}}

}

\caption{\label{fig-minimax-contradiction}Visualizing the Contradiction:
If the blue curve's maximum were below the red line, its average risk
would be lower than the Bayes risk of the red estimator.}

\end{figure}%

\subsection{Minimaxity via Limiting Bayes
Risks}\label{minimaxity-via-limiting-bayes-risks}

Sometimes the Minimax rule corresponds to an ``improper'' prior (a prior
that does not integrate to 1, like a uniform distribution on the real
line). We approach these via a limiting sequence.

\begin{theorem}[Minimaxity of Limit-Attaining
Rules]\protect\hypertarget{thm-limits}{}\label{thm-limits}

Let \(\{\delta_n\}\) be a sequence of Bayes rules with respect to priors
\(\{\pi_n\}\). Let \(r(\pi_n, \delta_n)\) be the associated Bayes risks.
If there exists a rule \(\delta_0\) such that:
\[\sup_{\theta} R(\theta, \delta_0) \le \lim_{n \to \infty} r(\pi_n, \delta_n)\]
Then \(\delta_0\) is Minimax.

\end{theorem}

\begin{proof}
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Define Limit:} Let
  \(V = \lim_{n \to \infty} r(\pi_n, \delta_n)\). We are given that
  \(\sup_{\theta} R(\theta, \delta_0) \le V\).
\item
  \textbf{Contradiction Setup:} Suppose \(\delta_0\) is \emph{not}
  Minimax. Then there exists a rule \(\delta^*\) such that:
  \[\sup_{\theta} R(\theta, \delta^*) < \sup_{\theta} R(\theta, \delta_0) \le V\]
  Let \(\sup_{\theta} R(\theta, \delta^*) = V - \epsilon\) for some
  \(\epsilon > 0\).
\item
  \textbf{Bounded Risk of \(\delta^*\):} The Bayes risk of \(\delta^*\)
  is bounded by its maximum risk:
  \[r(\pi_n, \delta^*) = \int R(\theta, \delta^*) \pi_n(\theta) d\theta \le V - \epsilon\]
  Therefore,
  \(\lim_{n \to \infty} r(\pi_n, \delta^*) \le V - \epsilon\).
\item
  \textbf{Optimality of \(\delta_n\):} Since \(\delta_n\) is the Bayes
  rule for \(\pi_n\), it minimizes Bayes risk. This creates the
  inequality pair shown in the figure (Orange \(\le\) Blue):
  \[r(\pi_n, \delta_n) \le r(\pi_n, \delta^*)\]
\item
  \textbf{The Contradiction:} Combining the inequalities, we get:
  \[\lim_{n \to \infty} r(\pi_n, \delta_n) \le \lim_{n \to \infty} r(\pi_n, \delta^*) \le V - \epsilon\]
  This implies \(V \le V - \epsilon\), which is impossible. Thus
  \(\delta_0\) must be Minimax. \(\blacksquare\)
\end{enumerate}

\end{proof}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{decision_files/figure-pdf/fig-visualize-limit-bayes-1.pdf}}

}

\caption{\label{fig-visualize-limit-bayes}Visual Proof: We examine the
Bayes risks at two steps, \(n=j\) (squares) and \(n=j+1\) (circles). In
both steps, the optimal risk \(r(\pi_n, \delta_n)\) (orange) must be
lower than the hypothetical risk \(r(\pi_n, \delta^*)\) (blue). Even as
the sequence rises (j+1 is higher than j), the blue points are capped by
the bound \(V-\epsilon\). This `traps' the orange points, making it
impossible for them to ever reach the Limit V.}

\end{figure}%

\subsection{Procedure: Verifying
Minimaxity}\label{procedure-verifying-minimaxity}

The theorem above provides a practical recipe for identifying Minimax
rules, particularly in unbounded parameter spaces (where a standard
Least Favorable Prior often does not exist). The procedure is often used
``backwards''---we guess a rule and then construct a sequence to prove
it is Minimax.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Propose a Candidate Rule (\(\delta_0\)):} Identify a rule that
  intuitively seems robust. Typically, we look for an \textbf{Equalizer
  Rule}, which is a rule with constant risk (\(R(\theta, \delta_0) = C\)
  for all \(\theta\)). If the risk is constant, then
  \(\sup_\theta R(\theta, \delta_0) = C\).
\item
  \textbf{Construct a Sequence of Priors (\(\pi_n\)):} Choose a sequence
  of priors that becomes increasingly ``diffuse'' or ``flat'' as
  \(n \to \infty\) (e.g., Uniform on \([-n, n]\) or Normal with variance
  \(n\)). These approximate the ``improper'' prior corresponding to the
  candidate rule.
\item
  \textbf{Compute Bayes Risks (\(r_n\)):} Calculate the Bayes risk
  \(r(\pi_n, \delta_n)\) for each prior in the sequence. Note that you
  do not necessarily need the formula for the Bayes rule \(\delta_n\)
  itself, only its associated risk.
\item
  \textbf{Verify the Condition:} Check if the limit of the Bayes risks
  approaches the maximum risk of your candidate:
  \[ \lim_{n \to \infty} r(\pi_n, \delta_n) = \sup_{\theta} R(\theta, \delta_0) \]
  If this holds, \(\delta_0\) is Minimax.
\end{enumerate}

\begin{example}[]\protect\hypertarget{exm-normal-mean}{}\label{exm-normal-mean}

\textbf{Example: The Normal Mean}

Consider a single observation \(X \sim N(\theta, 1)\) with squared error
loss \(L(\theta, \delta) = (\theta - \delta)^2\). We suspect the sample
mean (in this case, just \(X\) itself) is the Minimax estimator.

\textbf{Step 1: Candidate Rule}

Let \(\delta_0(X) = X\). The risk is the variance of the estimator:
\[ R(\theta, \delta_0) = E[(\theta - X)^2] = \text{Var}(X) = 1 \] Since
the risk is constant (1) for all \(\theta\),
\(\sup_\theta R(\theta, \delta_0) = 1\).

\textbf{Step 2: Sequence of Priors}

We choose a sequence of Normal priors \(\pi_n \sim N(0, n)\). As \(n\)
increases, the variance increases, making the prior flatter over the
real line.

\textbf{Step 3: Bayes Risks}

For a Normal prior \(\theta \sim N(0, \tau^2)\) and data
\(X \sim N(\theta, \sigma^2)\), the Bayes risk is known to be:
\[ r(\pi, \delta_\pi) = \frac{\sigma^2 \tau^2}{\sigma^2 + \tau^2} \]
Substituting our values (\(\sigma^2=1, \tau^2=n\)):
\[ r(\pi_n, \delta_n) = \frac{1 \cdot n}{1 + n} = \frac{n}{n+1} \]

\textbf{Step 4: Verification}

We take the limit of the sequence of Bayes risks:
\[ \lim_{n \to \infty} r(\pi_n, \delta_n) = \lim_{n \to \infty} \frac{n}{n+1} = 1 \]
Comparing this to our candidate:
\[ \sup_{\theta} R(\theta, \delta_0) = 1 \le 1 \] The condition holds.
Therefore, \(\delta_0(X) = X\) is the Minimax estimator for \(\theta\).

\end{example}

\subsection{Bayes Rule as a Working Horse to Find a Minimax
Rule}\label{bayes-rule-as-a-working-horse-to-find-a-minimax-rule}

\subsubsection{The Minimax Theorem (Saddle
Point)}\label{the-minimax-theorem-saddle-point}

This theorem connects the search for a Minimax rule to the search for a
Least Favorable Prior. It justifies the strategy of ``finding the worst
prior and solving it.''

\begin{theorem}[The Minimax
Theorem]\protect\hypertarget{thm-minimax}{}\label{thm-minimax}

Let \(\mathcal{D}\) be the set of all decision rules and \(\Pi\) be the
set of all prior distributions. Let \(r(\pi, \delta)\) denote the Bayes
risk.

The Minimax value equals the Maximin Bayes value:
\[ \inf_{\delta \in \mathcal{D}} \sup_{\pi \in \Pi} r(\pi, \delta) = \sup_{\pi \in \Pi} \inf_{\delta \in \mathcal{D}} r(\pi, \delta) \]

Furthermore, a pair \((\delta_0, \pi_0)\) is a \textbf{Saddle Point} if
for all \(\delta \in \mathcal{D}\) and \(\pi \in \Pi\):
\[ r(\pi_0, \delta) \ge r(\pi_0, \delta_0) \ge r(\pi, \delta_0) \] If
such a saddle point exists, then:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\delta_0\) is a \textbf{Minimax rule}.
\item
  \(\pi_0\) is a \textbf{Least Favorable Prior}.
\end{enumerate}

\end{theorem}

\begin{proof}
\textbf{Goal:} We wish to show that if \((\delta_0, \pi_0)\) is a saddle
point, then
\(\sup_{\theta} R(\theta, \delta_0) \le \sup_{\theta} R(\theta, \delta)\)
for any other rule \(\delta\).

\textbf{1. Interpret the Saddle Point Inequalities:} The condition is
given as two simultaneous inequalities: \[
\begin{aligned}
(A) \quad & r(\pi_0, \delta_0) \le r(\pi_0, \delta) \quad \text{for all } \delta \\
(B) \quad & r(\pi, \delta_0) \le r(\pi_0, \delta_0) \quad \text{for all } \pi
\end{aligned}
\]

\textbf{2. Analyze Inequality (A):} Since
\(r(\pi_0, \delta_0) \le r(\pi_0, \delta)\) for all \(\delta\),
\(\delta_0\) minimizes the Bayes risk with respect to \(\pi_0\).

\begin{itemize}
\tightlist
\item
  Therefore, \textbf{\(\delta_0\) is the Bayes rule} for \(\pi_0\).
\end{itemize}

\textbf{3. Analyze Inequality (B):} Since
\(r(\pi, \delta_0) \le r(\pi_0, \delta_0)\) for all \(\pi\), the prior
\(\pi_0\) maximizes the average risk of \(\delta_0\).

\begin{itemize}
\item
  Since the supremum over all priors includes point-mass priors (which
  yield the risk at a single \(\theta\)), maximizing over \(\pi\) is
  equivalent to maximizing over \(\theta\):
  \[ \sup_{\pi} r(\pi, \delta_0) = \sup_{\theta} R(\theta, \delta_0) \]
\item
  Therefore, Inequality (B) implies:
  \[ \sup_{\theta} R(\theta, \delta_0) = r(\pi_0, \delta_0) \]
\end{itemize}

\textbf{4. Combine to Prove Minimaxity:} Let \(\delta^*\) be any
arbitrary decision rule. We compute its worst-case risk: \[
\begin{aligned}
\sup_{\theta} R(\theta, \delta^*) &= \sup_{\pi} r(\pi, \delta^*) & \text{(Max risk = Max average risk)} \\
&\ge r(\pi_0, \delta^*) & \text{(Supremum $\ge$ specific value)} \\
&\ge r(\pi_0, \delta_0) & \text{(From Inequality A: $\delta_0$ is Bayes for $\pi_0$)} \\
&= \sup_{\theta} R(\theta, \delta_0) & \text{(From Step 3)}
\end{aligned}
\]

\textbf{5. Conclusion:} We have shown that for any \(\delta^*\):
\[ \sup_{\theta} R(\theta, \delta^*) \ge \sup_{\theta} R(\theta, \delta_0) \]
Thus, \(\delta_0\) minimizes the maximum risk. \(\delta_0\) is Minimax.
\(\blacksquare\)
\end{proof}

\subsubsection{Alternating Optimization on the Risk
Surface}\label{alternating-optimization-on-the-risk-surface}

The Minimax solution can be found computationally by iteratively
optimizing one variable while holding the other fixed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fix Prior \(\pi\), Minimize Risk:} We search the valley bottom
  for the current \(\pi\).
\item
  \textbf{Fix Rule \(\delta\), Maximize Risk:} We search the hill top
  for the current \(\delta\).
\end{enumerate}

This creates a ``zigzag'' path on the surface that converges to the
saddle point.

\section{Admissibility of Bayes
Rules}\label{admissibility-of-bayes-rules}

Bayes rules are generally good candidates for admissibility. If a rule
is Bayes, it is likely efficient, provided the prior doesn't ignore
parts of the parameter space.

\begin{theorem}[Admissibility of Bayes Rules (Finite
Support)]\protect\hypertarget{thm-admissibility-finite}{}\label{thm-admissibility-finite}

If the parameter space \(\Theta\) is finite (or countable) and the prior
\(\pi\) assigns positive probability to every \(\theta \in \Theta\)
(i.e., \(\pi(\theta) > 0\) for all \(\theta\)), then any Bayes rule
\(\delta_\pi\) is admissible.

\end{theorem}

\begin{proof}
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Contradiction Setup:} Suppose \(\delta_\pi\) is inadmissible.
  Then there exists a rule \(\delta'\) that dominates it. By definition
  of domination:

  \begin{itemize}
  \tightlist
  \item
    \(R(\theta, \delta') \le R(\theta, \delta_\pi)\) for all \(\theta\).
  \item
    \(R(\theta_k, \delta') < R(\theta_k, \delta_\pi)\) for at least one
    \(\theta_k\).
  \end{itemize}
\item
  \textbf{Bayes Risk Difference:} Consider the difference in Bayes risk:
  \[r(\pi, \delta_\pi) - r(\pi, \delta') = \sum_{\theta \in \Theta} \pi(\theta) [R(\theta, \delta_\pi) - R(\theta, \delta')]\]
\item
  \textbf{Strict Positivity:}

  \begin{itemize}
  \tightlist
  \item
    Since \(\delta'\) dominates \(\delta_\pi\), each term
    \([R(\theta, \delta_\pi) - R(\theta, \delta')]\) is non-negative
    (\(\ge 0\)).
  \item
    At \(\theta_k\), the term is strictly positive (\(> 0\)).
  \item
    We assumed the prior has full support, so \(\pi(\theta) > 0\) for
    all \(\theta\).
  \end{itemize}
\item
  \textbf{Summation:} A sum of non-negative terms where at least one
  term is strictly positive must be strictly positive.
  \[r(\pi, \delta_\pi) - r(\pi, \delta') > 0 \implies r(\pi, \delta') < r(\pi, \delta_\pi)\]
\item
  \textbf{Conclusion:} This contradicts the definition that
  \(\delta_\pi\) is a Bayes rule (which must minimize Bayes risk).
  Therefore, \(\delta_\pi\) is admissible. \(\blacksquare\)
\end{enumerate}

\end{proof}

\subsection{Admissibility of Unique Bayes
Rules}\label{admissibility-of-unique-bayes-rules}

If the Bayes rule is unique, we can drop the requirement that the
parameter space be discrete or finite.

\begin{theorem}[Admissibility of Unique Bayes
Rules]\protect\hypertarget{thm-admissibility-unique}{}\label{thm-admissibility-unique}

Let \(\delta_\pi\) be a Bayes rule with respect to \(\pi\). If
\(\delta_\pi\) is the \textbf{unique} Bayes rule (up to risk
equivalence), then \(\delta_\pi\) is admissible.

\end{theorem}

\begin{proof}
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Contradiction Setup:} Suppose \(\delta_\pi\) is inadmissible.
  Then there exists a rule \(\delta'\) such that:
  \(R(\theta, \delta') \le R(\theta, \delta_\pi)\) for all \(\theta\),
  with strict inequality for some set of \(\theta\).
\item
  \textbf{Bayes Risk Inequality:} Taking the expectation with respect to
  \(\pi\):
  \[r(\pi, \delta') = \int R(\theta, \delta') \pi(\theta) d\theta \le \int R(\theta, \delta_\pi) \pi(\theta) d\theta = r(\pi, \delta_\pi)\]
\item
  \textbf{Minimality:} Since \(\delta_\pi\) is Bayes, it minimizes the
  risk, so \(r(\pi, \delta_\pi) \le r(\pi, \delta')\). Combining these
  gives \(r(\pi, \delta') = r(\pi, \delta_\pi)\).
\item
  \textbf{Uniqueness:} This implies that \(\delta'\) is also a Bayes
  rule. However, we assumed that \(\delta_\pi\) is the \textbf{unique}
  Bayes rule. Therefore, \(\delta'\) must be equal to \(\delta_\pi\) (in
  terms of risk functions).
\item
  \textbf{Conclusion:} If \(\delta'\) and \(\delta_\pi\) have identical
  risk functions, then \(\delta'\) cannot strictly dominate
  \(\delta_\pi\). This contradicts the assumption of inadmissibility.
  Thus, \(\delta_\pi\) is admissible. \(\blacksquare\)
\end{enumerate}

\end{proof}

\bookmarksetup{startatroot}

\chapter{Bayesian Methods}\label{bayesian-methods}

\section{Fundamental Elements of Bayesian
Inference}\label{fundamental-elements-of-bayesian-inference}

The foundation of Bayesian inference relies on the relationship between
the prior distribution, the likelihood of the data, and the posterior
distribution. This relationship is governed by Bayes' Theorem (or Law).

\begin{definition}[Posterior
Distribution]\protect\hypertarget{def-posterior}{}\label{def-posterior}

Suppose we have a parameter \(\theta\) with a prior distribution denoted
by \(\pi(\theta)\). If we observe data \(x\) drawn from a distribution
with probability density function (pdf) \(f(x; \theta)\), then the
\textbf{posterior density} of \(\theta\) given the data \(x\) is defined
as:

\[
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{m(x)}
\]

where \(m(x)\) is the \textbf{marginal distribution} (or marginal
likelihood) of the data, calculated as: \[
m(x) = \int_{\Theta} \pi(\theta) f(x;\theta) d\theta
\]

In this context, \(m(x)\) acts as a normalizing constant. Since it
depends only on the data \(x\) and not on the parameter \(\theta\), it
ensures that the posterior density integrates to 1 but does not
influence the \textbf{shape} of the posterior distribution.

Thus, we often state the proportional relationship:

\[
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
\]

\end{definition}

\begin{example}[Binomial-beta
Conjugacy]\protect\hypertarget{exm-binomial-beta}{}\label{exm-binomial-beta}

Consider an experiment where \(x|\theta \sim \text{Bin}(n, \theta)\).
The likelihood function is:

\[
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
\]

Suppose we choose a Beta distribution as the prior for \(\theta\), such
that \(\theta \sim \text{Beta}(a, b)\). The prior density is:

\[
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
\]

where \(B(a,b)\) is the Beta function defined as
\(\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta\).

To find the posterior, we multiply the prior and the likelihood:

\[
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
\]

Combining terms with the same base:

\[
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
\]

We can recognize this kernel as a Beta distribution. Therefore, we
conclude that the posterior distribution is:

\[
\theta|x \sim \text{Beta}(a+x, b+n-x)
\]

\textbf{Properties of the Posterior:}

\begin{itemize}
\item
  The posterior mean is: \[E(\theta|x) = \frac{a+x}{a+b+n}\] As
  \(n \to \infty\), this approximates the maximum likelihood estimate
  \(\frac{x}{n}\).
\item
  The posterior variance is:
  \[\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\] For
  large \(n\), this approximates
  \(\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}\).
\end{itemize}

\textbf{Numerical Illustration:}

Suppose we are estimating a probability \(\theta\).

\begin{itemize}
\tightlist
\item
  \textbf{Prior:} \(\theta \sim \text{Beta}(2, 2)\) (Mean = 0.5).
\item
  \textbf{Data:} 10 trials, 8 successes (\(n=10, x=8\)).
\item
  \textbf{Posterior:}
  \(\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)\) (Mean
  \(\approx\) 0.71).
\end{itemize}

The plot below shows the prior (dashed) and posterior (solid) densities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{200}\NormalTok{)}

\CommentTok{\# Prior: Beta(2, 2)}
\NormalTok{prior }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(theta, }\AttributeTok{shape1 =} \DecValTok{2}\NormalTok{, }\AttributeTok{shape2 =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Posterior: Beta(10, 4)}
\NormalTok{posterior }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(theta, }\AttributeTok{shape1 =} \DecValTok{10}\NormalTok{, }\AttributeTok{shape2 =} \DecValTok{4}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(theta, posterior, }\AttributeTok{type =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(theta), }\AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Beta Prior vs Posterior"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(prior, posterior))))}
\FunctionTok{lines}\NormalTok{(theta, prior, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Prior Beta(2,2)"}\NormalTok{, }\StringTok{"Posterior Beta(10,4)"}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-beta-conjugacy-1.pdf}}

}

\caption{\label{fig-beta-conjugacy}Prior vs Posterior for Beta-Binomial
Example}

\end{figure}%

\end{example}

\begin{example}[Normal-normal Conjugacy (known
Variance)]\protect\hypertarget{exm-normal-normal}{}\label{exm-normal-normal}

Let \(X_1, X_2, \dots, X_n\) be independent and identically distributed
(i.i.d.) variables such that \(X_i \sim N(\mu, \sigma^2)\), where
\(\sigma^2\) is known.

We assign a Normal prior to the mean \(\mu\):
\(\mu \sim N(\mu_0, \sigma_0^2)\).

To find the posterior \(\pi(\mu|x_1, \dots, x_n)\), let
\(x = (x_1, \dots, x_n)\). The posterior is proportional to:

\[
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
\]

\[
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
\]

\textbf{Posterior Precision:}

It is often more convenient to work with \textbf{precision} (the inverse
of variance). Let:

\begin{itemize}
\tightlist
\item
  \(\tau_0 = 1/\sigma_0^2\) (Prior precision)
\item
  \(\tau = 1/\sigma^2\) (Data precision)
\item
  \(\tau_1 = 1/\sigma_1^2\) (Posterior precision)
\end{itemize}

The relationship is additive:

\[
\tau_1 = \tau_0 + n\tau
\]

\[
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
\]

The posterior mean \(\mu_1\) is a weighted average of the prior mean and
the sample mean:

\[
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
\]

So, the posterior distribution is:

\[
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
\]

\textbf{Numerical Illustration:}

Suppose we estimate a mean height \(\mu\).

\begin{itemize}
\tightlist
\item
  \textbf{Known Variance:} \(\sigma^2 = 100\) (\(\tau = 0.01\)).
\item
  \textbf{Prior:} \(\mu \sim N(175, 25)\) (Precision \(\tau_0 = 0.04\)).
\item
  \textbf{Data:} \(n=10, \bar{x}=180\). (Total data precision
  \(n\tau = 0.1\)).
\item
  \textbf{Posterior:}

  \begin{itemize}
  \tightlist
  \item
    Precision \(\tau_1 = 0.04 + 0.1 = 0.14\).
  \item
    Variance \(\sigma_1^2 \approx 7.14\).
  \item
    Mean \(\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6\).
  \end{itemize}
\end{itemize}

The plot below illustrates the prior (dashed) and posterior (solid)
normal densities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu\_vals }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{150}\NormalTok{, }\DecValTok{200}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{200}\NormalTok{)}

\CommentTok{\# Prior: N(175, 25) {-}\textgreater{} SD = 5}
\NormalTok{prior\_norm }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(mu\_vals, }\AttributeTok{mean =} \DecValTok{175}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Posterior: N(178.6, 7.14) {-}\textgreater{} SD = Sqrt(7.14) Approx 2.67}
\NormalTok{posterior\_norm }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(mu\_vals, }\AttributeTok{mean =} \FloatTok{178.6}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{7.14}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(mu\_vals, posterior\_norm, }\AttributeTok{type =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(mu), }\AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Normal Prior vs Posterior"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(prior\_norm, posterior\_norm))))}
\FunctionTok{lines}\NormalTok{(mu\_vals, prior\_norm, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Prior N(175, 25)"}\NormalTok{, }\StringTok{"Posterior N(178.6, 7.14)"}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-normal-conjugacy-1.pdf}}

}

\caption{\label{fig-normal-conjugacy}Prior vs Posterior for
Normal-Normal Example}

\end{figure}%

\end{example}

\begin{example}[Discrete Posterior
Calculation]\protect\hypertarget{exm-discrete-posterior}{}\label{exm-discrete-posterior}

Consider the following table where we calculate the posterior
probabilities for a discrete parameter space.

Let the parameter \(\theta\) take values \(\{1, 2, 3\}\) with prior
probabilities \(\pi(\theta)\). Let the data \(x\) take values
\(\{0, 1, 2, \dots\}\).

Given:

\begin{itemize}
\tightlist
\item
  Prior \(\pi(\theta)\): \(\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3\).
\item
  Likelihood \(\pi(x|\theta)\):

  \begin{itemize}
  \tightlist
  \item
    If \(\theta=1\), \(x \sim \text{Uniform on } \{0, 1\}\) (Prob =
    1/2).
  \item
    If \(\theta=2\), \(x \sim \text{Uniform on } \{0, 1, 2\}\) (Prob =
    1/3).
  \item
    If \(\theta=3\), \(x \sim \text{Uniform on } \{0, 1, 2, 3\}\) (Prob
    = 1/4).
  \end{itemize}
\end{itemize}

Suppose we observe \(x=2\). The calculation of the posterior
probabilities is summarized in the table below:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2083}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2083}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2083}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2083}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\theta=1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\theta=2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\theta=3\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Prior} \(\pi(\theta)\) & \(1/3\) & \(1/3\) & \(1/3\) & \(1\) \\
\textbf{Likelihood} \(\pi(x=2|\theta)\) & \(0\) & \(1/3\) & \(1/4\) &
- \\
\textbf{Product} \(\pi(\theta)\pi(x|\theta)\) & \(0\) & \(1/9\) &
\(1/12\) & \(7/36\) \\
\textbf{Posterior} \(\pi(\theta|x)\) & \(0\) & \(4/7\) & \(3/7\) &
\(1\) \\
\end{longtable}

The marginal sum (evidence) is calculated as
\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\). The posterior values are
obtained by dividing the product row by this sum.

\end{example}

\begin{example}[Normal with Unknown Mean and
Variance]\protect\hypertarget{exm-Normal-with-Unknown-Mean-and-Variance}{}\label{exm-Normal-with-Unknown-Mean-and-Variance}

Consider \(X_1, \dots, X_n \sim N(\mu, 1/\tau)\), where both \(\mu\) and
the precision \(\tau\) are unknown.

We use a \textbf{Normal-Gamma} conjugate prior:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\tau \sim \text{Gamma}(\alpha, \beta)\)
  \[\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}\]
\item
  \(\mu|\tau \sim N(\nu, 1/(k\tau))\)
  \[\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}\]
\end{enumerate}

The joint prior is the product of the conditional and the marginal: \[
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
\]

\textbf{Derivation of the Posterior:}

First, we write the likelihood in terms of the sufficient statistics
\(\bar{x}\) and \(S_{xx} = \sum (x_i - \bar{x})^2\): \[
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
\]

Multiplying the prior by the likelihood gives the joint posterior: \[
\begin{aligned}
\pi(\mu, \tau | x) &\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
\]

Next, we complete the square for the terms involving \(\mu\) inside the
brackets. It can be shown that: \[
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
\]

Substituting this back into the joint density and grouping terms that do
not depend on \(\mu\): \[
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
\]

\textbf{Results:}

By inspecting the factored equation above, we identify the updated
parameters:

\begin{itemize}
\item
  \textbf{Marginal Posterior of \(\tau\):} The first part corresponds to
  a Gamma kernel \(\tau^{\alpha' - 1} e^{-\beta'\tau}\).
  \[\tau|x \sim \text{Gamma}(\alpha', \beta')\] where
  \(\alpha' = \alpha + n/2\) and
  \(\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2\).
\item
  \textbf{Conditional Posterior of \(\mu\):} The second part corresponds
  to a Normal kernel with precision \(k'\tau\).
  \[\mu|\tau, x \sim N(\nu', 1/(k'\tau))\] where \(k' = k + n\) and
  \(\nu' = \frac{k\nu + n\bar{x}}{k+n}\).
\end{itemize}

\end{example}

\section{Bayes Rules}\label{bayes-rules}

The general form of Bayes rule is derived by minimizing risk.

\begin{definition}[Risk Function and Bayes
Risk]\protect\hypertarget{def-risk}{}\label{def-risk}

~

\begin{itemize}
\tightlist
\item
  \textbf{Risk Function:}
  \(R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx\)
\item
  \textbf{Bayes Risk:} The expected risk with respect to the prior.
  \[r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta\]
\end{itemize}

\end{definition}

\begin{theorem}[Minimization of Bayes
Risk]\protect\hypertarget{thm-bayes-rule-minimization}{}\label{thm-bayes-rule-minimization}

Minimizing the Bayes risk \(r(\pi, d)\) is equivalent to minimizing the
posterior expected loss for each observed \(x\). That is, the Bayes rule
\(d(x)\) satisfies: \[
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
\]

\end{theorem}

\begin{proof}
We start by writing the Bayes risk essentially as a double integral over
the parameters and the data. Substituting the definition of the risk
function \(R(\theta, d)\):

\[
\begin{aligned}
r(\pi, d) &= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
\]

Assuming the conditions for Fubini's Theorem are met, we switch the
order of integration:

\[
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
\]

Recall that the joint density can be factored as
\(f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)\), where
\(m(x)\) is the marginal density of the data. Substituting this into the
inner integral:

\[
\begin{aligned}
r(\pi, d) &= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
\]

Since the marginal density \(m(x)\) is non-negative, minimizing the
total integral \(r(\pi, d)\) with respect to the decision rule
\(d(\cdot)\) is equivalent to minimizing the term inside the brackets
for every \(x\) (specifically where \(m(x) > 0\)).

The term inside the brackets is the \textbf{Posterior Expected Loss}:

\[
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
\]
\end{proof}

\begin{tcolorbox}[enhanced jigsaw, toptitle=1mm, colback=white, breakable, toprule=.15mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, coltitle=black, bottomtitle=1mm, colframe=quarto-callout-important-color-frame, opacityback=0, titlerule=0mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, arc=.35mm, rightrule=.15mm, opacitybacktitle=0.6]

Therefore, to minimize the Bayes risk, one just need to choose \(d(x)\)
to minimize the posterior expected loss for each \(x\).

\end{tcolorbox}

The following diagram summarizes the general workflow for deriving a
Bayes estimator:

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-bayes-workflow-1.pdf}}

}

\caption{\label{fig-bayes-workflow}Workflow for Finding the Bayes Rule}

\end{figure}%

\subsection{Common Loss Functions and Bayes
Estimators}\label{common-loss-functions-and-bayes-estimators}

\subsubsection{Squared Error Loss (point
Estimate)}\label{squared-error-loss-point-estimate}

\[L(\theta, a) = (\theta - a)^2\]

To find the optimal estimator \(d(x)\), we minimize the posterior
expected loss \(E_{\theta|x}[(\theta - d(x))^2]\). Taking the derivative
with respect to \(d\) and setting it to 0:

\[-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)\]

\textbf{Result:} The Bayes rule under squared error loss is the
\textbf{posterior mean}.

\subsubsection{Absolute Error Loss}\label{absolute-error-loss}

\[L(\theta, d) = |\theta - d|\]

To find the Bayes rule, we minimize the posterior expected loss:

\[
\psi(d) = E_{\theta|x} [ |\theta - d| ] = \int_{-\infty}^{\infty} |\theta - d| \, dF(\theta|x)
\]

where \(F(\theta|x)\) is the cumulative distribution function (CDF) of
the posterior. Splitting the integral at the decision point \(d\):

\[
\psi(d) = \int_{-\infty}^{d} (d - \theta) \, dF(\theta|x) + \int_{d}^{\infty} (\theta - d) \, dF(\theta|x)
\]

We find the minimum by analyzing the rate of change of \(\psi(d)\) with
respect to \(d\). Differentiating (or taking the subgradient for
non-differentiable points):

\[
\frac{d}{dd} \psi(d) = \int_{-\infty}^{d} 1 \, dF(\theta|x) - \int_{d}^{\infty} 1 \, dF(\theta|x) = P(\theta \le d|x) - P(\theta > d|x)
\]

Setting this derivative to zero implies we seek a point where the
probability mass to the left equals the probability mass to the right:

\[
P(\theta \le d|x) = P(\theta > d|x)
\]

Since the total probability is 1, this condition simplifies to finding
\(d\) such that the cumulative probability is \(1/2\).

\textbf{General Case (Discrete or Mixed Distributions)}

In cases where the posterior distribution is discrete or has jump
discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific
value), an exact solution to \(F(d) = 0.5\) may not exist. To
generalize, the Bayes rule is defined as any \textbf{median} \(m\) of
the posterior distribution.

A median is formally defined as any value \(m\) that satisfies the
following two conditions simultaneously:

\begin{itemize}
\tightlist
\item
  \(P(\theta \le m|x) \ge \frac{1}{2}\)
\item
  \(P(\theta \ge m|x) \ge \frac{1}{2}\)
\end{itemize}

\textbf{Result:} The Bayes rule under absolute error loss is the
\textbf{posterior median}.

\subsubsection{Hypothesis Testing (0-1
Loss)}\label{hypothesis-testing-0-1-loss}

Consider the hypothesis test \(H_0: \theta \in \Theta_0\) versus
\(H_1: \theta \in \Theta_1\). We define the decision space as
\(\mathcal{A} = \{0, 1\}\), where \(a=0\) means accepting \(H_0\) and
\(a=1\) means rejecting \(H_0\) (accepting \(H_1\)).

\textbf{Case 1: 0-1 Loss}

The standard 0-1 loss function assigns a penalty of 1 for an incorrect
decision and 0 for a correct one:
\[L(\theta, a) = \begin{cases} 0 & \text{if } \theta \in \Theta_0, a=0 \ (\text{Correct } H_0) \\ 1 & \text{if } \theta \in \Theta_0, a=1 \ (\text{Type I Error}) \\ 1 & \text{if } \theta \in \Theta_1, a=0 \ (\text{Type II Error}) \\ 0 & \text{if } \theta \in \Theta_1, a=1 \ (\text{Correct } H_1) \end{cases}\]

To find the Bayes rule, we minimize the \textbf{posterior expected loss}
for a given \(x\), denoted as \(E_{\theta|x}[L(\theta, a)]\).

\begin{itemize}
\item
  \textbf{Expected Loss for choosing \(a=0\) (Accept \(H_0\)):} \[
    E_{\theta|x}[L(\theta, 0)] = 0 \cdot P(\theta \in \Theta_0|x) + 1 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_1|x)
    \]
\item
  \textbf{Expected Loss for choosing \(a=1\) (Reject \(H_0\)):} \[
    E_{\theta|x}[L(\theta, 1)] = 1 \cdot P(\theta \in \Theta_0|x) + 0 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_0|x)
    \]
\end{itemize}

The Bayes rule selects the action with the smaller expected loss. Thus,
we choose \(a=1\) if: \[
P(\theta \in \Theta_0|x) \le P(\theta \in \Theta_1|x)
\] This confirms that under 0-1 loss, the Bayes rule simply selects the
hypothesis with the higher posterior probability.

\textbf{Case 2: General Loss (Asymmetric Costs)}

In many practical applications, the cost of errors is not symmetric. For
example, a Type I error (false rejection) might be more costly than a
Type II error. Let \(c_1\) be the cost of a Type I error and \(c_2\) be
the cost of a Type II error. Usually, we normalize one cost to 1.

Suppose the loss function is:
\[L(\theta, a) = \begin{cases} 0 & \text{if } \theta \in \Theta_0, a=0 \\ c & \text{if } \theta \in \Theta_0, a=1 \ (\text{Cost of Type I Error}) \\ 1 & \text{if } \theta \in \Theta_1, a=0 \ (\text{Cost of Type II Error}) \\ 0 & \text{if } \theta \in \Theta_1, a=1 \end{cases}\]

We again calculate the posterior expected loss:

\begin{itemize}
\item
  \textbf{Expected Loss for \(a=0\):}
  \[E[L(\theta, 0)|x] = 0 \cdot P(\Theta_0|x) + 1 \cdot P(\Theta_1|x) = P(\Theta_1|x)\]
\item
  \textbf{Expected Loss for \(a=1\):}
  \[E[L(\theta, 1)|x] = c \cdot P(\Theta_0|x) + 0 \cdot P(\Theta_1|x) = c P(\Theta_0|x)\]
\end{itemize}

We reject \(H_0\) (\(a=1\)) if the expected loss of doing so is lower:
\[
c P(\Theta_0|x) \le P(\Theta_1|x)
\]

Since \(P(\Theta_1|x) = 1 - P(\Theta_0|x)\), we can rewrite this
condition as: \[
c P(\Theta_0|x) \le 1 - P(\Theta_0|x) \implies (1+c) P(\Theta_0|x) \le 1
\] \[
P(\Theta_0|x) \le \frac{1}{1+c}
\]

\textbf{Result:} With asymmetric costs, we accept \(H_1\) only if the
posterior probability of the null hypothesis is sufficiently small
(below the threshold \(\frac{1}{1+c}\)). If the cost of false rejection
\(c\) is high, we require stronger evidence against \(H_0\).

\subsubsection{Classification Prediction (categorical
Parameter)}\label{classification-prediction-categorical-parameter}

In classification problems, the parameter of interest is a discrete
class label \(\theta\) (often denoted as \(y\)) taking values in a set
of categories \(\{1, 2, \dots, K\}\). The goal is to predict the true
class label based on observed features \(x\).

We typically employ the \textbf{0-1 loss function}, which assigns a
penalty of 1 for a misclassification and 0 for a correct prediction:

\[L(\theta, \hat{\theta}) = \begin{cases} 0 & \text{if } \hat{\theta} = \theta \ (\text{Correct Classification}) \\ 1 & \text{if } \hat{\theta} \neq \theta \ (\text{Misclassification}) \end{cases}\]

To find the optimal classification rule (the Bayes Classifier), we
minimize the posterior expected loss, which is equivalent to minimizing
the probability of misclassification.

\[
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta} L(\theta, \hat{\theta}) \pi(\theta|x)
\]

Since the loss is 1 only when the predicted class \(\hat{\theta}\)
differs from the true class \(\theta\), this sum simplifies to:

\[
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta \neq \hat{\theta}} 1 \cdot \pi(\theta|x) = P(\theta \neq \hat{\theta} | x) = 1 - P(\theta = \hat{\theta} | x)
\]

Minimizing the misclassification rate
\(1 - P(\theta = \hat{\theta} | x)\) is mathematically equivalent to
maximizing the probability of being correct,
\(P(\theta = \hat{\theta} | x)\).

\textbf{Result:} The Bayes rule for classification is to predict the
class with the highest posterior probability. While this is technically
the \textbf{Maximum A Posteriori (MAP)} estimator, in the context of
machine learning and pattern recognition, this decision rule is known as
the \textbf{Bayes Optimal Classifier}.

\[
\hat{\theta}_{\text{Bayes}}(x) = \underset{k \in \{1, \dots, K\}}{\arg\max} \ P(\theta = k | x)
\]

\subsubsection{Interval Estimation and Highest Posterior Density
(HPD)}\label{interval-estimation-and-highest-posterior-density-hpd}

We can motivate the choice of a Credible Interval by defining a specific
loss function for interval estimation. Suppose we seek an estimate \(d\)
and specify a tolerance \(\delta > 0\). We define the loss function as:

\[
L(\theta, d) = \begin{cases} 
0 & \text{if } |\theta - d| \le \delta \\
1 & \text{if } |\theta - d| > \delta
\end{cases}
\]

The \textbf{Expected Posterior Loss} is the posterior probability that
\(\theta\) lies outside the interval \((d - \delta, d + \delta)\).
Therefore, minimizing the loss is equivalent to finding the interval of
fixed length \(2\delta\) that \textbf{maximizes the posterior
probability}:

\[ P(d - \delta \le \theta \le d + \delta \mid x) \]

In practice, we typically reverse this formulation: instead of fixing
the length \(2\delta\), we fix the coverage probability \(1-\alpha\)
(e.g., 0.95) and seek the interval with the \textbf{shortest possible
length}. This results in the \textbf{Highest Posterior Density (HPD)}
interval, defined as the region where the posterior density exceeds a
certain threshold \(c\):

\[ C_{HPD} = \{ \theta : \pi(\theta \mid x) \ge c \} \]

This HPD interval is optimal because it includes the ``most likely''
values of \(\theta\) and, for a unimodal distribution, provides the
narrowest interval for a given confidence level.

\textbf{Comparison with Equal-Tailed Intervals:}

\begin{itemize}
\tightlist
\item
  \textbf{Equal-Tailed Interval:} We simply cut off \(\alpha/2\)
  probability from each tail of the distribution. This is easy to
  compute but may not be the shortest interval if the distribution is
  skewed.
\item
  \textbf{HPD Interval:} This is the shortest possible interval for the
  given coverage. For unimodal distributions, the probability density at
  the two endpoints of the HPD interval is identical.
\end{itemize}

The plot below illustrates a skewed posterior distribution (Gamma).
Notice how the \textbf{HPD Interval (Blue)} is shifted toward the mode
(the peak) to capture the highest density values, resulting in a shorter
interval length compared to the \textbf{Equal-Tailed Interval (Red)}.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)}
\NormalTok{x\_vals }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{y\_vals }\OtherTok{\textless{}{-}} \FunctionTok{dgamma}\NormalTok{(x\_vals, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{, }\AttributeTok{rate =} \FloatTok{0.5}\NormalTok{)}

\DocumentationTok{\#\# Target Coverage}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.10}
\NormalTok{target\_prob }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha}

\DocumentationTok{\#\# 1. Equal{-}tailed Interval (quantiles)}
\NormalTok{eq\_lower }\OtherTok{\textless{}{-}} \FunctionTok{qgamma}\NormalTok{(alpha}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{, }\AttributeTok{rate =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{eq\_upper }\OtherTok{\textless{}{-}} \FunctionTok{qgamma}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{, }\AttributeTok{rate =} \FloatTok{0.5}\NormalTok{)}

\DocumentationTok{\#\# 2. HPD Interval (density Threshold Optimization)}
\DocumentationTok{\#\# We Look for a Density Threshold K Such That the Area Above K Is 0.90}
\NormalTok{find\_hpd }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dist\_vals, density\_vals, probability) \{}
  \DocumentationTok{\#\# Sort density values}
\NormalTok{  ord }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(density\_vals, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  sorted\_dens }\OtherTok{\textless{}{-}}\NormalTok{ density\_vals[ord]}
\NormalTok{  sorted\_dist }\OtherTok{\textless{}{-}}\NormalTok{ dist\_vals[ord]}
  
  \DocumentationTok{\#\# Accumulate probability (approximation)}
\NormalTok{  dx }\OtherTok{\textless{}{-}} \FunctionTok{diff}\NormalTok{(dist\_vals)[}\DecValTok{1}\NormalTok{]}
\NormalTok{  cum\_prob }\OtherTok{\textless{}{-}} \FunctionTok{cumsum}\NormalTok{(sorted\_dens }\SpecialCharTok{*}\NormalTok{ dx)}
  
  \DocumentationTok{\#\# Find cutoff index}
\NormalTok{  cutoff\_idx }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(cum\_prob }\SpecialCharTok{\textgreater{}=}\NormalTok{ probability)[}\DecValTok{1}\NormalTok{]}
  
  \DocumentationTok{\#\# Get the subset of x values}
\NormalTok{  hpd\_set }\OtherTok{\textless{}{-}}\NormalTok{ sorted\_dist[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{cutoff\_idx]}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(hpd\_set), }\FunctionTok{max}\NormalTok{(hpd\_set)))}
\NormalTok{\}}

\NormalTok{hpd\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{find\_hpd}\NormalTok{(x\_vals, y\_vals, target\_prob)}
\NormalTok{hpd\_lower }\OtherTok{\textless{}{-}}\NormalTok{ hpd\_bounds[}\DecValTok{1}\NormalTok{]}
\NormalTok{hpd\_upper }\OtherTok{\textless{}{-}}\NormalTok{ hpd\_bounds[}\DecValTok{2}\NormalTok{]}

\DocumentationTok{\#\# Plotting}
\FunctionTok{plot}\NormalTok{(x\_vals, y\_vals, }\AttributeTok{type =} \StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"90\% Credible Intervals (Skewed Posterior)"}\NormalTok{,}
     \AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(theta), }\AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{max}\NormalTok{(y\_vals) }\SpecialCharTok{*} \FloatTok{1.2}\NormalTok{))}

\DocumentationTok{\#\# Shade HPD}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(x\_vals[x\_vals }\SpecialCharTok{\textgreater{}=}\NormalTok{ hpd\_lower }\SpecialCharTok{\&}\NormalTok{ x\_vals }\SpecialCharTok{\textless{}=}\NormalTok{ hpd\_upper], hpd\_upper, hpd\_lower),}
        \FunctionTok{c}\NormalTok{(y\_vals[x\_vals }\SpecialCharTok{\textgreater{}=}\NormalTok{ hpd\_lower }\SpecialCharTok{\&}\NormalTok{ x\_vals }\SpecialCharTok{\textless{}=}\NormalTok{ hpd\_upper], }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
        \AttributeTok{col =} \FunctionTok{rgb}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}

\DocumentationTok{\#\# Draw Equal{-}tailed Lines (red)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{c}\NormalTok{(eq\_lower, eq\_upper), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# Draw HPD Lines (blue)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{c}\NormalTok{(hpd\_lower, hpd\_upper), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Posterior Density"}\NormalTok{, }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Equal{-}Tailed (Len: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(eq\_upper }\SpecialCharTok{{-}}\NormalTok{ eq\_lower, }\DecValTok{2}\NormalTok{), }\StringTok{")"}\NormalTok{), }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"HPD (Len: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(hpd\_upper }\SpecialCharTok{{-}}\NormalTok{ hpd\_lower, }\DecValTok{2}\NormalTok{), }\StringTok{")"}\NormalTok{)),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}
       \AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\FunctionTok{rgb}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{)), }\AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-hpd-vs-equal-1.pdf}}

}

\caption{\label{fig-hpd-vs-equal}Comparison of HPD and Equal-Tailed
Intervals for a Skewed Distribution}

\end{figure}%

\subsection{Finding Minimax Rules with Bayes
Rules}\label{finding-minimax-rules-with-bayes-rules}

Theorem~\ref{thm-minimax-constant} states that if a Bayes estimator
\(\delta^\pi\) (derived from a prior \(\pi\)) yields a constant risk
\(R(\theta, \delta^\pi) = c\) across the entire parameter space
\(\Theta\), then that estimator is necessarily minimax.

This result is a cornerstone of decision theory because it provides a
sufficient condition for minimaxity. While the minimax criterion focuses
on the ``worst-case scenario'' by minimizing the maximum possible risk,
the Bayes criterion focuses on the ``average-case scenario'' relative to
a prior. When the risk is constant, these two perspectives align: the
average risk equals the maximum risk, and no other estimator can achieve
a lower maximum without also having a lower Bayes risk, which would
contradict the optimality of the Bayes rule.

In practice, this means that to find a minimax estimator, one can often
search for a ``least favorable prior'' that results in a Bayes estimator
with a flat risk profile.

\begin{example}[Binomial Minimax
Estimator]\protect\hypertarget{exm-binomial-minimax}{}\label{exm-binomial-minimax}

Let \(X \sim \text{Bin}(n, \theta)\) and
\(\theta \sim \text{Beta}(a, b)\). The squared error loss is
\(L(\theta, d) = (\theta - d)^2\). The Bayes estimator is the posterior
mean: \[d(x) = \frac{a+x}{a+b+n}\]

We calculate the risk \(R(\theta, d)\):

\[
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
\]

Let \(c = a+b+n\).
\[R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]\]

Using the bias-variance decomposition and knowing \(E(x) = n\theta\) and
\(E(x^2) = (n\theta)^2 + n\theta(1-\theta)\), we expand the risk
function. To make the risk constant (independent of \(\theta\)), we set
the coefficients of \(\theta\) and \(\theta^2\) to zero.

Solving the resulting system of equations yields:
\[a = b = \frac{\sqrt{n}}{2}\]

Thus, the minimax estimator is:
\[d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}\]

This differs from the standard MLE \(\hat{p} = x/n\) and the uniform
prior Bayes estimator (\(a=b=1\)).

\end{example}

\section{Stein's Paradox and the James-stein
Estimator}\label{steins-paradox-and-the-james-stein-estimator}

In high-dimensional estimation (\(p \ge 3\)), the Maximum Likelihood
Estimator (MLE) is inadmissible under squared error loss. The
\textbf{James-Stein Estimator} dominates the MLE, meaning it achieves
lower risk for all values of \(\theta\).

Consider the setting:

\begin{itemize}
\tightlist
\item
  Data: \(X \sim N_p(\theta, I)\)
\item
  Prior: \(\theta \sim N_p(0, \tau^2 I)\)
\item
  Estimator: \(d^{JS}(x) = \left( 1 - \frac{p-2}{||x||^2} \right) x\)
\end{itemize}

We can derive the Bayes Risk \(r(\pi, d^{JS})\) of this estimator using
two equivalent methods: minimizing the expected frequentist risk, or
minimizing the expected posterior loss.

\begin{theorem}[Bayes Risk of James-stein
Estimator]\protect\hypertarget{thm-js-bayes-risk}{}\label{thm-js-bayes-risk}

For \(p \ge 3\), the Bayes risk of the James-Stein estimator \(d^{JS}\)
with respect to the prior \(\theta \sim N(0, \tau^2 I)\) is:

\[
r(\pi, d^{JS}) = \frac{p\tau^2 + 2}{\tau^2 + 1}
\]

\end{theorem}

\begin{proof}
\textbf{Method 1: Integration over the Prior (Frequentist Risk
approach)}

The Bayes risk is defined as \(r(\pi, d) = E_\pi [ R(\theta, d) ]\).

First, recall the frequentist risk of the James-Stein estimator for a
fixed \(\theta\). Using Stein's Lemma, the risk is given by: \[
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
\]

To find the Bayes risk, we take the expectation of this risk with
respect to the prior \(\pi(\theta)\): \[
r(\pi, d^{JS}) = \int R(\theta, d^{JS}) \pi(\theta) d\theta = p - (p-2)^2 E_\pi \left[ E_\theta \left( \frac{1}{||X||^2} \right) \right]
\]

By the law of iterated expectations, \(E_\pi [ E_\theta (\cdot) ]\) is
equivalent to the expectation with respect to the marginal distribution
of \(X\), denoted as \(m(x)\). Under the conjugate prior, the marginal
distribution is \(X \sim N(0, (1+\tau^2)I)\).

Consequently, the quantity \(\frac{||X||^2}{1+\tau^2}\) follows a
Chi-squared distribution with \(p\) degrees of freedom (\(\chi^2_p\)).
The expectation of the inverse chi-square is: \[
E \left[ \frac{1}{||X||^2} \right] = \frac{1}{1+\tau^2} E \left[ \frac{1}{\chi^2_p} \right] = \frac{1}{1+\tau^2} \cdot \frac{1}{p-2}
\]

Substituting this back into the risk equation: \[
\begin{aligned}
r(\pi, d^{JS}) &= p - (p-2)^2 \cdot \frac{1}{(p-2)(1+\tau^2)} \\
&= p - \frac{p-2}{1+\tau^2} \\
&= \frac{p(1+\tau^2) - (p-2)}{1+\tau^2} \\
&= \frac{p\tau^2 + p - p + 2}{1+\tau^2} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\end{aligned}
\]
\end{proof}

\begin{proof}
\textbf{Method 2: Integration over the Marginal (Posterior Loss
approach)}

Alternatively, we can compute the Bayes risk by first finding the
posterior expected loss for a given \(x\), and then averaging over the
marginal distribution of \(x\): \[
r(\pi, d) = E_m [ E_{\theta|x} [ L(\theta, d(x)) ] ]
\]

\textbf{Step 1: Posterior Expected Loss}

The posterior distribution of \(\theta\) given \(x\) is: \[
\theta | x \sim N \left( \frac{\tau^2}{1+\tau^2}x, \frac{\tau^2}{1+\tau^2}I \right)
\]

The expected squared error loss can be decomposed into the variance
(trace) and the squared bias: \[
E_{\theta|x} [ ||\theta - d^{JS}(x)||^2 ] = \text{tr}(\text{Var}(\theta|x)) + || E[\theta|x] - d^{JS}(x) ||^2
\]

\begin{itemize}
\item
  \textbf{Trace term:}
  \[\text{tr} \left( \frac{\tau^2}{1+\tau^2} I_p \right) = \frac{p\tau^2}{1+\tau^2}\]
\item
  \textbf{Squared Bias term:} Let \(B = \frac{1}{1+\tau^2}\). Then
  \(E[\theta|x] = (1-B)x\). The estimator is
  \(d^{JS}(x) = (1 - \frac{p-2}{||x||^2})x\). The difference is: \[
    E[\theta|x] - d^{JS}(x) = \left( (1-B) - \left( 1 - \frac{p-2}{||x||^2} \right) \right) x = \left( \frac{p-2}{||x||^2} - B \right) x
    \] Squaring the norm gives: \[
    \left( \frac{p-2}{||x||^2} - B \right)^2 ||x||^2 = \frac{(p-2)^2}{||x||^2} - 2B(p-2) + B^2 ||x||^2
    \]
\end{itemize}

\textbf{Step 2: Expectation with respect to Marginal \(X\)}

We now take the expectation \(E_m[\cdot]\) of the posterior loss. Recall
\(X \sim N(0, (1+\tau^2)I)\), so \(E[||X||^2] = p(1+\tau^2)\) and
\(E[1/||X||^2] = \frac{1}{(p-2)(1+\tau^2)}\).

\begin{itemize}
\tightlist
\item
  \textbf{Expectation of Trace term:} Constant, remains
  \(\frac{p\tau^2}{1+\tau^2}\).
\item
  \textbf{Expectation of Bias term:} \[
    \begin{aligned}
    E_m \left[ \frac{(p-2)^2}{||X||^2} - \frac{2(p-2)}{1+\tau^2} + \frac{||X||^2}{(1+\tau^2)^2} \right] &= (p-2)^2 \frac{1}{(p-2)(1+\tau^2)} - \frac{2(p-2)}{1+\tau^2} + \frac{p(1+\tau^2)}{(1+\tau^2)^2} \\
    &= \frac{p-2}{1+\tau^2} - \frac{2p-4}{1+\tau^2} + \frac{p}{1+\tau^2} \\
    &= \frac{p - 2 - 2p + 4 + p}{1+\tau^2} \\
    &= \frac{2}{1+\tau^2}
    \end{aligned}
    \]
\end{itemize}

\textbf{Step 3: Combine Terms}

\[
r(\pi, d^{JS}) = \underbrace{\frac{p\tau^2}{1+\tau^2}}_{\text{Variance Part}} + \underbrace{\frac{2}{1+\tau^2}}_{\text{Bias Part}} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\]

Both methods yield the same result.
\end{proof}

\begin{theorem}[Inadmissibility of the MLE in High Dimensions (stein's
Phenomenon)]\protect\hypertarget{thm-mle-inadmissible}{}\label{thm-mle-inadmissible}

Let \(X \sim N_p(\theta, I)\) be a \(p\)-dimensional random vector with
\(p \ge 3\). Under the squared error loss function
\(L(\theta, d) = ||\theta - d||^2\), the standard Maximum Likelihood
Estimator \(d^0(X) = X\) is \textbf{inadmissible}.

\end{theorem}

\begin{proof}
To show that \(d^0(X) = X\) is inadmissible, we must find another
estimator that dominates it (i.e., has equal or lower risk for all
\(\theta\), and strictly lower risk for at least one \(\theta\)).

First, consider the risk of the standard estimator \(d^0\). Since
\(X_i \sim N(\theta_i, 1)\) are independent:

\[
R(\theta, d^0) = E_\theta [ ||X - \theta||^2 ] = \sum_{i=1}^p E [ (X_i - \theta_i)^2 ] = \sum_{i=1}^p \text{Var}(X_i) = p
\]

Now consider the James-Stein estimator
\(d^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X\). As established
in the derivation of the Bayes Risk in Theorem~\ref{thm-js-bayes-risk},
the frequentist risk function of \(d^{JS}\) is:

\[
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
\]

Since the random variable \(||X||^2\) is non-negative and not
identically infinity, the expectation \(E_\theta [ 1/||X||^2 ]\) is
strictly positive for all \(\theta\). Therefore:

\[
R(\theta, d^{JS}) < p = R(\theta, d^0) \quad \text{for all } \theta \in \mathbb{R}^p
\]

Because \(d^{JS}\) achieves a strictly lower risk than \(d^0\)
everywhere in the parameter space, \(d^0\) is dominated by \(d^{JS}\)
and is thus inadmissible.
\end{proof}

\subsection{Practical Application: One-way ANOVA and ``borrowing
Strength''}\label{practical-application-one-way-anova-and-borrowing-strength}

\begin{example}[]\protect\hypertarget{exm-anova-js}{}\label{exm-anova-js}

Consider a One-Way ANOVA setting where we wish to estimate the means of
\(p\) different independent groups (e.g., the true batting averages of
\(p=10\) baseball players, or the efficacy of \(p=5\) different hospital
treatments).

\begin{itemize}
\tightlist
\item
  \textbf{Model:} Let \(X_i \sim N(\theta_i, \sigma^2)\) be the observed
  sample mean for group \(i\), for \(i = 1, \dots, p\).
\item
  \textbf{Goal:} Estimate the vector of true means
  \(\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)\) simultaneously.
  The loss is the sum of squared errors:
  \(L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}) = \sum (\theta_i - \hat{\theta}_i)^2\).
\end{itemize}

\textbf{The MLE Approach (Total Separation):} The standard estimator is
\(\hat{\theta}_i^{\text{MLE}} = X_i\). This estimates each group
entirely independently, using only data from that specific group. If a
specific player has a lucky streak, their estimate is very high; if they
are unlucky, it is very low.

\textbf{The James-Stein Approach (Shrinkage / Pooling):} In this
context, the James-Stein estimator (specifically the variation shrinking
toward the grand mean \(\bar{X}\)) is: \[
\hat{\theta}_i^{JS} = \bar{X} + \left( 1 - \frac{(p-3)\sigma^2}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X})
\]

\textbf{Why is this better?} Even though the groups might be physically
independent (e.g., distinct hospitals), the James-Stein estimator
\textbf{``borrows strength''} from the ensemble.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Noise Reduction:} Extreme observations \(X_i\) are likely to
  contain more positive noise than signal. Shrinking them toward the
  global average \(\bar{X}\) reduces this variance.
\item
  \textbf{Stein's Paradox:} While \(\hat{\theta}_i^{JS}\) introduces
  bias (estimates are pulled toward the center), the reduction in
  variance is so significant that the \textbf{Total Risk} (sum of
  squared errors over all groups) is strictly lower than that of the
  MLE, provided \(p \ge 3\).
\end{enumerate}

Thus, estimating the groups \emph{together} yields a more accurate
global picture than estimating them \emph{separately}, even if the
groups are independent.

\end{example}

\subsection{Why is this Paradoxical?}\label{why-is-this-paradoxical}

The result that \(d^{JS}\) dominates \(d^0\) is called \textbf{Stein's
Paradox} because it defies intuition in several ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Independence Irrelevance:} The result holds even if the
  components \(X_i\) are completely unrelated (e.g., \(X_1\) is the
  price of tea in China, \(X_2\) is the temperature in Saskatoon, and
  \(X_3\) is the weight of a local cat). It seems absurd that combining
  unrelated data improves the estimate of each, but the combined risk is
  indeed lower.
\item
  \textbf{No ``Free Lunch'':} The James-Stein estimator does not improve
  every individual component \(\theta_i\) simultaneously for every
  realization. Instead, it minimizes the \textbf{total} risk
  \(\sum E(\hat{\theta}_i - \theta_i)^2\). It sacrifices accuracy on
  outliers (by biasing them) to gain significant stability on the bulk
  of the data.
\item
  \textbf{Destruction of Symmetry:} The MLE is invariant under
  translation and rotation. The James-Stein estimator breaks this
  symmetry by shrinking toward an arbitrary point (usually the origin or
  the grand mean), yet it yields a better objective performance.
\end{enumerate}

\subsection{What We Learned}\label{what-we-learned}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bias-Variance Tradeoff:} This is the most famous example where
  introducing \textbf{bias} (shrinkage) leads to a massive reduction in
  \textbf{variance}, thereby reducing the overall Mean Squared Error
  (MSE). Unbiasedness is not always a virtue in estimation.
\item
  \textbf{Inadmissibility in High Dimensions:} Intuitions formed in 1D
  or 2D (where MLE is admissible) fail in higher dimensions
  (\(p \ge 3\)). The volume of space grows so fast that ``standard''
  diffuse priors or MLEs become inefficient.
\item
  \textbf{Hierarchical Modeling:} Stein's result provides the
  theoretical foundation for \textbf{Hierarchical Bayesian Models}. When
  we assume parameters come from a common distribution (e.g.,
  \(\theta_i \sim N(\mu, \tau^2)\)), we naturally derive shrinkage
  estimators that ``borrow strength'' across groups, formalized as
  Empirical Bayes or fully Bayesian methods.
\end{enumerate}

\section{Empirical Bayes Rules}\label{empirical-bayes-rules}

The James-Stein estimator provides a natural entry point into the
concept of \textbf{Empirical Bayes (EB)}. While the Stein estimator was
originally derived using frequentist risk arguments, it can be
intuitively understood as a Bayesian estimator where the parameters of
the prior distribution are estimated from the data itself.

\subsection{The General Empirical Bayes
Framework}\label{the-general-empirical-bayes-framework}

In a standard Bayesian analysis, the hyperparameters of the prior are
fixed based on subjective belief or external information. In contrast,
Empirical Bayes uses the observed data to ``learn'' the prior.

The workflow typically follows these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hierarchical Model:} We assume the data \(X\) comes from a
  distribution \(f(x|\theta)\), and the parameter \(\theta\) comes from
  a prior \(\pi(\theta|\eta)\) controlled by hyperparameters \(\eta\).
\item
  \textbf{Marginal Likelihood (Evidence):} We integrate out the
  parameter \(\theta\) to obtain the marginal distribution of the data
  given the hyperparameters:
  \[m(x|\eta) = \int f(x|\theta) \pi(\theta|\eta) d\theta\]
\item
  \textbf{Estimation of Hyperparameters:} Instead of fixing \(\eta\), we
  estimate it by maximizing the marginal likelihood (Type-II Maximum
  Likelihood) or using method-of-moments:
  \[\hat{\eta} = \underset{\eta}{\arg\max} \ m(x|\eta)\]
\item
  \textbf{Posterior Inference:} We proceed with standard Bayesian
  inference, but we substitute the estimated estimate \(\hat{\eta}\)
  into the posterior:
  \[\pi(\theta|x, \hat{\eta}) \propto f(x|\theta) \pi(\theta|\hat{\eta})\]
\end{enumerate}

\textbf{Discussion:}

\begin{itemize}
\tightlist
\item
  \textbf{``Borrowing Strength'':} EB allows us to pool information
  across independent groups to estimate the common structure (the prior)
  governing them.
\item
  \textbf{The Critique:} A purist Bayesian might object that using the
  data twice (once to estimate the prior, once to estimate \(\theta\))
  underestimates the uncertainty. A fully Bayesian Hierarchical model
  would instead place a ``hyperprior'' on \(\eta\) and integrate it out.
\end{itemize}

\subsection{Deriving James-Stein as Empirical
Bayes}\label{deriving-james-stein-as-empirical-bayes}

We can derive the James-Stein rule explicitly using this framework.

\textbf{Model:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Likelihood: \(X_i | \mu_i \sim N(\mu_i, 1)\) for \(i=1, \dots, p\).
\item
  Prior: \(\mu_i \sim N(0, \tau^2)\). Here, \(\tau^2\) is the unknown
  hyperparameter.
\end{enumerate}

\textbf{Step 1: The Ideal Bayes Estimator}

If we knew \(\tau^2\), the posterior distribution of \(\mu_i\) would be
Normal with mean:
\[E(\mu_i|x_i, \tau^2) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i\]
We define the shrinkage factor \(B = \frac{1}{1+\tau^2}\).

\textbf{Step 2: Marginal Estimation}

Since \(\mu_i\) and \(X_i-\mu_i\) are independent normals, the marginal
distribution of the data is: \[X_i \sim N(0, 1+\tau^2)\] Consequently,
the sum of squares \(S = ||X||^2 = \sum X_i^2\) follows a scaled
Chi-squared distribution: \[S \sim (1+\tau^2) \chi^2_p\]

\textbf{Step 3: Estimating the Shrinkage Factor}

We need to estimate \(B = \frac{1}{1+\tau^2}\). Note that the expected
value of an inverse Chi-square variable is
\(E[1/\chi^2_p] = \frac{1}{p-2}\). Therefore:
\[E \left[ \frac{p-2}{S} \right] = \frac{p-2}{1+\tau^2} E\left[\frac{1}{\chi^2_p}\right] = \frac{p-2}{1+\tau^2} \cdot \frac{1}{p-2} = \frac{1}{1+\tau^2} = B\]

Thus, \(\hat{B} = \frac{p-2}{||X||^2}\) is an unbiased estimator of the
optimal shrinkage factor.

\textbf{Step 4: The Empirical Bayes Rule}

Plugging \(\hat{B}\) into the ideal Bayes estimator recovers the
James-Stein rule:
\[\delta^{EB}(X) = \left( 1 - \hat{B} \right) X = \left( 1 - \frac{p-2}{||X||^2} \right) X\]

\section{Hierarchical Modeling via
MCMC}\label{hierarchical-modeling-via-mcmc}

In complex Bayesian settings where the posterior distribution cannot be
derived analytically, we utilize hierarchical structures to represent
levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate
the resulting distributions.

\subsection{Hierarchical Model
Structure}\label{hierarchical-model-structure}

A hierarchical model decomposes a complex joint distribution into a
series of conditional levels. The general mathematical form is:

\[
\begin{aligned}
\text{Level 1 (Data Likelihood):} & \quad X_i | \mu_i, \sigma^2 \sim f(x_i | \mu_i, \sigma^2) \\
\text{Level 2 (Parameters):} & \quad \mu_i | \theta, \tau^2 \sim \pi(\mu_i | \theta, \tau^2) \\
\text{Level 3 (Hyperparameters):} & \quad \theta, \tau^2 \sim \pi(\theta, \tau^2)
\end{aligned}
\]

The goal is to compute the joint posterior distribution of all
unobserved parameters given the data \(X = \{X_1, \dots, X_n\}\):

\[
p(\boldsymbol{\mu}, \theta, \tau^2 | X) \propto \left[ \prod_{i=1}^n f(x_i | \mu_i, \sigma^2) \pi(\mu_i | \theta, \tau^2) \right] \pi(\theta, \tau^2)
\]

\subsection{Graphical Model Representation (Tree
Structure)}\label{graphical-model-representation-tree-structure}

The following tree diagram illustrates the conditional dependencies.
Note that the parameters \(\mu_i\) are conditionally independent given
the hyperparameter \(\theta\), which facilitates ``borrowing strength''
across groups.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-hierarchical-tree-1.pdf}}

}

\caption{\label{fig-hierarchical-tree}Hierarchical Tree Structure}

\end{figure}%

\subsection{MCMC Estimation}\label{mcmc-estimation}

In hierarchical models, the joint posterior distribution
\(p(\boldsymbol{\mu}, \theta | X)\) often lacks a closed-form analytical
solution due to the integration required for the normalizing constant.
We use \textbf{Markov Chain Monte Carlo (MCMC)} to draw sequence of
samples \(\{\boldsymbol{\mu}^{(t)}, \theta^{(t)}\}\) that converge to
the target posterior distribution.

\subsubsection{Gibbs Sampling Algorithm}\label{gibbs-sampling-algorithm}

\phantomsection\label{alg-gibbs-sampling}
Gibbs sampling is an algorithm for sampling from a multivariate
distribution by sequentially sampling from the \textbf{full conditional
distributions}. To sample from a target distribution
\(p(\theta_1, \theta_2, \dots, \theta_k)\), the algorithm iterates
through each variable, updating it conditioned on the current values of
all other variables:

\[
\begin{aligned}
\theta_1^{(t+1)} &\sim p(\theta_1 | \theta_2^{(t)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
\theta_2^{(t+1)} &\sim p(\theta_2 | \theta_1^{(t+1)}, \theta_3^{(t)}, \dots, \theta_k^{(t)}) \\
&\vdots \\
\theta_k^{(t+1)} &\sim p(\theta_k | \theta_1^{(t+1)}, \theta_2^{(t+1)}, \dots, \theta_{k-1}^{(t+1)})
\end{aligned}
\]

\begin{example}[Gibbs Sampling for Groups of Normal
Data]\protect\hypertarget{exm-hierarchical-gibbs}{}\label{exm-hierarchical-gibbs}

\textbf{The Model}

To apply the general Gibbs sampling framework
\(\theta_1, \theta_2, \dots, \theta_k\) to our specific hierarchical
model, we identify the variables as follows:

\begin{itemize}
\item
  \textbf{Data Observations (\(X_i\)):} These are the known, measured
  values at the lowest level of the hierarchy (e.g., test scores of
  students in school \(i\)). In the Gibbs sampler, these remain fixed
  and condition the updates of the parameters.
\item
  \textbf{Group-Level Parameters (\(\theta_1 = \mu_i\)):} These
  represent the latent means for each specific group or cluster. In the
  update step, \(\mu_i\) acts as the first block of variables. It is
  updated by ``compromising'' between the local data \(X_i\) and the
  global characteristic \(\theta\).
\item
  \textbf{Global Hyperparameter (\(\theta_2 = \theta\)):} This
  represents the common mean across all groups. It acts as the second
  block in the sampler. Its update depends on the current state of all
  \(\mu_i\) values, effectively ``pooling'' information from all groups
  to estimate the overall population center.
\end{itemize}

\textbf{Gibbs Update in Hierarchical Models}

In the hierarchical tree structure provided earlier, let our parameter
vector be \((\mu_i, \theta)\). The ``orthogonality'' of the updates
becomes clear when we derive the full conditionals for a Gaussian case:

\begin{itemize}
\item
  \textbf{Case \(\theta_1 = \mu_i\):} Sample \(\mu_i^{(t+1)}\) from
  \(p(\mu_i | X_i, \theta^{(t)})\). This is a normal distribution with:
  \[
  \mu_i^{(t+1)} \sim N\left( \frac{\tau^2 X_i + \sigma^2 \theta^{(t)}}{\sigma^2 + \tau^2}, \frac{\sigma^2 \tau^2}{\sigma^2 + \tau^2} \right)
  \]
\item
  \textbf{Case \(\theta_2 = \theta\):} Sample \(\theta^{(t+1)}\) from
  \(p(\theta | \boldsymbol{\mu}^{(t+1)})\). Assuming a flat prior
  \(\pi(\theta) \propto 1\): \[
  \theta^{(t+1)} \sim N\left( \frac{1}{n} \sum_{i=1}^n \mu_i^{(t+1)}, \frac{\tau^2}{n} \right)
  \]
\end{itemize}

\end{example}

\textbf{Visual Characteristic:} Gibbs sampling moves along the
coordinate axes because it updates one parameter at a time while holding
others constant.

\subsubsection{Metropolis-Hastings (MH)
Sampling}\label{metropolis-hastings-mh-sampling}

When the full conditional distributions are not easy to sample from, we
use the Metropolis-Hastings algorithm. At each step \(t\):

\begin{itemize}
\tightlist
\item
  \textbf{Propose:} Draw a candidate state \(\theta^*\) from a proposal
  distribution \(q(\theta^* | \theta^{(t)})\).
\item
  \textbf{Accept/Reject:} Calculate the acceptance probability: \[
  \alpha = \min \left( 1, \frac{p(\theta^* | X) q(\theta^{(t)} | \theta^*)}{p(\theta^{(t)} | X) q(\theta^* | \theta^{(t)})} \right)
  \]
\item
  Set \(\theta^{(t+1)} = \theta^*\) with probability \(\alpha\);
  otherwise, set \(\theta^{(t+1)} = \theta^{(t)}\).
\end{itemize}

\textbf{Visual Characteristic:} MH sampling moves in arbitrary
directions and can ``stay put'' if a proposal is rejected, exploring the
space via a random walk.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FloatTok{0.8}
\NormalTok{log\_target }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{ }\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ (x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{rho}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{*}\NormalTok{y }\SpecialCharTok{+}\NormalTok{ y}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) \}}

\CommentTok{\# Gibbs Path (Step{-}wise update)}
\NormalTok{gx }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{; gy }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2}
\NormalTok{gx\_path }\OtherTok{\textless{}{-}}\NormalTok{ gx; gy\_path }\OtherTok{\textless{}{-}}\NormalTok{ gy}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{25}\NormalTok{) \{}
\NormalTok{  gx }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, rho }\SpecialCharTok{*}\NormalTok{ gy, }\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{  gx\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(gx\_path, gx, gx); gy\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(gy\_path, gy, gy) }\CommentTok{\# Horizontal move}
\NormalTok{  gy }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, rho }\SpecialCharTok{*}\NormalTok{ gx, }\FunctionTok{sqrt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{  gx\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(gx\_path, gx); gy\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(gy\_path, gy) }\CommentTok{\# Vertical move}
\NormalTok{\}}

\CommentTok{\# MH Path (Random Walk)}
\NormalTok{mx }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{50}\NormalTok{); my }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\NormalTok{mx[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{; my[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{50}\NormalTok{) \{}
\NormalTok{  px }\OtherTok{\textless{}{-}}\NormalTok{ mx[i}\DecValTok{{-}1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{); py }\OtherTok{\textless{}{-}}\NormalTok{ my[i}\DecValTok{{-}1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
\NormalTok{  acc }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(}\FunctionTok{log\_target}\NormalTok{(px, py) }\SpecialCharTok{{-}} \FunctionTok{log\_target}\NormalTok{(mx[i}\DecValTok{{-}1}\NormalTok{], my[i}\DecValTok{{-}1}\NormalTok{]))}
  \ControlFlowTok{if}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textless{}}\NormalTok{ acc) \{ mx[i] }\OtherTok{\textless{}{-}}\NormalTok{ px; my[i] }\OtherTok{\textless{}{-}}\NormalTok{ py \} }\ControlFlowTok{else}\NormalTok{ \{ mx[i] }\OtherTok{\textless{}{-}}\NormalTok{ mx[i}\DecValTok{{-}1}\NormalTok{]; my[i] }\OtherTok{\textless{}{-}}\NormalTok{ my[i}\DecValTok{{-}1}\NormalTok{] \}}
\NormalTok{\}}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\NormalTok{t\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length=}\DecValTok{50}\NormalTok{); z }\OtherTok{\textless{}{-}} \FunctionTok{outer}\NormalTok{(t\_seq, t\_seq, }\ControlFlowTok{function}\NormalTok{(x,y) }\FunctionTok{exp}\NormalTok{(}\FunctionTok{log\_target}\NormalTok{(x,y)))}
\FunctionTok{plot}\NormalTok{(gx\_path, gy\_path, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Gibbs (Orthogonal Steps)"}\NormalTok{, }\AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(theta[}\DecValTok{1}\NormalTok{]), }\AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(theta[}\DecValTok{2}\NormalTok{]))}
\FunctionTok{contour}\NormalTok{(t\_seq, t\_seq, z, }\AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col=}\StringTok{"gray"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(mx, my, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Metropolis{-}Hastings (Random Walk)"}\NormalTok{, }\AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(theta[}\DecValTok{1}\NormalTok{]), }\AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(theta[}\DecValTok{2}\NormalTok{]))}
\FunctionTok{contour}\NormalTok{(t\_seq, t\_seq, z, }\AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col=}\StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-mcmc-comparison-1.pdf}}

}

\caption{\label{fig-mcmc-comparison}Comparison of Sampling Paths}

\end{figure}%

\section{Case Study: 1998 Major League Baseball Home Run
Race}\label{case-study-1998-major-league-baseball-home-run-race}

In 1998, the baseball world was captivated by Mark McGwire and Sammy
Sosa as they chased Roger Maris' 1961 record of 61 home runs in a single
season. While McGwire and Sosa finished with 70 and 66 home runs
respectively, we consider whether such performance could have been
predicted using pre-season exhibition data.

For a set of \(i = 1, \dots, 17\) players (including McGwire and Sosa),
we observe their batting records in pre-season exhibition matches. Our
goal is to estimate each player's home run ``strike rate'' for the
competitive season.

\subsection{Transforming Data}\label{transforming-data}

We utilize the pre-season home runs (\(y_i\)) and at-bats (\(n_i\)) for
17 players. The data is transformed using a variance-stabilizing
transformation to approximate a normal distribution with known variance
\(\sigma^2 = 1\).

\[
x_i = \sqrt{n_i} \arcsin\left( 2 \frac{y_i}{n_i} - 1 \right)
\]

The goal is to estimate the latent parameter \(\mu_i\) for each player
and compare it to the ``true'' regular season performance.

\subsection{\texorpdfstring{True Season Parameter (\(\mu_i\) or
\(p_i^{Season}\))}{True Season Parameter (\textbackslash mu\_i or p\_i\^{}\{Season\})}}\label{true-season-parameter-mu_i-or-p_iseason}

To validate our estimates, we define the ``true'' parameter value
\(\mu_i\) using the player's performance over the full competitive
season. Let \(Y_i\) be the total home runs and \(N_i\) be the total
at-bats in the regular season. The true transformed rate is calculated
as:

\[
\mu_i^{\text{season}} = \sqrt{n_i} \arcsin\left( 2 \frac{Y_i}{N_i} - 1 \right)
\]

Note that while we use the season-long probability (\(Y_i/N_i\)), we
scale it by the pre-season sample size (\(\sqrt{n_i}\)). This ensures
that \(\mu_i^{\text{season}}\) is on the same scale as our observations
\(x_i\), allowing for direct comparison of the estimation error.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(brms)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(tidyr)}

\CommentTok{\# 1. Input Raw Data}
\NormalTok{ni }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{58}\NormalTok{, }\DecValTok{59}\NormalTok{, }\DecValTok{74}\NormalTok{, }\DecValTok{84}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{63}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{53}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{38}\NormalTok{, }\DecValTok{58}\NormalTok{)}
\NormalTok{yi }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{Ni }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{509}\NormalTok{, }\DecValTok{643}\NormalTok{, }\DecValTok{633}\NormalTok{, }\DecValTok{645}\NormalTok{, }\DecValTok{606}\NormalTok{, }\DecValTok{555}\NormalTok{, }\DecValTok{619}\NormalTok{, }\DecValTok{609}\NormalTok{, }\DecValTok{552}\NormalTok{, }\DecValTok{540}\NormalTok{, }\DecValTok{561}\NormalTok{, }\DecValTok{440}\NormalTok{, }\DecValTok{585}\NormalTok{, }\DecValTok{531}\NormalTok{, }\DecValTok{454}\NormalTok{, }\DecValTok{504}\NormalTok{, }\DecValTok{244}\NormalTok{)}
\NormalTok{Yi }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{70}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{56}\NormalTok{, }\DecValTok{46}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{15}\NormalTok{)}

\CommentTok{\# 2. Calculate Derived Values}
\NormalTok{p\_pre   }\OtherTok{\textless{}{-}}\NormalTok{ yi }\SpecialCharTok{/}\NormalTok{ ni                        }\CommentTok{\# Pre{-}season Probability}
\NormalTok{x       }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(ni) }\SpecialCharTok{*} \FunctionTok{asin}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ p\_pre }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\CommentTok{\# Transformed Pre{-}season (x\_i)}

\NormalTok{p\_season }\OtherTok{\textless{}{-}}\NormalTok{ Yi }\SpecialCharTok{/}\NormalTok{ Ni                       }\CommentTok{\# Season Probability}
\NormalTok{true\_mu  }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(ni) }\SpecialCharTok{*} \FunctionTok{asin}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ p\_season }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\CommentTok{\# Transformed Season (mu\_i)}

\CommentTok{\# 3. Create Main Data Frame}
\NormalTok{baseball\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Player =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{,}
  \AttributeTok{Pre\_HR =}\NormalTok{ yi,}
  \AttributeTok{Pre\_AtBats =}\NormalTok{ ni,}
  \AttributeTok{p\_pre =} \FunctionTok{round}\NormalTok{(p\_pre, }\DecValTok{3}\NormalTok{),}
  \AttributeTok{x =}\NormalTok{ x,}
  \AttributeTok{sei =} \DecValTok{1}\NormalTok{, }\CommentTok{\# Known standard error for transformed data}
  \AttributeTok{Season\_HR =}\NormalTok{ Yi,}
  \AttributeTok{Season\_AtBats =}\NormalTok{ Ni,}
  \AttributeTok{p\_season =}\NormalTok{ p\_season,}
  \AttributeTok{true\_mu =}\NormalTok{ true\_mu}
\NormalTok{)}

\CommentTok{\# 4. Display the Data}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(baseball\_data, }
             \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Player"}\NormalTok{, }\StringTok{"$y\_i$"}\NormalTok{, }\StringTok{"$n\_i$"}\NormalTok{, }\StringTok{"$p\_i\^{}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{text\{pre\}\}$"}\NormalTok{, }\StringTok{"$x\_i$"}\NormalTok{, }\StringTok{"SE"}\NormalTok{, }
                           \StringTok{"$Y\_i$"}\NormalTok{, }\StringTok{"$N\_i$"}\NormalTok{, }\StringTok{"$p\_i\^{}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{text\{seas\}\}$"}\NormalTok{, }\StringTok{"$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{mu\_i$"}\NormalTok{),}
             \AttributeTok{align =} \StringTok{"c"}\NormalTok{,}
             \AttributeTok{digits =} \DecValTok{3}\NormalTok{,}
             \AttributeTok{caption =} \StringTok{"1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0808}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0707}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0707}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.2020}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0909}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0404}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0707}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0707}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.2121}}
  >{\centering\arraybackslash}p{(\linewidth - 18\tabcolsep) * \real{0.0909}}@{}}
\caption{1998 MLB Statistics: Raw Counts, Probabilities, and Transformed
Data}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Player
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(y_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(n_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(p_i^{\text{pre}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(x_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(Y_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(N_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(p_i^{\text{seas}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\mu_i\)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Player
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(y_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(n_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(p_i^{\text{pre}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(x_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(Y_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(N_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(p_i^{\text{seas}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\mu_i\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 7 & 58 & 0.121 & -6.559 & 1 & 70 & 509 & 0.138 & -6.176 \\
2 & 9 & 59 & 0.153 & -5.901 & 1 & 66 & 643 & 0.103 & -7.055 \\
3 & 4 & 74 & 0.054 & -9.476 & 1 & 56 & 633 & 0.088 & -8.317 \\
4 & 7 & 84 & 0.083 & -9.029 & 1 & 46 & 645 & 0.071 & -9.441 \\
5 & 3 & 69 & 0.043 & -9.558 & 1 & 45 & 606 & 0.074 & -8.463 \\
6 & 6 & 63 & 0.095 & -7.488 & 1 & 44 & 555 & 0.079 & -7.937 \\
7 & 2 & 60 & 0.033 & -9.323 & 1 & 43 & 619 & 0.069 & -8.035 \\
8 & 10 & 54 & 0.185 & -5.005 & 1 & 40 & 609 & 0.066 & -7.734 \\
9 & 2 & 53 & 0.038 & -8.589 & 1 & 37 & 552 & 0.067 & -7.622 \\
10 & 2 & 60 & 0.033 & -9.323 & 1 & 34 & 540 & 0.063 & -8.238 \\
11 & 4 & 66 & 0.061 & -8.720 & 1 & 32 & 561 & 0.057 & -8.843 \\
12 & 3 & 66 & 0.045 & -9.270 & 1 & 30 & 440 & 0.068 & -8.469 \\
13 & 2 & 72 & 0.028 & -10.487 & 1 & 29 & 585 & 0.050 & -9.518 \\
14 & 5 & 64 & 0.078 & -8.034 & 1 & 28 & 531 & 0.053 & -8.859 \\
15 & 3 & 42 & 0.071 & -6.673 & 1 & 23 & 454 & 0.051 & -7.237 \\
16 & 2 & 38 & 0.053 & -6.829 & 1 & 21 & 504 & 0.042 & -7.149 \\
17 & 6 & 58 & 0.103 & -6.975 & 1 & 15 & 244 & 0.061 & -8.146 \\
\end{longtable}

In this analysis, we model the home run strike rates of 17 Major League
Baseball players using pre-season exhibition data from 1998. We apply
five statistical methods ranging from simple independent estimation to
advanced Bayesian decision theory.

\subsection{\texorpdfstring{Methods for Estimating \(\mu_i\)
(Transformed
Scale)}{Methods for Estimating \textbackslash mu\_i (Transformed Scale)}}\label{methods-for-estimating-mu_i-transformed-scale}

\subsubsection{Method 1: Simple Estimation
(MLE)}\label{method-1-simple-estimation-mle}

The Maximum Likelihood Estimator (MLE) assumes each player's performance
is independent. It relies solely on the observed pre-season data.

\[ \hat{\mu}_i^{MLE} = X_i \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simple Estimate is just the data itself}
\NormalTok{mu\_mle }\OtherTok{\textless{}{-}}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{x}

\CommentTok{\# MSE Calculation (Transformed Scale)}
\NormalTok{mse\_mle }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((mu\_mle }\SpecialCharTok{{-}}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{true\_mu)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Method 2: Empirical Bayes
(James-Stein)}\label{method-2-empirical-bayes-james-stein}

The James-Stein estimator introduces a global mean \(\bar{X}\) and
shrinks individual estimates toward it. This assumes the players come
from a common population distribution.

\[ \hat{\mu}_i^{JS} = \bar{X} + \left( 1 - \frac{k-3}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X}) \]

where \(k=17\) is the number of players.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(baseball\_data}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{S }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((baseball\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}}\NormalTok{ theta\_hat)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{shrinkage\_factor }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{14} \SpecialCharTok{/}\NormalTok{ S)}

\NormalTok{mu\_js }\OtherTok{\textless{}{-}}\NormalTok{ theta\_hat }\SpecialCharTok{+}\NormalTok{ shrinkage\_factor }\SpecialCharTok{*}\NormalTok{ (baseball\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}}\NormalTok{ theta\_hat)}

\CommentTok{\# MSE Calculation (Transformed Scale)}
\NormalTok{mse\_js }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((mu\_js }\SpecialCharTok{{-}}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{true\_mu)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Method 3: Fully Bayesian MCMC
(brms)}\label{method-3-fully-bayesian-mcmc-brms}

We use a hierarchical Bayesian model where parameters are treated as
random variables. We implement this using \texttt{brms}.

\[
\begin{aligned}
X_i &\sim N(\mu_i, 1) \\
\mu_i &\sim N(\theta, \tau^2) \\
\theta &\sim N(0, 10) \\
\tau &\sim \text{Cauchy}(0, 2)
\end{aligned}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit Random Intercept Model: x | se(1) \textasciitilde{} 1 + (1|Player)}
\NormalTok{fit\_brms }\OtherTok{\textless{}{-}} \FunctionTok{brm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ x }\SpecialCharTok{|} \FunctionTok{se}\NormalTok{(sei, }\AttributeTok{sigma =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Player),}
  \AttributeTok{data =}\NormalTok{ baseball\_data,}
  \AttributeTok{prior =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{prior}\NormalTok{(}\FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{class =} \StringTok{"Intercept"}\NormalTok{),}
    \FunctionTok{prior}\NormalTok{(}\FunctionTok{cauchy}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{class =} \StringTok{"sd"}\NormalTok{)}
\NormalTok{  ),}
  \AttributeTok{chains =} \DecValTok{2}\NormalTok{, }\AttributeTok{iter =} \DecValTok{4000}\NormalTok{, }\AttributeTok{warmup =} \DecValTok{1000}\NormalTok{, }\AttributeTok{seed =} \DecValTok{123}\NormalTok{,}
  \AttributeTok{refresh =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# Extract Point Estimates (Posterior Means)}
\NormalTok{post\_means }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(fit\_brms)[, }\StringTok{"Estimate"}\NormalTok{]}
\NormalTok{mu\_brms }\OtherTok{\textless{}{-}}\NormalTok{ post\_means}

\CommentTok{\# MSE Calculation (Transformed Scale)}
\NormalTok{mse\_brms }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((mu\_brms }\SpecialCharTok{{-}}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{true\_mu)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Comparison of Estimates of
\(\mu_i\)}{Comparison of Estimates of \textbackslash mu\_i}}\label{comparison-of-estimates-of-mu_i}

\textbf{Full Comparison of Estimates (Transformed Scale)}

The following table presents the transformed data (\(x_i\)) and the true
season parameter (\(\mu_i\)) alongside the estimates from the three
methods. The rows are sorted by \(x_i\) to visualize how the shrinkage
methods (James-Stein and Bayesian) pull the estimates away from the
extremes and toward the population mean compared to the raw MLE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Compile all estimates into a single data frame}
\NormalTok{df\_estimates }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Player =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{,}
  \AttributeTok{ni =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats,       }
  \AttributeTok{x\_i =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{x,               }\CommentTok{\# MLE Estimate (Raw Data)}
  \AttributeTok{mu\_js =}\NormalTok{ mu\_js,                       }\CommentTok{\# James{-}Stein Estimate}
  \AttributeTok{mu\_bayes =}\NormalTok{ mu\_brms,                  }\CommentTok{\# Fully Bayesian Estimate}
  \AttributeTok{mu\_true =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{true\_mu      }\CommentTok{\# True Season Parameter}
\NormalTok{)}

\CommentTok{\# 2. Sort by x\_i (ascending)}
\NormalTok{df\_sorted }\OtherTok{\textless{}{-}}\NormalTok{ df\_estimates[}\FunctionTok{order}\NormalTok{(df\_estimates}\SpecialCharTok{$}\NormalTok{x\_i), ]}

\CommentTok{\# 3. Display the table}
\NormalTok{df\_display\_mu }\OtherTok{\textless{}{-}}\NormalTok{ df\_sorted}
\NormalTok{df\_display\_mu[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{6}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(df\_display\_mu[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{6}\NormalTok{], }\DecValTok{3}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_display\_mu[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Player"}\NormalTok{, }\StringTok{"x\_i"}\NormalTok{, }\StringTok{"mu\_js"}\NormalTok{, }\StringTok{"mu\_bayes"}\NormalTok{, }\StringTok{"mu\_true"}\NormalTok{)],}
             \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{,}
             \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Player"}\NormalTok{, }\StringTok{"$x\_i$ (MLE)"}\NormalTok{, }\StringTok{"$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{hat\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{mu\}\_\{JS\}$"}\NormalTok{, }
                           \StringTok{"$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{hat\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{mu\}\_\{Bayes\}$"}\NormalTok{, }\StringTok{"$}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{mu\_\{true\}$"}\NormalTok{),}
             \AttributeTok{align =} \StringTok{"c"}\NormalTok{,}
             \AttributeTok{caption =} \StringTok{"Comparison of Estimates (Sorted by Pre{-}season $x\_i$)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1081}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1757}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2432}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2838}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1892}}@{}}

\caption{\label{tbl-estimates-sorted}Comparison of Estimates (Sorted by
Pre-season \(x_i\))}

\tabularnewline

\caption{Comparison of Estimates (Sorted by Pre-season
\(x_i\))}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Player
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(x_i\) (MLE)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\hat{\mu}_{JS}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\hat{\mu}_{Bayes}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\mu_{true}\)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Player
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(x_i\) (MLE)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\hat{\mu}_{JS}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\hat{\mu}_{Bayes}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\mu_{true}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
13 & -10.487 & -9.589 & -8.746 & -9.518 \\
5 & -9.558 & -9.006 & -8.478 & -8.463 \\
3 & -9.476 & -8.954 & -8.470 & -8.317 \\
7 & -9.323 & -8.858 & -8.412 & -8.035 \\
10 & -9.323 & -8.858 & -8.415 & -8.238 \\
12 & -9.270 & -8.825 & -8.412 & -8.469 \\
4 & -9.029 & -8.673 & -8.331 & -9.441 \\
11 & -8.720 & -8.479 & -8.260 & -8.843 \\
9 & -8.589 & -8.397 & -8.206 & -7.622 \\
14 & -8.034 & -8.048 & -8.054 & -8.859 \\
6 & -7.488 & -7.705 & -7.897 & -7.937 \\
17 & -6.975 & -7.384 & -7.754 & -8.146 \\
16 & -6.829 & -7.292 & -7.714 & -7.149 \\
15 & -6.673 & -7.194 & -7.663 & -7.237 \\
1 & -6.559 & -7.122 & -7.628 & -6.176 \\
2 & -5.901 & -6.709 & -7.441 & -7.055 \\
8 & -5.005 & -6.146 & -7.186 & -7.734 \\

\end{longtable}

\textbf{Plots of Errors (Sorted by \(x_i\))}

This plot displays the Squared Error for each player. The x-axis
represents the players sorted from lowest pre-season performance to
highest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate Squared Errors using the SORTED dataframe}
\NormalTok{err\_mle  }\OtherTok{\textless{}{-}}\NormalTok{ (df\_sorted}\SpecialCharTok{$}\NormalTok{x\_i }\SpecialCharTok{{-}}\NormalTok{ df\_sorted}\SpecialCharTok{$}\NormalTok{mu\_true)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{err\_js   }\OtherTok{\textless{}{-}}\NormalTok{ (df\_sorted}\SpecialCharTok{$}\NormalTok{mu\_js }\SpecialCharTok{{-}}\NormalTok{ df\_sorted}\SpecialCharTok{$}\NormalTok{mu\_true)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{err\_brms }\OtherTok{\textless{}{-}}\NormalTok{ (df\_sorted}\SpecialCharTok{$}\NormalTok{mu\_bayes }\SpecialCharTok{{-}}\NormalTok{ df\_sorted}\SpecialCharTok{$}\NormalTok{mu\_true)}\SpecialCharTok{\^{}}\DecValTok{2}

\CommentTok{\# Determine Y{-}axis range}
\NormalTok{y\_max }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(err\_mle, err\_js, err\_brms))}

\CommentTok{\# Plot MLE Errors (Baseline)}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, err\_mle, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Player Index (Sorted by Pre{-}season Performance)"}\NormalTok{, }
     \AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{(Squared}\SpecialCharTok{\textasciitilde{}}\NormalTok{Error}\SpecialCharTok{\textasciitilde{}}\ErrorTok{\textasciitilde{}}\NormalTok{(}\FunctionTok{hat}\NormalTok{(mu) }\SpecialCharTok{{-}}\NormalTok{ mu[true])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),}
     \AttributeTok{main =} \StringTok{"Estimation Error Comparison (Sorted)"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, y\_max))}

\CommentTok{\# Add James{-}Stein Errors}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, err\_js, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}

\CommentTok{\# Add Bayesian (brms) Errors}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, err\_brms, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{17}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\CommentTok{\# Add Grid and Legend}
\FunctionTok{grid}\NormalTok{()}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Mean Squared Error"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MLE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_mle, }\DecValTok{3}\NormalTok{)), }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"JS: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_js, }\DecValTok{3}\NormalTok{)), }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Bayes: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_brms, }\DecValTok{3}\NormalTok{))),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{17}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-error-index-sorted-1.pdf}}

}

\caption{\label{fig-error-index-sorted}Squared Error by Sorted Player
Index (Transformed Scale)}

\end{figure}%

\subsection{\texorpdfstring{Methods for Estimating \(p_i\)
directly}{Methods for Estimating p\_i directly}}\label{methods-for-estimating-p_i-directly}

\subsubsection{\texorpdfstring{Method 1-3: Converting \(\hat \mu_i\)
back to
\(p_i\)}{Method 1-3: Converting \textbackslash hat \textbackslash mu\_i back to p\_i}}\label{method-1-3-converting-hat-mu_i-back-to-p_i}

The first three methods (MLE, James-Stein, and Normal-Normal Bayes)
estimated the parameter \(\mu_i\) on the transformed scale. To obtain
the probability estimates \(\hat{p}_i\), we apply the inverse of the
variance-stabilizing transformation:

\[ \hat{p}_i = \frac{1}{2} \left( \sin\left( \frac{\hat{\mu}_i}{\sqrt{n_i}} \right) + 1 \right) \]

where \(\hat{\mu}_i\) corresponds to the estimate derived from Method 1,
2, or 3, and \(n_i\) is the number of pre-season at-bats for player
\(i\).

\subsubsection{Method 4: Hierarchical Logistic Regression
(Logit-Normal)}\label{method-4-hierarchical-logistic-regression-logit-normal}

In this fourth method, we model the probability \(p_i\) directly using a
hierarchical structure on the log-odds scale, rather than transforming
the data.

We assume the count \(y_i\) follows a Binomial distribution. The
log-odds (logit) of the success rate \(p_i\) are drawn from a common
Normal distribution with unknown mean \(\mu_0\) and standard deviation
\(\tau_0\).

\[
\begin{aligned}
y_i | p_i &\sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) &\sim N(\mu_0, \tau_0^2) \\
\mu_0 &\sim N(0, 10) \\
\tau_0 &\sim \text{Cauchy}(0, 2)
\end{aligned}
\]

We implement this in \texttt{brms} using the \texttt{binomial} family
with a logit link. The individual point estimate \(\hat{p}_i\) is the
\textbf{posterior mean} of \(p_i\). Note that because the inverse-logit
function is non-linear, the posterior mean of \(p_i\) is not simply the
inverse-logit of the posterior mean of the random effect; \texttt{brms}
handles this integration automatically via the \texttt{fitted()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Fit Hierarchical Logistic Regression}
\CommentTok{\# Formula: y | trials(n) \textasciitilde{} 1 + (1 | Player)}
\CommentTok{\# This estimates a global intercept (mu\_0) and random intercepts for each player (logit(p\_i))}
\NormalTok{fit\_logit }\OtherTok{\textless{}{-}} \FunctionTok{brm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ Pre\_HR }\SpecialCharTok{|} \FunctionTok{trials}\NormalTok{(Pre\_AtBats) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Player),}
  \AttributeTok{data =}\NormalTok{ baseball\_data,}
  \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
  \AttributeTok{prior =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{prior}\NormalTok{(}\FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{class =} \StringTok{"Intercept"}\NormalTok{),}
    \FunctionTok{prior}\NormalTok{(}\FunctionTok{cauchy}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{class =} \StringTok{"sd"}\NormalTok{)}
\NormalTok{  ),}
  \AttributeTok{chains =} \DecValTok{2}\NormalTok{, }\AttributeTok{iter =} \DecValTok{4000}\NormalTok{, }\AttributeTok{warmup =} \DecValTok{1000}\NormalTok{, }\AttributeTok{seed =} \DecValTok{123}\NormalTok{,}
  \AttributeTok{refresh =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# 2. Extract Posterior Means of p\_i}
\CommentTok{\# fitted() returns the posterior expectations of the response (Expected Count). }
\NormalTok{fitted\_counts }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(fit\_logit) }
\NormalTok{p\_hat\_logit }\OtherTok{\textless{}{-}}\NormalTok{ fitted\_counts[, }\StringTok{"Estimate"}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats}
\end{Highlighting}
\end{Shaded}

\subsubsection{Method 5: Optimal Bayes Estimator (Weighted
Median)}\label{method-5-optimal-bayes-estimator-weighted-median}

While the posterior mean (Method 4) minimizes the Mean Squared Error
(MSE), it is not necessarily optimal for the \textbf{Relative
Standardized Error} metric we defined earlier:
\[L(p, \hat{p}) = \frac{|p - \hat{p}|}{\min(p, 1-p)}\]

This is a form of weighted absolute error loss, where the weight is
\(w(p) = \frac{1}{\min(p, 1-p)}\). Theoretical derivation shows that the
estimator minimizing the expected posterior loss for this function is
the \textbf{Weighted Posterior Median}.

We compute this by extracting the full posterior samples from the
Logit-Normal model (Method 4) and calculating the weighted median for
each player.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Extract Posterior Samples (N\_samples x 17 players)}
\CommentTok{\# posterior\_epred gives samples of the expected count (N * p)}
\NormalTok{post\_counts }\OtherTok{\textless{}{-}} \FunctionTok{posterior\_epred}\NormalTok{(fit\_logit) }

\CommentTok{\# Convert to probability scale by dividing by trials}
\NormalTok{p\_samples }\OtherTok{\textless{}{-}} \FunctionTok{sweep}\NormalTok{(post\_counts, }\DecValTok{2}\NormalTok{, baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats, }\StringTok{"/"}\NormalTok{)}

\CommentTok{\# 2. Define Function for Weighted Median}
\CommentTok{\# Finds the value \textquotesingle{}q\textquotesingle{} such that sum(weights where x \textless{}= q) \textgreater{}= 0.5 * total\_weight}
\NormalTok{get\_weighted\_median }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(samples) \{}
  \CommentTok{\# Calculate weights based on the loss function denominator}
  \CommentTok{\# Avoid division by exact zero (unlikely but safer)}
\NormalTok{  denom }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(samples, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ samples)}
\NormalTok{  denom[denom }\SpecialCharTok{\textless{}} \FloatTok{1e{-}6}\NormalTok{] }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6} 
\NormalTok{  weights }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ denom}
  
  \CommentTok{\# Normalize weights}
\NormalTok{  weights\_norm }\OtherTok{\textless{}{-}}\NormalTok{ weights }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(weights)}
  
  \CommentTok{\# Sort samples and weights}
\NormalTok{  ord }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(samples)}
\NormalTok{  samp\_sorted }\OtherTok{\textless{}{-}}\NormalTok{ samples[ord]}
\NormalTok{  w\_sorted }\OtherTok{\textless{}{-}}\NormalTok{ weights\_norm[ord]}
  
  \CommentTok{\# Find cutoff}
\NormalTok{  cum\_w }\OtherTok{\textless{}{-}} \FunctionTok{cumsum}\NormalTok{(w\_sorted)}
\NormalTok{  idx }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(cum\_w }\SpecialCharTok{\textgreater{}=} \FloatTok{0.5}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
  
  \FunctionTok{return}\NormalTok{(samp\_sorted[idx])}
\NormalTok{\}}

\CommentTok{\# 3. Apply to all players}
\NormalTok{p\_hat\_optimal }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(p\_samples, }\DecValTok{2}\NormalTok{, get\_weighted\_median)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Comparison of All Five Estimates (Probability
Scale)}\label{comparison-of-all-five-estimates-probability-scale}

We now compare all five methods: MLE, James-Stein (transformed), Bayes
Normal-Normal (transformed), Hierarchical Logit-Normal (Posterior Mean),
and Optimal Bayes (Weighted Median).

\textbf{1. MSE Comparison}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Prepare Estimates from Previous Steps}
\NormalTok{inv\_trans }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, n) \{ }\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{sin}\NormalTok{(mu }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(n)) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) \}}

\CommentTok{\# Convert transformed estimates back to probability scale}
\NormalTok{p\_mle    }\OtherTok{\textless{}{-}} \FunctionTok{inv\_trans}\NormalTok{(baseball\_data}\SpecialCharTok{$}\NormalTok{x, baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats)}
\NormalTok{p\_js     }\OtherTok{\textless{}{-}} \FunctionTok{inv\_trans}\NormalTok{(mu\_js, baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats)}
\NormalTok{p\_normal }\OtherTok{\textless{}{-}} \FunctionTok{inv\_trans}\NormalTok{(mu\_brms, baseball\_data}\SpecialCharTok{$}\NormalTok{Pre\_AtBats) }
\CommentTok{\# p\_hat\_logit (Method 4) and p\_hat\_optimal (Method 5) are already calculated}

\CommentTok{\# 2. Combine into DataFrame}
\NormalTok{df\_compare }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Player =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{Player,}
  \AttributeTok{x\_i    =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{x, }\CommentTok{\# For sorting}
  \AttributeTok{p\_true =}\NormalTok{ baseball\_data}\SpecialCharTok{$}\NormalTok{p\_season,}
  \AttributeTok{p\_mle  =}\NormalTok{ p\_mle,}
  \AttributeTok{p\_js   =}\NormalTok{ p\_js,}
  \AttributeTok{p\_norm =}\NormalTok{ p\_normal,}
  \AttributeTok{p\_logit =}\NormalTok{ p\_hat\_logit,}
  \AttributeTok{p\_opt   =}\NormalTok{ p\_hat\_optimal}
\NormalTok{)}

\CommentTok{\# Sort by initial performance}
\NormalTok{df\_compare\_sorted }\OtherTok{\textless{}{-}}\NormalTok{ df\_compare[}\FunctionTok{order}\NormalTok{(df\_compare}\SpecialCharTok{$}\NormalTok{x\_i), ]}

\CommentTok{\# 3. Calculate MSE}
\NormalTok{mse\_p\_mle   }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_mle }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{mse\_p\_js    }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_js }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{mse\_p\_norm  }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_norm }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{mse\_p\_logit }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_logit }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{mse\_p\_opt   }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_opt }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\CommentTok{\# 4. Plot MSE}
\NormalTok{y\_max }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{((df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_mle }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, (df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_mle }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }
     \AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Player Index (Sorted by Pre{-}season)"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Squared Error"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Squared Error by Method"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, y\_max))}

\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, (df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_js }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, (df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_norm }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{17}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, (df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_logit }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{15}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, (df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_opt }\SpecialCharTok{{-}}\NormalTok{ df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{18}\NormalTok{, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}

\FunctionTok{grid}\NormalTok{()}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MLE [MSE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_p\_mle, }\DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"JS [MSE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_p\_js, }\DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Normal{-}Bayes [MSE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_p\_norm, }\DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Logit{-}Normal [MSE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_p\_logit, }\DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Optimal{-}Bayes [MSE: "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mse\_p\_opt, }\DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{)),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"purple"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{18}\NormalTok{),}
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{0.75}\NormalTok{,}
       \AttributeTok{bg =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/compare-five-methods-1.pdf}}

\textbf{2. Relative Standardized Error}

We also evaluate the methods using the relative error metric that
penalizes deviations based on the rarity of the event:
\[ \text{Metric}_i = \frac{|p_i^{\text{true}} - \hat{p}_i|}{\min(p_i^{\text{true}}, 1 - p_i^{\text{true}})} \]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Define Metric}
\NormalTok{calc\_metric }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p\_hat, p\_true) \{}
\NormalTok{  denom }\OtherTok{\textless{}{-}} \FunctionTok{pmin}\NormalTok{(p\_true, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p\_true)}
  \FunctionTok{abs}\NormalTok{(p\_hat }\SpecialCharTok{{-}}\NormalTok{ p\_true) }\SpecialCharTok{/}\NormalTok{ denom}
\NormalTok{\}}

\CommentTok{\# 2. Calculate Metric}
\NormalTok{rel\_mle   }\OtherTok{\textless{}{-}} \FunctionTok{calc\_metric}\NormalTok{(df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_mle, df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}
\NormalTok{rel\_js    }\OtherTok{\textless{}{-}} \FunctionTok{calc\_metric}\NormalTok{(df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_js, df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}
\NormalTok{rel\_norm  }\OtherTok{\textless{}{-}} \FunctionTok{calc\_metric}\NormalTok{(df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_norm, df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}
\NormalTok{rel\_logit }\OtherTok{\textless{}{-}} \FunctionTok{calc\_metric}\NormalTok{(df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_logit, df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}
\NormalTok{rel\_opt   }\OtherTok{\textless{}{-}} \FunctionTok{calc\_metric}\NormalTok{(df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_opt, df\_compare\_sorted}\SpecialCharTok{$}\NormalTok{p\_true)}

\CommentTok{\# 3. Sum of Errors}
\NormalTok{sum\_rel\_mle   }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(rel\_mle)}
\NormalTok{sum\_rel\_js    }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(rel\_js)}
\NormalTok{sum\_rel\_norm  }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(rel\_norm)}
\NormalTok{sum\_rel\_logit }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(rel\_logit)}
\NormalTok{sum\_rel\_opt   }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(rel\_opt)}

\CommentTok{\# 4. Plot}
\NormalTok{y\_max\_rel }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{c}\NormalTok{(rel\_mle, rel\_js, rel\_norm, rel\_logit, rel\_opt)) }\SpecialCharTok{*} \FloatTok{1.1}

\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, rel\_mle, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Player Index (Sorted by Pre{-}season)"}\NormalTok{, }
     \AttributeTok{ylab =} \StringTok{"Relative Standardized Error"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Assessment of Estimation Methods"}\NormalTok{,}
     \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, y\_max\_rel))}

\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, rel\_js, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, rel\_norm, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{17}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, rel\_logit, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{15}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkgreen"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{17}\NormalTok{, rel\_opt, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{18}\NormalTok{, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}

\FunctionTok{grid}\NormalTok{()}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Method [Sum Relative Error]"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MLE ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(sum\_rel\_mle, }\DecValTok{3}\NormalTok{), }\StringTok{"]"}\NormalTok{), }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"James{-}Stein ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(sum\_rel\_js, }\DecValTok{3}\NormalTok{), }\StringTok{"]"}\NormalTok{), }
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Normal{-}Bayes ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(sum\_rel\_norm, }\DecValTok{3}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Logit{-}Normal ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(sum\_rel\_logit, }\DecValTok{3}\NormalTok{), }\StringTok{"]"}\NormalTok{),}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{"Optimal{-}Bayes ["}\NormalTok{, }\FunctionTok{round}\NormalTok{(sum\_rel\_opt, }\DecValTok{3}\NormalTok{), }\StringTok{"]"}\NormalTok{)),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{18}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
       \AttributeTok{cex =} \FloatTok{0.75}\NormalTok{, }
       \AttributeTok{bg =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bayesian_files/figure-pdf/fig-error-relative-final-1.pdf}}

}

\caption{\label{fig-error-relative-final}Relative Error Assessment: Five
Methods}

\end{figure}%

\section{Bayesian Predictive
Distributions}\label{bayesian-predictive-distributions}

A key feature of Bayesian analysis is the ability to make inference
about future observations, rather than just the model parameters. The
\textbf{posterior predictive distribution} describes the probability of
observing a new data point \(y^*\) given the observed data \(y\).

\begin{definition}[Posterior Predictive
Distribution]\protect\hypertarget{def-posterior-predictive}{}\label{def-posterior-predictive}

Let \(f(y^*|\theta)\) be the sampling distribution of a future
observation \(y^*\) given parameter \(\theta\), and let
\(\pi(\theta|y)\) be the posterior distribution of \(\theta\) given
observed data \(y\). The posterior predictive density is obtained by
marginalizing over the parameter \(\theta\):

\[
f(y^*|y) = \int_\Theta f(y^*|\theta) \pi(\theta|y) \, d\theta
\]

\end{definition}

This distribution incorporates two distinct sources of uncertainty:

\begin{itemize}
\tightlist
\item
  \textbf{Sampling Uncertainty (Aleatoric):} The inherent variability of
  the data generation process, represented by the variance in
  \(f(y^*|\theta)\).
\item
  \textbf{Parameter Uncertainty (Epistemic):} The uncertainty regarding
  the true value of \(\theta\), represented by the variance in the
  posterior \(\pi(\theta|y)\).
\end{itemize}

As sample size \(n \to \infty\), the parameter uncertainty vanishes (the
posterior approaches a point mass), and the predictive distribution
converges to the true data-generating distribution.

\begin{example}[Normal-Normal Predictive
Distribution]\protect\hypertarget{exm-predictive-normal}{}\label{exm-predictive-normal}

Consider a case where the data \(y_1, \dots, y_n\) are independent and
normally distributed with unknown mean \(\mu\) and known variance
\(\sigma^2\):

\[
Y_i | \mu \sim N(\mu, \sigma^2)
\]

Assume a conjugate prior for the mean:
\(\mu \sim N(\mu_0, \sigma_0^2)\). The posterior distribution is
\(\mu|y \sim N(\mu_n, \sigma_n^2)\), where \(\mu_n\) and \(\sigma_n^2\)
are the updated posterior hyperparameters.

The predictive distribution for a new observation \(y^*\) is derived as:

\[
\begin{aligned}
f(y^*|y) &= \int_{-\infty}^{\infty} f(y^*|\mu) \pi(\mu|y) \, d\mu \\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y^*-\mu)^2}{2\sigma^2}} \times \frac{1}{\sqrt{2\pi\sigma_n^2}} e^{-\frac{(\mu-\mu_n)^2}{2\sigma_n^2}} \, d\mu
\end{aligned}
\]

This convolution of two Gaussians results in a new Gaussian
distribution:

\[
y^* | y \sim N(\mu_n, \sigma^2 + \sigma_n^2)
\]

Here, the total predictive variance is the sum of the data variance
(\(\sigma^2\)) and the posterior uncertainty about the mean
(\(\sigma_n^2\)).

\end{example}




\end{document}
