---
title: "Hypothesis Testing"
engine: knitr
format: 
  html: default
  pdf: default
---

## General Terminologies

### Hypothesis Testing 

We formulate the problem of hypothesis testing as deciding between two competing claims about a parameter $\theta$:

$$
H_0: \theta \in \Theta_0 \quad \text{(Null Hypothesis)}
$$

$$
H_1: \theta \in \Theta_1 \quad \text{(Alternative Hypothesis)}
$$

::: {#def-simple-composite}
### Simple and Composite Hypotheses
A hypothesis is called **simple** if it specifies a single value for the parameter (e.g., $\Theta_0$ contains only one point). It is called **composite** if it specifies more than one value.
:::

::: {#exm-normal-hypotheses}
### Normal Mean Test
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$.

* If $\sigma^2$ is known, $H_0: \mu = \mu_0$ is a simple hypothesis.
* If $\sigma^2$ is unknown, $H_0: \mu = \mu_0$ is a composite hypothesis (since $\sigma^2$ can vary).
:::

### Test Functions and Size

A test is defined by a **critical region** $C_\alpha$ such that we reject $H_0$ if the data $x \in C_\alpha$. Equivalently, we can define a **test function** $\phi(x)$ representing the probability of rejecting $H_0$ given data $x$.

* A non-randomized test is given as follows:

$$
\phi(x) = I(x \in C_\alpha) = \begin{cases} 1 & \text{if } x \in C_\alpha \text{ (Reject } H_0 \text{)} \\ 0 & \text{otherwise} \end{cases}
$$

* A randomized test, $\phi(x)$ can take values in $[0, 1]$, which can be expressed typically as follows:


   $$
   \phi(x) = \begin{cases} 
   1 & \text{if } x \in C_1 \\
   \gamma & \text{if } x \in C_* \\
   0 & \text{otherwise}
   \end{cases}
   $$

   where:

   * $C_1$ is the region where we strictly reject $H_0$.
   * $C_*$ is the boundary region (often where $T(x) = k$) where we reject $H_0$ with probability $\gamma$.

* More generally, $\phi(x)$ is just a function of $x$ with values in $[0,1]$, which represents the probability that we will reject $H_0$. 


::: {#exm-randomized-test}
### Randomized Test for Binomial
Let $X \sim \text{Bin}(n=10, \theta)$. Consider testing $H_0: \theta = 1/2$ vs $H_1: \theta > 1/2$ with target size $\alpha = 0.05$.

Suppose we choose a critical region $X \ge k$.

* If $k=9$, $P(X \ge 9 | \theta=0.5) \approx 0.0107$.
* If $k=8$, $P(X \ge 8 | \theta=0.5) \approx 0.0547$.

Since we cannot achieve exactly 0.05 with a non-randomized test (the survival function jumps over 0.05), we must use a randomized test function.


The randomized test is defined as:

$$
\phi(x) = \begin{cases} 
1 & \text{if } x \in C_1 \text{ (i.e., } x \ge 9) \\
\gamma & \text{if } x \in C_* \text{ (i.e., } x = 8) \\
0 & \text{otherwise}
\end{cases}
$$

From the figure, we see that $\alpha = 0.05$ lies between $P(X \ge 9)$ and $P(X \ge 8)$. We always reject the "tail" where probabilities are strictly less than $\alpha$ (here $x \ge 9$). At the boundary $x=8$, we cannot reject with probability 1 (which would give total size 0.0547), nor with probability 0 (which would give total size 0.0107).

We choose $\gamma$ to bridge this gap:

$$
\begin{aligned}
\alpha &= P(X \ge 9) + \gamma \cdot P(X = 8) \\
0.05 &= 0.01074 + \gamma \cdot (P(X \ge 8) - P(X \ge 9)) \\
0.05 &= 0.01074 + \gamma \cdot (0.05469 - 0.01074)
\end{aligned}
$$

Solving for $\gamma$:

$$
\gamma = \frac{0.05 - 0.01074}{0.04395} \approx \frac{39}{44} \approx 0.89
$$
:::


```{tikz fig-binomial-survival-zoom}
%| fig-cap: "Survival Function P(X >= k) with randomization detail"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 90% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{patterns, spy}"

\begin{tikzpicture}[
    scale=1.2,
    spy using outlines={circle, magnification=5, size=3.5cm, connect spies, thick, gray}
]

% --- Definitions ---
\def\ysc{5} % Y-scale factor

% --- Axes ---
\draw[->] (-0.5,0) -- (11.5,0) node[right] {$k$};
\draw[->] (0,0) -- (0,\ysc+0.5) node[above] {$P(X \ge k)$};

% --- Ticks ---
\foreach \x in {0,1,...,10} \draw (\x,0.1) -- (\x,-0.1) node[below] {\footnotesize \x};
\draw (0.1, \ysc) -- (-0.1, \ysc) node[left] {\footnotesize 1.0};
\draw (0.1, 0.5*\ysc) -- (-0.1, 0.5*\ysc) node[left] {\footnotesize 0.5};

% --- Data Points (Binomial 10, 0.5 Survival) ---
% P(>=8) = 0.0547, P(>=9) = 0.0107
\coordinate (P0) at (0, 1.0*\ysc);
\coordinate (P1) at (1, 0.999*\ysc);
\coordinate (P2) at (2, 0.989*\ysc);
\coordinate (P3) at (3, 0.945*\ysc);
\coordinate (P4) at (4, 0.828*\ysc);
\coordinate (P5) at (5, 0.623*\ysc);
\coordinate (P6) at (6, 0.377*\ysc);
\coordinate (P7) at (7, 0.172*\ysc);
\coordinate (P8) at (8, 0.0547*\ysc);
\coordinate (P9) at (9, 0.0107*\ysc);
\coordinate (P10) at (10, 0.001*\ysc);

% --- Drawing Sticks (Vertical Lines) ---
% We use very small points (0.4pt) so they don't look huge in the zoom
\foreach \i in {0,...,10} {
    \draw[blue, thick] (\i,0) -- (P\i);
    \fill[blue] (P\i) circle (0.4pt); 
}

% --- Alpha Threshold Line ---
\draw[red, dashed, thick] (-0.5, 0.05*\ysc) -- (11, 0.05*\ysc);
\node[red, left] at (-0.5, 0.05*\ysc) {$\alpha=0.05$};

% --- Zoom / Spy Region ---
% We spy on the gap between k=8 and k=9 around the alpha line
% Spy point: x=8.5, y=0.03*5 (approx middle of gap)
\coordinate (FocusPoint) at (8.2, 0.035*\ysc);
\coordinate (ZoomLocation) at (7, 0.5*\ysc); 

\spy [black, magnification=6] on (FocusPoint) in node at (ZoomLocation);

% --- Annotations (Outside spy to avoid double text) ---
\node[right, font=\footnotesize, align=left] at (6.5, 0.85*\ysc) {
    \textbf{Randomization Zone}\\
    Zoom shows $k=8$ and $k=9$.\\
    $\alpha$ passes between them.
};

\end{tikzpicture}
```


::: {#def-test-size}
### Size of a Test
The **size** of a test $\phi(x)$, denoted by $\alpha$, is the maximum probability of rejecting the null hypothesis when it is true:

$$
\alpha = \sup_{\theta \in \Theta_0} \text{Pr}(\text{Reject } H_0 \mid \theta) = \sup_{\theta \in \Theta_0} E_\theta[\phi(X)]
$$
:::


## Power Function

The **power function** of a test, denoted $W(\theta)$ or $\beta(\theta)$, is the probability of rejecting $H_0$ as a function of $\theta$:

$$
W(\theta) = E_\theta[\phi(X)]
$$

Ideally, we want:

1.  $E_\theta[\phi(X)] \le \alpha$ for all $\theta \in \Theta_0$ (Size control).
2.  $E_\theta[\phi(X)]$ to be as large as possible for $\theta \in \Theta_1$ (High power).

## The Neyman-Pearson Lemma

Consider testing a simple null against a simple alternative:
$H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$.

We define the **Likelihood Ratio** $\Lambda(x)$ as:

$$
\Lambda(x) = \frac{f_1(x)}{f_0(x)} = \frac{f(x; \theta_1)}{f(x; \theta_0)}
$$

::: {#def-lrt}
### Likelihood Ratio Test (LRT)
A test $\phi(x)$ is a Likelihood Ratio Test if it has the form:

$$
\phi(x) = \begin{cases} 
1 & \text{if } \Lambda(x) > k \\
\gamma(x) & \text{if } \Lambda(x) = k \\
0 & \text{if } \Lambda(x) < k
\end{cases}
$$

where $k \ge 0$ is a constant and $0 \le \gamma(x) \le 1$.
:::

```{r}
#| label: fig-lrt-density-primary
#| fig-cap: "Densities of $\\Lambda$ (primary view) with the LRT function $\\phi(x)$ scaled to the bottom 10% of the axis."
#| fig-width: 7
#| fig-height: 5

library(ggplot2)

# 1. Define Constants
k_val     <- 7
gamma_val <- 0.5
df_val    <- 3
ncp_h0    <- 0
ncp_h1    <- 5.5

# 2. Calculate Scaling
# We find the peak density to determine the Y-axis range.
max_dens <- max(
  dchisq(df_val - 2, df = df_val, ncp = ncp_h0), # Mode approx df-2
  dchisq(df_val + ncp_h1 - 2, df = df_val, ncp = ncp_h1)
)
# Make the graph slightly taller than the max density for headroom
y_limit <- max_dens * 1.1

# The user wants phi=1 to equal 0.1 of the range (approx 0.1 * y_limit)
phi_scale <- 0.1 * y_limit

# 3. Define Scaled Segments for phi(x)
# Original y=1 becomes y=phi_scale
df_segments <- data.frame(
  x_start = c(0, k_val),
  x_end   = c(k_val, 15),
  y_start = c(0, phi_scale),
  y_end   = c(0, phi_scale)
)

ggplot() +
  # --- Layer 1: Densities (Primary Scale) ---
  
  # H0 Density (Red)
  stat_function(fun = dchisq, args = list(df = df_val, ncp = ncp_h0), 
                geom = "area", fill = "red", alpha = 0.2) +
  stat_function(fun = dchisq, args = list(df = df_val, ncp = ncp_h0), 
                color = "red", linetype = "dashed") +
  
  # H1 Density (Blue)
  stat_function(fun = dchisq, args = list(df = df_val, ncp = ncp_h1), 
                geom = "area", fill = "blue", alpha = 0.2) +
  stat_function(fun = dchisq, args = list(df = df_val, ncp = ncp_h1), 
                color = "blue", linetype = "dashed") +

  # --- Layer 2: Scaled Test Function phi(x) (Bottom 10%) ---
  
  # The horizontal steps
  geom_segment(data = df_segments, 
               aes(x = x_start, xend = x_end, y = y_start, yend = y_end), 
               size = 1.2) +
  
  # The vertical threshold line (goes up to the density to show the cut)
  geom_segment(aes(x = k_val, xend = k_val, y = 0, yend = phi_scale), 
               linetype = "dotted", color = "black") +
  
  # Points scaled to phi_scale
  geom_point(aes(x = k_val, y = gamma_val * phi_scale), size = 3) + 
  geom_point(aes(x = k_val, y = 0), size = 3, shape = 21, fill = "white") +
  geom_point(aes(x = k_val, y = phi_scale), size = 3, shape = 21, fill = "white") +

  # --- Layer 3: Annotations ---
  
  # Label for gamma
  annotate("text", x = k_val + 0.2, y = gamma_val * phi_scale, 
           label = expression(gamma), hjust = 0, fontface = "bold") +
  
  # Labels for Densities
  annotate("text", x = 2, y = max_dens * 0.9, 
           label = expression(H[0]), color = "red", size = 5) +
  annotate("text", x = 8, y = max_dens * 0.4, 
           label = expression(H[1]), color = "blue", size = 5) +

  # --- Layer 4: Axes ---
  scale_y_continuous(
    name = "Density f(x)",
    limits = c(0, y_limit),
    
    # Secondary axis shows the 0-1 scale for phi
    sec.axis = sec_axis(~ . / phi_scale, 
                        name = expression(phi(x) ~ "(Indicator)"),
                        breaks = c(0, 0.5, 1))
  ) +
  scale_x_continuous(name = expression(Lambda(x))) +
  
  theme_minimal() +
  theme(
    axis.title.y.right = element_text(angle = 90, vjust = 0.5),
    panel.grid.minor = element_blank()
  )
```

### Neyman-Pearson Lemma
::: {#thm-neyman-pearson}
### Neyman-Pearson Lemma
a) **Optimality:** For any $k$ and $\gamma(x)$, the LRT $\phi_0(x)$ defined above has maximum power among all tests whose size is less than or equal to the size of $\phi_0(x)$.

a) **Existence:** Given $\alpha \in (0, 1)$, there exist constants $k$ and $\gamma_0$ such that the LRT defined by this $k$ and $\gamma(x) = \gamma_0$ has size exactly $\alpha$.

c) ** Uniqueness:** If a test $\phi$ has size $\alpha$ and is of maximum power among all tests of size $\alpha$, then $\phi$ is necessarily an LRT, except possibly on a set of measure zero under $H_0$ and $H_1$.
:::

### A Derivation with The Lagrange Multiplier Approach

To make the optimality of the Likelihood Ratio Test (LRT) intuitive, we can frame the search for the best test function $\phi(x)$ as a constrained optimization problem.

We want to maximize the power of the test:

$$
\text{Power}(\phi) = \int \phi(x) f_1(x) dx
$$

subject to the constraint on the size of the test $\alpha$:

$$
\text{Size}(\phi) = \int \phi(x) f_0(x) dx = \alpha
$$

Using the method of Lagrange multipliers, we define the objective function $L$ with a multiplier $k$:

$$
L(\phi, k) = \int \phi(x) f_1(x) dx - k \left( \int \phi(x) f_0(x) dx - \alpha \right)
$$

Rearranging the terms inside the integral, we get:

$$
L(\phi, k) = \int \phi(x) [f_1(x) - k f_0(x)] dx + k\alpha
$$

To maximize $L$ with respect to $\phi(x)$, we look at the integrand. Since $0 \le \phi(x) \le 1$, we should choose $\phi(x)$ to be as large as possible whenever its coefficient is positive, and as small as possible whenever its coefficient is negative:

* If $f_1(x) - k f_0(x) > 0$, set $\phi(x) = 1$.
* If $f_1(x) - k f_0(x) < 0$, set $\phi(x) = 0$.
* If $f_1(x) - k f_0(x) = 0$, the value of $\phi(x)$ does not affect the integral (this is where $\gamma$ comes in).

This decision rule is equivalent to:

$$
\phi(x) = 
\begin{cases} 
1 & \text{if } \frac{f_1(x)}{f_0(x)} > k \\
0 & \text{if } \frac{f_1(x)}{f_0(x)} < k
\end{cases}
$$

This is precisely the form of the Likelihood Ratio Test. The "shadow price" or Lagrange multiplier $k$ represents the critical threshold that balances the gain in power against the cost of increasing the Type I error.

::: {.proof}
**Proof of (a) Optimality:**
Let $\phi_0$ be the LRT with size $\alpha$, and $\phi$ be any other test with size $\le \alpha$.
Define $U(x) = (\phi_0(x) - \phi(x))(f_1(x) - k f_0(x))$.

We analyze the sign of $U(x)$:

* If $f_1(x) - k f_0(x) > 0 \implies \Lambda(x) > k$, then $\phi_0(x) = 1$. Since $\phi(x) \le 1$, $\phi_0(x) - \phi(x) \ge 0$. Thus $U(x) \ge 0$.
* If $f_1(x) - k f_0(x) < 0 \implies \Lambda(x) < k$, then $\phi_0(x) = 0$. Since $\phi(x) \ge 0$, $\phi_0(x) - \phi(x) \le 0$. Thus $U(x) \ge 0$.
* If $f_1(x) - k f_0(x) = 0$, then $U(x) = 0$.

Therefore, $U(x) \ge 0$ for all $x$. Integrating $U(x)$:

$$
\int U(x) dx = \int (\phi_0 - \phi)(f_1 - k f_0) dx \ge 0
$$

Expanding the integral:

$$
\int \phi_0 f_1 - \int \phi f_1 - k \left( \int \phi_0 f_0 - \int \phi f_0 \right) \ge 0
$$

$$
E_{\theta_1}[\phi_0] - E_{\theta_1}[\phi] - k (E_{\theta_0}[\phi_0] - E_{\theta_0}[\phi]) \ge 0
$$

Since $E_{\theta_0}[\phi_0] = \alpha$ and $E_{\theta_0}[\phi] \le \alpha$, the term $(E_{\theta_0}[\phi_0] - E_{\theta_0}[\phi]) \ge 0$. Given $k \ge 0$:

$$
E_{\theta_1}[\phi_0] - E_{\theta_1}[\phi] \ge 0 \implies \text{Power}(\phi_0) \ge \text{Power}(\phi)
$$

**Proof of (b) Existence:**
Let $G(k) = P_{\theta_0}(\Lambda(X) \le k)$. $G(k)$ is the cumulative distribution function of the random variable $\Lambda(X)$, so it is non-decreasing.
We seek $k_0$ such that $1 - G(k_0) \approx \alpha$.
Because of discrete jumps, we might not hit $\alpha$ exactly.
We choose $k_0$ such that:

$$
P_{\theta_0}(\Lambda(X) > k_0) \le \alpha \le P_{\theta_0}(\Lambda(X) \ge k_0)
$$

Set $\gamma_0 = \frac{\alpha - P_{\theta_0}(\Lambda(X) > k_0)}{P_{\theta_0}(\Lambda(X) = k_0)}$.
:::

```{tikz fig-np-optimality}
%| fig-cap: "Visualization of the Optimality Proof. The LRT region $C_\\alpha$ contains all points where the likelihood ratio is high ($>k$). Any competitor $C_\\alpha^*$ works by swapping a 'Good' region (A) for a 'Bad' region (B). The integral of U(x) sums the loss from A and the deficit from B, showing both lead to power loss."
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 80% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{patterns}"

\begin{tikzpicture}[scale=1.2]
% Define Curve Functions (approximations for visual)
\def\fnull{2.5*exp(-0.5*(\x+1.5)^2)} % H0 centered left
\def\falt{2.5*exp(-0.5*(\x-1.5)^2)}  % H1 centered right (Renamed from \fi to avoid clash)

% Draw Axes
\draw[->] (-4,0) -- (4,0) node[right] {$x$};
\draw[->] (0,0) -- (0,3) node[above] {Density};

% Draw Densities
\draw[blue, thick] plot[domain=-4:4, samples=100] (\x, {\fnull});
\node[blue] at (-2, 1.5) {$f_0(x)$};

\draw[red, thick] plot[domain=-4:4, samples=100] (\x, {\falt});
\node[red] at (2, 1.5) {$f_1(x)$};

% Cutoff k (intersection roughly at 0 for these params, let's shift slightly for visual)
\def\xc{0.2} % Cutoff point
\draw[dashed, thick] (\xc, 0) -- (\xc, 2.8);
\node[above] at (\xc, 2.8) {$k$ (cutoff)};

% Region definitions
% C_alpha (LRT) is everything to the right of xc
% C_alpha* (Competitor) "misses" a chunk (A) and "adds" a chunk (B)

% Region A: The part of C_alpha that C_alpha* excludes (The Loss)
% Let's say C* excludes [0.2, 1.0]
\fill[pattern=north west lines, pattern color=purple] (0.2,0) -- plot[domain=0.2:1.0] (\x, {\falt}) -- (1.0,0) -- cycle;
\node[purple, align=center] at (0.6, 1.8) {\textbf{Region A}\\(In $C_\alpha$, not $C_\alpha^*$)\\$f_1 > k f_0$};
\draw[->, purple] (0.6, 1.5) -- (0.6, 0.5);

% Region B: The part C_alpha* includes that C_alpha doesn't (The Bad Gain)
% Let's say C* includes [-1.5, -0.7]
\fill[pattern=north east lines, pattern color=orange] (-1.5,0) -- plot[domain=-1.5:-0.7] (\x, {\falt}) -- (-0.7,0) -- cycle;
\node[orange, align=center] at (-1.1, 1.8) {\textbf{Region B}\\(In $C_\alpha^*$, not $C_\alpha$)\\$f_1 < k f_0$};
\draw[->, orange] (-1.1, 1.5) -- (-1.1, 0.5);

% Labels for the Rejection Sets on Axis
\draw[|-|, thick, purple] (0.2, -0.4) -- (4, -0.4);
\node[purple, below] at (2.1, -0.4) {$C_\alpha$ (LRT)};

\draw[thick, orange] (-1.5, -0.8) -- (-0.7, -0.8);
\draw[->, thick, orange] (1.0, -0.8) -- (4, -0.8); % The part of C_alpha it kept
\node[orange, below] at (0, -0.8) {$C_\alpha^*$ (Competitor)};

\end{tikzpicture}
```

```{tikz fig-mlr-equivalence}
%| fig-cap: "Equivalence of Likelihood Ratio Test and thresholding the statistic T(x) under Monotone Likelihood Ratio (MLR). Because $\Lambda(x)$ is strictly increasing in $T(x)$, the condition $\Lambda(x) > k$ is equivalent to $T(x) > t_0$."
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 70% !important;"'

\begin{tikzpicture}[scale=1.2]
% Axes
\draw[->] (0,0) -- (6,0) node[right] {$T(x)$ (Statistic)};
\draw[->] (0,0) -- (0,4.5) node[above] {$\Lambda(x) = \frac{L(\theta_1)}{L(\theta_0)}$};

% Monotone Increasing Function
\draw[blue, very thick] (0.5, 0.5) .. controls (2, 1) and (3, 3.5) .. (5.5, 4.2) node[right] {$\Lambda(t)$};

% Threshold k (Horizontal Line)
\draw[red, thick, dashed] (0, 2.8) -- (5.5, 2.8) node[right] {threshold $k$};

% Drop down to t_0
% Approximate intersection is around x=2.6 based on control points
% Let's force the visual intersection to be nice
\coordinate (Intersection) at (2.65, 2.8);
\draw[dashed] (Intersection) -- (2.65, 0);
\node[below] at (2.65, 0) {$t_0$};

% Rejection Regions
\draw[->, red, thick] (2.65, 0.2) -- (5.5, 0.2);
\node[red, below] at (4.1, 0.2) {Reject $H_0$ ($T(x) > t_0$)};

\draw[->, red, thick] (2.8, 2.9) -- (2.8, 4.0);
\node[red, right] at (2.8, 3.5) {$\Lambda(x) > k$};

% Equivalence Text
\node[align=center, fill=white, inner sep=2pt] at (1.5, 3.5) {
    \textbf{Equivalence}:\\
    $\Lambda(x) > k \iff T(x) > t_0$
};

\end{tikzpicture}
```

## Uniformly Most Powerful (UMP) Tests

When the alternative hypothesis is composite ($H_1: \theta \in \Theta_1$), we seek a test that is "best" for *all* $\theta \in \Theta_1$.

::: {#def-ump}
### Uniformly Most Powerful Test
A test $\phi_0(x)$ of size $\alpha$ is **Uniformly Most Powerful (UMP)** if:

1.  $E_{\theta}[\phi_0(X)] \le \alpha$ for all $\theta \in \Theta_0$.
2.  For any other test $\phi(x)$ satisfying (1), $E_{\theta}[\phi_0(X)] \ge E_{\theta}[\phi(X)]$ for all $\theta \in \Theta_1$.
:::

```{tikz fig-ump-power}
%| fig-cap: "Power functions of UMP test vs. another test"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 70% !important;"'

\begin{tikzpicture}
\draw[->] (0,0) -- (6,0) node[right] {$\theta$};
\draw[->] (0,0) -- (0,4) node[above] {Power $W(\theta)$};

\draw[dashed] (3,0) -- (3,4);
\node at (3,-0.3) {$\theta_0$};
\node at (1.5, -0.3) {$\Theta_0$};
\node at (4.5, -0.3) {$\Theta_1$};

\draw[thick, blue] (0,0.5) .. controls (2,0.6) and (3,1.5) .. (6,3.5) node[right] {$\phi_0$ (UMP)};
\draw[thick, red] (0,0.8) .. controls (2,0.9) and (3,1.5) .. (6,2.5) node[right] {$\phi$};

\draw[dotted] (0,1.5) -- (3,1.5);
\node at (-0.3, 1.5) {$\alpha$};

\end{tikzpicture}
```

## Monotone Likelihood Ratio (MLR)

::: {#def-mlr}
### Monotone Likelihood Ratio
A family of densities $\{f(x; \theta)\}$ has a **Monotone Likelihood Ratio (MLR)** with respect to a statistic $T(x)$ if for any $\theta_1 > \theta_0$, the ratio:

$$
\frac{f(x; \theta_1)}{f(x; \theta_0)}
$$

is a non-decreasing function of $T(x)$.
:::

Common examples include the one-parameter Exponential Family:
$f(x; \theta) = h(x) c(\theta) \exp\{w(\theta) T(x)\}$.
If $w(\theta)$ is increasing, the family has MLR w.r.t $T(x)$.

### Karlin-Rubin Theorem {.unnumbered}
::: {#thm-karlin-rubin}
### Karlin-Rubin Theorem 
Suppose $X$ has a distribution from a family with MLR with respect to $T(X)$, and the distribution of $T(X)$ is continuous.
Consider testing $H_0: \theta \le \theta_0$ vs $H_1: \theta > \theta_0$.

The test:

$$
\phi(x) = \begin{cases} 
1 & \text{if } T(x) > t_0 \\
0 & \text{if } T(x) \le t_0
\end{cases}
$$

where $t_0$ is determined by $P_{\theta_0}(T(X) > t_0) = \alpha$, is the UMP size $\alpha$ test.
:::

::: {.proof}
### Proof of Theorem 4.2 (UMP for MLR Families)

**The Test:**
Define the test $\phi(x)$ as:
$$
\phi(x) = \begin{cases} 
1 & \text{if } T(x) > t_0 \\
0 & \text{if } T(x) \le t_0
\end{cases}
$$
where $t_0$ is determined such that the power at the boundary is $\alpha$, i.e., $W_{LR}(\theta_0) = \alpha$.

**(1) Application of Neyman-Pearson Lemma**
Because the family has a Monotone Likelihood Ratio (MLR) in $T(x)$, for any specific alternative $\theta_1 > \theta_0$, the likelihood ratio $\Lambda(x)$ is an increasing function of $T(x)$.
Therefore, the rejection region $T(x) > t_0$ corresponds to $\Lambda(x) > k$.
By the Neyman-Pearson Lemma, $\phi(x)$ is the Most Powerful (MP) test for testing $H_0': \theta = \theta_0$ vs $H_1': \theta = \theta_1$.

**(2) Monotonicity of the Power Function**
We claim that $W_{LR}(\theta)$ is a non-decreasing function of $\theta$.

*Proof:*
Let $\theta_2 < \theta_1$. Consider testing $\theta = \theta_2$ vs $\theta = \theta_1$.
Let $\beta = W_{LR}(\theta_2)$.
Define a constant dummy test $\phi^*(x) = \beta$ for all $x$. The power of this test is constant: $W_{\phi^*}(\theta) = \beta$.
Since $\phi(x)$ corresponds to the likelihood ratio test form (reject for large $T$) for $\theta_2$ vs $\theta_1$, it is the MP test of size $\beta$.
By the optimality of the NP Lemma, the power of $\phi$ at $\theta_1$ must be at least the power of the competitor $\phi^*$:
$$
W_{LR}(\theta_1) \ge W_{\phi^*}(\theta_1) = \beta = W_{LR}(\theta_2)
$$
Thus, $W_{LR}(\theta)$ is non-decreasing.

**(3) Size Control**
Since $W_{LR}(\theta)$ is non-decreasing and we set $W_{LR}(\theta_0) = \alpha$:
$$
W_{LR}(\theta) \le W_{LR}(\theta_0) = \alpha \quad \text{for all } \theta \le \theta_0
$$
This proves the test satisfies the size constraint for the composite null $H_0: \theta \le \theta_0$.

**(4) UMP Property**
Let $\phi'(x)$ be any other test with size $\le \alpha$ for $H_0: \theta \le \theta_0$. This implies $W_{\phi'}(\theta_0) \le \alpha$.
For any specific $\theta_1 > \theta_0$, we treat the problem as testing $\theta_0$ vs $\theta_1$.
Since $\phi(x)$ is the MP test for $\theta_0$ vs $\theta_1$ (from Step 1), and $\phi'$ is a valid competitor (size $\le \alpha$ at $\theta_0$), we have:
$$
W_{LR}(\theta_1) \ge W_{\phi'}(\theta_1)
$$
Since this holds for all $\theta_1 > \theta_0$, $\phi(x)$ is the Uniformly Most Powerful (UMP) test.
:::




```{tikz fig-mlr-dist}
%| fig-cap: "Distribution of statistic T under H0 and H1 with MLR"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 70% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{patterns}"

\begin{tikzpicture}
\draw[->] (0,0) -- (6,0) node[right] {$t$};
\draw[->] (0,0) -- (0,3);

\draw[thick, blue] plot [smooth, tension=0.7] coordinates {(0,0) (1,2.5) (2,0.5) (5,0.1)};
\node[blue] at (1, 2.7) {$f(t|\theta_0)$};

\draw[thick, red] plot [smooth, tension=0.7] coordinates {(0,0) (2,0.5) (3,2.5) (5,0.2)};
\node[red] at (3, 2.7) {$f(t|\theta_1)$};

\draw[dashed] (3.5, 0) -- (3.5, 2);
\node at (3.5, -0.3) {$t_0$};

\fill[pattern=north east lines, pattern color=black] (3.5,0) -- plot[smooth, tension=0.7] coordinates {(3.5, 0.1) (5,0.1)} -- (5,0) -- cycle;
\node at (4.2, 0.5) {$\alpha$};

\end{tikzpicture}
```


::: {#exm-ump-gamma}
### UMP Test for Exponential/Gamma
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$ with pdf $f(x) = \frac{1}{\theta} e^{-x/\theta}$.
Test $H_0: \theta = \theta_0$ vs $H_1: \theta > \theta_0$.
The sum $T = \sum X_i$ is a sufficient statistic, and $T \sim \text{Gamma}(n, \theta)$.

The Likelihood Ratio for $\theta_1 > \theta_0$ is:

$$
\frac{L(\theta_1)}{L(\theta_0)} = \frac{\theta_1^{-n} e^{-\sum x_i / \theta_1}}{\theta_0^{-n} e^{-\sum x_i / \theta_0}} = \left(\frac{\theta_0}{\theta_1}\right)^n \exp \left\{ \left( \frac{1}{\theta_0} - \frac{1}{\theta_1} \right) \sum x_i \right\}
$$

Since $\theta_1 > \theta_0$, the term $(\frac{1}{\theta_0} - \frac{1}{\theta_1})$ is positive. Thus, $\Lambda(x)$ is an increasing function of $\sum x_i$.
Rejecting for large $\Lambda(x)$ is equivalent to rejecting for $\sum x_i > C$.

This test form does not depend on the specific $\theta_1$, so it is UMP for all $\theta > \theta_0$.
:::


### Note on Two-Sided Hypotheses
For testing $H_0: \theta = \theta_0$ vs $H_1: \theta \neq \theta_0$ (e.g., in a Normal distribution), a UMP test generally **does not exist**. This is because the "best" rejection region for $\theta > \theta_0$ (right tail) is completely different from the "best" region for $\theta < \theta_0$ (left tail).