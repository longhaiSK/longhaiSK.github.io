---
title: "Uniformly Minimum Variance Estimators"
engine: knitr
format: 
  html: default
  pdf: default
---



## Sufficiency and Completeness

### Sufficient Statistics


::: {#def-sufficient-statistic}

### Sufficient Statistic
A statistic $T=T(X)$ is sufficient for $\theta$ if one of the following conditions holds:

1.  **Factorization Theorem:**
    $$
    f(x;\theta) = h(x) g(t(x);\theta)
    $$
    Where $h(x)$ is irrelevant to $\theta$.

2.  **Likelihood Ratio Theorem:**
    For any pair of data sets $x$ and $x'$ such that $t(x)=t(x')$, the ratio
    $$
    \Lambda_x(\theta_1, \theta_2) = \frac{f(x, \theta_2)}{f(x, \theta_1)}
    $$
    is independent of the specific realization $x$ (it depends only on $t(x)$). Specifically, $\Lambda_x(\theta_1, \theta_2) = \Lambda_{x'}(\theta_1, \theta_2)$.

3.  **Conditional Distribution:**
    The conditional distribution of $X$ given $T(X)=t$, denoted as $f(x|t(x), \theta)$, is independent of $\theta$.
    $$
    f(x|t(x), \theta) = f(x|t(x))
    $$

:::

::: {.proof}
**Proof of Equivalence (1 $\Rightarrow$ 2):**

Suppose $t(x) = t(x')$. From the Factorization Theorem:

$$
f(x;\theta) = h(x)g(t(x),\theta)
$$

The ratio becomes:

$$
\Lambda_x(\theta_1, \theta_2) = \frac{h(x)g(t(x), \theta_2)}{h(x)g(t(x), \theta_1)} = \frac{g(t(x), \theta_2)}{g(t(x), \theta_1)}
$$

Similarly for $x'$:

$$
\Lambda_{x'}(\theta_1, \theta_2) = \frac{g(t(x'), \theta_2)}{g(t(x'), \theta_1)}
$$

Since $t(x) = t(x')$, $\Lambda_x(\theta_1, \theta_2) = \Lambda_{x'}(\theta_1, \theta_2)$.

:::

::: {#exm-uniform-sufficiency}

### Uniform Distribution $U(\theta-1, \theta+1)$
Consider a random sample $X_1, \dots, X_n$ from a Uniform distribution with range $(\theta-1, \theta+1)$.

The density for a single observation is:

$$
f(x_i|\theta) = \frac{1}{(\theta+1) - (\theta-1)} I(\theta-1 < x_i < \theta+1) = \frac{1}{2} I(\theta-1 < x_i < \theta+1)
$$

The joint PDF (likelihood) is:

$$
L(\theta; x) = \prod_{i=1}^n \frac{1}{2} I(\theta-1 < x_i < \theta+1)
$$

$$
L(\theta; x) = 2^{-n} \cdot I( \min(x_i) > \theta-1 ) \cdot I( \max(x_i) < \theta+1 )
$$

Using order statistics notation where $X_{(1)} = \min(X_i)$ and $X_{(n)} = \max(X_i)$:

$$
L(\theta; x) = 2^{-n} \cdot I( \theta < X_{(1)} + 1 ) \cdot I( \theta > X_{(n)} - 1 )
$$

$$
L(\theta; x) = 2^{-n} \cdot I( X_{(n)} - 1 < \theta < X_{(1)} + 1 )
$$

By the **Factorization Theorem**, we can define:

* $h(x) = 2^{-n}$ (or simply 1, grouping constants into $g$)
   * $g(T(x), \theta) = I( X_{(n)} - 1 < \theta < X_{(1)} + 1 )$

Thus, the sufficient statistic is the pair of order statistics:
$$
T(X) = (X_{(1)}, X_{(n)})
$$

:::


::: {#exm-gamma-sufficiency}

### Gamma Distribution
Let $X_1, \dots, X_n$ be i.i.d. $\Gamma(\alpha, \beta)$. The pdf is:

$$
f(x|\alpha, \beta) = \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1} e^{-x/\beta}, \quad x > 0
$$

The joint likelihood is:

$$
L(\alpha, \beta; x) = \left( \frac{1}{\Gamma(\alpha)\beta^\alpha} \right)^n \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left( -\frac{1}{\beta} \sum_{i=1}^n x_i \right)
$$

By the Factorization Theorem, we can identify the parts that depend on the data and the parameters:
$$
g(T(x), \theta) = \left( \prod_{i=1}^n x_i \right)^{\alpha} \exp\left( -\frac{1}{\beta} \sum_{i=1}^n x_i \right)
$$

Thus, the sufficient statistics are:
$$
T(X) = \left( \prod_{i=1}^n X_i, \sum_{i=1}^n X_i \right)
$$

:::


::: {#exm-exp-family-sufficiency}

### General Exponential Family
Many common distributions (Normal, Poisson, Gamma, Binomial) belong to the Exponential Family. The lecture notes (Page 7) note that if a density can be written in the form:

$$
f(x|\theta) = h(x) c(\theta) \exp\left( \sum_{j=1}^k w_j(\theta) t_j(x) \right)
$$

Then, by the Factorization Theorem, the statistic:

$$
T(X) = \left( \sum_{i=1}^n t_1(x_i), \dots, \sum_{i=1}^n t_k(x_i) \right)
$$

is a sufficient statistic for $\theta$.

For example, if we have $X_1, \dots, X_n \sim \text{Bernoulli}(p)$:

$$
f(x|p) = p^x (1-p)^{1-x} = (1-p) \exp\left( x \ln\left(\frac{p}{1-p}\right) \right)
$$

Here, $t(x) = x$. Therefore, $T(X) = \sum_{i=1}^n X_i$ is sufficient for $p$.

:::


::: {#thm-mss-ratio-implication}

### MSS Condition Theorem
Let $T(X)$ be a **sufficient statistic**. $T(X)$ is a **Minimal Sufficient Statistic (MSS)** if and only if for any pair of data sets $x$ and $y$, the following conditions imply $T(x)=T(y)$:

1.  **Proportional Likelihoods:** $L(\theta; x) = c(x, y) L(\theta; y)$ for all $\theta$.
2.  **Ratio Independence:** $\frac{L(\theta; x)}{L(\theta; y)}$ is constant as a function of $\theta$.
3.  **Equality of Likelihood Ratio Statistics:** $\Lambda_x(\theta_1, \theta_2) = \Lambda_y(\theta_1, \theta_2)$ for all $\theta_1, \theta_2$.

**Logic:**
Since $T$ is already sufficient, we know $T(x)=T(y) \implies \text{Proportional Likelihood Condition}$.
The condition for **minimality** is the reverse: $\text{Proportional Likelihood Condition} \implies T(x)=T(y)$.

:::

::: {.proof}
**Proof**

We assume Conditions (1), (2), and (3) are equivalent descriptions of $x$ and $y$ having proportional likelihoods. We denote "Conditions (1-3)" collectively as the **Proportional Likelihood Condition**.

**Direction 1: $T$ is MSS $\implies$ (Proportional Likelihood Condition $\implies T(x)=T(y)$)**

Assume $T(X)$ is a Minimal Sufficient Statistic. By definition, $T$ is a function of *any* other sufficient statistic $S$.

Let us construct a specific statistic $S^*(X)$ defined by the likelihood partitions:
$$ S^*(x) = \{ z : L(\theta; z) = c \cdot L(\theta; x) \text{ for some constant } c \} $$
(i.e., $S^*(x)$ groups all data points that satisfy the Proportional Likelihood Condition).
$S^*(X)$ is known to be a sufficient statistic.

Since $T$ is MSS, it must be a function of $S^*$. That is, there exists some function $g$ such that $T(x) = g(S^*(x))$.
If the Proportional Likelihood Condition holds for $x$ and $y$, then by definition $S^*(x) = S^*(y)$.
Therefore:
$$ S^*(x) = S^*(y) \implies g(S^*(x)) = g(S^*(y)) \implies T(x) = T(y) $$

**Direction 2: (Proportional Likelihood Condition $\implies T(x)=T(y)$) $\implies$ $T$ is MSS**

Assume that for any $x,y$, if the Proportional Likelihood Condition holds, then $T(x)=T(y)$.
We must show that $T$ is a function of any sufficient statistic $S$.

Let $S(X)$ be *any* sufficient statistic.
By the Factorization Theorem, if $S(x) = S(y)$, then:
$$ L(\theta; x) = h(x)g(S(x); \theta) \quad \text{and} \quad L(\theta; y) = h(y)g(S(y); \theta) $$
Since $S(x)=S(y)$, the term $g(S(x);\theta) = g(S(y);\theta)$.
The ratio is:
$$ \frac{L(\theta; x)}{L(\theta; y)} = \frac{h(x)}{h(y)} $$
This ratio is independent of $\theta$, so the Proportional Likelihood Condition holds.

By our assumption, Proportional Likelihood Condition $\implies T(x)=T(y)$.
So, we have shown:
$$ S(x) = S(y) \implies T(x) = T(y) $$
This implies that $T$ is a function of $S$.
Since $S$ was arbitrary, $T$ is a function of every sufficient statistic.
Therefore, $T$ is Minimal Sufficient.

:::

```{tikz fig-log-likelihood}
%| fig-cap: "Visualizing Proportional Likelihoods (Parallel Log-Likelihoods)"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 70% !important;"'

\begin{tikzpicture}
    % Axes
    \draw[->] (0,0) -- (6,0) node[right] {$\theta$};
    \draw[->] (0,0) -- (0,5) node[above] {$\ell(\theta) = \ln L(\theta)$};

    % Curve 1: l(theta; x) - Concave down (typical log-like shape)
    \draw[thick, blue] (1, 1.5) .. controls (3, 5.5) .. (5, 1.5) node[right] {$\ell(\theta; x)$};
    
    % Curve 2: l(theta; y) - Vertically shifted down by constant k
    % The shape must be identical, just shifted y-1
    \draw[thick, red, dashed] (1, 0.5) .. controls (3, 4.5) .. (5, 0.5) node[right] {$\ell(\theta; y)$};

    % Visualizing the constant vertical difference
    \draw[<->] (3, 3.5) -- (3, 2.5) node[midway, right] {$\ln c$};
    
    % Guide line
    \draw[dotted] (3, 0) -- (3, 2.5);

    % Annotation
    \node at (3, 5) [blue] {Parallel Curves (Constant Difference)};
    \node at (3, -0.8) {Condition: $\ell(\theta; x) - \ell(\theta; y) = \text{constant}$};
\end{tikzpicture}
```
::: {#exm-poisson-mss}

### Poisson Distribution
Let $X_1, \dots, X_n \sim \text{Poisson}(\theta)$.
$T(X) = \sum X_i$ is minimal sufficient.
$S(X) = (\sum X_i, X_1)$ is sufficient but not minimal.

:::

::: {.proof}
**Proof that $S(X) = (\sum X_i, X_1)$ is Not Minimal**

We use the **MSS Condition Theorem** stated above.
For $S(X)$ to be minimal sufficient, the following implication must hold for any pair of data sets $x$ and $y$:
$$ \text{Proportional Likelihood Condition} \implies S(x) = S(y) $$

**Step 1: Evaluate the Proportional Likelihood Condition**
For the Poisson distribution, the likelihood ratio between two data sets $x$ and $y$ is:
$$
\frac{L(\theta; x)}{L(\theta; y)} = \theta^{\left(\sum x_i - \sum y_i\right)} \frac{\prod y_i!}{\prod x_i!}
$$
The **Proportional Likelihood Condition** requires this ratio to be independent of $\theta$.
This holds if and only if the exponent of $\theta$ is zero:
$$ \sum x_i = \sum y_i $$

**Step 2: Test the Implication for $S(X)$**
The requirement becomes:
$$ \sum x_i = \sum y_i \implies \left(\sum x_i, x_1\right) = \left(\sum y_i, y_1\right) $$

**Step 3: Counter-Example**
Consider sample size $n=2$ and two data sets:

* $x = (2, 0)$ where $\sum x_i = 2$ and $x_1 = 2$.
* $y = (1, 1)$ where $\sum y_i = 2$ and $y_1 = 1$.

**Check Proportional Likelihood Condition:**
$\sum x_i = 2$ and $\sum y_i = 2$. The condition holds.

**Check Statistic Equality:**
$$ S(x) = (2, 2) $$
$$ S(y) = (2, 1) $$
$$ S(x) \neq S(y) $$

**Conclusion:**
We have found a case where the Proportional Likelihood Condition holds, but $S(x) \neq S(y)$.
The implication fails. Therefore, $S(X)$ is **not** Minimal Sufficient.

:::


## Completeness

::: {#def-completeness}

### Complete Statistic
A statistic $T$ is said to be **complete** if for any real-valued function $g$,
$$
E[g(T)|\theta] = 0 \quad \text{for all } \theta
$$
implies
$$
P(g(T) = 0 | \theta) = 1 \quad \text{for all } \theta
$$

:::

Significance: If $T$ is complete, then there exists at most one unbiased estimator for $\theta$ that is a function of $T$.

::: {#exm-uniform-incomplete}

### Uniform Distribution (Not Complete)
Let $X_1, \dots, X_n \sim \text{Unif}(\theta-1, \theta+1)$.
The density is:

$$
f(x) = \prod I(\theta-1 < x_i < \theta+1) = I(\theta \in (x_{(n)}-1, x_{(1)}+1))
$$

The statistic $T(X) = (X_{(1)}, X_{(n)})$ is a Minimal Sufficient Statistic. However, it is **not complete**.

Consider the range $R = X_{(n)} - X_{(1)}$. The distribution of $R$ does not depend on $\theta$ (it is an ancillary statistic). Let $g(T) = X_{(n)} - X_{(1)} - c$, where $c = E[X_{(n)} - X_{(1)}]$.
Then $E[g(T)] = 0$ for all $\theta$, but $g(T)$ is not identically zero.

:::

::: {#lem-exponential-complete}

### Exponential Family Completeness
If $T = (T_1, \dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then $T$ is complete.

:::

## UMVUE

::: {#def-umvue}

### Uniformly Minimum Variance Unbiased Estimator (UMVUE)
A statistic $T(x)$ is a UMVUE for $\theta$ if:

1.  $E(T(x)|\theta) = \theta$ for all $\theta$ (Unbiased).
   2.  $Var(T(x)|\theta) \le Var(d(x)|\theta)$ for all $\theta$ and for all other unbiased estimators $d(x)$.

:::

The relationship between statistics types is visualized below:

```{tikz fig-stats-hierarchy}
%| fig-cap: "Hierarchy of Statistics"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 50% !important;"'

\begin{tikzpicture}
  \draw (0,0) circle (2.5cm);
  \node at (0, 1.8) {Sufficient};
  
  \draw (0,-0.3) circle (1.8cm);
  \node at (0, 0.8) {Minimal SS};
  
  \draw (0,-0.8) circle (1.0cm);
  \node at (0, -0.8) {Complete};
\end{tikzpicture}

```

::: {#thm-lehmann-scheffe}

### Lehmann-Scheffe Theorem
If $T$ is a complete and sufficient statistic, and there is an unbiased estimator $d(X)$ such that $E[d(X)] = \theta$, then $\phi(T) = E[d(X)|T]$ is the unique UMVUE for $\theta$.

:::

::: {#thm-rao-blackwell}

### Rao-Blackwell Theorem
Given that $T$ is a sufficient statistic and $d_1(x)$ is an unbiased estimator ($E[d_1(x)] = \theta$).
Define $g(T) = E[d_1(x) | T]$. Then:

1.  $g(T)$ is a statistic (free of $\theta$ because $T$ is sufficient).
   2.  $E[g(T)] = \theta$ (Unbiased).
3.  $Var(g(T)) \le Var(d_1(x))$.

:::

::: {.proof}
**Proof of Rao-Blackwell:**

1.  Since $T$ is sufficient, the conditional distribution $X|T$ is independent of $\theta$, so $g(T)$ is a valid statistic.
   2.  By the Law of Iterated Expectations:
    $$
    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \theta
    $$

3.  By the variance decomposition formula:
    $$
    Var(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]
    $$
    $$
    Var(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]
    $$
    Since $(d_1(X) - g(T))^2 \ge 0$, we have $Var(g(T)) \le Var(d_1(X))$.

:::

## Methods for Finding UMVUE

To find the UMVUE for a parameter $\theta$:

1.  **Find a Complete Sufficient Statistic:**
    Identify $T$ which is complete and sufficient for $\theta$ (often using the Exponential Family properties).

2.  **Find an Unbiased Estimator:**
    Find any simple statistic $d(X)$ such that $E[d(X)] = \theta$.

3.  **Rao-Blackwellize:**
    Compute $g(T) = E[d(X)|T]$. The result $g(T)$ is the UMVUE.

::: {#exm-poisson-umvue}

### Poisson UMVUE
Let $X_1, \dots, X_n \sim \text{Poisson}(\lambda)$. Find the UMVUE for $\lambda$ and $\lambda^2$.

1. For $\lambda$:
$T = \sum X_i$ is a complete sufficient statistic (Poisson is exponential family).
Let $d_1(X) = X_1$.
$E[X_1] = \lambda$.
We compute $g(T) = E[X_1 | T]$.
Since the conditional distribution of $X_1$ given $T=t$ is Binomial($t, 1/n$):

$$
E[X_1 | T] = t \cdot \frac{1}{n} = \frac{T}{n} = \bar{X}
$$

Thus, $\bar{X}$ is the UMVUE for $\lambda$.

2. For $\lambda^2$:
We know $Var(X_1) = \lambda = E(X_1^2) - (E(X_1))^2$.
So $E(X_1^2) - \lambda = \lambda^2$, which implies $E(X_1^2 - X_1) = \lambda^2$.
Let $d_2(X) = X_1^2 - X_1$. This is an unbiased estimator for $\lambda^2$.

We calculate $g(T) = E[X_1^2 - X_1 | T]$.

$$
g(T) = E[X_1^2 | T] - E[X_1 | T]
$$

Using the second moment of the Binomial distribution $Bin(T, 1/n)$:
$E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \frac{1}{n}(1-\frac{1}{n}) + (\frac{T}{n})^2$.

$$
g(T) = \left[ \frac{T}{n} - \frac{T}{n^2} + \frac{T^2}{n^2} \right] - \frac{T}{n} = \frac{T^2 - T}{n^2} = \frac{T(T-1)}{n^2}
$$

Thus, $\frac{T(T-1)}{n^2}$ is the UMVUE for $\lambda^2$.

:::