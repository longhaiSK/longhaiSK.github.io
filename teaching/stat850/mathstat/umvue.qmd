---
title: "Uniformly Minimum Variance Estimators"
engine: knitr
format: 
  html: default
  pdf: default
---

## Sufficiency and Completeness

In the context of statistical inference, we consider the likelihood function $L(\theta; x) = f(x, \theta)$, where $\theta$ is the variable and $x$ is fixed.

::: {#exm-normal-likelihood}

### Normal Distribution Likelihood
Suppose $X_{1}, \dots, X_{n} | \mu, \sigma^{2} \sim N(\mu, \sigma^{2})$. The data is represented as $x = (x_1, \dots, x_n)$ and the parameter vector is $\theta = (\mu, \sigma^{2})$. The likelihood function is given by:

$$ 
L(\theta; x) = f(x | \theta) = (2\pi)^{-\frac{n}{2}} (\sigma^{2})^{-\frac{n}{2}} \exp\left\{ -\frac{1}{2\sigma^{2}} \sum (x_i - \mu)^2 \right\} 
$$

This can be rewritten to show it is determined by $\bar{x}$ and $s^2$.

:::

::: {#def-sufficient-statistic}

### Sufficient Statistic
A statistic $T = T(x)$ is sufficient for $\theta$ if one of the following conditions is met:

* Factorization Criterion:
  The joint density can be factored as $f(x | \theta) = h(x) g(T(x), \theta)$, where $h(x)$ is independent of $\theta$.

* Likelihood Ratio Theorem:
  For any pair of data sets $x$ and $x'$ such that $T(x) = T(x')$, the ratio $\frac{f(x, \theta_2)}{f(x, \theta_1)} = \frac{f(x', \theta_2)}{f(x', \theta_1)}$ for all $\theta_1, \theta_2$.

* Conditional Distribution:
  The conditional distribution of the data given the statistic, $f(x | T(x), \theta)$, is independent of $\theta$.

:::


## Minimal Sufficient Statistics

::: {#def-minimal-sufficient}

### Minimal Sufficient Statistics (MSS)
A sufficient statistic $T(X)$ is minimal sufficient if $T(X)$ is a function of any other sufficient statistic $S$. That is, for any sufficient statistic $S$, there exists a function $g$ such that $T(x) = g(S)$.

:::

::: {#thm-mss-ratio}

### Theorem 6.1: Ratio Characterization of MSS
$T(x)$ is minimal sufficient if and only if for any pair $x$ and $x'$, $T(x) = T(x')$ is equivalent to the ratio $\frac{f(x, \theta_2)}{f(x, \theta_1)}$ being equal to $\frac{f(x', \theta_2)}{f(x', \theta_1)}$ for all $\theta_1, \theta_2$.

:::

## Completeness

::: {#def-complete-statistic}

### Complete Statistic
A statistic $T$ is said to be complete if for every real-valued function $g$, $E[g(T) | \theta] = 0$ for all $\theta$ implies $P(g(T) = 0 | \theta) = 1$ for all $\theta$.

:::

If a statistic is complete, then an unbiased estimator of $\theta$ based on that statistic is unique.

::: {#thm-exponential-family-completeness}

### Lemma 6.3: Exponential Family Completeness
If $T = (T_1, \dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in its parameter space, then $T$ is complete.

:::

::: {.proof}
The density of $T$ follows the form $f(t_1, \dots, t_k | \theta) = C(\theta) h(t) \exp(\sum \theta_i t_i)$. Suppose $E[g(T) | \theta] = 0$:

$$ 
\int g(t) C(\theta) h(t) \exp\left( \sum \theta_i t_i \right) dt = 0 
$$

This integral represents a Laplace transform. Since the transform is zero over an open set, the function $g(t)h(t)$ must be zero almost everywhere, implying $g(T) = 0$ almost surely.

:::

## Uniformly Minimum Variance Unbiased Estimators (UMVUE)

::: {#def-umvue}

### UMVUE
A statistic $T(x)$ is a Uniformly Minimum Variance Unbiased Estimator for $\tau(\theta)$ if:

* It is unbiased: $E[T(x) | \theta] = \tau(\theta)$ for all $\theta$.

* It has minimum variance: $Var(T(x) | \theta) \leq Var(d(x) | \theta)$ for all $\theta$, where $d(x)$ is any other unbiased estimator of $\tau(\theta)$.

:::

::: {#thm-rao-blackwell}

### Theorem 6.3: Rao-Blackwell Theorem
Let $d_1(x)$ be an unbiased estimator of $\theta$ and $T$ be a sufficient statistic. Let $g(T) = E[d_1(x) | T]$. Then:

1. $g(T)$ is a statistic (independent of $\theta$).

2. $E[g(T) | \theta] = \theta$.

3. $Var(g(T) | \theta) \leq Var(d_1(x) | \theta)$.

:::

::: {.proof}
Since $T$ is sufficient, the conditional distribution of $X$ given $T$ does not depend on $\theta$, making $E[d_1(x) | T]$ a valid statistic. By the law of total expectation, $E[E[d_1(x) | T]] = E[d_1(x)] = \theta$. By Jensen's Inequality or the law of total variance, the variance of the conditional expectation is less than or equal to the total variance.

:::

### Lehmann-Scheff√© Theorem

If $T$ is a sufficient and complete statistic, then $g(T) = E[d(X) | T]$ is the unique UMVUE.

### Methods for finding UMVUE

1. Find a complete sufficient statistic $T$ for $\theta$.

2. Find any unbiased estimator $d(x)$ such that $E[d(x) | \theta] = \theta$.

3. Calculate $g(T) = E[d(x) | T]$. This result is the UMVUE.

::: {#exm-poisson-umvue}

### UMVUE for Poisson Mean
Let $X_1, \dots, X_n \sim \text{Poisson}(\lambda)$.

1. $T = \sum X_i$ is a complete sufficient statistic.

2. Let $d(X) = X_1$. Since $E[X_1] = \lambda$, it is an unbiased estimator.

3. Find $E[X_1 | T]$. Since $X_1 | \sum X_i = k \sim \text{Binomial}(k, 1/n)$, we have:

$$ 
E[X_1 | T] = \frac{T}{n} = \bar{X} 
$$

Thus, $\bar{X}$ is the UMVUE for $\lambda$.

:::