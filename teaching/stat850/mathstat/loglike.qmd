---
title: Likelihood Theory
engine: knitr
format: 
  html: default
  pdf: default
---

## Definitions and Notations

### Regular Family

::: {#def-regular-family}


### Regular Families
A family of probability density functions is said to be a **Regular Family** if the support $\{\mathbf{x} : f(\mathbf{x}|\boldsymbol{\theta}) > 0\}$ does not depend on the parameter vector $\boldsymbol{\theta}$.

This condition allows for the interchange of differentiation and integration:
$$
\nabla_{\boldsymbol{\theta}} \int \exp\{\ell(\boldsymbol{\theta}; \mathbf{x})\} d\mathbf{x} = \int \nabla_{\boldsymbol{\theta}} \exp\{\ell(\boldsymbol{\theta}; \mathbf{x})\} d\mathbf{x}
$$

:::

### Score and Fisher Information

Before stating the theorem, we define the following notations for the score and information in the context of a parameter vector $\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)^T \in \mathbb{R}^p$:

:::{#def-score-fisher}

1.  **Score Vector ($\mathbf{U}$):** The gradient of the log-likelihood. It is a random column vector of dimension $p \times 1$.
    $$
    \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) = \nabla \ell(\boldsymbol{\theta}; \mathbf{X}) = \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \boldsymbol{\theta}} = 
    \begin{bmatrix}
    \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_1} \\[6pt]
    \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_2} \\[6pt]
    \vdots \\[6pt]
    \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_p}
    \end{bmatrix}
    $$

2.  **Observed Information Matrix ($\mathbf{J}$):** The negative Hessian of the log-likelihood. It is a symmetric random matrix of dimension $p \times p$, measuring the curvature of the log-likelihood surface.
    $$
    \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) = - \nabla^2 \ell(\boldsymbol{\theta}; \mathbf{X}) = - \frac{\partial^2 \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^T} = -
    \begin{bmatrix}
    \frac{\partial^2 \ell}{\partial \theta_1^2} & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_p} \\[8pt]
    \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_2^2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_p} \\[8pt]
    \vdots & \vdots & \ddots & \vdots \\[8pt]
    \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_p \partial \theta_2} & \cdots & \frac{\partial^2 \ell}{\partial \theta_p^2}
    \end{bmatrix}
    $$

3.  **(Expected) Fisher Information Matrix ($\mathbf{I}$):** The covariance matrix of the score vector. It is a deterministic $p \times p$ matrix (for a fixed $\boldsymbol{\theta}$).
    $$
    \mathbf{I}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}} \left[ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) \right]
    $$

:::

## Mean and Covariance of Score Vector

::: {#thm-score-identities}

## Bartlett's Identities: Mean and Covariance of Score Vector

Let $\{f(\mathbf{x}|\boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}$ be a regular family of probability density functions. The following identities hold relating the moments of the score vector $\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})$ and the observed information matrix $\mathbf{J}(\boldsymbol{\theta}; \mathbf{X})$:

1.  **First Moment Identity:** The expected score is zero vector.
    $$
    E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{0}
    $$

2.  **Second Moment Identity:** The expected observed information equals the covariance of the score vector (Fisher Information).
    $$
    \text{Cov}_{\boldsymbol{\theta}} \left( \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \right) = E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{I}(\boldsymbol{\theta})
    $$

:::

:::{#rem-score-moments}
The only assumption in the theorem above is that the families are regular. Therefore, we do not need to assume the log-likelihood $\ell(\boldsymbol{\theta})$ is "well-behaved" (e.g., approximately quadratic or independence within $\mathbf{X}$) for these two identities to hold.

:::

::: {.proof}

1. Proof of the First Moment Identity

We start with the fundamental property that a density function integrates to 1 over the sample space of $\mathbf{X}$:
$$
\int f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = 1
$$
Differentiating both sides with respect to the parameter vector $\boldsymbol{\theta}$:
$$
\nabla_{\boldsymbol{\theta}} \int f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
$$
Assuming regularity allows us to interchange differentiation and integration:
$$
\int \nabla_{\boldsymbol{\theta}} f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
$$
Using the identity $\nabla_{\boldsymbol{\theta}} f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) \nabla_{\boldsymbol{\theta}} \log f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})$:
$$
\int \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
$$
This is precisely the definition of the expectation:
$$
E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{0}
$$

2. Proof of the Second Moment Identity

We differentiate the result of the First Moment Identity ($E[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]=\mathbf{0}$) with respect to $\boldsymbol{\theta}^T$.
$$
\nabla_{\boldsymbol{\theta}^T} \int \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) \, d\mathbf{x} = \mathbf{0}
$$
Applying the product rule inside the integral (remembering $\mathbf{U}$ is a vector):
$$
\int \left[ \left( \nabla_{\boldsymbol{\theta}^T} \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \right) f(\mathbf{x}|\boldsymbol{\theta}) + \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \left( \nabla_{\boldsymbol{\theta}^T} f(\mathbf{x}|\boldsymbol{\theta}) \right) \right] d\mathbf{x} = \mathbf{0}
$$
We analyze the two terms in the bracket:

* **Term 1:** $\nabla_{\boldsymbol{\theta}^T} \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})$ is the Jacobian of the score, which is the Hessian of the log-likelihood, $\nabla^2 \ell(\boldsymbol{\theta}; \mathbf{x})$. By definition, this is $-\mathbf{J}(\boldsymbol{\theta}; \mathbf{x})$.
* **Term 2:** We use the identity $\nabla_{\boldsymbol{\theta}^T} f(\mathbf{x}|\boldsymbol{\theta}) = f(\mathbf{x}|\boldsymbol{\theta}) (\nabla_{\boldsymbol{\theta}} \log f(\mathbf{x}|\boldsymbol{\theta}))^T = f(\mathbf{x}|\boldsymbol{\theta}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})^T$.

Substituting these back into the integral:
$$
\int \left[ -\mathbf{J}(\boldsymbol{\theta}; \mathbf{x}) f(\mathbf{x}|\boldsymbol{\theta}) + \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{x})^T f(\mathbf{x}|\boldsymbol{\theta}) \right] d\mathbf{x} = \mathbf{0}
$$
This simplifies to expectations:
$$
-E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] + E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T ] = \mathbf{0}
$$
Rearranging gives:
$$
E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = E_{\boldsymbol{\theta}} [ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T ]
$$
Finally, recall the definition of the covariance matrix for a random vector with zero mean. Since $E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})] = \mathbf{0}$, we have:
$$
\text{Cov}_{\boldsymbol{\theta}}(\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})) = E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T] - E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})]^T = E_{\boldsymbol{\theta}}[\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})\mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^T]
$$
Therefore, we conclude:
$$
\text{Cov}_{\boldsymbol{\theta}}(\mathbf{U}(\boldsymbol{\theta}; \mathbf{X}))=E_{\boldsymbol{\theta}} [ \mathbf{J}(\boldsymbol{\theta}; \mathbf{X}) ] = \mathbf{I}(\boldsymbol{\theta})
$$

:::

## Cramer-Rao Lower Bound

In estimation theory, we often wish to know the limit of how well a parameter can be estimated. The following theorem provides a lower bound on the variance of any estimator.

::: {#thm-crlb}

### Cramer-Rao Lower Bound for Scalar Estimator

Let $X$ be a random variable with probability density function (or probability mass function) $f(x|\theta)$, where $\theta \in \Theta$ is a scalar unknown parameter. Let $T(X)$ be any estimator with finite variance, and let $m(\theta) = E_\theta[T(X)]$ denote its expectation.

Assume the following **regularity conditions** hold:

1.  The support of $X$, denoted $\mathcal{X} = \{x : f(x|\theta) > 0\}$, does not depend on $\theta$.

2.  The differentiation with respect to $\theta$ and integration (or summation) with respect to $x$ can be interchanged.

Then, the variance of $T(X)$ satisfies:

$$
\text{Var}_\theta(T(X)) \ge \frac{[m'(\theta)]^2}{I(\theta)}
$$

where $I(\theta) = E_\theta \left[ \left( \frac{\partial}{\partial \theta} \log f(X|\theta) \right)^2 \right]$ is the scalar Fisher Information.

**Particular Case:** If $T(X)$ is an **unbiased** estimator of $\theta$ (i.e., $m(\theta) = \theta$ and $m'(\theta)=1$), then:

$$
\text{Var}_\theta(T(X)) \ge \frac{1}{I(\theta)}
$$

:::

::: {.proof}
Let $U = \frac{\partial}{\partial \theta} \log f(X|\theta)$ be the scalar Score function. From the properties of the Score function under the stated regularity conditions, we know that the score has mean zero and variance equal to the Fisher Information:

$$
E_\theta[U] = 0 \quad \text{and} \quad \text{Var}_\theta(U) = I(\theta)
$$

Consider the covariance between the estimator $T(X)$ and the Score $U$. By the Cauchy-Schwarz inequality (applied to covariance), we have:

$$
[\text{Cov}_\theta(T, U)]^2 \le \text{Var}_\theta(T) \text{Var}_\theta(U)
$$

We now evaluate the covariance term explicitly. By definition:

$$
\begin{aligned}
\text{Cov}_\theta(T, U) &= E_\theta[T(X) U] - E_\theta[T]E_\theta[U] \\
&= E_\theta\left[ T(X) \frac{\partial}{\partial \theta} \log f(X|\theta) \right] - m(\theta) \cdot 0 \\
&= \int T(x) \left( \frac{\partial \log f(x|\theta)}{\partial \theta} \right) f(x|\theta) \, dx \\
&= \int T(x) \left( \frac{1}{f(x|\theta)} \frac{\partial f(x|\theta)}{\partial \theta} \right) f(x|\theta) \, dx \\
&= \int T(x) \frac{\partial f(x|\theta)}{\partial \theta} \, dx
\end{aligned}
$$

Invoking the regularity condition that allows the interchange of derivative and integral, we move the derivative outside the integral:

$$
\text{Cov}_\theta(T, U) = \frac{\partial}{\partial \theta} \int T(x) f(x|\theta) \, dx = \frac{\partial}{\partial \theta} E_\theta[T(X)] = m'(\theta)
$$

Substituting this result and $\text{Var}_\theta(U) = I(\theta)$ back into the covariance inequality:

$$
[m'(\theta)]^2 \le \text{Var}_\theta(T) \cdot I(\theta)
$$

Rearranging the terms yields the desired lower bound:

$$
\text{Var}_\theta(T(X)) \ge \frac{[m'(\theta)]^2}{I(\theta)}
$$

:::


## Multivariate Cramer-Rao Lower Bound

::: {#thm-multivariate-crlb}

### Multivariate Cramer-Rao Lower Bound

Let $\mathbf{X}$ be a random vector with density $f(\mathbf{x}|\boldsymbol{\theta})$, where $\boldsymbol{\theta} \in \mathbb{R}^p$ is a vector of unknown parameters. Let $\mathbf{T}(\mathbf{X}) \in \mathbb{R}^k$ be any estimator with finite covariance matrix, and let $\mathbf{m}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}}[\mathbf{T}(\mathbf{X})]$ denote its expectation vector.

Let $\mathbf{I}(\boldsymbol{\theta})$ be the $p \times p$ Fisher Information Matrix:

$$
\mathbf{I}(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}} \left[ \mathbf{U}(\boldsymbol{\theta}; \mathbf{X}) \mathbf{U}(\boldsymbol{\theta}; \mathbf{X})^\top \right]
$$

Let $\mathbf{D}(\boldsymbol{\theta}) = \frac{\partial \mathbf{m}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}$ be the $k \times p$ Jacobian matrix of the expectation, where $D_{ij} = \frac{\partial m_i}{\partial \theta_j}$.

Under standard regularity conditions, the covariance matrix of $\mathbf{T}$ satisfies the inequality:

$$
\text{Var}_{\boldsymbol{\theta}}(\mathbf{T}) \succeq \mathbf{D}(\boldsymbol{\theta}) [\mathbf{I}(\boldsymbol{\theta})]^{-1} \mathbf{D}(\boldsymbol{\theta})^\top
$$

Here, $\mathbf{A} \succeq \mathbf{B}$ means that the matrix $\mathbf{A} - \mathbf{B}$ is positive semi-definite (i.e., for any vector $\mathbf{v}$, $\mathbf{v}^\top (\mathbf{A} - \mathbf{B}) \mathbf{v} \ge 0$).

:::

::: {.proof}
Let $\mathbf{U} = \nabla_{\boldsymbol{\theta}} \log f(\mathbf{X}|\boldsymbol{\theta})$ be the $p \times 1$ Score vector. We know that $E[\mathbf{U}] = \mathbf{0}$ and $\text{Var}(\mathbf{U}) = \mathbf{I}(\boldsymbol{\theta})$.

Consider the covariance between the estimator $\mathbf{T}$ and the Score $\mathbf{U}$. By an argument similar to the scalar case (interchanging derivative and integral), we find:

$$
\text{Cov}(\mathbf{T}, \mathbf{U}) = E[\mathbf{T} \mathbf{U}^\top] = \mathbf{D}(\boldsymbol{\theta})
$$

Now, define the block vector $\mathbf{Z} = \begin{pmatrix} \mathbf{T} \\ \mathbf{U} \end{pmatrix}$. The covariance matrix of $\mathbf{Z}$ is necessarily positive semi-definite:

$$
\text{Var}(\mathbf{Z}) = \begin{pmatrix} \text{Var}(\mathbf{T}) & \text{Cov}(\mathbf{T}, \mathbf{U}) \\ \text{Cov}(\mathbf{U}, \mathbf{T}) & \text{Var}(\mathbf{U}) \end{pmatrix} = \begin{pmatrix} \Sigma_{\mathbf{T}} & \mathbf{D} \\ \mathbf{D}^\top & \mathbf{I} \end{pmatrix} \succeq 0
$$

For this block matrix to be positive semi-definite, the Schur complement of the block $\mathbf{I}$ must be positive semi-definite (assuming $\mathbf{I}$ is positive definite/invertible):

$$
\Sigma_{\mathbf{T}} - \mathbf{D} \mathbf{I}^{-1} \mathbf{D}^\top \succeq 0
$$

Thus, $\text{Var}(\mathbf{T}) \succeq \mathbf{D} \mathbf{I}^{-1} \mathbf{D}^\top$.

:::

::: {#cor-scalar-crlb}

### Corollary: Scalar Estimator
Consider the case where $T(\mathbf{X})$ is a scalar estimator ($k=1$) for a scalar parameter $\theta$ ($p=1$).

1.  **Matrices to Scalars:** The covariance matrix $\text{Var}(\mathbf{T})$ becomes the scalar variance $\text{Var}(T)$. The Fisher Information matrix $\mathbf{I}(\boldsymbol{\theta})$ becomes the scalar $I(\theta)$.
    
2.  **Jacobian to Derivative:** The Jacobian matrix $\mathbf{D}(\boldsymbol{\theta})$ reduces to the scalar derivative $m'(\theta)$.

Substituting these into the multivariate bound:

$$
\text{Var}(T) - m'(\theta) [I(\theta)]^{-1} m'(\theta) \ge 0
$$

$$
\text{Var}(T) \ge \frac{[m'(\theta)]^2}{I(\theta)}
$$

:::


::: {#rem-crlb-generality}

### Generality of the Lower Bound
The power of the Cramer-Rao Lower Bound lies in its independence from the specific method of estimation. It relies solely on the properties of the underlying probability model (specifically, the curvature of the log-likelihood function) and the bias of the estimator. Consequently, it provides a universal benchmark for precision:

1.  **Fundamental Limit**
    It represents the limit of "extractable information" about $\boldsymbol{\theta}$ contained in the data $\mathbf{X}$. No matter how clever the estimation algorithm is (e.g., Method of Moments, Bayes estimators, etc.), the variance cannot be reduced beyond this intrinsic bound determined by the Fisher Information.

2.  **Efficiency Standard**
    It allows us to define the concept of an *efficient estimator*. Any unbiased estimator that attains this lower bound is the Uniformly Minimum Variance Unbiased Estimator (UMVUE).

3.  **Asymptotic Justification**
    While finite-sample estimators may not always achieve this bound, the Maximum Likelihood Estimator (MLE) is asymptotically efficient. This means that as the sample size $n \to \infty$, the variance of the MLE approaches the CRLB, justifying the popularity of likelihood-based inference.

:::

### Example with Exponential Likelihood

::: {#exm-exponential-crlb}

Let $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{Exp}(\theta)$, where the density is $f(x|\theta) = \frac{1}{\theta} e^{-x/\theta}$. We illustrate the likelihood identities and the efficiency of the sample mean.

1. The Score Function ($U$)
The log-likelihood function is:
$$
\ell(\theta; \mathbf{x}) = \sum_{i=1}^n \left( -\log \theta - \frac{x_i}{\theta} \right) = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^n x_i
$$
The Score function is the first derivative with respect to $\theta$:
$$
U(\theta; \mathbf{x}) = \frac{\partial \ell}{\partial \theta} = -\frac{n}{\theta} + \frac{\sum x_i}{\theta^2}
$$
*Check First Moment:* $E[U] = -\frac{n}{\theta} + \frac{1}{\theta^2} \sum E[X_i] = -\frac{n}{\theta} + \frac{n\theta}{\theta^2} = 0$. (Verified)

2. Fisher Information ($I(\theta)$)
We calculate the information using two different definitions to verify Bartlett's identity.

* **Method A: Negative Expected Hessian**
    $$
    U'(\theta) = \frac{\partial U}{\partial \theta} = \frac{n}{\theta^2} - \frac{2\sum x_i}{\theta^3}
    $$
    $$
    I(\theta) = -E[U'(\theta)] = -\left( \frac{n}{\theta^2} - \frac{2 n \theta}{\theta^3} \right) = -\left( \frac{n}{\theta^2} - \frac{2n}{\theta^2} \right) = \frac{n}{\theta^2}
    $$

* **Method B: Variance of the Score**
    $$
    \text{Var}(U) = \text{Var}\left( -\frac{n}{\theta} + \frac{\sum X_i}{\theta^2} \right) = \frac{1}{\theta^4} \text{Var}\left( \sum X_i \right)
    $$
    Since $X_i$ are independent with $\text{Var}(X_i) = \theta^2$:
    $$
    \text{Var}(U) = \frac{1}{\theta^4} (n \theta^2) = \frac{n}{\theta^2}
    $$

**Result:** $\text{Var}(U) = -E[U'] = I(\theta)$. (Identity Verified)

3. Cramer-Rao Lower Bound (CRLB)
Consider the estimator $T(\mathbf{X}) = \bar{X}$.

* **Expectation:** $m(\theta) = E[\bar{X}] = \theta$. Thus, $T$ is unbiased and $m'(\theta) = 1$.
* **Actual Variance:**
    $$
    \text{Var}(T(\mathbf{X})) = \text{Var}(\bar{X}) = \frac{\text{Var}(X)}{n} = \frac{\theta^2}{n}
    $$

* **Theoretical Lower Bound:**
    $$
    \text{CRLB} = \frac{[m'(\theta)]^2}{I(\theta)} = \frac{1^2}{n/\theta^2} = \frac{\theta^2}{n}
    $$

**Conclusion:**
$$
\text{Var}(T(\mathbf{X})) = \frac{\theta^2}{n} \ge \frac{\theta^2}{n}
$$
The variance of $T(\mathbf{X})$ achieves the lower bound exactly. Therefore, $\bar{X}$ is an **efficient estimator** for $\theta$.

:::

## Exponential Families

::: {#def-exponential-family}

### Exponential Family
A family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\ell(\boldsymbol{\theta}; \mathbf{x}) = \log f(\mathbf{x}|\boldsymbol{\theta})$, can be expressed as the sum of three distinct terms:

$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})
$$

Exponentiating this yields the density form:

$$
f(\mathbf{x}|\boldsymbol{\theta}) = h(\mathbf{x}) \exp\left\{ \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) \right\}
$$

where:

* $\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)$ is the vector of model parameters.
* $\eta_i(\boldsymbol{\theta})$ are the **natural parameter functions**.
* $\mathbf{T}(\mathbf{x}) = (T_1(\mathbf{x}), \dots, T_k(\mathbf{x}))$ constitutes the vector of **sufficient statistics** for $\boldsymbol{\theta}$.
* $A(\boldsymbol{\theta})$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.
* $h(\mathbf{x})$ is the base measure.

:::


### Examples

#### Exponential Distribution {.unnumbered}

::: {#exm-exponential-dist}

### Exponential Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Exp}(\theta)$, where $\theta$ is the scale parameter.
$$
f(\mathbf{x}|\theta) = \theta^{-n} \exp\left\{ -\frac{1}{\theta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = -\frac{1}{\theta} \sum_{i=1}^n x_i - n \log \theta
$$

Identifying the components:

* $\eta_1(\theta) = -\frac{1}{\theta}$
* $T_1(\mathbf{x}) = \sum_{i=1}^n x_i$
* $A(\theta) = n \log \theta$
* $\log h(\mathbf{x}) = 0$

:::


#### Gamma Distribution {.unnumbered}
::: {#exm-gamma-dist}

### Gamma Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Gamma}(\alpha, \beta)$. The density is:
$$
f(\mathbf{x}|\boldsymbol{\theta}) = [\Gamma(\alpha)\beta^\alpha]^{-n} \left( \prod_{i=1}^n x_i \right)^{\alpha-1} \exp\left\{ -\frac{1}{\beta} \sum_{i=1}^n x_i \right\}
$$

The log-likelihood is:
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = (\alpha-1) \sum_{i=1}^n \log x_i - \frac{1}{\beta} \sum_{i=1}^n x_i - \left[ n \log \Gamma(\alpha) + n\alpha \log \beta \right]
$$

Identifying the components:

* $\eta_1(\boldsymbol{\theta}) = \alpha - 1$, $\quad T_1(\mathbf{x}) = \sum \log x_i$
* $\eta_2(\boldsymbol{\theta}) = -\frac{1}{\beta}$, $\quad T_2(\mathbf{x}) = \sum x_i$
* $A(\boldsymbol{\theta}) = n \log \Gamma(\alpha) + n\alpha \log \beta$

:::


#### Beta Distribution {.unnumbered}
::: {#exm-beta-dist}

### Beta Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} \text{Beta}(a, b)$ with $\boldsymbol{\theta} = (a, b)$.
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = (a-1) \sum_{i=1}^n \log x_i + (b-1) \sum_{i=1}^n \log(1-x_i) - n \log B(a, b)
$$

This is an exponential family with $k=2$.

* $\eta_1 = a-1$, $T_1 = \sum \log x_i$
* $\eta_2 = b-1$, $T_2 = \sum \log(1-x_i)$
* $A(\boldsymbol{\theta}) = n \log B(a, b)$

:::


#### Normal Distribution {.unnumbered}
::: {#exm-normal-dist}

### Normal Distribution
Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. The log-likelihood is:
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
$$

Identifying the components:

* $\eta_1 = \frac{\mu}{\sigma^2}$, $T_1 = \sum x_i$
* $\eta_2 = -\frac{1}{2\sigma^2}$, $T_2 = \sum x_i^2$
* $A(\boldsymbol{\theta}) = \frac{n\mu^2}{2\sigma^2} + n \log \sigma + \frac{n}{2} \log(2\pi)$

:::

### Examples of Non-exponential Families

A model is **not** in the exponential family if the support depends on the parameter.


#### Uniform Distribution {.unnumbered}
::: {#exm-uniform-dist}

### Uniform Distribution
Let $X \sim U(0, \theta)$.
$$
\ell(\theta; x) = -\log \theta + \log I(0 < x < \theta)
$$

The term $\log I(0 < x < \theta)$ couples $x$ and $\theta$ in a way that cannot be separated into a sum $\sum \eta_i(\theta) T_i(x)$.

:::


### Moments of Sufficient Statistics of Exponential Families

#### Means of Sufficient Statistics (General Case)

::: {#thm-moments-exp-family}

#### Means via the Score Function
For a regular exponential family with log-likelihood $\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})$, the expectation of the sufficient statistics can be found by setting the expected score to zero:

$$
E_{\boldsymbol{\theta}} \left[ \frac{\partial \ell(\boldsymbol{\theta}; \mathbf{X})}{\partial \theta_j} \right] = 0
$$

Substituting the specific form of $\ell(\boldsymbol{\theta}; \mathbf{X})$:

$$
\sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} E[T_i(\mathbf{X})] = \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j} \quad \text{for } j=1, \dots, d
$$

:::

::: {.proof}
The log-likelihood is:
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = \sum_{i=1}^k \eta_i(\boldsymbol{\theta}) T_i(\mathbf{x}) - A(\boldsymbol{\theta}) + \log h(\mathbf{x})
$$

Differentiating with respect to $\theta_j$:
$$
\frac{\partial \ell}{\partial \theta_j} = \sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} T_i(\mathbf{x}) - \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j}
$$

Taking the expectation and using the regularity condition $E[\frac{\partial \ell}{\partial \theta_j}] = 0$:
$$
E\left[ \sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} T_i(\mathbf{X}) - \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j} \right] = 0
$$

$$
\sum_{i=1}^k \frac{\partial \eta_i(\boldsymbol{\theta})}{\partial \theta_j} E[T_i(\mathbf{X})] = \frac{\partial A(\boldsymbol{\theta})}{\partial \theta_j}
$$

:::

#### Natural Parameterization

::: {#def-natural-parameterization}

#### Natural Parameterization (Canonical Form)

If the parameterization is chosen such that the natural parameters are the components of the parameter vector itself (i.e., $\boldsymbol{\eta}(\boldsymbol{\theta}) = \boldsymbol{\theta}$), the exponential family is said to be in **Canonical Form** or **Natural Parameterization**.

The log-likelihood for the natural parameter vector $\boldsymbol{\eta} = (\eta_1, \dots, \eta_k)^T$ simplifies to:
$$
\ell(\boldsymbol{\eta}; \mathbf{x}) = \sum_{i=1}^k \eta_i T_i(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
$$
or in vector notation:
$$
\ell(\boldsymbol{\eta}; \mathbf{x}) = \boldsymbol{\eta}^T \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
$$
where $A(\boldsymbol{\eta})$ is the log-partition function.

:::


:::{#def-curvedexpfam}

#### Full vs. Curved Exponential Families

* **Full Exponential Family:** When the natural parameters $\boldsymbol{\eta}$ can vary independently in an open set of $\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).
* **Curved Exponential Family:** When the dimension of the parameter vector $\boldsymbol{\theta}$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\boldsymbol{\eta}(\boldsymbol{\theta})$ to lie on a non-linear curve or surface within the natural parameter space.

:::

::: {#exm-curved-normal-natural}

#### Curved Exponential Family Example
Consider the $N(\theta, \theta^2)$ distribution ($d=1$). The log-likelihood is:
$$
\ell(\theta; \mathbf{x}) = -\frac{1}{2\theta^2} \sum x_i^2 + \frac{1}{\theta} \sum x_i - n \log \theta - \text{const}
$$

Here:

* $\eta_1(\theta) = -\frac{1}{2\theta^2}$, $T_1 = \sum x_i^2$
* $\eta_2(\theta) = \frac{1}{\theta}$, $T_2 = \sum x_i$

Since $d=1$ but $k=2$, and $\eta_1 = -\frac{1}{2}\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.

:::

#### Mean and Variance of Sufficient Statistics

::: {#thm-cumulant-generating}

#### Mean and Variance of Sufficient Statistics

For an exponential family in canonical form, the log-partition function $A(\boldsymbol{\eta})$ acts as the **Cumulant Generating Function** for the sufficient statistic vector $\mathbf{T}(\mathbf{X})$. The derivatives of $A(\boldsymbol{\eta})$ yield the moments of $\mathbf{T}(\mathbf{X})$ as follows:

1.  **Mean (First Derivative):**
    $$
    E[\mathbf{T}(\mathbf{X})] = \nabla A(\boldsymbol{\eta})
    $$

2.  **Covariance (Second Derivative):**
    $$
    \text{Var}(\mathbf{T}(\mathbf{X})) = \nabla^2 A(\boldsymbol{\eta})
    $$

**Link to Fisher Information:**
In the canonical parameterization, the observed information matrix is constant (non-stochastic) and equals the Hessian of $A(\boldsymbol{\eta})$. Therefore, the covariance of the sufficient statistics is exactly the Fisher Information Matrix:

$$
\text{Var}(\mathbf{T}(\mathbf{X})) = \mathbf{I}(\boldsymbol{\eta})
$$

This implies that $\mathbf{T}(\mathbf{X})$ is an efficient estimator for the mean parameter $\mathbf{m}(\boldsymbol{\eta}) = E[\mathbf{T}(\mathbf{X})]$, as it achieves the Cramer-Rao Lower Bound with equality (identity link).

:::

::: {.proof}
**Derivation**

These results follow directly from Bartlett's Identities (Theorem @thm-score-identities) applied to the canonical log-likelihood:
$$
\ell(\boldsymbol{\eta}; \mathbf{x}) = \boldsymbol{\eta}^T \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}) + \log h(\mathbf{x})
$$

**For the Mean:**
The score function (gradient of $\ell$) is:
$$
\mathbf{U}(\boldsymbol{\eta}) = \nabla_{\boldsymbol{\eta}} \ell(\boldsymbol{\eta}; \mathbf{x}) = \mathbf{T}(\mathbf{x}) - \nabla A(\boldsymbol{\eta})
$$
By the First Moment Identity, $E[\mathbf{U}(\boldsymbol{\eta})] = \mathbf{0}$:
$$
E[\mathbf{T}(\mathbf{X}) - \nabla A(\boldsymbol{\eta})] = \mathbf{0} \implies E[\mathbf{T}(\mathbf{X})] = \nabla A(\boldsymbol{\eta})
$$

**For the Covariance:**
The observed information (negative Hessian of $\ell$) is:
$$
\mathbf{J}(\boldsymbol{\eta}) = -\nabla^2_{\boldsymbol{\eta}} \ell(\boldsymbol{\eta}; \mathbf{x}) = -\nabla_{\boldsymbol{\eta}} (\mathbf{T}(\mathbf{x}) - \nabla A(\boldsymbol{\eta})) = \nabla^2 A(\boldsymbol{\eta})
$$
Note that $\mathbf{T}(\mathbf{x})$ is constant with respect to $\boldsymbol{\eta}$, so its derivative vanishes.
By the Second Moment Identity, $\mathbf{I}(\boldsymbol{\eta}) = E[\mathbf{J}(\boldsymbol{\eta})] = \text{Cov}(\mathbf{U}(\boldsymbol{\eta}))$.
Since $\mathbf{U}(\boldsymbol{\eta}) = \mathbf{T}(\mathbf{X}) - \text{constant}$, $\text{Cov}(\mathbf{U}(\boldsymbol{\eta})) = \text{Cov}(\mathbf{T}(\mathbf{X}))$.
Therefore:
$$
\text{Cov}(\mathbf{T}(\mathbf{X})) = E[\nabla^2 A(\boldsymbol{\eta})] = \nabla^2 A(\boldsymbol{\eta})
$$

:::

#### Examples

##### Moments of the Binomial Distribution {.unnumbered}

::: {#exm-bernoulli-moments}

#### Moments of the Binomial Distribution

Consider $n$ independent coin flips $X_1, \dots, X_n \sim \text{Bernoulli}(p)$. We find the mean and variance of $T = \sum X_i$.

1. **Log-Likelihood Form**
   
   The standard log-likelihood is:
   $$
   \ell(p; \mathbf{x}) = \log\left(\frac{p}{1-p}\right) \sum x_i + n \log(1-p)
   $$

   * Natural Parameter: $\eta = \log\left(\frac{p}{1-p}\right) \implies p = \frac{e^\eta}{1+e^\eta}$.
   * Log-Partition Function: $A(\eta) = -n \log(1-p) = n \log(1+e^\eta)$.

   **Canonical Log-Likelihood $\ell(\eta)$:**
   $$
   \ell(\eta; \mathbf{x}) = \eta \left(\sum x_i\right) - n \log(1+e^\eta)
   $$

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = n \frac{e^\eta}{1+e^\eta} = np
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = n \frac{e^\eta (1+e^\eta) - e^\eta(e^\eta)}{(1+e^\eta)^2} = n \frac{e^\eta}{(1+e^\eta)^2} = np(1-p)
   $$

:::


#####  Moments of the Gamma Sufficient Statistic {.unnumbered}
::: {#exm-exponential-moments}

### Moments of the Gamma Sufficient Statistic

Consider $X_i \sim \text{Exp}(\lambda)$. We find the moments of $T = \sum X_i$.

1. **Log-Likelihood Form**
   
   The standard log-likelihood is:
   $$
   \ell(\lambda; \mathbf{x}) = -\lambda \sum x_i + n \log \lambda
   $$

   * Natural Parameter: $\eta = -\lambda$.
   * Log-Partition Function: $A(\eta) = -n \log \lambda = -n \log(-\eta)$.

   **Canonical Log-Likelihood $\ell(\eta)$:**
   $$
   \ell(\eta; \mathbf{x}) = \eta \left(\sum x_i\right) - \left[ -n \log(-\eta) \right] = \eta \sum x_i + n \log(-\eta)
   $$

2. **Calculating Moments**
   $$
   E[T] = \frac{\partial A}{\partial \eta} = -n \frac{1}{-\eta}(-1) = -\frac{n}{\eta} = \frac{n}{\lambda}
   $$

   $$
   \text{Var}(T) = \frac{\partial^2 A}{\partial \eta^2} = \frac{\partial}{\partial \eta} \left(-\frac{n}{\eta}\right) = \frac{n}{\eta^2} = \frac{n}{\lambda^2}
   $$

:::


#####  Moments of Normal Sufficient Statistics {.unnumbered}
::: {#exm-normal-sufficient-moments}

### Moments of Normal Sufficient Statistics

Consider $X_i \sim N(\mu, \sigma^2)$.

1. **Log-Likelihood Form**
   
   The standard log-likelihood is:
   $$
   \ell(\boldsymbol{\theta}; \mathbf{x}) = \frac{\mu}{\sigma^2} \sum x_i - \frac{1}{2\sigma^2} \sum x_i^2 - \left[ \frac{n\mu^2}{2\sigma^2} + \frac{n}{2} \log(2\pi\sigma^2) \right]
   $$
   
   * Natural Parameters: $\eta_1 = \frac{\mu}{\sigma^2}, \quad \eta_2 = -\frac{1}{2\sigma^2}$.
   * Log-Partition Function (in terms of $\boldsymbol{\eta}$):
     Using $\sigma^2 = -\frac{1}{2\eta_2}$ and $\mu = -\frac{\eta_1}{2\eta_2}$:
     $$
     A(\boldsymbol{\eta}) = -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2) + \frac{n}{2}\log(2\pi)
     $$

   **Canonical Log-Likelihood $\ell(\boldsymbol{\eta})$:**
   $$
   \ell(\boldsymbol{\eta}; \mathbf{x}) = \eta_1 \left(\sum x_i\right) + \eta_2 \left(\sum x_i^2\right) - \left[ -\frac{n \eta_1^2}{4 \eta_2} - \frac{n}{2} \log(-2\eta_2) \right]
   $$

2. **First Moments (Means)**
   $$
   E[T_1] = E\left[\sum X_i\right] = \frac{\partial A}{\partial \eta_1} = -\frac{2n\eta_1}{4\eta_2} = -\frac{n\eta_1}{2\eta_2} = n\mu
   $$
   $$
   E[T_2] = E\left[\sum X_i^2\right] = \frac{\partial A}{\partial \eta_2} = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2(-2\eta_2)}(-2) = \frac{n\eta_1^2}{4\eta_2^2} - \frac{n}{2\eta_2}
   $$
   Subbing back $\mu, \sigma$:
   $$
   = n\mu^2 + n\sigma^2 = n(\mu^2 + \sigma^2)
   $$

3. **Second Moment (Covariance)**
   $$
   \text{Cov}(T_1, T_2) = \frac{\partial^2 A}{\partial \eta_1 \partial \eta_2} = \frac{\partial}{\partial \eta_2} \left( -\frac{n\eta_1}{2\eta_2} \right) = \frac{n\eta_1}{2\eta_2^2} = 2n\mu\sigma^2
   $$

4. **Independence of $\bar{X}$ and $S^2$**
   We verify that $\text{Cov}(\bar{X}, S^2) = 0$.
   
   Express $\bar{X}$ and $S^2$ in terms of $T_1$ and $T_2$:
   $$
   \bar{X} = \frac{1}{n} T_1
   $$
   $$
   S^2 = \frac{1}{n-1} \left( \sum X_i^2 - n\bar{X}^2 \right) = \frac{1}{n-1} \left( T_2 - \frac{1}{n} T_1^2 \right)
   $$
   
   Now compute the covariance (ignoring constants $\frac{1}{n(n-1)}$ for now):
   $$
   \text{Cov}\left(T_1, T_2 - \frac{1}{n}T_1^2\right) = \text{Cov}(T_1, T_2) - \frac{1}{n} \text{Cov}(T_1, T_1^2)
   $$
   
   We need $\text{Cov}(T_1, T_1^2)$. Since $T_1 = \sum X_i \sim N(n\mu, n\sigma^2)$, we use the property of the normal distribution that for $Y \sim N(\theta, \tau^2)$, $\text{Cov}(Y, Y^2) = 2\theta \tau^2$.
   Here $\theta = n\mu$ and $\tau^2 = n\sigma^2$:
   $$
   \text{Cov}(T_1, T_1^2) = 2(n\mu)(n\sigma^2) = 2n^2\mu\sigma^2
   $$
   
   Substituting this back into the expression:
   $$
   \text{Cov}\left(T_1, T_2 - \frac{1}{n}T_1^2\right) = \underbrace{2n\mu\sigma^2}_{\text{From Part 3}} - \frac{1}{n} (2n^2\mu\sigma^2) = 2n\mu\sigma^2 - 2n\mu\sigma^2 = 0
   $$
   
   Since $\bar{X}$ and $S^2$ are uncorrelated and derived from normally distributed data, they are **independent**.

:::
