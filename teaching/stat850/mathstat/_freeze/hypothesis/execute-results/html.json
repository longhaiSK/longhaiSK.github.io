{
  "hash": "3a9567a3af3fa07c39f6bdfc779308a0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis Testing\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n## Formulation for Hypothesis Testing\n\n### Hypothesis\n\nWe formulate the problem of hypothesis testing as deciding between two competing claims about a parameter $\\theta$:\n\n$$\nH_0: \\theta \\in \\Theta_0 \\quad \\text{(Null Hypothesis)}\n$$\n\n$$\nH_1: \\theta \\in \\Theta_1 \\quad \\text{(Alternative Hypothesis)}\n$$\n\n::: {#def-simple-composite}\n### Simple and Composite Hypotheses\nA hypothesis is called **simple** if it specifies a single value for the parameter (e.g., $\\Theta_0$ contains only one point). It is called **composite** if it specifies more than one value.\n:::\n\n::: {#exm-normal-hypotheses}\n### Normal Mean Test\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$.\n\n* If $\\sigma^2$ is known, $H_0: \\mu = \\mu_0$ is a simple hypothesis.\n* If $\\sigma^2$ is unknown, $H_0: \\mu = \\mu_0$ is a composite hypothesis (since $\\sigma^2$ can vary).\n:::\n\n### Test Functions\n\nA test is defined by a **critical region** $C_\\alpha$ such that we reject $H_0$ if the data $x \\in C_\\alpha$. Equivalently, we can define a **test function** $\\phi(x)$ representing the probability of rejecting $H_0$ given data $x$.\n\n* A **non-randomized** test is given as follows:\n\n$$\n\\phi(x) = I(x \\in C_\\alpha) = \\begin{cases} 1 & \\text{if } x \\in C_\\alpha \\text{ (Reject } H_0 \\text{)} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\n* A **randomized** test, $\\phi(x)$ can take values in $[0, 1]$, which can be expressed typically as follows:\n\n\n   $$\n   \\phi(x) = \\begin{cases} \n   1 & \\text{if } x \\in C_1 \\\\\n   \\gamma & \\text{if } x \\in C_* \\\\\n   0 & \\text{otherwise}\n   \\end{cases}\n   $$\n\n   where:\n\n   * $C_1$ is the region where we strictly reject $H_0$.\n   * $C_*$ is the boundary region (often where $T(x) = k$) where we reject $H_0$ with probability $\\gamma$.\n\n* More generally, $\\phi(x)$ is just a function of $x$ with values in $[0,1]$, which represents the probability that we will reject $H_0$. \n\n\n::: {#exm-randomized-test}\n### Randomized Test for Binomial\nLet $X \\sim \\text{Bin}(n=10, \\theta)$. Consider testing $H_0: \\theta = 1/2$ vs $H_1: \\theta > 1/2$ with target size $\\alpha = 0.05$.\n\nSuppose we choose a critical region $X \\ge k$.\n\n* If $k=9$, $P(X \\ge 9 | \\theta=0.5) \\approx 0.0107$.\n* If $k=8$, $P(X \\ge 8 | \\theta=0.5) \\approx 0.0547$.\n\nSince we cannot achieve exactly 0.05 with a non-randomized test (the survival function jumps over 0.05), we must use a randomized test function.\n\n\nThe randomized test is defined as:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } x \\in C_1 \\text{ (i.e., } x \\ge 9) \\\\\n\\gamma & \\text{if } x \\in C_* \\text{ (i.e., } x = 8) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nFrom the figure, we see that $\\alpha = 0.05$ lies between $P(X \\ge 9)$ and $P(X \\ge 8)$. We always reject the \"tail\" where probabilities are strictly less than $\\alpha$ (here $x \\ge 9$). At the boundary $x=8$, we cannot reject with probability 1 (which would give total size 0.0547), nor with probability 0 (which would give total size 0.0107).\n\nWe choose $\\gamma$ to bridge this gap:\n\n$$\n\\begin{aligned}\n\\alpha &= P(X \\ge 9) + \\gamma \\cdot P(X = 8) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (P(X \\ge 8) - P(X \\ge 9)) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (0.05469 - 0.01074)\n\\end{aligned}\n$$\n\nSolving for $\\gamma$:\n\n$$\n\\gamma = \\frac{0.05 - 0.01074}{0.04395} \\approx \\frac{39}{44} \\approx 0.89\n$$\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Survival Function P(X >= k) with randomization detail](hypothesis_files/figure-html/fig-binomial-survival-zoom-1.png){#fig-binomial-survival-zoom fig-align='center' width=576 style=\"width: 90% !important;\"}\n:::\n:::\n\n\n\n\n\n\n### Size\n\n::: {#def-test-size}\n### Size of a Test\nThe **size** of a test $\\phi(x)$ is the maximum probability of rejecting the null hypothesis when it is true:\n\n$$\n\\text{Size}(\\phi) = \\sup_{\\theta \\in \\Theta_0} W_\\phi(\\theta) = \\sup_{\\theta \\in \\Theta_0} E_\\theta[\\phi(X)]\n$$\n:::\n\n\n### Power \nWe distinguish between the power function varying over parameters and the power metric of a specific test.\n\n1. **Power Function ($W_\\phi(\\theta)$)**\nThe probability of rejecting $H_0$ as a function of the parameter $\\theta$:\n\n$$\nW_\\phi(\\theta) = E_\\theta[\\phi(X)]\n$$\n\n2. **Power of the Test ($\\text{Power}(\\phi)$)**\nIn the context of a specific alternative hypothesis (e.g., $H_1: \\theta = \\theta_1$), we define the power as a scalar functional of $\\phi$:\n\n$$\n\\text{Power}(\\phi) = E_{\\theta_1}[\\phi(X)]\n$$\n\nIdeally, we want:\n\n* $W_\\phi(\\theta) \\le \\text{Size}(\\phi)$ for all $\\theta \\in \\Theta_0$ (Control Type I error).\n* $\\text{Power}(\\phi)$ to be as large as possible (Maximize sensitivity to $H_1$).\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# 1. Define Parameters\nmu0 <- 0\nmu1 <- 3\nsigma <- 1\nc_val <- 1.5      # Critical value\ngamma_val <- 0.5  # Randomization constant\n\n# 2. Scaling Constants\nmax_dens <- dnorm(mu0, mean = mu0, sd = sigma)\ny_limit <- max_dens * 1.1\nphi_scale <- 0.1 * y_limit\n\n# 3. Define the Test Function phi(x) (Single Test)\n# Step function: 0 -> 1 at c_val\ndf_phi <- data.frame(\n  x_start = c(-3, c_val),\n  x_end   = c(c_val, 6),\n  y_start = c(0, phi_scale),\n  y_end   = c(0, phi_scale)\n)\n\nggplot() +\n  # --- Layer 1: Densities (Solid Lines, No Fill) ---\n  \n  # H0: Normal(0, 1) - Blue (Cool)\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                color = \"blue\", size = 0.8) +\n  \n  # H1: Normal(3, 1) - Red (Hot)\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                color = \"red\", size = 0.8) +\n\n  # --- Layer 2: Test Function phi(x) (Thick Pink Line) ---\n  \n  # The horizontal segments\n  geom_segment(data = df_phi, \n               aes(x = x_start, xend = x_end, y = y_start, yend = y_end), \n               color = \"deeppink\", size = 2.5, alpha = 0.5) +\n  \n  # The vertical threshold line\n  geom_segment(aes(x = c_val, xend = c_val, y = 0, yend = phi_scale), \n               linetype = \"dotted\", color = \"deeppink\", size = 0.8) +\n  \n  # Points at discontinuity\n  geom_point(aes(x = c_val, y = gamma_val * phi_scale), \n             color = \"deeppink\", size = 3) + \n  geom_point(aes(x = c_val, y = 0), size = 3, shape = 21, fill = \"white\", color = \"deeppink\") +\n  geom_point(aes(x = c_val, y = phi_scale), size = 3, shape = 21, fill = \"white\", color = \"deeppink\") +\n\n  # --- Layer 3: Annotations ---\n  \n  # Gamma label\n  annotate(\"text\", x = c_val + 0.2, y = gamma_val * phi_scale, \n           label = expression(gamma), hjust = 0, fontface = \"bold\", color = \"deeppink\") +\n  \n  # H0 / H1 Labels\n  annotate(\"text\", x = mu0, y = max_dens * 0.9, \n           label = expression(H[0]), color = \"blue\", size = 5) +\n  annotate(\"text\", x = mu1, y = max_dens * 0.9, \n           label = expression(H[1]), color = \"red\", size = 5) +\n  \n  # Critical Value Label\n  annotate(\"text\", x = c_val, y = -0.01, label = \"c\", vjust = 1) +\n\n  # --- Layer 4: Scales ---\n  scale_y_continuous(\n    name = \"Density f(x)\",\n    limits = c(-0.02, y_limit),\n    expand = c(0, 0),\n    \n    # Secondary Axis for phi\n    sec.axis = sec_axis(~ . / phi_scale, \n                        name = expression(phi(x)),\n                        breaks = c(0, 1))\n  ) +\n  scale_x_continuous(name = \"Observation x\", limits = c(-3, 6)) +\n  \n  theme_minimal() +\n  theme(\n    axis.title.y.right = element_text(angle = 90, vjust = 0.5),\n    panel.grid.minor = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![Illustration of a Test Function $\\phi(x)$ (Pink) relative to Size ($H_0$, Blue) and Power ($H_1$, Red).](hypothesis_files/figure-html/fig-test-function-size-power-1.png){#fig-test-function-size-power width=672}\n:::\n:::\n\n\n\n## The Neyman-Pearson Lemma\n\nConsider testing a simple null against a simple alternative:\n$H_0: \\theta = \\theta_0$ vs $H_1: \\theta = \\theta_1$.\n\nWe define the **Likelihood Ratio** $\\Lambda(x)$ as:\n\n$$\n\\Lambda(x) = \\frac{f_1(x)}{f_0(x)} = \\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\n::: {#def-lrt}\n### Likelihood Ratio Test (LRT)\nA test $\\phi(x)$ is a Likelihood Ratio Test if it has the form:\n\n$$\n\\phi_{\\text{LRT}}(x) = \\begin{cases} \n1 & \\text{if } \\Lambda(x) > k \\\\\n\\gamma(x) & \\text{if } \\Lambda(x) = k \\\\\n0 & \\text{if } \\Lambda(x) < k\n\\end{cases}\n$$\n\nwhere $k \\ge 0$ is a constant and $0 \\le \\gamma(x) \\le 1$.\n:::\n\n\n\n### Neyman-Pearson Lemma\n::: {#thm-neyman-pearson}\n### Neyman-Pearson Lemma\na) **Optimality:** For any $k$ and $\\gamma(x)$, the LRT $\\phi_0(x)$ defined above has maximum power among all tests whose size is less than or equal to the size of $\\phi_0(x)$.\n\na) **Existence:** Given $\\alpha \\in (0, 1)$, there exist constants $k$ and $\\gamma_0$ such that the LRT defined by this $k$ and $\\gamma(x) = \\gamma_0$ has size exactly $\\alpha$.\n\nc) **Uniqueness:** If a test $\\phi$ has size $\\alpha$ and is of maximum power among all tests of size $\\alpha$, then $\\phi$ is necessarily an LRT, except possibly on a set of measure zero under $H_0$ and $H_1$.\n:::\n\n### A Derivation with The Lagrange Multiplier Approach\n\nTo make the optimality of the Likelihood Ratio Test (LRT) intuitive, we can frame the search for the best test function $\\phi(x)$ as a constrained optimization problem.\n\nWe want to maximize the power of the test:\n\n$$\n\\text{Power}(\\phi) = \\int \\phi(x) f_1(x) dx\n$$\n\nsubject to the constraint on the size of the test $\\alpha$:\n\n$$\n\\text{Size}(\\phi) = \\int \\phi(x) f_0(x) dx = \\alpha\n$$\n\nUsing the method of Lagrange multipliers, we define the objective function $L$ with a multiplier $k$:\n\n$$\nL(\\phi, k) = \\int \\phi(x) f_1(x) dx - k \\left( \\int \\phi(x) f_0(x) dx - \\alpha \\right)\n$$\n\nRearranging the terms inside the integral, we get:\n\n$$\nL(\\phi, k) = \\int \\phi(x) [f_1(x) - k f_0(x)] dx + k\\alpha\n$$\n\nTo maximize $L$ with respect to $\\phi(x)$, we look at the integrand. Since $0 \\le \\phi(x) \\le 1$, we should choose $\\phi(x)$ to be as large as possible whenever its coefficient is positive, and as small as possible whenever its coefficient is negative:\n\n* If $f_1(x) - k f_0(x) > 0$, set $\\phi(x) = 1$.\n* If $f_1(x) - k f_0(x) < 0$, set $\\phi(x) = 0$.\n* If $f_1(x) - k f_0(x) = 0$, the value of $\\phi(x)$ does not affect the integral (this is where $\\gamma$ comes in).\n\nThis decision rule is equivalent to:\n\n$$\n\\phi(x) = \n\\begin{cases} \n1 & \\text{if } \\frac{f_1(x)}{f_0(x)} > k \\\\\n0 & \\text{if } \\frac{f_1(x)}{f_0(x)} < k\n\\end{cases}\n$$\n\nThis is precisely the form of the Likelihood Ratio Test. The \"shadow price\" or Lagrange multiplier $k$ represents the critical threshold that balances the gain in power against the cost of increasing the Type I error.\n\n### Proof of NP Lemma\n\n::: {.proof}\n\n**Proof of (a) Optimality:**\nLet $\\phi_{\\text{LRT}}$ be the LRT with size $\\alpha$, and $\\phi$ be any other test with size $\\le \\alpha$.\nDefine the function $U(x)$ as the difference in test functions weighted by the linear combination of densities:\n\n$$\nU(x) = (\\phi_{}(x) - \\phi(x))(f_1(x) - k f_0(x))\n$$\n\nWe analyze the sign of $U(x)$ by looking at the sign of its two factors in three cases:\n\n* If $f_1(x) - k f_0(x) > 0$ (implies $\\Lambda(x) > k$). Since $\\phi_{\\text{LRT}}(x) = 1$ and $\\phi(x) \\le 1$, we have:\n    \n    $$\n    \\begin{aligned}\n    \\phi_{\\text{LRT}}(x) - \\phi(x) &\\ge 0 \\\\\n    U(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) &\\ge 0\n    \\end{aligned}\n    $$\n\n* If $f_1(x) - k f_0(x) < 0$ (implies $\\Lambda(x) < k$). Since $\\phi_{\\text{LRT}}(x) = 0$ and $\\phi(x) \\ge 0$, we have:\n\n    $$\n    \\begin{aligned}\n    \\phi_{\\text{LRT}}(x) - \\phi(x) &\\le 0 \\\\\n    U(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) &\\ge 0\n    \\end{aligned}\n    $$\n\n* If $f_1(x) - k f_0(x) = 0$.\n    The product is zero regardless of the test functions.\n    \n    $$\n    U(x) = 0\n    $$\n\nCombining these cases, we conclude that the product is non-negative for all $x$:\n\n$$\nU(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) \\ge 0\n$$\n\nTherefore, integrating $U(x)$ over the entire domain:\n\n$$\n\\int U(x) dx = \\int (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) dx \\ge 0\n$$\n\nExpanding the integral:\n\n$$\n\\int \\phi_{\\text{LRT}}(x) f_1(x) \\, dx - \\int \\phi(x) f_1(x) \\, dx - k \\left( \\int \\phi_{\\text{LRT}}(x) f_0(x) \\, dx - \\int \\phi(x) f_0(x) \\, dx \\right) \\ge 0\n$$\nConverting to expectations:\n\n$$\nE_{\\theta_1}[\\phi_{\\text{LRT}}] - E_{\\theta_1}[\\phi] \\ge k (E_{\\theta_0}[\\phi_{\\text{LRT}}] - E_{\\theta_0}[\\phi])  \n$$\n\nSince $E_{\\theta_0}[\\phi_{\\text{LRT}}] =\\text{Size}(\\phi_{\\text{LRT}})= \\alpha$ and we require that $E_{\\theta_0}[\\phi] = \\text{Size}(\\phi)\\le \\alpha$,  \n$$\nE_{\\theta_0}[\\phi_{\\text{LRT}}] - E_{\\theta_0}[\\phi] \\ge 0\n$$\n\nThereore, given that $k \\ge 0$:\n\n$$\n\\text{Power}(\\phi_{\\text{LRT}}) \\ge \\text{Power}(\\phi)\n$$\n\n**Proof of (b) Existence:**\n\nLet $G(k) = P_{\\theta_0}(\\Lambda(X) \\le k)$. $G(k)$ is the cumulative distribution function of the random variable $\\Lambda(X)$, so it is non-decreasing.\nWe seek $k_0$ such that $1 - G(k_0) \\approx \\alpha$.\nBecause of discrete jumps, we might not hit $\\alpha$ exactly.\nWe choose $k_0$ such that:\n\n$$\nP_{\\theta_0}(\\Lambda(X) > k_0) \\le \\alpha \\le P_{\\theta_0}(\\Lambda(X) \\ge k_0)\n$$\n\nSet $\\gamma_0 = \\frac{\\alpha - P_{\\theta_0}(\\Lambda(X) > k_0)}{P_{\\theta_0}(\\Lambda(X) = k_0)}$.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# 1. Define Parameters\nmu0 <- 0\nmu1 <- 3\nsigma <- 1\nc_lrt <- 2\ngamma_val <- 0.5\n\n# Scaling constants\nmax_dens <- dnorm(mu0, mean = mu0, sd = sigma)\ny_limit <- max_dens * 1.1\nphi_scale <- 0.1 * y_limit\n\n# 2. Define Test Functions Data\ndf_lrt <- data.frame(\n  x_start = c(-3, c_lrt),\n  x_end   = c(c_lrt, 6),\n  y_val   = c(0, phi_scale),\n  Test    = \"phi[LRT]\" \n)\n\ndf_other <- data.frame(\n  x_start = c(-3, 0.5, 1, 3),\n  x_end   = c(0.5, 1, 3,6),\n  y_val   = c(0, phi_scale, 0, 0),\n  Test    = \"phi\"\n)\n\nggplot() +\n  # --- Layer 1: Densities (Solid Lines, No Fill) ---\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                color = \"blue\", size = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                color = \"red\", size = 0.8) +\n\n  # --- Layer 2: Test Functions (Segments) ---\n  \n  # 2a. LRT Segments (Thick, Transparent Pink)\n  geom_segment(data = df_lrt,\n               aes(x = x_start, xend = x_end, \n                   y = y_val, yend = y_val, \n                   color = Test, linetype = Test, size = Test),\n               alpha = 0.4) + \n  \n  # Vertical connector for LRT\n  geom_segment(aes(x = c_lrt, xend = c_lrt, y = 0, yend = phi_scale),\n               color = \"deeppink\", linetype = \"dotted\", size = 0.5, alpha = 0.6) +\n\n  # 2b. Other Test Segments (Thin Black Opaque)\n  geom_segment(data = df_other,\n               aes(x = x_start, xend = x_end, \n                   y = y_val, yend = y_val, \n                   color = Test, linetype = Test, size = Test)) +\n  \n  # Vertical connectors for Other Test\n  geom_segment(aes(x = 1, xend = 1, y = 0, yend = phi_scale),\n               color = \"black\", linetype = \"dotted\", size = 0.5) +\n  geom_segment(aes(x = 3, xend = 3, y = 0, yend = phi_scale),\n               color = \"black\", linetype = \"dotted\", size = 0.5) +\n\n  # --- Layer 3: Annotations ---\n  # LRT Gamma Point (Pink)\n  geom_point(aes(x = c_lrt, y = gamma_val * phi_scale), \n             color = \"deeppink\", size = 3) +\n  \n  # Density Labels\n  annotate(\"text\", x = mu0, y = max_dens * 0.9, \n           label = expression(H[0]), color = \"blue\", size = 5) +\n  annotate(\"text\", x = mu1, y = max_dens * 0.9, \n           label = expression(H[1]), color = \"red\", size = 5) +\n\n  # --- Layer 4: Scales and Legend ---\n  scale_y_continuous(\n    name = \"Density f(x)\", limits = c(-0.02, y_limit), expand = c(0, 0),\n    sec.axis = sec_axis(~ . / phi_scale, name = expression(phi(x)), breaks = c(0, 1))\n  ) +\n  scale_x_continuous(name = \"Observation x\", limits = c(-3, 6)) +\n  \n  # Manual Legend Controls (FIXED using named vectors for labels)\n  scale_color_manual(name = \"Test Function\",\n                     values = c(\"phi[LRT]\" = \"deeppink\", \"phi\" = \"black\"),\n                     labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  scale_linetype_manual(name = \"Test Function\",\n                        values = c(\"phi[LRT]\" = \"solid\", \"phi\" = \"solid\"),\n                        labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  scale_size_manual(name = \"Test Function\",\n                    values = c(\"phi[LRT]\" = 4,   # Extra Thick\n                               \"phi\" = 1), # Thin\n                    labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  \n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.title.y.right = element_text(angle = 90, vjust = 0.5),\n    panel.grid.minor = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![Visualizing the NP Lemma: The thick, transparent pink line is $\\phi_{\text{LRT}}$. The thin solid black line is $\\phi$. Overlap is visible as a black line inside pink.](hypothesis_files/figure-html/fig-lrt-proof-1.png){#fig-lrt-proof width=672}\n:::\n:::\n\n\n\n## Uniformly Most Powerful (UMP) Tests\n\nWhen the alternative hypothesis is composite ($H_1: \\theta \\in \\Theta_1$), we seek a test that is \"best\" for *all* $\\theta \\in \\Theta_1$.\n\n::: {#def-ump}\n### Uniformly Most Powerful Test\n\nA test $\\phi_0(x)$ of size $\\alpha$ is **Uniformly Most Powerful (UMP)** if:\n\n1.  $E_{\\theta}[\\phi_0(X)] \\le \\alpha$ for all $\\theta \\in \\Theta_0$.\n2.  For any other test $\\phi(x)$ satisfying (1), $E_{\\theta}[\\phi_0(X)] \\ge E_{\\theta}[\\phi(X)]$ for all $\\theta \\in \\Theta_1$.\n   \n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Power functions of UMP test vs. another test](hypothesis_files/figure-html/fig-ump-power-1.png){#fig-ump-power fig-align='center' width=576 style=\"width: 80% !important;\"}\n:::\n:::\n\n\n## Monotone Likelihood Ratio (MLR)\n\n::: {#def-mlr}\n### Monotone Likelihood Ratio\nA family of densities $\\{f(x; \\theta)\\}$ has a **Monotone Likelihood Ratio (MLR)** with respect to a statistic $T(x)$ if for any $\\theta_1 > \\theta_0$, the ratio:\n\n$$\n\\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\nis a non-decreasing function of $T(x)$.\n:::\n\nCommon examples include the one-parameter Exponential Family:\n$f(x; \\theta) = h(x) c(\\theta) \\exp\\{w(\\theta) T(x)\\}$.\nIf $w(\\theta)$ is increasing, the family has MLR w.r.t $T(x)$.\n\n\n:::: {#exm-mlr-gamma}\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$ with pdf $f(x) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nTest $$H_0: \\theta = \\theta_0 \\text{ vs } H_1: \\theta > \\theta_0.$$\n\n\nThe Likelihood Ratio for $\\theta_1 > \\theta_0$ is:\n\n$$\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n$$\n\nSince $\\theta_1 > \\theta_0$, the term $(\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})$ is positive. Thus, $\\Lambda(x)$ is an increasing function of the sum $T (x) = \\sum x_i$.\n\nRejecting for large $\\Lambda(x)$ is equivalent to rejecting for $T(x)=\\sum x_i > C$.\n\nUnder $H_0$, $X_i \\sim \\text{Exp}(\\theta_0)$, which is equivalent to $\\text{Gamma}(1, \\theta_0)$. By the reproductive property of the Gamma distribution:\n\n$$T = \\sum_{i=1}^n X_i \\sim \\text{Gamma}(n, \\theta_0)$$\n\nAlternatively, using the relationship with the Chi-square distribution:\n\n$$\\frac{2T}{\\theta_0} \\sim \\chi^2_{2n}$$\n\nTo find $C$ for a significance level $\\alpha$, we set $P(T > C | \\theta_0) = \\alpha$. Using the $\\chi^2$ transformation:\n\n$$P\\left(\\frac{2T}{\\theta_0} > \\frac{2C}{\\theta_0}\\right) = \\alpha \\implies \\frac{2C}{\\theta_0} = \\chi^2_{2n, \\alpha}$$\n\nThus, the critical value is:\n\n$$C = \\frac{\\theta_0}{2} \\chi^2_{2n, \\alpha}$$\n\nwhere $\\chi^2_{2n, \\alpha}$ is the upper-$\\alpha$ quantile of a Chi-square distribution with $2n$ degrees of freedom. \n\n::: {.callout-important}\nWe note that the value $C$ does not depend on $\\theta_1$.\n:::\n\n::::\n\n\n\n### Karlin-Rubin Theorem \n\n::: {#thm-karlin-rubin}\n#### Karlin-Rubin Theorem \nSuppose $X$ has a distribution from a family with MLR with respect to $T(X)$, and the distribution of $T(X)$ is continuous.\nConsider testing $H_0: \\theta \\le \\theta_0$ vs $H_1: \\theta > \\theta_0$.\n\nThe test:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } T(x) > t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n$$\n\nwhere $t_0$ is determined by $P_{\\theta_0}(T(X) > t_0) = \\alpha$, is the UMP size $\\alpha$ test.\n:::\n\n::: {.proof}\n\n**The Test:**\nDefine the test $\\phi(x)$ as:\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } T(x) > t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n$$\nwhere $t_0$ is determined such that the power at the boundary is $\\alpha$, i.e., $W_{LR}(\\theta_0) = \\alpha$.\n\n(1) **Application of Neyman-Pearson Lemma**\n\n    Because the family has a Monotone Likelihood Ratio (MLR) in $T(x)$, for any specific alternative $\\theta_1 > \\theta_0$, the likelihood ratio $\\Lambda(x)$ is an increasing function of $T(x)$.\n    Therefore, the rejection region $T(x) > t_0$ corresponds to $\\Lambda(x) > k$.\n    By the Neyman-Pearson Lemma, $\\phi(x)$ is the Most Powerful (MP) test for testing $H_0': \\theta = \\theta_0$ vs $H_1': \\theta = \\theta_1$.\n\n(2) **Monotonicity of the Power Function** \n\n  We claim that $W_{LR}(\\theta)$ is a non-decreasing function of $\\theta$.\n\n  Let $\\theta_2 < \\theta_1$. Consider testing $\\theta = \\theta_2$ vs $\\theta = \\theta_1$.\n  Let $\\beta = W_{LR}(\\theta_2)$.\n  Define a constant dummy test $\\phi^*(x) = \\beta$ for all $x$. The power of this test is constant: $W_{\\phi^*}(\\theta) = \\beta$.\n  Since $\\phi(x)$ corresponds to the likelihood ratio test form (reject for large $T$) for $\\theta_2$ vs $\\theta_1$, it is the MP test of size $\\beta$.\n  By the optimality of the NP Lemma, the power of $\\phi$ at $\\theta_1$ must be at least the power of the competitor $\\phi^*$:\n  $$\n  W_{LR}(\\theta_1) \\ge W_{\\phi^*}(\\theta_1) = \\beta = W_{LR}(\\theta_2)\n  $$\n  Thus, $W_{LR}(\\theta)$ is non-decreasing.\n\n(3) **Size Control**\n\n  Since $W_{LR}(\\theta)$ is non-decreasing and we set $W_{LR}(\\theta_0) = \\alpha$:\n  $$\n  W_{LR}(\\theta) \\le W_{LR}(\\theta_0) = \\alpha \\quad \\text{for all } \\theta \\le \\theta_0\n  $$\n  This proves the test satisfies the size constraint for the composite null $H_0: \\theta \\le \\theta_0$.\n\n(4) **UMP Property**\n\n  Let $\\phi'(x)$ be any other test with size $\\le \\alpha$ for $H_0: \\theta \\le \\theta_0$. This implies $W_{\\phi'}(\\theta_0) \\le \\alpha$.\n  For any specific $\\theta_1 > \\theta_0$, we treat the problem as testing $\\theta_0$ vs $\\theta_1$.\n  Since $\\phi(x)$ is the MP test for $\\theta_0$ vs $\\theta_1$ (from Step 1), and $\\phi'$ is a valid competitor (size $\\le \\alpha$ at $\\theta_0$), we have:\n  $$\n  W_{LR}(\\theta_1) \\ge W_{\\phi'}(\\theta_1)\n  $$\n  Since this holds for all $\\theta_1 > \\theta_0$, $\\phi(x)$ is the Uniformly Most Powerful (UMP) test.\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of statistic T under H0 and H1 with MLR](hypothesis_files/figure-html/fig-mlr-dist-1.png){#fig-mlr-dist fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n\n::: {#exm-ump-gamma}\n### UMP Test for Exponential/Gamma\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$ with pdf $f(x) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nTest $H_0: \\theta = \\theta_0$ vs $H_1: \\theta > \\theta_0$.\nThe sum $T = \\sum X_i$ is a sufficient statistic, and $T \\sim \\text{Gamma}(n, \\theta)$.\n\nThe Likelihood Ratio for $\\theta_1 > \\theta_0$ is:\n\n$$\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n$$\n\nSince $\\theta_1 > \\theta_0$, the term $(\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})$ is positive. Thus, $\\Lambda(x)$ is an increasing function of $\\sum x_i$.\n\nRejecting for large $\\Lambda(x)$ is equivalent to rejecting for $\\sum x_i > C$.\n\n\n$T \\sim \\text{Gamma}(n, \\theta)$\n\nThis test form does not depend on the specific $\\theta_1$, so it is UMP for all $\\theta > \\theta_0$.\n\n:::\n\n\n### Note on Two-Sided Hypotheses\nFor testing $H_0: \\theta = \\theta_0$ vs $H_1: \\theta \\neq \\theta_0$ (e.g., in a Normal distribution), a UMP test generally **does not exist**. This is because the \"best\" rejection region for $\\theta > \\theta_0$ (right tail) is completely different from the \"best\" region for $\\theta < \\theta_0$ (left tail).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}