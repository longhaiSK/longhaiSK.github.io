{
  "hash": "9631ae6816e375d5a4a47ce87816b9a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis Testing\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n## General Terminologies\n\n### Hypothesis Testing \n\nWe formulate the problem of hypothesis testing as deciding between two competing claims about a parameter $\\theta$:\n\n$$\nH_0: \\theta \\in \\Theta_0 \\quad \\text{(Null Hypothesis)}\n$$\n\n$$\nH_1: \\theta \\in \\Theta_1 \\quad \\text{(Alternative Hypothesis)}\n$$\n\n::: {#def-simple-composite}\n### Simple and Composite Hypotheses\nA hypothesis is called **simple** if it specifies a single value for the parameter (e.g., $\\Theta_0$ contains only one point). It is called **composite** if it specifies more than one value.\n:::\n\n::: {#exm-normal-hypotheses}\n### Normal Mean Test\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$.\n\n* If $\\sigma^2$ is known, $H_0: \\mu = \\mu_0$ is a simple hypothesis.\n* If $\\sigma^2$ is unknown, $H_0: \\mu = \\mu_0$ is a composite hypothesis (since $\\sigma^2$ can vary).\n:::\n\n### Test Functions and Size\n\nA test is defined by a **critical region** $C_\\alpha$ such that we reject $H_0$ if the data $x \\in C_\\alpha$. Equivalently, we can define a **test function** $\\phi(x)$ representing the probability of rejecting $H_0$ given data $x$.\n\n* A non-randomized test is given as follows:\n\n$$\n\\phi(x) = I(x \\in C_\\alpha) = \\begin{cases} 1 & \\text{if } x \\in C_\\alpha \\text{ (Reject } H_0 \\text{)} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\n* A randomized test, $\\phi(x)$ can take values in $[0, 1]$, which can be expressed typically as follows:\n\n\n   $$\n   \\phi(x) = \\begin{cases} \n   1 & \\text{if } x \\in C_1 \\\\\n   \\gamma & \\text{if } x \\in C_* \\\\\n   0 & \\text{otherwise}\n   \\end{cases}\n   $$\n\n   where:\n\n   * $C_1$ is the region where we strictly reject $H_0$.\n   * $C_*$ is the boundary region (often where $T(x) = k$) where we reject $H_0$ with probability $\\gamma$.\n\n* More generally, $\\phi(x)$ is just a function of $x$ with values in $[0,1]$, which represents the probability that we will reject $H_0$. \n\n\n::: {#exm-randomized-test}\n### Randomized Test for Binomial\nLet $X \\sim \\text{Bin}(n=10, \\theta)$. Consider testing $H_0: \\theta = 1/2$ vs $H_1: \\theta > 1/2$ with target size $\\alpha = 0.05$.\n\nSuppose we choose a critical region $X \\ge k$.\n\n* If $k=9$, $P(X \\ge 9 | \\theta=0.5) \\approx 0.0107$.\n* If $k=8$, $P(X \\ge 8 | \\theta=0.5) \\approx 0.0547$.\n\nSince we cannot achieve exactly 0.05 with a non-randomized test (the survival function jumps over 0.05), we must use a randomized test function.\n\n\nThe randomized test is defined as:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } x \\in C_1 \\text{ (i.e., } x \\ge 9) \\\\\n\\gamma & \\text{if } x \\in C_* \\text{ (i.e., } x = 8) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nFrom the figure, we see that $\\alpha = 0.05$ lies between $P(X \\ge 9)$ and $P(X \\ge 8)$. We always reject the \"tail\" where probabilities are strictly less than $\\alpha$ (here $x \\ge 9$). At the boundary $x=8$, we cannot reject with probability 1 (which would give total size 0.0547), nor with probability 0 (which would give total size 0.0107).\n\nWe choose $\\gamma$ to bridge this gap:\n\n$$\n\\begin{aligned}\n\\alpha &= P(X \\ge 9) + \\gamma \\cdot P(X = 8) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (P(X \\ge 8) - P(X \\ge 9)) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (0.05469 - 0.01074)\n\\end{aligned}\n$$\n\nSolving for $\\gamma$:\n\n$$\n\\gamma = \\frac{0.05 - 0.01074}{0.04395} \\approx \\frac{39}{44} \\approx 0.89\n$$\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Survival Function P(X >= k) with randomization detail](hypothesis_files/figure-html/fig-binomial-survival-zoom-1.png){#fig-binomial-survival-zoom fig-align='center' width=576 style=\"width: 90% !important;\"}\n:::\n:::\n\n\n\n\n\n## Power and Size Function\n\n::: {#def-test-size}\n### Size of a Test\nThe **size** of a test $\\phi(x)$ is the maximum probability of rejecting the null hypothesis when it is true:\n\n$$\n\\text{Size}(\\phi) = \\sup_{\\theta \\in \\Theta_0} W_\\phi(\\theta) = \\sup_{\\theta \\in \\Theta_0} E_\\theta[\\phi(X)]\n$$\n:::\n\n### Power Definitions\nWe distinguish between the power function varying over parameters and the power metric of a specific test.\n\n**1. Power Function ($W_\\phi(\\theta)$)**\nThe probability of rejecting $H_0$ as a function of the parameter $\\theta$:\n\n$$\nW_\\phi(\\theta) = E_\\theta[\\phi(X)]\n$$\n\n**2. Power of the Test ($\\text{Power}(\\phi)$)**\nIn the context of a specific alternative hypothesis (e.g., $H_1: \\theta = \\theta_1$), we define the power as a scalar functional of $\\phi$:\n\n$$\n\\text{Power}(\\phi) = E_{\\theta_1}[\\phi(X)]\n$$\n\nIdeally, we want:\n\n* $W_\\phi(\\theta) \\le \\text{Size}(\\phi)$ for all $\\theta \\in \\Theta_0$ (Control Type I error).\n* $\\text{Power}(\\phi)$ to be as large as possible (Maximize sensitivity to $H_1$).\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# 1. Define Normal Distribution Parameters\nmu0 <- 0\nmu1 <- 3\nsigma <- 1\nc_val <- 1.5      # Critical value on x-axis (Threshold)\ngamma_val <- 0.5  # Randomization constant at the boundary\n\n# 2. Calculate Scaling for Dual View\n# Peak density of N(0,1) is approx 0.4\nmax_dens <- dnorm(mu0, mean = mu0, sd = sigma)\ny_limit <- max_dens * 1.1\n\n# Set phi=1 to represent 10% of the y-axis range\nphi_scale <- 0.1 * y_limit\n\n# 3. Define the Test Function phi(x) based on x\n# For Normal mean shift (mu1 > mu0), LRT rejects for large x.\n# Step: 0 -> 1 at c_val\ndf_segments <- data.frame(\n  x_start = c(-3, c_val),\n  x_end   = c(c_val, 6),\n  y_start = c(0, phi_scale),\n  y_end   = c(0, phi_scale)\n)\n\nggplot() +\n  # --- Layer 1: Densities (Primary Scale) ---\n  \n  # H0: Normal(0, 1) - Red\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                geom = \"area\", fill = \"red\", alpha = 0.2) +\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                color = \"red\", linetype = \"dashed\") +\n  \n  # H1: Normal(3, 1) - Blue\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                geom = \"area\", fill = \"blue\", alpha = 0.2) +\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                color = \"blue\", linetype = \"dashed\") +\n\n  # --- Layer 2: Scaled Test Function phi(x) ---\n  \n  # The horizontal steps (Indicator of rejection region)\n  geom_segment(data = df_segments, \n               aes(x = x_start, xend = x_end, y = y_start, yend = y_end), \n               size = 1.2) +\n  \n  # The vertical threshold line at critical value c\n  geom_segment(aes(x = c_val, xend = c_val, y = 0, yend = phi_scale), \n               linetype = \"dotted\", color = \"black\") +\n  \n  # Points for gamma and discontinuity (Scaled)\n  geom_point(aes(x = c_val, y = gamma_val * phi_scale), size = 3) + \n  geom_point(aes(x = c_val, y = 0), size = 3, shape = 21, fill = \"white\") +\n  geom_point(aes(x = c_val, y = phi_scale), size = 3, shape = 21, fill = \"white\") +\n\n  # --- Layer 3: Annotations ---\n  \n  annotate(\"text\", x = c_val + 0.2, y = gamma_val * phi_scale, \n           label = expression(gamma), hjust = 0, fontface = \"bold\") +\n  \n  # Label H0 and H1\n  annotate(\"text\", x = mu0, y = dnorm(mu0, mu0, sigma) * 0.9, \n           label = expression(H[0]), color = \"red\", size = 5) +\n  annotate(\"text\", x = mu1, y = dnorm(mu1, mu1, sigma) * 0.9, \n           label = expression(H[1]), color = \"blue\", size = 5) +\n  \n  # Label the Critical Value\n  annotate(\"text\", x = c_val, y = -0.01, label = \"c\", vjust = 1) +\n\n  # --- Layer 4: Axes ---\n  scale_y_continuous(\n    name = \"Density f(x)\",\n    limits = c(-0.02, y_limit), # Small negative limit for \"c\" label\n    expand = c(0, 0),\n    \n    # Secondary Axis for phi\n    sec.axis = sec_axis(~ . / phi_scale, \n                        name = expression(phi(x) ~ \"(Rejection Prob)\"),\n                        breaks = c(0, 1))\n  ) +\n  scale_x_continuous(name = \"Observation x\", limits = c(-3, 6)) +\n  \n  theme_minimal() +\n  theme(\n    axis.title.y.right = element_text(angle = 90, vjust = 0.5),\n    panel.grid.minor = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![Illustration of Test Function, Size, and Power](hypothesis_files/figure-html/fig-lrt-normal-x-1.png){#fig-lrt-normal-x width=672}\n:::\n:::\n\n## The Neyman-Pearson Lemma\n\nConsider testing a simple null against a simple alternative:\n$H_0: \\theta = \\theta_0$ vs $H_1: \\theta = \\theta_1$.\n\nWe define the **Likelihood Ratio** $\\Lambda(x)$ as:\n\n$$\n\\Lambda(x) = \\frac{f_1(x)}{f_0(x)} = \\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\n::: {#def-lrt}\n### Likelihood Ratio Test (LRT)\nA test $\\phi(x)$ is a Likelihood Ratio Test if it has the form:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } \\Lambda(x) > k \\\\\n\\gamma(x) & \\text{if } \\Lambda(x) = k \\\\\n0 & \\text{if } \\Lambda(x) < k\n\\end{cases}\n$$\n\nwhere $k \\ge 0$ is a constant and $0 \\le \\gamma(x) \\le 1$.\n:::\n\n\n\n### Neyman-Pearson Lemma\n::: {#thm-neyman-pearson}\n### Neyman-Pearson Lemma\na) **Optimality:** For any $k$ and $\\gamma(x)$, the LRT $\\phi_0(x)$ defined above has maximum power among all tests whose size is less than or equal to the size of $\\phi_0(x)$.\n\na) **Existence:** Given $\\alpha \\in (0, 1)$, there exist constants $k$ and $\\gamma_0$ such that the LRT defined by this $k$ and $\\gamma(x) = \\gamma_0$ has size exactly $\\alpha$.\n\nc) **Uniqueness:** If a test $\\phi$ has size $\\alpha$ and is of maximum power among all tests of size $\\alpha$, then $\\phi$ is necessarily an LRT, except possibly on a set of measure zero under $H_0$ and $H_1$.\n:::\n\n### A Derivation with The Lagrange Multiplier Approach\n\nTo make the optimality of the Likelihood Ratio Test (LRT) intuitive, we can frame the search for the best test function $\\phi(x)$ as a constrained optimization problem.\n\nWe want to maximize the power of the test:\n\n$$\n\\text{Power}(\\phi) = \\int \\phi(x) f_1(x) dx\n$$\n\nsubject to the constraint on the size of the test $\\alpha$:\n\n$$\n\\text{Size}(\\phi) = \\int \\phi(x) f_0(x) dx = \\alpha\n$$\n\nUsing the method of Lagrange multipliers, we define the objective function $L$ with a multiplier $k$:\n\n$$\nL(\\phi, k) = \\int \\phi(x) f_1(x) dx - k \\left( \\int \\phi(x) f_0(x) dx - \\alpha \\right)\n$$\n\nRearranging the terms inside the integral, we get:\n\n$$\nL(\\phi, k) = \\int \\phi(x) [f_1(x) - k f_0(x)] dx + k\\alpha\n$$\n\nTo maximize $L$ with respect to $\\phi(x)$, we look at the integrand. Since $0 \\le \\phi(x) \\le 1$, we should choose $\\phi(x)$ to be as large as possible whenever its coefficient is positive, and as small as possible whenever its coefficient is negative:\n\n* If $f_1(x) - k f_0(x) > 0$, set $\\phi(x) = 1$.\n* If $f_1(x) - k f_0(x) < 0$, set $\\phi(x) = 0$.\n* If $f_1(x) - k f_0(x) = 0$, the value of $\\phi(x)$ does not affect the integral (this is where $\\gamma$ comes in).\n\nThis decision rule is equivalent to:\n\n$$\n\\phi(x) = \n\\begin{cases} \n1 & \\text{if } \\frac{f_1(x)}{f_0(x)} > k \\\\\n0 & \\text{if } \\frac{f_1(x)}{f_0(x)} < k\n\\end{cases}\n$$\n\nThis is precisely the form of the Likelihood Ratio Test. The \"shadow price\" or Lagrange multiplier $k$ represents the critical threshold that balances the gain in power against the cost of increasing the Type I error.\n\n### Proof of NP Lemma\n\n::: {.proof}\n**Proof of (a) Optimality:**\nLet $\\phi_0$ be the LRT with size $\\alpha$, and $\\phi$ be any other test with size $\\le \\alpha$.\nDefine $U(x) = (\\phi_0(x) - \\phi(x))(f_1(x) - k f_0(x))$.\n\nWe analyze the sign of $U(x)$:\n\n* If $f_1(x) - k f_0(x) > 0 \\implies \\Lambda(x) > k$, then $\\phi_0(x) = 1$. Since $\\phi(x) \\le 1$, $\\phi_0(x) - \\phi(x) \\ge 0$. Thus $U(x) \\ge 0$.\n* If $f_1(x) - k f_0(x) < 0 \\implies \\Lambda(x) < k$, then $\\phi_0(x) = 0$. Since $\\phi(x) \\ge 0$, $\\phi_0(x) - \\phi(x) \\le 0$. Thus $U(x) \\ge 0$.\n* If $f_1(x) - k f_0(x) = 0$, then $U(x) = 0$.\n\nTherefore, $U(x) \\ge 0$ for all $x$. Integrating $U(x)$:\n\n$$\n\\int U(x) dx = \\int (\\phi_0 - \\phi)(f_1 - k f_0) dx \\ge 0\n$$\n\nExpanding the integral:\n\n$$\n\\int \\phi_0 f_1 - \\int \\phi f_1 - k \\left( \\int \\phi_0 f_0 - \\int \\phi f_0 \\right) \\ge 0\n$$\n\n$$\nE_{\\theta_1}[\\phi_0] - E_{\\theta_1}[\\phi] - k (E_{\\theta_0}[\\phi_0] - E_{\\theta_0}[\\phi]) \\ge 0\n$$\n\nSince $E_{\\theta_0}[\\phi_0] = \\alpha$ and $E_{\\theta_0}[\\phi] \\le \\alpha$, the term $(E_{\\theta_0}[\\phi_0] - E_{\\theta_0}[\\phi]) \\ge 0$. Given $k \\ge 0$:\n\n$$\nE_{\\theta_1}[\\phi_0] - E_{\\theta_1}[\\phi] \\ge 0 \\implies \\text{Power}(\\phi_0) \\ge \\text{Power}(\\phi)\n$$\n\n**Proof of (b) Existence:**\nLet $G(k) = P_{\\theta_0}(\\Lambda(X) \\le k)$. $G(k)$ is the cumulative distribution function of the random variable $\\Lambda(X)$, so it is non-decreasing.\nWe seek $k_0$ such that $1 - G(k_0) \\approx \\alpha$.\nBecause of discrete jumps, we might not hit $\\alpha$ exactly.\nWe choose $k_0$ such that:\n\n$$\nP_{\\theta_0}(\\Lambda(X) > k_0) \\le \\alpha \\le P_{\\theta_0}(\\Lambda(X) \\ge k_0)\n$$\n\nSet $\\gamma_0 = \\frac{\\alpha - P_{\\theta_0}(\\Lambda(X) > k_0)}{P_{\\theta_0}(\\Lambda(X) = k_0)}$.\n:::\n\n\n\n\n## Uniformly Most Powerful (UMP) Tests\n\nWhen the alternative hypothesis is composite ($H_1: \\theta \\in \\Theta_1$), we seek a test that is \"best\" for *all* $\\theta \\in \\Theta_1$.\n\n::: {#def-ump}\n### Uniformly Most Powerful Test\nA test $\\phi_0(x)$ of size $\\alpha$ is **Uniformly Most Powerful (UMP)** if:\n\n1.  $E_{\\theta}[\\phi_0(X)] \\le \\alpha$ for all $\\theta \\in \\Theta_0$.\n2.  For any other test $\\phi(x)$ satisfying (1), $E_{\\theta}[\\phi_0(X)] \\ge E_{\\theta}[\\phi(X)]$ for all $\\theta \\in \\Theta_1$.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Power functions of UMP test vs. another test](hypothesis_files/figure-html/fig-ump-power-1.png){#fig-ump-power fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n## Monotone Likelihood Ratio (MLR)\n\n::: {#def-mlr}\n### Monotone Likelihood Ratio\nA family of densities $\\{f(x; \\theta)\\}$ has a **Monotone Likelihood Ratio (MLR)** with respect to a statistic $T(x)$ if for any $\\theta_1 > \\theta_0$, the ratio:\n\n$$\n\\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\nis a non-decreasing function of $T(x)$.\n:::\n\nCommon examples include the one-parameter Exponential Family:\n$f(x; \\theta) = h(x) c(\\theta) \\exp\\{w(\\theta) T(x)\\}$.\nIf $w(\\theta)$ is increasing, the family has MLR w.r.t $T(x)$.\n\n### Karlin-Rubin Theorem {.unnumbered}\n::: {#thm-karlin-rubin}\n### Karlin-Rubin Theorem \nSuppose $X$ has a distribution from a family with MLR with respect to $T(X)$, and the distribution of $T(X)$ is continuous.\nConsider testing $H_0: \\theta \\le \\theta_0$ vs $H_1: \\theta > \\theta_0$.\n\nThe test:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } T(x) > t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n$$\n\nwhere $t_0$ is determined by $P_{\\theta_0}(T(X) > t_0) = \\alpha$, is the UMP size $\\alpha$ test.\n:::\n\n::: {.proof}\n### Proof of Theorem 4.2 (UMP for MLR Families)\n\n**The Test:**\nDefine the test $\\phi(x)$ as:\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } T(x) > t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n$$\nwhere $t_0$ is determined such that the power at the boundary is $\\alpha$, i.e., $W_{LR}(\\theta_0) = \\alpha$.\n\n**(1) Application of Neyman-Pearson Lemma**\nBecause the family has a Monotone Likelihood Ratio (MLR) in $T(x)$, for any specific alternative $\\theta_1 > \\theta_0$, the likelihood ratio $\\Lambda(x)$ is an increasing function of $T(x)$.\nTherefore, the rejection region $T(x) > t_0$ corresponds to $\\Lambda(x) > k$.\nBy the Neyman-Pearson Lemma, $\\phi(x)$ is the Most Powerful (MP) test for testing $H_0': \\theta = \\theta_0$ vs $H_1': \\theta = \\theta_1$.\n\n**(2) Monotonicity of the Power Function**\nWe claim that $W_{LR}(\\theta)$ is a non-decreasing function of $\\theta$.\n\n*Proof:*\nLet $\\theta_2 < \\theta_1$. Consider testing $\\theta = \\theta_2$ vs $\\theta = \\theta_1$.\nLet $\\beta = W_{LR}(\\theta_2)$.\nDefine a constant dummy test $\\phi^*(x) = \\beta$ for all $x$. The power of this test is constant: $W_{\\phi^*}(\\theta) = \\beta$.\nSince $\\phi(x)$ corresponds to the likelihood ratio test form (reject for large $T$) for $\\theta_2$ vs $\\theta_1$, it is the MP test of size $\\beta$.\nBy the optimality of the NP Lemma, the power of $\\phi$ at $\\theta_1$ must be at least the power of the competitor $\\phi^*$:\n$$\nW_{LR}(\\theta_1) \\ge W_{\\phi^*}(\\theta_1) = \\beta = W_{LR}(\\theta_2)\n$$\nThus, $W_{LR}(\\theta)$ is non-decreasing.\n\n**(3) Size Control**\nSince $W_{LR}(\\theta)$ is non-decreasing and we set $W_{LR}(\\theta_0) = \\alpha$:\n$$\nW_{LR}(\\theta) \\le W_{LR}(\\theta_0) = \\alpha \\quad \\text{for all } \\theta \\le \\theta_0\n$$\nThis proves the test satisfies the size constraint for the composite null $H_0: \\theta \\le \\theta_0$.\n\n**(4) UMP Property**\nLet $\\phi'(x)$ be any other test with size $\\le \\alpha$ for $H_0: \\theta \\le \\theta_0$. This implies $W_{\\phi'}(\\theta_0) \\le \\alpha$.\nFor any specific $\\theta_1 > \\theta_0$, we treat the problem as testing $\\theta_0$ vs $\\theta_1$.\nSince $\\phi(x)$ is the MP test for $\\theta_0$ vs $\\theta_1$ (from Step 1), and $\\phi'$ is a valid competitor (size $\\le \\alpha$ at $\\theta_0$), we have:\n$$\nW_{LR}(\\theta_1) \\ge W_{\\phi'}(\\theta_1)\n$$\nSince this holds for all $\\theta_1 > \\theta_0$, $\\phi(x)$ is the Uniformly Most Powerful (UMP) test.\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of statistic T under H0 and H1 with MLR](hypothesis_files/figure-html/fig-mlr-dist-1.png){#fig-mlr-dist fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n\n::: {#exm-ump-gamma}\n### UMP Test for Exponential/Gamma\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$ with pdf $f(x) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nTest $H_0: \\theta = \\theta_0$ vs $H_1: \\theta > \\theta_0$.\nThe sum $T = \\sum X_i$ is a sufficient statistic, and $T \\sim \\text{Gamma}(n, \\theta)$.\n\nThe Likelihood Ratio for $\\theta_1 > \\theta_0$ is:\n\n$$\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n$$\n\nSince $\\theta_1 > \\theta_0$, the term $(\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})$ is positive. Thus, $\\Lambda(x)$ is an increasing function of $\\sum x_i$.\nRejecting for large $\\Lambda(x)$ is equivalent to rejecting for $\\sum x_i > C$.\n\nThis test form does not depend on the specific $\\theta_1$, so it is UMP for all $\\theta > \\theta_0$.\n:::\n\n\n### Note on Two-Sided Hypotheses\nFor testing $H_0: \\theta = \\theta_0$ vs $H_1: \\theta \\neq \\theta_0$ (e.g., in a Normal distribution), a UMP test generally **does not exist**. This is because the \"best\" rejection region for $\\theta > \\theta_0$ (right tail) is completely different from the \"best\" region for $\\theta < \\theta_0$ (left tail).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}