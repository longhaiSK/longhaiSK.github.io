{
  "hash": "170b981f7bb3fe6345d79793e388e85d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hypothesis Testing\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n## General Terminologies\n\n### Hypothesis Testing \n\nWe formulate the problem of hypothesis testing as deciding between two competing claims about a parameter $\\theta$:\n\n$$\nH_0: \\theta \\in \\Theta_0 \\quad \\text{(Null Hypothesis)}\n$$\n\n$$\nH_1: \\theta \\in \\Theta_1 \\quad \\text{(Alternative Hypothesis)}\n$$\n\n::: {#def-simple-composite}\n### Simple and Composite Hypotheses\nA hypothesis is called **simple** if it specifies a single value for the parameter (e.g., $\\Theta_0$ contains only one point). It is called **composite** if it specifies more than one value.\n:::\n\n::: {#exm-normal-hypotheses}\n### Normal Mean Test\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$.\n\n* If $\\sigma^2$ is known, $H_0: \\mu = \\mu_0$ is a simple hypothesis.\n* If $\\sigma^2$ is unknown, $H_0: \\mu = \\mu_0$ is a composite hypothesis (since $\\sigma^2$ can vary).\n:::\n\n### Test Functions and Size\n\nA test is defined by a **critical region** $C_\\alpha$ such that we reject $H_0$ if the data $x \\in C_\\alpha$. Equivalently, we can define a **test function** $\\phi(x)$ representing the probability of rejecting $H_0$ given data $x$.\n\n* A non-randomized test is given as follows:\n\n$$\n\\phi(x) = I(x \\in C_\\alpha) = \\begin{cases} 1 & \\text{if } x \\in C_\\alpha \\text{ (Reject } H_0 \\text{)} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\n* A randomized test, $\\phi(x)$ can take values in $[0, 1]$, which can be expressed typically as follows:\n\n\n   $$\n   \\phi(x) = \\begin{cases} \n   1 & \\text{if } x \\in C_1 \\\\\n   \\gamma & \\text{if } x \\in C_* \\\\\n   0 & \\text{otherwise}\n   \\end{cases}\n   $$\n\n   where:\n\n   * $C_1$ is the region where we strictly reject $H_0$.\n   * $C_*$ is the boundary region (often where $T(x) = k$) where we reject $H_0$ with probability $\\gamma$.\n\n* More generally, $\\phi(x)$ is just a function of $x$ with values in $[0,1]$, which represents the probability that we will reject $H_0$. \n\n\n::: {#exm-randomized-test}\n### Randomized Test for Binomial\nLet $X \\sim \\text{Bin}(n=10, \\theta)$. Consider testing $H_0: \\theta = 1/2$ vs $H_1: \\theta > 1/2$ with target size $\\alpha = 0.05$.\n\nSuppose we choose a critical region $X \\ge k$.\n\n* If $k=9$, $P(X \\ge 9 | \\theta=0.5) \\approx 0.0107$.\n* If $k=8$, $P(X \\ge 8 | \\theta=0.5) \\approx 0.0547$.\n\nSince we cannot achieve exactly 0.05 with a non-randomized test (the survival function jumps over 0.05), we must use a randomized test function.\n:::\n\nThe randomized test is defined as:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } x \\in C_1 \\text{ (i.e., } x \\ge 9) \\\\\n\\gamma & \\text{if } x \\in C_* \\text{ (i.e., } x = 8) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nFrom the figure, we see that $\\alpha = 0.05$ lies between $P(X \\ge 9)$ and $P(X \\ge 8)$. We always reject the \"tail\" where probabilities are strictly less than $\\alpha$ (here $x \\ge 9$). At the boundary $x=8$, we cannot reject with probability 1 (which would give total size 0.0547), nor with probability 0 (which would give total size 0.0107).\n\nWe choose $\\gamma$ to bridge this gap:\n\n$$\n\\begin{aligned}\n\\alpha &= P(X \\ge 9) + \\gamma \\cdot P(X = 8) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (P(X \\ge 8) - P(X \\ge 9)) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (0.05469 - 0.01074)\n\\end{aligned}\n$$\n\nSolving for $\\gamma$:\n\n$$\n\\gamma = \\frac{0.05 - 0.01074}{0.04395} \\approx \\frac{39}{44} \\approx 0.89\n$$\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Full Survival Function P(X >= k) for Bin(10, 0.5) with zoomed detail for test construction.](hypothesis_files/figure-html/fig-binomial-survival-full-1.png){#fig-binomial-survival-full fig-align='center' width=576 style=\"width: 85% !important;\"}\n:::\n:::\n\n\n\n::: {#def-test-size}\n### Size of a Test\nThe **size** of a test $\\phi(x)$, denoted by $\\alpha$, is the maximum probability of rejecting the null hypothesis when it is true:\n\n$$\n\\alpha = \\sup_{\\theta \\in \\Theta_0} \\text{Pr}(\\text{Reject } H_0 \\mid \\theta) = \\sup_{\\theta \\in \\Theta_0} E_\\theta[\\phi(X)]\n$$\n:::\n\n\n## Power Function\n\nThe **power function** of a test, denoted $W(\\theta)$ or $\\beta(\\theta)$, is the probability of rejecting $H_0$ as a function of $\\theta$:\n\n$$\nW(\\theta) = E_\\theta[\\phi(X)]\n$$\n\nIdeally, we want:\n\n1.  $E_\\theta[\\phi(X)] \\le \\alpha$ for all $\\theta \\in \\Theta_0$ (Size control).\n2.  $E_\\theta[\\phi(X)]$ to be as large as possible for $\\theta \\in \\Theta_1$ (High power).\n\n## The Neyman-Pearson Lemma\n\nConsider testing a simple null against a simple alternative:\n$H_0: \\theta = \\theta_0$ vs $H_1: \\theta = \\theta_1$.\n\nWe define the **Likelihood Ratio** $\\Lambda(x)$ as:\n\n$$\n\\Lambda(x) = \\frac{f_1(x)}{f_0(x)} = \\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\n::: {#def-lrt}\n### Likelihood Ratio Test (LRT)\nA test $\\phi(x)$ is a Likelihood Ratio Test if it has the form:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } \\Lambda(x) > k \\\\\n\\gamma(x) & \\text{if } \\Lambda(x) = k \\\\\n0 & \\text{if } \\Lambda(x) < k\n\\end{cases}\n$$\n\nwhere $k \\ge 0$ is a constant and $0 \\le \\gamma(x) \\le 1$.\n:::\n\n::: {#thm-neyman-pearson}\n### Neyman-Pearson Lemma\n**a) Optimality:** For any $k$ and $\\gamma(x)$, the LRT $\\phi_0(x)$ defined above has maximum power among all tests whose size is less than or equal to the size of $\\phi_0(x)$.\n\n**b) Existence:** Given $\\alpha \\in (0, 1)$, there exist constants $k$ and $\\gamma_0$ such that the LRT defined by this $k$ and $\\gamma(x) = \\gamma_0$ has size exactly $\\alpha$.\n\n**c) Uniqueness:** If a test $\\phi$ has size $\\alpha$ and is of maximum power among all tests of size $\\alpha$, then $\\phi$ is necessarily an LRT, except possibly on a set of measure zero under $H_0$ and $H_1$.\n:::\n\n::: {.proof}\n**Proof of (a) Optimality:**\nLet $\\phi_0$ be the LRT with size $\\alpha$, and $\\phi$ be any other test with size $\\le \\alpha$.\nDefine $U(x) = (\\phi_0(x) - \\phi(x))(f_1(x) - k f_0(x))$.\n\nWe analyze the sign of $U(x)$:\n\n* If $f_1(x) - k f_0(x) > 0 \\implies \\Lambda(x) > k$, then $\\phi_0(x) = 1$. Since $\\phi(x) \\le 1$, $\\phi_0(x) - \\phi(x) \\ge 0$. Thus $U(x) \\ge 0$.\n* If $f_1(x) - k f_0(x) < 0 \\implies \\Lambda(x) < k$, then $\\phi_0(x) = 0$. Since $\\phi(x) \\ge 0$, $\\phi_0(x) - \\phi(x) \\le 0$. Thus $U(x) \\ge 0$.\n* If $f_1(x) - k f_0(x) = 0$, then $U(x) = 0$.\n\nTherefore, $U(x) \\ge 0$ for all $x$. Integrating $U(x)$:\n\n$$\n\\int U(x) dx = \\int (\\phi_0 - \\phi)(f_1 - k f_0) dx \\ge 0\n$$\n\nExpanding the integral:\n\n$$\n\\int \\phi_0 f_1 - \\int \\phi f_1 - k \\left( \\int \\phi_0 f_0 - \\int \\phi f_0 \\right) \\ge 0\n$$\n\n$$\nE_{\\theta_1}[\\phi_0] - E_{\\theta_1}[\\phi] - k (E_{\\theta_0}[\\phi_0] - E_{\\theta_0}[\\phi]) \\ge 0\n$$\n\nSince $E_{\\theta_0}[\\phi_0] = \\alpha$ and $E_{\\theta_0}[\\phi] \\le \\alpha$, the term $(E_{\\theta_0}[\\phi_0] - E_{\\theta_0}[\\phi]) \\ge 0$. Given $k \\ge 0$:\n\n$$\nE_{\\theta_1}[\\phi_0] - E_{\\theta_1}[\\phi] \\ge 0 \\implies \\text{Power}(\\phi_0) \\ge \\text{Power}(\\phi)\n$$\n:::\n\n::: {.proof}\n**Proof of (b) Existence:**\nLet $G(k) = P_{\\theta_0}(\\Lambda(X) \\le k)$. $G(k)$ is the cumulative distribution function of the random variable $\\Lambda(X)$, so it is non-decreasing.\nWe seek $k_0$ such that $1 - G(k_0) \\approx \\alpha$.\nBecause of discrete jumps, we might not hit $\\alpha$ exactly.\nWe choose $k_0$ such that:\n\n$$\nP_{\\theta_0}(\\Lambda(X) > k_0) \\le \\alpha \\le P_{\\theta_0}(\\Lambda(X) \\ge k_0)\n$$\n\nSet $\\gamma_0 = \\frac{\\alpha - P_{\\theta_0}(\\Lambda(X) > k_0)}{P_{\\theta_0}(\\Lambda(X) = k_0)}$.\n:::\n\n## Uniformly Most Powerful (UMP) Tests\n\nWhen the alternative hypothesis is composite ($H_1: \\theta \\in \\Theta_1$), we seek a test that is \"best\" for *all* $\\theta \\in \\Theta_1$.\n\n::: {#def-ump}\n### Uniformly Most Powerful Test\nA test $\\phi_0(x)$ of size $\\alpha$ is **Uniformly Most Powerful (UMP)** if:\n\n1.  $E_{\\theta}[\\phi_0(X)] \\le \\alpha$ for all $\\theta \\in \\Theta_0$.\n2.  For any other test $\\phi(x)$ satisfying (1), $E_{\\theta}[\\phi_0(X)] \\ge E_{\\theta}[\\phi(X)]$ for all $\\theta \\in \\Theta_1$.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Power functions of UMP test vs. another test](hypothesis_files/figure-html/fig-ump-power-1.png){#fig-ump-power fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n::: {#exm-ump-gamma}\n### UMP Test for Exponential/Gamma\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$ with pdf $f(x) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nTest $H_0: \\theta = \\theta_0$ vs $H_1: \\theta > \\theta_0$.\nThe sum $T = \\sum X_i$ is a sufficient statistic, and $T \\sim \\text{Gamma}(n, \\theta)$.\n\nThe Likelihood Ratio for $\\theta_1 > \\theta_0$ is:\n\n$$\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n$$\n\nSince $\\theta_1 > \\theta_0$, the term $(\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})$ is positive. Thus, $\\Lambda(x)$ is an increasing function of $\\sum x_i$.\nRejecting for large $\\Lambda(x)$ is equivalent to rejecting for $\\sum x_i > C$.\n\nThis test form does not depend on the specific $\\theta_1$, so it is UMP for all $\\theta > \\theta_0$.\n:::\n\n## Monotone Likelihood Ratio (MLR)\n\n::: {#def-mlr}\n### Monotone Likelihood Ratio\nA family of densities $\\{f(x; \\theta)\\}$ has a **Monotone Likelihood Ratio (MLR)** with respect to a statistic $T(x)$ if for any $\\theta_1 > \\theta_0$, the ratio:\n\n$$\n\\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n$$\n\nis a non-decreasing function of $T(x)$.\n:::\n\nCommon examples include the one-parameter Exponential Family:\n$f(x; \\theta) = h(x) c(\\theta) \\exp\\{w(\\theta) T(x)\\}$.\nIf $w(\\theta)$ is increasing, the family has MLR w.r.t $T(x)$.\n\n::: {#thm-karlin-rubin}\n### Karlin-Rubin Theorem (Theorem 4.2)\nSuppose $X$ has a distribution from a family with MLR with respect to $T(X)$, and the distribution of $T(X)$ is continuous.\nConsider testing $H_0: \\theta \\le \\theta_0$ vs $H_1: \\theta > \\theta_0$.\n\nThe test:\n\n$$\n\\phi(x) = \\begin{cases} \n1 & \\text{if } T(x) > t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n$$\n\nwhere $t_0$ is determined by $P_{\\theta_0}(T(X) > t_0) = \\alpha$, is the UMP size $\\alpha$ test.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of statistic T under H0 and H1 with MLR](hypothesis_files/figure-html/fig-mlr-dist-1.png){#fig-mlr-dist fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n### Note on Two-Sided Hypotheses\nFor testing $H_0: \\theta = \\theta_0$ vs $H_1: \\theta \\neq \\theta_0$ (e.g., in a Normal distribution), a UMP test generally **does not exist**. This is because the \"best\" rejection region for $\\theta > \\theta_0$ (right tail) is completely different from the \"best\" region for $\\theta < \\theta_0$ (left tail).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}