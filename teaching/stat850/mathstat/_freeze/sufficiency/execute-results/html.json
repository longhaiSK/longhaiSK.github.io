{
  "hash": "c78296537b14ec5336f93fd2e1352259",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sufficient Statistic\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n## Sufficient Statistics\n\n::: {#def-sufficient-statistic}\n### Sufficient Statistic\nA statistic $T=T(\\mathbf{X})$ is sufficient for $\\theta$ if one of the following three equivalent conditions holds:\n\n1.  **Parallel Log-Likelihood**\n\n    For any pair of data sets $\\mathbf{x}$ and $\\mathbf{y}$ such that $T(\\mathbf{x})=T(\\mathbf{y})$, the difference in their log-likelihoods ($\\ell(\\theta;\\mathbf{x})=\\ln(f(\\mathbf{x};\\theta)))$ is constant with respect to $\\theta$:\n    $$\n    \\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y}) \\quad \\text{for all } \\theta\n    $$\n    where $c(\\mathbf{x},\\mathbf{y})$ depends only on $\\mathbf{x}$ and $\\mathbf{y}$, not on $\\theta$.\n\n2.  **Factorization of Likelihood**\n\n    The likelihood function of $\\theta$ given $\\mathbf{x}$ can be expressed as:\n    $$\n    L(\\theta; \\mathbf{x}) = h(\\mathbf{x})g(T(\\mathbf{x});\\theta)\n    $$\n    where $h(\\mathbf{x})$ is irrelevant to $\\theta$.\n\n3.  **Non-informative Conditional Distribution of $\\mathbf{X}|T(\\mathbf{X})$**\n\n    The conditional distribution of $\\mathbf{X}$ given $T(\\mathbf{X})=t$, denoted as $f(\\mathbf{x}|t, \\theta)$, is independent of $\\theta$.\n    $$\n    f(\\mathbf{x}|T(\\mathbf{x})=t, \\theta) = f(\\mathbf{x}|t)\n    $$\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Visualizing Parallel Log-Likelihoods](sufficiency_files/figure-html/fig-par-log-likelihood-1.png){#fig-par-log-likelihood fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n::: {#thm-paralell-loglik}\n### Factorization Theorem\nThe three conditions in the definitions of @def-sufficient-statistic are equivalent.\n:::\n\n<details>\n<summary>Click to view Complete Proof of Equivalence</summary>\n\n::: {.proof}\n**Proof of Equivalence**\n\nWe show the equivalence by proving the implications in a cycle or pairs: $(2 \\Rightarrow 1)$, $(1 \\Rightarrow 2)$, $(2 \\Rightarrow 3)$, and $(3 \\Rightarrow 2)$.\n\n1.  **Factorization $\\Rightarrow$ Log-Likelihood Difference $(2 \\Rightarrow 1)$**\n\n    Assume the **Factorization Theorem** holds: $L(\\theta; \\mathbf{x}) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)$.\n    Consider any pair $\\mathbf{x}, \\mathbf{y}$ such that $T(\\mathbf{x}) = T(\\mathbf{y})$.\n    $$\n    \\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = [\\ln h(\\mathbf{x}) + \\ln g(T(\\mathbf{x}); \\theta)] - [\\ln h(\\mathbf{y}) + \\ln g(T(\\mathbf{y}); \\theta)]\n    $$\n    Since $T(\\mathbf{x})=T(\\mathbf{y})$, the terms $\\ln g(T(\\mathbf{x}); \\theta)$ and $\\ln g(T(\\mathbf{y}); \\theta)$ are identical and cancel out.\n    $$\n    \\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = \\ln h(\\mathbf{x}) - \\ln h(\\mathbf{y})\n    $$\n    This difference depends only on $\\mathbf{x}$ and $\\mathbf{y}$ (via $h$), and is independent of $\\theta$. Thus, condition 1 holds.\n\n2.  **Log-Likelihood Difference $\\Rightarrow$ Factorization $(1 \\Rightarrow 2)$**\n\n    Assume Condition 1 holds. For any $\\mathbf{x}$ and $\\mathbf{y}$ with $T(\\mathbf{x})=T(\\mathbf{y})$, $\\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y})$.\n    Exponentiating, we get $L(\\theta; \\mathbf{x}) = k(\\mathbf{x}, \\mathbf{y}) L(\\theta; \\mathbf{y})$, where $k$ is independent of $\\theta$.\n\n    For each value $t$ in the range of $T$, select a fixed representative data point $\\mathbf{x}_t$ such that $T(\\mathbf{x}_t) = t$.\n    For any data point $\\mathbf{x}$, let $t = T(\\mathbf{x})$. Using the relation above:\n    $$\n    L(\\theta; \\mathbf{x}) = k(\\mathbf{x}, \\mathbf{x}_t) L(\\theta; \\mathbf{x}_t)\n    $$\n    Define $h(\\mathbf{x}) = k(\\mathbf{x}, \\mathbf{x}_{T(\\mathbf{x})})$ and $g(t; \\theta) = L(\\theta; \\mathbf{x}_t)$.\n    Then:\n    $$\n    L(\\theta; \\mathbf{x}) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)\n    $$\n    This is exactly the Factorization form.\n\n3.  **Factorization $\\Rightarrow$ Conditional Distribution $(2 \\Rightarrow 3)$**\n\n    Assume $f(\\mathbf{x}; \\theta) = h(\\mathbf{x})g(T(\\mathbf{x}); \\theta)$. We derive the conditional distribution $P(\\mathbf{X}=\\mathbf{x} | T(\\mathbf{X})=t)$.\n    If $T(\\mathbf{x}) \\neq t$, the probability is 0 (independent of $\\theta$).\n    If $T(\\mathbf{x}) = t$:\n    $$\n    P(\\mathbf{X}=\\mathbf{x} | T(\\mathbf{X})=t) = \\frac{P(\\mathbf{X}=\\mathbf{x}, T(\\mathbf{X})=t)}{P(T(\\mathbf{X})=t)} = \\frac{f(\\mathbf{x}; \\theta)}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} f(\\mathbf{y}; \\theta)}\n    $$\n    Substitute the factorization:\n    $$\n    = \\frac{h(\\mathbf{x})g(t; \\theta)}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})g(t; \\theta)} = \\frac{h(\\mathbf{x})g(t; \\theta)}{g(t; \\theta) \\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})}\n    $$\n    The term $g(t; \\theta)$ cancels out:\n    $$\n    = \\frac{h(\\mathbf{x})}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})}\n    $$\n    This expression depends only on $\\mathbf{x}$ and $h(\\cdot)$, and is entirely free of $\\theta$. Thus, Condition 3 holds.\n\n4.  **Conditional Distribution $\\Rightarrow$ Factorization $(3 \\Rightarrow 2)$**\n\n    Assume $f(\\mathbf{x} | T(\\mathbf{x}); \\theta) = k(\\mathbf{x})$, where $k(\\mathbf{x})$ is independent of $\\theta$.\n    We can write the joint distribution as:\n    $$\n    f(\\mathbf{x}; \\theta) = f(\\mathbf{x} | T(\\mathbf{x})=t; \\theta) \\cdot P(T(\\mathbf{X})=t; \\theta)\n    $$\n    Substitute the assumption:\n    $$\n    f(\\mathbf{x}; \\theta) = k(\\mathbf{x}) \\cdot P(T(\\mathbf{X})=T(\\mathbf{x}); \\theta)\n    $$\n    Let $h(\\mathbf{x}) = k(\\mathbf{x})$ and $g(t; \\theta) = P(T(\\mathbf{X})=t; \\theta)$.\n    Then:\n    $$\n    f(\\mathbf{x}; \\theta) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)\n    $$\n    This recovers the Factorization form.\n:::\n</details>\n\n::: {#exm-uniform-sufficiency}\n### Uniform Distribution $U(\\theta-1, \\theta+1)$\nConsider a random sample $\\mathbf{X} = (X_1, \\dots, X_n)$ from a Uniform distribution with range $(\\theta-1, \\theta+1)$.\n\nThe density for a single observation is:\n$$\nf(x_i|\\theta) = \\frac{1}{(\\theta+1) - (\\theta-1)} I(\\theta-1 < x_i < \\theta+1) = \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\nThe joint PDF (likelihood) for the vector $\\mathbf{x}$ is:\n$$\nL(\\theta; \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\n$$\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( \\min(x_i) > \\theta-1 ) \\cdot I( \\max(x_i) < \\theta+1 )\n$$\n\nUsing order statistics notation where $X_{(1)} = \\min(X_i)$ and $X_{(n)} = \\max(X_i)$:\n$$\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( \\theta < X_{(1)} + 1 ) \\cdot I( \\theta > X_{(n)} - 1 )\n$$\n\n$$\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )\n$$\n\nBy the **Factorization Theorem**, we can define:\n\n* $h(\\mathbf{x}) = 2^{-n}$ (or simply 1, grouping constants into $g$)\n* $g(T(\\mathbf{x}), \\theta) = I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )$\n\nThus, the sufficient statistic is the pair of order statistics:\n$$\nT(\\mathbf{X}) = (X_{(1)}, X_{(n)})\n$$\n:::\n\n::: {#exm-gamma-sufficiency}\n### Gamma Distribution\nLet $\\mathbf{X} = (X_1, \\dots, X_n)$ be i.i.d. $\\Gamma(\\alpha, \\beta)$. The pdf is:\n\n$$\nf(x_i|\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}, \\quad x_i > 0\n$$\n\nThe joint likelihood is:\n\n$$\nL(\\alpha, \\beta; \\mathbf{x}) = \\left( \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\right)^n \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nBy the Factorization Theorem, we can identify the parts that depend on the data and the parameters:\n$$\ng(T(\\mathbf{x}), \\mathbf{\\theta}) = \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nThus, the sufficient statistics are:\n$$\nT(\\mathbf{X}) = \\left( \\prod_{i=1}^n X_i, \\sum_{i=1}^n X_i \\right)\n$$\n:::\n\n::: {#exm-exp-family-sufficiency}\n### Sufficient Statistic of Exponential Family\nMany common distributions (Normal, Poisson, Gamma, Binomial) belong to the **Exponential Family**, which has a density in the form:\n\n$$\nf(\\mathbf{x}|\\theta) = h(\\mathbf{x}) c(\\theta) \\exp\\left( \\sum_{j=1}^k \\pi_j(\\theta) t_j(\\mathbf{x}) \\right)\n$$\n\nThen, by the Factorization Theorem, the statistic:\n\n$$\nT(\\mathbf{X}) = \\left( \\sum_{i=1}^n t_1(x_i), \\dots, \\sum_{i=1}^n t_k(x_i) \\right)\n$$\n\nis a sufficient statistic for $\\theta$.\n:::\n\n::: {#exm-bernoulli-expfamily}\n### Bernoulli as Exponential Family\nLet $X_1, \\dots, X_n \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)$.\nTo find the sufficient statistic, we write the **Joint PDF** of the sample in the canonical Exponential Family form:\n\n$$\nf(\\mathbf{x}|\\theta) = h(\\mathbf{x}) c(\\theta) \\exp\\left( \\sum_{j=1}^k \\pi_j(\\theta) T_j(\\mathbf{x}) \\right)\n$$\n\n1.  **Write the Joint PDF**\n\n    $$\n    f(\\mathbf{x}|p) = \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}\n    $$\n\n2.  **Convert to Exponential Form**\n\n    $$\n    \\begin{aligned}\n    f(\\mathbf{x}|p) &= \\exp\\left( \\sum_{i=1}^n \\left[ x_i \\ln p + (1-x_i) \\ln(1-p) \\right] \\right) \\\\\n    &= \\exp\\left( \\sum_{i=1}^n \\left[ x_i \\ln p + \\ln(1-p) - x_i \\ln(1-p) \\right] \\right) \\\\\n    &= \\exp\\left( \\sum_{i=1}^n \\ln(1-p) + \\sum_{i=1}^n x_i \\left[ \\ln p - \\ln(1-p) \\right] \\right)\n    \\end{aligned}\n    $$\n\n3.  **Factor into Components**\n    We separate the terms to match the definition:\n\n    $$\n    f(\\mathbf{x}|p) = \\underbrace{1}_{h(\\mathbf{x})} \\cdot \\underbrace{(1-p)^n}_{c(p)} \\cdot \\exp\\left( \\underbrace{\\ln\\left(\\frac{p}{1-p}\\right)}_{\\pi_1(p)} \\underbrace{\\sum_{i=1}^n x_i}_{T_1(\\mathbf{x})} \\right)\n    $$\n\n**Conclusion:**\nBy inspection of the exponent, the statistic coupled with the parameter $\\pi_1(p)$ is the sufficient statistic:\n\n$$\nT(\\mathbf{X}) = \\sum_{i=1}^n X_i\n$$\n:::\n\n::: {#rem-sufficient-likelihood}\n\n### Sufficient Statistic is the sufficient \"Parameter\" of Likelihood\nThere is a dual relationship between the sufficient statistic and the parameter $\\theta$. Conventionally, we view $f(x|\\theta)$ as a function of $x$ parameterized by $\\theta$.\n\nHowever, in Bayesian inference or likelihood theory, we often view the likelihood $L(\\theta; x)$ as a function of $\\theta$ determined by the observed data $x$. The Factorization Theorem implies:\n\n$$\nL(\\theta; \\mathbf{x}) \\propto g(T(\\mathbf{x})|\\theta)\n$$\n\nThis suggests that $T(\\mathbf{x})$ completely determines the shape of the likelihood function. In this specific sense, the sufficient statistic $T(\\mathbf{x})$ acts as the **\"parameter\"** of the likelihood function itself.\n\nFor the exponential family that we will discuss below, this duality is explicit:\n\n$$\n\\log L(\\theta; \\mathbf{x}) = \\text{const} + \\sum_{i=1}^k \\eta_i(\\theta) T_i(\\mathbf{x}) - n A(\\theta)\n$$\n\nHere, $T_i(\\mathbf{x})$ serves as the coefficient (or parameter) for the function $\\eta_i(\\theta)$.\n\n:::\n\n## Minimal Sufficient Statistics\n\n::: {#def-mss}\n### Minimal Sufficient Statistic (MSS)\n\nA statistic $T(X)$ is a **Minimal Sufficient Statistic** if:\n\n1.  **Sufficiency:** $T(X)$ is a sufficient statistic for $\\theta$.\n   2.  **Minimality:** For any other sufficient statistic $S(X)$, $T(X)$ is a function of $S(X)$.\n    $$\n    T(X) = g(S(X))\n    $$\n    *(This implies that $T(X)$ provides the greatest possible data reduction without losing information about $\\theta$. If $S(x) = S(y)$, then it must be that $T(x) = T(y)$).*\n\n:::\n\n::: {#thm-mss-loglik}\n\n### MSS Condition Theorem\nLet $T(X)$ be a **sufficient statistic**. $T(X)$ is a **Minimal Sufficient Statistic (MSS)** if and only if for any pair of data sets $x$ and $y$:\n$$\n\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y) \\text{ for all } \\theta \\implies T(x) = T(y)\n$$\nwhere $c(x, y)$ is a constant independent of $\\theta$.\n\n:::\n\n<details>\n<summary>Click to view the Proof</summary>\n\n::: {.proof}\n\n\n**Direction 1: Sufficiency (Implication holds $\\implies$ $T$ is MSS)**\n\nAssume that for any $x, y$, $[\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)] \\implies T(x) = T(y)$. We must show that $T$ is a function of *any* sufficient statistic $U$.\n\n1.  Let $U(X)$ be any sufficient statistic. Assume $U(x) = U(y)$.\n   2.  By the **Factorization Theorem**, the likelihoods are:\n    $$\n    L(\\theta; x) = h(x)g(U(x), \\theta)\n    $$\n    $$\n    L(\\theta; y) = h(y)g(U(y), \\theta)\n    $$\n\n3.  Since $U(x) = U(y)$, the factor $g(U(x), \\theta)$ is identical to $g(U(y), \\theta)$.\n    Taking the log-ratio:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = \\ln h(x) - \\ln h(y)\n    $$\n    The term $\\ln h(x) - \\ln h(y)$ depends only on $x$ and $y$, not on $\\theta$. Let this be $c(x,y)$.\n    $$\n    \\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\n    $$\n\n4.  By our main assumption, this condition implies $T(x) = T(y)$.\n   5.  Thus, we have shown that $U(x) = U(y) \\implies T(x) = T(y)$. This means $T$ is a function of $U$.\n    Since $U$ is arbitrary, $T$ is Minimal Sufficient.\n\n**Direction 2: Necessity ($T$ is MSS $\\implies$ Implication holds)**\n\nAssume $T$ is Minimal Sufficient. We must prove that if $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ for all $\\theta$, then $T(x) = T(y)$.\n\n1.  **Define the Statistic $S(x)$:**\n    Let $S(x)$ be the set of all possible datasets $z$ which give the same log-likelihood shape as $x$:\n    $$\n    S(x) = \\{z \\mid \\ell(\\theta; z) = \\ell(\\theta; x) + c_z \\text{ for all } \\theta \\}\n    $$\n    This statistic $S(x)$ represents the equivalence class of $x$ under the parallel log-likelihood relationship.\n    If the condition $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ holds, then by definition $x$ and $y$ generate the same equivalence class, so $S(x) = S(y)$.\n\n2.  **Show $S(x)$ is Sufficient (Directly via Likelihood Ratio):**\n    To prove $S$ is sufficient, we check the **Likelihood Ratio Condition** (Condition 2 from Section 1.1).\n    Suppose $S(x) = S(y)$. By the definition of $S$, this implies:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y)\n    $$\n    By the definition of sufficiency, $S(X)$ is a sufficient statistic.\n\n3.  **Use Minimality of $T$:**\n    Since $T$ is a **Minimal** Sufficient Statistic, it is a function of *any* sufficient statistic.\n    Therefore, $T$ must be a function of $S$. That is, $T(x) = f(S(x))$.\n\n4.  **Conclusion:**\n    Assume $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$.\n    Then $S(x) = S(y)$.\n    Consequently, $T(x) = f(S(x)) = f(S(y)) = T(y)$.\n\n:::\n\n</details>\n\n\n::: {#exm-bernoulli-mss-check}\n\n### Checking Minimality via Log-Likelihood Condition\nLet $X_1, X_2, X_3 \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)$.\nWe determine the MSS by checking the implication from the **MSS Condition Theorem**:\n$$\n\\text{Parallel Log-Likelihoods} \\implies T(x) = T(y)\n$$\n\n**Step 1: Establishing the MSS**\n\nFirst, we find the condition under which two log-likelihoods are parallel.\n$$\n\\ell(p; x) = (\\sum x_i) \\ln p + (n - \\sum x_i) \\ln(1-p)\n$$\nThe difference $\\ell(p; x) - \\ell(p; y)$ depends on $p$ only through the term $(\\sum x_i - \\sum y_i) \\ln \\frac{p}{1-p}$.\nFor this difference to be constant (independent of $p$), the coefficient must be zero:\n$$\n\\text{Parallel Log-Likelihoods} \\iff \\sum x_i = \\sum y_i\n$$\nThe statistic that corresponds exactly to this condition is $T(X) = \\sum X_i$.\nSince $\\sum x_i = \\sum y_i$ trivially implies $T(x) = T(y)$, $T(X)$ is the **Minimal Sufficient Statistic**.\n\n**Step 2: Why $S(X) = (X_1, \\sum_{i=2}^3 X_i)$ is NOT Minimal**\n\nNow consider the \"richer\" statistic $S(X)$. If $S$ were minimal, the parallel condition must imply $S(x) = S(y)$.\nWe check:\n$$\n\\sum x_i = \\sum y_i \\stackrel{?}{\\implies} (x_1, \\sum_{i=2}^3 x_i) = (y_1, \\sum_{i=2}^3 y_i)\n$$\n**Counter-Example:**\n\nLet $x = (1, 0, 1)$ and $y = (0, 1, 1)$.\n\n1.  **Check Parallel Condition:**\n   \n    $\\sum x_i = 2$ and $\\sum y_i = 2$.\n    The sums are equal, so the log-likelihoods are parallel.\n\n2.  **Check Statistic Equality:**\n   \n    $$S(x) = (1, 1)$$\n    $$S(y) = (0, 2)$$\n    $$S(x) \\neq S(y)$$\n\n**Conclusion:** The parallel condition holds, but $S(x) \\neq S(y)$. The implication fails.\nThis proves that $S(X)$ is **not** minimalâ€”it retains \"extra\" information (the position of the first success) that is not relevant to the likelihood shape.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}