{
  "hash": "b94bd4302fec110ac71886ae67cf7a5b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Likelihood Theory\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Definitions and Notations\n\n### Regular Family\n\n::: {#def-regular-family}\n\n\n### Regular Families\nA family of probability density functions is said to be a **Regular Family** if the support $\\{\\mathbf{x} : f(\\mathbf{x}|\\boldsymbol{\\theta}) > 0\\}$ does not depend on the parameter vector $\\boldsymbol{\\theta}$.\n\nThis condition allows for the interchange of differentiation and integration:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\int \\exp\\{\\ell(\\boldsymbol{\\theta}; \\mathbf{x})\\} d\\mathbf{x} = \\int \\nabla_{\\boldsymbol{\\theta}} \\exp\\{\\ell(\\boldsymbol{\\theta}; \\mathbf{x})\\} d\\mathbf{x}\n$$\n\n:::\n\n### Score and Fisher Information\n\nBefore stating the theorem, we define the following notations for the score and information in the context of a parameter vector $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_p)^T \\in \\mathbb{R}^p$:\n\n:::{#def-score-fisher}\n\n1.  **Score Vector ($\\mathbf{U}$):** The gradient of the log-likelihood. It is a random column vector of dimension $p \\times 1$.\n    $$\n    \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) = \\nabla \\ell(\\boldsymbol{\\theta}; \\mathbf{X}) = \\frac{\\partial \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\boldsymbol{\\theta}} = \n    \\begin{bmatrix}\n    \\frac{\\partial \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\theta_1} \\\\[6pt]\n    \\frac{\\partial \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\theta_2} \\\\[6pt]\n    \\vdots \\\\[6pt]\n    \\frac{\\partial \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\theta_p}\n    \\end{bmatrix}\n    $$\n\n2.  **Observed Information Matrix ($\\mathbf{J}$):** The negative Hessian of the log-likelihood. It is a symmetric random matrix of dimension $p \\times p$, measuring the curvature of the log-likelihood surface.\n    $$\n    \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) = - \\nabla^2 \\ell(\\boldsymbol{\\theta}; \\mathbf{X}) = - \\frac{\\partial^2 \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^T} = -\n    \\begin{bmatrix}\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_1^2} & \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_p} \\\\[8pt]\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_2 \\partial \\theta_1} & \\frac{\\partial^2 \\ell}{\\partial \\theta_2^2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_2 \\partial \\theta_p} \\\\[8pt]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[8pt]\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_p \\partial \\theta_1} & \\frac{\\partial^2 \\ell}{\\partial \\theta_p \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_p^2}\n    \\end{bmatrix}\n    $$\n\n3.  **(Expected) Fisher Information Matrix ($\\mathbf{I}$):** The covariance matrix of the score vector. It is a deterministic $p \\times p$ matrix (for a fixed $\\boldsymbol{\\theta}$).\n    $$\n    \\mathbf{I}(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}} \\left[ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) \\right]\n    $$\n\n:::\n\n## Mean and Covariance of Score Vector\n\n::: {#thm-score-identities}\n\n## Bartlett's Identities: Mean and Covariance of Score Vector\n\nLet $\\{f(\\mathbf{x}|\\boldsymbol{\\theta}) : \\boldsymbol{\\theta} \\in \\Theta\\}$ be a regular family of probability density functions. The following identities hold relating the moments of the score vector $\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})$ and the observed information matrix $\\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X})$:\n\n1.  **First Moment Identity:** The expected score is zero vector.\n    $$\n    E_{\\boldsymbol{\\theta}} [ \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) ] = \\mathbf{0}\n    $$\n\n2.  **Second Moment Identity:** The expected observed information equals the covariance of the score vector (Fisher Information).\n    $$\n    \\text{Cov}_{\\boldsymbol{\\theta}} \\left( \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) \\right) = E_{\\boldsymbol{\\theta}} [ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) ] = \\mathbf{I}(\\boldsymbol{\\theta})\n    $$\n\n:::\n\n:::{#rem-score-moments}\nThe only assumption in the theorem above is that the families are regular. Therefore, we do not need to assume the log-likelihood $\\ell(\\boldsymbol{\\theta})$ is \"well-behaved\" (e.g., approximately quadratic or independence within $\\mathbf{X}$) for these two identities to hold.\n\n:::\n\n::: {.proof}\n\n1. Proof of the First Moment Identity\n\nWe start with the fundamental property that a density function integrates to 1 over the sample space of $\\mathbf{X}$:\n$$\n\\int f(\\mathbf{x}|\\boldsymbol{\\theta}) \\, d\\mathbf{x} = 1\n$$\nDifferentiating both sides with respect to the parameter vector $\\boldsymbol{\\theta}$:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\int f(\\mathbf{x}|\\boldsymbol{\\theta}) \\, d\\mathbf{x} = \\mathbf{0}\n$$\nAssuming regularity allows us to interchange differentiation and integration:\n$$\n\\int \\nabla_{\\boldsymbol{\\theta}} f(\\mathbf{x}|\\boldsymbol{\\theta}) \\, d\\mathbf{x} = \\mathbf{0}\n$$\nUsing the identity $\\nabla_{\\boldsymbol{\\theta}} f(\\mathbf{x}|\\boldsymbol{\\theta}) = f(\\mathbf{x}|\\boldsymbol{\\theta}) \\nabla_{\\boldsymbol{\\theta}} \\log f(\\mathbf{x}|\\boldsymbol{\\theta}) = f(\\mathbf{x}|\\boldsymbol{\\theta}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x})$:\n$$\n\\int \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) f(\\mathbf{x}|\\boldsymbol{\\theta}) \\, d\\mathbf{x} = \\mathbf{0}\n$$\nThis is precisely the definition of the expectation:\n$$\nE_{\\boldsymbol{\\theta}} [ \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) ] = \\mathbf{0}\n$$\n\n2. Proof of the Second Moment Identity\n\nWe differentiate the result of the First Moment Identity ($E[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})]=\\mathbf{0}$) with respect to $\\boldsymbol{\\theta}^T$.\n$$\n\\nabla_{\\boldsymbol{\\theta}^T} \\int \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) f(\\mathbf{x}|\\boldsymbol{\\theta}) \\, d\\mathbf{x} = \\mathbf{0}\n$$\nApplying the product rule inside the integral (remembering $\\mathbf{U}$ is a vector):\n$$\n\\int \\left[ \\left( \\nabla_{\\boldsymbol{\\theta}^T} \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) \\right) f(\\mathbf{x}|\\boldsymbol{\\theta}) + \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) \\left( \\nabla_{\\boldsymbol{\\theta}^T} f(\\mathbf{x}|\\boldsymbol{\\theta}) \\right) \\right] d\\mathbf{x} = \\mathbf{0}\n$$\nWe analyze the two terms in the bracket:\n\n* **Term 1:** $\\nabla_{\\boldsymbol{\\theta}^T} \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x})$ is the Jacobian of the score, which is the Hessian of the log-likelihood, $\\nabla^2 \\ell(\\boldsymbol{\\theta}; \\mathbf{x})$. By definition, this is $-\\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{x})$.\n* **Term 2:** We use the identity $\\nabla_{\\boldsymbol{\\theta}^T} f(\\mathbf{x}|\\boldsymbol{\\theta}) = f(\\mathbf{x}|\\boldsymbol{\\theta}) (\\nabla_{\\boldsymbol{\\theta}} \\log f(\\mathbf{x}|\\boldsymbol{\\theta}))^T = f(\\mathbf{x}|\\boldsymbol{\\theta}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x})^T$.\n\nSubstituting these back into the integral:\n$$\n\\int \\left[ -\\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{x}) f(\\mathbf{x}|\\boldsymbol{\\theta}) + \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x})^T f(\\mathbf{x}|\\boldsymbol{\\theta}) \\right] d\\mathbf{x} = \\mathbf{0}\n$$\nThis simplifies to expectations:\n$$\n-E_{\\boldsymbol{\\theta}} [ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) ] + E_{\\boldsymbol{\\theta}} [ \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})^T ] = \\mathbf{0}\n$$\nRearranging gives:\n$$\nE_{\\boldsymbol{\\theta}} [ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) ] = E_{\\boldsymbol{\\theta}} [ \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})^T ]\n$$\nFinally, recall the definition of the covariance matrix for a random vector with zero mean. Since $E_{\\boldsymbol{\\theta}}[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})] = \\mathbf{0}$, we have:\n$$\n\\text{Cov}_{\\boldsymbol{\\theta}}(\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})) = E_{\\boldsymbol{\\theta}}[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})^T] - E_{\\boldsymbol{\\theta}}[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})]E_{\\boldsymbol{\\theta}}[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})]^T = E_{\\boldsymbol{\\theta}}[\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})^T]\n$$\nTherefore, we conclude:\n$$\n\\text{Cov}_{\\boldsymbol{\\theta}}(\\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}))=E_{\\boldsymbol{\\theta}} [ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) ] = \\mathbf{I}(\\boldsymbol{\\theta})\n$$\n\n:::\n\n## Cramer-Rao Lower Bound\n\nIn estimation theory, we often wish to know the limit of how well a parameter can be estimated. The following theorem provides a lower bound on the variance of any estimator.\n\n::: {#thm-crlb}\n\n### Cramer-Rao Lower Bound for Scalar Estimator\n\nLet $X$ be a random variable with probability density function (or probability mass function) $f(x|\\theta)$, where $\\theta \\in \\Theta$ is a scalar unknown parameter. Let $T(X)$ be any estimator with finite variance, and let $m(\\theta) = E_\\theta[T(X)]$ denote its expectation.\n\nAssume the following **regularity conditions** hold:\n\n1.  The support of $X$, denoted $\\mathcal{X} = \\{x : f(x|\\theta) > 0\\}$, does not depend on $\\theta$.\n\n2.  The differentiation with respect to $\\theta$ and integration (or summation) with respect to $x$ can be interchanged.\n\nThen, the variance of $T(X)$ satisfies:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n\nwhere $I(\\theta) = E_\\theta \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta) \\right)^2 \\right]$ is the scalar Fisher Information.\n\n**Particular Case:** If $T(X)$ is an **unbiased** estimator of $\\theta$ (i.e., $m(\\theta) = \\theta$ and $m'(\\theta)=1$), then:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{1}{I(\\theta)}\n$$\n\n:::\n\n::: {.proof}\nLet $U = \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)$ be the scalar Score function. From the properties of the Score function under the stated regularity conditions, we know that the score has mean zero and variance equal to the Fisher Information:\n\n$$\nE_\\theta[U] = 0 \\quad \\text{and} \\quad \\text{Var}_\\theta(U) = I(\\theta)\n$$\n\nConsider the covariance between the estimator $T(X)$ and the Score $U$. By the Cauchy-Schwarz inequality (applied to covariance), we have:\n\n$$\n[\\text{Cov}_\\theta(T, U)]^2 \\le \\text{Var}_\\theta(T) \\text{Var}_\\theta(U)\n$$\n\nWe now evaluate the covariance term explicitly. By definition:\n\n$$\n\\begin{aligned}\n\\text{Cov}_\\theta(T, U) &= E_\\theta[T(X) U] - E_\\theta[T]E_\\theta[U] \\\\\n&= E_\\theta\\left[ T(X) \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta) \\right] - m(\\theta) \\cdot 0 \\\\\n&= \\int T(x) \\left( \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} \\right) f(x|\\theta) \\, dx \\\\\n&= \\int T(x) \\left( \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} \\right) f(x|\\theta) \\, dx \\\\\n&= \\int T(x) \\frac{\\partial f(x|\\theta)}{\\partial \\theta} \\, dx\n\\end{aligned}\n$$\n\nInvoking the regularity condition that allows the interchange of derivative and integral, we move the derivative outside the integral:\n\n$$\n\\text{Cov}_\\theta(T, U) = \\frac{\\partial}{\\partial \\theta} \\int T(x) f(x|\\theta) \\, dx = \\frac{\\partial}{\\partial \\theta} E_\\theta[T(X)] = m'(\\theta)\n$$\n\nSubstituting this result and $\\text{Var}_\\theta(U) = I(\\theta)$ back into the covariance inequality:\n\n$$\n[m'(\\theta)]^2 \\le \\text{Var}_\\theta(T) \\cdot I(\\theta)\n$$\n\nRearranging the terms yields the desired lower bound:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n\n:::\n\n\n## Multivariate Cramer-Rao Lower Bound\n\n::: {#thm-multivariate-crlb}\n\n### Multivariate Cramer-Rao Lower Bound\n\nLet $\\mathbf{X}$ be a random vector with density $f(\\mathbf{x}|\\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta} \\in \\mathbb{R}^p$ is a vector of unknown parameters. Let $\\mathbf{T}(\\mathbf{X}) \\in \\mathbb{R}^k$ be any estimator with finite covariance matrix, and let $\\mathbf{m}(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}}[\\mathbf{T}(\\mathbf{X})]$ denote its expectation vector.\n\nLet $\\mathbf{I}(\\boldsymbol{\\theta})$ be the $p \\times p$ Fisher Information Matrix:\n\n$$\n\\mathbf{I}(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}} \\left[ \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X}) \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{X})^\\top \\right]\n$$\n\nLet $\\mathbf{D}(\\boldsymbol{\\theta}) = \\frac{\\partial \\mathbf{m}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$ be the $k \\times p$ Jacobian matrix of the expectation, where $D_{ij} = \\frac{\\partial m_i}{\\partial \\theta_j}$.\n\nUnder standard regularity conditions, the covariance matrix of $\\mathbf{T}$ satisfies the inequality:\n\n$$\n\\text{Var}_{\\boldsymbol{\\theta}}(\\mathbf{T}) \\succeq \\mathbf{D}(\\boldsymbol{\\theta}) [\\mathbf{I}(\\boldsymbol{\\theta})]^{-1} \\mathbf{D}(\\boldsymbol{\\theta})^\\top\n$$\n\nHere, $\\mathbf{A} \\succeq \\mathbf{B}$ means that the matrix $\\mathbf{A} - \\mathbf{B}$ is positive semi-definite (i.e., for any vector $\\mathbf{v}$, $\\mathbf{v}^\\top (\\mathbf{A} - \\mathbf{B}) \\mathbf{v} \\ge 0$).\n\n:::\n\n::: {.proof}\nLet $\\mathbf{U} = \\nabla_{\\boldsymbol{\\theta}} \\log f(\\mathbf{X}|\\boldsymbol{\\theta})$ be the $p \\times 1$ Score vector. We know that $E[\\mathbf{U}] = \\mathbf{0}$ and $\\text{Var}(\\mathbf{U}) = \\mathbf{I}(\\boldsymbol{\\theta})$.\n\nConsider the covariance between the estimator $\\mathbf{T}$ and the Score $\\mathbf{U}$. By an argument similar to the scalar case (interchanging derivative and integral), we find:\n\n$$\n\\text{Cov}(\\mathbf{T}, \\mathbf{U}) = E[\\mathbf{T} \\mathbf{U}^\\top] = \\mathbf{D}(\\boldsymbol{\\theta})\n$$\n\nNow, define the block vector $\\mathbf{Z} = \\begin{pmatrix} \\mathbf{T} \\\\ \\mathbf{U} \\end{pmatrix}$. The covariance matrix of $\\mathbf{Z}$ is necessarily positive semi-definite:\n\n$$\n\\text{Var}(\\mathbf{Z}) = \\begin{pmatrix} \\text{Var}(\\mathbf{T}) & \\text{Cov}(\\mathbf{T}, \\mathbf{U}) \\\\ \\text{Cov}(\\mathbf{U}, \\mathbf{T}) & \\text{Var}(\\mathbf{U}) \\end{pmatrix} = \\begin{pmatrix} \\Sigma_{\\mathbf{T}} & \\mathbf{D} \\\\ \\mathbf{D}^\\top & \\mathbf{I} \\end{pmatrix} \\succeq 0\n$$\n\nFor this block matrix to be positive semi-definite, the Schur complement of the block $\\mathbf{I}$ must be positive semi-definite (assuming $\\mathbf{I}$ is positive definite/invertible):\n\n$$\n\\Sigma_{\\mathbf{T}} - \\mathbf{D} \\mathbf{I}^{-1} \\mathbf{D}^\\top \\succeq 0\n$$\n\nThus, $\\text{Var}(\\mathbf{T}) \\succeq \\mathbf{D} \\mathbf{I}^{-1} \\mathbf{D}^\\top$.\n\n:::\n\n::: {#cor-scalar-crlb}\n\n### Corollary: Scalar Estimator\nConsider the case where $T(\\mathbf{X})$ is a scalar estimator ($k=1$) for a scalar parameter $\\theta$ ($p=1$).\n\n1.  **Matrices to Scalars:** The covariance matrix $\\text{Var}(\\mathbf{T})$ becomes the scalar variance $\\text{Var}(T)$. The Fisher Information matrix $\\mathbf{I}(\\boldsymbol{\\theta})$ becomes the scalar $I(\\theta)$.\n    \n2.  **Jacobian to Derivative:** The Jacobian matrix $\\mathbf{D}(\\boldsymbol{\\theta})$ reduces to the scalar derivative $m'(\\theta)$.\n\nSubstituting these into the multivariate bound:\n\n$$\n\\text{Var}(T) - m'(\\theta) [I(\\theta)]^{-1} m'(\\theta) \\ge 0\n$$\n\n$$\n\\text{Var}(T) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n\n:::\n\n\n::: {#rem-crlb-generality}\n\n### Generality of the Lower Bound\nThe power of the Cramer-Rao Lower Bound lies in its independence from the specific method of estimation. It relies solely on the properties of the underlying probability model (specifically, the curvature of the log-likelihood function) and the bias of the estimator. Consequently, it provides a universal benchmark for precision:\n\n1.  **Fundamental Limit**\n    It represents the limit of \"extractable information\" about $\\boldsymbol{\\theta}$ contained in the data $\\mathbf{X}$. No matter how clever the estimation algorithm is (e.g., Method of Moments, Bayes estimators, etc.), the variance cannot be reduced beyond this intrinsic bound determined by the Fisher Information.\n\n2.  **Efficiency Standard**\n    It allows us to define the concept of an *efficient estimator*. Any unbiased estimator that attains this lower bound is the Uniformly Minimum Variance Unbiased Estimator (UMVUE).\n\n3.  **Asymptotic Justification**\n    While finite-sample estimators may not always achieve this bound, the Maximum Likelihood Estimator (MLE) is asymptotically efficient. This means that as the sample size $n \\to \\infty$, the variance of the MLE approaches the CRLB, justifying the popularity of likelihood-based inference.\n\n:::\n\n### Example with Exponential Likelihood\n\n::: {#exm-exponential-crlb}\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Exp}(\\theta)$, where the density is $f(x|\\theta) = \\frac{1}{\\theta} e^{-x/\\theta}$. We illustrate the likelihood identities and the efficiency of the sample mean.\n\n1. The Score Function ($U$)\nThe log-likelihood function is:\n$$\n\\ell(\\theta; \\mathbf{x}) = \\sum_{i=1}^n \\left( -\\log \\theta - \\frac{x_i}{\\theta} \\right) = -n \\log \\theta - \\frac{1}{\\theta} \\sum_{i=1}^n x_i\n$$\nThe Score function is the first derivative with respect to $\\theta$:\n$$\nU(\\theta; \\mathbf{x}) = \\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{\\theta} + \\frac{\\sum x_i}{\\theta^2}\n$$\n*Check First Moment:* $E[U] = -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum E[X_i] = -\\frac{n}{\\theta} + \\frac{n\\theta}{\\theta^2} = 0$. (Verified)\n\n2. Fisher Information ($I(\\theta)$)\nWe calculate the information using two different definitions to verify Bartlett's identity.\n\n* **Method A: Negative Expected Hessian**\n    $$\n    U'(\\theta) = \\frac{\\partial U}{\\partial \\theta} = \\frac{n}{\\theta^2} - \\frac{2\\sum x_i}{\\theta^3}\n    $$\n    $$\n    I(\\theta) = -E[U'(\\theta)] = -\\left( \\frac{n}{\\theta^2} - \\frac{2 n \\theta}{\\theta^3} \\right) = -\\left( \\frac{n}{\\theta^2} - \\frac{2n}{\\theta^2} \\right) = \\frac{n}{\\theta^2}\n    $$\n\n* **Method B: Variance of the Score**\n    $$\n    \\text{Var}(U) = \\text{Var}\\left( -\\frac{n}{\\theta} + \\frac{\\sum X_i}{\\theta^2} \\right) = \\frac{1}{\\theta^4} \\text{Var}\\left( \\sum X_i \\right)\n    $$\n    Since $X_i$ are independent with $\\text{Var}(X_i) = \\theta^2$:\n    $$\n    \\text{Var}(U) = \\frac{1}{\\theta^4} (n \\theta^2) = \\frac{n}{\\theta^2}\n    $$\n\n**Result:** $\\text{Var}(U) = -E[U'] = I(\\theta)$. (Identity Verified)\n\n3. Cramer-Rao Lower Bound (CRLB)\nConsider the estimator $T(\\mathbf{X}) = \\bar{X}$.\n\n* **Expectation:** $m(\\theta) = E[\\bar{X}] = \\theta$. Thus, $T$ is unbiased and $m'(\\theta) = 1$.\n* **Actual Variance:**\n    $$\n    \\text{Var}(T(\\mathbf{X})) = \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\theta^2}{n}\n    $$\n\n* **Theoretical Lower Bound:**\n    $$\n    \\text{CRLB} = \\frac{[m'(\\theta)]^2}{I(\\theta)} = \\frac{1^2}{n/\\theta^2} = \\frac{\\theta^2}{n}\n    $$\n\n**Conclusion:**\n$$\n\\text{Var}(T(\\mathbf{X})) = \\frac{\\theta^2}{n} \\ge \\frac{\\theta^2}{n}\n$$\nThe variance of $T(\\mathbf{X})$ achieves the lower bound exactly. Therefore, $\\bar{X}$ is an **efficient estimator** for $\\theta$.\n\n:::\n\n## Exponential Families\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\log f(\\mathbf{x}|\\boldsymbol{\\theta})$, can be expressed as the sum of three distinct terms:\n\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\sum_{i=1}^k \\eta_i(\\boldsymbol{\\theta}) T_i(\\mathbf{x}) - A(\\boldsymbol{\\theta}) + \\log h(\\mathbf{x})\n$$\n\nExponentiating this yields the density form:\n\n$$\nf(\\mathbf{x}|\\boldsymbol{\\theta}) = h(\\mathbf{x}) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i(\\boldsymbol{\\theta}) T_i(\\mathbf{x}) - A(\\boldsymbol{\\theta}) \\right\\}\n$$\n\nwhere:\n\n* $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_d)$ is the vector of model parameters.\n* $\\eta_i(\\boldsymbol{\\theta})$ are the **natural parameter functions**.\n* $\\mathbf{T}(\\mathbf{x}) = (T_1(\\mathbf{x}), \\dots, T_k(\\mathbf{x}))$ constitutes the vector of **sufficient statistics** for $\\boldsymbol{\\theta}$.\n* $A(\\boldsymbol{\\theta})$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.\n* $h(\\mathbf{x})$ is the base measure.\n\n:::\n\n\n### Examples\n\n#### Exponential Distribution {.unnumbered}\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = -\\frac{1}{\\theta} \\sum_{i=1}^n x_i - n \\log \\theta\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\theta) = -\\frac{1}{\\theta}$\n* $T_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$\n* $A(\\theta) = n \\log \\theta$\n* $\\log h(\\mathbf{x}) = 0$\n\n:::\n\n\n#### Gamma Distribution {.unnumbered}\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$. The density is:\n$$\nf(\\mathbf{x}|\\boldsymbol{\\theta}) = [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i - \\left[ n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\boldsymbol{\\theta}) = \\alpha - 1$, $\\quad T_1(\\mathbf{x}) = \\sum \\log x_i$\n* $\\eta_2(\\boldsymbol{\\theta}) = -\\frac{1}{\\beta}$, $\\quad T_2(\\mathbf{x}) = \\sum x_i$\n* $A(\\boldsymbol{\\theta}) = n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta$\n\n:::\n\n\n#### Beta Distribution {.unnumbered}\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\boldsymbol{\\theta} = (a, b)$.\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) - n \\log B(a, b)\n$$\n\nThis is an exponential family with $k=2$.\n\n* $\\eta_1 = a-1$, $T_1 = \\sum \\log x_i$\n* $\\eta_2 = b-1$, $T_2 = \\sum \\log(1-x_i)$\n* $A(\\boldsymbol{\\theta}) = n \\log B(a, b)$\n\n:::\n\n\n#### Normal Distribution {.unnumbered}\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. The log-likelihood is:\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(2\\pi\\sigma^2) \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1 = \\frac{\\mu}{\\sigma^2}$, $T_1 = \\sum x_i$\n* $\\eta_2 = -\\frac{1}{2\\sigma^2}$, $T_2 = \\sum x_i^2$\n* $A(\\boldsymbol{\\theta}) = \\frac{n\\mu^2}{2\\sigma^2} + n \\log \\sigma + \\frac{n}{2} \\log(2\\pi)$\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n\n#### Uniform Distribution {.unnumbered}\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\n\\ell(\\theta; x) = -\\log \\theta + \\log I(0 < x < \\theta)\n$$\n\nThe term $\\log I(0 < x < \\theta)$ couples $x$ and $\\theta$ in a way that cannot be separated into a sum $\\sum \\eta_i(\\theta) T_i(x)$.\n\n:::\n\n\n### Moments of Sufficient Statistics of Exponential Families\n\n#### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n#### Means via the Score Function\nFor a regular exponential family with log-likelihood $\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\sum \\eta_i(\\boldsymbol{\\theta}) T_i(\\mathbf{x}) - A(\\boldsymbol{\\theta}) + \\log h(\\mathbf{x})$, the expectation of the sufficient statistics can be found by setting the expected score to zero:\n\n$$\nE_{\\boldsymbol{\\theta}} \\left[ \\frac{\\partial \\ell(\\boldsymbol{\\theta}; \\mathbf{X})}{\\partial \\theta_j} \\right] = 0\n$$\n\nSubstituting the specific form of $\\ell(\\boldsymbol{\\theta}; \\mathbf{X})$:\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\boldsymbol{\\theta})}{\\partial \\theta_j} E[T_i(\\mathbf{X})] = \\frac{\\partial A(\\boldsymbol{\\theta})}{\\partial \\theta_j} \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nThe log-likelihood is:\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\sum_{i=1}^k \\eta_i(\\boldsymbol{\\theta}) T_i(\\mathbf{x}) - A(\\boldsymbol{\\theta}) + \\log h(\\mathbf{x})\n$$\n\nDifferentiating with respect to $\\theta_j$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\boldsymbol{\\theta})}{\\partial \\theta_j} T_i(\\mathbf{x}) - \\frac{\\partial A(\\boldsymbol{\\theta})}{\\partial \\theta_j}\n$$\n\nTaking the expectation and using the regularity condition $E[\\frac{\\partial \\ell}{\\partial \\theta_j}] = 0$:\n$$\nE\\left[ \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\boldsymbol{\\theta})}{\\partial \\theta_j} T_i(\\mathbf{X}) - \\frac{\\partial A(\\boldsymbol{\\theta})}{\\partial \\theta_j} \\right] = 0\n$$\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\boldsymbol{\\theta})}{\\partial \\theta_j} E[T_i(\\mathbf{X})] = \\frac{\\partial A(\\boldsymbol{\\theta})}{\\partial \\theta_j}\n$$\n\n:::\n\n#### Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n#### Natural Parameterization (Canonical Form)\n\nIf the parameterization is chosen such that the natural parameters are the components of the parameter vector itself (i.e., $\\boldsymbol{\\eta}(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}$), the exponential family is said to be in **Canonical Form** or **Natural Parameterization**.\n\nThe log-likelihood for the natural parameter vector $\\boldsymbol{\\eta} = (\\eta_1, \\dots, \\eta_k)^T$ simplifies to:\n$$\n\\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = \\sum_{i=1}^k \\eta_i T_i(\\mathbf{x}) - A(\\boldsymbol{\\eta}) + \\log h(\\mathbf{x})\n$$\nor in vector notation:\n$$\n\\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = \\boldsymbol{\\eta}^T \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol{\\eta}) + \\log h(\\mathbf{x})\n$$\nwhere $A(\\boldsymbol{\\eta})$ is the log-partition function.\n\n:::\n\n\n:::{#def-curvedexpfam}\n\n#### Full vs. Curved Exponential Families\n\n* **Full Exponential Family:** When the natural parameters $\\boldsymbol{\\eta}$ can vary independently in an open set of $\\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).\n* **Curved Exponential Family:** When the dimension of the parameter vector $\\boldsymbol{\\theta}$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ to lie on a non-linear curve or surface within the natural parameter space.\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n#### Curved Exponential Family Example\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i - n \\log \\theta - \\text{const}\n$$\n\nHere:\n\n* $\\eta_1(\\theta) = -\\frac{1}{2\\theta^2}$, $T_1 = \\sum x_i^2$\n* $\\eta_2(\\theta) = \\frac{1}{\\theta}$, $T_2 = \\sum x_i$\n\nSince $d=1$ but $k=2$, and $\\eta_1 = -\\frac{1}{2}\\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.\n\n:::\n\n#### Mean and Variance of Sufficient Statistics\n\n::: {#thm-cumulant-generating}\n\n#### Mean and Variance of Sufficient Statistics\n\nFor an exponential family in canonical form, the log-partition function $A(\\boldsymbol{\\eta})$ acts as the **Cumulant Generating Function** for the sufficient statistic vector $\\mathbf{T}(\\mathbf{X})$. The derivatives of $A(\\boldsymbol{\\eta})$ yield the moments of $\\mathbf{T}(\\mathbf{X})$ as follows:\n\n1.  **Mean (First Derivative):**\n    $$\n    E[\\mathbf{T}(\\mathbf{X})] = \\nabla A(\\boldsymbol{\\eta})\n    $$\n\n2.  **Covariance (Second Derivative):**\n    $$\n    \\text{Var}(\\mathbf{T}(\\mathbf{X})) = \\nabla^2 A(\\boldsymbol{\\eta})\n    $$\n\n**Link to Fisher Information:**\nIn the canonical parameterization, the observed information matrix is constant (non-stochastic) and equals the Hessian of $A(\\boldsymbol{\\eta})$. Therefore, the covariance of the sufficient statistics is exactly the Fisher Information Matrix:\n\n$$\n\\text{Var}(\\mathbf{T}(\\mathbf{X})) = \\mathbf{I}(\\boldsymbol{\\eta})\n$$\n\nThis implies that $\\mathbf{T}(\\mathbf{X})$ is an efficient estimator for the mean parameter $\\mathbf{m}(\\boldsymbol{\\eta}) = E[\\mathbf{T}(\\mathbf{X})]$, as it achieves the Cramer-Rao Lower Bound with equality (identity link).\n\n:::\n\n::: {.proof}\n**Derivation**\n\nThese results follow directly from Bartlett's Identities (Theorem @thm-score-identities) applied to the canonical log-likelihood:\n$$\n\\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = \\boldsymbol{\\eta}^T \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol{\\eta}) + \\log h(\\mathbf{x})\n$$\n\n**For the Mean:**\nThe score function (gradient of $\\ell$) is:\n$$\n\\mathbf{U}(\\boldsymbol{\\eta}) = \\nabla_{\\boldsymbol{\\eta}} \\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = \\mathbf{T}(\\mathbf{x}) - \\nabla A(\\boldsymbol{\\eta})\n$$\nBy the First Moment Identity, $E[\\mathbf{U}(\\boldsymbol{\\eta})] = \\mathbf{0}$:\n$$\nE[\\mathbf{T}(\\mathbf{X}) - \\nabla A(\\boldsymbol{\\eta})] = \\mathbf{0} \\implies E[\\mathbf{T}(\\mathbf{X})] = \\nabla A(\\boldsymbol{\\eta})\n$$\n\n**For the Covariance:**\nThe observed information (negative Hessian of $\\ell$) is:\n$$\n\\mathbf{J}(\\boldsymbol{\\eta}) = -\\nabla^2_{\\boldsymbol{\\eta}} \\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = -\\nabla_{\\boldsymbol{\\eta}} (\\mathbf{T}(\\mathbf{x}) - \\nabla A(\\boldsymbol{\\eta})) = \\nabla^2 A(\\boldsymbol{\\eta})\n$$\nNote that $\\mathbf{T}(\\mathbf{x})$ is constant with respect to $\\boldsymbol{\\eta}$, so its derivative vanishes.\nBy the Second Moment Identity, $\\mathbf{I}(\\boldsymbol{\\eta}) = E[\\mathbf{J}(\\boldsymbol{\\eta})] = \\text{Cov}(\\mathbf{U}(\\boldsymbol{\\eta}))$.\nSince $\\mathbf{U}(\\boldsymbol{\\eta}) = \\mathbf{T}(\\mathbf{X}) - \\text{constant}$, $\\text{Cov}(\\mathbf{U}(\\boldsymbol{\\eta})) = \\text{Cov}(\\mathbf{T}(\\mathbf{X}))$.\nTherefore:\n$$\n\\text{Cov}(\\mathbf{T}(\\mathbf{X})) = E[\\nabla^2 A(\\boldsymbol{\\eta})] = \\nabla^2 A(\\boldsymbol{\\eta})\n$$\n\n:::\n\n#### Examples\n\n##### Moments of the Binomial Distribution {.unnumbered}\n\n::: {#exm-bernoulli-moments}\n\n#### Moments of the Binomial Distribution\n\nConsider $n$ independent coin flips $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$. We find the mean and variance of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   \n   The standard log-likelihood is:\n   $$\n   \\ell(p; \\mathbf{x}) = \\log\\left(\\frac{p}{1-p}\\right) \\sum x_i + n \\log(1-p)\n   $$\n\n   * Natural Parameter: $\\eta = \\log\\left(\\frac{p}{1-p}\\right) \\implies p = \\frac{e^\\eta}{1+e^\\eta}$.\n   * Log-Partition Function: $A(\\eta) = -n \\log(1-p) = n \\log(1+e^\\eta)$.\n\n   **Canonical Log-Likelihood $\\ell(\\eta)$:**\n   $$\n   \\ell(\\eta; \\mathbf{x}) = \\eta \\left(\\sum x_i\\right) - n \\log(1+e^\\eta)\n   $$\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = n \\frac{e^\\eta}{1+e^\\eta} = np\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = n \\frac{e^\\eta (1+e^\\eta) - e^\\eta(e^\\eta)}{(1+e^\\eta)^2} = n \\frac{e^\\eta}{(1+e^\\eta)^2} = np(1-p)\n   $$\n\n:::\n\n\n#####  Moments of the Gamma Sufficient Statistic {.unnumbered}\n::: {#exm-exponential-moments}\n\n### Moments of the Gamma Sufficient Statistic\n\nConsider $X_i \\sim \\text{Exp}(\\lambda)$. We find the moments of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   \n   The standard log-likelihood is:\n   $$\n   \\ell(\\lambda; \\mathbf{x}) = -\\lambda \\sum x_i + n \\log \\lambda\n   $$\n\n   * Natural Parameter: $\\eta = -\\lambda$.\n   * Log-Partition Function: $A(\\eta) = -n \\log \\lambda = -n \\log(-\\eta)$.\n\n   **Canonical Log-Likelihood $\\ell(\\eta)$:**\n   $$\n   \\ell(\\eta; \\mathbf{x}) = \\eta \\left(\\sum x_i\\right) - \\left[ -n \\log(-\\eta) \\right] = \\eta \\sum x_i + n \\log(-\\eta)\n   $$\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = -n \\frac{1}{-\\eta}(-1) = -\\frac{n}{\\eta} = \\frac{n}{\\lambda}\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = \\frac{\\partial}{\\partial \\eta} \\left(-\\frac{n}{\\eta}\\right) = \\frac{n}{\\eta^2} = \\frac{n}{\\lambda^2}\n   $$\n\n:::\n\n\n#####  Moments of Normal Sufficient Statistics {.unnumbered}\n::: {#exm-normal-sufficient-moments}\n\n### Moments of Normal Sufficient Statistics\n\nConsider $X_i \\sim N(\\mu, \\sigma^2)$.\n\n1. **Log-Likelihood Form**\n   \n   The standard log-likelihood is:\n   $$\n   \\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\frac{\\mu}{\\sigma^2} \\sum x_i - \\frac{1}{2\\sigma^2} \\sum x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(2\\pi\\sigma^2) \\right]\n   $$\n   \n   * Natural Parameters: $\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\quad \\eta_2 = -\\frac{1}{2\\sigma^2}$.\n   * Log-Partition Function (in terms of $\\boldsymbol{\\eta}$):\n     Using $\\sigma^2 = -\\frac{1}{2\\eta_2}$ and $\\mu = -\\frac{\\eta_1}{2\\eta_2}$:\n     $$\n     A(\\boldsymbol{\\eta}) = -\\frac{n \\eta_1^2}{4 \\eta_2} - \\frac{n}{2} \\log(-2\\eta_2) + \\frac{n}{2}\\log(2\\pi)\n     $$\n\n   **Canonical Log-Likelihood $\\ell(\\boldsymbol{\\eta})$:**\n   $$\n   \\ell(\\boldsymbol{\\eta}; \\mathbf{x}) = \\eta_1 \\left(\\sum x_i\\right) + \\eta_2 \\left(\\sum x_i^2\\right) - \\left[ -\\frac{n \\eta_1^2}{4 \\eta_2} - \\frac{n}{2} \\log(-2\\eta_2) \\right]\n   $$\n\n2. **First Moments (Means)**\n   $$\n   E[T_1] = E\\left[\\sum X_i\\right] = \\frac{\\partial A}{\\partial \\eta_1} = -\\frac{2n\\eta_1}{4\\eta_2} = -\\frac{n\\eta_1}{2\\eta_2} = n\\mu\n   $$\n   $$\n   E[T_2] = E\\left[\\sum X_i^2\\right] = \\frac{\\partial A}{\\partial \\eta_2} = \\frac{n\\eta_1^2}{4\\eta_2^2} - \\frac{n}{2(-2\\eta_2)}(-2) = \\frac{n\\eta_1^2}{4\\eta_2^2} - \\frac{n}{2\\eta_2}\n   $$\n   Subbing back $\\mu, \\sigma$:\n   $$\n   = n\\mu^2 + n\\sigma^2 = n(\\mu^2 + \\sigma^2)\n   $$\n\n3. **Second Moment (Covariance)**\n   $$\n   \\text{Cov}(T_1, T_2) = \\frac{\\partial^2 A}{\\partial \\eta_1 \\partial \\eta_2} = \\frac{\\partial}{\\partial \\eta_2} \\left( -\\frac{n\\eta_1}{2\\eta_2} \\right) = \\frac{n\\eta_1}{2\\eta_2^2} = 2n\\mu\\sigma^2\n   $$\n\n4. **Independence of $\\bar{X}$ and $S^2$**\n   We verify that $\\text{Cov}(\\bar{X}, S^2) = 0$.\n   \n   Express $\\bar{X}$ and $S^2$ in terms of $T_1$ and $T_2$:\n   $$\n   \\bar{X} = \\frac{1}{n} T_1\n   $$\n   $$\n   S^2 = \\frac{1}{n-1} \\left( \\sum X_i^2 - n\\bar{X}^2 \\right) = \\frac{1}{n-1} \\left( T_2 - \\frac{1}{n} T_1^2 \\right)\n   $$\n   \n   Now compute the covariance (ignoring constants $\\frac{1}{n(n-1)}$ for now):\n   $$\n   \\text{Cov}\\left(T_1, T_2 - \\frac{1}{n}T_1^2\\right) = \\text{Cov}(T_1, T_2) - \\frac{1}{n} \\text{Cov}(T_1, T_1^2)\n   $$\n   \n   We need $\\text{Cov}(T_1, T_1^2)$. Since $T_1 = \\sum X_i \\sim N(n\\mu, n\\sigma^2)$, we use the property of the normal distribution that for $Y \\sim N(\\theta, \\tau^2)$, $\\text{Cov}(Y, Y^2) = 2\\theta \\tau^2$.\n   Here $\\theta = n\\mu$ and $\\tau^2 = n\\sigma^2$:\n   $$\n   \\text{Cov}(T_1, T_1^2) = 2(n\\mu)(n\\sigma^2) = 2n^2\\mu\\sigma^2\n   $$\n   \n   Substituting this back into the expression:\n   $$\n   \\text{Cov}\\left(T_1, T_2 - \\frac{1}{n}T_1^2\\right) = \\underbrace{2n\\mu\\sigma^2}_{\\text{From Part 3}} - \\frac{1}{n} (2n^2\\mu\\sigma^2) = 2n\\mu\\sigma^2 - 2n\\mu\\sigma^2 = 0\n   $$\n   \n   Since $\\bar{X}$ and $S^2$ are uncorrelated and derived from normally distributed data, they are **independent**.\n\n:::\n",
    "supporting": [
      "loglike_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}