{
  "hash": "6569f363a8349e199e61be46c3fbe024",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Methods\"\nformat: html\n---\n\n\n## Bayes Theorem\nSuppose $\\theta \\sim \\pi(\\theta)$ and $X \\sim f(x; \\theta)$. The posterior density of $\\theta$ given $X$ is:\n\n$$\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{\\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta} \\propto \\pi(\\theta) \\cdot L(\\theta; x)\n$$\n\nwhere $L(\\theta; x)$ is the likelihood.\n\n## Examples\n\n### 1. Binomial-Beta\n\n* $X|\\theta \\sim \\text{Bin}(n, \\theta) \\Rightarrow f(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}$\n* Prior $\\theta \\sim \\text{Beta}(a, b) \\Rightarrow \\pi(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}$\n\n**Posterior:**\n$$\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x} = \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n$$\n\nSo, $\\theta|x \\sim \\text{Beta}(a+x, b+n-x)$.\n\n**Moments:**\n\n* Mean: $E(\\theta|x) = \\frac{a+x}{a+b+n} \\approx \\frac{x}{n}$ (for small $n$)\n* Variance: $\\text{Var}(\\theta|x) = \\frac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}$\n\n### 2. Normal-Normal (Known Variance)\n\n* $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$ where $\\sigma^2$ is known.\n* Prior $\\mu \\sim N(\\mu_0, \\sigma_0^2)$.\n\nLet $\\tau_0 = 1/\\sigma_0^2$ (prior precision), $\\tau = 1/\\sigma^2$ (data precision).\nThe posterior precision is $\\tau_1 = \\tau_0 + n\\tau$.\n\n**Posterior:**\n$$\n\\mu|x \\sim N\\left( \\frac{\\tau_0 \\mu_0 + n\\tau \\bar{x}}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n$$\n\nThis shows the posterior mean is a weighted average of the prior mean and the sample mean.\n\n---\n\n# Part 3: Bayes Estimators and Loss Functions\n\nTo find a Bayes rule $d(x)$, we minimize the posterior expected loss:\n$$\n\\min_d \\int_{\\Theta} L(\\theta, d) \\pi(\\theta|x) d\\theta\n$$\n\n## 1. Squared Error Loss: $L(\\theta, a) = (\\theta - a)^2$\nMinimizing $E_{\\theta|x}[(\\theta - d)^2]$ leads to:\n$$\nd(x) = E(\\theta|x) \\quad \\text{(Posterior Mean)}\n$$\n\n## 2. Absolute Error Loss: $L(\\theta, a) = |\\theta - a|$\nMinimizing $E_{\\theta|x}[|\\theta - d|]$ leads to:\n$$\n\\int_{-\\infty}^d \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = 0.5\n$$\nSo, $d(x) = \\text{Median of } \\pi(\\theta|x)$.\n\n## 3. 0-1 Loss (Hypothesis Testing)\n\n* Loss is 1 if error, 0 if correct.\n* Testing $\\Theta_0$ vs $\\Theta_1$.\n* Bayes Rule: Choose class with highest posterior probability.\n    * Reject $H_0$ if $P(\\theta \\in \\Theta_1 | x) > P(\\theta \\in \\Theta_0 | x)$.\n\n## 4. Interval Estimation\nWe want an interval $A = (d-\\delta, d+\\delta)$ minimizing risk (maximizing coverage probability $1-\\alpha$).\n\n**Highest Posterior Density (HPD) Interval:**\nThe set $C = \\{ \\theta : \\pi(\\theta|x) \\ge k \\}$ where $P(\\theta \\in C|x) = 1-\\alpha$.\nThis is the shortest interval for a given confidence level if the posterior is unimodal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nx <- seq(0, 15, length.out = 1000)\ny <- dgamma(x, shape = 3, rate = 0.5)\ndf <- data.frame(x = x, y = y)\n\n# Approximate HPD cutoff (visual)\nhpd_level <- 0.05\ncutoff <- 0.08 # Chosen for visual representation of the cut\n\nggplot(df, aes(x, y)) +\n  geom_line(size = 1) +\n  geom_area(data = subset(df, y > cutoff), fill = \"skyblue\", alpha = 0.5) +\n  geom_hline(yintercept = cutoff, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = 10, y = cutoff + 0.02, label = \"HPD Cutoff line\", color = \"red\") +\n  labs(title = \"Highest Posterior Density (HPD) Interval\", \n       subtitle = \"Points with density higher than the red line form the HPD set\",\n       x = \"Theta\", y = \"Posterior Density\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Illustration of Highest Posterior Density (HPD) Interval vs Equi-tailed Interval on a skewed posterior.](bayesian_files/figure-html/fig-hpd-1.png){#fig-hpd width=576}\n:::\n:::\n\n\n---\n\n# Part 4: Minimax Rules via Bayes\n\n**Goal:** Find a minimax estimator for $\\theta$ where $X \\sim \\text{Bin}(n, \\theta)$.\n**Loss:** Squared Error $L(\\theta, d) = (\\theta - d)^2$.\n\nStrategy: Find a prior $\\text{Beta}(a, b)$ such that the Bayes risk $R(\\theta, d_{\\text{Bayes}})$ is constant for all $\\theta$. By Theorem 2.2 (Equalizer Rule), if an extended Bayes rule has constant risk, it is Minimax.\n\nThe Bayes estimator is $d(x) = \\frac{a+x}{a+b+n}$.\nThe risk is:\n$$\nR(\\theta, d) = E\\left[ \\left( \\theta - \\frac{a+X}{a+b+n} \\right)^2 \\right]\n$$\nLet $c = a+b+n$.\n$$\nR(\\theta, d) = \\frac{1}{c^2} \\left[ (c\\theta - a)^2 - 2(c\\theta - a)n\\theta + n\\theta(1-\\theta) + n^2\\theta^2 \\right]\n$$\n\nTo make this constant (independent of $\\theta$), the coefficients of $\\theta$ and $\\theta^2$ must vanish or balance out.\nSolving the resulting system yields:\n$$\na = b = \\frac{\\sqrt{n}}{2}\n$$\n\n**Minimax Estimator:**\n$$\nd_{\\text{minimax}}(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\ntheta <- seq(0, 1, length.out = 100)\n\n# MLE Risk: theta(1-theta)/n\nrisk_mle <- theta * (1 - theta) / n\n\n# Minimax Risk: Constant 1 / (4 * (sqrt(n) + 1)^2) ? No, calculation is slightly different\n# Actually, Risk is constant = 1 / (4 * (sqrt(n) + n)^2) * n ??? \n# Let's just use the formula derived: 1 / (2*sqrt(n) + 2)^2 approx. \n# For standard minimax: Risk = 1 / (4 * (sqrt(n) + 1)^2) if we view it as sample size equivalent.\n# Correct constant risk calculation:\nrisk_minimax_val <- 1 / (4 * (sqrt(n) + 1)^2) # Standard result check\n# Actually let's compute it numerically to be safe\na <- sqrt(n)/2\nb <- sqrt(n)/2\ndenom <- n + sqrt(n)\nrisk_minimax <- numeric(length(theta))\nfor(i in 1:length(theta)) {\n  risk_minimax[i] <- sum(dbinom(0:n, n, theta[i]) * ((0:n + a)/denom - theta[i])^2)\n}\n\ndf_risk <- data.frame(\n  theta = rep(theta, 2),\n  Risk = c(risk_mle, risk_minimax),\n  Estimator = rep(c(\"MLE (x/n)\", \"Minimax\"), each = 100)\n)\n\nggplot(df_risk, aes(x = theta, y = Risk, color = Estimator)) +\n  geom_line(size = 1.2) +\n  theme_minimal() +\n  labs(title = \"Comparison of Risk Functions (n=10)\", \n       y = \"Risk (MSE)\", x = \"Theta\")\n```\n\n::: {.cell-output-display}\n![Risk Functions: MLE vs Minimax for Binomial(n=10).](bayesian_files/figure-html/fig-minimax-risk-1.png){#fig-minimax-risk width=576}\n:::\n:::\n\n\n---\n\n# Part 5: Stein Estimation\n\n**Context:** Estimating a multivariate normal mean $\\mu = (\\mu_1, \\dots, \\mu_p)^T$ where $X \\sim N_p(\\mu, I)$.\n**Loss:** Sum of squared errors $L(\\mu, d) = ||\\mu - d||^2$.\n\n**Stein's Lemma:**\nIf $Y \\sim N(\\mu, 1)$ and $h(y)$ is differentiable:\n$$\nE[(Y-\\mu)h(Y)] = E[h'(Y)]\n$$\n\n**James-Stein Estimator:**\n$$\nd^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\n$$\nThis estimator shrinks the observation vector $X$ towards the origin (or a grand mean).\n\n**Result:**\nIf $p \\ge 3$, the James-Stein estimator dominates the MLE ($d^0(X) = X$).\n$$\nR(\\mu, d^{JS}) < R(\\mu, d^0) = p \\quad \\text{for all } \\mu\n$$\n\n## Baseball Example (Efron & Morris)\nWe observe batting averages for $p=18$ players.\n\n* MLE: Individual batting averages.\n* JS: Shrinks individual averages toward the global average.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating mock data similar to the baseball example\nplayer <- 1:10\nMLE <- c(0.400, 0.370, 0.350, 0.300, 0.280, 0.250, 0.220, 0.200, 0.150, 0.100)\nGrandMean <- mean(MLE)\nshrinkage_factor <- 0.6 # c = 1 - (p-2)/S\nJS <- GrandMean + shrinkage_factor * (MLE - GrandMean)\n\ndf_base <- data.frame(player, MLE, JS)\n\nggplot(df_base) +\n  geom_point(aes(x = MLE, y = 1), color = \"red\", size = 3) +\n  geom_point(aes(x = JS, y = 1), color = \"blue\", size = 3) +\n  geom_segment(aes(x = MLE, y = 1, xend = JS, yend = 1), arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_vline(xintercept = GrandMean, linetype = \"dashed\") +\n  annotate(\"text\", x = GrandMean, y = 1.1, label = \"Grand Mean\") +\n  ylim(0.9, 1.2) +\n  labs(title = \"James-Stein Shrinkage Effect\", \n       subtitle = \"Red: MLE, Blue: JS Estimator\",\n       x = \"Batting Average\") +\n  theme_void() +\n  theme(axis.title.x = element_text(), axis.text.x = element_text())\n```\n\n::: {.cell-output-display}\n![Visualizing James-Stein Shrinkage (Mock Data based on Baseball Example). The arrows show MLEs being pulled toward the Grand Mean.](bayesian_files/figure-html/fig-shrinkage-1.png){#fig-shrinkage width=576}\n:::\n:::\n\n\n---\n\n# Part 6: Empirical Bayes & Hierarchical Models\n\n## Empirical Bayes\nInstead of fixing hyperparameters $(\\mu_0, \\sigma_0^2)$, we estimate them from the marginal distribution of the data.\n$$\nm(x) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\n$$\nWe estimate $\\eta$ by maximizing $m(x)$ (Type-II MLE) or method of moments.\n\n**Example:**\nIf $X_i \\sim N(\\mu_i, 1)$ and $\\mu_i \\sim N(0, \\tau^2)$, then marginally $X_i \\sim N(0, 1+\\tau^2)$. We can use $S = \\sum X_i^2$ to estimate $\\tau^2$.\n\n## Hierarchical Models\nWe assume a multistage structure:\n\n1.  Data model: $X|\\theta \\sim f(x|\\theta)$\n2.  Parameter model: $\\theta|\\lambda \\sim \\pi(\\theta|\\lambda)$\n3.  Hyperparameter model: $\\lambda \\sim h(\\lambda)$\n\n**Computation:**\nSince analytical solutions are often impossible, we use **Markov Chain Monte Carlo (MCMC)**.\n\n### Gibbs Sampling\nTo sample from the joint posterior $f(\\theta, \\lambda | x)$, we sample iteratively from the full conditional distributions:\n\n1.  Draw $\\theta^{(k+1)} \\sim f(\\theta | \\lambda^{(k)}, x)$\n2.  Draw $\\lambda^{(k+1)} \\sim f(\\lambda | \\theta^{(k+1)}, x)$\n\n### Metropolis-Hastings\nIf a conditional distribution is hard to sample from directly:\n\n1.  Propose $\\theta^*$ from a proposal density $q(\\theta^* | \\theta^{(t)})$.\n2.  Calculate acceptance ratio $\\alpha = \\min \\left( 1, \\frac{f(\\theta^*|x)q(\\theta^{(t)}|\\theta^*)}{f(\\theta^{(t)}|x)q(\\theta^*|\\theta^{(t)})} \\right)$.\n3.  Accept $\\theta^*$ with probability $\\alpha$.\n\n---\n\n# Part 7: Predictive Distributions\n\nWe want to predict a new observation $Y^*$.\n$$\nf(y^* | y) = \\int f(y^* | \\theta) \\pi(\\theta | y) d\\theta\n$$\n\n**Numerical Methods:**\nIf we have posterior samples $\\theta^{(1)}, \\dots, \\theta^{(N)}$ from MCMC:\n\n**Method 1: Density Averaging**\n$$\n\\hat{f}(y^* | y) \\approx \\frac{1}{N} \\sum_{i=1}^N f(y^* | \\theta^{(i)})\n$$\nThis is a Rao-Blackwellized estimator and usually has lower variance.\n\n**Method 2: Direct Sampling**\nFor each $\\theta^{(i)}$, draw $Y^{*(i)} \\sim f(y^* | \\theta^{(i)})$.\nThe histogram of $Y^{*(i)}$ approximates the predictive density.",
    "supporting": [
      "bayesian_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}