{
  "hash": "10e3d27b1184152f7a1d1c70b523da60",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Methods\n\n## Fundamental Elements of Bayesian Inference\n\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes' Theorem (or Law).\n\n::: {#thm-bayes}\n### Posterior Distribution\n\nSuppose we have a parameter $\\theta$ with a prior distribution denoted by $\\pi(\\theta)$. If we observe data $x$ drawn from a distribution with probability density function (pdf) $f(x; \\theta)$, then the **posterior density** of $\\theta$ given the data $x$ is defined as:\n\n$$\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{\\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta}\n$$\n\nIn this equation:\n\n* $\\pi(\\theta)$ is the **prior**.\n* $f(x;\\theta)$ is the **likelihood**.\n* The denominator is the marginal distribution of $x$, often represented as a normalizing constant $c(x)$ which is free of $\\theta$.\n\nThus, we can state the proportional relationship:\n\n$$\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n$$\n:::\n\n\n\n::: {#exm-binomial-beta}\n### Binomial-beta Conjugacy\n\nConsider an experiment where $x|\\theta \\sim \\text{Bin}(n, \\theta)$. The likelihood function is:\n\n$$\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n$$\n\nSuppose we choose a Beta distribution as the prior for $\\theta$, such that $\\theta \\sim \\text{Beta}(a, b)$. The prior density is:\n\n$$\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n$$\n\nwhere $B(a,b)$ is the Beta function defined as $\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta$.\n\nTo find the posterior, we multiply the prior and the likelihood:\n\n$$\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n$$\n\nCombining terms with the same base:\n\n$$\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n$$\n\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\n$$\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n$$\n\n**Properties of the Posterior:**\n\n* The posterior mean is:\n    $$E(\\theta|x) = \\frac{a+x}{a+b+n}$$\n    As $n \\to \\infty$, this approximates the maximum likelihood estimate $\\frac{x}{n}$.\n\n* The posterior variance is:\n    $$\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}$$\n    For large $n$, this approximates $\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}$.\n\n**Numerical Illustration:**\n\nSuppose we are estimating a probability $\\theta$.\n\n* **Prior:** $\\theta \\sim \\text{Beta}(2, 2)$ (Mean = 0.5).\n* **Data:** 10 trials, 8 successes ($n=10, x=8$).\n* **Posterior:** $\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)$ (Mean $\\approx$ 0.71).\n\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- seq(0, 1, length.out = 200)\n\n# Prior: Beta(2, 2)\nprior <- dbeta(theta, shape1 = 2, shape2 = 2)\n\n# Posterior: Beta(10, 4)\nposterior <- dbeta(theta, shape1 = 10, shape2 = 4)\n\nplot(theta, posterior, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = \"Beta Prior vs Posterior\", ylim = c(0, max(c(prior, posterior))))\nlines(theta, prior, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,4)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n```\n\n::: {.cell-output-display}\n![Prior vs Posterior for Beta-Binomial Example](bayesian_files/figure-html/fig-beta-conjugacy-1.png){#fig-beta-conjugacy width=576}\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {#exm-normal-normal}\n### Normal-normal Conjugacy (known Variance)\n\nLet $X_1, X_2, \\dots, X_n$ be independent and identically distributed (i.i.d.) variables such that $X_i \\sim N(\\mu, \\sigma^2)$, where $\\sigma^2$ is known.\n\nWe assign a Normal prior to the mean $\\mu$: $\\mu \\sim N(\\mu_0, \\sigma_0^2)$.\n\nTo find the posterior $\\pi(\\mu|x_1, \\dots, x_n)$, let $x = (x_1, \\dots, x_n)$. The posterior is proportional to:\n\n$$\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n$$\n\n$$\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n$$\n\n\n\n**Posterior Precision:**\n\nIt is often more convenient to work with **precision** (the inverse of variance). Let:\n\n* $\\tau_0 = 1/\\sigma_0^2$ (Prior precision)\n* $\\tau = 1/\\sigma^2$ (Data precision)\n* $\\tau_1 = 1/\\sigma_1^2$ (Posterior precision)\n\nThe relationship is additive:\n\n$$\n\\tau_1 = \\tau_0 + n\\tau\n$$\n\n$$\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n$$\n\nThe posterior mean $\\mu_1$ is a weighted average of the prior mean and the sample mean:\n\n$$\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n$$\n\nSo, the posterior distribution is:\n\n$$\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n$$\n\n**Numerical Illustration:**\n\nSuppose we estimate a mean height $\\mu$.\n\n* **Known Variance:** $\\sigma^2 = 100$ ($\\tau = 0.01$).\n* **Prior:** $\\mu \\sim N(175, 25)$ (Precision $\\tau_0 = 0.04$).\n* **Data:** $n=10, \\bar{x}=180$. (Total data precision $n\\tau = 0.1$).\n* **Posterior:**\n  * Precision $\\tau_1 = 0.04 + 0.1 = 0.14$.\n  * Variance $\\sigma_1^2 \\approx 7.14$.\n  * Mean $\\mu_1 = \\frac{175(0.04) + 180(0.1)}{0.14} \\approx 178.6$.\n\nThe plot below illustrates the prior (dashed) and posterior (solid) normal densities.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_vals <- seq(150, 200, length.out = 200)\n\n# Prior: N(175, 25) -> SD = 5\nprior_norm <- dnorm(mu_vals, mean = 175, sd = 5)\n\n# Posterior: N(178.6, 7.14) -> SD = Sqrt(7.14) Approx 2.67\nposterior_norm <- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))\n\nplot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Density\",\n     main = \"Normal Prior vs Posterior\",\n     ylim = c(0, max(c(prior_norm, posterior_norm))))\nlines(mu_vals, prior_norm, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior N(175, 25)\", \"Posterior N(178.6, 7.14)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n```\n\n::: {.cell-output-display}\n![Prior vs Posterior for Normal-Normal Example](bayesian_files/figure-html/fig-normal-conjugacy-1.png){#fig-normal-conjugacy width=576}\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n::: {#exm-discrete-posterior}\n### Discrete Posterior Calculation\n\nConsider the following table where we calculate the posterior probabilities for a discrete parameter space.\n\nLet the parameter $\\theta$ take values $\\{1, 2, 3\\}$ with prior probabilities $\\pi(\\theta)$. Let the data $x$ take values $\\{0, 1, 2, \\dots\\}$.\n\nGiven:\n\n* Prior $\\pi(\\theta)$: $\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3$.\n* Likelihood $\\pi(x|\\theta)$:\n    * If $\\theta=1$, $x \\sim \\text{Uniform on } \\{0, 1\\}$ (Prob = 1/2).\n    * If $\\theta=2$, $x \\sim \\text{Uniform on } \\{0, 1, 2\\}$ (Prob = 1/3).\n    * If $\\theta=3$, $x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}$ (Prob = 1/4).\n\nSuppose we observe $x=2$. The calculation of the posterior probabilities is summarized in the table below:\n\n| | $\\theta=1$ | $\\theta=2$ | $\\theta=3$ | Sum |\n|:---|:---:|:---:|:---:|:---:|\n| **Prior** $\\pi(\\theta)$ | $1/3$ | $1/3$ | $1/3$ | $1$ |\n| **Likelihood** $\\pi(x=2|\\theta)$ | $0$ | $1/3$ | $1/4$ | - |\n| **Product** $\\pi(\\theta)\\pi(x|\\theta)$ | $0$ | $1/9$ | $1/12$ | $7/36$ |\n| **Posterior** $\\pi(\\theta|x)$ | $0$ | $4/7$ | $3/7$ | $1$ |\n\nThe marginal sum (evidence) is calculated as $0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36$. The posterior values are obtained by dividing the product row by this sum.\n:::\n\n::: {#exm-Normal-with-Unknown-Mean-and-Variance}\n### Normal with Unknown Mean and Variance\n\nConsider $X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)$, where both $\\mu$ and the precision $\\tau$ are unknown.\n\nWe use a **Normal-Gamma** conjugate prior:\n\n1.  $\\tau \\sim \\text{Gamma}(\\alpha, \\beta)$\n    $$\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}$$\n\n2.  $\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))$\n    $$\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}$$\n\nThe joint prior is the product of the conditional and the marginal:\n$$\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n$$\n\n**Derivation of the Posterior:**\n\nFirst, we write the likelihood in terms of the sufficient statistics $\\bar{x}$ and $S_{xx} = \\sum (x_i - \\bar{x})^2$:\n$$\nL(\\mu, \\tau|x) \\propto \\tau^{n/2} \\exp\\left\\{ -\\frac{\\tau}{2} \\left[ S_{xx} + n(\\bar{x}-\\mu)^2 \\right] \\right\\}\n$$\n\nMultiplying the prior by the likelihood gives the joint posterior:\n$$\n\\begin{aligned}\n\\pi(\\mu, \\tau | x) &\\propto \\tau^{\\alpha - 1/2} e^{-\\beta\\tau} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2} \\cdot \\tau^{n/2} e^{-\\frac{\\tau}{2}S_{xx}} e^{-\\frac{n\\tau}{2}(\\mu-\\bar{x})^2} \\\\\n&\\propto \\tau^{\\alpha + n/2 - 1/2} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{1}{2}\\left( k(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 \\right) \\right] \\right\\}\n\\end{aligned}\n$$\n\nNext, we complete the square for the terms involving $\\mu$ inside the brackets. It can be shown that:\n$$\nk(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 = (k+n)\\left(\\mu - \\frac{k\\nu+n\\bar{x}}{k+n}\\right)^2 + \\frac{nk}{n+k}(\\bar{x}-\\nu)^2\n$$\n\nSubstituting this back into the joint density and grouping terms that do not depend on $\\mu$:\n$$\n\\pi(\\mu, \\tau | x) \\propto \\underbrace{\\tau^{\\alpha + n/2 - 1} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2 \\right] \\right\\}}_{\\text{Marginal of } \\tau} \\cdot \\underbrace{\\tau^{1/2} \\exp\\left\\{ -\\frac{(k+n)\\tau}{2} \\left( \\mu - \\frac{k\\nu+n\\bar{x}}{k+n} \\right)^2 \\right\\}}_{\\text{Conditional of } \\mu|\\tau}\n$$\n\n**Results:**\n\nBy inspecting the factored equation above, we identify the updated parameters:\n\n* **Marginal Posterior of $\\tau$:**\n    The first part corresponds to a Gamma kernel $\\tau^{\\alpha' - 1} e^{-\\beta'\\tau}$.\n    $$\\tau|x \\sim \\text{Gamma}(\\alpha', \\beta')$$\n    where $\\alpha' = \\alpha + n/2$ and $\\beta' = \\beta + \\frac{1}{2}\\sum(x_i-\\bar{x})^2 + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2$.\n\n* **Conditional Posterior of $\\mu$:**\n    The second part corresponds to a Normal kernel with precision $k'\\tau$.\n    $$\\mu|\\tau, x \\sim N(\\nu', 1/(k'\\tau))$$\n    where $k' = k + n$ and $\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}$.\n:::\n\n## Decision Theory and Bayes Rules\n\nThe general form of Bayes rule is derived by minimizing risk.\n\n::: {#def-risk}\n### Risk Function and Bayes Risk\n\n* **Risk Function:** $R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx$\n* **Bayes Risk:** The expected risk with respect to the prior.\n    $$r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta$$\n:::\n\n::: {#thm-bayes-rule-minimization}\n### Minimization of Bayes Risk\n\nMinimizing the Bayes risk $r(\\pi, d)$ is equivalent to minimizing the posterior expected loss for each observed $x$. That is, the Bayes rule $d(x)$ satisfies:\n$$\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n$$\n:::\n\n::: {.proof}\nWe start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function $R(\\theta, d)$:\n\n$$\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n$$\n\nAssuming the conditions for Fubini's Theorem are met, we switch the order of integration:\n\n$$\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n$$\n\nRecall that the joint density can be factored as $f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)$, where $m(x)$ is the marginal density of the data. Substituting this into the inner integral:\n\n$$\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n$$\n\nSince the marginal density $m(x)$ is non-negative, minimizing the total integral $r(\\pi, d)$ with respect to the decision rule $d(\\cdot)$ is equivalent to minimizing the term inside the brackets for every $x$ (specifically where $m(x) > 0$).\n\nThe term inside the brackets is the **Posterior Expected Loss**:\n\n$$\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n$$\n\nTherefore, to minimize the Bayes risk, one must choose $d(x)$ to minimize the posterior expected loss for each $x$.\n:::\n\n### Common Loss Functions and Estimators\n\n::: {#ex-squared-loss}\n### Squared Error Loss (point Estimate)\n    \n$$L(\\theta, a) = (\\theta - a)^2$$\n    \nTo find the optimal estimator $d(x)$, we minimize $E_{\\theta|x}[(\\theta - d(x))^2]$. Taking the derivative with respect to $d$ and setting to 0:\n    \n$$-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)$$\n    \n**Result:** The Bayes rule is the **posterior mean**.\n:::\n\n::: {#ex-absolute-loss}\n### Absolute Error Loss\n    \n$$L(\\theta, d) = |\\theta - d|$$\n    \nMinimizing $E_{\\theta|x}[|\\theta - d|]$ requires solving:\n    \n$$\\int_{-\\infty}^{d} \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = \\frac{1}{2}$$\n    \n**Result:** The Bayes rule is the **posterior median**.\n:::\n\n::: {#ex-hypothesis-testing}\n### Hypothesis Testing (0-1 Loss)\n    \nTesting $H_0: \\theta \\in \\Theta_0$ vs $H_1: \\theta \\in \\Theta_1$.\n    \n$$L(\\theta, a) = \\begin{cases} 1 & \\text{if error} \\\\ 0 & \\text{if correct} \\end{cases}$$\n    \nThe Bayes rule selects the hypothesis with the higher posterior probability.\n    \n$$d(x) = 1 \\iff P(\\theta \\in \\Theta_1 | x) \\ge P(\\theta \\in \\Theta_0 | x)$$\n:::\n\n::: {#def-hpd}\n### Highest Posterior Density (HPD) Interval\n    \nIn interval estimation, we prescribe a set $A = (d-\\delta, d+\\delta)$ and minimize the loss associated with $\\theta$ falling outside this interval.\n    \nThe Bayes rule $d(x)$ is the center of the interval with the highest probability coverage. This leads to the **Highest Posterior Density (HPD)** interval.\n    \nIn practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the **Equal-Tailed Interval**, where we cut off $\\alpha/2$ probability from each tail.\n:::\n\n## Minimax Estimation\n\nA decision rule $d(x)$ is **minimax** if it minimizes the maximum possible risk: $\\sup_\\theta R(\\theta, d)$.\n\n::: {#thm-minimax-constant}\n### Constant Risk Theorem\nIf a Bayes rule $d^\\pi$ has constant risk (i.e., $R(\\theta, d^\\pi) = c$ for all $\\theta$), then $d^\\pi$ is a minimax estimator.\n:::\n\n::: {#ex-binomial-minimax}\n### Binomial Minimax Estimator\n\nLet $X \\sim \\text{Bin}(n, \\theta)$ and $\\theta \\sim \\text{Beta}(a, b)$.\nThe squared error loss is $L(\\theta, d) = (\\theta - d)^2$.\nThe Bayes estimator is the posterior mean:\n$$d(x) = \\frac{a+x}{a+b+n}$$\n\nWe calculate the risk $R(\\theta, d)$:\n\n$$\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n$$\n\nLet $c = a+b+n$.\n$$R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]$$\n\nUsing the bias-variance decomposition and knowing $E(x) = n\\theta$ and $E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)$, we expand the risk function. To make the risk constant (independent of $\\theta$), we set the coefficients of $\\theta$ and $\\theta^2$ to zero.\n\nSolving the resulting system of equations yields:\n$$a = b = \\frac{\\sqrt{n}}{2}$$\n\nThus, the minimax estimator is:\n$$d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}$$\n\nThis differs from the standard MLE $\\hat{p} = x/n$ and the uniform prior Bayes estimator ($a=b=1$).\n:::\n\n## Stein Estimation and Shrinkage\n\nConsider estimating a multivariate mean vector $\\mu = (\\mu_1, \\dots, \\mu_p)$ given independent observations $X_i \\sim N(\\mu_i, 1)$ for $i=1, \\dots, p$.\n\nThe standard estimator is the MLE: $d^0(X) = X$.\nThe loss function is the sum of squared errors: $L(\\mu, d) = ||\\mu - d||^2 = \\sum (\\mu_i - d_i)^2$.\n\n::: {#thm-stein-inadmissibility}\n### Stein's Result\n\nWhen $p \\ge 3$, the estimator $d^0(X)$ is **inadmissible**. There exists an estimator that strictly dominates it (has lower risk everywhere).\n:::\n\nConsider the class of shrinkage estimators:\n$$d^a(X) = \\left( 1 - \\frac{a}{||X||^2} \\right) X$$\nwhere $X = (X_1, \\dots, X_p)^T$.\n\nWhen $a > 0$, this estimator \"shrinks\" the data vector toward the origin $(0, \\dots, 0)$.\n\n::: {#lem-stein}\n### Stein's Lemma\n\nIf $X \\sim N(\\mu, 1)$, then for a differentiable function $h$:\n$$E[(X-\\mu)h(X)] = E[h'(X)]$$\n:::\n\n::: {.proof}\nUsing this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator $d^a$.\n\n$$R(\\mu, d^a) = E || \\mu - d^a(X) ||^2$$\n\nAfter expanding and applying Stein's Lemma, the risk becomes:\n$$R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)$$\n\nFor $d^a$ to possess lower risk than $d^0$ (where risk = $p$), we need the term in the brackets to be positive:\n$$2a(p-2) - a^2 > 0 \\implies 0 < a < 2(p-2)$$\n\nThe optimal choice (minimizing risk) is $a = p-2$.\nThis yields the **James-Stein Estimator**:\n$$\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X$$\n:::\n\n## Empirical Bayes\n\nThe James-Stein estimator can be motivated via an Empirical Bayes approach.\n\n**Model:**\n\n1.  $X_i | \\mu_i \\sim N(\\mu_i, 1)$\n2.  Prior: $\\mu_i \\sim N(0, \\tau^2)$\n\nThe posterior mean for $\\mu_i$ (if $\\tau^2$ were known) is:\n$$E(\\mu_i|x_i) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i$$\n\nThe marginal distribution of $X_i$ is $N(0, 1+\\tau^2)$.\nConsequently, $S = \\sum X_i^2 \\sim (1+\\tau^2) \\chi^2_p$.\n\nWe can estimate the unknown shrinkage factor $B = \\frac{1}{1+\\tau^2}$ using the data.\nSince $E[ \\frac{p-2}{S} ] = \\frac{1}{1+\\tau^2}$, we replace the theoretical shrinkage factor with its unbiased estimate:\n$$\\hat{B} = \\frac{p-2}{||X||^2}$$\n\nThis recovers the James-Stein rule:\n$$\\delta^{EB}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X$$\n\n::: {#ex-baseball}\n### Baseball Example (efron & Morris)\n\nWe illustrate Stein estimation using baseball batting averages.\nLet $y_i$ be the number of hits for player $i$ in their first $n=45$ at-bats.\nLet $\\hat{p}_i = y_i/n$ be the observed average.\n\nTo apply the Normal model, we use a variance-stabilizing transformation:\n$$X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)$$\nUnder this transformation, $X_i \\approx N(\\mu_i, 1)$.\n\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value $\\mu_0$).\nResult: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages $\\hat{p}_i$.\n:::\n\n## Predictive Distributions\n\nA key feature of Bayesian analysis is the predictive distribution for a future observation $x^*$.\n\n$$f(x^*|x) = \\int f(x^*|\\theta) \\pi(\\theta|x) d\\theta$$\n\n::: {#ex-predictive-normal}\n### Normal-normal Predictive Distribution\n\nIf $x_1, \\dots, x_n \\sim N(\\mu, \\sigma^2)$ (with $\\sigma^2$ known) and $\\mu \\sim N(\\mu_0, \\sigma_0^2)$, the predictive distribution for a new observation $x^*$ is:\n\n$$x^*|x \\sim N(\\mu_1, \\sigma^2 + \\sigma_1^2)$$\n\nwhere $\\mu_1$ and $\\sigma_1^2$ are the posterior mean and variance of $\\mu$. The predictive variance includes both the inherent sampling uncertainty ($\\sigma^2$) and the uncertainty about the parameter ($\\sigma_1^2$).\n:::\n\n## Hierarchical Modeling and MCMC\n\nWhen analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).\n\n**Hierarchical Structure:**\n\n1.  **Data:** $X_i | \\mu_i \\sim f(x_i|\\mu_i)$\n2.  **Parameters:** $\\mu_i | \\theta \\sim \\pi(\\mu_i|\\theta)$\n3.  **Hyperparameters:** $\\theta \\sim \\pi(\\theta)$\n\n**Gibbs Sampling:**\nTo estimate the posterior $f(\\mu, \\theta | x)$, we sample iteratively from the **full conditional distributions**:\n\n1.  Sample $\\mu_i$ from $f(\\mu_i | x, \\theta)$.\n2.  Sample $\\theta$ from $f(\\theta | \\mu, x)$.\n\n::: {#ex-hierarchical-baseball}\n### Baseball Example with Hierarchical Model\n\n* $Y_i \\sim \\text{Bin}(n_i, p_i)$\n* Logit transform: $\\mu_i = \\text{logit}(p_i)$\n* $\\mu_i \\sim N(\\theta, \\tau^2)$\n* Priors on $\\theta$ and $\\tau^2$.\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use **Metropolis-Hastings** steps within the Gibbs sampler.\n\n**Algorithm:**\n\n1.  Initialize parameters $\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}$.\n2.  Propose new values based on a candidate distribution.\n3.  Accept or reject based on the acceptance probability ratio (Likelihood $\\times$ Prior ratio).\n4.  Repeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., $f(\\mu_j|x)$) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.\n:::\n",
    "supporting": [
      "bayesian_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}