{
  "hash": "686e52eb693bb2bd586dac1316517406",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Likelihood Theory\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Definitions and Notations\n\n### Regular Family\n\n::: {#def-regular-family}\n\n\n### Regular Families\nA family of probability density functions is said to be a **Regular Family** if the support $\\{x : f(x|\\theta) > 0\\}$ does not depend on the parameter $\\theta$.\n\nThis condition allows for the interchange of differentiation and integration:\n$$\n\\frac{\\partial}{\\partial \\theta} \\int \\exp\\{\\ell(\\theta; x)\\} dx = \\int \\frac{\\partial}{\\partial \\theta} \\exp\\{\\ell(\\theta; x)\\} dx\n$$\n\n:::\n\n### Score and Fisher Information\n\nBefore stating the theorem, we define the following notations for the score and information in the context of a parameter vector $\\theta = (\\theta_1, \\dots, \\theta_p)^T \\in \\mathbb{R}^p$:\n\n:::{#def-score-fisher}\n\n1.  **Score Vector ($U$):** The gradient of the log-likelihood. It is a random column vector of dimension $p \\times 1$.\n    $$\n    U(\\theta; X) = \\nabla \\ell(\\theta; X) = \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta} = \n    \\begin{bmatrix}\n    \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_1} \\\\[6pt]\n    \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_2} \\\\[6pt]\n    \\vdots \\\\[6pt]\n    \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_p}\n    \\end{bmatrix}\n    $$\n\n2.  **Observed Information Matrix ($J$):** The negative Hessian of the log-likelihood. It is a symmetric random matrix of dimension $p \\times p$, measuring the curvature of the log-likelihood surface.\n    $$\n    J(\\theta; X) = - \\nabla^2 \\ell(\\theta; X) = - \\frac{\\partial^2 \\ell(\\theta; X)}{\\partial \\theta \\partial \\theta^T} = -\n    \\begin{bmatrix}\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_1^2} & \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_1 \\partial \\theta_p} \\\\[8pt]\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_2 \\partial \\theta_1} & \\frac{\\partial^2 \\ell}{\\partial \\theta_2^2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_2 \\partial \\theta_p} \\\\[8pt]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[8pt]\n    \\frac{\\partial^2 \\ell}{\\partial \\theta_p \\partial \\theta_1} & \\frac{\\partial^2 \\ell}{\\partial \\theta_p \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 \\ell}{\\partial \\theta_p^2}\n    \\end{bmatrix}\n    $$\n\n3.  **(Expected) Fisher Information Matrix ($I$):** The covariance matrix of the score vector. It is a deterministic $p \\times p$ matrix (for a fixed $\\theta$).\n    $$\n    I(\\theta) = E_\\theta \\left[ J(\\theta; X) \\right]\n    $$\n\n:::\n\n## Mean and Covariance of Score Vector\n\n::: {#thm-score-identities}\n## Bartlett's Identities: Mean and Covariance of Score Vector\n\nLet $\\{f(x|\\theta) : \\theta \\in \\Theta\\}$ be a regular family of probability density functions. The following identities hold relating the moments of the score vector $U(\\theta; X)$ and the observed information matrix $J(\\theta; X)$:\n\n1.  **First Moment Identity:** The expected score is zero.\n    $$\n    E_\\theta [ U(\\theta; X) ] = 0\n    $$\n\n2.  **Second Moment Identity:** The expected observed information equals the covariance of the score vector (Fisher Information).\n    $$\n    \\text{Cov}_\\theta \\left( U(\\theta; X) \\right) = E_\\theta [ J(\\theta; X) ] = I(\\theta)\n    $$\n\n:::\n\n:::{#rem-score-moments}\nThe only assumption in the theorem above is that the families are regular. Therefore, we do not need to assume the log-likelihood $\\ell(\\theta)$ is \"well-behaved\" (e.g., approximately quadratic or indepedence within $X$) for these two identities to hold.\n:::\n\n::: {.proof}\n**1. Proof of the First Moment Identity**\n\nWe start with the fundamental property that a density function integrates to 1 over the sample space of $X$:\n$$\n\\int f(x|\\theta) \\, dx = 1\n$$\nDifferentiating both sides with respect to the parameter vector $\\theta$:\n$$\n\\nabla_\\theta \\int f(x|\\theta) \\, dx = 0\n$$\nAssuming regularity allows us to interchange differentiation and integration:\n$$\n\\int \\nabla_\\theta f(x|\\theta) \\, dx = 0\n$$\nUsing the identity $\\nabla_\\theta f(x|\\theta) = f(x|\\theta) \\nabla_\\theta \\log f(x|\\theta) = f(x|\\theta) U(\\theta; x)$:\n$$\n\\int U(\\theta; x) f(x|\\theta) \\, dx = 0\n$$\nThis is precisely the definition of the expectation:\n$$\nE_\\theta [ U(\\theta; X) ] = 0\n$$\n\n**2. Proof of the Second Moment Identity**\n\nWe differentiate the result of the First Moment Identity ($E[U(\\theta; X)]=0$) with respect to $\\theta^T$.\n$$\n\\nabla_{\\theta^T} \\int U(\\theta; x) f(x|\\theta) \\, dx = 0\n$$\nApplying the product rule inside the integral:\n$$\n\\int \\left[ \\left( \\nabla_{\\theta^T} U(\\theta; x) \\right) f(x|\\theta) + U(\\theta; x) \\left( \\nabla_{\\theta^T} f(x|\\theta) \\right) \\right] dx = 0\n$$\nWe analyze the two terms in the bracket:\n\n* **Term 1:** $\\nabla_{\\theta^T} U(\\theta; x)$ is the Jacobian of the score, which is the Hessian of the log-likelihood, $\\nabla^2 \\ell(\\theta; x)$. By definition, this is $-J(\\theta; x)$.\n* **Term 2:** We use the identity $\\nabla_{\\theta^T} f(x|\\theta) = f(x|\\theta) (\\nabla_{\\theta} \\log f(x|\\theta))^T = f(x|\\theta) U(\\theta; x)^T$.\n\nSubstituting these back into the integral:\n$$\n\\int \\left[ -J(\\theta; x) f(x|\\theta) + U(\\theta; x) U(\\theta; x)^T f(x|\\theta) \\right] dx = 0\n$$\nThis simplifies to expectations:\n$$\n-E_\\theta [ J(\\theta; X) ] + E_\\theta [ U(\\theta; X) U(\\theta; X)^T ] = 0\n$$\nRearranging gives:\n$$\nE_\\theta [ J(\\theta; X) ] = E_\\theta [ U(\\theta; X) U(\\theta; X)^T ]\n$$\nFinally, recall the definition of the covariance matrix for a random vector with zero mean. Since $E_\\theta[U(\\theta; X)] = 0$, we have:\n$$\n\\text{Cov}_\\theta(U(\\theta; X)) = E_\\theta[U(\\theta; X)U(\\theta; X)^T] - E_\\theta[U(\\theta; X)]E_\\theta[U(\\theta; X)]^T = E_\\theta[U(\\theta; X)U(\\theta; X)^T]\n$$\nTherefore, we conclude:\n$$\n\\text{Cov}_\\theta(U(\\theta; X))=E_\\theta [ J(\\theta; X) ] = I(\\theta)\n$$\n:::\n\n## Cramer-Rao Lower Bound\n\nIn estimation theory, we often wish to know the limit of how well a parameter can be estimated. The following theorem provides a lower bound on the variance of any estimator.\n\n::: {#thm-crlb}\n### Cramer-Rao Lower Bound\n\nLet $X$ be a random variable with probability density function (or probability mass function) $f(x|\\theta)$, where $\\theta \\in \\Theta$ is an unknown parameter. Let $T(X)$ be any estimator of $\\theta$ with finite variance, and let $m(\\theta) = E_\\theta[T(X)]$ denote its expectation.\n\nAssume the following **regularity conditions** hold:\n\n1.  The support of $X$, denoted $\\mathcal{X} = \\{x : f(x|\\theta) > 0\\}$, does not depend on $\\theta$.\n\n2.  The differentiation with respect to $\\theta$ and integration (or summation) with respect to $x$ can be interchanged.\n\nThen, the variance of $T(X)$ satisfies:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n\nwhere $I(\\theta) = E_\\theta \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta) \\right)^2 \\right]$ is the Fisher Information.\n\n**Particular Case:** If $T(X)$ is an **unbiased** estimator of $\\theta$ (i.e., $m(\\theta) = \\theta$ and $m'(\\theta)=1$), then:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{1}{I(\\theta)}\n$$\n:::\n\n::: {.proof}\nLet $U = \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)$ be the Score function. From the properties of the Score function under the stated regularity conditions, we know that the score has mean zero and variance equal to the Fisher Information:\n\n$$\nE_\\theta[U] = 0 \\quad \\text{and} \\quad \\text{Var}_\\theta(U) = I(\\theta)\n$$\n\nConsider the covariance between the estimator $T(X)$ and the Score $U$. By the Cauchy-Schwarz inequality (applied to covariance), we have:\n\n$$\n[\\text{Cov}_\\theta(T, U)]^2 \\le \\text{Var}_\\theta(T) \\text{Var}_\\theta(U)\n$$\n\nWe now evaluate the covariance term explicitly. By definition:\n\n$$\n\\begin{aligned}\n\\text{Cov}_\\theta(T, U) &= E_\\theta[T(X) U] - E_\\theta[T]E_\\theta[U] \\\\\n&= E_\\theta\\left[ T(X) \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta) \\right] - m(\\theta) \\cdot 0 \\\\\n&= \\int T(x) \\left( \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} \\right) f(x|\\theta) \\, dx \\\\\n&= \\int T(x) \\left( \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} \\right) f(x|\\theta) \\, dx \\\\\n&= \\int T(x) \\frac{\\partial f(x|\\theta)}{\\partial \\theta} \\, dx\n\\end{aligned}\n$$\n\nInvoking the regularity condition that allows the interchange of derivative and integral, we move the derivative outside the integral:\n\n$$\n\\text{Cov}_\\theta(T, U) = \\frac{\\partial}{\\partial \\theta} \\int T(x) f(x|\\theta) \\, dx = \\frac{\\partial}{\\partial \\theta} E_\\theta[T(X)] = m'(\\theta)\n$$\n\nSubstituting this result and $\\text{Var}_\\theta(U) = I(\\theta)$ back into the covariance inequality:\n\n$$\n[m'(\\theta)]^2 \\le \\text{Var}_\\theta(T) \\cdot I(\\theta)\n$$\n\nRearranging the terms yields the desired lower bound:\n\n$$\n\\text{Var}_\\theta(T(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n:::\n\n::: {#rem-crlb-generality}\n### Generality of the Lower Bound\nThe power of the Cramer-Rao Lower Bound lies in its independence from the specific method of estimation. It relies solely on the properties of the underlying probability model (specifically, the curvature of the log-likelihood function) and the bias of the estimator. Consequently, it provides a universal benchmark for precision:\n\n1.  **Fundamental Limit**\n    It represents the limit of \"extractable information\" about $\\theta$ contained in the data $X$. No matter how clever the estimation algorithm is (e.g., Method of Moments, Bayes estimators, etc.), the variance cannot be reduced beyond this intrinsic bound determined by the Fisher Information.\n\n2.  **Efficiency Standard**\n    It allows us to define the concept of an *efficient estimator*. Any unbiased estimator that attains this lower bound is the Uniformly Minimum Variance Unbiased Estimator (UMVUE).\n\n3.  **Asymptotic Justification**\n    While finite-sample estimators may not always achieve this bound, the Maximum Likelihood Estimator (MLE) is asymptotically efficient. This means that as the sample size $n \\to \\infty$, the variance of the MLE approaches the CRLB, justifying the popularity of likelihood-based inference.\n:::\n\n::: {#exm-exponential-crlb}\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Exp}(\\theta)$, where $f(x|\\theta) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nLog-likelihood:\n$$\nl(\\theta; x) = -n \\log \\theta - \\frac{1}{\\theta} \\sum x_i\n$$\nScore function:\n$$\nS(\\theta; x) = -\\frac{n}{\\theta} + \\frac{\\sum x_i}{\\theta^2}\n$$\nSetting $S=0 \\implies \\hat{\\theta}_{\\text{MLE}} = \\bar{x}$.\n\nFisher Information:\n$$\nI(\\theta) = \\text{Var}(S) = \\frac{1}{\\theta^4} \\text{Var}(\\sum X_i) = \\frac{1}{\\theta^4} n \\theta^2 = \\frac{n}{\\theta^2}\n$$\nAlternatively using the second derivative:\n$$\nS'(\\theta) = \\frac{n}{\\theta^2} - \\frac{2\\sum x_i}{\\theta^3}\n$$\n$$\n-E[S'] = -\\left( \\frac{n}{\\theta^2} - \\frac{2 n \\theta}{\\theta^3} \\right) = -\\left( \\frac{n}{\\theta^2} - \\frac{2n}{\\theta^2} \\right) = \\frac{n}{\\theta^2}\n$$\n\nVariance of MLE:\n$$\n\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\theta^2}{n}\n$$\nCRLB for unbiased estimator:\n$$\n\\text{CRLB} = \\frac{1}{I(\\theta)} = \\frac{1}{n/\\theta^2} = \\frac{\\theta^2}{n}\n$$\nSince $\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{CRLB}$, the MLE is efficient.\n\n:::\n\n## Exponential Families\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\\ell(\\theta; x) = \\log f(x|\\theta)$, can be expressed as the sum of three distinct terms:\n\n$$\n\\ell(\\theta; x) = \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)\n$$\n\nExponentiating this yields the density form:\n\n$$\nf(x|\\theta) = h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the vector of model parameters.\n* $\\eta_i(\\theta)$ are the **natural parameter functions**.\n* $T(x) = (T_1(x), \\dots, T_k(x))$ constitutes the vector of **sufficient statistics** for $\\theta$.\n* $A(\\theta)$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.\n* $h(x)$ is the base measure.\n\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = -\\frac{1}{\\theta} \\sum_{i=1}^n x_i - n \\log \\theta\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\theta) = -\\frac{1}{\\theta}$\n* $T_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$\n* $A(\\theta) = n \\log \\theta$\n* $\\log h(\\mathbf{x}) = 0$\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$. The density is:\n$$\nf(\\mathbf{x}|\\theta) = [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i - \\left[ n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\theta) = \\alpha - 1$, $\\quad T_1(\\mathbf{x}) = \\sum \\log x_i$\n* $\\eta_2(\\theta) = -\\frac{1}{\\beta}$, $\\quad T_2(\\mathbf{x}) = \\sum x_i$\n* $A(\\theta) = n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta$\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n$$\n\\ell(\\theta; \\mathbf{x}) = (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) - n \\log B(a, b)\n$$\n\nThis is an exponential family with $k=2$.\n\n* $\\eta_1 = a-1$, $T_1 = \\sum \\log x_i$\n* $\\eta_2 = b-1$, $T_2 = \\sum \\log(1-x_i)$\n* $A(\\theta) = n \\log B(a, b)$\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. The log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(2\\pi\\sigma^2) \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1 = \\frac{\\mu}{\\sigma^2}$, $T_1 = \\sum x_i$\n* $\\eta_2 = -\\frac{1}{2\\sigma^2}$, $T_2 = \\sum x_i^2$\n* $A(\\theta) = \\frac{n\\mu^2}{2\\sigma^2} + n \\log \\sigma + \\frac{n}{2} \\log(2\\pi)$\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\n\\ell(\\theta; x) = -\\log \\theta + \\log I(0 < x < \\theta)\n$$\n\nThe term $\\log I(0 < x < \\theta)$ couples $x$ and $\\theta$ in a way that cannot be separated into a sum $\\sum \\eta_i(\\theta) T_i(x)$.\n\n:::\n\n\n## Moments of Sufficient Statistics of Exponential Families\n\n### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n### Means via the Score Function\nFor a regular exponential family with log-likelihood $\\ell(\\theta; x) = \\sum \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)$, the expectation of the sufficient statistics can be found by setting the expected score to zero:\n\n$$\nE_\\theta \\left[ \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_j} \\right] = 0\n$$\n\nSubstituting the specific form of $\\ell(\\theta; X)$:\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} E[T_i(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j} \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nThe log-likelihood is:\n$$\n\\ell(\\theta; x) = \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)\n$$\n\nDifferentiating with respect to $\\theta_j$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} T_i(x) - \\frac{\\partial A(\\theta)}{\\partial \\theta_j}\n$$\n\nTaking the expectation and using the regularity condition $E[\\frac{\\partial \\ell}{\\partial \\theta_j}] = 0$:\n$$\nE\\left[ \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} T_i(X) - \\frac{\\partial A(\\theta)}{\\partial \\theta_j} \\right] = 0\n$$\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} E[T_i(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j}\n$$\n\n:::\n\n### Natural Parameterization\n\n::: {#def-natural-parameterization}\n### Natural Parameterization (Canonical Form)\n\nIf the parameterization is chosen such that the natural parameters are the components of the parameter vector itself (i.e., $\\eta(\\theta) = \\theta$), the exponential family is said to be in **Canonical Form** or **Natural Parameterization**.\n\nThe log-likelihood for the natural parameter vector $\\eta = (\\eta_1, \\dots, \\eta_k)^T$ simplifies to:\n$$\n\\ell(\\eta; x) = \\sum_{i=1}^k \\eta_i T_i(x) - A(\\eta) + \\log h(x)\n$$\nor in vector notation:\n$$\n\\ell(\\eta; x) = \\eta^T T(x) - A(\\eta) + \\log h(x)\n$$\nwhere $A(\\eta)$ is the log-partition function.\n:::\n\n\n:::{#def-curvedexpfam}\n\n### Full vs. Curved Exponential Families\n\n* **Full Exponential Family:** When the natural parameters $\\eta$ can vary independently in an open set of $\\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).\n* **Curved Exponential Family:** When the dimension of the parameter vector $\\theta$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\\eta(\\theta)$ to lie on a non-linear curve or surface within the natural parameter space.\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n### Curved Exponential Family Example\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The log-likelihood is:\n$$\n\\ell(\\theta; x) = -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i - n \\log \\theta - \\text{const}\n$$\n\nHere:\n\n* $\\eta_1(\\theta) = -\\frac{1}{2\\theta^2}$, $T_1 = \\sum x_i^2$\n* $\\eta_2(\\theta) = \\frac{1}{\\theta}$, $T_2 = \\sum x_i$\n\nSince $d=1$ but $k=2$, and $\\eta_1 = -\\frac{1}{2}\\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.\n\n:::\n\n### Mean and Variance of Sufficient Statitics of Exponential Families\n\n::: {#thm-cumulant-generating}\n### Mean and Variance of Sufficient Statitics\n\nFor an exponential family in canonical form, the log-partition function $A(\\eta)$ acts as the **Cumulant Generating Function** for the sufficient statistic vector $T(X)$. The derivatives of $A(\\eta)$ yield the moments of $T(X)$ as follows:\n\n1.  **Mean (First Derivative):**\n    $$\n    E[T_i(X)] = \\frac{\\partial A(\\eta)}{\\partial \\eta_i}\n    $$\n    In vector form: $E[T(X)] = \\nabla A(\\eta)$.\n\n2.  **Covariance (Second Derivative):**\n    $$\n    \\text{Cov}(T_i(X), T_j(X)) = \\frac{\\partial^2 A(\\eta)}{\\partial \\eta_i \\partial \\eta_j}\n    $$\n    In matrix form: $\\text{Var}(T(X)) = \\nabla^2 A(\\eta)$.\n\n\n:::\n\n::: {.proof}\n**Derivation**\n\nThese results follow directly from Bartlett's Identities (Theorem @thm-score-identities) applied to the canonical log-likelihood:\n$$\n\\ell(\\eta; x) = \\eta^T T(x) - A(\\eta) + \\log h(x)\n$$\n\n**For the Mean:**\nThe score function (gradient of $\\ell$) is:\n$$\nU(\\eta) = \\nabla_\\eta \\ell(\\eta; x) = T(x) - \\nabla A(\\eta)\n$$\nBy the First Moment Identity, $E[U(\\eta)] = 0$:\n$$\nE[T(X) - \\nabla A(\\eta)] = 0 \\implies E[T(X)] = \\nabla A(\\eta)\n$$\n\n**For the Covariance:**\nThe observed information (negative Hessian of $\\ell$) is:\n$$\nJ(\\eta) = -\\nabla^2_\\eta \\ell(\\eta; x) = -\\nabla_\\eta (T(x) - \\nabla A(\\eta)) = \\nabla^2 A(\\eta)\n$$\nNote that $T(x)$ is constant with respect to $\\eta$, so its derivative vanishes.\nBy the Second Moment Identity, $I(\\eta) = E[J(\\eta)] = \\text{Cov}(U(\\eta))$.\nSince $U(\\eta) = T(X) - \\text{constant}$, $\\text{Cov}(U(\\eta)) = \\text{Cov}(T(X))$.\nTherefore:\n$$\n\\text{Cov}(T(X)) = E[\\nabla^2 A(\\eta)] = \\nabla^2 A(\\eta)\n$$\n:::\n\n::: {#exm-bernoulli-moments}\n\n### Moments of the Binomial Distribution\n\nConsider $n$ independent coin flips $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$. We find the mean and variance of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\theta; x) = \\log\\left(\\frac{p}{1-p}\\right) \\sum x_i + n \\log(1-p)\n   $$\n\n   * Natural Parameter: $\\eta = \\log\\left(\\frac{p}{1-p}\\right)$.\n   * Log-Partition Function: $A(\\eta) = -n \\log(1-p) = n \\log(1+e^\\eta)$.\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = n \\frac{e^\\eta}{1+e^\\eta} = np\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = n \\frac{e^\\eta}{(1+e^\\eta)^2} = np(1-p)\n   $$\n\n:::\n\n::: {#exm-exponential-moments}\n\n### Moments of the Gamma Sufficient Statistic\n\nConsider $X_i \\sim \\text{Exp}(\\lambda)$. We find the moments of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\lambda; x) = -\\lambda \\sum x_i + n \\log \\lambda\n   $$\n\n   * Natural Parameter: $\\eta = -\\lambda$.\n   * Log-Partition Function: $A(\\eta) = -n \\log \\lambda = -n \\log(-\\eta)$.\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = -n \\frac{1}{-\\eta}(-1) = -\\frac{n}{\\eta} = \\frac{n}{\\lambda}\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = \\frac{n}{\\eta^2} = \\frac{n}{\\lambda^2}\n   $$\n\n:::\n\n::: {#exm-normal-sufficient-moments}\n\n### Moments of Normal Sufficient Statistics\n\nConsider $X_i \\sim N(\\mu, \\sigma^2)$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\theta; x) = \\frac{\\mu}{\\sigma^2} \\sum x_i - \\frac{1}{2\\sigma^2} \\sum x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(\\sigma^2) \\right]\n   $$\n   \n   Natural Parameters: $\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\eta_2 = -\\frac{1}{2\\sigma^2}$.\n   \n   Log-Partition Function (in terms of $\\eta$):\n   $$\n   A(\\eta) = -\\frac{n \\eta_1^2}{4 \\eta_2} - \\frac{n}{2} \\log(-2\\eta_2)\n   $$\n\n2. **First Moments (Means)**\n   $$\n   E[T_1] = E\\left[\\sum X_i\\right] = \\frac{\\partial A}{\\partial \\eta_1} = -\\frac{n\\eta_1}{2\\eta_2} = n\\mu\n   $$\n   $$\n   E[T_2] = E\\left[\\sum X_i^2\\right] = \\frac{\\partial A}{\\partial \\eta_2} = \\frac{n\\eta_1^2}{4\\eta_2^2} - \\frac{n}{2\\eta_2} = n(\\mu^2 + \\sigma^2)\n   $$\n\n3. **Second Moment (Covariance)**\n   $$\n   \\text{Cov}(T_1, T_2) = \\frac{\\partial^2 A}{\\partial \\eta_1 \\partial \\eta_2} = \\frac{n\\eta_1}{2\\eta_2^2} = 2n\\mu\\sigma^2\n   $$\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}