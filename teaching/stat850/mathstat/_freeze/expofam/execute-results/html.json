{
  "hash": "670dd9c165554212c6b6eb970cfc25ed",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Families\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Sufficient Statistics\n\n::: {#def-sufficient-statistic}\n### Sufficient Statistic\nA statistic $T(\\mathbf{X})$ is said to be **sufficient** for the parameter $\\theta$ if the conditional distribution of the sample $\\mathbf{X}$ given the value of the statistic $T(\\mathbf{X}) = t$ does not depend on $\\theta$.\n\nIn other words, once $T(\\mathbf{X})$ is known, no other information in the sample $\\mathbf{X}$ provides additional information about $\\theta$.\n:::\n\n::: {#thm-factorization}\n### Factorization Theorem\nLet $f(\\mathbf{x}|\\theta)$ denote the joint probability density function (or probability mass function) of a sample $\\mathbf{X}$. A statistic $T(\\mathbf{X})$ is **sufficient** for $\\theta$ if and only if the density can be factored as:\n\n$$\nf(\\mathbf{x}|\\theta) = g(T(\\mathbf{x})|\\theta) h(\\mathbf{x})\n$$\n\nwhere:\n\n* $g(T(\\mathbf{x})|\\theta)$ depends on the data $\\mathbf{x}$ only through the statistic $T(\\mathbf{x})$ and the parameter $\\theta$.\n\n* $h(\\mathbf{x})$ does not depend on $\\theta$.\n:::\n\n::: {#exm-normal-sufficient}\n\n### Sufficient Statistics for Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. The joint density is given by:\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\nExpanding the exponent:\n\n$$\nf(\\mathbf{x}|\\theta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^n x_i^2 - 2\\mu \\sum_{i=1}^n x_i + n\\mu^2 \\right) \\right\\}\n$$\n\nWe can rearrange this into the factorization form $g(T(\\mathbf{x})|\\theta) h(\\mathbf{x})$:\n\n$$\nf(\\mathbf{x}|\\theta) = \\underbrace{ (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\} }_{g(T(\\mathbf{x})|\\theta)} \\cdot \\underbrace{ 1 }_{h(\\mathbf{x})}\n$$\n\nHere, the function $g$ depends on the data only through the pair of statistics:\n\n$$\nT(\\mathbf{x}) = \\left( \\sum_{i=1}^n X_i, \\sum_{i=1}^n X_i^2 \\right)\n$$\n\nThus, $T(\\mathbf{X}) = (\\sum X_i, \\sum X_i^2)$ is sufficient for $\\theta = (\\mu, \\sigma^2)$ .\n\n:::\n\n::: {#rem-sufficient-likelihood}\n### Sufficient Statistic as Parameter of Likelihood\nThere is a dual relationship between the sufficient statistic and the parameter $\\theta$. Conventionally, we view $f(x|\\theta)$ as a function of $x$ parameterized by $\\theta$.\n\nHowever, in Bayesian inference or likelihood theory, we often view the likelihood $L(\\theta; x)$ as a function of $\\theta$ determined by the observed data $x$. The Factorization Theorem implies:\n\n$$\nL(\\theta; \\mathbf{x}) \\propto g(T(\\mathbf{x})|\\theta)\n$$\n\nThis suggests that $T(\\mathbf{x})$ completely determines the shape of the likelihood function. In this specific sense, the sufficient statistic $T(\\mathbf{x})$ acts as the **\"parameter\"** of the likelihood function itself.\n\nFor the exponential family that we will discuss below, this duality is explicit:\n\n$$\n\\log L(\\theta; \\mathbf{x}) = \\text{const} + \\sum_{i=1}^k \\eta_i(\\theta) T_i(\\mathbf{x}) - n A(\\theta)\n$$\n\nHere, $T_i(\\mathbf{x})$ serves as the coefficient (or parameter) for the function $\\eta_i(\\theta)$.\n:::\n\n## Exponential Families\n\n::: {#def-exponential-family}\n### Exponential Family\nA family of probability density functions (or probability mass functions) $f(x|\\theta)$ is said to be an **Exponential Family** if it can be written in the form:\n\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the parameter vector.\n\n* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.\n\n* By the Factorization Theorem, the vector $T(x) = (\\tau_1(x), \\dots, \\tau_k(x))$ constitutes a **sufficient statistic** for $\\theta$.\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{-x_i/\\theta} = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $C(\\theta) = \\theta^{-n}$\n\n* $h(\\mathbf{x}) = 1$\n\n* $\\pi_1(\\theta) = -\\frac{1}{\\theta}$\n\n* $\\tau_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$.\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}\n$$\n\n$$\n= [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nRewriting in the canonical form:\n\n$$\n= [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere:\n\n* $\\pi_1(\\theta) = \\alpha - 1$, $\\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2(\\theta) = -\\frac{1}{\\beta}$, $\\tau_2(\\mathbf{x}) = \\sum x_i$.\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}\n$$\n\n$$\n= [B(a, b)]^{-n} \\exp\\left\\{ (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) \\right\\}\n$$\n\nThis is an exponential family with $k=2$.\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$ with $\\theta = (\\mu, \\sigma^2)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{n\\mu^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= \\left[ (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} e^{-\\frac{n\\mu^2}{2\\sigma^2}} \\right] \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere $d=2$ and $k=2$.\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\theta} I(0 < x < \\theta)\n$$\n\nThis cannot be written in the required form because the indicator function $I(0 < x < \\theta)$ cannot be factorized into separate functions of $x$ and $\\theta$ inside an exponential.\n\n:::\n\n::: {#exm-cauchy-dist}\n\n### Cauchy Distribution\nLet $X \\sim \\text{Cauchy}(\\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\pi [1 + (x-\\theta)^2]}\n$$\n\nThis involves $\\log(1 + (x-\\theta)^2)$ in the exponent, which cannot be separated into sums of products $\\pi_i(\\theta)\\tau_i(x)$.\n\n:::\n\n## Regular Families\n\nIn the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.\n\n::: {#def-regular-family}\n\n### Regular Family\nA family of probability density functions $f(x|\\theta)$ is said to be a **Regular Family** (or \"sufficiently well-behaved\") if the domain of $x$ for which $f(x|\\theta) > 0$ (the support) does not depend on the parameter $\\theta$.\n\nThis condition is necessary to satisfy the identity that allows differentiation under the integral sign:\n\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx\n$$\n\n:::\n\n### Why Regularity Matters {.unnumbered}\n\nIf a family is regular, we can differentiate the identity $\\int f(x|\\theta) dx = 1$ with respect to $\\theta$. This operation yields the fundamental properties of the **Score Function** $\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)$:\n\n1.  **First Moment Identity:** The expected value of the score function is zero.\n\n$$E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = 0$$\n\n2.  **Second Moment Identity:** The Fisher Information (variance of the score) is equal to the negative expected Hessian.\n\n$$E_\\theta \\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_j \\partial \\theta_l} \\right] = -E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_l} \\right]$$\n\n### An Example of Non-regular Distribution  {.unnumbered}\nThe **Uniform Distribution** $U(0, \\theta)$ is a classic example of a **non-regular** family. Because the support $(0, \\theta)$ depends on $\\theta$, the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.\n\n## Moments of Sufficient Statistics of Exponential Families\n\n### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n### Means of Sufficient Statistics (General Case)\nFor a random variable $X$ belonging to an exponential family with density $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\\}$, the moments of the sufficient statistics $\\tau_i(X)$ satisfy the system of equations:\n$$\n\\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} E[\\tau_i(X)] = 0 \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nFor regular families, we can interchange differentiation and integration. Since $\\int f(x|\\theta) dx = 1$, we have:\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = 0 \\implies \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx = 0\n$$\n\nUsing the identity $\\frac{\\partial f}{\\partial \\theta} = f(x|\\theta) \\frac{\\partial \\log f}{\\partial \\theta}$, we derive the fundamental moment property:\n\n$$\nE\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} f(x|\\theta) dx = 0\n$$\n\nFor the exponential family, the log-likelihood is given by:\n\n$$\n\\log f(x|\\theta) = \\log C(\\theta) + \\log h(x) + \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\n$$\n\nTaking the derivative with respect to $\\theta_j$:\n\n$$\n\\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} = \\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} \\tau_i(x)\n$$\n\nTaking expectations and applying the condition $E[\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)] = 0$ yields the theorem statement.\n\n:::\n\n::: {#exm-normal-moments}\n\n### Moments of Normal Sufficient Statistics\nConsider the Normal distribution $N(\\mu, \\sigma^2)$ where $\\theta = (\\mu, \\sigma^2)$. The density is:\n$$\nf(\\mathbf{x}|\\theta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify the components :\n\n* $C(\\mu, \\sigma^2) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)$\n\n* $\\pi_1 = -\\frac{1}{2\\sigma^2}, \\quad \\tau_1(\\mathbf{x}) = \\sum x_i^2$\n\n* $\\pi_2 = \\frac{\\mu}{\\sigma^2}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i$\n\nWe apply the theorem by differentiating with respect to $\\mu$ and $\\sigma^2$.\n\n1. Differentiate with respect to $\\mu$:\n\n$$\n   \\frac{\\partial \\log C}{\\partial \\mu} = -\\frac{n\\mu}{\\sigma^2}\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   \\frac{\\partial \\pi_1}{\\partial \\mu} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\mu} = \\frac{1}{\\sigma^2}\n   $$\n\n   The theorem equation becomes:\n\n   $$\n   -\\frac{n\\mu}{\\sigma^2} + 0 \\cdot E[\\sum X_i^2] + \\frac{1}{\\sigma^2} E[\\sum X_i] = 0\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   \\implies E[\\sum_{i=1}^n X_i] = n\\mu\n   $$\n\n2. Differentiate with respect to $\\sigma^2$:\n\n$$\n   \\frac{\\partial \\log C}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2}\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   \\frac{\\partial \\pi_1}{\\partial \\sigma^2} = \\frac{1}{2(\\sigma^2)^2}, \\quad \\frac{\\partial \\pi_2}{\\partial \\sigma^2} = -\\frac{\\mu}{(\\sigma^2)^2}\n   $$\n\n   The theorem equation becomes:\n\n   $$\n   \\left( -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2} \\right) + \\frac{1}{2(\\sigma^2)^2} E[\\sum X_i^2] - \\frac{\\mu}{(\\sigma^2)^2} E[\\sum X_i] = 0\n   $$\n\n   Multiplying by $2(\\sigma^2)^2$:\n\n   $$\n   -n\\sigma^2 + n\\mu^2 + E[\\sum X_i^2] - 2\\mu(n\\mu) = 0\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   E[\\sum X_i^2] = n\\sigma^2 + n\\mu^2\n   $$\n\n   This recovers the standard second moment $E[X^2] = \\sigma^2 + \\mu^2$.\n\n:::\n\n::: {#exm-gamma-moments}\n\n### Moments of Gamma Sufficient Statistics\nConsider the Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$ with $\\theta = (\\alpha, \\beta)$. The density is :\n$$\nf(\\mathbf{x}|\\theta) = [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $\\log C(\\alpha, \\beta) = -n \\log \\Gamma(\\alpha) - n\\alpha \\log \\beta$\n\n* $\\pi_1 = \\alpha - 1, \\quad \\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2 = -\\frac{1}{\\beta}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i$\n\n1. Differentiate with respect to $\\beta$:\n\n$$\n   \\frac{\\partial \\log C}{\\partial \\beta} = -\\frac{n\\alpha}{\\beta}\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   \\frac{\\partial \\pi_1}{\\partial \\beta} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\beta} = \\frac{1}{\\beta^2}\n   $$\n\n   The theorem yields:\n\n   $$\n   -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2} E[\\sum X_i] = 0 \\implies E[\\sum X_i] = n\\alpha\\beta\n   $$\n\n2. Differentiate with respect to $\\alpha$:\n\n$$\n   \\frac{\\partial \\log C}{\\partial \\alpha} = -n \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} - n \\log \\beta = -n \\psi(\\alpha) - n \\log \\beta\n   $$\n\n   where $\\psi(\\alpha)$ is the digamma function.\n\n   $$\n   \\frac{\\partial \\pi_1}{\\partial \\alpha} = 1, \\quad \\frac{\\partial \\pi_2}{\\partial \\alpha} = 0\n   $$\n\n   The theorem yields:\n\n   $$\n   (-n \\psi(\\alpha) - n \\log \\beta) + 1 \\cdot E[\\sum \\log X_i] = 0\n   $$\n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n  \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n   \n\n   \n\n  \n\n   \n\n \n\n   \n\n  \n\n $$\n   \\implies E[\\sum_{i=1}^n \\log X_i] = n(\\psi(\\alpha) + \\log \\beta)\n   $$\n\n:::\n\n\n### Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n### Natural Parameterization\nSuppose an exponential family is given by:\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nWe define the **natural parameters** $\\eta_i$ as $\\eta_i = \\pi_i(\\theta)$. Let $\\eta = (\\eta_1, \\dots, \\eta_k)$. The density becomes:\n\n$$\nf(x|\\eta) = C^*(\\eta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i \\tau_i(x) \\right\\}\n$$\n\n:::\n\n::: {#def-natural-space}\n\n### Natural Parameter Space\nThe natural parameter space $\\mathcal{H}$ is defined as:\n$$\n\\mathcal{H} = \\{ \\eta = (\\pi_1(\\theta), \\dots, \\pi_k(\\theta)) : \\int h(x) e^{\\sum \\eta_i \\tau_i(x)} dx < \\infty \\}\n$$\n\nwhere the condition ensures $C(\\theta)$ is finite.\n\n:::\n\n:::{#def-curvedexpfam}\n\n### Full vs. Curved Exponential Families\n\nLet $d$ be the dimension of $\\theta$ and $k$ be the dimension of the sufficient statistic vector $\\tau(x)$.\n\n* If $d = k$, we say $f(x|\\theta)$ is a **Full Exponential Family**.\n\n* If $d < k$, we say $f(x|\\theta)$ is a **Curved Exponential Family**.\n\n:::\n\n::: {#exm-natural-normal}\n\n### Natural Parameterization of Normal Distribution\nConsider the full Normal family $N(\\mu, \\sigma^2)$ where both parameters are unknown ($d=2$). We previously identified the sufficient statistics $T_1(x) = \\sum x_i$ and $T_2(x) = \\sum x_i^2$ ($k=2$). Since $d=k$, this is a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are defined by the mapping:\n\n$$\n\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\quad \\eta_2 = -\\frac{1}{2\\sigma^2}\n$$\n\nThe density can be rewritten purely in terms of $\\eta$:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i + \\eta_2 \\sum_{i=1}^n x_i^2 - A(\\eta) \\right\\}\n$$\nwhere the log-partition function $A(\\eta)$ absorbs the normalizing constants.\n\n:::\n\n::: {#exm-natural-gamma}\n\n### Natural Parameterization of Gamma Distribution\nConsider the Gamma family $\\text{Gamma}(\\alpha, \\beta)$ ($d=2$). We identified the sufficient statistics $T_1(x) = \\sum \\log x_i$ and $T_2(x) = \\sum x_i$ ($k=2$). Since $d=k$, this is also a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are derived from the canonical form :\n\n$$\n\\eta_1 = \\alpha - 1, \\quad \\eta_2 = -\\frac{1}{\\beta}\n$$\n\nThe density in natural parameterization is:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n \\log x_i + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n### Curved Exponential Family (Natural Parameterization)\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The density is:\n$$\nf(x|\\theta) \\propto \\exp\\left\\{ -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i \\right\\}\n$$\n\nTo express this in the natural parameterization, we define $\\eta = (\\eta_1, \\eta_2)$ as:\n\n$$\n\\eta_1 = -\\frac{1}{2\\theta^2}, \\quad \\eta_2 = \\frac{1}{\\theta}\n$$\n\nThe density becomes:\n\n$$\nf(x|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i^2 + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\nHowever, the natural parameters $\\eta_1$ and $\\eta_2$ are not independent. They satisfy the constraint:\n\n$$\n\\eta_1 = -\\frac{1}{2} \\eta_2^2\n$$\n\nBecause the parameter space $\\mathcal{H}$ forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a **Curved Exponential Family**.\n\n:::\n\n### Mean and Covariance of  Natural Exponential Families\n\n\n::: {#thm-natural-covariance}\n\n### Mean and Covariance of Natural Exponential Families\nIf the exponential family is in its **canonical form** (natural parameterization) where $\\pi_i(\\theta) = \\theta_i$ for each $i$, then the mean and covariance of the sufficient statistics are given by:\n$$\nE_\\theta[\\tau_i(X)] = - \\frac{\\partial}{\\partial \\theta_i} \\log C(\\theta)\n$$\n\n$$\n\\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X)) = - \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log C(\\theta)\n$$\n\n:::\n\n::: {.proof}\nWe use the second-order regularity condition for the score function:\n$$\nE_\\theta\\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_i \\partial \\theta_j} \\right] = - E_\\theta\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_i} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right]\n$$\n\nIn the case of a natural exponential family, $\\pi_k(\\theta) = \\theta_k$. The derivative of the log-likelihood simplifies to:\n\n$$\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\frac{\\partial \\log C(\\theta)}{\\partial \\theta_i} + \\tau_i(x)\n$$\n\nDifferentiating again with respect to $\\theta_j$:\n\n$$\n\\frac{\\partial^2 \\log f}{\\partial \\theta_i \\partial \\theta_j} = \\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\n$$\n\nSince this second derivative is non-random (it does not depend on $x$), its expectation is simply itself.\n\nNow consider the right-hand side of the regularity condition. From the first moment property, we know $E[\\tau_i(X)] = - \\frac{\\partial \\log C}{\\partial \\theta_i}$. Thus, the score function is:\n\n$$\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\tau_i(X) - E[\\tau_i(X)]\n$$\n\nSubstituting these into the identity:\n\n$$\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - E\\left[ (\\tau_i(X) - E[\\tau_i(X)]) (\\tau_j(X) - E[\\tau_j(X)]) \\right]\n$$\n\n$$\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - \\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X))\n$$\n\nMultiplying by $-1$ gives the result.\n\n:::\n\n\n\n## Distributions of Sufficient Statistics\n\n::: {#lem-joint-distribution}\n\n### Joint Distribution of Sufficient Statistics\nIf $X$ has a distribution in the exponential family $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum \\pi_i(\\theta) \\tau_i(x)\\}$, then the joint distribution of the sufficient statistics $T = (\\tau_1(X), \\dots, \\tau_k(X))$ is also in the exponential family with the same natural parameters.\n\n:::\n\n::: {.proof}\nLet $X$ be discrete. The probability mass function of $T$ is:\n$$\nP(T_1 = y_1, \\dots, T_k = y_k | \\theta) = \\sum_{\\{x : \\tau(x) = y\\}} P(X=x|\\theta)\n$$\n\n$$\n= \\sum_{\\{x : \\tau(x) = y\\}} C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nSince the exponential term and $C(\\theta)$ depend only on $y$ and $\\theta$, they can be pulled out of the sum:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\} \\left( \\sum_{\\{x : \\tau(x) = y\\}} h(x) \\right)\n$$\n\nDefining $h^*(y) = \\sum_{\\{x : \\tau(x) = y\\}} h(x)$, we get:\n\n$$\nf_T(y|\\theta) = C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nwhich is of the exponential family form .\n\n:::\n\n::: {#lem-marginal-distribution}\n\n### Marginal Distribution\nLet $S$ be a subset of indices $\\{1, \\dots, k\\}$. If $\\pi_i(\\theta)$ are constant for all $i \\notin S$, then the marginal distribution of the statistics $T_S = \\{ \\tau_j(X) : j \\in S \\}$ is of the exponential family form with natural parameters $\\pi_j(\\theta)$ for $j \\in S$.\n\n:::\n\n::: {.proof}\nLet $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):\n$$\nP(T_S = y_S | \\theta) = \\sum_{y_{S^c}} C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j + \\sum_{l \\in S^c} \\pi_l(\\theta) y_l \\right\\}\n$$\n\nSince $\\pi_l(\\theta)$ is constant for $l \\in S^c$, the term $\\exp\\{\\sum_{l \\in S^c} \\pi_l y_l\\}$ does not depend on $\\theta$. We can group it with $h^*(y)$:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j \\right\\} \\sum_{y_{S^c}} h^*(y) \\exp\\left\\{ \\sum_{l \\in S^c} \\pi_l y_l \\right\\}\n$$\n\nThe sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form .\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}