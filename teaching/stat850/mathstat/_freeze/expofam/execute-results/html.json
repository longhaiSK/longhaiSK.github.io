{
  "hash": "bd03e687fd6a9ca11d3a7993d8b6e081",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Families\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n## Exponential Families\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) is said to be an **Exponential Family** if the log-likelihood function, denoted by $\\ell(\\theta; x) = \\log f(x|\\theta)$, can be expressed as the sum of three distinct terms:\n\n$$\n\\ell(\\theta; x) = \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)\n$$\n\nExponentiating this yields the density form:\n\n$$\nf(x|\\theta) = h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the vector of model parameters.\n* $\\eta_i(\\theta)$ are the **natural parameter functions**.\n* $T(x) = (T_1(x), \\dots, T_k(x))$ constitutes the vector of **sufficient statistics** for $\\theta$.\n* $A(\\theta)$ is the **log-partition function** (or cumulant function), which ensures the density integrates to 1.\n* $h(x)$ is the base measure.\n\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = -\\frac{1}{\\theta} \\sum_{i=1}^n x_i - n \\log \\theta\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\theta) = -\\frac{1}{\\theta}$\n* $T_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$\n* $A(\\theta) = n \\log \\theta$\n* $\\log h(\\mathbf{x}) = 0$\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$. The density is:\n$$\nf(\\mathbf{x}|\\theta) = [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nThe log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i - \\left[ n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1(\\theta) = \\alpha - 1$, $\\quad T_1(\\mathbf{x}) = \\sum \\log x_i$\n* $\\eta_2(\\theta) = -\\frac{1}{\\beta}$, $\\quad T_2(\\mathbf{x}) = \\sum x_i$\n* $A(\\theta) = n \\log \\Gamma(\\alpha) + n\\alpha \\log \\beta$\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n$$\n\\ell(\\theta; \\mathbf{x}) = (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) - n \\log B(a, b)\n$$\n\nThis is an exponential family with $k=2$.\n\n* $\\eta_1 = a-1$, $T_1 = \\sum \\log x_i$\n* $\\eta_2 = b-1$, $T_2 = \\sum \\log(1-x_i)$\n* $A(\\theta) = n \\log B(a, b)$\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. The log-likelihood is:\n$$\n\\ell(\\theta; \\mathbf{x}) = \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(2\\pi\\sigma^2) \\right]\n$$\n\nIdentifying the components:\n\n* $\\eta_1 = \\frac{\\mu}{\\sigma^2}$, $T_1 = \\sum x_i$\n* $\\eta_2 = -\\frac{1}{2\\sigma^2}$, $T_2 = \\sum x_i^2$\n* $A(\\theta) = \\frac{n\\mu^2}{2\\sigma^2} + n \\log \\sigma + \\frac{n}{2} \\log(2\\pi)$\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\n\\ell(\\theta; x) = -\\log \\theta + \\log I(0 < x < \\theta)\n$$\n\nThe term $\\log I(0 < x < \\theta)$ couples $x$ and $\\theta$ in a way that cannot be separated into a sum $\\sum \\eta_i(\\theta) T_i(x)$.\n\n:::\n\n## Regular Families\n\nIn the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.\n\n::: {#def-regular-family}\n\n### Regular Family\nA family of probability density functions is said to be a **Regular Family** if the support $\\{x : f(x|\\theta) > 0\\}$ does not depend on the parameter $\\theta$.\n\nThis condition allows for the interchange of differentiation and integration:\n$$\n\\frac{\\partial}{\\partial \\theta} \\int \\exp\\{\\ell(\\theta; x)\\} dx = \\int \\frac{\\partial}{\\partial \\theta} \\exp\\{\\ell(\\theta; x)\\} dx\n$$\n\n:::\n\n### Why Regularity Matters {.unnumbered}\n\nIf a family is regular, we can differentiate the identity $\\int f(x|\\theta) dx = 1$ with respect to $\\theta$. This yields the fundamental properties of the **Score Function** $\\nabla \\ell(\\theta; X)$:\n\n1.  **First Moment Identity:** The expected value of the score is zero.\n    $$E_\\theta \\left[ \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_j} \\right] = 0$$\n\n2.  **Second Moment Identity:** The Fisher Information is the negative expected Hessian.\n    $$E_\\theta \\left[ \\frac{\\partial^2 \\ell(\\theta; X)}{\\partial \\theta_j \\partial \\theta_k} \\right] = -E_\\theta \\left[ \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_j} \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_k} \\right]$$\n\n## Moments of Sufficient Statistics\n\n### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n### Means via the Score Function\nFor a regular exponential family with log-likelihood $\\ell(\\theta; x) = \\sum \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)$, the expectation of the sufficient statistics can be found by setting the expected score to zero:\n\n$$\nE_\\theta \\left[ \\frac{\\partial \\ell(\\theta; X)}{\\partial \\theta_j} \\right] = 0\n$$\n\nSubstituting the specific form of $\\ell(\\theta; X)$:\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} E[T_i(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j} \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nThe log-likelihood is:\n$$\n\\ell(\\theta; x) = \\sum_{i=1}^k \\eta_i(\\theta) T_i(x) - A(\\theta) + \\log h(x)\n$$\n\nDifferentiating with respect to $\\theta_j$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} T_i(x) - \\frac{\\partial A(\\theta)}{\\partial \\theta_j}\n$$\n\nTaking the expectation and using the regularity condition $E[\\frac{\\partial \\ell}{\\partial \\theta_j}] = 0$:\n$$\nE\\left[ \\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} T_i(X) - \\frac{\\partial A(\\theta)}{\\partial \\theta_j} \\right] = 0\n$$\n\n$$\n\\sum_{i=1}^k \\frac{\\partial \\eta_i(\\theta)}{\\partial \\theta_j} E[T_i(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta_j}\n$$\n\n:::\n\n### Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n### Natural Parameterization\nIf the parameters are chosen such that $\\eta_i(\\theta) = \\theta_i$ (implying $d=k$), the family is in **Canonical Form** (or Natural Parameterization). The log-likelihood simplifies to:\n\n$$\n\\ell(\\eta; x) = \\sum_{i=1}^k \\eta_i T_i(x) - A(\\eta) + \\log h(x)\n$$\n\nIn this case, the moment generating properties become direct derivatives of the log-partition function $A(\\eta)$.\n\n**Mean:**\n$$\nE[T_i(X)] = \\frac{\\partial A(\\eta)}{\\partial \\eta_i}\n$$\n\n**Variance/Covariance:**\n$$\n\\text{Cov}(T_i(X), T_j(X)) = \\frac{\\partial^2 A(\\eta)}{\\partial \\eta_i \\partial \\eta_j}\n$$\n\n:::\n\n:::{#def-curvedexpfam}\n\n### Full vs. Curved Exponential Families\n\n* **Full Exponential Family:** When the natural parameters $\\eta$ can vary independently in an open set of $\\mathbb{R}^k$ (i.e., $d=k$ and the mapping is a bijection).\n* **Curved Exponential Family:** When the dimension of the parameter vector $\\theta$ is smaller than the number of sufficient statistics ($d < k$), forcing the natural parameters $\\eta(\\theta)$ to lie on a non-linear curve or surface within the natural parameter space.\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n### Curved Exponential Family Example\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The log-likelihood is:\n$$\n\\ell(\\theta; x) = -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i - n \\log \\theta - \\text{const}\n$$\n\nHere:\n\n* $\\eta_1(\\theta) = -\\frac{1}{2\\theta^2}$, $T_1 = \\sum x_i^2$\n* $\\eta_2(\\theta) = \\frac{1}{\\theta}$, $T_2 = \\sum x_i$\n\nSince $d=1$ but $k=2$, and $\\eta_1 = -\\frac{1}{2}\\eta_2^2$, the parameters are constrained to a parabola. This is a **Curved Exponential Family**.\n\n:::\n\n::: {#exm-bernoulli-moments}\n\n### Moments of the Binomial Distribution\n\nConsider $n$ independent coin flips $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$. We find the mean and variance of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\theta; x) = \\log\\left(\\frac{p}{1-p}\\right) \\sum x_i + n \\log(1-p)\n   $$\n\n   * Natural Parameter: $\\eta = \\log\\left(\\frac{p}{1-p}\\right)$.\n   * Log-Partition Function: $A(\\eta) = -n \\log(1-p) = n \\log(1+e^\\eta)$.\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = n \\frac{e^\\eta}{1+e^\\eta} = np\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = n \\frac{e^\\eta}{(1+e^\\eta)^2} = np(1-p)\n   $$\n\n:::\n\n::: {#exm-exponential-moments}\n\n### Moments of the Gamma Sufficient Statistic\n\nConsider $X_i \\sim \\text{Exp}(\\lambda)$. We find the moments of $T = \\sum X_i$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\lambda; x) = -\\lambda \\sum x_i + n \\log \\lambda\n   $$\n\n   * Natural Parameter: $\\eta = -\\lambda$.\n   * Log-Partition Function: $A(\\eta) = -n \\log \\lambda = -n \\log(-\\eta)$.\n\n2. **Calculating Moments**\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\eta} = -n \\frac{1}{-\\eta}(-1) = -\\frac{n}{\\eta} = \\frac{n}{\\lambda}\n   $$\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\eta^2} = \\frac{n}{\\eta^2} = \\frac{n}{\\lambda^2}\n   $$\n\n:::\n\n::: {#exm-normal-sufficient-moments}\n\n### Moments of Normal Sufficient Statistics\n\nConsider $X_i \\sim N(\\mu, \\sigma^2)$.\n\n1. **Log-Likelihood Form**\n   $$\n   \\ell(\\theta; x) = \\frac{\\mu}{\\sigma^2} \\sum x_i - \\frac{1}{2\\sigma^2} \\sum x_i^2 - \\left[ \\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2} \\log(\\sigma^2) \\right]\n   $$\n   \n   Natural Parameters: $\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\eta_2 = -\\frac{1}{2\\sigma^2}$.\n   \n   Log-Partition Function (in terms of $\\eta$):\n   $$\n   A(\\eta) = -\\frac{n \\eta_1^2}{4 \\eta_2} - \\frac{n}{2} \\log(-2\\eta_2)\n   $$\n\n2. **First Moments (Means)**\n   $$\n   E[T_1] = E\\left[\\sum X_i\\right] = \\frac{\\partial A}{\\partial \\eta_1} = -\\frac{n\\eta_1}{2\\eta_2} = n\\mu\n   $$\n   $$\n   E[T_2] = E\\left[\\sum X_i^2\\right] = \\frac{\\partial A}{\\partial \\eta_2} = \\frac{n\\eta_1^2}{4\\eta_2^2} - \\frac{n}{2\\eta_2} = n(\\mu^2 + \\sigma^2)\n   $$\n\n3. **Second Moment (Covariance)**\n   $$\n   \\text{Cov}(T_1, T_2) = \\frac{\\partial^2 A}{\\partial \\eta_1 \\partial \\eta_2} = \\frac{n\\eta_1}{2\\eta_2^2} = 2n\\mu\\sigma^2\n   $$\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}