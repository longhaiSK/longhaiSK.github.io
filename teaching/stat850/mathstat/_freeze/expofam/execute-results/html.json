{
  "hash": "18dcb98f8bb930b95ec4dc5573eba49d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Families\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n\n## Exponential Families\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) $f(x|\\theta)$ is said to be an **Exponential Family** if it can be written in the form:\n\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the parameter vector.\n\n* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.\n\n* By the Factorization Theorem, the vector $T(x) = (\\tau_1(x), \\dots, \\tau_k(x))$ constitutes a **sufficient statistic** for $\\theta$.\n\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{-x_i/\\theta} = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $C(\\theta) = \\theta^{-n}$\n\n* $h(\\mathbf{x}) = 1$\n\n* $\\pi_1(\\theta) = -\\frac{1}{\\theta}$\n\n* $\\tau_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$.\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}\n$$\n\n$$\n= [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nRewriting in the canonical form:\n\n$$\n= [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere:\n\n* $\\pi_1(\\theta) = \\alpha - 1$, $\\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2(\\theta) = -\\frac{1}{\\beta}$, $\\tau_2(\\mathbf{x}) = \\sum x_i$.\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}\n$$\n\n$$\n= [B(a, b)]^{-n} \\exp\\left\\{ (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) \\right\\}\n$$\n\nThis is an exponential family with $k=2$.\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$ with $\\theta = (\\mu, \\sigma^2)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{n\\mu^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= \\left[ (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} e^{-\\frac{n\\mu^2}{2\\sigma^2}} \\right] \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere $d=2$ and $k=2$.\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\theta} I(0 < x < \\theta)\n$$\n\nThis cannot be written in the required form because the indicator function $I(0 < x < \\theta)$ cannot be factorized into separate functions of $x$ and $\\theta$ inside an exponential.\n\n:::\n\n::: {#exm-cauchy-dist}\n\n### Cauchy Distribution\nLet $X \\sim \\text{Cauchy}(\\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\pi [1 + (x-\\theta)^2]}\n$$\n\nThis involves $\\log(1 + (x-\\theta)^2)$ in the exponent, which cannot be separated into sums of products $\\pi_i(\\theta)\\tau_i(x)$.\n\n:::\n\n## Regular Families\n\nIn the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.\n\n::: {#def-regular-family}\n\n### Regular Family\nA family of probability density functions $f(x|\\theta)$ is said to be a **Regular Family** (or \"sufficiently well-behaved\") if the domain of $x$ for which $f(x|\\theta) > 0$ (the support) does not depend on the parameter $\\theta$.\n\nThis condition is necessary to satisfy the identity that allows differentiation under the integral sign:\n\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx\n$$\n\n:::\n\n### Why Regularity Matters {.unnumbered}\n\nIf a family is regular, we can differentiate the identity $\\int f(x|\\theta) dx = 1$ with respect to $\\theta$. This operation yields the fundamental properties of the **Score Function** $\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)$:\n\n1.  **First Moment Identity:** The expected value of the score function is zero.\n\n$$E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = 0$$\n\n2.  **Second Moment Identity:** The Fisher Information (variance of the score) is equal to the negative expected Hessian.\n\n$$E_\\theta \\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_j \\partial \\theta_l} \\right] = -E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_l} \\right]$$\n\n### An Example of Non-regular Distribution  {.unnumbered}\nThe **Uniform Distribution** $U(0, \\theta)$ is a classic example of a **non-regular** family. Because the support $(0, \\theta)$ depends on $\\theta$, the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.\n\n## Moments of Sufficient Statistics of Exponential Families\n\n### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n### Means of Sufficient Statistics (General Case)\nFor a random variable $X$ belonging to an exponential family with density $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\\}$, the moments of the sufficient statistics $\\tau_i(X)$ satisfy the system of equations:\n$$\n\\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} E[\\tau_i(X)] = 0 \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nFor regular families, we can interchange differentiation and integration. Since $\\int f(x|\\theta) dx = 1$, we have:\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = 0 \\implies \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx = 0\n$$\n\nUsing the identity $\\frac{\\partial f}{\\partial \\theta} = f(x|\\theta) \\frac{\\partial \\log f}{\\partial \\theta}$, we derive the fundamental moment property:\n\n$$\nE\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} f(x|\\theta) dx = 0\n$$\n\nFor the exponential family, the log-likelihood is given by:\n\n$$\n\\log f(x|\\theta) = \\log C(\\theta) + \\log h(x) + \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\n$$\n\nTaking the derivative with respect to $\\theta_j$:\n\n$$\n\\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} = \\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} \\tau_i(x)\n$$\n\nTaking expectations and applying the condition $E[\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)] = 0$ yields the theorem statement.\n\n:::\n\n\n### Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n### Natural Parameterization\nSuppose an exponential family is given by:\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nWe define the **natural parameters** $\\eta_i$ as $\\eta_i = \\pi_i(\\theta)$. Let $\\eta = (\\eta_1, \\dots, \\eta_k)$. The density becomes:\n\n$$\nf(x|\\eta) = C^*(\\eta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i \\tau_i(x) \\right\\}\n$$\n\n:::\n\n::: {#def-natural-space}\n\n### Natural Parameter Space\nThe natural parameter space $\\mathcal{H}$ is defined as:\n$$\n\\mathcal{H} = \\{ \\eta = (\\pi_1(\\theta), \\dots, \\pi_k(\\theta)) : \\int h(x) e^{\\sum \\eta_i \\tau_i(x)} dx < \\infty \\}\n$$\n\nwhere the condition ensures $C(\\theta)$ is finite.\n\n:::\n\n:::{#def-curvedexpfam}\n\n### Full vs. Curved Exponential Families\n\nLet $d$ be the dimension of $\\theta$ and $k$ be the dimension of the sufficient statistic vector $\\tau(x)$.\n\n* If $d = k$, we say $f(x|\\theta)$ is a **Full Exponential Family**.\n\n* If $d < k$, we say $f(x|\\theta)$ is a **Curved Exponential Family**.\n\n:::\n\n::: {#exm-natural-normal}\n\n### Natural Parameterization of Normal Distribution\nConsider the full Normal family $N(\\mu, \\sigma^2)$ where both parameters are unknown ($d=2$). We previously identified the sufficient statistics $T_1(x) = \\sum x_i$ and $T_2(x) = \\sum x_i^2$ ($k=2$). Since $d=k$, this is a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are defined by the mapping:\n\n$$\n\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\quad \\eta_2 = -\\frac{1}{2\\sigma^2}\n$$\n\nThe density can be rewritten purely in terms of $\\eta$:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i + \\eta_2 \\sum_{i=1}^n x_i^2 - A(\\eta) \\right\\}\n$$\nwhere the log-partition function $A(\\eta)$ absorbs the normalizing constants.\n\n:::\n\n::: {#exm-natural-gamma}\n\n### Natural Parameterization of Gamma Distribution\nConsider the Gamma family $\\text{Gamma}(\\alpha, \\beta)$ ($d=2$). We identified the sufficient statistics $T_1(x) = \\sum \\log x_i$ and $T_2(x) = \\sum x_i$ ($k=2$). Since $d=k$, this is also a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are derived from the canonical form :\n\n$$\n\\eta_1 = \\alpha - 1, \\quad \\eta_2 = -\\frac{1}{\\beta}\n$$\n\nThe density in natural parameterization is:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n \\log x_i + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n### Curved Exponential Family (Natural Parameterization)\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The density is:\n$$\nf(x|\\theta) \\propto \\exp\\left\\{ -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i \\right\\}\n$$\n\nTo express this in the natural parameterization, we define $\\eta = (\\eta_1, \\eta_2)$ as:\n\n$$\n\\eta_1 = -\\frac{1}{2\\theta^2}, \\quad \\eta_2 = \\frac{1}{\\theta}\n$$\n\nThe density becomes:\n\n$$\nf(x|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i^2 + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\nHowever, the natural parameters $\\eta_1$ and $\\eta_2$ are not independent. They satisfy the constraint:\n\n$$\n\\eta_1 = -\\frac{1}{2} \\eta_2^2\n$$\n\nBecause the parameter space $\\mathcal{H}$ forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a **Curved Exponential Family**.\n\n:::\n\n\n::: {#exm-bernoulli-moments}\n\n### Moments of the Binomial Distribution\n\nConsider $n$ independent coin flips $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$. We wish to find the mean and variance of the sufficient statistic $T = \\sum_{i=1}^n X_i$.\n\n1. **Exponential Family Form**\n\n   The joint probability mass function is:\n\n   $$\n   P(X=x) = \\exp\\left\\{ \\log\\left(\\frac{p}{1-p}\\right) \\sum_{i=1}^n x_i + n \\log(1-p) \\right\\}\n   $$\n\n   We identify the components in canonical form:\n\n   * **Natural Parameter:** $\\theta = \\log\\left(\\frac{p}{1-p}\\right)$. Inverting this gives $p = \\frac{e^\\theta}{1+e^\\theta}$.\n   * **Sufficient Statistic:** $\\tau(x) = \\sum x_i$.\n   * **Log-Partition Function ($-\\log C(\\theta)$):**\n     To identify $\\log C(\\theta)$ (or $A(\\theta)$ in some texts), we look at the term independent of $x$ but dependent on parameters. In the standard form $\\exp(\\theta \\tau(x) - A(\\theta))$, we have:\n\n     $$\n     A(\\theta) = - n \\log(1-p) = - n \\log\\left(\\frac{1}{1+e^\\theta}\\right) = n \\log(1+e^\\theta)\n     $$\n\n2. **Calculating the Mean**\n\n   Using the theorem $E[\\tau(X)] = \\frac{\\partial A(\\theta)}{\\partial \\theta}$:\n\n   $$\n   E\\left[\\sum X_i\\right] = \\frac{\\partial}{\\partial \\theta} \\left( n \\log(1+e^\\theta) \\right) = n \\frac{e^\\theta}{1+e^\\theta}\n   $$\n\n   Substituting back $p = \\frac{e^\\theta}{1+e^\\theta}$, we recover the standard expectation:\n\n   $$\n   E[T] = np\n   $$\n\n3. **Calculating the Variance**\n\n   Using the theorem $\\text{Var}(\\tau(X)) = \\frac{\\partial^2 A(\\theta)}{\\partial \\theta^2}$:\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial}{\\partial \\theta} \\left( n \\frac{e^\\theta}{1+e^\\theta} \\right)\n   $$\n\n   Using the quotient rule or recognizing the derivative of the sigmoid function:\n\n   $$\n   \\text{Var}(T) = n \\left( \\frac{e^\\theta (1+e^\\theta) - e^\\theta (e^\\theta)}{(1+e^\\theta)^2} \\right) = n \\frac{e^\\theta}{(1+e^\\theta)^2}\n   $$\n\n   We can rewrite this as $n \\frac{e^\\theta}{1+e^\\theta} \\frac{1}{1+e^\\theta} = n p (1-p)$.\n\n   $$\n   \\text{Var}(T) = np(1-p)\n   $$\n\n:::\n\n::: {#exm-exponential-moments}\n\n### Moments of the Gamma Sufficient Statistic\n\nConsider $n$ independent variables $X_1, \\dots, X_n \\sim \\text{Exp}(\\lambda)$. We want to find the mean and variance of $T = \\sum_{i=1}^n X_i$.\n\n1. **Exponential Family Form**\n\n   The joint density is:\n\n   $$\n   f(x) = \\lambda^n \\exp\\left\\{ -\\lambda \\sum_{i=1}^n x_i \\right\\} = \\exp\\left\\{ -\\lambda \\sum x_i + n \\log \\lambda \\right\\}\n   $$\n\n   * **Natural Parameter:** $\\theta = -\\lambda$ (so $\\lambda = -\\theta$).\n   * **Sufficient Statistic:** $\\tau(x) = \\sum x_i$.\n   * **Log-Partition Function:**\n     Identify the normalization term $A(\\theta)$. Note that usually $\\log f = \\theta \\tau - A(\\theta)$.\n\n     $$\n     A(\\theta) = - n \\log \\lambda = - n \\log(-\\theta)\n     $$\n\n2. **Calculating the Mean**\n\n   $$\n   E[T] = \\frac{\\partial A}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -n \\log(-\\theta) \\right)\n   $$\n\n   Using the chain rule:\n\n   $$\n   E[T] = -n \\frac{1}{-\\theta} (-1) = - \\frac{n}{\\theta}\n   $$\n\n   Substituting $\\theta = -\\lambda$:\n\n   $$\n   E[T] = \\frac{n}{\\lambda}\n   $$\n\n3. **Calculating the Variance**\n\n   $$\n   \\text{Var}(T) = \\frac{\\partial^2 A}{\\partial \\theta^2} = \\frac{\\partial}{\\partial \\theta} \\left( -n \\theta^{-1} \\right)\n   $$\n\n   $$\n   \\text{Var}(T) = -n (-1) \\theta^{-2} = \\frac{n}{\\theta^2}\n   $$\n\n   Substituting $\\theta = -\\lambda$:\n\n   $$\n   \\text{Var}(T) = \\frac{n}{(-\\lambda)^2} = \\frac{n}{\\lambda^2}\n   $$\n\n   These match the mean and variance of a Gamma$(n, \\lambda)$ distribution.\n\n:::\n\n::: {#exm-normal-sufficient-moments}\n\n### Moments of Normal Sufficient Statistics\n\nConsider $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$. We determine the expectation and covariance of the natural sufficient statistics $T_1 = \\sum X_i$ and $T_2 = \\sum X_i^2$.\n\n1. **Canonical Form and Partition Function**\n\n   The joint density is:\n\n   $$\n   f(x) \\propto \\exp\\left\\{ \\frac{\\mu}{\\sigma^2} \\sum x_i - \\frac{1}{2\\sigma^2} \\sum x_i^2 - \\left( \\frac{n\\mu^2}{2\\sigma^2} + n\\log\\sigma \\right) \\right\\}\n   $$\n\n   * **Natural Parameters:**\n     $\\theta_1 = \\frac{\\mu}{\\sigma^2}$\n     $\\theta_2 = -\\frac{1}{2\\sigma^2}$\n\n   * **Parameter Mapping:**\n     $\\sigma^2 = -\\frac{1}{2\\theta_2}$\n     $\\mu = \\theta_1 \\sigma^2 = -\\frac{\\theta_1}{2\\theta_2}$\n\n   * **Log-Partition Function ($A(\\theta)$):**\n     Substituting the inverse mappings into the normalization constant $\\frac{n\\mu^2}{2\\sigma^2} + \\frac{n}{2}\\log(\\sigma^2)$:\n\n     $$\n     A(\\theta) = \\frac{n (-\\theta_1/2\\theta_2)^2}{2 (-1/2\\theta_2)} + \\frac{n}{2} \\log\\left(-\\frac{1}{2\\theta_2}\\right)\n     $$\n\n     Simplifying:\n\n     $$\n     A(\\theta) = -\\frac{n \\theta_1^2}{4 \\theta_2} - \\frac{n}{2} \\log(-2\\theta_2)\n     $$\n\n2. **First Moment (Means)**\n\n   * **Mean of $\\sum X_i$:**\n\n     $$\n     E[T_1] = \\frac{\\partial A}{\\partial \\theta_1} = -\\frac{2n\\theta_1}{4\\theta_2} = -\\frac{n\\theta_1}{2\\theta_2} = n\\mu\n     $$\n\n   * **Mean of $\\sum X_i^2$:**\n\n     $$\n     E[T_2] = \\frac{\\partial A}{\\partial \\theta_2} = -\\frac{n\\theta_1^2}{4} (- \\theta_2^{-2}) - \\frac{n}{2} \\frac{1}{-2\\theta_2}(-2) = \\frac{n\\theta_1^2}{4\\theta_2^2} - \\frac{n}{2\\theta_2}\n     $$\n\n     Substituting back:\n\n     $$\n     E[T_2] = n \\left( \\frac{\\mu}{\\sigma^2} \\sigma^2 \\right)^2 \\frac{1}{(-1/2\\sigma^2)^2 (-1/2\\sigma^2)^{-2}} \\dots \\text{ (Simpler to use } \\mu, \\sigma)\n     $$\n\n     Using the mapping terms identified in $E[T_1]$: $\\frac{\\theta_1}{-2\\theta_2} = \\mu$ and $\\frac{-1}{2\\theta_2} = \\sigma^2$.\n\n     $$\n     E[T_2] = n \\mu^2 + n \\sigma^2\n     $$\n\n3. **Second Moment (Covariance)**\n\n   We find the covariance between $T_1 = \\sum X$ and $T_2 = \\sum X^2$ by taking the cross derivative $\\frac{\\partial^2 A}{\\partial \\theta_1 \\partial \\theta_2}$.\n\n   $$\n   \\text{Cov}(T_1, T_2) = \\frac{\\partial}{\\partial \\theta_2} \\left( E[T_1] \\right) = \\frac{\\partial}{\\partial \\theta_2} \\left( -\\frac{n\\theta_1}{2} \\theta_2^{-1} \\right)\n   $$\n\n   $$\n   = -\\frac{n\\theta_1}{2} (-1)\\theta_2^{-2} = \\frac{n\\theta_1}{2\\theta_2^2}\n   $$\n\n   Substitute parameters back ($\\theta_1 = \\mu/\\sigma^2, \\theta_2 = -1/2\\sigma^2$):\n\n   $$\n   \\text{Cov}(\\sum X, \\sum X^2) = \\frac{n (\\mu/\\sigma^2)}{2 (-1/2\\sigma^2)^2} = \\frac{n\\mu}{\\sigma^2} \\cdot \\frac{1}{2 (1/4\\sigma^4)} = \\frac{n\\mu}{\\sigma^2} \\cdot 2\\sigma^4 = 2n\\mu\\sigma^2\n   $$\n\n:::\n\n::: {#exm-variance-expectation}\n\n### Expectation of Sample Variance via Natural Moments\n\nWe can calculate the expectation of the sample variance $S^2 = \\frac{1}{n-1}\\sum (X_i - \\bar{X})^2$ without integrating, by using the moments of the natural sufficient statistics derived in @exm-normal-sufficient-moments.\n\n1. **Decomposition of $S^2$**\n\n   Standard algebraic expansion of the variance term gives:\n\n   $$\n   S^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^n X_i^2 - n \\bar{X}^2 \\right) = \\frac{1}{n-1} \\left( \\sum X_i^2 - \\frac{1}{n} (\\sum X_i)^2 \\right)\n   $$\n\n   Notice that $S^2$ is a function of the natural sufficient statistics $T_2 = \\sum X_i^2$ and $T_1 = \\sum X_i$.\n\n2. **Expectation Calculation**\n\n   We apply the linearity of expectation:\n\n   $$\n   E[S^2] = \\frac{1}{n-1} \\left( E[T_2] - \\frac{1}{n} E[T_1^2] \\right)\n   $$\n\n   From the previous example, we derived:\n\n   * $E[T_2] = n(\\mu^2 + \\sigma^2)$\n   * $E[T_1] = n\\mu$\n\n   However, we need $E[T_1^2]$. Recall that $\\text{Var}(T_1) = E[T_1^2] - (E[T_1])^2$.\n   Using the Hessian property $\\text{Var}(T_1) = \\frac{\\partial^2 A}{\\partial \\theta_1^2}$:\n\n   $$\n   \\text{Var}(T_1) = \\frac{\\partial}{\\partial \\theta_1} (n \\mu) = \\frac{\\partial}{\\partial \\theta_1} \\left( -\\frac{n \\theta_1}{2 \\theta_2} \\right) = -\\frac{n}{2\\theta_2} = n\\sigma^2\n   $$\n\n   Therefore:\n\n   $$\n   E[T_1^2] = \\text{Var}(T_1) + (E[T_1])^2 = n\\sigma^2 + (n\\mu)^2\n   $$\n\n3. **Result**\n\n   Substitute $E[T_2]$ and $E[T_1^2]$ back into the expression for $E[S^2]$:\n\n   $$\n   \\begin{aligned}\n   E[S^2] &= \\frac{1}{n-1} \\left[ (n\\mu^2 + n\\sigma^2) - \\frac{1}{n} (n\\sigma^2 + n^2\\mu^2) \\right] \\\\\n   &= \\frac{1}{n-1} \\left[ n\\mu^2 + n\\sigma^2 - \\sigma^2 - n\\mu^2 \\right] \\\\\n   &= \\frac{1}{n-1} \\left[ n\\sigma^2 - \\sigma^2 \\right] \\\\\n   &= \\frac{1}{n-1} \\sigma^2 (n-1) \\\\\n   &= \\sigma^2\n   \\end{aligned}\n   $$\n\n   This confirms $S^2$ is an unbiased estimator, derived entirely from the derivatives of the log-partition function.\n\n:::\n\n### Conditional Distributions\n\n::: {#thm-conditional-exp-family}\n\n### Conditional Distributions in Exponential Families\n\nConsider a standard exponential family with two natural parameters $\\theta_1, \\theta_2$ and sufficient statistics $T_1, T_2$:\n$$\nf(t_1, t_2 | \\theta_1, \\theta_2) = \\exp\\left\\{ \\theta_1 t_1 + \\theta_2 t_2 - A(\\theta_1, \\theta_2) \\right\\} h(t_1, t_2)\n$$\n\nThe conditional distribution of $T_2$ given $T_1 = t_1$ has the density:\n$$\nf(t_2 | t_1; \\theta) = \\frac{f(t_1, t_2)}{f_{T_1}(t_1)}\n$$\n\nSubstituting the joint density structure:\n$$\nf(t_2 | t_1; \\theta) \\propto \\frac{\\exp\\{ \\theta_1 t_1 + \\theta_2 t_2 \\} h(t_1, t_2)}{ \\int \\exp\\{ \\theta_1 t_1 + \\theta_2 t_2 \\} h(t_1, t_2) dt_2 }\n$$\n\nNotice that the term $\\exp(\\theta_1 t_1)$ factors out of the integral in the denominator and cancels with the numerator. The term $A(\\theta)$ also cancels out.\n\n**Result:**\n$$\nf(t_2 | t_1; \\theta_2) = C(t_1, \\theta_2) \\exp\\{ \\theta_2 t_2 \\} h(t_1, t_2)\n$$\nCrucially, **this conditional distribution does not depend on $\\theta_1$**. \n\nThis property allows us to eliminate nuisance parameters ($\\theta_1$) by conditioning on their sufficient statistics ($T_1$).\n\n:::\n\n::: {#exm-poisson-conditional-binomial}\n\n### Deriving the Binomial from Poisson Components\n\nConsider two independent Poisson variables $X \\sim \\text{Poisson}(\\lambda_1)$ and $Y \\sim \\text{Poisson}(\\lambda_2)$. We define the sufficient statistics as the sum $T_1 = X + Y$ and the first component $T_2 = X$.\n\n1. **Joint Distribution as Exponential Family**\n\n   The joint probability mass function is:\n   $$\n   \\begin{aligned}\n   P(X=x, Y=y) &= \\frac{e^{-\\lambda_1}\\lambda_1^x}{x!} \\frac{e^{-\\lambda_2}\\lambda_2^y}{y!} \\\\\n   &= \\exp\\left\\{ x \\log \\lambda_1 + y \\log \\lambda_2 - (\\lambda_1 + \\lambda_2) \\right\\} \\frac{1}{x! y!}\n   \\end{aligned}\n   $$\n\n   We express this in terms of $T_1 = x+y$ (so $y = t_1 - t_2$) and $T_2 = x$:\n\n   $$\n   \\begin{aligned}\n   P(T_1=t_1, T_2=t_2) &= \\exp\\left\\{ t_2 \\log \\lambda_1 + (t_1 - t_2) \\log \\lambda_2 - (\\lambda_1 + \\lambda_2) \\right\\} \\frac{1}{t_2! (t_1-t_2)!} \\\\\n   &= \\exp\\left\\{ t_1 \\log \\lambda_2 + t_2 (\\log \\lambda_1 - \\log \\lambda_2) - (\\lambda_1 + \\lambda_2) \\right\\} \\binom{t_1}{t_2} \\frac{1}{t_1!}\n   \\end{aligned}\n   $$\n\n   We identify the natural parameters:\n   * Parameter for fixed $T_1$: $\\theta_1 = \\log \\lambda_2$ (Nuisance)\n   * Parameter for $T_2$: $\\theta_2 = \\log(\\lambda_1/\\lambda_2)$ (Interest)\n\n2. **Applying the Conditional Property**\n\n   According to @thm-conditional-exp-family, the conditional distribution of $T_2$ (the count $X$) given $T_1$ (the total $X+Y$) must be an exponential family depending *only* on $\\theta_2 = \\log(\\lambda_1/\\lambda_2)$.\n\n   $$\n   P(T_2 = t_2 | T_1 = t_1) \\propto \\binom{t_1}{t_2} \\exp\\left\\{ t_2 \\log\\left(\\frac{\\lambda_1}{\\lambda_2}\\right) \\right\\}\n   $$\n\n   Let $p = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}$. Then $\\frac{p}{1-p} = \\frac{\\lambda_1}{\\lambda_2}$. The term $\\exp\\{ t_2 \\theta_2 \\}$ becomes $(\\frac{p}{1-p})^{t_2}$.\n\n   $$\n   P(T_2 = k | T_1 = n) \\propto \\binom{n}{k} \\left( \\frac{p}{1-p} \\right)^k\n   $$\n\n   This is the kernel of a **Binomial$(n, p)$** distribution.\n\n   **Conclusion:**\n   Conditioning on the total count $n$, the distribution of the first count $X$ is Binomial with probability parameter $p = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}$. Note that the absolute magnitude of the rates ($\\lambda_1, \\lambda_2$) has disappeared; only their ratio remains.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}