{
  "hash": "639d946d0ca8849d7a78807016108e6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Families\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Exponential Family\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) $f(x|\\theta)$ is said to be an **Exponential Family** if it can be written in the form:\n\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the parameter vector.\n\n* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.\n\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{-x_i/\\theta} = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $C(\\theta) = \\theta^{-n}$\n\n* $h(\\mathbf{x}) = 1$\n\n* $\\pi_1(\\theta) = -\\frac{1}{\\theta}$\n\n* $\\tau_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$.\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$.\n\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}\n$$\n\n$$\n= [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nRewriting in the canonical form:\n\n$$\n= [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere:\n\n* $\\pi_1(\\theta) = \\alpha - 1$, $\\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2(\\theta) = -\\frac{1}{\\beta}$, $\\tau_2(\\mathbf{x}) = \\sum x_i$.\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}\n$$\n\n$$\n= [B(a, b)]^{-n} \\exp\\left\\{ (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) \\right\\}\n$$\n\nThis is an exponential family with $k=2$.\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$ with $\\theta = (\\mu, \\sigma^2)$.\n\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{n\\mu^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= \\left[ (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} e^{-\\frac{n\\mu^2}{2\\sigma^2}} \\right] \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere $d=2$ and $k=2$.\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n\n$$\nf(x|\\theta) = \\frac{1}{\\theta} I(0 < x < \\theta)\n$$\n\nThis cannot be written in the required form because the indicator function $I(0 < x < \\theta)$ cannot be factorized into separate functions of $x$ and $\\theta$ inside an exponential.\n\n:::\n\n::: {#exm-cauchy-dist}\n\n### Cauchy Distribution\nLet $X \\sim \\text{Cauchy}(\\theta)$.\n\n$$\nf(x|\\theta) = \\frac{1}{\\pi [1 + (x-\\theta)^2]}\n$$\n\nThis involves $\\log(1 + (x-\\theta)^2)$ in the exponent, which cannot be separated into sums of products $\\pi_i(\\theta)\\tau_i(x)$.\n\n:::\n\n## Properties of Exponential Families\n\nFor regular families, we can interchange differentiation and integration. Since $\\int f(x|\\theta) dx = 1$, we have:\n\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = 0 \\implies \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx = 0\n$$\n\nUsing the identity $\\frac{\\partial f}{\\partial \\theta} = f(x|\\theta) \\frac{\\partial \\log f}{\\partial \\theta}$, we derive the fundamental moment property:\n\n$$\nE\\left[ \\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta) \\right] = 0\n$$\n\nDifferentiating a second time yields:\n\n$$\nE\\left[ \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log f(X|\\theta) \\right] = - E\\left[ \\frac{\\partial \\log f}{\\partial \\theta_i} \\frac{\\partial \\log f}{\\partial \\theta_j} \\right]\n$$\n\nFor the exponential family $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum \\pi_i(\\theta) \\tau_i(x)\\}$, the log-likelihood is:\n\n$$\n\\log f(x|\\theta) = \\log C(\\theta) + \\log h(x) + \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\n$$\n\nTaking the expectation of the derivative with respect to $\\theta_j$ and setting it to zero gives:\n\n$$\n\\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} E[\\tau_i(X)] = 0\n$$\n\nThis system of equations allows us to find the moments $E[\\tau_i(X)]$.\n\n## Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n### Natural Parameterization\nSuppose an exponential family is given by:\n\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nWe define the **natural parameters** $\\eta_i$ as $\\eta_i = \\pi_i(\\theta)$. Let $\\eta = (\\eta_1, \\dots, \\eta_k)$. The density becomes:\n\n$$\nf(x|\\eta) = C^*(\\eta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i \\tau_i(x) \\right\\}\n$$\n\n:::\n\n::: {#def-natural-space}\n\n### Natural Parameter Space\nThe natural parameter space $\\mathcal{H}$ is defined as:\n\n$$\n\\mathcal{H} = \\{ \\eta = (\\pi_1(\\theta), \\dots, \\pi_k(\\theta)) : \\int h(x) e^{\\sum \\eta_i \\tau_i(x)} dx < \\infty \\}\n$$\n\nwhere the condition ensures $C(\\theta)$ is finite.\n\n:::\n\n### Full vs. Curved Exponential Families\n\nLet $d$ be the dimension of $\\theta$ and $k$ be the dimension of the sufficient statistic vector $\\tau(x)$.\n\n* If $d = k$, we say $f(x|\\theta)$ is a **Full Exponential Family**.\n\n* If $d < k$, we say $f(x|\\theta)$ is a **Curved Exponential Family**.\n\n::: {#exm-curved-normal}\n\n### Curved Normal Distribution\nConsider $N(\\theta, \\theta^2)$ where $\\mu = \\theta$ and $\\sigma^2 = \\theta^2$. This implies a relationship between the parameters.\n\n$$\nf(x|\\theta) \\propto \\exp\\left\\{ -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i \\right\\}\n$$\n\nHere we have two sufficient statistics ($\\sum x_i^2, \\sum x_i$) but only one parameter $\\theta$ ($d=1, k=2$). The natural parameters are $\\eta_1 = -1/(2\\theta^2)$ and $\\eta_2 = 1/\\theta$, which satisfies $\\eta_1 = -\\eta_2^2 / 2$. This constraint forms a curve in the parameter space.\n\n:::\n\n### Moments in Natural Parameterization\n\nFor a natural exponential family, differentiating the log-partition function generates cumulants:\n\n$$\nE[\\tau_j(X)] = - \\frac{\\partial}{\\partial \\eta_j} \\log C^*(\\eta)\n$$\n\n$$\n\\text{Cov}(\\tau_i(X), \\tau_j(X)) = - \\frac{\\partial^2}{\\partial \\eta_i \\partial \\eta_j} \\log C^*(\\eta)\n$$\n\n## Lemmas on Sufficient Statistics\n\n::: {#lem-joint-distribution}\n\n### Joint Distribution of Sufficient Statistics\nIf $X$ has a distribution in the exponential family $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum \\pi_i(\\theta) \\tau_i(x)\\}$, then the joint distribution of the sufficient statistics $T = (\\tau_1(X), \\dots, \\tau_k(X))$ is also in the exponential family with the same natural parameters.\n\n:::\n\n::: {.proof}\nLet $X$ be discrete. The probability mass function of $T$ is:\n\n$$\nP(T_1 = y_1, \\dots, T_k = y_k | \\theta) = \\sum_{\\{x : \\tau(x) = y\\}} P(X=x|\\theta)\n$$\n\n$$\n= \\sum_{\\{x : \\tau(x) = y\\}} C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nSince the exponential term and $C(\\theta)$ depend only on $y$ and $\\theta$, they can be pulled out of the sum:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\} \\left( \\sum_{\\{x : \\tau(x) = y\\}} h(x) \\right)\n$$\n\nDefining $h^*(y) = \\sum_{\\{x : \\tau(x) = y\\}} h(x)$, we get:\n\n$$\nf_T(y|\\theta) = C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nwhich is of the exponential family form .\n\n:::\n\n::: {#lem-marginal-distribution}\n\n### Marginal Distribution\nLet $S$ be a subset of indices $\\{1, \\dots, k\\}$. If $\\pi_i(\\theta)$ are constant for all $i \\notin S$, then the marginal distribution of the statistics $T_S = \\{ \\tau_j(X) : j \\in S \\}$ is of the exponential family form with natural parameters $\\pi_j(\\theta)$ for $j \\in S$.\n\n:::\n\n::: {.proof}\nLet $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):\n\n$$\nP(T_S = y_S | \\theta) = \\sum_{y_{S^c}} C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j + \\sum_{l \\in S^c} \\pi_l(\\theta) y_l \\right\\}\n$$\n\nSince $\\pi_l(\\theta)$ is constant for $l \\in S^c$, the term $\\exp\\{\\sum_{l \\in S^c} \\pi_l y_l\\}$ does not depend on $\\theta$. We can group it with $h^*(y)$:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j \\right\\} \\sum_{y_{S^c}} h^*(y) \\exp\\left\\{ \\sum_{l \\in S^c} \\pi_l y_l \\right\\}\n$$\n\nThe sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form .\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}