{
  "hash": "bae56867aad6d429a8a9236187d72480",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Families\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n## Exponential Families\n\n::: {#def-exponential-family}\n\n### Exponential Family\nA family of probability density functions (or probability mass functions) $f(x|\\theta)$ is said to be an **Exponential Family** if it can be written in the form:\n\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nwhere:\n\n* $\\theta = (\\theta_1, \\dots, \\theta_d)$ is the parameter vector.\n\n* $k$ is the number of terms in the exponent. Note that $d$ may be less than $k$.\n\n* By the Factorization Theorem, the vector $T(x) = (\\tau_1(x), \\dots, \\tau_k(x))$ constitutes a **sufficient statistic** for $\\theta$.\n\n:::\n\n### Examples of Exponential Families\n\n::: {#exm-exponential-dist}\n\n### Exponential Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)$, where $\\theta$ is the scale parameter.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{-x_i/\\theta} = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $C(\\theta) = \\theta^{-n}$\n\n* $h(\\mathbf{x}) = 1$\n\n* $\\pi_1(\\theta) = -\\frac{1}{\\theta}$\n\n* $\\tau_1(\\mathbf{x}) = \\sum_{i=1}^n x_i$.\n\n:::\n\n::: {#exm-gamma-dist}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}\n$$\n\n$$\n= [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nRewriting in the canonical form:\n\n$$\n= [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere:\n\n* $\\pi_1(\\theta) = \\alpha - 1$, $\\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2(\\theta) = -\\frac{1}{\\beta}$, $\\tau_2(\\mathbf{x}) = \\sum x_i$.\n\n:::\n\n::: {#exm-beta-dist}\n\n### Beta Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)$ with $\\theta = (a, b)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}\n$$\n\n$$\n= [B(a, b)]^{-n} \\exp\\left\\{ (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) \\right\\}\n$$\n\nThis is an exponential family with $k=2$.\n\n:::\n\n::: {#exm-normal-dist}\n\n### Normal Distribution\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$ with $\\theta = (\\mu, \\sigma^2)$.\n$$\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{n\\mu^2}{2\\sigma^2} \\right\\}\n$$\n\n$$\n= \\left[ (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} e^{-\\frac{n\\mu^2}{2\\sigma^2}} \\right] \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere $d=2$ and $k=2$.\n\n:::\n\n### Examples of Non-exponential Families\n\nA model is **not** in the exponential family if the support depends on the parameter.\n\n::: {#exm-uniform-dist}\n\n### Uniform Distribution\nLet $X \\sim U(0, \\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\theta} I(0 < x < \\theta)\n$$\n\nThis cannot be written in the required form because the indicator function $I(0 < x < \\theta)$ cannot be factorized into separate functions of $x$ and $\\theta$ inside an exponential.\n\n:::\n\n::: {#exm-cauchy-dist}\n\n### Cauchy Distribution\nLet $X \\sim \\text{Cauchy}(\\theta)$.\n$$\nf(x|\\theta) = \\frac{1}{\\pi [1 + (x-\\theta)^2]}\n$$\n\nThis involves $\\log(1 + (x-\\theta)^2)$ in the exponent, which cannot be separated into sums of products $\\pi_i(\\theta)\\tau_i(x)$.\n\n:::\n\n## Regular Families\n\nIn the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.\n\n::: {#def-regular-family}\n\n### Regular Family\nA family of probability density functions $f(x|\\theta)$ is said to be a **Regular Family** (or \"sufficiently well-behaved\") if the domain of $x$ for which $f(x|\\theta) > 0$ (the support) does not depend on the parameter $\\theta$.\n\nThis condition is necessary to satisfy the identity that allows differentiation under the integral sign:\n\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx\n$$\n\n:::\n\n### Why Regularity Matters {.unnumbered}\n\nIf a family is regular, we can differentiate the identity $\\int f(x|\\theta) dx = 1$ with respect to $\\theta$. This operation yields the fundamental properties of the **Score Function** $\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)$:\n\n1.  **First Moment Identity:** The expected value of the score function is zero.\n\n$$E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = 0$$\n\n2.  **Second Moment Identity:** The Fisher Information (variance of the score) is equal to the negative expected Hessian.\n\n$$E_\\theta \\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_j \\partial \\theta_l} \\right] = -E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_l} \\right]$$\n\n### An Example of Non-regular Distribution  {.unnumbered}\nThe **Uniform Distribution** $U(0, \\theta)$ is a classic example of a **non-regular** family. Because the support $(0, \\theta)$ depends on $\\theta$, the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.\n\n## Moments of Sufficient Statistics of Exponential Families\n\n### Means of Sufficient Statistics (General Case)\n\n::: {#thm-moments-exp-family}\n\n### Means of Sufficient Statistics (General Case)\nFor a random variable $X$ belonging to an exponential family with density $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\\}$, the moments of the sufficient statistics $\\tau_i(X)$ satisfy the system of equations:\n$$\n\\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} E[\\tau_i(X)] = 0 \\quad \\text{for } j=1, \\dots, d\n$$\n\n:::\n\n::: {.proof}\nFor regular families, we can interchange differentiation and integration. Since $\\int f(x|\\theta) dx = 1$, we have:\n$$\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = 0 \\implies \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx = 0\n$$\n\nUsing the identity $\\frac{\\partial f}{\\partial \\theta} = f(x|\\theta) \\frac{\\partial \\log f}{\\partial \\theta}$, we derive the fundamental moment property:\n\n$$\nE\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} f(x|\\theta) dx = 0\n$$\n\nFor the exponential family, the log-likelihood is given by:\n\n$$\n\\log f(x|\\theta) = \\log C(\\theta) + \\log h(x) + \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\n$$\n\nTaking the derivative with respect to $\\theta_j$:\n\n$$\n\\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} = \\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} \\tau_i(x)\n$$\n\nTaking expectations and applying the condition $E[\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)] = 0$ yields the theorem statement.\n\n:::\n\n::: {#exm-normal-moments}\n\n### Moments of Normal Sufficient Statistics\nConsider the Normal distribution $N(\\mu, \\sigma^2)$ where $\\theta = (\\mu, \\sigma^2)$. The density is:\n$$\nf(\\mathbf{x}|\\theta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify the components :\n\n* $C(\\mu, \\sigma^2) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)$\n\n* $\\pi_1 = -\\frac{1}{2\\sigma^2}, \\quad \\tau_1(\\mathbf{x}) = \\sum x_i^2$\n\n* $\\pi_2 = \\frac{\\mu}{\\sigma^2}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i$\n\nWe apply the theorem by differentiating with respect to $\\mu$ and $\\sigma^2$.\n\n1. Differentiate with respect to $\\mu$:\n\n   $$\n      \\frac{\\partial \\log C}{\\partial \\mu} = -\\frac{n\\mu}{\\sigma^2}\n   $$\n\n      \n\n   $$\n      \\frac{\\partial \\pi_1}{\\partial \\mu} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\mu} = \\frac{1}{\\sigma^2}\n   $$\n\n      The theorem equation becomes:\n\n   $$\n      -\\frac{n\\mu}{\\sigma^2} + 0 \\cdot E[\\sum X_i^2] + \\frac{1}{\\sigma^2} E[\\sum X_i] = 0\n   $$\n\n      \n\n\n   $$\n      \\implies E[\\sum_{i=1}^n X_i] = n\\mu\n   $$\n\n1. Differentiate with respect to $\\sigma^2$:\n\n   $$\n      \\frac{\\partial \\log C}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2}\n   $$\n\n      \n\n   \n\n   $$\n      \\frac{\\partial \\pi_1}{\\partial \\sigma^2} = \\frac{1}{2(\\sigma^2)^2}, \\quad \\frac{\\partial \\pi_2}{\\partial \\sigma^2} = -\\frac{\\mu}{(\\sigma^2)^2}\n   $$\n\n      The theorem equation becomes:\n\n   $$\n      \\left( -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2} \\right) + \\frac{1}{2(\\sigma^2)^2} E[\\sum X_i^2] - \\frac{\\mu}{(\\sigma^2)^2} E[\\sum X_i] = 0\n   $$\n\n      Multiplying by $2(\\sigma^2)^2$:\n\n   $$\n      -n\\sigma^2 + n\\mu^2 + E[\\sum X_i^2] - 2\\mu(n\\mu) = 0\n   $$\n\n      \n\n\n   $$\n      E[\\sum X_i^2] = n\\sigma^2 + n\\mu^2\n   $$\n\n   This recovers the standard second moment $E[X^2] = \\sigma^2 + \\mu^2$.\n\n:::\n\n::: {#exm-gamma-moments}\n\n### Moments of Gamma Sufficient Statistics\nConsider the Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$ with $\\theta = (\\alpha, \\beta)$. The density is :\n$$\nf(\\mathbf{x}|\\theta) = [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n$$\n\nHere we identify:\n\n* $\\log C(\\alpha, \\beta) = -n \\log \\Gamma(\\alpha) - n\\alpha \\log \\beta$\n\n* $\\pi_1 = \\alpha - 1, \\quad \\tau_1(\\mathbf{x}) = \\sum \\log x_i$\n\n* $\\pi_2 = -\\frac{1}{\\beta}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i$\n\n1. Differentiate with respect to $\\beta$:\n\n   $$\n      \\frac{\\partial \\log C}{\\partial \\beta} = -\\frac{n\\alpha}{\\beta}\n   $$\n\n\n   $$\n      \\frac{\\partial \\pi_1}{\\partial \\beta} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\beta} = \\frac{1}{\\beta^2}\n   $$\n\n      The theorem yields:\n\n   $$\n      -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2} E[\\sum X_i] = 0 \\implies E[\\sum X_i] = n\\alpha\\beta\n   $$\n\n1. Differentiate with respect to $\\alpha$:\n\n   $$\n      \\frac{\\partial \\log C}{\\partial \\alpha} = -n \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} - n \\log \\beta = -n \\psi(\\alpha) - n \\log \\beta\n   $$\n\n      where $\\psi(\\alpha)$ is the digamma function.\n\n   $$\n      \\frac{\\partial \\pi_1}{\\partial \\alpha} = 1, \\quad \\frac{\\partial \\pi_2}{\\partial \\alpha} = 0\n   $$\n\n      The theorem yields:\n\n   $$\n      (-n \\psi(\\alpha) - n \\log \\beta) + 1 \\cdot E[\\sum \\log X_i] = 0\n   $$\n\n   $$\n   \\implies E[\\sum_{i=1}^n \\log X_i] = n(\\psi(\\alpha) + \\log \\beta)\n   $$\n\n:::\n\n\n### Natural Parameterization\n\n::: {#def-natural-parameterization}\n\n### Natural Parameterization\nSuppose an exponential family is given by:\n$$\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n$$\n\nWe define the **natural parameters** $\\eta_i$ as $\\eta_i = \\pi_i(\\theta)$. Let $\\eta = (\\eta_1, \\dots, \\eta_k)$. The density becomes:\n\n$$\nf(x|\\eta) = C^*(\\eta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i \\tau_i(x) \\right\\}\n$$\n\n:::\n\n::: {#def-natural-space}\n\n### Natural Parameter Space\nThe natural parameter space $\\mathcal{H}$ is defined as:\n$$\n\\mathcal{H} = \\{ \\eta = (\\pi_1(\\theta), \\dots, \\pi_k(\\theta)) : \\int h(x) e^{\\sum \\eta_i \\tau_i(x)} dx < \\infty \\}\n$$\n\nwhere the condition ensures $C(\\theta)$ is finite.\n\n:::\n\n:::{#def-curvedexpfam}\n\n### Full vs. Curved Exponential Families\n\nLet $d$ be the dimension of $\\theta$ and $k$ be the dimension of the sufficient statistic vector $\\tau(x)$.\n\n* If $d = k$, we say $f(x|\\theta)$ is a **Full Exponential Family**.\n\n* If $d < k$, we say $f(x|\\theta)$ is a **Curved Exponential Family**.\n\n:::\n\n::: {#exm-natural-normal}\n\n### Natural Parameterization of Normal Distribution\nConsider the full Normal family $N(\\mu, \\sigma^2)$ where both parameters are unknown ($d=2$). We previously identified the sufficient statistics $T_1(x) = \\sum x_i$ and $T_2(x) = \\sum x_i^2$ ($k=2$). Since $d=k$, this is a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are defined by the mapping:\n\n$$\n\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\quad \\eta_2 = -\\frac{1}{2\\sigma^2}\n$$\n\nThe density can be rewritten purely in terms of $\\eta$:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i + \\eta_2 \\sum_{i=1}^n x_i^2 - A(\\eta) \\right\\}\n$$\nwhere the log-partition function $A(\\eta)$ absorbs the normalizing constants.\n\n:::\n\n::: {#exm-natural-gamma}\n\n### Natural Parameterization of Gamma Distribution\nConsider the Gamma family $\\text{Gamma}(\\alpha, \\beta)$ ($d=2$). We identified the sufficient statistics $T_1(x) = \\sum \\log x_i$ and $T_2(x) = \\sum x_i$ ($k=2$). Since $d=k$, this is also a **Full Exponential Family**.\n\nThe natural parameters $\\eta = (\\eta_1, \\eta_2)$ are derived from the canonical form :\n\n$$\n\\eta_1 = \\alpha - 1, \\quad \\eta_2 = -\\frac{1}{\\beta}\n$$\n\nThe density in natural parameterization is:\n\n$$\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n \\log x_i + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\n:::\n\n::: {#exm-curved-normal-natural}\n\n### Curved Exponential Family (Natural Parameterization)\nConsider the $N(\\theta, \\theta^2)$ distribution ($d=1$). The density is:\n$$\nf(x|\\theta) \\propto \\exp\\left\\{ -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i \\right\\}\n$$\n\nTo express this in the natural parameterization, we define $\\eta = (\\eta_1, \\eta_2)$ as:\n\n$$\n\\eta_1 = -\\frac{1}{2\\theta^2}, \\quad \\eta_2 = \\frac{1}{\\theta}\n$$\n\nThe density becomes:\n\n$$\nf(x|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i^2 + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n$$\n\nHowever, the natural parameters $\\eta_1$ and $\\eta_2$ are not independent. They satisfy the constraint:\n\n$$\n\\eta_1 = -\\frac{1}{2} \\eta_2^2\n$$\n\nBecause the parameter space $\\mathcal{H}$ forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a **Curved Exponential Family**.\n\n:::\n\n### Mean and Covariance of  Natural Exponential Families\n\n\n::: {#thm-natural-covariance}\n\n### Mean and Covariance of Natural Exponential Families\nIf the exponential family is in its **canonical form** (natural parameterization) where $\\pi_i(\\theta) = \\theta_i$ for each $i$, then the mean and covariance of the sufficient statistics are given by:\n$$\nE_\\theta[\\tau_i(X)] = - \\frac{\\partial}{\\partial \\theta_i} \\log C(\\theta)\n$$\n\n$$\n\\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X)) = - \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log C(\\theta)\n$$\n\n:::\n\n::: {.proof}\nWe use the second-order regularity condition for the score function:\n$$\nE_\\theta\\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_i \\partial \\theta_j} \\right] = - E_\\theta\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_i} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right]\n$$\n\nIn the case of a natural exponential family, $\\pi_k(\\theta) = \\theta_k$. The derivative of the log-likelihood simplifies to:\n\n$$\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\frac{\\partial \\log C(\\theta)}{\\partial \\theta_i} + \\tau_i(x)\n$$\n\nDifferentiating again with respect to $\\theta_j$:\n\n$$\n\\frac{\\partial^2 \\log f}{\\partial \\theta_i \\partial \\theta_j} = \\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\n$$\n\nSince this second derivative is non-random (it does not depend on $x$), its expectation is simply itself.\n\nNow consider the right-hand side of the regularity condition. From the first moment property, we know $E[\\tau_i(X)] = - \\frac{\\partial \\log C}{\\partial \\theta_i}$. Thus, the score function is:\n\n$$\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\tau_i(X) - E[\\tau_i(X)]\n$$\n\nSubstituting these into the identity:\n\n$$\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - E\\left[ (\\tau_i(X) - E[\\tau_i(X)]) (\\tau_j(X) - E[\\tau_j(X)]) \\right]\n$$\n\n$$\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - \\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X))\n$$\n\nMultiplying by $-1$ gives the result.\n\n:::\n\n\n\n## Distributions of Sufficient Statistics\n\n::: {#lem-joint-distribution}\n\n### Joint Distribution of Sufficient Statistics\nIf $X$ has a distribution in the exponential family $f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum \\pi_i(\\theta) \\tau_i(x)\\}$, then the joint distribution of the sufficient statistics $T = (\\tau_1(X), \\dots, \\tau_k(X))$ is also in the exponential family with the same natural parameters.\n\n:::\n\n::: {.proof}\nLet $X$ be discrete. The probability mass function of $T$ is:\n$$\nP(T_1 = y_1, \\dots, T_k = y_k | \\theta) = \\sum_{\\{x : \\tau(x) = y\\}} P(X=x|\\theta)\n$$\n\n$$\n= \\sum_{\\{x : \\tau(x) = y\\}} C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nSince the exponential term and $C(\\theta)$ depend only on $y$ and $\\theta$, they can be pulled out of the sum:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\} \\left( \\sum_{\\{x : \\tau(x) = y\\}} h(x) \\right)\n$$\n\nDefining $h^*(y) = \\sum_{\\{x : \\tau(x) = y\\}} h(x)$, we get:\n\n$$\nf_T(y|\\theta) = C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n$$\n\nwhich is of the exponential family form .\n\n:::\n\n::: {#lem-marginal-distribution}\n\n### Marginal Distribution Lemma for Exponetial Families (MDL)\nLet $S$ be a subset of indices $\\{1, \\dots, k\\}$. If $\\pi_i(\\theta)$ are constant for all $i \\notin S$, then the marginal distribution of the statistics $T_S = \\{ \\tau_j(X) : j \\in S \\}$ is of the exponential family form with natural parameters $\\pi_j(\\theta)$ for $j \\in S$.\n\n:::\n\n::: {.proof}\nLet $T$ be discrete. We sum over the variables not in $S$ (denoted $T_{S^c}$):\n$$\nP(T_S = y_S | \\theta) = \\sum_{y_{S^c}} C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j + \\sum_{l \\in S^c} \\pi_l(\\theta) y_l \\right\\}\n$$\n\nSince $\\pi_l(\\theta)$ is constant for $l \\in S^c$, the term $\\exp\\{\\sum_{l \\in S^c} \\pi_l y_l\\}$ does not depend on $\\theta$. We can group it with $h^*(y)$:\n\n$$\n= C(\\theta) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j \\right\\} \\sum_{y_{S^c}} h^*(y) \\exp\\left\\{ \\sum_{l \\in S^c} \\pi_l y_l \\right\\}\n$$\n\nThe sum becomes a new base measure $h^{**}(y_S)$, yielding the exponential family form.\n\n:::\n\n::: {#exm-bernoulli-failure}\n\n### Failure of MDL: The Coupled Bernoulli\n\nSuppose we try to be clever and rewrite the Bernoulli joint density treating Heads ($T_1$) and Tails ($T_2$) as distinct sufficient statistics:\n\n$$\nf(x|p) = \\exp\\left\\{ (\\log p) \\sum x_i + (\\log(1-p)) (n - \\sum x_i) \\right\\}\n$$\n\nLet's define separate natural parameters to make it look like a 2-parameter exponential family:\n\n  * $\\eta_1 = \\log p$\n  * $\\eta_2 = \\log (1-p)$\n  * $T_1 = \\sum x_i$ (Heads)\n  * $T_2 = n - \\sum x_i$ (Tails)\n\nThe density looks like:\n$$\nf(t_1, t_2) \\propto h(t_1, t_2) \\exp( \\eta_1 T_1 + \\eta_2 T_2 )\n$$\n\n**Attempting to apply the Lemma:**\nIf we blindly followed the Lemma to find the marginal distribution of Heads ($T_1$), we would say: \"Treat $\\eta_2$ as constant, ignore the $T_2$ part, and focus on $T_1$.\"\n\n$$\n\\text{Hypothetical Marginal}(t_1) \\stackrel{?}{\\propto} h^*(t_1) \\exp( \\eta_1 t_1 ) = \\binom{n}{t_1} p^{t_1}\n$$\n\n**The Result:** $f(t_1) \\propto \\binom{n}{t_1} p^{t_1}$.\nThis is **WRONG**. It is missing the $(1-p)^{n-t_1}$ term. It suggests the probability of heads grows infinitely with $p$ without being penalized by the probability of tails shrinking.\n\n**Why it Failed:**\nThe Marginal Distribution Lemma requires the natural parameters $\\eta_1$ and $\\eta_2$ to be **variationally independent** (the parameter space must contain a rectangle).\n\n1.  **Parameter Constraint:** $\\eta_1$ and $\\eta_2$ are coupled by the constraint $e^{\\eta_1} + e^{\\eta_2} = 1$ (since $p + (1-p) = 1$). You cannot vary $\\eta_1$ (change $p$) while holding $\\eta_2$ constant.\n   2.  **Statistic Constraint:** $T_1$ and $T_2$ are perfectly collinear ($T_1 + T_2 = n$). The support of $(T_1, T_2)$ is a line segment, not a 2D grid. The base measure $h(t_1, t_2)$ does not factor because $h(t_1, t_2)$ is zero everywhere except on that line.\n\n:::\n\n::: {#exm-normal-marginal-comparison}\n\n### Marginal Distributions of Normal Sufficient Statistics\nConsider the Normal model $N(\\mu, \\sigma^2)$ with sufficient statistics $T_1 = \\sum X_i^2$ and $T_2 = \\sum X_i$. The natural parameters are $\\pi_1 = -\\frac{1}{2\\sigma^2}$ and $\\pi_2 = \\frac{\\mu}{\\sigma^2}$.\n\n1. Marginal of $T_2$ (fixing $\\sigma^2$)\n   Suppose $\\sigma^2$ is known (constant).\n\n   * **Lemma Condition:** $\\pi_1 = -1/(2\\sigma^2)$ is constant. The condition holds.\n   * **Implied Form by MDL:** The Marginal Distribution Lemma claims that $T_2$ follows an exponential family with natural parameter $\\pi_2 = \\frac{\\mu}{\\sigma^2}$:\n   $$\n   f_{T_2}(t_2|\\theta) \\propto h^*(t_2) \\exp\\left\\{ \\frac{\\mu}{\\sigma^2} t_2 \\right\\}\n   $$\n\n   * **Comparison with Known Distribution:**\n      We know $T_2 = \\sum X_i \\sim N(n\\mu, n\\sigma^2)$. The density is:\n      $$\n      f(t_2|\\mu) = \\frac{1}{\\sqrt{2\\pi n \\sigma^2}} \\exp\\left\\{ -\\frac{(t_2 - n\\mu)^2}{2n\\sigma^2} \\right\\}\n      $$\n\n      Expanding the square $-\\frac{1}{2n\\sigma^2}(t_2^2 - 2n\\mu t_2 + n^2\\mu^2)$ and regrouping terms:\n\n      $$\n      f(t_2|\\mu) = \\underbrace{ \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} }_{C(\\mu)} \\underbrace{ \\frac{1}{\\sqrt{2\\pi n \\sigma^2}} \\exp\\left\\{ -\\frac{t_2^2}{2n\\sigma^2} \\right\\} }_{h(t_2)} \\exp\\left\\{ \\frac{\\mu}{\\sigma^2} t_2 \\right\\}\n      $$\n      \n      **Result:** The Lemma correctly identifies the form, with the natural parameter $\\frac{\\mu}{\\sigma^2}$.\n\n2. Marginal of $T_1$ (fixing $\\mu$)\n   Suppose $\\mu$ is known (constant). We investigate whether the marginal distribution of $T_1 = \\sum X_i^2$ remains in the exponential family by checking the behavior of the remaining natural parameter $\\pi_2 = \\frac{\\mu}{\\sigma^2}$ with respect to the free parameter $\\sigma^2$.\n\n   **Case A ($\\mu = 0$):**\n\n   In this case, $\\pi_2 = 0$, which is trivially constant with respect to $\\sigma^2$. The condition of the Marginal Distribution Lemma is satisfied.\n\n   * **Implied Form by MDL:** The lemma claims that $T_1$ must follow an exponential family form with natural parameter $\\pi_1 = -\\frac{1}{2\\sigma^2}$:\n   $$\n   f_{T_1}(t_1|\\sigma^2) \\propto h^*(t_1) \\exp\\left\\{ -\\frac{1}{2\\sigma^2} t_1 \\right\\}\n   $$\n\n   * **Verification:** We know that for $\\mu=0$, the scaled statistic $T_1 / \\sigma^2$ follows a central Chi-squared distribution with $n$ degrees of freedom ($\\chi^2_n$). The density is proportional to:\n   $$\n   f(t_1) \\propto t_1^{n/2 - 1} \\exp\\left\\{ -\\frac{t_1}{2\\sigma^2} \\right\\}\n   $$\n   This matches the form claimed by the MDL perfectly, with natural parameter $-1/(2\\sigma^2)$ and base measure $h^*(t_1) = t_1^{n/2-1}$.\n\n   **Case B ($\\mu \\neq 0$):**\n\n   In this case, $\\pi_2 = \\frac{\\mu}{\\sigma^2}$ is a function of $\\sigma^2$. As $\\sigma^2$ changes, $\\pi_2$ changes. The condition of the Lemma \"$\\pi_i(\\theta)$ are constant for all $i \\notin S$\" **fails**. Therefore, the structure claimed by the lemma—a simple exponential family with parameter $\\pi_1$—is not guaranteed.\n\n   * **Verification:** We know that for $\\mu \\neq 0$, the statistic $T_1/\\sigma^2$ follows a **Non-central Chi-squared** distribution $\\chi'^2_n(\\lambda)$ with non-centrality parameter $\\lambda = \\frac{\\sum \\mu^2}{\\sigma^2} = \\frac{n\\mu^2}{\\sigma^2}$.\n\n   * The density of a non-central Chi-squared involves an infinite mixture of central densities:\n     $$\n     f(t_1) = \\sum_{k=0}^\\infty P(K=k) f_{\\chi^2_{n+2k}}(t_1)\n     $$\n     where the weights $P(K=k)$ depend on $\\lambda$ (and thus $\\sigma^2$).\n\n   * **Result:** Because the parameter $\\sigma^2$ appears inside the Poisson weights of the infinite sum, the density **cannot** be factored into the simple form $C(\\sigma^2)h(t_1)\\exp(\\pi_1 t_1)$. This confirms that when the orthogonality condition is violated, the marginal distribution of a sufficient statistic leaves the simple exponential family.\n\n:::\n\n::: {#exm-variance-distribution-mdl}\n\n### Distribution of Sample Variance $S^2$ via Marginal Distribution Lemma\n\nConsider the Normal model $X_i \\sim N(\\mu, \\sigma^2)$. We wish to find the marginal distribution of the sufficient statistic $S^2 = \\frac{1}{n-1}\\sum (X_i - \\bar{X})^2$.\n\n1. **Jacobian of the Transformation**\n   When transforming the joint density from the $n$ data points $(X_1, \\dots, X_n)$ to the sufficient statistics $(\\bar{X}, S^2)$, we must account for the change in volume (the Jacobian). \n   \n   The vector of residuals $(X_1 - \\bar{X}, \\dots, X_n - \\bar{X})$ lies on an $(n-1)$-dimensional sphere with squared radius proportional to $s^2$. The surface area of this sphere scales as $(s^2)^{\\frac{n-1}{2}-1}$. Thus, the volume element transforms as:\n   \n   $$\n   \\prod dx_i \\propto (s^2)^{\\frac{n-3}{2}} d s^2 d\\bar{x}\n   $$\n\n2. **Decomposition of the Joint Density**\n   Incorporating this Jacobian into the exponential family form, the joint density of $(\\bar{X}, S^2)$ is:\n\n   $$\n   \\begin{aligned}\n   f(s^2, \\bar{x}) &\\propto \\underbrace{(s^2)^{\\frac{n-3}{2}}}_{\\text{Jacobian } h(s^2, \\bar{x})} \\cdot \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum x_i^2 + \\frac{\\mu}{\\sigma^2}\\sum x_i \\right\\} \\\\\n   &\\propto (s^2)^{\\frac{n-3}{2}} \\cdot \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\left[ (n-1)s^2 + n\\bar{x}^2 \\right] + \\frac{n\\mu}{\\sigma^2}\\bar{x} \\right\\}\n   \\end{aligned}\n   $$\n\n   Grouping the terms by variable:\n\n   $$\n   f(s^2, \\bar{x}) \\propto \\underbrace{\\left[ (s^2)^{\\frac{n-3}{2}} \\exp\\left\\{ -\\frac{n-1}{2\\sigma^2} s^2 \\right\\} \\right]}_{\\text{Terms involving } s^2} \\cdot \\underbrace{\\left[ \\exp\\left\\{ -\\frac{n}{2\\sigma^2}\\bar{x}^2 + \\frac{n\\mu}{\\sigma^2}\\bar{x} \\right\\} \\right]}_{\\text{Terms involving } \\bar{x}}\n   $$\n\n3. **Marginal Integration**\n   To find the marginal distribution of $S^2$, we integrate out $\\bar{x}$. Since the density factors completely (orthogonality), the integral over $\\bar{x}$ contributes only a multiplicative constant $C(\\mu, \\sigma^2)$ and does not depend on $s^2$.\n   \n   The marginal density is simply the $s^2$ component retained from the joint density:\n\n   $$\n   f_{S^2}(s^2) \\propto (s^2)^{\\frac{n-3}{2}} \\exp\\left\\{ -\\frac{n-1}{2\\sigma^2} s^2 \\right\\}\n   $$\n\n4. **Identification of Degrees of Freedom**\n   We match this result against the kernel of a standard Gamma distribution (or Chi-squared type), $f(y) \\propto y^{k-1} e^{-\\beta y}$.\n   \n   * **Shape Parameter ($k$):**\n       $$\n       k - 1 = \\frac{n-3}{2} \\implies k = \\frac{n-1}{2}\n       $$\n       Since the degrees of freedom $\\nu$ is defined as $2k$, we have $\\nu = n-1$.\n       \n   * **Rate Parameter ($\\beta$):**\n       $$\n       \\beta = \\frac{n-1}{2\\sigma^2}\n       $$\n   \n   This confirms that $S^2 \\sim \\text{Gamma}(\\frac{n-1}{2}, \\frac{n-1}{2\\sigma^2})$, or equivalently, $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}$.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}