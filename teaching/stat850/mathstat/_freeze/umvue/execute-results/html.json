{
  "hash": "116945074889004bb306dbdd38ddba78",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Uniformly Minimum Variance Estimators\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n## Sufficiency and Completeness\n\n### Sufficient Statistics\n\n\n::: {#def-sufficient-statistic}\n\n### Sufficient Statistic\nA statistic $T=T(X)$ is sufficient for $\\theta$ if one of the following conditions holds:\n\n1.  **Factorization Theorem:**\n    $$\n    f(x;\\theta) = h(x) g(t(x);\\theta)\n    $$\n    Where $h(x)$ is irrelevant to $\\theta$.\n\n2.  **Likelihood Ratio Theorem:**\n    For any pair of data sets $x$ and $x'$ such that $t(x)=t(x')$, the ratio\n    $$\n    \\Lambda_x(\\theta_1, \\theta_2) = \\frac{f(x, \\theta_2)}{f(x, \\theta_1)}\n    $$\n    is independent of the specific realization $x$ (it depends only on $t(x)$). Specifically, $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_{x'}(\\theta_1, \\theta_2)$.\n\n3.  **Conditional Distribution:**\n    The conditional distribution of $X$ given $T(X)=t$, denoted as $f(x|t(x), \\theta)$, is independent of $\\theta$.\n    $$\n    f(x|t(x), \\theta) = f(x|t(x))\n    $$\n\n:::\n\n::: {.proof}\n**Proof of Equivalence (1 $\\Rightarrow$ 2):**\n\nSuppose $t(x) = t(x')$. From the Factorization Theorem:\n\n$$\nf(x;\\theta) = h(x)g(t(x),\\theta)\n$$\n\nThe ratio becomes:\n\n$$\n\\Lambda_x(\\theta_1, \\theta_2) = \\frac{h(x)g(t(x), \\theta_2)}{h(x)g(t(x), \\theta_1)} = \\frac{g(t(x), \\theta_2)}{g(t(x), \\theta_1)}\n$$\n\nSimilarly for $x'$:\n\n$$\n\\Lambda_{x'}(\\theta_1, \\theta_2) = \\frac{g(t(x'), \\theta_2)}{g(t(x'), \\theta_1)}\n$$\n\nSince $t(x) = t(x')$, $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_{x'}(\\theta_1, \\theta_2)$.\n\n:::\n\n::: {#exm-uniform-sufficiency}\n\n### Uniform Distribution $U(\\theta-1, \\theta+1)$\nConsider a random sample $X_1, \\dots, X_n$ from a Uniform distribution with range $(\\theta-1, \\theta+1)$.\n\nThe density for a single observation is:\n\n$$\nf(x_i|\\theta) = \\frac{1}{(\\theta+1) - (\\theta-1)} I(\\theta-1 < x_i < \\theta+1) = \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\nThe joint PDF (likelihood) is:\n\n$$\nL(\\theta; x) = \\prod_{i=1}^n \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\min(x_i) > \\theta-1 ) \\cdot I( \\max(x_i) < \\theta+1 )\n$$\n\nUsing order statistics notation where $X_{(1)} = \\min(X_i)$ and $X_{(n)} = \\max(X_i)$:\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\theta < X_{(1)} + 1 ) \\cdot I( \\theta > X_{(n)} - 1 )\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )\n$$\n\nBy the **Factorization Theorem**, we can define:\n\n* $h(x) = 2^{-n}$ (or simply 1, grouping constants into $g$)\n   * $g(T(x), \\theta) = I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )$\n\nThus, the sufficient statistic is the pair of order statistics:\n$$\nT(X) = (X_{(1)}, X_{(n)})\n$$\n\n:::\n\n\n::: {#exm-gamma-sufficiency}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n$ be i.i.d. $\\Gamma(\\alpha, \\beta)$. The pdf is:\n\n$$\nf(x|\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n$$\n\nThe joint likelihood is:\n\n$$\nL(\\alpha, \\beta; x) = \\left( \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\right)^n \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nBy the Factorization Theorem, we can identify the parts that depend on the data and the parameters:\n$$\ng(T(x), \\theta) = \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nThus, the sufficient statistics are:\n$$\nT(X) = \\left( \\prod_{i=1}^n X_i, \\sum_{i=1}^n X_i \\right)\n$$\n\n:::\n\n\n::: {#exm-exp-family-sufficiency}\n\n### General Exponential Family\nMany common distributions (Normal, Poisson, Gamma, Binomial) belong to the Exponential Family. The lecture notes (Page 7) note that if a density can be written in the form:\n\n$$\nf(x|\\theta) = h(x) c(\\theta) \\exp\\left( \\sum_{j=1}^k w_j(\\theta) t_j(x) \\right)\n$$\n\nThen, by the Factorization Theorem, the statistic:\n\n$$\nT(X) = \\left( \\sum_{i=1}^n t_1(x_i), \\dots, \\sum_{i=1}^n t_k(x_i) \\right)\n$$\n\nis a sufficient statistic for $\\theta$.\n\nFor example, if we have $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$:\n\n$$\nf(x|p) = p^x (1-p)^{1-x} = (1-p) \\exp\\left( x \\ln\\left(\\frac{p}{1-p}\\right) \\right)\n$$\n\nHere, $t(x) = x$. Therefore, $T(X) = \\sum_{i=1}^n X_i$ is sufficient for $p$.\n\n:::\n\n## Minimal Sufficient Statistic\n\n::: {#lem-loglik-equivalence}\n### Equivalence of Log-Likelihood Conditions\nFor any pair of data sets $x$ and $y$, the following three conditions are equivalent descriptions of the relationship between their log-likelihood functions $\\ell(\\theta; \\cdot) = \\ln L(\\theta; \\cdot)$:\n\n1.  **Parallel Log-Likelihoods:**\n    $$\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y) \\quad \\text{for all } \\theta$$\n    (where $c(x,y)$ is independent of $\\theta$).\n\n2.  **Constant Difference:**\n    $$\\ell(\\theta; x) - \\ell(\\theta; y) \\quad \\text{is constant as a function of } \\theta$$\n\n3.  **Equality of Log-Likelihood Differences:**\n    $$\\ell(\\theta_2; x) - \\ell(\\theta_1; x) = \\ell(\\theta_2; y) - \\ell(\\theta_1; y) \\quad \\text{for all } \\theta_1, \\theta_2$$\n\n:::\n\n::: {#thm-mss-loglik}\n### MSS Condition Theorem\nLet $T(X)$ be a **sufficient statistic**. $T(X)$ is a **Minimal Sufficient Statistic (MSS)** if and only if for any pair of data sets $x$ and $y$:\n$$\n\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y) \\text{ for all } \\theta \\implies T(x) = T(y)\n$$\nwhere $c(x, y)$ is a constant independent of $\\theta$.\n:::\n\n::: {.proof}\n**Direction 1: Necessity ($T$ is MSS $\\implies$ Implication holds)**\n\nAssume $T$ is Minimal Sufficient. We must prove that if $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ for all $\\theta$, then $T(x) = T(y)$.\n\n1.  **Define the Statistic $S(x)$:**\n    Let $S(x)$ be the set of all possible datasets $z$ which give the same log-likelihood shape as $x$:\n    $$\n    S(x) = \\{z \\mid \\ell(\\theta; z) = \\ell(\\theta; x) + c_z \\text{ for all } \\theta \\}\n    $$\n    This statistic $S(x)$ represents the equivalence class of $x$ under the parallel log-likelihood relationship.\n    If the condition $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ holds, then by definition $x$ and $y$ generate the same equivalence class, so $S(x) = S(y)$.\n\n2.  **Show $S(x)$ is Sufficient (Directly via Likelihood Ratio):**\n    To prove $S$ is sufficient, we check the **Likelihood Ratio Condition** (Condition 2 from Section 1.1).\n    Suppose $S(x) = S(y)$. By the definition of $S$, this implies:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y)\n    $$\n    Exponentiating both sides:\n    $$\n    \\frac{L(\\theta; x)}{L(\\theta; y)} = e^{c(x, y)}\n    $$\n    The likelihood ratio is independent of $\\theta$. By the Likelihood Ratio definition of sufficiency, $S(X)$ is a sufficient statistic.\n\n3.  **Use Minimality of $T$:**\n    Since $T$ is a **Minimal** Sufficient Statistic, it is a function of *any* sufficient statistic.\n    Therefore, $T$ must be a function of $S$. That is, $T(x) = f(S(x))$.\n\n4.  **Conclusion:**\n    Assume $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$.\n    Then $S(x) = S(y)$.\n    Consequently, $T(x) = f(S(x)) = f(S(y)) = T(y)$.\n\n**Direction 2: Sufficiency (Implication holds $\\implies$ $T$ is MSS)**\n\nAssume that for any $x, y$, $[\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)] \\implies T(x) = T(y)$. We must show that $T$ is a function of *any* sufficient statistic $U$.\n\n1.  Let $U(X)$ be any sufficient statistic. Assume $U(x) = U(y)$.\n2.  By the **Factorization Theorem**, the likelihoods are:\n    $$\n    L(\\theta; x) = h(x)g(U(x), \\theta)\n    $$\n    $$\n    L(\\theta; y) = h(y)g(U(y), \\theta)\n    $$\n3.  Since $U(x) = U(y)$, the factor $g(U(x), \\theta)$ is identical to $g(U(y), \\theta)$.\n    Taking the log-ratio:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = \\ln h(x) - \\ln h(y)\n    $$\n    The term $\\ln h(x) - \\ln h(y)$ depends only on $x$ and $y$, not on $\\theta$. Let this be $c(x,y)$.\n    $$\n    \\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\n    $$\n4.  By our main assumption, this condition implies $T(x) = T(y)$.\n5.  Thus, we have shown that $U(x) = U(y) \\implies T(x) = T(y)$. This means $T$ is a function of $U$.\n    Since $U$ is arbitrary, $T$ is Minimal Sufficient.\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Visualizing Parallel Log-Likelihoods](umvue_files/figure-html/fig-log-likelihood-1.png){#fig-log-likelihood fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n::: {#exm-poisson-mss-log}\n### Poisson Distribution (Log-Likelihood Approach)\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\theta)$.\nTo check if $S(X) = (\\sum X_i, X_1)$ is minimal, we test the implication:\n$$\\text{Parallel Log-Likelihoods} \\implies S(x) = S(y)$$\n\n**1. Check Parallel Condition:**\n$$\\ell(\\theta; x) = -n\\theta + (\\sum x_i)\\ln\\theta - \\sum \\ln(x_i!)$$\nThe difference is:\n$$\\ell(\\theta; x) - \\ell(\\theta; y) = (\\sum x_i - \\sum y_i)\\ln\\theta + K$$\nFor this to be constant in $\\theta$ (Parallel), the $\\ln\\theta$ term must vanish:\n$$\\sum x_i = \\sum y_i$$\n\n**2. Check Implication:**\nDoes $\\sum x_i = \\sum y_i \\implies (\\sum x_i, x_1) = (\\sum y_i, y_1)$?\nNo. Consider $x=(2,0)$ and $y=(1,1)$.\nSum is equal ($2=2$), so Log-Likelihoods are parallel.\nBut $S(x)=(2,2) \\neq S(y)=(2,1)$.\nThus, $S(X)$ is **not** minimal.\n:::\n\n## Completeness\n\n::: {#def-completeness}\n\n### Complete Statistic\nA statistic $T$ is said to be **complete** if for any real-valued function $g$,\n$$\nE[g(T)|\\theta] = 0 \\quad \\text{for all } \\theta\n$$\nimplies\n$$\nP(g(T) = 0 | \\theta) = 1 \\quad \\text{for all } \\theta\n$$\n\n:::\n\nSignificance: If $T$ is complete, then there exists at most one unbiased estimator for $\\theta$ that is a function of $T$.\n\n::: {#exm-uniform-incomplete}\n\n### Uniform Distribution (Not Complete)\nLet $X_1, \\dots, X_n \\sim \\text{Unif}(\\theta-1, \\theta+1)$.\nThe density is:\n\n$$\nf(x) = \\prod I(\\theta-1 < x_i < \\theta+1) = I(\\theta \\in (x_{(n)}-1, x_{(1)}+1))\n$$\n\nThe statistic $T(X) = (X_{(1)}, X_{(n)})$ is a Minimal Sufficient Statistic. However, it is **not complete**.\n\nConsider the range $R = X_{(n)} - X_{(1)}$. The distribution of $R$ does not depend on $\\theta$ (it is an ancillary statistic). Let $g(T) = X_{(n)} - X_{(1)} - c$, where $c = E[X_{(n)} - X_{(1)}]$.\nThen $E[g(T)] = 0$ for all $\\theta$, but $g(T)$ is not identically zero.\n\n:::\n\n::: {#lem-exponential-complete}\n\n### Exponential Family Completeness\nIf $T = (T_1, \\dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then $T$ is complete.\n\n:::\n\n## UMVUE\n\n::: {#def-umvue}\n\n### Uniformly Minimum Variance Unbiased Estimator (UMVUE)\nA statistic $T(x)$ is a UMVUE for $\\theta$ if:\n\n1.  $E(T(x)|\\theta) = \\theta$ for all $\\theta$ (Unbiased).\n   2.  $Var(T(x)|\\theta) \\le Var(d(x)|\\theta)$ for all $\\theta$ and for all other unbiased estimators $d(x)$.\n\n:::\n\nThe relationship between statistics types is visualized below:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hierarchy of Statistics](umvue_files/figure-html/fig-stats-hierarchy-1.png){#fig-stats-hierarchy fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n::: {#thm-lehmann-scheffe}\n\n### Lehmann-Scheffe Theorem\nIf $T$ is a complete and sufficient statistic, and there is an unbiased estimator $d(X)$ such that $E[d(X)] = \\theta$, then $\\phi(T) = E[d(X)|T]$ is the unique UMVUE for $\\theta$.\n\n:::\n\n::: {#thm-rao-blackwell}\n\n### Rao-Blackwell Theorem\nGiven that $T$ is a sufficient statistic and $d_1(x)$ is an unbiased estimator ($E[d_1(x)] = \\theta$).\nDefine $g(T) = E[d_1(x) | T]$. Then:\n\n1.  $g(T)$ is a statistic (free of $\\theta$ because $T$ is sufficient).\n   2.  $E[g(T)] = \\theta$ (Unbiased).\n3.  $Var(g(T)) \\le Var(d_1(x))$.\n\n:::\n\n::: {.proof}\n**Proof of Rao-Blackwell:**\n\n1.  Since $T$ is sufficient, the conditional distribution $X|T$ is independent of $\\theta$, so $g(T)$ is a valid statistic.\n   2.  By the Law of Iterated Expectations:\n    $$\n    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \\theta\n    $$\n\n3.  By the variance decomposition formula:\n    $$\n    Var(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]\n    $$\n    $$\n    Var(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]\n    $$\n    Since $(d_1(X) - g(T))^2 \\ge 0$, we have $Var(g(T)) \\le Var(d_1(X))$.\n\n:::\n\n## Methods for Finding UMVUE\n\nTo find the UMVUE for a parameter $\\theta$:\n\n1.  **Find a Complete Sufficient Statistic:**\n    Identify $T$ which is complete and sufficient for $\\theta$ (often using the Exponential Family properties).\n\n2.  **Find an Unbiased Estimator:**\n    Find any simple statistic $d(X)$ such that $E[d(X)] = \\theta$.\n\n3.  **Rao-Blackwellize:**\n    Compute $g(T) = E[d(X)|T]$. The result $g(T)$ is the UMVUE.\n\n::: {#exm-poisson-umvue}\n\n### Poisson UMVUE\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$. Find the UMVUE for $\\lambda$ and $\\lambda^2$.\n\n1. For $\\lambda$:\n$T = \\sum X_i$ is a complete sufficient statistic (Poisson is exponential family).\nLet $d_1(X) = X_1$.\n$E[X_1] = \\lambda$.\nWe compute $g(T) = E[X_1 | T]$.\nSince the conditional distribution of $X_1$ given $T=t$ is Binomial($t, 1/n$):\n\n$$\nE[X_1 | T] = t \\cdot \\frac{1}{n} = \\frac{T}{n} = \\bar{X}\n$$\n\nThus, $\\bar{X}$ is the UMVUE for $\\lambda$.\n\n2. For $\\lambda^2$:\nWe know $Var(X_1) = \\lambda = E(X_1^2) - (E(X_1))^2$.\nSo $E(X_1^2) - \\lambda = \\lambda^2$, which implies $E(X_1^2 - X_1) = \\lambda^2$.\nLet $d_2(X) = X_1^2 - X_1$. This is an unbiased estimator for $\\lambda^2$.\n\nWe calculate $g(T) = E[X_1^2 - X_1 | T]$.\n\n$$\ng(T) = E[X_1^2 | T] - E[X_1 | T]\n$$\n\nUsing the second moment of the Binomial distribution $Bin(T, 1/n)$:\n$E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \\frac{1}{n}(1-\\frac{1}{n}) + (\\frac{T}{n})^2$.\n\n$$\ng(T) = \\left[ \\frac{T}{n} - \\frac{T}{n^2} + \\frac{T^2}{n^2} \\right] - \\frac{T}{n} = \\frac{T^2 - T}{n^2} = \\frac{T(T-1)}{n^2}\n$$\n\nThus, $\\frac{T(T-1)}{n^2}$ is the UMVUE for $\\lambda^2$.\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}