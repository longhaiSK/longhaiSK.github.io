{
  "hash": "5aff6d4bca7362566bf538c63d3d7f89",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sufficient Statistic and Minimum Variance Estimators\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Sufficient Statistics\n\n::: {#def-sufficient-statistic}\n\n### Sufficient Statistic\nA statistic $T=T(X)$ is sufficient for $\\theta$ if one of the following conditions holds:\n\n1.  **Log-Likelihood Difference:**\n    For any pair of data sets $x$ and $y$ such that $T(x)=T(y)$, the difference in their log-likelihoods is constant with respect to $\\theta$:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y) \\quad \\text{for all } \\theta\n    $$\n    where $c(x,y)$ depends only on $x$ and $y$, not on $\\theta$.\n\n2.  **Factorization Theorem:**\n    $$\n    f(x;\\theta) = h(x) g(t(x);\\theta)\n    $$\n    Where $h(x)$ is irrelevant to $\\theta$.\n\n3.  **Conditional Distribution:**\n    The conditional distribution of $X$ given $T(X)=t$, denoted as $f(x|t(x), \\theta)$, is independent of $\\theta$.\n    $$\n    f(x|t(x), \\theta) = f(x|t(x))\n    $$\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Visualizing Parallel Log-Likelihoods](umvue_files/figure-html/fig-par-log-likelihood-1.png){#fig-par-log-likelihood fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n<details>\n<summary>Click to view Complete Proof of Equivalence</summary>\n\n::: {.proof}\n**Proof of Equivalence**\n\nWe show the equivalence by proving the implications in a cycle or pairs: $(2 \\Rightarrow 1)$, $(1 \\Rightarrow 2)$, $(2 \\Rightarrow 3)$, and $(3 \\Rightarrow 2)$.\n\n1. Factorization $\\Rightarrow$ Log-Likelihood Difference $(2 \\Rightarrow 1)$\n\nAssume the **Factorization Theorem** holds: $L(\\theta; x) = h(x) g(T(x); \\theta)$.\nConsider any pair $x, y$ such that $T(x) = T(y)$.\n$$\n\\ell(\\theta; x) - \\ell(\\theta; y) = [\\ln h(x) + \\ln g(T(x); \\theta)] - [\\ln h(y) + \\ln g(T(y); \\theta)]\n$$\nSince $T(x)=T(y)$, the terms $\\ln g(T(x); \\theta)$ and $\\ln g(T(y); \\theta)$ are identical and cancel out.\n$$\n\\ell(\\theta; x) - \\ell(\\theta; y) = \\ln h(x) - \\ln h(y)\n$$\nThis difference depends only on $x$ and $y$ (via $h$), and is independent of $\\theta$. Thus, condition 1 holds.\n\n2. Log-Likelihood Difference $\\Rightarrow$ Factorization $(1 \\Rightarrow 2)$\n\nAssume Condition 1 holds. For any $x$ and $y$ with $T(x)=T(y)$, $\\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y)$.\nExponentiating, we get $L(\\theta; x) = k(x, y) L(\\theta; y)$, where $k$ is independent of $\\theta$.\n\nFor each value $t$ in the range of $T$, select a fixed representative data point $x_t$ such that $T(x_t) = t$.\nFor any data point $x$, let $t = T(x)$. Using the relation above:\n$$\nL(\\theta; x) = k(x, x_t) L(\\theta; x_t)\n$$\nDefine $h(x) = k(x, x_{T(x)})$ and $g(t; \\theta) = L(\\theta; x_t)$.\nThen:\n$$\nL(\\theta; x) = h(x) g(T(x); \\theta)\n$$\nThis is exactly the Factorization form.\n\n3. Factorization $\\Rightarrow$ Conditional Distribution $(2 \\Rightarrow 3)$\n\nAssume $f(x; \\theta) = h(x)g(T(x); \\theta)$. We derive the conditional distribution $P(X=x | T(X)=t)$.\nIf $T(x) \\neq t$, the probability is 0 (independent of $\\theta$).\nIf $T(x) = t$:\n$$\nP(X=x | T(X)=t) = \\frac{P(X=x, T(X)=t)}{P(T(X)=t)} = \\frac{f(x; \\theta)}{\\sum_{\\{y : T(y)=t\\}} f(y; \\theta)}\n$$\nSubstitute the factorization:\n$$\n= \\frac{h(x)g(t; \\theta)}{\\sum_{\\{y : T(y)=t\\}} h(y)g(t; \\theta)} = \\frac{h(x)g(t; \\theta)}{g(t; \\theta) \\sum_{\\{y : T(y)=t\\}} h(y)}\n$$\nThe term $g(t; \\theta)$ cancels out:\n$$\n= \\frac{h(x)}{\\sum_{\\{y : T(y)=t\\}} h(y)}\n$$\nThis expression depends only on $x$ and $h(\\cdot)$, and is entirely free of $\\theta$. Thus, Condition 3 holds.\n\n4. Conditional Distribution $\\Rightarrow$ Factorization $(3 \\Rightarrow 2)$\n\nAssume $f(x | T(x); \\theta) = k(x)$, where $k(x)$ is independent of $\\theta$.\nWe can write the joint distribution as:\n$$\nf(x; \\theta) = f(x | T(x)=t; \\theta) \\cdot P(T(X)=t; \\theta)\n$$\nSubstitute the assumption:\n$$\nf(x; \\theta) = k(x) \\cdot P(T(X)=T(x); \\theta)\n$$\nLet $h(x) = k(x)$ and $g(t; \\theta) = P(T(X)=t; \\theta)$.\nThen:\n$$\nf(x; \\theta) = h(x) g(T(x); \\theta)\n$$\nThis recovers the Factorization form.\n\n:::\n\n</details>\n\n::: {#exm-uniform-sufficiency}\n\n### Uniform Distribution $U(\\theta-1, \\theta+1)$\nConsider a random sample $X_1, \\dots, X_n$ from a Uniform distribution with range $(\\theta-1, \\theta+1)$.\n\nThe density for a single observation is:\n\n$$\nf(x_i|\\theta) = \\frac{1}{(\\theta+1) - (\\theta-1)} I(\\theta-1 < x_i < \\theta+1) = \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\nThe joint PDF (likelihood) is:\n\n$$\nL(\\theta; x) = \\prod_{i=1}^n \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\min(x_i) > \\theta-1 ) \\cdot I( \\max(x_i) < \\theta+1 )\n$$\n\nUsing order statistics notation where $X_{(1)} = \\min(X_i)$ and $X_{(n)} = \\max(X_i)$:\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\theta < X_{(1)} + 1 ) \\cdot I( \\theta > X_{(n)} - 1 )\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )\n$$\n\nBy the **Factorization Theorem**, we can define:\n\n* $h(x) = 2^{-n}$ (or simply 1, grouping constants into $g$)\n   * $g(T(x), \\theta) = I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )$\n\nThus, the sufficient statistic is the pair of order statistics:\n$$\nT(X) = (X_{(1)}, X_{(n)})\n$$\n\n:::\n\n\n::: {#exm-gamma-sufficiency}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n$ be i.i.d. $\\Gamma(\\alpha, \\beta)$. The pdf is:\n\n$$\nf(x|\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n$$\n\nThe joint likelihood is:\n\n$$\nL(\\alpha, \\beta; x) = \\left( \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\right)^n \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nBy the Factorization Theorem, we can identify the parts that depend on the data and the parameters:\n$$\ng(T(x), \\theta) = \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nThus, the sufficient statistics are:\n$$\nT(X) = \\left( \\prod_{i=1}^n X_i, \\sum_{i=1}^n X_i \\right)\n$$\n\n:::\n\n\n::: {#exm-exp-family-sufficiency}\n\n### General Exponential Family\nMany common distributions (Normal, Poisson, Gamma, Binomial) belong to the Exponential Family. The lecture notes (Page 7) note that if a density can be written in the form:\n\n$$\nf(x|\\theta) = h(x) c(\\theta) \\exp\\left( \\sum_{j=1}^k w_j(\\theta) t_j(x) \\right)\n$$\n\nThen, by the Factorization Theorem, the statistic:\n\n$$\nT(X) = \\left( \\sum_{i=1}^n t_1(x_i), \\dots, \\sum_{i=1}^n t_k(x_i) \\right)\n$$\n\nis a sufficient statistic for $\\theta$.\n\n\n:::\n\n::: {#exm-bernoulli-expfamily}\n\n### Bernoulli as Exponential Family\nLet $X_1, \\dots, X_n \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)$.\nTo find the sufficient statistic, we write the joint density in the canonical **Exponential Family** form:\n$$\nf(x; \\theta) = h(x) \\exp\\left( \\eta(\\theta) T(x) - A(\\theta) \\right)\n$$\n\n1. Write the Joint PDF:\n    $$\nf(x|p) = \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}\n$$\n\n2. Convert to Exponential Form:\n    $$\n\\begin{aligned}\nf(x|p) &= \\exp\\left( \\ln \\left( \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} \\right) \\right) \\\\\n&= \\exp\\left( \\sum_{i=1}^n \\left[ x_i \\ln p + (1-x_i) \\ln(1-p) \\right] \\right) \\\\\n&= \\exp\\left( (\\ln p) \\sum x_i + \\ln(1-p) \\left( n - \\sum x_i \\right) \\right)\n\\end{aligned}\n$$\n\n3. Group Terms by $x$ and $p$:\n    $$\n\\begin{aligned}\nf(x|p) &= \\exp\\left( \\underbrace{\\left( \\ln p - \\ln(1-p) \\right)}_{\\eta(p)} \\underbrace{\\left(\\sum_{i=1}^n x_i\\right)}_{T(x)} + \\underbrace{n \\ln(1-p)}_{-A(p)} \\right)\n\\end{aligned}\n$$\n\n**Conclusion:**\nBy the structure of the exponential family, the term multiplying the natural parameter $\\eta(p) = \\ln\\left(\\frac{p}{1-p}\\right)$ is the sufficient statistic:\n$$\nT(X) = \\sum_{i=1}^n X_i\n$$\n\n:::\n\n## Minimal Sufficient Statistic\n\n::: {#def-mss}\n\n### Minimal Sufficient Statistic (MSS)\nA statistic $T(X)$ is a **Minimal Sufficient Statistic** if:\n\n1.  **Sufficiency:** $T(X)$ is a sufficient statistic for $\\theta$.\n   2.  **Minimality:** For any other sufficient statistic $S(X)$, $T(X)$ is a function of $S(X)$.\n    $$\n    T(X) = g(S(X))\n    $$\n    *(This implies that $T(X)$ provides the greatest possible data reduction without losing information about $\\theta$. If $S(x) = S(y)$, then it must be that $T(x) = T(y)$).*\n\n:::\n\n::: {#thm-mss-loglik}\n\n### MSS Condition Theorem\nLet $T(X)$ be a **sufficient statistic**. $T(X)$ is a **Minimal Sufficient Statistic (MSS)** if and only if for any pair of data sets $x$ and $y$:\n$$\n\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y) \\text{ for all } \\theta \\implies T(x) = T(y)\n$$\nwhere $c(x, y)$ is a constant independent of $\\theta$.\n\n:::\n\n<details>\n<summary>Click to view the Proof</summary>\n\n::: {.proof}\n\n\n**Direction 1: Sufficiency (Implication holds $\\implies$ $T$ is MSS)**\n\nAssume that for any $x, y$, $[\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)] \\implies T(x) = T(y)$. We must show that $T$ is a function of *any* sufficient statistic $U$.\n\n1.  Let $U(X)$ be any sufficient statistic. Assume $U(x) = U(y)$.\n   2.  By the **Factorization Theorem**, the likelihoods are:\n    $$\n    L(\\theta; x) = h(x)g(U(x), \\theta)\n    $$\n    $$\n    L(\\theta; y) = h(y)g(U(y), \\theta)\n    $$\n\n3.  Since $U(x) = U(y)$, the factor $g(U(x), \\theta)$ is identical to $g(U(y), \\theta)$.\n    Taking the log-ratio:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = \\ln h(x) - \\ln h(y)\n    $$\n    The term $\\ln h(x) - \\ln h(y)$ depends only on $x$ and $y$, not on $\\theta$. Let this be $c(x,y)$.\n    $$\n    \\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\n    $$\n\n4.  By our main assumption, this condition implies $T(x) = T(y)$.\n   5.  Thus, we have shown that $U(x) = U(y) \\implies T(x) = T(y)$. This means $T$ is a function of $U$.\n    Since $U$ is arbitrary, $T$ is Minimal Sufficient.\n\n**Direction 2: Necessity ($T$ is MSS $\\implies$ Implication holds)**\n\nAssume $T$ is Minimal Sufficient. We must prove that if $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ for all $\\theta$, then $T(x) = T(y)$.\n\n1.  **Define the Statistic $S(x)$:**\n    Let $S(x)$ be the set of all possible datasets $z$ which give the same log-likelihood shape as $x$:\n    $$\n    S(x) = \\{z \\mid \\ell(\\theta; z) = \\ell(\\theta; x) + c_z \\text{ for all } \\theta \\}\n    $$\n    This statistic $S(x)$ represents the equivalence class of $x$ under the parallel log-likelihood relationship.\n    If the condition $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$ holds, then by definition $x$ and $y$ generate the same equivalence class, so $S(x) = S(y)$.\n\n2.  **Show $S(x)$ is Sufficient (Directly via Likelihood Ratio):**\n    To prove $S$ is sufficient, we check the **Likelihood Ratio Condition** (Condition 2 from Section 1.1).\n    Suppose $S(x) = S(y)$. By the definition of $S$, this implies:\n    $$\n    \\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y)\n    $$\n    By the definition of sufficiency, $S(X)$ is a sufficient statistic.\n\n3.  **Use Minimality of $T$:**\n    Since $T$ is a **Minimal** Sufficient Statistic, it is a function of *any* sufficient statistic.\n    Therefore, $T$ must be a function of $S$. That is, $T(x) = f(S(x))$.\n\n4.  **Conclusion:**\n    Assume $\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)$.\n    Then $S(x) = S(y)$.\n    Consequently, $T(x) = f(S(x)) = f(S(y)) = T(y)$.\n\n:::\n</details>\n\n\n::: {#exm-bernoulli-mss-check}\n\n### Checking Minimality via Log-Likelihood Condition\nLet $X_1, X_2, X_3 \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)$.\nWe determine the MSS by checking the implication from the **MSS Condition Theorem**:\n$$\n\\text{Parallel Log-Likelihoods} \\implies T(x) = T(y)\n$$\n\n**Step 1: Establishing the MSS**\nFirst, we find the condition under which two log-likelihoods are parallel.\n$$\n\\ell(p; x) = (\\sum x_i) \\ln p + (n - \\sum x_i) \\ln(1-p)\n$$\nThe difference $\\ell(p; x) - \\ell(p; y)$ depends on $p$ only through the term $(\\sum x_i - \\sum y_i) \\ln \\frac{p}{1-p}$.\nFor this difference to be constant (independent of $p$), the coefficient must be zero:\n$$\n\\text{Parallel Log-Likelihoods} \\iff \\sum x_i = \\sum y_i\n$$\nThe statistic that corresponds exactly to this condition is $T(X) = \\sum X_i$.\nSince $\\sum x_i = \\sum y_i$ trivially implies $T(x) = T(y)$, $T(X)$ is the **Minimal Sufficient Statistic**.\n\n**Step 2: Why $S(X) = (X_1, \\sum_{i=2}^3 X_i)$ is NOT Minimal**\nNow consider the \"richer\" statistic $S(X)$. If $S$ were minimal, the parallel condition must imply $S(x) = S(y)$.\nWe check:\n$$\n\\sum x_i = \\sum y_i \\stackrel{?}{\\implies} (x_1, \\sum_{i=2}^3 x_i) = (y_1, \\sum_{i=2}^3 y_i)\n$$\n**Counter-Example:**\nLet $x = (1, 0, 1)$ and $y = (0, 1, 1)$.\n\n1.  **Check Parallel Condition:**\n    $\\sum x_i = 2$ and $\\sum y_i = 2$.\n    The sums are equal, so the log-likelihoods are parallel.\n\n2.  **Check Statistic Equality:**\n    $$S(x) = (1, 1)$$\n    $$S(y) = (0, 2)$$\n    $$S(x) \\neq S(y)$$\n\n**Conclusion:** The parallel condition holds, but $S(x) \\neq S(y)$. The implication fails.\nThis proves that $S(X)$ is **not** minimalâ€”it retains \"extra\" information (the position of the first success) that is not relevant to the likelihood shape.\n\n:::\n\n## Completeness\n\n::: {#def-completeness}\n\n### Complete Statistic\nA statistic $T$ is said to be **complete** if for any real-valued function $g$,\n$$\nE[g(T)|\\theta] = 0 \\quad \\text{for all } \\theta\n$$\nimplies\n$$\nP(g(T) = 0 | \\theta) = 1 \\quad \\text{for all } \\theta\n$$\n\n:::\n\nSignificance: If $T$ is complete, then there exists at most one unbiased estimator for $\\theta$ that is a function of $T$.\n\n::: {#exm-uniform-incomplete}\n\n### Uniform Distribution (Not Complete)\nLet $X_1, \\dots, X_n \\sim \\text{Unif}(\\theta-1, \\theta+1)$.\nThe density is:\n\n$$\nf(x) = \\prod I(\\theta-1 < x_i < \\theta+1) = I(\\theta \\in (x_{(n)}-1, x_{(1)}+1))\n$$\n\nThe statistic $T(X) = (X_{(1)}, X_{(n)})$ is a Minimal Sufficient Statistic. However, it is **not complete**.\n\nConsider the range $R = X_{(n)} - X_{(1)}$. The distribution of $R$ does not depend on $\\theta$ (it is an ancillary statistic). Let $g(T) = X_{(n)} - X_{(1)} - c$, where $c = E[X_{(n)} - X_{(1)}]$.\nThen $E[g(T)] = 0$ for all $\\theta$, but $g(T)$ is not identically zero.\n\n:::\n\n::: {#lem-exponential-complete}\n\n### Exponential Family Completeness\nIf $T = (T_1, \\dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then $T$ is complete.\n\n:::\n\n## UMVUE\n\n::: {#def-umvue}\n\n### Uniformly Minimum Variance Unbiased Estimator (UMVUE)\nA statistic $T(x)$ is a UMVUE for $\\theta$ if:\n\n1.  $E(T(x)|\\theta) = \\theta$ for all $\\theta$ (Unbiased).\n   2.  $Var(T(x)|\\theta) \\le Var(d(x)|\\theta)$ for all $\\theta$ and for all other unbiased estimators $d(x)$.\n\n:::\n\nThe relationship between statistics types is visualized below:\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hierarchy of Statistics](umvue_files/figure-html/fig-stats-hierarchy-1.png){#fig-stats-hierarchy fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n::: {#thm-lehmann-scheffe}\n\n### Lehmann-Scheffe Theorem\nIf $T$ is a complete and sufficient statistic, and there is an unbiased estimator $d(X)$ such that $E[d(X)] = \\theta$, then $\\phi(T) = E[d(X)|T]$ is the unique UMVUE for $\\theta$.\n\n:::\n\n::: {#thm-rao-blackwell}\n\n### Rao-Blackwell Theorem\nGiven that $T$ is a sufficient statistic and $d_1(x)$ is an unbiased estimator ($E[d_1(x)] = \\theta$).\nDefine $g(T) = E[d_1(x) | T]$. Then:\n\n1.  $g(T)$ is a statistic (free of $\\theta$ because $T$ is sufficient).\n   2.  $E[g(T)] = \\theta$ (Unbiased).\n   3.  $Var(g(T)) \\le Var(d_1(x))$.\n\n:::\n\n::: {.proof}\n**Proof of Rao-Blackwell:**\n\n1.  Since $T$ is sufficient, the conditional distribution $X|T$ is independent of $\\theta$, so $g(T)$ is a valid statistic.\n   2.  By the Law of Iterated Expectations:\n    $$\n    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \\theta\n    $$\n\n3.  By the variance decomposition formula:\n    $$\n    Var(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]\n    $$\n    $$\n    Var(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]\n    $$\n    Since $(d_1(X) - g(T))^2 \\ge 0$, we have $Var(g(T)) \\le Var(d_1(X))$.\n\n:::\n\n## Methods for Finding UMVUE\n\nTo find the UMVUE for a parameter $\\theta$:\n\n1.  **Find a Complete Sufficient Statistic:**\n    Identify $T$ which is complete and sufficient for $\\theta$ (often using the Exponential Family properties).\n\n2.  **Find an Unbiased Estimator:**\n    Find any simple statistic $d(X)$ such that $E[d(X)] = \\theta$.\n\n3.  **Rao-Blackwellize:**\n    Compute $g(T) = E[d(X)|T]$. The result $g(T)$ is the UMVUE.\n\n::: {#exm-poisson-umvue}\n\n### Poisson UMVUE\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$. Find the UMVUE for $\\lambda$ and $\\lambda^2$.\n\n1. For $\\lambda$:\n    $T = \\sum X_i$ is a complete sufficient statistic (Poisson is exponential family).\nLet $d_1(X) = X_1$.\n$E[X_1] = \\lambda$.\nWe compute $g(T) = E[X_1 | T]$.\nSince the conditional distribution of $X_1$ given $T=t$ is Binomial($t, 1/n$):\n\n$$\nE[X_1 | T] = t \\cdot \\frac{1}{n} = \\frac{T}{n} = \\bar{X}\n$$\n\nThus, $\\bar{X}$ is the UMVUE for $\\lambda$.\n\n2. For $\\lambda^2$:\n    We know $Var(X_1) = \\lambda = E(X_1^2) - (E(X_1))^2$.\nSo $E(X_1^2) - \\lambda = \\lambda^2$, which implies $E(X_1^2 - X_1) = \\lambda^2$.\nLet $d_2(X) = X_1^2 - X_1$. This is an unbiased estimator for $\\lambda^2$.\n\nWe calculate $g(T) = E[X_1^2 - X_1 | T]$.\n\n$$\ng(T) = E[X_1^2 | T] - E[X_1 | T]\n$$\n\nUsing the second moment of the Binomial distribution $Bin(T, 1/n)$:\n$E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \\frac{1}{n}(1-\\frac{1}{n}) + (\\frac{T}{n})^2$.\n\n$$\ng(T) = \\left[ \\frac{T}{n} - \\frac{T}{n^2} + \\frac{T^2}{n^2} \\right] - \\frac{T}{n} = \\frac{T^2 - T}{n^2} = \\frac{T(T-1)}{n^2}\n$$\n\nThus, $\\frac{T(T-1)}{n^2}$ is the UMVUE for $\\lambda^2$.\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}