{
  "hash": "0adb093b0d19d5ba888d49c4596bc664",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Minimum Variance Estimators\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n\n\n## Completeness\n\n::: {#def-completeness}\n\n### Complete Statistic\nA statistic $T$ is said to be **complete** if for any real-valued function $g$,\n$$\nE[g(T)|\\theta] = 0 \\quad \\text{for all } \\theta\n$$\nimplies\n$$\nP(g(T) = 0 | \\theta) = 1 \\quad \\text{for all } \\theta\n$$\n\n:::\n\nSignificance: If $T$ is complete, then there exists at most one unbiased estimator for $\\theta$ that is a function of $T$.\n\n::: {#exm-uniform-incomplete}\n\n### Uniform Distribution (Not Complete)\nLet $X_1, \\dots, X_n \\sim \\text{Unif}(\\theta-1, \\theta+1)$.\nThe density is:\n\n$$\nf(x) = \\prod I(\\theta-1 < x_i < \\theta+1) = I(\\theta \\in (x_{(n)}-1, x_{(1)}+1))\n$$\n\nThe statistic $T(X) = (X_{(1)}, X_{(n)})$ is a Minimal Sufficient Statistic. However, it is **not complete**.\n\nConsider the range $R = X_{(n)} - X_{(1)}$. The distribution of $R$ does not depend on $\\theta$ (it is an ancillary statistic). Let $g(T) = X_{(n)} - X_{(1)} - c$, where $c = E[X_{(n)} - X_{(1)}]$.\nThen $E[g(T)] = 0$ for all $\\theta$, but $g(T)$ is not identically zero.\n\n:::\n\n::: {#lem-exponential-complete}\n\n### Exponential Family Completeness\nIf $T = (T_1, \\dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then $T$ is complete.\n\n:::\n\n## UMVUE\n\n::: {#def-umvue}\n\n### Uniformly Minimum Variance Unbiased Estimator (UMVUE)\nA statistic $T(x)$ is a UMVUE for $\\theta$ if:\n\n1.  $E(T(x)|\\theta) = \\theta$ for all $\\theta$ (Unbiased).\n   2.  $Var(T(x)|\\theta) \\le Var(d(x)|\\theta)$ for all $\\theta$ and for all other unbiased estimators $d(x)$.\n\n:::\n\nThe relationship between statistics types is visualized below:\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hierarchy of Statistics](umvue_files/figure-html/fig-stats-hierarchy-1.png){#fig-stats-hierarchy fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n::: {#thm-lehmann-scheffe}\n\n### Lehmann-Scheffe Theorem\nIf $T$ is a complete and sufficient statistic, and there is an unbiased estimator $d(X)$ such that $E[d(X)] = \\theta$, then $\\phi(T) = E[d(X)|T]$ is the unique UMVUE for $\\theta$.\n\n:::\n\n::: {#thm-rao-blackwell}\n\n### Rao-Blackwell Theorem\nGiven that $T$ is a sufficient statistic and $d_1(x)$ is an unbiased estimator ($E[d_1(x)] = \\theta$).\nDefine $g(T) = E[d_1(x) | T]$. Then:\n\n1.  $g(T)$ is a statistic (free of $\\theta$ because $T$ is sufficient).\n   2.  $E[g(T)] = \\theta$ (Unbiased).\n   3.  $Var(g(T)) \\le Var(d_1(x))$.\n\n:::\n\n::: {.proof}\n**Proof of Rao-Blackwell:**\n\n1.  Since $T$ is sufficient, the conditional distribution $X|T$ is independent of $\\theta$, so $g(T)$ is a valid statistic.\n   2.  By the Law of Iterated Expectations:\n    $$\n    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \\theta\n    $$\n\n3.  By the variance decomposition formula:\n    $$\n    Var(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]\n    $$\n    $$\n    Var(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]\n    $$\n    Since $(d_1(X) - g(T))^2 \\ge 0$, we have $Var(g(T)) \\le Var(d_1(X))$.\n\n:::\n\n## Methods for Finding UMVUE\n\nTo find the UMVUE for a parameter $\\theta$:\n\n1.  **Find a Complete Sufficient Statistic:**\n    Identify $T$ which is complete and sufficient for $\\theta$ (often using the Exponential Family properties).\n\n2.  **Find an Unbiased Estimator:**\n    Find any simple statistic $d(X)$ such that $E[d(X)] = \\theta$.\n\n3.  **Rao-Blackwellize:**\n    Compute $g(T) = E[d(X)|T]$. The result $g(T)$ is the UMVUE.\n\n::: {#exm-poisson-umvue}\n\n### Poisson UMVUE\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$. Find the UMVUE for $\\lambda$ and $\\lambda^2$.\n\n1. For $\\lambda$:\n    $T = \\sum X_i$ is a complete sufficient statistic (Poisson is exponential family).\nLet $d_1(X) = X_1$.\n$E[X_1] = \\lambda$.\nWe compute $g(T) = E[X_1 | T]$.\nSince the conditional distribution of $X_1$ given $T=t$ is Binomial($t, 1/n$):\n\n$$\nE[X_1 | T] = t \\cdot \\frac{1}{n} = \\frac{T}{n} = \\bar{X}\n$$\n\nThus, $\\bar{X}$ is the UMVUE for $\\lambda$.\n\n2. For $\\lambda^2$:\n    We know $Var(X_1) = \\lambda = E(X_1^2) - (E(X_1))^2$.\nSo $E(X_1^2) - \\lambda = \\lambda^2$, which implies $E(X_1^2 - X_1) = \\lambda^2$.\nLet $d_2(X) = X_1^2 - X_1$. This is an unbiased estimator for $\\lambda^2$.\n\nWe calculate $g(T) = E[X_1^2 - X_1 | T]$.\n\n$$\ng(T) = E[X_1^2 | T] - E[X_1 | T]\n$$\n\nUsing the second moment of the Binomial distribution $Bin(T, 1/n)$:\n$E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \\frac{1}{n}(1-\\frac{1}{n}) + (\\frac{T}{n})^2$.\n\n$$\ng(T) = \\left[ \\frac{T}{n} - \\frac{T}{n^2} + \\frac{T^2}{n^2} \\right] - \\frac{T}{n} = \\frac{T^2 - T}{n^2} = \\frac{T(T-1)}{n^2}\n$$\n\nThus, $\\frac{T(T-1)}{n^2}$ is the UMVUE for $\\lambda^2$.\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}