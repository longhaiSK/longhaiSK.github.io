{
  "hash": "23225e5d939848026a5674c63711ddf3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Uniformly Minimum Variance Estimators\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Sufficiency and Completeness\n\nIn the context of statistical inference, we consider the likelihood function $L(\\theta; x) = f(x, \\theta)$, where $\\theta$ is the variable and $x$ is fixed.\n\n::: {#exm-normal-likelihood}\n\n### Normal Distribution Likelihood\nSuppose $X_{1}, \\dots, X_{n} | \\mu, \\sigma^{2} \\sim N(\\mu, \\sigma^{2})$. The data is represented as $x = (x_1, \\dots, x_n)$ and the parameter vector is $\\theta = (\\mu, \\sigma^{2})$. The likelihood function is given by:\n\n$$ \nL(\\theta; x) = f(x | \\theta) = (2\\pi)^{-\\frac{n}{2}} (\\sigma^{2})^{-\\frac{n}{2}} \\exp\\left\\{ -\\frac{1}{2\\sigma^{2}} \\sum (x_i - \\mu)^2 \\right\\} \n$$\n\nThis can be rewritten to show it is determined by $\\bar{x}$ and $s^2$.\n\n:::\n\n::: {#def-sufficient-statistic}\n\n### Sufficient Statistic\nA statistic $T = T(x)$ is sufficient for $\\theta$ if one of the following conditions is met:\n\n* Factorization Criterion:\n  The joint density can be factored as $f(x | \\theta) = h(x) g(T(x), \\theta)$, where $h(x)$ is independent of $\\theta$.\n\n* Likelihood Ratio Theorem:\n  For any pair of data sets $x$ and $x'$ such that $T(x) = T(x')$, the ratio $\\frac{f(x, \\theta_2)}{f(x, \\theta_1)} = \\frac{f(x', \\theta_2)}{f(x', \\theta_1)}$ for all $\\theta_1, \\theta_2$.\n\n* Conditional Distribution:\n  The conditional distribution of the data given the statistic, $f(x | T(x), \\theta)$, is independent of $\\theta$.\n\n:::\n\n\n## Minimal Sufficient Statistics\n\n::: {#def-minimal-sufficient}\n\n### Minimal Sufficient Statistics (MSS)\nA sufficient statistic $T(X)$ is minimal sufficient if $T(X)$ is a function of any other sufficient statistic $S$. That is, for any sufficient statistic $S$, there exists a function $g$ such that $T(x) = g(S)$.\n\n:::\n\n::: {#thm-mss-ratio}\n\n### Theorem 6.1: Ratio Characterization of MSS\n$T(x)$ is minimal sufficient if and only if for any pair $x$ and $x'$, $T(x) = T(x')$ is equivalent to the ratio $\\frac{f(x, \\theta_2)}{f(x, \\theta_1)}$ being equal to $\\frac{f(x', \\theta_2)}{f(x', \\theta_1)}$ for all $\\theta_1, \\theta_2$.\n\n:::\n\n## Completeness\n\n::: {#def-complete-statistic}\n\n### Complete Statistic\nA statistic $T$ is said to be complete if for every real-valued function $g$, $E[g(T) | \\theta] = 0$ for all $\\theta$ implies $P(g(T) = 0 | \\theta) = 1$ for all $\\theta$.\n\n:::\n\nIf a statistic is complete, then an unbiased estimator of $\\theta$ based on that statistic is unique.\n\n::: {#thm-exponential-family-completeness}\n\n### Lemma 6.3: Exponential Family Completeness\nIf $T = (T_1, \\dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in its parameter space, then $T$ is complete.\n\n:::\n\n::: {.proof}\nThe density of $T$ follows the form $f(t_1, \\dots, t_k | \\theta) = C(\\theta) h(t) \\exp(\\sum \\theta_i t_i)$. Suppose $E[g(T) | \\theta] = 0$:\n\n$$ \n\\int g(t) C(\\theta) h(t) \\exp\\left( \\sum \\theta_i t_i \\right) dt = 0 \n$$\n\nThis integral represents a Laplace transform. Since the transform is zero over an open set, the function $g(t)h(t)$ must be zero almost everywhere, implying $g(T) = 0$ almost surely.\n\n:::\n\n## Uniformly Minimum Variance Unbiased Estimators (UMVUE)\n\n::: {#def-umvue}\n\n### UMVUE\nA statistic $T(x)$ is a Uniformly Minimum Variance Unbiased Estimator for $\\tau(\\theta)$ if:\n\n* It is unbiased: $E[T(x) | \\theta] = \\tau(\\theta)$ for all $\\theta$.\n\n* It has minimum variance: $Var(T(x) | \\theta) \\leq Var(d(x) | \\theta)$ for all $\\theta$, where $d(x)$ is any other unbiased estimator of $\\tau(\\theta)$.\n\n:::\n\n::: {#thm-rao-blackwell}\n\n### Theorem 6.3: Rao-Blackwell Theorem\nLet $d_1(x)$ be an unbiased estimator of $\\theta$ and $T$ be a sufficient statistic. Let $g(T) = E[d_1(x) | T]$. Then:\n\n1. $g(T)$ is a statistic (independent of $\\theta$).\n\n2. $E[g(T) | \\theta] = \\theta$.\n\n3. $Var(g(T) | \\theta) \\leq Var(d_1(x) | \\theta)$.\n\n:::\n\n::: {.proof}\nSince $T$ is sufficient, the conditional distribution of $X$ given $T$ does not depend on $\\theta$, making $E[d_1(x) | T]$ a valid statistic. By the law of total expectation, $E[E[d_1(x) | T]] = E[d_1(x)] = \\theta$. By Jensen's Inequality or the law of total variance, the variance of the conditional expectation is less than or equal to the total variance.\n\n:::\n\n### Lehmann-Scheff√© Theorem\n\nIf $T$ is a sufficient and complete statistic, then $g(T) = E[d(X) | T]$ is the unique UMVUE.\n\n### Methods for finding UMVUE\n\n1. Find a complete sufficient statistic $T$ for $\\theta$.\n\n2. Find any unbiased estimator $d(x)$ such that $E[d(x) | \\theta] = \\theta$.\n\n3. Calculate $g(T) = E[d(x) | T]$. This result is the UMVUE.\n\n::: {#exm-poisson-umvue}\n\n### UMVUE for Poisson Mean\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$.\n\n1. $T = \\sum X_i$ is a complete sufficient statistic.\n\n2. Let $d(X) = X_1$. Since $E[X_1] = \\lambda$, it is an unbiased estimator.\n\n3. Find $E[X_1 | T]$. Since $X_1 | \\sum X_i = k \\sim \\text{Binomial}(k, 1/n)$, we have:\n\n$$ \nE[X_1 | T] = \\frac{T}{n} = \\bar{X} \n$$\n\nThus, $\\bar{X}$ is the UMVUE for $\\lambda$.\n\n:::",
    "supporting": [
      "umvue_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}