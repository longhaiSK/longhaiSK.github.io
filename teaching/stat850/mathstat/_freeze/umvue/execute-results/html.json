{
  "hash": "c9d34566f819f3a007d9b78c79fc752d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Uniformly Minimum Variance Estimators\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n## Sufficiency and Completeness\n\n### Sufficient Statistics\n\n\n::: {#def-sufficient-statistic}\n\n### Sufficient Statistic\nA statistic $T=T(X)$ is sufficient for $\\theta$ if one of the following conditions holds:\n\n1.  **Factorization Theorem:**\n    $$\n    f(x;\\theta) = h(x) g(t(x);\\theta)\n    $$\n    Where $h(x)$ is irrelevant to $\\theta$.\n\n2.  **Likelihood Ratio Theorem:**\n    For any pair of data sets $x$ and $x'$ such that $t(x)=t(x')$, the ratio\n    $$\n    \\Lambda_x(\\theta_1, \\theta_2) = \\frac{f(x, \\theta_2)}{f(x, \\theta_1)}\n    $$\n    is independent of the specific realization $x$ (it depends only on $t(x)$). Specifically, $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_{x'}(\\theta_1, \\theta_2)$.\n\n3.  **Conditional Distribution:**\n    The conditional distribution of $X$ given $T(X)=t$, denoted as $f(x|t(x), \\theta)$, is independent of $\\theta$.\n    $$\n    f(x|t(x), \\theta) = f(x|t(x))\n    $$\n\n:::\n\n::: {.proof}\n**Proof of Equivalence (1 $\\Rightarrow$ 2):**\n\nSuppose $t(x) = t(x')$. From the Factorization Theorem:\n\n$$\nf(x;\\theta) = h(x)g(t(x),\\theta)\n$$\n\nThe ratio becomes:\n\n$$\n\\Lambda_x(\\theta_1, \\theta_2) = \\frac{h(x)g(t(x), \\theta_2)}{h(x)g(t(x), \\theta_1)} = \\frac{g(t(x), \\theta_2)}{g(t(x), \\theta_1)}\n$$\n\nSimilarly for $x'$:\n\n$$\n\\Lambda_{x'}(\\theta_1, \\theta_2) = \\frac{g(t(x'), \\theta_2)}{g(t(x'), \\theta_1)}\n$$\n\nSince $t(x) = t(x')$, $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_{x'}(\\theta_1, \\theta_2)$.\n\n:::\n\n::: {#exm-uniform-sufficiency}\n\n### Uniform Distribution $U(\\theta-1, \\theta+1)$\nConsider a random sample $X_1, \\dots, X_n$ from a Uniform distribution with range $(\\theta-1, \\theta+1)$.\n\nThe density for a single observation is:\n\n$$\nf(x_i|\\theta) = \\frac{1}{(\\theta+1) - (\\theta-1)} I(\\theta-1 < x_i < \\theta+1) = \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\nThe joint PDF (likelihood) is:\n\n$$\nL(\\theta; x) = \\prod_{i=1}^n \\frac{1}{2} I(\\theta-1 < x_i < \\theta+1)\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\min(x_i) > \\theta-1 ) \\cdot I( \\max(x_i) < \\theta+1 )\n$$\n\nUsing order statistics notation where $X_{(1)} = \\min(X_i)$ and $X_{(n)} = \\max(X_i)$:\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( \\theta < X_{(1)} + 1 ) \\cdot I( \\theta > X_{(n)} - 1 )\n$$\n\n$$\nL(\\theta; x) = 2^{-n} \\cdot I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )\n$$\n\nBy the **Factorization Theorem**, we can define:\n\n* $h(x) = 2^{-n}$ (or simply 1, grouping constants into $g$)\n   * $g(T(x), \\theta) = I( X_{(n)} - 1 < \\theta < X_{(1)} + 1 )$\n\nThus, the sufficient statistic is the pair of order statistics:\n$$\nT(X) = (X_{(1)}, X_{(n)})\n$$\n\n:::\n\n\n::: {#exm-gamma-sufficiency}\n\n### Gamma Distribution\nLet $X_1, \\dots, X_n$ be i.i.d. $\\Gamma(\\alpha, \\beta)$. The pdf is:\n\n$$\nf(x|\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha-1} e^{-x/\\beta}, \\quad x > 0\n$$\n\nThe joint likelihood is:\n\n$$\nL(\\alpha, \\beta; x) = \\left( \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\right)^n \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nBy the Factorization Theorem, we can identify the parts that depend on the data and the parameters:\n$$\ng(T(x), \\theta) = \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n$$\n\nThus, the sufficient statistics are:\n$$\nT(X) = \\left( \\prod_{i=1}^n X_i, \\sum_{i=1}^n X_i \\right)\n$$\n\n:::\n\n\n::: {#exm-exp-family-sufficiency}\n\n### General Exponential Family\nMany common distributions (Normal, Poisson, Gamma, Binomial) belong to the Exponential Family. The lecture notes (Page 7) note that if a density can be written in the form:\n\n$$\nf(x|\\theta) = h(x) c(\\theta) \\exp\\left( \\sum_{j=1}^k w_j(\\theta) t_j(x) \\right)\n$$\n\nThen, by the Factorization Theorem, the statistic:\n\n$$\nT(X) = \\left( \\sum_{i=1}^n t_1(x_i), \\dots, \\sum_{i=1}^n t_k(x_i) \\right)\n$$\n\nis a sufficient statistic for $\\theta$.\n\nFor example, if we have $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$:\n\n$$\nf(x|p) = p^x (1-p)^{1-x} = (1-p) \\exp\\left( x \\ln\\left(\\frac{p}{1-p}\\right) \\right)\n$$\n\nHere, $t(x) = x$. Therefore, $T(X) = \\sum_{i=1}^n X_i$ is sufficient for $p$.\n\n:::\n\n\n::: {#thm-mss-ratio-implication}\n\n### MSS Condition Theorem\nLet $T(X)$ be a **sufficient statistic**. $T(X)$ is a **Minimal Sufficient Statistic (MSS)** if and only if for any pair of data sets $x$ and $y$, the following conditions imply $T(x)=T(y)$:\n\n1.  **Proportional Likelihoods:** $L(\\theta; x) = c(x, y) L(\\theta; y)$ for all $\\theta$.\n2.  **Ratio Independence:** $\\frac{L(\\theta; x)}{L(\\theta; y)}$ is constant as a function of $\\theta$.\n3.  **Equality of Likelihood Ratio Statistics:** $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_y(\\theta_1, \\theta_2)$ for all $\\theta_1, \\theta_2$.\n\n**Logic:**\nSince $T$ is already sufficient, we know $T(x)=T(y) \\implies \\text{Proportional Likelihood Condition}$.\nThe condition for **minimality** is the reverse: $\\text{Proportional Likelihood Condition} \\implies T(x)=T(y)$.\n\n:::\n\n::: {.proof}\n**Proof**\n\nWe assume Conditions (1), (2), and (3) are equivalent descriptions of $x$ and $y$ having proportional likelihoods. We denote \"Conditions (1-3)\" collectively as the **Proportional Likelihood Condition**.\n\n**Direction 1: $T$ is MSS $\\implies$ (Proportional Likelihood Condition $\\implies T(x)=T(y)$)**\n\nAssume $T(X)$ is a Minimal Sufficient Statistic. By definition, $T$ is a function of *any* other sufficient statistic $S$.\n\nLet us construct a specific statistic $S^*(X)$ defined by the likelihood partitions:\n$$ S^*(x) = \\{ z : L(\\theta; z) = c \\cdot L(\\theta; x) \\text{ for some constant } c \\} $$\n(i.e., $S^*(x)$ groups all data points that satisfy the Proportional Likelihood Condition).\n$S^*(X)$ is known to be a sufficient statistic.\n\nSince $T$ is MSS, it must be a function of $S^*$. That is, there exists some function $g$ such that $T(x) = g(S^*(x))$.\nIf the Proportional Likelihood Condition holds for $x$ and $y$, then by definition $S^*(x) = S^*(y)$.\nTherefore:\n$$ S^*(x) = S^*(y) \\implies g(S^*(x)) = g(S^*(y)) \\implies T(x) = T(y) $$\n\n**Direction 2: (Proportional Likelihood Condition $\\implies T(x)=T(y)$) $\\implies$ $T$ is MSS**\n\nAssume that for any $x,y$, if the Proportional Likelihood Condition holds, then $T(x)=T(y)$.\nWe must show that $T$ is a function of any sufficient statistic $S$.\n\nLet $S(X)$ be *any* sufficient statistic.\nBy the Factorization Theorem, if $S(x) = S(y)$, then:\n$$ L(\\theta; x) = h(x)g(S(x); \\theta) \\quad \\text{and} \\quad L(\\theta; y) = h(y)g(S(y); \\theta) $$\nSince $S(x)=S(y)$, the term $g(S(x);\\theta) = g(S(y);\\theta)$.\nThe ratio is:\n$$ \\frac{L(\\theta; x)}{L(\\theta; y)} = \\frac{h(x)}{h(y)} $$\nThis ratio is independent of $\\theta$, so the Proportional Likelihood Condition holds.\n\nBy our assumption, Proportional Likelihood Condition $\\implies T(x)=T(y)$.\nSo, we have shown:\n$$ S(x) = S(y) \\implies T(x) = T(y) $$\nThis implies that $T$ is a function of $S$.\nSince $S$ was arbitrary, $T$ is a function of every sufficient statistic.\nTherefore, $T$ is Minimal Sufficient.\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Visualizing Proportional Likelihoods (Parallel Log-Likelihoods)](umvue_files/figure-html/fig-log-likelihood-1.png){#fig-log-likelihood fig-align='center' width=576 style=\"width: 70% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n::: {#exm-poisson-mss}\n\n### Poisson Distribution\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\theta)$.\n$T(X) = \\sum X_i$ is minimal sufficient.\n$S(X) = (\\sum X_i, X_1)$ is sufficient but not minimal.\n\n:::\n\n::: {.proof}\n**Proof that $S(X) = (\\sum X_i, X_1)$ is Not Minimal**\n\nWe use the **MSS Condition Theorem** stated above.\nFor $S(X)$ to be minimal sufficient, the following implication must hold for any pair of data sets $x$ and $y$:\n$$ \\text{Proportional Likelihood Condition} \\implies S(x) = S(y) $$\n\n**Step 1: Evaluate the Proportional Likelihood Condition**\nFor the Poisson distribution, the likelihood ratio between two data sets $x$ and $y$ is:\n$$\n\\frac{L(\\theta; x)}{L(\\theta; y)} = \\theta^{\\left(\\sum x_i - \\sum y_i\\right)} \\frac{\\prod y_i!}{\\prod x_i!}\n$$\nThe **Proportional Likelihood Condition** requires this ratio to be independent of $\\theta$.\nThis holds if and only if the exponent of $\\theta$ is zero:\n$$ \\sum x_i = \\sum y_i $$\n\n**Step 2: Test the Implication for $S(X)$**\nThe requirement becomes:\n$$ \\sum x_i = \\sum y_i \\implies \\left(\\sum x_i, x_1\\right) = \\left(\\sum y_i, y_1\\right) $$\n\n**Step 3: Counter-Example**\nConsider sample size $n=2$ and two data sets:\n\n* $x = (2, 0)$ where $\\sum x_i = 2$ and $x_1 = 2$.\n* $y = (1, 1)$ where $\\sum y_i = 2$ and $y_1 = 1$.\n\n**Check Proportional Likelihood Condition:**\n$\\sum x_i = 2$ and $\\sum y_i = 2$. The condition holds.\n\n**Check Statistic Equality:**\n$$ S(x) = (2, 2) $$\n$$ S(y) = (2, 1) $$\n$$ S(x) \\neq S(y) $$\n\n**Conclusion:**\nWe have found a case where the Proportional Likelihood Condition holds, but $S(x) \\neq S(y)$.\nThe implication fails. Therefore, $S(X)$ is **not** Minimal Sufficient.\n\n:::\n\n\n## Completeness\n\n::: {#def-completeness}\n\n### Complete Statistic\nA statistic $T$ is said to be **complete** if for any real-valued function $g$,\n$$\nE[g(T)|\\theta] = 0 \\quad \\text{for all } \\theta\n$$\nimplies\n$$\nP(g(T) = 0 | \\theta) = 1 \\quad \\text{for all } \\theta\n$$\n\n:::\n\nSignificance: If $T$ is complete, then there exists at most one unbiased estimator for $\\theta$ that is a function of $T$.\n\n::: {#exm-uniform-incomplete}\n\n### Uniform Distribution (Not Complete)\nLet $X_1, \\dots, X_n \\sim \\text{Unif}(\\theta-1, \\theta+1)$.\nThe density is:\n\n$$\nf(x) = \\prod I(\\theta-1 < x_i < \\theta+1) = I(\\theta \\in (x_{(n)}-1, x_{(1)}+1))\n$$\n\nThe statistic $T(X) = (X_{(1)}, X_{(n)})$ is a Minimal Sufficient Statistic. However, it is **not complete**.\n\nConsider the range $R = X_{(n)} - X_{(1)}$. The distribution of $R$ does not depend on $\\theta$ (it is an ancillary statistic). Let $g(T) = X_{(n)} - X_{(1)} - c$, where $c = E[X_{(n)} - X_{(1)}]$.\nThen $E[g(T)] = 0$ for all $\\theta$, but $g(T)$ is not identically zero.\n\n:::\n\n::: {#lem-exponential-complete}\n\n### Exponential Family Completeness\nIf $T = (T_1, \\dots, T_k)$ is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then $T$ is complete.\n\n:::\n\n## UMVUE\n\n::: {#def-umvue}\n\n### Uniformly Minimum Variance Unbiased Estimator (UMVUE)\nA statistic $T(x)$ is a UMVUE for $\\theta$ if:\n\n1.  $E(T(x)|\\theta) = \\theta$ for all $\\theta$ (Unbiased).\n   2.  $Var(T(x)|\\theta) \\le Var(d(x)|\\theta)$ for all $\\theta$ and for all other unbiased estimators $d(x)$.\n\n:::\n\nThe relationship between statistics types is visualized below:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hierarchy of Statistics](umvue_files/figure-html/fig-stats-hierarchy-1.png){#fig-stats-hierarchy fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n::: {#thm-lehmann-scheffe}\n\n### Lehmann-Scheffe Theorem\nIf $T$ is a complete and sufficient statistic, and there is an unbiased estimator $d(X)$ such that $E[d(X)] = \\theta$, then $\\phi(T) = E[d(X)|T]$ is the unique UMVUE for $\\theta$.\n\n:::\n\n::: {#thm-rao-blackwell}\n\n### Rao-Blackwell Theorem\nGiven that $T$ is a sufficient statistic and $d_1(x)$ is an unbiased estimator ($E[d_1(x)] = \\theta$).\nDefine $g(T) = E[d_1(x) | T]$. Then:\n\n1.  $g(T)$ is a statistic (free of $\\theta$ because $T$ is sufficient).\n   2.  $E[g(T)] = \\theta$ (Unbiased).\n3.  $Var(g(T)) \\le Var(d_1(x))$.\n\n:::\n\n::: {.proof}\n**Proof of Rao-Blackwell:**\n\n1.  Since $T$ is sufficient, the conditional distribution $X|T$ is independent of $\\theta$, so $g(T)$ is a valid statistic.\n   2.  By the Law of Iterated Expectations:\n    $$\n    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \\theta\n    $$\n\n3.  By the variance decomposition formula:\n    $$\n    Var(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]\n    $$\n    $$\n    Var(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]\n    $$\n    Since $(d_1(X) - g(T))^2 \\ge 0$, we have $Var(g(T)) \\le Var(d_1(X))$.\n\n:::\n\n## Methods for Finding UMVUE\n\nTo find the UMVUE for a parameter $\\theta$:\n\n1.  **Find a Complete Sufficient Statistic:**\n    Identify $T$ which is complete and sufficient for $\\theta$ (often using the Exponential Family properties).\n\n2.  **Find an Unbiased Estimator:**\n    Find any simple statistic $d(X)$ such that $E[d(X)] = \\theta$.\n\n3.  **Rao-Blackwellize:**\n    Compute $g(T) = E[d(X)|T]$. The result $g(T)$ is the UMVUE.\n\n::: {#exm-poisson-umvue}\n\n### Poisson UMVUE\nLet $X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)$. Find the UMVUE for $\\lambda$ and $\\lambda^2$.\n\n1. For $\\lambda$:\n$T = \\sum X_i$ is a complete sufficient statistic (Poisson is exponential family).\nLet $d_1(X) = X_1$.\n$E[X_1] = \\lambda$.\nWe compute $g(T) = E[X_1 | T]$.\nSince the conditional distribution of $X_1$ given $T=t$ is Binomial($t, 1/n$):\n\n$$\nE[X_1 | T] = t \\cdot \\frac{1}{n} = \\frac{T}{n} = \\bar{X}\n$$\n\nThus, $\\bar{X}$ is the UMVUE for $\\lambda$.\n\n2. For $\\lambda^2$:\nWe know $Var(X_1) = \\lambda = E(X_1^2) - (E(X_1))^2$.\nSo $E(X_1^2) - \\lambda = \\lambda^2$, which implies $E(X_1^2 - X_1) = \\lambda^2$.\nLet $d_2(X) = X_1^2 - X_1$. This is an unbiased estimator for $\\lambda^2$.\n\nWe calculate $g(T) = E[X_1^2 - X_1 | T]$.\n\n$$\ng(T) = E[X_1^2 | T] - E[X_1 | T]\n$$\n\nUsing the second moment of the Binomial distribution $Bin(T, 1/n)$:\n$E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \\frac{1}{n}(1-\\frac{1}{n}) + (\\frac{T}{n})^2$.\n\n$$\ng(T) = \\left[ \\frac{T}{n} - \\frac{T}{n^2} + \\frac{T^2}{n^2} \\right] - \\frac{T}{n} = \\frac{T^2 - T}{n^2} = \\frac{T(T-1)}{n^2}\n$$\n\nThus, $\\frac{T(T-1)}{n^2}$ is the UMVUE for $\\lambda^2$.\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}