{
  "hash": "2b53983f379fb3b5e61e92b45a39207e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Statistical Inference\"\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n## Population Model (Data Model)\n\nWe begin with observations (units) $X_1, X_2, \\dots, X_n$. These may be vectors. We regard these observations as a realization of random variables.\n\n::: {#def-population-distribution}\n### Population Distribution\nWe assume that $X_1, X_2, \\dots, X_n \\sim f(x)$.\nThe function $f(x)$ is called the **population distribution**.\n:::\n\n### Assumptions and Scope {.unnumbered}\n\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). The assumption of identical distribution can be relaxed to regression settings in which the distributions of $x_i$'s are independent but dependent on covariate $x_i$. \n\nIn **Parametric Statistics**, we assume $f(x)$ is of a known analytic form but involves unknown parameters.  \n\n::: {#exm-normal-distribution}\n### Parametric Model: Normal\nConsider the Normal distribution:\n$$f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\nHere, the parameter space is $\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}$.\nThe goal is to learn aspects of the unknown $\\theta$ from observations $X_1, \\dots, X_n$.\n:::\n\n::: {#exm-bernoulli-distribution}\n### Parametric Model: Bernoulli\nConsider a sequence of binary outcomes (e.g., Success/Failure) where each $X_i \\in \\{0, 1\\}$. We assume $X_i \\sim \\text{Bernoulli}(\\theta)$.\nThe probability mass function is:\n$$f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}$$\nHere, the parameter space is $\\Theta = [0, 1]$, where $\\theta$ represents the probability of success.\n:::\n\n## Probabilistic Model vs. Statistical Inference\n\nThere is a fundamental distinction between probability and statistics regarding the parameter $\\theta$. We can visualize this using a \"shooting target\" analogy:\n\n* **$\\theta$ (The Center):** The true, unknown bullseye location.\n* **$x$ (The Shots):** The observed holes on the target board.\n\n* **Probability (Deductive):** The center $\\theta$ is **known**. We predict where the shots $x$ will land.\n* **Statistics (Inductive):** The shots $x$ are **observed** on the board. The center $\\theta$ is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).](introstatinf_files/figure-pdf/fig-prob-vs-stat-base-1.pdf){#fig-prob-vs-stat-base}\n:::\n:::\n\n\n## A Motivating Example: The Lady Tasting Tea\n\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\n\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare $n$ cups of tea.\n\n* **Random Variable:** Let $X_i=1$ if she identifies the cup correctly, and $0$ otherwise.\n* **Parameter:** Let $\\theta$ be the probability that she correctly identifies a cup.\n* **The Data:** Suppose we observe that she identifies **70%** of cups correctly ($\\bar{x} = 0.7$), which is a summary of the observed vector of $x_i$, for example, \n\n$$x=(0,1,1,0, 1,1,0,1,1,1)$$\n\n::: {.panel-tabset}\n### Small Sample (n=10)\nWe observe **7 out of 10** correct ($k=7$).\n$$\\bar{x} = 0.7$$\n\n### Large Sample (n=40)\nWe observe **28 out of 40** correct ($k=28$).\n$$\\bar{x} = 0.7$$\n:::\n\n## Questions to Answer in Statistical Inference\n\nUsing this example, we identify the four main types of statistical inference.\n\n##### Point Estimation {.unnumbered}\nWe want to use a single number to capture the parameter: $\\hat{\\theta} = \\theta(X_1, \\dots, X_n)$.\n\n* *Tea Example:* Our best guess for her success rate is $\\hat{\\theta} = 0.7$.\n\n##### Hypothesis Testing {.unnumbered}\nWe want to test a theory about the parameter: $H_0$ vs $H_1$.\n\n* *Tea Example:* Is she just guessing? We test $H_0: \\theta = 0.5$ vs $H_1: \\theta > 0.5$.\n\n##### Model Assessment {.unnumbered}\nWe want to test a theory about the parameter: $H_0$ vs $H_1$.\n\n* *Example:* Can we use a reduced model? What level of complexity of $f(x; \\theta)$ is necessary?\n\n##### Interval Estimation {.unnumbered}\nWe want to construct an interval likely to contain the parameter: $\\theta \\in (L, U)$.\n\n* *Tea Example:* We might say her true skill $\\theta$ is likely between $0.45$ and $0.95$.\n\n##### Prediction {.unnumbered}\nWe want to predict a new observation $Y_{n+1}$ given previous data.\n\n* *Tea Example:* If we give her an $(n+1)$-th cup, what is the probability she identifies it correctly?\n\n## The Likelihood Function\n\nThe bridge between probability and statistics is the Likelihood Function.\n\n::: {#def-likelihood}\n### Likelihood Function\nLet $f(x_1, \\dots, x_n; \\theta)$ be the joint probability density (or mass) function of the data given the parameter $\\theta$. When we view this function as a function of $\\theta$ for fixed observed data $x_1, \\dots, x_n$, we call it the **likelihood function**, denoted $L(\\theta)$.\n$$L(\\theta) = f(x_1, \\dots, x_n; \\theta)$$\n:::\n\n### Example: Lady Tasting Tea {.unnumbered}\n\nFor our Tea Tasting data, the likelihood is proportional to the Binomial probability:\n$$L(\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}$$\n\n::: {.panel-tabset}\n### n=10 (k=7)\n\nHere, $L(\\theta) = \\binom{10}{7} \\theta^{7} (1-\\theta)^{3}$.\n\n\n::: {.cell}\n\n:::\n\n\n| $\\theta$ | Calculation $\\binom{10}{7} \\theta^{7} (1-\\theta)^{3}$ | $L(\\theta)$ |\n| :--- | :--- | :--- |\n| 0.0 | 120 $\\times 0^{7} \\times 1^{3}$ | 0.0000 |\n| 0.2 | 120 $\\times 0.2^{7} \\times 0.8^{3}$ | 0.0008 |\n| 0.4 | 120 $\\times 0.4^{7} \\times 0.6^{3}$ | 0.0425 |\n| 0.6 | 120 $\\times 0.6^{7} \\times 0.4^{3}$ | 0.2150 |\n| 0.7 | 120 $\\times 0.7^{7} \\times 0.3^{3}$ | **0.2668** (Max) |\n| 0.8 | 120 $\\times 0.8^{7} \\times 0.2^{3}$ | 0.2013 |\n| 1.0 | 120 $\\times 1^{7} \\times 0^{3}$ | 0.0000 |\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Likelihood Function (n= 10 )](introstatinf_files/figure-pdf/fig-likelihood-tea-small-1.pdf){#fig-likelihood-tea-small}\n:::\n:::\n\n\n### n=40 (k=28)\nHere, $L(\\theta) = \\binom{40}{28} \\theta^{28} (1-\\theta)^{12}$.\nNotice how the likelihood becomes **narrower** (more peaked) with more data, even though the peak remains at 0.7.\n\n\n::: {.cell}\n\n:::\n\n\n| $\\theta$ | Calculation $\\binom{40}{28} \\theta^{28} (1-\\theta)^{12}$ | $L(\\theta)$ |\n| :--- | :--- | :--- |\n| 0.0 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0^{28} \\times 1^{12}$ | 0.0000 |\n| 0.2 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0.2^{28} \\times 0.8^{12}$ | 0.0000 |\n| 0.4 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0.4^{28} \\times 0.6^{12}$ | 0.0001 |\n| 0.6 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0.6^{28} \\times 0.4^{12}$ | 0.0576 |\n| 0.7 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0.7^{28} \\times 0.3^{12}$ | **0.1366** (Max) |\n| 0.8 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 0.8^{28} \\times 0.2^{12}$ | 0.0443 |\n| 1.0 | \\ensuremath{5.5868535\\times 10^{9}} $\\times 1^{28} \\times 0^{12}$ | 0.0000 |\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Likelihood Function (n= 40 )](introstatinf_files/figure-pdf/fig-likelihood-tea-large-1.pdf){#fig-likelihood-tea-large}\n:::\n:::\n\n:::\n\n#### Questions {.unnumbered}\n\n* Is an estimator like $\\bar x$, which is called Maximum Likelihood Estimator (MLE), a good estimator in general?\n* What do you discover from actually observing the two likelihood unctions of different sample size $n$?\n* Is the likelihood function central to all inference problems?\n* What are the essential 'parameters' of the likelihood function?\n\n\nThere are two primary frameworks for \"How\" to perform these inferences.\n\n## Frequentist Inference\n\n* **Concept:** $\\theta$ is unknown but fixed; Data $X$ is random.\n* **Sampling Distribution:** We analyze how $\\hat{\\theta}$ behaves under hypothetical repeated sampling.\n\n### Example: Frequentist Test of Lady Tasting Tea {.unnumbered}\nWe test $H_0: \\theta=0.5$ (Guessing) vs $H_1: \\theta > 0.5$ (Skill).\nWe analyze the behavior of $\\bar{X}$ assuming $H_0$ is true. The rejection region (one-sided) is shaded red.\n\n::: {.panel-tabset}\n\n### n=10 (k=7)\nWe calculate the P-value: Probability of observing $\\ge 7$ correct out of 10, assuming $\\theta=0.5$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Sampling Distribution (n= 10 )](introstatinf_files/figure-pdf/fig-freq-sampling-tea-small-1.pdf){#fig-freq-sampling-tea-small}\n:::\n:::\n\n\n### n=40 (k=28)\nWe calculate the P-value: Probability of observing $\\ge 28$ correct out of 40.\nWith a larger sample size, the same proportion (0.7) provides **stronger evidence** against the null.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Sampling Distribution (n= 40 )](introstatinf_files/figure-pdf/fig-freq-sampling-tea-large-1.pdf){#fig-freq-sampling-tea-large}\n:::\n:::\n\n:::\n\n\n### Questions to Answer\n\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\n* **MLE:** Can we use the Maximum Likelihood Estimator (MLE) $\\hat{\\theta}$ for general models even no closed-form solution exists? Is MLE a good method?\n* **Sampling Distributions:** What is the distribution of $\\hat{\\theta}_{\\text{MLE}}$? What's its mean and standard deviation?\n* **Confidence Intervals:** How to construct CI with $\\hat{\\theta}$?\n* **Hypothesis Testing:** How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?\n\n\n## Bayesian Inference\n\n* **Concept:** $\\theta$ is regarded as a random variable.\n* **Posterior:** Posterior $\\propto$ Likelihood $\\times$ Prior.\n\n### Example: Bayesian Analysis of the Lady Tasting Tea {.unnumbered}\nPrior: $\\text{Beta}(1,1)$ (Uniform).\n\n::: {.panel-tabset}\n### n=10 (k=7)\nPosterior: $\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Bayesian Update (n= 10 )](introstatinf_files/figure-pdf/fig-bayes-tea-small-1.pdf){#fig-bayes-tea-small}\n:::\n:::\n\n\n### n=40 (k=28)\nPosterior: $\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)$. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Bayesian Update (n= 40 )](introstatinf_files/figure-pdf/fig-bayes-tea-large-1.pdf){#fig-bayes-tea-large}\n:::\n:::\n\n:::\n\n### Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\n* **Posterior Derivation:** How do we derive the posterior distribution $f(\\theta|x)$ for various likelihoods and priors?\n* **Comparing with Other methods:** Are Bayesain methods good or not or general inference?\n* **Computation:** When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\n* **Summarization:** How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\n* **Prediction:** How do we solve the integral required to compute the posterior predictive distribution for future data?\n* **Prior:** How to choose our prior? What's its effect on our inference?\n* **Model Comparison and Assessment:** How to assess a Bayesian model?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}