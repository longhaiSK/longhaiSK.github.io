{
  "hash": "bcef3bda48a4579148c13d92c8865d78",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n## Likelihood and MLE\n\n::: {#def-likelihood-mle}\n\n### Maximum Likelihood Estimation\n\n1.  **Likelihood Function:**\n\n    Let $f(\\mathbf{x}|\\boldsymbol{\\theta})$ be the joint probability density function of the data $\\mathbf{X}$. When viewed as a function of the parameter $\\boldsymbol{\\theta}$ given fixed data $\\mathbf{x}$, it is called the likelihood function:\n    $$\n    L(\\boldsymbol{\\theta}; \\mathbf{x}) = f(\\mathbf{x}|\\boldsymbol{\\theta})\n    $$\n\n2.  **Log-likelihood:**\n\n    It is usually easier to maximize the natural logarithm of the likelihood:\n    $$\n    \\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\log L(\\boldsymbol{\\theta}; \\mathbf{x})\n    $$\n\n3.  **Maximum Likelihood Estimator (MLE):**\n\n    The MLE $\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}$ is the value that maximizes the likelihood function:\n    $$\n    \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}(\\mathbf{x}) = \\operatorname*{argmax}_{\\boldsymbol{\\theta} \\in \\Theta} \\ell(\\boldsymbol{\\theta}; \\mathbf{x})\n    $$\n\n4.  **Score Function ($\\mathbf{U}$):**\n\n    The score function is defined as the gradient of the log-likelihood with respect to the parameter vector. \n    $$\n    \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) = \\nabla_{\\boldsymbol{\\theta}} \\ell(\\boldsymbol{\\theta}; \\mathbf{x})\n    $$\n    Finding the MLE often involves solving the score equation: \n    $$\n    \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) \\equiv 0\n    $$\n5.  **Observed Fisher Inforamtion:**\n\n   $$\n   \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) = - \\nabla^2 \\ell(\\boldsymbol{\\theta}; \\mathbf{X})\n   $$\n\n6.  **(Expected) Fisher Information Matrix ($\\mathbf{I}$):** \n   \n    The covariance matrix of the score vector. It is a deterministic $p \\times p$ matrix (for a fixed $\\boldsymbol{\\theta}$).\n    $$\n    \\mathbf{I}(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}} \\left[ \\mathbf{J}(\\boldsymbol{\\theta}; \\mathbf{X}) \\right]\n    $$\n\n:::\n\n## Examples\n\n::: {#exm-normal-mle-score}\n\n### Normal Distribution: MLE and Score Convergence\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. We define $\\boldsymbol{\\theta} = (\\mu, \\sigma^2)^T$.\n\n1. Log-Likelihood\n\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n$$\n\n2. Score Function and MLE\n\nThe Score vector $\\mathbf{U}(\\boldsymbol{\\theta})$ has two components:\n\n* **Component 1 (Mean):**\n    $$\n    U_\\mu = \\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)\n    $$\n    Setting $U_\\mu = 0$ yields $\\hat{\\mu}_{\\text{MLE}} = \\bar{x}$.\n\n* **Component 2 (Variance):**\n    $$\n    U_{\\sigma^2} = \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (x_i - \\mu)^2\n    $$\n    Setting $U_{\\sigma^2} = 0$ yields $\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum (x_i - \\hat{\\mu})^2$.\n\n3. Asymptotic Normality of the Score\n\nWe now demonstrate that $\\mathbf{U}(\\boldsymbol{\\theta})$ follows a Normal distribution $N(\\mathbf{0}, \\mathbf{I}(\\boldsymbol{\\theta}))$.\n\n* **For $U_\\mu$:**\n    $$\n    U_\\mu = \\frac{n}{\\sigma^2} (\\bar{X} - \\mu)\n    $$\n    Since $\\bar{X} \\sim N(\\mu, \\sigma^2/n)$, $U_\\mu$ is a linear transformation of a Normal variable. It is **exactly Normal**:\n    $$\n    E[U_\\mu] = 0, \\quad \\text{Var}(U_\\mu) = \\left(\\frac{n}{\\sigma^2}\\right)^2 \\text{Var}(\\bar{X}) = \\frac{n^2}{\\sigma^4} \\frac{\\sigma^2}{n} = \\frac{n}{\\sigma^2}\n    $$\n    Thus, $U_\\mu \\sim N(0, \\frac{n}{\\sigma^2})$.\n\n* **For $U_{\\sigma^2}$:**\n    We rewrite $U_{\\sigma^2}$ as a sum of i.i.d. variables. Let $Z_i = ((X_i - \\mu)/\\sigma)^2$. Note that $Z_i \\sim \\chi^2_1$, with variance 2.\n    $$\n    U_{\\sigma^2} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left( \\left(\\frac{X_i - \\mu}{\\sigma}\\right)^2 - 1 \\right) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Z_i - 1)\n    $$\n    Since this is a sum of i.i.d. random variables with mean 0, the **Central Limit Theorem** applies:\n    $$\n    U_{\\sigma^2} \\xrightarrow{d} \\text{Normal}\n    $$\n    The limiting variance is:\n    $$\n    \\text{Var}(U_{\\sigma^2}) = \\frac{1}{4\\sigma^4} \\sum \\text{Var}(Z_i) = \\frac{1}{4\\sigma^4} (n \\times 2) = \\frac{n}{2\\sigma^4}\n    $$\n\n**Conclusion:**\nThe covariance matrix of the Score approaches the Fisher Information matrix:\n$$\n\\text{Var}(\\mathbf{U}) = \\begin{bmatrix} \\frac{n}{\\sigma^2} & 0 \\\\ 0 & \\frac{n}{2\\sigma^4} \\end{bmatrix} = \\mathbf{I}(\\boldsymbol{\\theta})\n$$\n\n:::\n\n::: {#exm-uniform-mle-boundary}\n\n### Uniform Distribution (Boundary Case)\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{\\text{Unif}}(0, \\theta)$. The likelihood is:\n$$\nL(\\theta; \\mathbf{x}) = \\frac{1}{\\theta^n} I(x_{(n)} \\le \\theta)\n$$\nThis function strictly decreases for $\\theta \\ge x_{(n)}$ and is zero otherwise. Thus:\n$$\n\\hat{\\theta}_{\\text{MLE}} = x_{(n)}\n$$\nNote that the Score equation approach fails here because the support depends on $\\theta$, making the log-likelihood discontinuous at the boundary.\n\n:::\n\n## Review of Convergence Theorems for Probability\n\n\n\n::: {#thm-lln}\n\n### Weak Law of Large Numbers (WLLN)\nLet $X_1, \\dots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \\mu$ and finite variance $\\text{Var}(X_i) = \\sigma^2 < \\infty$.\n\nThen, the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$ converges in probability to $\\mu$:\n$$\n\\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty\n$$\nFormal definition: For any $\\epsilon > 0$, $\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0$.\n\n:::\n\n\n\n::: {#thm-clt}\n\n### Central Limit Theorem for IID Cases\nLet $X_1, \\dots, X_n$ be i.i.d. random variables with mean $E[X_i] = \\mu$ and finite variance $0 < \\text{Var}(X_i) = \\sigma^2 < \\infty$.\n\nThen, the random variable $\\sqrt{n}(\\bar{X}_n - \\mu)$ converges in distribution to a normal distribution with mean 0 and variance $\\sigma^2$:\n$$\n\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\n$$\n\n:::\n\n\n::: {#thm-lindeberg-feller}\n\n### Lindeberg-Feller CLT (For Non-Identical Distributions)\nThis variation is crucial for regression analysis (e.g., OLS properties with fixed regressors) where variables are independent but **not** identically distributed.\n\nLet $X_1, \\dots, X_n$ be independent random variables with $E[X_i] = \\mu_i$ and $\\text{Var}(X_i) = \\sigma_i^2$.\nDefine the **average variance** $\\tilde{\\sigma}_n^2$:\n$$\n\\tilde{\\sigma}_n^2 = \\frac{1}{n} \\sum_{i=1}^n \\sigma_i^2\n$$\n\nIf the **Lindeberg Condition** holds:\nFor every $\\epsilon > 0$,\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n \\tilde{\\sigma}_n^2} \\sum_{i=1}^n E\\left[ (X_i - \\mu_i)^2 \\cdot I\\left( |X_i - \\mu_i| > \\epsilon \\sqrt{n \\tilde{\\sigma}_n^2} \\right) \\right] = 0\n$$\n\nThen the standardized sum converges to a standard normal:\n$$\n\\frac{\\sum_{i=1}^n (X_i - \\mu_i)}{\\sqrt{n \\tilde{\\sigma}_n^2}} \\xrightarrow{d} N(0, 1)\n$$\n\n:::\n\n::: {#cor-lindeberg-feller-approx}\n\n### Approximating Distribution for Sample Mean (Non-i.i.d.)\n\nUnder the conditions of the Lindeberg-Feller CLT, we can derive the asymptotic distribution for the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nLet $\\bar{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n \\mu_i$ be the average mean. Note that the denominator in the CLT is simply $\\sqrt{n} \\tilde{\\sigma}_n$.\n\nThe standardized sum converges to $N(0,1)$:\n$$\n\\frac{\\sum (X_i - \\mu_i)}{\\sqrt{n \\tilde{\\sigma}_n^2}} = \\frac{n(\\bar{X}_n - \\bar{\\mu}_n)}{\\sqrt{n} \\tilde{\\sigma}_n} = \\frac{\\sqrt{n}(\\bar{X}_n - \\bar{\\mu}_n)}{\\tilde{\\sigma}_n} \\xrightarrow{d} N(0, 1)\n$$\n\nThis implies the following **approximate distributions** for large $n$:\n\n1.  **For the Sample Mean:**\n\n    $$\n    \\bar{X}_n \\overset{\\cdot}{\\sim} N\\left(\\bar{\\mu}_n, \\frac{\\tilde{\\sigma}_n^2}{n}\\right)\n    $$\n\n2.  **For the Scaled Difference (Root-n consistency):**\n\n    $$\n    \\sqrt{n}(\\bar{X}_n - \\bar{\\mu}_n) \\overset{\\cdot}{\\sim} N\\left(0, \\tilde{\\sigma}_n^2\\right)\n    $$\n\n*Note: If all $X_i$ share the same mean $\\mu$, simply replace $\\bar{\\mu}_n$ with $\\mu$.*\n\n:::\n\n::: {#thm-slutsky}\n\n### Slutsky's Theorem\nLet $X_n$ and $Y_n$ be sequences of random variables. If $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{p} c$, where $c$ is a constant, then:\n\n1.  **Sum:** $X_n + Y_n \\xrightarrow{d} X + c$\n\n2.  **Product:** $X_n Y_n \\xrightarrow{d} cX$\n\n3.  **Quotient:** $X_n / Y_n \\xrightarrow{d} X/c$ (provided $c \\ne 0$)\n\n:::\n\n::: {#thm-generalized-slutsky}\n\n### Generalized Slutsky's Theorem (Continuous Mapping)\n\nThe arithmetic operations in Slutsky's theorem are special cases of a broader property.\n\nLet $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{p} c$, where $c$ is a constant.\nLet $g: \\mathbb{R}^2 \\to \\mathbb{R}^k$ be a function that is **continuous** at every point $(x, c)$ where $x$ is in the support of $X$.\n\nThen:\n$$\ng(X_n, Y_n) \\xrightarrow{d} g(X, c)\n$$\n\nThis implies that for any \"well-behaved\" algebraic combination (polynomials, exponentials, etc.) of a sequence converging in distribution and a sequence converging in probability to a constant, the limit behaves as if the constant were substituted directly.\n\n:::\n\n::: {#exm-asymptotic-normal-variance}\n\n### Asymptotic Normality of Sample Variance\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. We wish to derive the asymptotic distribution of the MLE for variance, \n\n$$\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2.$$\n\n1. Algebraic Expansion\n\n    We rewrite the estimator by adding and subtracting the true mean $\\mu$:\n    $$\n    \\sum_{i=1}^n (X_i - \\bar{X})^2 = \\sum_{i=1}^n ((X_i - \\mu) - (\\bar{X} - \\mu))^2\n    $$\n    $$\n    = \\sum_{i=1}^n (X_i - \\mu)^2 - 2(\\bar{X} - \\mu)\\sum_{i=1}^n(X_i - \\mu) + n(\\bar{X} - \\mu)^2\n    $$\n    Since $\\sum(X_i - \\mu) = n(\\bar{X} - \\mu)$:\n    $$\n    = \\sum_{i=1}^n (X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2\n    $$\n    Dividing by $n$, we get:\n    $$\n    \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - (\\bar{X} - \\mu)^2\n    $$\n\n2. Scaling by $\\sqrt{n}$\n\n    We rearrange to look at the pivotal quantity $\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2)$:\n    $$\n    \\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) = \\sqrt{n}\\left( \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\sigma^2 \\right) - \\sqrt{n}(\\bar{X} - \\mu)^2\n    $$\n\n3. Applying Convergence Theorems\n\n\n    * **Term 1 (CLT):**\n    \n        Let $W_i = (X_i - \\mu)^2$. Since $X_i \\sim N(\\mu, \\sigma^2)$, $W_i/\\sigma^2 \\sim \\chi^2_1$.\n        moments of $W_i$: $E[W_i] = \\sigma^2$ and $\\text{Var}(W_i) = 2\\sigma^4$.\n        By the standard CLT:\n        $$\n        \\sqrt{n}(\\bar{W} - E[W]) = \\sqrt{n}\\left( \\frac{1}{n}\\sum (X_i - \\mu)^2 - \\sigma^2 \\right) \\xrightarrow{d} N(0, 2\\sigma^4)\n        $$\n\n    * **Term 2 (Slutsky):**\n    \n        Consider the term $\\sqrt{n}(\\bar{X} - \\mu)^2$. We can rewrite this as:\n        $$\n        \\underbrace{\\sqrt{n}(\\bar{X} - \\mu)}_{\\xrightarrow{d} N(0, \\sigma^2)} \\cdot \\underbrace{(\\bar{X} - \\mu)}_{\\xrightarrow{p} 0 \\text{ by LLN}}\n        $$\n        By Slutsky's Theorem, the product converges to $Z \\cdot 0 = 0$.\n\n**Conclusion:**\n\nCombining the terms:\n$$\n\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) \\xrightarrow{d} N(0, 2\\sigma^4) - 0 = N(0, 2\\sigma^4)\n$$\n\n:::\n\n## Asymptotic Theory of Maximum Likelihood\n\n### Consistency of the MLE\n\n#### IID Cases\n\nConsistency establishes that as the sample size grows, the estimator converges in probability to the true parameter value.\n\n::: {#thm-mle-consistency}\n\n#### Consistency of MLE\nLet $Y_1, \\dots, Y_n$ be i.i.d. with density $f(y|\\boldsymbol{\\theta})$. Let $\\boldsymbol{\\theta}_0$ be the true parameter. Under regularity conditions (specifically identifiability and compactness), the Maximum Likelihood Estimator $\\hat{\\boldsymbol{\\theta}}_n$ satisfies:\n$$\n\\hat{\\boldsymbol{\\theta}}_n \\xrightarrow{p} \\boldsymbol{\\theta}_0 \\quad \\text{as } n \\to \\infty\n$$\n\n:::\n\n::: {.proof}\n\n1. **The Objective Function**\n\n   The MLE maximizes the average log-likelihood:\n   $$\n   M_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\log f(Y_i|\\boldsymbol{\\theta})\n   $$\n   By the **Weak Law of Large Numbers (WLLN)**, for any fixed $\\boldsymbol{\\theta}$, $M_n(\\boldsymbol{\\theta})$ converges in probability to its expectation:\n   $$\n   M_n(\\boldsymbol{\\theta}) \\xrightarrow{p} M(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}_0} [\\log f(Y|\\boldsymbol{\\theta})]\n   $$\n\n2. **Identifying the Maximum (Information Inequality)**\n\n   We compare the expected log-likelihood at an arbitrary $\\boldsymbol{\\theta}$ versus the true $\\boldsymbol{\\theta}_0$. Consider the difference:\n   $$\n   M(\\boldsymbol{\\theta}) - M(\\boldsymbol{\\theta}_0) = E_{\\boldsymbol{\\theta}_0} \\left[ \\log f(Y|\\boldsymbol{\\theta}) - \\log f(Y|\\boldsymbol{\\theta}_0) \\right] = E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right) \\right]\n   $$\n   By **Jensen's Inequality** (since $\\log(y)$ is strictly concave), $E[\\log Z] \\le \\log E[Z]$.\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right) \\right] \\le \\log E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right]\n   $$\n   Evaluating the expectation on the right:\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right] = \\int \\left( \\frac{f(y|\\boldsymbol{\\theta})}{f(y|\\boldsymbol{\\theta}_0)} \\right) f(y|\\boldsymbol{\\theta}_0) \\, dy = \\int f(y|\\boldsymbol{\\theta}) \\, dy = 1\n   $$\n   Therefore:\n   $$\n   M(\\boldsymbol{\\theta}) - M(\\boldsymbol{\\theta}_0) \\le \\log(1) = 0 \\implies M(\\boldsymbol{\\theta}) \\le M(\\boldsymbol{\\theta}_0)\n   $$\n   Assuming the model is **identifiable** (i.e., $f(y|\\boldsymbol{\\theta}) = f(y|\\boldsymbol{\\theta}_0) \\iff \\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0$), the maximum is unique at $\\boldsymbol{\\theta}_0$. Since $\\hat{\\boldsymbol{\\theta}}_n$ maximizes $M_n(\\boldsymbol{\\theta})$, which converges to $M(\\boldsymbol{\\theta})$, $\\hat{\\boldsymbol{\\theta}}_n$ must converge to $\\boldsymbol{\\theta}_0$.\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Consistency: As n increases, the log-likelihood function concentrates around the true parameter.](mle_files/figure-html/fig-likelihood-consistency-1.png){#fig-likelihood-consistency fig-align='center' width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Non-IID Cases\n\nIn regression settings, observations are independent but not identically distributed (i.n.i.d.). We generalize the consistency result by requiring that the \"average\" information accumulates sufficiently.\n\n::: {#thm-mle-consistency-general}\n\n#### Consistency of MLE (General Case)\nLet $Y_1, \\dots, Y_n$ be independent observations with densities $f_i(y|\\boldsymbol{\\theta})$ (e.g., depending on covariates $x_i$). Let $\\boldsymbol{\\theta}_0$ be the true parameter.\n\nUnder the following conditions:\n\n1.  **Parameter Space:** Compact parameter space $\\Theta$.\n\n2.  **Identification:** For any $\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0$, the average Kullback-Leibler divergence is strictly positive in the limit:\n\n    $$\\liminf_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\text{KL}(f_i(\\cdot|\\boldsymbol{\\theta}_0) || f_i(\\cdot|\\boldsymbol{\\theta})) > 0$$\n\n3.  **Uniform Convergence:** The log-likelihood satisfies a Uniform Law of Large Numbers (ULLN).\n\nThen $\\hat{\\boldsymbol{\\theta}}_n \\xrightarrow{p} \\boldsymbol{\\theta}_0$.\n\n:::\n\n::: {.proof}\n\n1. **The Objective Function**\n\n   The MLE maximizes the average log-likelihood:\n   $$\n   M_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\log f_i(Y_i|\\boldsymbol{\\theta})\n   $$\n   Unlike the i.i.d. case, the terms in the sum have different expectations. We rely on a **WLLN for Independent Variables** (e.g., Chebyshev's WLLN). Provided the variances are bounded, $M_n(\\boldsymbol{\\theta})$ converges in probability to the **average expectation**:\n   $$\n   M_n(\\boldsymbol{\\theta}) \\xrightarrow{p} \\bar{M}_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n E_{\\boldsymbol{\\theta}_0} [\\log f_i(Y_i|\\boldsymbol{\\theta})]\n   $$\n   *Note: For consistency of the maximizer, we strictly require **Uniform Convergence** over $\\Theta$, not just pointwise convergence.*\n\n2. **Identifying the Maximum (Average KL Divergence)**\n\n   We compare the limit function at $\\boldsymbol{\\theta}$ versus $\\boldsymbol{\\theta}_0$. Consider the average difference:\n   $$\n   \\bar{M}_n(\\boldsymbol{\\theta}) - \\bar{M}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\sum_{i=1}^n E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right) \\right]\n   $$\n   By Jensen's Inequality applied to *each* term in the sum:\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right] \\le \\log E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right] = \\log(1) = 0\n   $$\n   Summing these inequalities implies that $\\bar{M}_n(\\boldsymbol{\\theta})$ is uniquely maximized at $\\boldsymbol{\\theta}_0$ (provided the identification condition holds). Since the sample function $M_n(\\boldsymbol{\\theta})$ converges uniformly to this maximized limit function, the argmax $\\hat{\\boldsymbol{\\theta}}_n$ converges to $\\boldsymbol{\\theta}_0$.\n\n:::\n\n### Asymptotic Normality of the Score Vector\n\nThe Score function acts as the \"engine\" for the normality of the MLE. We treat the I.I.D. and non-I.I.D. cases separately.\n\n#### IID Cases\n\n::: {#thm-score-normality-iid}\n\n#### Normality of Score (I.I.D.)\nLet $Y_1, \\dots, Y_n$ be i.i.d. with density $f(y|\\boldsymbol{\\theta})$.\nDefine the **Fisher Information matrix** for a single observation as the expected outer product of the score:\n$$\n\\mathbf{I}_1(\\boldsymbol{\\theta}) = E\\left[ \\left( \\nabla_{\\boldsymbol{\\theta}} \\log f(Y_1|\\boldsymbol{\\theta}) \\right) \\left( \\nabla_{\\boldsymbol{\\theta}} \\log f(Y_1|\\boldsymbol{\\theta}) \\right)^T \\right]\n$$\nThen, the scaled total score vector converges to a Normal distribution:\n$$\n\\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\xrightarrow{d} N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n$$\n\n:::\n\n::: {.proof}\nThe total score is the sum of independent score contributions:\n$$\n\\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\sum_{i=1}^n \\nabla \\log f(Y_i|\\boldsymbol{\\theta}_0) = \\sum_{i=1}^n \\mathbf{u}_i\n$$\nFrom **Bartlett's Identities**, for each observation $i$:\n\n1. **Mean**\n\n   $E[\\mathbf{u}_i] = \\mathbf{0}$.\n\n2. **Variance**\n\n   $\\text{Var}(\\mathbf{u}_i) = E[\\mathbf{u}_i \\mathbf{u}_i^T] = \\mathbf{I}_1(\\boldsymbol{\\theta}_0)$.\n\nSince the terms $\\mathbf{u}_i$ are i.i.d. with finite variance, we apply the **Multivariate Central Limit Theorem (Lindeberg-LÃ©vy)**:\n$$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\mathbf{u}_i \\xrightarrow{d} N(\\mathbf{0}, \\text{Var}(\\mathbf{u}_i)) = N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n$$\n\n:::\n\n#### Case B: The Non-I.I.D. Case (Regression Setting)\n\n::: {#thm-score-normality-noniid}\n\n#### Normality of Score (Independent, Non-Identical)\nLet $Y_1, \\dots, Y_n$ be independent but not necessarily identically distributed (e.g., due to different covariates). Let $\\mathbf{I}_n(\\boldsymbol{\\theta}_0) = \\sum_{i=1}^n \\text{Var}(\\mathbf{u}_i)$ be the total information.\n\nDefine the **Average Fisher Information Matrix**:\n$$\n\\bar{\\mathbf{I}}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\mathbf{I}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\sum_{i=1}^n E[\\mathbf{u}_i \\mathbf{u}_i^T]\n$$\n\nIf the **Lindeberg Condition** is satisfied for the sequence of score vectors, then the standardized score converges to a standard Normal:\n$$\n\\left( \\mathbf{I}_n(\\boldsymbol{\\theta}_0) \\right)^{-1/2} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\xrightarrow{d} N(\\mathbf{0}, \\mathbf{I}_p)\n$$\n\n**Approximating Distribution:**\nIf the average information converges to a positive definite limit $\\bar{\\mathbf{I}}_n \\to \\mathbf{I}_{\\infty}$, we have the following approximation for the scaled average score $\\sqrt{n} \\bar{\\mathbf{U}}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y})$:\n$$\n\\sqrt{n} \\bar{\\mathbf{U}}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\overset{\\cdot}{\\sim} N(\\mathbf{0}, \\bar{\\mathbf{I}}_n(\\boldsymbol{\\theta}_0))\n$$\n\n:::\n\n::: {.proof}\nThis is a direct application of the **Lindeberg-Feller Central Limit Theorem**.\nThe total score $\\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\sum \\mathbf{u}_i$ is a sum of independent random vectors with mean $\\mathbf{0}$ and variances $\\text{Var}(\\mathbf{u}_i)$.\nProvided that no single observation's score contribution dominates the sum (the Lindeberg condition), the standardized sum converges to a standard Normal distribution.\n\n:::\n\n### Asymptotic Normality of the MLE\n\nWe now transfer the normality from the Score function to the estimator $\\hat{\\boldsymbol{\\theta}}$ using a Taylor expansion (the Delta Method logic).\n\n::: {#def-regularity-conditions}\n\n#### Regularity Conditions for Asymptotic Normality\nThe following conditions are required to ensure the validity of the Taylor expansion and the convergence of the remainder term:\n\n1.  **Interiority:** The true parameter $\\boldsymbol{\\theta}_0$ lies in the interior of the parameter space $\\Theta$.\n\n2.  **Smoothness:** The log-likelihood function $\\log f(y|\\boldsymbol{\\theta})$ is three times continuously differentiable with respect to $\\boldsymbol{\\theta}$.\n\n3.  **Boundedness:** The third derivatives are bounded by an integrable function $M(y)$ (to control the Taylor remainder).\n\n4.  **Positive Information:** The Fisher Information Matrix $\\mathbf{I}_1(\\boldsymbol{\\theta}_0)$ exists and is non-singular (positive definite).\n\n5.  **Interchangeability:** Differentiation and integration can be interchanged for the density $f(y|\\boldsymbol{\\theta})$.\n\n:::\n\n::: {#thm-mle-normality}\n\n#### Asymptotic Normality of MLE\n\nUnder the regularity conditions above, the MLE $\\hat{\\boldsymbol{\\theta}}_n$ satisfies:\n$$\n\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} N\\left(\\mathbf{0}, [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1}\\right)\n$$\n\n:::\n\n::: {.proof}\n\n1. **Taylor Expansion of Score Equation**\n\n   We expand the Score function $\\mathbf{U}_n(\\boldsymbol{\\theta})$ around the true parameter $\\boldsymbol{\\theta}_0$. Since $\\hat{\\boldsymbol{\\theta}}_n$ is the MLE, $\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}_n) = \\mathbf{0}$.\n   \n   Define the **Observed Information Matrix** as the negative Hessian:\n   $$\n   \\mathbf{J}_n(\\boldsymbol{\\theta}) = -\\nabla^2 \\ell_n(\\boldsymbol{\\theta})\n   $$\n\n   ::: {.callout-note appearance=\"simple\"}\n   **The Linear Link (Score vs. Parameter Error)**\n   The fundamental approximation linking the random Score vector to the estimation error is:\n   $$\n   \\underbrace{\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}_n)}_{=\\mathbf{0}} \\approx \\mathbf{U}_n(\\boldsymbol{\\theta}_0) - \\mathbf{J}_n(\\boldsymbol{\\theta}_0) (\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   $$\n   \\implies \\quad \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\approx \\mathbf{J}_n(\\boldsymbol{\\theta}_0) (\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   :::\n\n2. **Scaling and Inversion**\n\n   Multiply the link equation by $\\frac{1}{\\sqrt{n}}$ and introduce $n$ to the Observed Information term to stabilize the limits:\n   $$\n   \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\approx \\left( \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) \\right) \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   Rearranging to solve for the estimator's distribution:\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\approx \\left[ \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) \\right]^{-1} \\left[ \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\right]\n   $$\n\n3. **Convergence of Components**\n\n   * **Numerator (Score):** From the I.I.D. Score Normality theorem:\n       $$\n       \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\xrightarrow{d} Z \\sim N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n       $$\n\n   * **Denominator (Observed Info):** By the **WLLN**, the average observed information converges to the expected information:\n       $$\n       \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) = -\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\log f(Y_i|\\boldsymbol{\\theta}_0) \\xrightarrow{p} -E[\\nabla^2 \\log f] = \\mathbf{I}_1(\\boldsymbol{\\theta}_0)\n       $$\n\n4. **Slutsky's Theorem**\n\n   Combining these via Slutsky's Theorem (matrix inverse is a continuous mapping):\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1} Z\n   $$\n   The variance of this limiting distribution is:\n   $$\n   \\text{Var}([\\mathbf{I}_1]^{-1} Z) = \\mathbf{I}_1^{-1} \\text{Var}(Z) (\\mathbf{I}_1^{-1})^T = \\mathbf{I}_1^{-1} \\mathbf{I}_1 \\mathbf{I}_1^{-1} = \\mathbf{I}_1^{-1}\n   $$\n   Thus:\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} N\\left( \\mathbf{0}, [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1} \\right)\n   $$\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# --- 1. Settings and Data Generation (Cauchy) ---\nset.seed(42)\nn_sim <- 10\nn_obs <- 10\ntheta_0 <- 10 \ngamma <- 1    \n\n# Generate Datasets\ndatasets <- matrix(rcauchy(n_sim * n_obs, location = theta_0, scale = gamma), \n                   nrow = n_sim, ncol = n_obs)\n\n# Define Score and Hessian Functions\nscore_cauchy <- function(y, theta) {\n  sum( (2 * (y - theta)) / (1 + (y - theta)^2) )\n}\nhessian_cauchy <- function(y, theta) {\n  u <- y - theta\n  sum( (2 * (u^2 - 1)) / (1 + u^2)^2 )\n}\n\n# --- 2. Analyze Main Dataset ---\ny_main <- datasets[1, ]\n\n# Find MLE numerically\nmle_res <- optimize(function(th) sum(dcauchy(y_main, location = th, log = TRUE)),\n                    interval = c(theta_0 - 5, theta_0 + 5), maximum = TRUE)\ntheta_hat <- mle_res$maximum\n\n# Calculate Geometry at theta_0\nu_true <- score_cauchy(y_main, theta_0)\nslope_true <- hessian_cauchy(y_main, theta_0)\n\n# --- 3. Prepare Plot Data ---\ntheta_seq <- seq(8, 12, length.out = 300)\n\n# Curves Data\nplot_data <- data.frame()\nfor(i in 1:n_sim) {\n  y_curr <- datasets[i, ]\n  u_vals <- sapply(theta_seq, function(th) score_cauchy(y_curr, th))\n  plot_data <- rbind(plot_data, data.frame(theta = theta_seq, U = u_vals, sim_id = as.factor(i), is_main = (i == 1)))\n}\n\n# Linear Approximation Data (Tangent Line)\ntangent_data <- data.frame(\n  theta = theta_seq,\n  U_lin = u_true + slope_true * (theta_seq - theta_0)\n)\n\n# --- 4. Plotting ---\nggplot() +\n  # Axes Reference\n  geom_hline(yintercept = 0, color = \"black\", size = 0.5) +\n  geom_vline(xintercept = theta_0, color = \"black\", linetype = \"dotted\") +\n  \n  # A. Background Curves (Grey)\n  geom_line(data = subset(plot_data, !is_main), \n            aes(x = theta, y = U, group = sim_id), \n            color = \"grey80\", alpha = 0.5) +\n  \n  # B. Main Curve (Blue) & Tangent (Red Dashed)\n  geom_line(data = subset(plot_data, is_main), aes(x = theta, y = U), \n            color = \"blue\", size = 1.2) +\n  geom_line(data = tangent_data, aes(x = theta, y = U_lin), \n            color = \"red\", linetype = \"dashed\", size = 1) +\n  \n  # C. THICK SEGMENTS for Delta U and Delta Theta\n  # Vertical Segment (Delta U): From axis to curve at theta_0\n  # Color: Purple\n  annotate(\"segment\", x = theta_0, xend = theta_0, y = 0, yend = u_true,\n           color = \"purple\", size = 2, alpha = 0.8) +\n  \n  # Horizontal Segment (Delta Theta): From theta_0 to theta_hat along the axis\n  # Color: Dark Green\n  annotate(\"segment\", x = theta_0, xend = theta_hat, y = 0, yend = 0,\n           color = \"darkgreen\", size = 2, alpha = 0.8) +\n  \n  # D. Points\n  geom_point(aes(x = theta_0, y = u_true), color = \"red\", size = 3) +\n  geom_point(aes(x = theta_hat, y = 0), color = \"darkgreen\", size = 3) +\n  \n  # E. Labels (Delta labels)\n  annotate(\"text\", x = theta_0 + 0.15, y = u_true / 2, \n           label = \"Delta~U\", parse = TRUE, color = \"purple\", fontface = \"bold\", hjust = 0) +\n  \n  annotate(\"text\", x = (theta_0 + theta_hat)/2, y = -1.0, \n           label = \"hat(theta)[n] - theta[0]\", parse = TRUE, color = \"darkgreen\", fontface = \"bold\") +\n  \n  # F. Parameter Labels (Revised Position)\n  annotate(\"text\", x = theta_hat, y = 1, label = \"hat(theta)[n]\", parse = TRUE, color = \"darkgreen\", size = 5) +\n  annotate(\"text\", x = theta_0, y = 1, label = \"theta[0]\", parse = TRUE, color = \"red\", size = 5) +\n\n  # G. Styling\n  coord_cartesian(ylim = c(-10, 10), xlim = c(8.5, 11.5)) +\n  labs(title = expression(paste(\"Score Function Approximation (Cauchy)\")),\n       subtitle = expression(paste(\"Linear Link: \", Delta, \"U \", approx, \" -J \", Delta, \"theta\")),\n       x = expression(theta), y = expression(U(theta))) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Linear Approximation of the Score Function: The vertical purple segment represents the Score at the true parameter ($\\Delta U$), while the horizontal green segment shows the estimation error ($\\hat{\\theta}_n - \\theta_0$). The red dashed line indicates the linear approximation ($-J(\\theta_0)$).](mle_files/figure-html/fig-score-approx-1.png){#fig-score-approx width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n### Wilks' Theorem \n\n#### Wilks' Theorem (Simple Null Hypothesis)\n\n::: {#thm-wilks-simple}\n\n#### Wilks' Theorem (Simple Null Hypothesis)\nConsider testing $H_0: \\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0$ against $H_1: \\boldsymbol{\\theta} \\ne \\boldsymbol{\\theta}_0$.\n\nDefine the **Likelihood Ratio** $\\Lambda_n$ as:\n$$\n\\Lambda_n = \\frac{L(\\boldsymbol{\\theta}_0)}{L(\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}})}\n$$\n\nDefine the **Deviance Statistic** $D_n$ as:\n$$\nD_n = -2 \\log \\Lambda_n = 2 [\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}_0)]\n$$\n\nUnder the null hypothesis, the Deviance converges to a Chi-squared distribution:\n$$\nD_n \\xrightarrow{d} \\chi^2_p\n$$\nwhere $p$ is the dimension of $\\boldsymbol{\\theta}$.\n\n:::\n\n::: {.proof}\nWe express the likelihood difference $\\Delta \\ell$ in terms of the Score vector $\\mathbf{U}_n(\\boldsymbol{\\theta}_0)$ and apply the Central Limit Theorem directly to the Score.\n\n1.  **Quadratic Approximation of Deviance**\n\n    Expand $\\ell_n(\\boldsymbol{\\theta}_0)$ around the MLE $\\hat{\\boldsymbol{\\theta}}$. Since the gradient at the MLE is zero ($\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}) = \\mathbf{0}$), the first-order term vanishes:\n    $$\n    \\ell_n(\\boldsymbol{\\theta}_0) \\approx \\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\mathbf{J}_n(\\hat{\\boldsymbol{\\theta}}) (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n    $$\n    Rearranging for the Deviance ($D_n$):\n    $$\n    D_n = 2[\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}_0)] \\approx (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\mathbf{J}_n(\\hat{\\boldsymbol{\\theta}}) (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n    $$\n\n2.  **Substituting the Score**\n\n    Recall the linear link we established in the Normality proof: $\\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\approx \\mathbf{J}_n(\\boldsymbol{\\theta}_0)(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)$.\n    Inverting this relationship gives the parameter error in terms of the Score:\n    $$\n    (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0) \\approx \\mathbf{J}_n^{-1} \\mathbf{U}_n(\\boldsymbol{\\theta}_0)\n    $$\n    Substitute this back into the Deviance equation:\n    $$\n    D_n \\approx (\\mathbf{J}_n^{-1} \\mathbf{U}_n)^T \\mathbf{J}_n (\\mathbf{J}_n^{-1} \\mathbf{U}_n)\n    $$\n    $$\n    \\boxed{D_n \\approx \\mathbf{U}_n(\\boldsymbol{\\theta}_0)^T \\mathbf{J}_n^{-1} \\mathbf{U}_n(\\boldsymbol{\\theta}_0)}\n    $$\n    This form (the Score Statistic) shows that the deviance is essentially the squared length of the Score vector, standardized by the Information.\n\n3.  **Asymptotic Convergence**\n\n    We rewrite the expression using normalized quantities to apply the limit theorems.\n\n    * **Score:** $\\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\xrightarrow{d} \\mathbf{Z} \\sim N(\\mathbf{0}, \\mathbf{I}_1)$.\n    * **Information:** $\\frac{1}{n} \\mathbf{J}_n \\xrightarrow{p} \\mathbf{I}_1$.\n\n    $$\n    D_n \\approx \\left( \\frac{1}{\\sqrt{n}} \\mathbf{U}_n \\right)^T \\left( \\frac{1}{n} \\mathbf{J}_n \\right)^{-1} \\left( \\frac{1}{\\sqrt{n}} \\mathbf{U}_n \\right)\n    $$\n    Taking the limit:\n    $$\n    D_n \\xrightarrow{d} \\mathbf{Z}^T \\mathbf{I}_1^{-1} \\mathbf{Z}\n    $$\n    Since $\\mathbf{Z} \\sim N(\\mathbf{0}, \\mathbf{I}_1)$, the quadratic form follows a Chi-squared distribution:\n    $$\n    \\mathbf{Z}^T \\mathbf{I}_1^{-1} \\mathbf{Z} \\sim \\chi^2_p\n    $$\n\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(latex2exp)\n\n# --- 1. Settings and Data Generation (Cauchy) ---\nset.seed(42)\nn_sim <- 10\nn_obs <- 10\ntheta_0 <- 10 \ngamma <- 1    \n\ndatasets <- matrix(rcauchy(n_sim * n_obs, location = theta_0, scale = gamma), \n                   nrow = n_sim, ncol = n_obs)\n\nscore_cauchy <- function(y, theta) {\n  sum( (2 * (y - theta)) / (1 + (y - theta)^2) )\n}\nhessian_cauchy <- function(y, theta) {\n  u <- y - theta\n  sum( (2 * (u^2 - 1)) / (1 + u^2)^2 )\n}\n\n# --- 2. Analyze Main Dataset ---\ny_main <- datasets[1, ]\nmle_res <- optimize(function(th) sum(dcauchy(y_main, location = th, log = TRUE)),\n                    interval = c(theta_0 - 5, theta_0 + 5), maximum = TRUE)\ntheta_hat <- mle_res$maximum\n\nu_true <- score_cauchy(y_main, theta_0)\nslope_true <- hessian_cauchy(y_main, theta_0)\n\n# --- 3. Prepare Plot Data ---\ntheta_seq <- seq(8, 12, length.out = 300)\n\nplot_data <- data.frame()\nfor(i in 1:n_sim) {\n  y_curr <- datasets[i, ]\n  u_vals <- sapply(theta_seq, function(th) score_cauchy(y_curr, th))\n  plot_data <- rbind(plot_data, data.frame(theta = theta_seq, U = u_vals, sim_id = as.factor(i), is_main = (i == 1)))\n}\n\ntangent_data <- data.frame(\n  theta = theta_seq,\n  U_lin = u_true + slope_true * (theta_seq - theta_0)\n)\n\n# --- 4. Plotting ---\nggplot() +\n  geom_hline(yintercept = 0, color = \"black\", size = 0.5) +\n  geom_vline(xintercept = theta_0, color = \"black\", linetype = \"dotted\") +\n  \n  # A. Background Curves\n  geom_line(data = subset(plot_data, !is_main), \n            aes(x = theta, y = U, group = sim_id), \n            color = \"grey80\", alpha = 0.5) +\n  \n  # B. RECTANGLE FILL\n  annotate(\"rect\", xmin = theta_0, xmax = theta_hat, ymin = 0, ymax = u_true,\n           fill = \"darkblue\", alpha = 0.2) +\n  \n  # C. Main Curve & Tangent\n  geom_line(data = subset(plot_data, is_main), aes(x = theta, y = U), \n            color = \"blue\", size = 1.2) +\n  geom_line(data = tangent_data, aes(x = theta, y = U_lin), \n            color = \"red\", linetype = \"dashed\", size = 1) +\n  \n  # D. THICK SEGMENTS\n  annotate(\"segment\", x = theta_0, xend = theta_0, y = 0, yend = u_true,\n           color = \"purple\", size = 1.5) +\n  annotate(\"segment\", x = theta_0, xend = theta_hat, y = 0, yend = 0,\n           color = \"darkgreen\", size = 1.5) +\n  \n  # E. Points\n  geom_point(aes(x = theta_0, y = u_true), color = \"red\", size = 3) +\n  geom_point(aes(x = theta_hat, y = 0), color = \"darkgreen\", size = 3) +\n  \n  # F. Labels\n  annotate(\"text\", x = theta_0 + 0.15, y = u_true / 2, \n           label = TeX(\"$\\\\Delta U$\"), color = \"purple\", fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = (theta_0 + theta_hat)/2, y = -1.0, \n           label = TeX(\"$\\\\Delta \\\\theta$\"), color = \"darkgreen\", fontface = \"bold\") +\n  annotate(\"text\", x = theta_hat, y = 1.5, label = TeX(\"$\\\\hat{\\\\theta}_n$\"), color = \"darkgreen\", size = 5) +\n  annotate(\"text\", x = theta_0, y = 1.5, label = TeX(\"$\\\\theta_0$\"), color = \"red\", size = 5) +\n\n  # G. ANNOTATIONS (Left Aligned)\n  \n  # 1. Integral Definition (Top)\n  annotate(\"text\", x = 9.8, y = 8, \n           label = TeX(r\"($2 \\Delta l =2 \\int_{\\theta_0}^{\\hat{\\theta}} U(\\theta)d\\theta \\approx \\Delta U \\times \\Delta \\theta$)\"), \n           size = 4.5, color = \"darkblue\", hjust = 0) + # hjust=0 ensures left alignment\n\n  # 2. Deviance Formula (Bottom)\n  annotate(\"text\", x = 9.8, y = 6, \n           label = TeX(r\"($D_n = 2\\Delta l \\approx \\Delta \\theta \\cdot \\Delta U \\approx (\\Delta U)^2 J^{-1}$)\"), \n           size = 4.5, color = \"black\", hjust = 0) +\n\n  # H. Styling\n  coord_cartesian(ylim = c(-10, 10), xlim = c(8.5, 11.5)) +\n  labs(title = TeX(\"Score Function Approximation (Cauchy)\"),\n       subtitle = TeX(\"Linear Link: $\\\\Delta U \\\\approx -J \\\\Delta \\\\theta$\"),\n       x = TeX(\"$\\\\theta$\"), y = TeX(\"$U(\\\\theta)$\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Geometric Interpretation of the Score Statistic: The shaded rectangle represents the product $\\Delta \\theta \\times \\Delta U$. The annotations highlight that the Deviance $D_n$ is approximately equal to this area.](mle_files/figure-html/fig-score-approx-latex-final-1.png){#fig-score-approx-latex-final width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Generalized Wilks' Theorem (Parameter Partitioning)\n\n\n::: {#lem-score-decomp}\n**Quadratic Form Decomposition of the Score**\n\nLet $\\mathbf{U}$ be a partitioned vector and $\\mathbf{J}$ be a partitioned symmetric positive-definite matrix:\n$$\n\\mathbf{U} = \\begin{pmatrix} \\mathbf{U}_{\\psi} \\\\ \\mathbf{U}_{\\lambda} \\end{pmatrix}, \\quad\n\\mathbf{J} = \\begin{pmatrix} \\mathbf{J}_{\\psi\\psi} & \\mathbf{J}_{\\psi\\lambda} \\\\ \\mathbf{J}_{\\lambda\\psi} & \\mathbf{J}_{\\lambda\\lambda} \\end{pmatrix}\n$$\nThe quadratic form $\\mathbf{U}^T \\mathbf{J}^{-1} \\mathbf{U}$ can be decomposed into the sum of a marginal term (involving only $\\lambda$) and a conditional term (involving $\\psi$ adjusted for $\\lambda$):\n$$\n\\mathbf{U}^T \\mathbf{J}^{-1} \\mathbf{U} = \\mathbf{U}_{\\lambda}^T \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda} + \\tilde{\\mathbf{U}}_{\\psi}^T (\\mathbf{J}^{\\psi\\psi}) \\tilde{\\mathbf{U}}_{\\psi}\n$$\nwhere:\n\n1.  $\\mathbf{J}^{\\psi\\psi} = (\\mathbf{J}_{\\psi\\psi} - \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{J}_{\\lambda\\psi})^{-1}$ is the top-left block of the inverse matrix $\\mathbf{J}^{-1}$ (the inverse of the Schur Complement).\n\n2.  $\\tilde{\\mathbf{U}}_{\\psi} = \\mathbf{U}_{\\psi} - \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda}$ is the \"effective\" score for $\\psi$, orthogonalized with respect to $\\lambda$.\n\n:::\n\n::: {.proof}\n\n1. Block Matrix Inversion\n\nUsing the Banachiewicz inversion formula, the blocks of the inverse matrix $\\mathbf{J}^{-1}$ are expressed in terms of the Schur complement $S = \\mathbf{J}_{\\psi\\psi} - \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{J}_{\\lambda\\psi}$:\n\n$$\n\\mathbf{J}^{-1} = \\begin{pmatrix} \\mathbf{J}^{\\psi\\psi} & \\mathbf{J}^{\\psi\\lambda} \\\\ \\mathbf{J}^{\\lambda\\psi} & \\mathbf{J}^{\\lambda\\lambda} \\end{pmatrix} = \\begin{pmatrix} S^{-1} & -S^{-1} \\mathbf{K} \\\\ -\\mathbf{K}^T S^{-1} & \\mathbf{J}_{\\lambda\\lambda}^{-1} + \\mathbf{K}^T S^{-1} \\mathbf{K} \\end{pmatrix}\n$$\nwhere we define the projection operator $\\mathbf{K} = \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1}$ for brevity.\n\n2. Algebraic Expansion\n\nExpand the quadratic form $\\mathbf{U}^T \\mathbf{J}^{-1} \\mathbf{U}$ using these blocks:\n$$\n\\begin{aligned}\n\\mathbf{U}^T \\mathbf{J}^{-1} \\mathbf{U} &= \\mathbf{U}_{\\psi}^T S^{-1} \\mathbf{U}_{\\psi} + 2\\mathbf{U}_{\\psi}^T (-S^{-1} \\mathbf{K}) \\mathbf{U}_{\\lambda} + \\mathbf{U}_{\\lambda}^T (\\mathbf{J}_{\\lambda\\lambda}^{-1} + \\mathbf{K}^T S^{-1} \\mathbf{K}) \\mathbf{U}_{\\lambda}\n\\end{aligned}\n$$\n\n3. Regrouping\n\nSeparate the term that does not involve $S^{-1}$:\n$$\n\\mathbf{U}^T \\mathbf{J}^{-1} \\mathbf{U} = \\mathbf{U}_{\\lambda}^T \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda} + \\left[ \\mathbf{U}_{\\psi}^T S^{-1} \\mathbf{U}_{\\psi} - 2\\mathbf{U}_{\\psi}^T S^{-1} \\mathbf{K} \\mathbf{U}_{\\lambda} + \\mathbf{U}_{\\lambda}^T \\mathbf{K}^T S^{-1} \\mathbf{K} \\mathbf{U}_{\\lambda} \\right]\n$$\n\n4. Completing the Square\n\nThe term in the brackets is a perfect quadratic form $(\\mathbf{a} - \\mathbf{b})^T S^{-1} (\\mathbf{a} - \\mathbf{b})$:\n$$\n\\left[ \\dots \\right] = (\\mathbf{U}_{\\psi} - \\mathbf{K} \\mathbf{U}_{\\lambda})^T S^{-1} (\\mathbf{U}_{\\psi} - \\mathbf{K} \\mathbf{U}_{\\lambda})\n$$\nSubstituting back the definitions of $\\mathbf{K}$ and $\\tilde{\\mathbf{U}}_{\\psi}$:\n$$\n(\\mathbf{U}_{\\psi} - \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda})^T S^{-1} (\\mathbf{U}_{\\psi} - \\mathbf{J}_{\\psi\\lambda} \\mathbf{J}_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda}) = \\tilde{\\mathbf{U}}_{\\psi}^T (\\mathbf{J}^{\\psi\\psi}) \\tilde{\\mathbf{U}}_{\\psi}\n$$\n\nThus, the identity is proven.\n\n:::\n\n::: {#thm-wilks-general}\n\n#### Wilks' Theorem for Composite Hypotheses\nLet the parameter vector $\\boldsymbol{\\theta}$ of dimension $p$ be partitioned into parameters of interest $\\boldsymbol{\\psi}$ (dimension $k$) and nuisance parameters $\\boldsymbol{\\lambda}$ (dimension $p-k$):\n$$\n\\boldsymbol{\\theta} = \\begin{pmatrix} \\boldsymbol{\\psi} \\\\ \\boldsymbol{\\lambda} \\end{pmatrix}\n$$\nConsider testing $H_0: \\boldsymbol{\\psi} = \\boldsymbol{\\psi}_0$ against $H_1: \\boldsymbol{\\psi} \\neq \\boldsymbol{\\psi}_0$. The nuisance parameters $\\boldsymbol{\\lambda}$ are unspecified under both hypotheses.\n\nThe Likelihood Ratio Test statistic (Deviance) converges to a Chi-squared distribution with degrees of freedom equal to the dimension of $\\boldsymbol{\\psi}$:\n$$\nD_n = 2 [\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\hat{\\boldsymbol{\\theta}}_0)] \\xrightarrow{d} \\chi^2_k\n$$\n\n:::\n\n::: {.proof}\n**Strategy:** We expand the quadratic approximation of the likelihood and partition the information matrix to isolate the contribution of the parameters of interest.\n\n1.  **Quadratic Approximation**\n\n    From the simple Wilks' theorem, we know that twice the log-likelihood difference approximates a quadratic form based on the Score vector $\\mathbf{U}$ and Information matrix $J$ at the true parameter:\n    $$\n    2[\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}^*)] \\approx \\mathbf{U}^T J^{-1} \\mathbf{U}\n    $$\n\n2.  **Partitioning**\n\n    Partition the Score vector and Information matrix according to $(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda})$:\n    $$\n    \\mathbf{U} = \\begin{pmatrix} \\mathbf{U}_{\\psi} \\\\ \\mathbf{U}_{\\lambda} \\end{pmatrix}, \\quad\n    J = \\begin{pmatrix} J_{\\psi\\psi} & J_{\\psi\\lambda} \\\\ J_{\\lambda\\psi} & J_{\\lambda\\lambda} \\end{pmatrix}\n    $$\n\n3.  **Decomposition of the Full Model ($H_1$)**\n\n    The fit of the full model uses the inverse of the full information matrix. Using the **Schur Complement** identity for block inversion, the total quadratic form decomposes into two orthogonal terms:\n    $$\n    \\underbrace{\\mathbf{U}^T J^{-1} \\mathbf{U}}_{\\text{Total Fit}} = \\underbrace{\\mathbf{U}_{\\lambda}^T J_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda}}_{\\text{Nuisance Fit}} + \\underbrace{\\tilde{\\mathbf{U}}_{\\psi}^T (J^{\\psi\\psi}) \\tilde{\\mathbf{U}}_{\\psi}}_{\\text{Interest Fit (Adjusted)}}\n    $$\n\n    * The first term represents the variation explained by $\\boldsymbol{\\lambda}$ ignoring $\\boldsymbol{\\psi}$.\n    * The second term represents the variation explained by $\\boldsymbol{\\psi}$ after accounting for $\\boldsymbol{\\lambda}$.\n\n4.  **Fit of the Reduced Model ($H_0$)**\n\n    Under $H_0$, $\\boldsymbol{\\psi}$ is fixed. The model only maximizes over $\\boldsymbol{\\lambda}$. Consequently, the quadratic approximation for the reduced model only involves the $\\lambda$-block:\n    $$\n    2[\\ell_n(\\hat{\\boldsymbol{\\theta}}_0) - \\ell_n(\\boldsymbol{\\theta}^*)] \\approx \\mathbf{U}_{\\lambda}^T J_{\\lambda\\lambda}^{-1} \\mathbf{U}_{\\lambda}\n    $$\n\n5.  **The Difference (Deviance)**\n\n    Subtracting the reduced fit from the full fit cancels out the nuisance term:\n    $$\n    \\begin{aligned}\n    D_n &= 2[\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}^*)] - 2[\\ell_n(\\hat{\\boldsymbol{\\theta}}_0) - \\ell_n(\\boldsymbol{\\theta}^*)] \\\\\n    &\\approx \\left( \\text{Nuisance Fit} + \\text{Interest Fit} \\right) - \\left( \\text{Nuisance Fit} \\right) \\\\\n    &= \\tilde{\\mathbf{U}}_{\\psi}^T (J^{\\psi\\psi}) \\tilde{\\mathbf{U}}_{\\psi}\n    \\end{aligned}\n    $$\n\n6.  **Distribution**\n\n    The remaining term is a quadratic form of the $k$-dimensional vector $\\tilde{\\mathbf{U}}_{\\psi}$. Since $\\mathbf{U}$ is asymptotically Normal, this quadratic form follows a Chi-squared distribution with degrees of freedom equal to the dimension of $\\boldsymbol{\\psi}$:\n    $$\n    D_n \\xrightarrow{d} \\chi^2_k\n    $$\n\n:::\n\n## Examples\n\n### Simple Normal Distributions\n\n::: {#exm-normal-mean-lrt}\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$ where $\\sigma^2$ is known. We test $H_0: \\mu = \\mu_0$ vs $H_1: \\mu \\ne \\mu_0$.\n\n1. Log-Likelihood and Derivatives\n\n    The log-likelihood function for the mean $\\mu$ is:\n    $$\n    l(\\mu) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\n    $$\n\n    * **Score Function (First Derivative):**\n        $$\n        l'(\\mu) = \\frac{d}{d\\mu} \\left[ -\\frac{1}{2\\sigma^2}\\sum (x_i - \\mu)^2 \\right] = \\frac{1}{\\sigma^2}\\sum (x_i - \\mu) = \\frac{n}{\\sigma^2}(\\bar{x} - \\mu)\n        $$\n        Setting $l'(\\mu) = 0$ yields the MLE: **$\\hat{\\mu} = \\bar{x}$**.\n\n    * **Observed Information (Second Derivative):**\n        $$\n        l''(\\mu) = \\frac{d}{d\\mu} \\left[ \\frac{n}{\\sigma^2}(\\bar{x} - \\mu) \\right] = -\\frac{n}{\\sigma^2}\n        $$\n        Note that in this specific case, the curvature is constant and does not depend on data. The **Fisher Information** (for sample size $n$) is:\n        $$\n        I_n = -E[l''(\\mu)] = \\frac{n}{\\sigma^2}\n        $$\n\n2. Taylor Approximation of Deviance\n\n    We expand the log-likelihood $l(\\mu_0)$ around the MLE $\\hat{\\mu}$.\n    $$\n    l(\\mu_0) \\approx l(\\hat{\\mu}) + (\\mu_0 - \\hat{\\mu})\\underbrace{l'(\\hat{\\mu})}_{0} + \\frac{1}{2}(\\mu_0 - \\hat{\\mu})^2 l''(\\hat{\\mu})\n    $$\n    Rearranging for the Deviance $D = 2[l(\\hat{\\mu}) - l(\\mu_0)]$ and substituting the specific expression for $l''(\\hat{\\mu}) = -n/\\sigma^2$:\n\n    $$\n    \\begin{aligned}\n    D &\\approx -(\\mu_0 - \\hat{\\mu})^2 l''(\\hat{\\mu}) \\\\\n    &= -(\\hat{\\mu} - \\mu_0)^2 \\left( -\\frac{n}{\\sigma^2} \\right) \\\\\n    &= \\frac{n}{\\sigma^2} (\\hat{\\mu} - \\mu_0)^2 \\\\\n    &= \\left( \\frac{\\hat{\\mu} - \\mu_0}{\\sigma/\\sqrt{n}} \\right)^2\n    \\end{aligned}\n    $$\n\n3. Conclusion\n\n    The term inside the square is exactly the Z-score for the sample mean:\n    $$\n    Z = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\n    $$\n    Therefore, the Deviance is the square of a standard normal variable:\n    $$\n    D = Z^2 \\sim \\chi^2_1\n    $$\n    In this Normal example, the \"approximation\" is actually exact because the log-likelihood is perfectly quadratic (the third derivative is zero).\n\n4. Simulation Verification\n\n    We simulate 2000 datasets ($n=50$) to verify that the standardized MLE and Score follow a Normal distribution, and the Deviance follows a $\\chi^2_1$ distribution.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn_sim <- 2000\nn <- 50\nmu_true <- 5\nsigma <- 1\nI_expected <- n / sigma^2 # Fisher Information = n/sigma^2\n\n# Storage\nmle_vals <- numeric(n_sim)\nscore_vals <- numeric(n_sim)\ndeviance_vals <- numeric(n_sim)\n\nfor(i in 1:n_sim) {\n  x <- rnorm(n, mean = mu_true, sd = sigma)\n  \n  # MLE\n  mu_hat <- mean(x)\n  mle_vals[i] <- mu_hat\n  \n  # Log-Likelihoods\n  # We use the exact function defined above\n  ll_hat <- sum(dnorm(x, mean = mu_hat, sd = sigma, log = TRUE))\n  ll_true <- sum(dnorm(x, mean = mu_true, sd = sigma, log = TRUE))\n  \n  # Score at Truth: U(mu_0) = n(x_bar - mu_0) / sigma^2\n  score_vals[i] <- (n * (mu_hat - mu_true)) / sigma^2\n  \n  # Deviance\n  deviance_vals[i] <- 2 * (ll_hat - ll_true)\n}\n\n# --- Plotting ---\npar(mfrow = c(1, 3))\n\n# 1. Standardized MLE: (mu_hat - mu) / SE\n\n# SE = 1/sqrt(I) = sigma/sqrt(n)\nz_mle <- (mle_vals - mu_true) * sqrt(I_expected)\nhist(z_mle, breaks = 20, freq = FALSE, col = \"lightblue\",\n     main = \"Std. MLE\", xlab = \"Z value\")\ncurve(dnorm(x), add = TRUE, col = \"red\", lwd = 2)\n\n# 2. Standardized Score: U / sqrt(I) ~ N(0, 1)\nz_score <- score_vals / sqrt(I_expected)\nhist(z_score, breaks = 20, freq = FALSE, col = \"lightgreen\",\n     main = \"Std. Score\", xlab = \"Z value\")\ncurve(dnorm(x), add = TRUE, col = \"blue\", lwd = 2)\n\n# 3. Deviance: D ~ Chi-sq(1)\nhist(deviance_vals, breaks = 20, freq = FALSE, col = \"lightpink\",\n     main = \"Deviance\", xlab = \"D\")\ncurve(dchisq(x, df = 1), add = TRUE, col = \"darkgreen\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![Simulation of Normal Mean Asymptotics (n=50, 2000 replicates). Left: The standardized MLE follows a Standard Normal. Center: The standardized Score follows a Standard Normal. Right: The Deviance follows a Chi-squared distribution with 1 degree of freedom.](mle_files/figure-html/fig-normal-sim-1.png){#fig-normal-sim width=576}\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n### Poisson Regression (Canonical Exponential Family)\n\n::: {#exm-poisson-glm}\nConsider a GLM where $Y_i \\sim \\text{Poisson}(\\lambda_i)$. We use the canonical link $\\log(\\lambda_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$.\n\n1.  **Exponential Family Representation**\n    We rewrite the total log-likelihood by grouping terms associated with each $\\beta_j$:\n    \n    $$\n    \\begin{aligned}\n    \\ell(\\boldsymbol{\\beta}) &= \\sum_{i=1}^n \\left( y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}} - \\log(y_i!) \\right) \\\\\n    &= \\sum_{i=1}^n \\sum_{j=0}^p y_i x_{ij} \\beta_j - \\sum_{i=1}^n e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}} - \\sum_{i=1}^n \\log(y_i!) \\\\\n    &= \\sum_{j=0}^p \\beta_j \\underbrace{\\left( \\sum_{i=1}^n y_i x_{ij} \\right)}_{T_j(\\mathbf{y})} - \\underbrace{\\sum_{i=1}^n e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}}_{A(\\boldsymbol{\\beta})} + \\text{const}\n    \\end{aligned}\n    $$\n\n    This is a multivariate exponential family in **canonical form** where:\n    * **Natural Parameters:** The regression coefficients $\\boldsymbol{\\beta} = (\\beta_0, \\dots, \\beta_p)^\\top$.\n    * **Sufficient Statistics:** $\\mathbf{T}(\\mathbf{y}) = \\mathbf{X}^\\top \\mathbf{y}$.\n    * **Log-Partition Function:** $A(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})$.\n\n2.  **Derivations via Moments of Sufficient Statistics**\n    Since we are in canonical form, the Score and Information are simply the derivatives of the log-partition function $A(\\boldsymbol{\\beta})$.\n\n    * **Score Vector ($\\mathbf{U}$):**\n        The expectation of the sufficient statistic is $\\nabla A(\\boldsymbol{\\beta})$. The score is the difference between observed and expected $T$:\n        $$\n        \\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{T}(\\mathbf{y}) - \\nabla A(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{y} - \\mathbf{X}^\\top \\boldsymbol{\\lambda} = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\lambda})\n        $$\n        *(where $\\nabla A(\\boldsymbol{\\beta}) = \\sum e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}} \\mathbf{x}_i = \\sum \\lambda_i \\mathbf{x}_i = \\mathbf{X}^\\top \\boldsymbol{\\lambda}$)*.\n\n    * **Fisher Information ($J$):**\n        The information is the Hessian of the log-partition function:\n        $$\n        J(\\boldsymbol{\\beta}) = \\nabla^2 A(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} (\\lambda_i \\mathbf{x}_i^\\top) = \\sum_{i=1}^n \\lambda_i \\mathbf{x}_i \\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n        $$\n        where $\\mathbf{W}$, in the context of Poisson regression, $\\mathbf{W}$ is the diagonal matrix of the expected values (which are equal to the variances):\n\n\n\n        $$\n        \\mathbf{W} = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n) = \n        \\begin{bmatrix}\n        \\lambda_1 & 0 & \\cdots & 0 \\\\\n        0 & \\lambda_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & \\lambda_n\n        \\end{bmatrix}\n        $$\n        where $\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})$.\n\n        \n\n3.  **Asymptotic Theory**\n\n    * **Score Normality:** The Score vector is a sum of independent zero-mean random vectors. By the Multivariate CLT:\n        $$ \\frac{1}{\\sqrt{n}}\\mathbf{U}(\\boldsymbol{\\beta}) \\xrightarrow{d} N(\\mathbf{0}, I) $$\n\n    * **MLE Normality:** By linear approximation $\\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{U}(\\boldsymbol{\\beta}) - J(\\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})$, the estimator converges as:\n        $$ \\sqrt{n}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\xrightarrow{d} N(\\mathbf{0}, I^{-1}) $$\n\n4.  **Simulation Verification**\n\n    We simulate a Poisson regression with an intercept and one continuous covariate ($p=2$). We verify that the MLE and Score follow their theoretical normal distributions derived from $A(\\boldsymbol{\\beta})$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn_sim <- 2000\nn <- 200\nbeta_true <- c(0.5, 1.5) # Intercept, Slope\n\n# Fixed covariate matrix\nx <- runif(n, 0, 1)\nX_mat <- cbind(1, x)\n\n# Expected Fisher Info at Truth (Using exponential family formula X^T W X)\n# Here Hessian of A(beta) = X^T W X\nlambda_true <- exp(X_mat %*% beta_true)\nW <- diag(as.vector(lambda_true))\nJ_true <- t(X_mat) %*% W %*% X_mat # Total Info\ninv_J <- solve(J_true)\n\n# Storage\nmle_slope_z <- numeric(n_sim)\nscore_slope_z <- numeric(n_sim)\ndeviance_vals <- numeric(n_sim)\n\nfor(i in 1:n_sim) {\n  y <- rpois(n, lambda = lambda_true)\n  \n  # Fit Model\n  fit <- glm(y ~ x, family = poisson)\n  \n  # 1. MLE Standardization (Slope)\n  beta_hat <- coef(fit)\n  se_slope <- sqrt(inv_J[2,2]) # Theoretical SE from Inverse Info\n  mle_slope_z[i] <- (beta_hat[2] - beta_true[2]) / se_slope\n  \n  # 2. Score Standardization (Slope component)\n  # U = X^T y - grad A(beta)\n  U <- t(X_mat) %*% (y - lambda_true)\n  se_score <- sqrt(J_true[2,2]) # Variance of Score is J (not inverse)\n  score_slope_z[i] <- U[2] / se_score\n  \n  # 3. Deviance (Likelihood Ratio vs Truth)\n  ll_hat <- as.numeric(logLik(fit))\n  ll_true <- sum(dpois(y, lambda = lambda_true, log = TRUE))\n  deviance_vals[i] <- 2 * (ll_hat - ll_true)\n}\n\n# --- Plotting ---\npar(mfrow = c(1, 3))\n\n# MLE Z-score\nhist(mle_slope_z, breaks = 20, freq = FALSE, col = \"lightblue\",\n     main = \"Std. MLE (Slope)\", xlab = \"Z value\")\ncurve(dnorm(x), add = TRUE, col = \"red\", lwd = 2)\n\n# Score Z-score\nhist(score_slope_z, breaks = 20, freq = FALSE, col = \"lightgreen\",\n     main = \"Std. Score (Slope)\", xlab = \"Z value\")\ncurve(dnorm(x), add = TRUE, col = \"blue\", lwd = 2)\n\n# Deviance (df = 2 parameters)\nhist(deviance_vals, breaks = 20, freq = FALSE, col = \"lightpink\",\n     main = \"Deviance (p=2)\", xlab = \"D\")\ncurve(dchisq(x, df = 2), add = TRUE, col = \"darkgreen\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![Simulation of Poisson Regression (n=200, 2000 replicates). Left: The standardized MLE for the slope. Center: The standardized Score for the slope. Right: The Deviance for the full vector behaves as Chi-squared with p=2 degrees of freedom.](mle_files/figure-html/fig-poisson-sim-nn-1.png){#fig-poisson-sim-nn width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n**Alternative Derivation: Sum of Individual Contributions**\n\nWe can also derive the Score and Information by viewing the full log-likelihood as a sum of individual contributions, where each contribution follows the scalar canonical form:\n\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ell_i(\\theta_i; y_i) = \\sum_{i=1}^n \\left[ y_i \\theta_i - A(\\theta_i) + \\log h(y_i) \\right]\n$$\n\n1.  **Exponential Family Properties**\n    * **Natural Parameter:** $\\theta_i = \\log \\lambda_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$.\n    * **Log-Partition Function:** $A(\\theta_i) = e^{\\theta_i} = \\lambda_i$.\n    * **Moments:** $E[Y_i] = A'(\\theta_i) = \\lambda_i$ and $\\text{Var}(Y_i) = A''(\\theta_i) = \\lambda_i$.\n\n2.  **Derivations via Chain Rule**\n    We derive the gradient and Hessian with respect to $\\boldsymbol{\\beta}$ using the chain rule term $\\frac{\\partial \\theta_i}{\\partial \\boldsymbol{\\beta}} = \\mathbf{x}_i$.\n\n    * **Score Vector ($\\mathbf{U}$):**\n        $$\n        \\mathbf{U}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\ell_i(\\theta_i) = \\sum_{i=1}^n (y_i - A'(\\theta_i)) \\frac{\\partial \\theta_i}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^n (y_i - \\lambda_i) \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\lambda})\n        $$\n\n    * **Fisher Information ($J$):**\n        The information is the negative Hessian. The derivative of the residual $(y_i - \\lambda_i)$ is $-A''(\\theta_i) \\frac{\\partial \\theta_i}{\\partial \\boldsymbol{\\beta}}$:\n        $$\n        J(\\boldsymbol{\\beta}) = - \\sum_{i=1}^n \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left[ (y_i - \\lambda_i) \\mathbf{x}_i \\right] = \\sum_{i=1}^n A''(\\theta_i) \\mathbf{x}_i \\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n        $$\n        where $\\mathbf{W} = \\text{diag}(\\lambda_i)$ is the weight matrix (which equals the variance for Poisson).\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}