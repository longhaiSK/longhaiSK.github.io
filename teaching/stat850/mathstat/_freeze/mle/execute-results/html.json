{
  "hash": "e926d6a4b95d95d4e38c7cc6cac42c44",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n## Definitions and Concepts\n\n### Likelihood and MLE\n\n::: {#def-likelihood-mle}\n\n### Maximum Likelihood Estimation\n\n1.  **Likelihood Function:**\n    Let $f(\\mathbf{x}|\\boldsymbol{\\theta})$ be the joint probability density function of the data $\\mathbf{X}$. When viewed as a function of the parameter $\\boldsymbol{\\theta}$ given fixed data $\\mathbf{x}$, it is called the likelihood function:\n    $$\n    L(\\boldsymbol{\\theta}; \\mathbf{x}) = f(\\mathbf{x}|\\boldsymbol{\\theta})\n    $$\n\n2.  **Log-likelihood:**\n    It is usually easier to maximize the natural logarithm of the likelihood:\n    $$\n    \\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\log L(\\boldsymbol{\\theta}; \\mathbf{x})\n    $$\n\n3.  **Maximum Likelihood Estimator (MLE):**\n    The MLE $\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}$ is the value that maximizes the likelihood function:\n    $$\n    \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}(\\mathbf{x}) = \\operatorname*{argmax}_{\\boldsymbol{\\theta} \\in \\Theta} \\ell(\\boldsymbol{\\theta}; \\mathbf{x})\n    $$\n\n4.  **Score Function ($\\mathbf{U}$):**\n    The score function is defined as the gradient of the log-likelihood with respect to the parameter vector. Finding the MLE often involves solving the score equation:\n    $$\n    \\mathbf{U}(\\boldsymbol{\\theta}; \\mathbf{x}) = \\nabla_{\\boldsymbol{\\theta}} \\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = \\mathbf{0}\n    $$\n\n:::\n\n## Examples\n\n::: {#exm-normal-mle-score}\n\n### Normal Distribution: MLE and Score Convergence\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. We define $\\boldsymbol{\\theta} = (\\mu, \\sigma^2)^T$.\n\n1. Log-Likelihood\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{x}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n$$\n\n2. Score Function and MLE\nThe Score vector $\\mathbf{U}(\\boldsymbol{\\theta})$ has two components:\n\n* **Component 1 (Mean):**\n    $$\n    U_\\mu = \\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)\n    $$\n    Setting $U_\\mu = 0$ yields $\\hat{\\mu}_{\\text{MLE}} = \\bar{x}$.\n\n* **Component 2 (Variance):**\n    $$\n    U_{\\sigma^2} = \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (x_i - \\mu)^2\n    $$\n    Setting $U_{\\sigma^2} = 0$ yields $\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum (x_i - \\hat{\\mu})^2$.\n\n3. Asymptotic Normality of the Score\nWe now demonstrate that $\\mathbf{U}(\\boldsymbol{\\theta})$ follows a Normal distribution $N(\\mathbf{0}, \\mathbf{I}(\\boldsymbol{\\theta}))$.\n\n* **For $U_\\mu$:**\n    $$\n    U_\\mu = \\frac{n}{\\sigma^2} (\\bar{X} - \\mu)\n    $$\n    Since $\\bar{X} \\sim N(\\mu, \\sigma^2/n)$, $U_\\mu$ is a linear transformation of a Normal variable. It is **exactly Normal**:\n    $$\n    E[U_\\mu] = 0, \\quad \\text{Var}(U_\\mu) = \\left(\\frac{n}{\\sigma^2}\\right)^2 \\text{Var}(\\bar{X}) = \\frac{n^2}{\\sigma^4} \\frac{\\sigma^2}{n} = \\frac{n}{\\sigma^2}\n    $$\n    Thus, $U_\\mu \\sim N(0, \\frac{n}{\\sigma^2})$.\n\n* **For $U_{\\sigma^2}$:**\n    We rewrite $U_{\\sigma^2}$ as a sum of i.i.d. variables. Let $Z_i = ((X_i - \\mu)/\\sigma)^2$. Note that $Z_i \\sim \\chi^2_1$, with variance 2.\n    $$\n    U_{\\sigma^2} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left( \\left(\\frac{X_i - \\mu}{\\sigma}\\right)^2 - 1 \\right) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Z_i - 1)\n    $$\n    Since this is a sum of i.i.d. random variables with mean 0, the **Central Limit Theorem** applies:\n    $$\n    U_{\\sigma^2} \\xrightarrow{d} \\text{Normal}\n    $$\n    The limiting variance is:\n    $$\n    \\text{Var}(U_{\\sigma^2}) = \\frac{1}{4\\sigma^4} \\sum \\text{Var}(Z_i) = \\frac{1}{4\\sigma^4} (n \\times 2) = \\frac{n}{2\\sigma^4}\n    $$\n\n**Conclusion:**\nThe covariance matrix of the Score approaches the Fisher Information matrix:\n$$\n\\text{Var}(\\mathbf{U}) = \\begin{bmatrix} \\frac{n}{\\sigma^2} & 0 \\\\ 0 & \\frac{n}{2\\sigma^4} \\end{bmatrix} = \\mathbf{I}(\\boldsymbol{\\theta})\n$$\n\n:::\n\n::: {#exm-uniform-mle-boundary}\n\n### Uniform Distribution (Boundary Case)\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{\\text{Unif}}(0, \\theta)$. The likelihood is:\n$$\nL(\\theta; \\mathbf{x}) = \\frac{1}{\\theta^n} I(x_{(n)} \\le \\theta)\n$$\nThis function strictly decreases for $\\theta \\ge x_{(n)}$ and is zero otherwise. Thus:\n$$\n\\hat{\\theta}_{\\text{MLE}} = x_{(n)}\n$$\nNote that the Score equation approach fails here because the support depends on $\\theta$, making the log-likelihood discontinuous at the boundary.\n\n:::\n\n## Review of Convergence Theorems for Probability\n\n\n\n::: {#thm-lln}\n\n### Weak Law of Large Numbers (WLLN)\nLet $X_1, \\dots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \\mu$ and finite variance $\\text{Var}(X_i) = \\sigma^2 < \\infty$.\n\nThen, the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$ converges in probability to $\\mu$:\n$$\n\\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty\n$$\nFormal definition: For any $\\epsilon > 0$, $\\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0$.\n\n:::\n\n\n\n::: {#thm-clt}\n\n### Central Limit Theorem for IID Cases\nLet $X_1, \\dots, X_n$ be i.i.d. random variables with mean $E[X_i] = \\mu$ and finite variance $0 < \\text{Var}(X_i) = \\sigma^2 < \\infty$.\n\nThen, the random variable $\\sqrt{n}(\\bar{X}_n - \\mu)$ converges in distribution to a normal distribution with mean 0 and variance $\\sigma^2$:\n$$\n\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\n$$\n\n:::\n\n::: {#rem-clt-history}\n\n### Historical Note: The Evolution of the CLT\n\nWhile the standard i.i.d. theorem is named after **Jarl Waldemar Lindeberg** and **Paul Lévy**, they were the \"closers\" of a discovery process that involved many others:\n\n1.  **Abraham de Moivre (1733):** The first to discover the \"Normal curve\" as an approximation to the Binomial distribution (specifically for the symmetric case $p=1/2$).\n\n2.  **Pierre-Simon Laplace (1812):** Generalized de Moivre's work to arbitrary $p$ (the De Moivre-Laplace Theorem) and intuitively grasped that sums of independent errors tend toward normality, though his proofs lacked modern rigor.\n\n3.  **Pafnuty Chebyshev (1887):** Attempted to prove the general CLT using the \"Method of Moments.\" While his proof had gaps, it established the moment-based approach later completed by his student, Markov.\n\n4.  **Aleksandr Lyapunov (1901):** Provided the first rigorous proof for independent variables using **Characteristic Functions**. However, he required a slightly stricter condition (existence of $(2+\\delta)$ moments) than necessary.\n\n5.  **Lindeberg (1922) & Lévy (1925):**\n    * **Lindeberg** proved the general CLT for independent variables under the famous \"Lindeberg Condition.\" notably using a direct \"replacement method\" (swapping distributions), **not** characteristic functions.\n    * **Lévy** rigorously proved the i.i.d. case and investigated stable laws using Characteristic Functions, establishing them as the standard tool for such proofs.\n\n6.  **William Feller (1935):** Proved the converse: if the individual random variables are asymptotically negligible (no single term dominates the sum), then the Lindeberg condition is not just sufficient, but **necessary** for the sum to converge to a Normal distribution. Hence, the general theorem is often called the **Lindeberg-Feller Theorem**.\n\n:::\n\n::: {#thm-lindeberg-feller}\n\n### Lindeberg-Feller CLT (For Non-Identical Distributions)\nThis variation is crucial for regression analysis (e.g., OLS properties with fixed regressors) where variables are independent but **not** identically distributed.\n\nLet $X_1, \\dots, X_n$ be independent random variables with $E[X_i] = \\mu_i$ and $\\text{Var}(X_i) = \\sigma_i^2$.\nDefine the **average variance** $\\tilde{\\sigma}_n^2$:\n$$\n\\tilde{\\sigma}_n^2 = \\frac{1}{n} \\sum_{i=1}^n \\sigma_i^2\n$$\n\nIf the **Lindeberg Condition** holds:\nFor every $\\epsilon > 0$,\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n \\tilde{\\sigma}_n^2} \\sum_{i=1}^n E\\left[ (X_i - \\mu_i)^2 \\cdot I\\left( |X_i - \\mu_i| > \\epsilon \\sqrt{n \\tilde{\\sigma}_n^2} \\right) \\right] = 0\n$$\n\nThen the standardized sum converges to a standard normal:\n$$\n\\frac{\\sum_{i=1}^n (X_i - \\mu_i)}{\\sqrt{n \\tilde{\\sigma}_n^2}} \\xrightarrow{d} N(0, 1)\n$$\n\n:::\n\n::: {#cor-lindeberg-feller-approx}\n\n### Approximating Distribution for Sample Mean (Non-i.i.d.)\n\nUnder the conditions of the Lindeberg-Feller CLT, we can derive the asymptotic distribution for the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nLet $\\bar{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n \\mu_i$ be the average mean. Note that the denominator in the CLT is simply $\\sqrt{n} \\tilde{\\sigma}_n$.\n\nThe standardized sum converges to $N(0,1)$:\n$$\n\\frac{\\sum (X_i - \\mu_i)}{\\sqrt{n \\tilde{\\sigma}_n^2}} = \\frac{n(\\bar{X}_n - \\bar{\\mu}_n)}{\\sqrt{n} \\tilde{\\sigma}_n} = \\frac{\\sqrt{n}(\\bar{X}_n - \\bar{\\mu}_n)}{\\tilde{\\sigma}_n} \\xrightarrow{d} N(0, 1)\n$$\n\nThis implies the following **approximate distributions** for large $n$:\n\n1.  **For the Sample Mean:**\n    $$\n    \\bar{X}_n \\overset{\\cdot}{\\sim} N\\left(\\bar{\\mu}_n, \\frac{\\tilde{\\sigma}_n^2}{n}\\right)\n    $$\n\n2.  **For the Scaled Difference (Root-n consistency):**\n    $$\n    \\sqrt{n}(\\bar{X}_n - \\bar{\\mu}_n) \\overset{\\cdot}{\\sim} N\\left(0, \\tilde{\\sigma}_n^2\\right)\n    $$\n\n*Note: If all $X_i$ share the same mean $\\mu$, simply replace $\\bar{\\mu}_n$ with $\\mu$.*\n\n:::\n\n::: {#thm-slutsky}\n\n### Slutsky's Theorem\nLet $X_n$ and $Y_n$ be sequences of random variables. If $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{p} c$, where $c$ is a constant, then:\n\n1.  **Sum:** $X_n + Y_n \\xrightarrow{d} X + c$\n2.  **Product:** $X_n Y_n \\xrightarrow{d} cX$\n3.  **Quotient:** $X_n / Y_n \\xrightarrow{d} X/c$ (provided $c \\ne 0$)\n\n:::\n\n::: {#thm-generalized-slutsky}\n\n### Generalized Slutsky's Theorem (Continuous Mapping)\n\nThe arithmetic operations in Slutsky's theorem are special cases of a broader property.\n\nLet $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{p} c$, where $c$ is a constant.\nLet $g: \\mathbb{R}^2 \\to \\mathbb{R}^k$ be a function that is **continuous** at every point $(x, c)$ where $x$ is in the support of $X$.\n\nThen:\n$$\ng(X_n, Y_n) \\xrightarrow{d} g(X, c)\n$$\n\nThis implies that for any \"well-behaved\" algebraic combination (polynomials, exponentials, etc.) of a sequence converging in distribution and a sequence converging in probability to a constant, the limit behaves as if the constant were substituted directly.\n\n:::\n\n::: {#exm-asymptotic-normal-variance}\n\n### Asymptotic Normality of Sample Variance\n\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. We wish to derive the asymptotic distribution of the MLE for variance, \n\n$$\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2.$$\n\n1. Algebraic Expansion\n\n    We rewrite the estimator by adding and subtracting the true mean $\\mu$:\n    $$\n    \\sum_{i=1}^n (X_i - \\bar{X})^2 = \\sum_{i=1}^n ((X_i - \\mu) - (\\bar{X} - \\mu))^2\n    $$\n    $$\n    = \\sum_{i=1}^n (X_i - \\mu)^2 - 2(\\bar{X} - \\mu)\\sum_{i=1}^n(X_i - \\mu) + n(\\bar{X} - \\mu)^2\n    $$\n    Since $\\sum(X_i - \\mu) = n(\\bar{X} - \\mu)$:\n    $$\n    = \\sum_{i=1}^n (X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2\n    $$\n    Dividing by $n$, we get:\n    $$\n    \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - (\\bar{X} - \\mu)^2\n    $$\n\n2. Scaling by $\\sqrt{n}$\n\n    We rearrange to look at the pivotal quantity $\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2)$:\n    $$\n    \\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) = \\sqrt{n}\\left( \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\sigma^2 \\right) - \\sqrt{n}(\\bar{X} - \\mu)^2\n    $$\n\n3. Applying Convergence Theorems\n\n\n    * **Term 1 (CLT):**\n    \n        Let $W_i = (X_i - \\mu)^2$. Since $X_i \\sim N(\\mu, \\sigma^2)$, $W_i/\\sigma^2 \\sim \\chi^2_1$.\n        moments of $W_i$: $E[W_i] = \\sigma^2$ and $\\text{Var}(W_i) = 2\\sigma^4$.\n        By the standard CLT:\n        $$\n        \\sqrt{n}(\\bar{W} - E[W]) = \\sqrt{n}\\left( \\frac{1}{n}\\sum (X_i - \\mu)^2 - \\sigma^2 \\right) \\xrightarrow{d} N(0, 2\\sigma^4)\n        $$\n\n    * **Term 2 (Slutsky):**\n    \n        Consider the term $\\sqrt{n}(\\bar{X} - \\mu)^2$. We can rewrite this as:\n        $$\n        \\underbrace{\\sqrt{n}(\\bar{X} - \\mu)}_{\\xrightarrow{d} N(0, \\sigma^2)} \\cdot \\underbrace{(\\bar{X} - \\mu)}_{\\xrightarrow{p} 0 \\text{ by LLN}}\n        $$\n        By Slutsky's Theorem, the product converges to $Z \\cdot 0 = 0$.\n\n**Conclusion:**\n\nCombining the terms:\n$$\n\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) \\xrightarrow{d} N(0, 2\\sigma^4) - 0 = N(0, 2\\sigma^4)\n$$\n\n:::\n\n## Asymptotic Theory of Maximum Likelihood\n\n### Consistency of the MLE\n\n#### IID Cases\n\nConsistency establishes that as the sample size grows, the estimator converges in probability to the true parameter value.\n\n::: {#thm-mle-consistency}\n\n#### Consistency of MLE\nLet $Y_1, \\dots, Y_n$ be i.i.d. with density $f(y|\\boldsymbol{\\theta})$. Let $\\boldsymbol{\\theta}_0$ be the true parameter. Under regularity conditions (specifically identifiability and compactness), the Maximum Likelihood Estimator $\\hat{\\boldsymbol{\\theta}}_n$ satisfies:\n$$\n\\hat{\\boldsymbol{\\theta}}_n \\xrightarrow{p} \\boldsymbol{\\theta}_0 \\quad \\text{as } n \\to \\infty\n$$\n:::\n\n::: {.proof}\n1. **The Objective Function**\n\n   The MLE maximizes the average log-likelihood:\n   $$\n   M_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\log f(Y_i|\\boldsymbol{\\theta})\n   $$\n   By the **Weak Law of Large Numbers (WLLN)**, for any fixed $\\boldsymbol{\\theta}$, $M_n(\\boldsymbol{\\theta})$ converges in probability to its expectation:\n   $$\n   M_n(\\boldsymbol{\\theta}) \\xrightarrow{p} M(\\boldsymbol{\\theta}) = E_{\\boldsymbol{\\theta}_0} [\\log f(Y|\\boldsymbol{\\theta})]\n   $$\n\n2. **Identifying the Maximum (Information Inequality)**\n\n   We compare the expected log-likelihood at an arbitrary $\\boldsymbol{\\theta}$ versus the true $\\boldsymbol{\\theta}_0$. Consider the difference:\n   $$\n   M(\\boldsymbol{\\theta}) - M(\\boldsymbol{\\theta}_0) = E_{\\boldsymbol{\\theta}_0} \\left[ \\log f(Y|\\boldsymbol{\\theta}) - \\log f(Y|\\boldsymbol{\\theta}_0) \\right] = E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right) \\right]\n   $$\n   By **Jensen's Inequality** (since $\\log(y)$ is strictly concave), $E[\\log Z] \\le \\log E[Z]$.\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right) \\right] \\le \\log E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right]\n   $$\n   Evaluating the expectation on the right:\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f(Y|\\boldsymbol{\\theta})}{f(Y|\\boldsymbol{\\theta}_0)} \\right] = \\int \\left( \\frac{f(y|\\boldsymbol{\\theta})}{f(y|\\boldsymbol{\\theta}_0)} \\right) f(y|\\boldsymbol{\\theta}_0) \\, dy = \\int f(y|\\boldsymbol{\\theta}) \\, dy = 1\n   $$\n   Therefore:\n   $$\n   M(\\boldsymbol{\\theta}) - M(\\boldsymbol{\\theta}_0) \\le \\log(1) = 0 \\implies M(\\boldsymbol{\\theta}) \\le M(\\boldsymbol{\\theta}_0)\n   $$\n   Assuming the model is **identifiable** (i.e., $f(y|\\boldsymbol{\\theta}) = f(y|\\boldsymbol{\\theta}_0) \\iff \\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0$), the maximum is unique at $\\boldsymbol{\\theta}_0$. Since $\\hat{\\boldsymbol{\\theta}}_n$ maximizes $M_n(\\boldsymbol{\\theta})$, which converges to $M(\\boldsymbol{\\theta})$, $\\hat{\\boldsymbol{\\theta}}_n$ must converge to $\\boldsymbol{\\theta}_0$.\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Consistency: As n increases, the log-likelihood function concentrates around the true parameter.](mle_files/figure-html/fig-likelihood-consistency-1.png){#fig-likelihood-consistency fig-align='center' width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Non-IID Cases\n\nIn regression settings, observations are independent but not identically distributed (i.n.i.d.). We generalize the consistency result by requiring that the \"average\" information accumulates sufficiently.\n\n::: {#thm-mle-consistency-general}\n#### Consistency of MLE (General Case)\nLet $Y_1, \\dots, Y_n$ be independent observations with densities $f_i(y|\\boldsymbol{\\theta})$ (e.g., depending on covariates $x_i$). Let $\\boldsymbol{\\theta}_0$ be the true parameter.\n\nUnder the following conditions:\n1.  **Parameter Space:** Compact parameter space $\\Theta$.\n2.  **Identification:** For any $\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0$, the average Kullback-Leibler divergence is strictly positive in the limit:\n    $$\\liminf_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\text{KL}(f_i(\\cdot|\\boldsymbol{\\theta}_0) || f_i(\\cdot|\\boldsymbol{\\theta})) > 0$$\n3.  **Uniform Convergence:** The log-likelihood satisfies a Uniform Law of Large Numbers (ULLN).\n\nThen $\\hat{\\boldsymbol{\\theta}}_n \\xrightarrow{p} \\boldsymbol{\\theta}_0$.\n:::\n\n::: {.proof}\n1. **The Objective Function**\n\n   The MLE maximizes the average log-likelihood:\n   $$\n   M_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\log f_i(Y_i|\\boldsymbol{\\theta})\n   $$\n   Unlike the i.i.d. case, the terms in the sum have different expectations. We rely on a **WLLN for Independent Variables** (e.g., Chebyshev's WLLN). Provided the variances are bounded, $M_n(\\boldsymbol{\\theta})$ converges in probability to the **average expectation**:\n   $$\n   M_n(\\boldsymbol{\\theta}) \\xrightarrow{p} \\bar{M}_n(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n E_{\\boldsymbol{\\theta}_0} [\\log f_i(Y_i|\\boldsymbol{\\theta})]\n   $$\n   *Note: For consistency of the maximizer, we strictly require **Uniform Convergence** over $\\Theta$, not just pointwise convergence.*\n\n2. **Identifying the Maximum (Average KL Divergence)**\n\n   We compare the limit function at $\\boldsymbol{\\theta}$ versus $\\boldsymbol{\\theta}_0$. Consider the average difference:\n   $$\n   \\bar{M}_n(\\boldsymbol{\\theta}) - \\bar{M}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\sum_{i=1}^n E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\left( \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right) \\right]\n   $$\n   By Jensen's Inequality applied to *each* term in the sum:\n   $$\n   E_{\\boldsymbol{\\theta}_0} \\left[ \\log \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right] \\le \\log E_{\\boldsymbol{\\theta}_0} \\left[ \\frac{f_i(Y_i|\\boldsymbol{\\theta})}{f_i(Y_i|\\boldsymbol{\\theta}_0)} \\right] = \\log(1) = 0\n   $$\n   Summing these inequalities implies that $\\bar{M}_n(\\boldsymbol{\\theta})$ is uniquely maximized at $\\boldsymbol{\\theta}_0$ (provided the identification condition holds). Since the sample function $M_n(\\boldsymbol{\\theta})$ converges uniformly to this maximized limit function, the argmax $\\hat{\\boldsymbol{\\theta}}_n$ converges to $\\boldsymbol{\\theta}_0$.\n:::\n\n### Asymptotic Normality of the Score Vector\n\nThe Score function acts as the \"engine\" for the normality of the MLE. We treat the I.I.D. and non-I.I.D. cases separately.\n\n#### IID Cases\n\n::: {#thm-score-normality-iid}\n#### Normality of Score (I.I.D.)\nLet $Y_1, \\dots, Y_n$ be i.i.d. with density $f(y|\\boldsymbol{\\theta})$.\nDefine the **Fisher Information matrix** for a single observation as the expected outer product of the score:\n$$\n\\mathbf{I}_1(\\boldsymbol{\\theta}) = E\\left[ \\left( \\nabla_{\\boldsymbol{\\theta}} \\log f(Y_1|\\boldsymbol{\\theta}) \\right) \\left( \\nabla_{\\boldsymbol{\\theta}} \\log f(Y_1|\\boldsymbol{\\theta}) \\right)^T \\right]\n$$\nThen, the scaled total score vector converges to a Normal distribution:\n$$\n\\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\xrightarrow{d} N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n$$\n:::\n\n::: {.proof}\nThe total score is the sum of independent score contributions:\n$$\n\\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\sum_{i=1}^n \\nabla \\log f(Y_i|\\boldsymbol{\\theta}_0) = \\sum_{i=1}^n \\mathbf{u}_i\n$$\nFrom **Bartlett's Identities**, for each observation $i$:\n\n1. **Mean**\n\n   $E[\\mathbf{u}_i] = \\mathbf{0}$.\n\n2. **Variance**\n\n   $\\text{Var}(\\mathbf{u}_i) = E[\\mathbf{u}_i \\mathbf{u}_i^T] = \\mathbf{I}_1(\\boldsymbol{\\theta}_0)$.\n\nSince the terms $\\mathbf{u}_i$ are i.i.d. with finite variance, we apply the **Multivariate Central Limit Theorem (Lindeberg-Lévy)**:\n$$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\mathbf{u}_i \\xrightarrow{d} N(\\mathbf{0}, \\text{Var}(\\mathbf{u}_i)) = N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n$$\n:::\n#### Case B: The Non-I.I.D. Case (Regression Setting)\n\n::: {#thm-score-normality-noniid}\n#### Normality of Score (Independent, Non-Identical)\nLet $Y_1, \\dots, Y_n$ be independent but not necessarily identically distributed (e.g., due to different covariates). Let $\\mathbf{I}_n(\\boldsymbol{\\theta}_0) = \\sum_{i=1}^n \\text{Var}(\\mathbf{u}_i)$ be the total information.\n\nDefine the **Average Fisher Information Matrix**:\n$$\n\\bar{\\mathbf{I}}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\mathbf{I}_n(\\boldsymbol{\\theta}_0) = \\frac{1}{n} \\sum_{i=1}^n E[\\mathbf{u}_i \\mathbf{u}_i^T]\n$$\n\nIf the **Lindeberg Condition** is satisfied for the sequence of score vectors, then the standardized score converges to a standard Normal:\n$$\n\\left( \\mathbf{I}_n(\\boldsymbol{\\theta}_0) \\right)^{-1/2} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\xrightarrow{d} N(\\mathbf{0}, \\mathbf{I}_p)\n$$\n\n**Approximating Distribution:**\nIf the average information converges to a positive definite limit $\\bar{\\mathbf{I}}_n \\to \\mathbf{I}_{\\infty}$, we have the following approximation for the scaled average score $\\sqrt{n} \\bar{\\mathbf{U}}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y})$:\n$$\n\\sqrt{n} \\bar{\\mathbf{U}}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) \\overset{\\cdot}{\\sim} N(\\mathbf{0}, \\bar{\\mathbf{I}}_n(\\boldsymbol{\\theta}_0))\n$$\n:::\n\n::: {.proof}\nThis is a direct application of the **Lindeberg-Feller Central Limit Theorem**.\nThe total score $\\mathbf{U}_n(\\boldsymbol{\\theta}_0; \\mathbf{Y}) = \\sum \\mathbf{u}_i$ is a sum of independent random vectors with mean $\\mathbf{0}$ and variances $\\text{Var}(\\mathbf{u}_i)$.\nProvided that no single observation's score contribution dominates the sum (the Lindeberg condition), the standardized sum converges to a standard Normal distribution.\n:::\n\n### Asymptotic Normality of the MLE\n\nWe now transfer the normality from the Score function to the estimator $\\hat{\\boldsymbol{\\theta}}$ using a Taylor expansion (the Delta Method logic).\n\n::: {#def-regularity-conditions}\n#### Regularity Conditions for Asymptotic Normality\nThe following conditions are required to ensure the validity of the Taylor expansion and the convergence of the remainder term:\n\n1.  **Interiority:** The true parameter $\\boldsymbol{\\theta}_0$ lies in the interior of the parameter space $\\Theta$.\n2.  **Smoothness:** The log-likelihood function $\\log f(y|\\boldsymbol{\\theta})$ is three times continuously differentiable with respect to $\\boldsymbol{\\theta}$.\n3.  **Boundedness:** The third derivatives are bounded by an integrable function $M(y)$ (to control the Taylor remainder).\n4.  **Positive Information:** The Fisher Information Matrix $\\mathbf{I}_1(\\boldsymbol{\\theta}_0)$ exists and is non-singular (positive definite).\n5.  **Interchangeability:** Differentiation and integration can be interchanged for the density $f(y|\\boldsymbol{\\theta})$.\n:::\n\n::: {#thm-mle-normality}\n#### Asymptotic Normality of MLE\nUnder the regularity conditions above, the MLE $\\hat{\\boldsymbol{\\theta}}_n$ satisfies:\n$$\n\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} N\\left(\\mathbf{0}, [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1}\\right)\n$$\n:::\n\n::: {.proof}\n1. **Taylor Expansion of Score Equation**\n\n   We expand the Score function $\\mathbf{U}_n(\\boldsymbol{\\theta})$ around the true parameter $\\boldsymbol{\\theta}_0$. Since $\\hat{\\boldsymbol{\\theta}}_n$ is the MLE, $\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}_n) = \\mathbf{0}$.\n   \n   Define the **Observed Information Matrix** as the negative Hessian:\n   $$\n   \\mathbf{J}_n(\\boldsymbol{\\theta}) = -\\nabla^2 \\ell_n(\\boldsymbol{\\theta})\n   $$\n\n   ::: {.callout-note appearance=\"simple\"}\n   **The Linear Link (Score vs. Parameter Error)**\n   The fundamental approximation linking the random Score vector to the estimation error is:\n   $$\n   \\underbrace{\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}_n)}_{=\\mathbf{0}} \\approx \\mathbf{U}_n(\\boldsymbol{\\theta}_0) - \\mathbf{J}_n(\\boldsymbol{\\theta}_0) (\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   $$\n   \\implies \\quad \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\approx \\mathbf{J}_n(\\boldsymbol{\\theta}_0) (\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   :::\n\n2. **Scaling and Inversion**\n\n   Multiply the link equation by $\\frac{1}{\\sqrt{n}}$ and introduce $n$ to the Observed Information term to stabilize the limits:\n   $$\n   \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\approx \\left( \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) \\right) \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0)\n   $$\n   Rearranging to solve for the estimator's distribution:\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\approx \\left[ \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) \\right]^{-1} \\left[ \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\right]\n   $$\n\n3. **Convergence of Components**\n\n   * **Numerator (Score):** From the I.I.D. Score Normality theorem:\n       $$\n       \\frac{1}{\\sqrt{n}} \\mathbf{U}_n(\\boldsymbol{\\theta}_0) \\xrightarrow{d} Z \\sim N(\\mathbf{0}, \\mathbf{I}_1(\\boldsymbol{\\theta}_0))\n       $$\n   * **Denominator (Observed Info):** By the **WLLN**, the average observed information converges to the expected information:\n       $$\n       \\frac{1}{n} \\mathbf{J}_n(\\boldsymbol{\\theta}_0) = -\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\log f(Y_i|\\boldsymbol{\\theta}_0) \\xrightarrow{p} -E[\\nabla^2 \\log f] = \\mathbf{I}_1(\\boldsymbol{\\theta}_0)\n       $$\n\n4. **Slutsky's Theorem**\n\n   Combining these via Slutsky's Theorem (matrix inverse is a continuous mapping):\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1} Z\n   $$\n   The variance of this limiting distribution is:\n   $$\n   \\text{Var}([\\mathbf{I}_1]^{-1} Z) = \\mathbf{I}_1^{-1} \\text{Var}(Z) (\\mathbf{I}_1^{-1})^T = \\mathbf{I}_1^{-1} \\mathbf{I}_1 \\mathbf{I}_1^{-1} = \\mathbf{I}_1^{-1}\n   $$\n   Thus:\n   $$\n   \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0) \\xrightarrow{d} N\\left( \\mathbf{0}, [\\mathbf{I}_1(\\boldsymbol{\\theta}_0)]^{-1} \\right)\n   $$\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# --- 1. Settings and Data Generation (Cauchy) ---\nset.seed(42)\nn_sim <- 10\nn_obs <- 10\ntheta_0 <- 10 \ngamma <- 1    \n\n# Generate Datasets\ndatasets <- matrix(rcauchy(n_sim * n_obs, location = theta_0, scale = gamma), \n                   nrow = n_sim, ncol = n_obs)\n\n# Define Score and Hessian Functions\nscore_cauchy <- function(y, theta) {\n  sum( (2 * (y - theta)) / (1 + (y - theta)^2) )\n}\nhessian_cauchy <- function(y, theta) {\n  u <- y - theta\n  sum( (2 * (u^2 - 1)) / (1 + u^2)^2 )\n}\n\n# --- 2. Analyze Main Dataset ---\ny_main <- datasets[1, ]\n# Find MLE numerically\nmle_res <- optimize(function(th) sum(dcauchy(y_main, location = th, log = TRUE)),\n                    interval = c(theta_0 - 5, theta_0 + 5), maximum = TRUE)\ntheta_hat <- mle_res$maximum\n\n# Calculate Geometry at theta_0\nu_true <- score_cauchy(y_main, theta_0)\nslope_true <- hessian_cauchy(y_main, theta_0)\n\n# --- 3. Prepare Plot Data ---\ntheta_seq <- seq(8, 12, length.out = 300)\n\n# Curves Data\nplot_data <- data.frame()\nfor(i in 1:n_sim) {\n  y_curr <- datasets[i, ]\n  u_vals <- sapply(theta_seq, function(th) score_cauchy(y_curr, th))\n  plot_data <- rbind(plot_data, data.frame(theta = theta_seq, U = u_vals, sim_id = as.factor(i), is_main = (i == 1)))\n}\n\n# Linear Approximation Data (Tangent Line)\ntangent_data <- data.frame(\n  theta = theta_seq,\n  U_lin = u_true + slope_true * (theta_seq - theta_0)\n)\n\n# --- 4. Plotting ---\nggplot() +\n  # Axes Reference\n  geom_hline(yintercept = 0, color = \"black\", size = 0.5) +\n  geom_vline(xintercept = theta_0, color = \"black\", linetype = \"dotted\") +\n  \n  # A. Background Curves (Grey)\n  geom_line(data = subset(plot_data, !is_main), \n            aes(x = theta, y = U, group = sim_id), \n            color = \"grey80\", alpha = 0.5) +\n  \n  # B. Main Curve (Blue) & Tangent (Red Dashed)\n  geom_line(data = subset(plot_data, is_main), aes(x = theta, y = U), \n            color = \"blue\", size = 1.2) +\n  geom_line(data = tangent_data, aes(x = theta, y = U_lin), \n            color = \"red\", linetype = \"dashed\", size = 1) +\n  \n  # C. THICK SEGMENTS for Delta U and Delta Theta\n  # Vertical Segment (Delta U): From axis to curve at theta_0\n  # Color: Purple\n  annotate(\"segment\", x = theta_0, xend = theta_0, y = 0, yend = u_true,\n           color = \"purple\", size = 2, alpha = 0.8) +\n  \n  # Horizontal Segment (Delta Theta): From theta_0 to theta_hat along the axis\n  # Color: Dark Green\n  annotate(\"segment\", x = theta_0, xend = theta_hat, y = 0, yend = 0,\n           color = \"darkgreen\", size = 2, alpha = 0.8) +\n  \n  # D. Points\n  geom_point(aes(x = theta_0, y = u_true), color = \"red\", size = 3) +\n  geom_point(aes(x = theta_hat, y = 0), color = \"darkgreen\", size = 3) +\n  \n  # E. Labels (Delta labels)\n  annotate(\"text\", x = theta_0 + 0.15, y = u_true / 2, \n           label = \"Delta~U\", parse = TRUE, color = \"purple\", fontface = \"bold\", hjust = 0) +\n  \n  annotate(\"text\", x = (theta_0 + theta_hat)/2, y = -1.0, \n           label = \"hat(theta)[n] - theta[0]\", parse = TRUE, color = \"darkgreen\", fontface = \"bold\") +\n  \n  # F. Parameter Labels (Revised Position)\n  annotate(\"text\", x = theta_hat, y = 1, label = \"hat(theta)[n]\", parse = TRUE, color = \"darkgreen\", size = 5) +\n  annotate(\"text\", x = theta_0, y = 1, label = \"theta[0]\", parse = TRUE, color = \"red\", size = 5) +\n\n  # G. Styling\n  coord_cartesian(ylim = c(-10, 10), xlim = c(8.5, 11.5)) +\n  labs(title = expression(paste(\"Score Function Approximation (Cauchy)\")),\n       subtitle = expression(paste(\"Linear Link: \", Delta, \"U \", approx, \" -J \", Delta, \"theta\")),\n       x = expression(theta), y = expression(U(theta))) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Linear Approximation of the Score Function: The vertical purple segment represents the Score at the true parameter ($\\Delta U$), while the horizontal green segment shows the estimation error ($\\hat{\\theta}_n - \\theta_0$). The red dashed line indicates the linear approximation ($-J(\\theta_0)$).](mle_files/figure-html/fig-score-approx-1.png){#fig-score-approx width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n### Wilks' Theorem (Likelihood Ratio Test)\n\nThis theorem connects the Likelihood Ratio statistic to the Chi-squared distribution.\n\n::: {#thm-wilks}\n#### Wilks' Theorem\nConsider testing $H_0: \\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0$ against $H_1: \\boldsymbol{\\theta} \\ne \\boldsymbol{\\theta}_0$.\nLet $\\Lambda_n$ be the likelihood ratio:\n$$\n\\Lambda_n = \\frac{L(\\boldsymbol{\\theta}_0)}{L(\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}})}\n$$\nThen the Deviance statistic ($-2 \\log \\Lambda$) converges to a Chi-squared distribution:\n$$\n-2 \\log \\Lambda_n = 2 [\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}_0)] \\xrightarrow{d} \\chi^2_p\n$$\nwhere $p$ is the dimension of $\\boldsymbol{\\theta}$.\n:::\n\n::: {.proof}\n1. **Taylor Expansion of Log-Likelihood**\n\n   Expand $\\ell_n(\\boldsymbol{\\theta}_0)$ around the MLE $\\hat{\\boldsymbol{\\theta}}$. Note that the first derivative term vanishes because $\\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}) = \\mathbf{0}$.\n   $$\n   \\ell_n(\\boldsymbol{\\theta}_0) \\approx \\ell_n(\\hat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta}_0 - \\hat{\\boldsymbol{\\theta}})^T \\mathbf{U}_n(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta}_0 - \\hat{\\boldsymbol{\\theta}})^T \\mathbf{H}_n(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta}_0 - \\hat{\\boldsymbol{\\theta}})\n   $$\n   $$\n   \\ell_n(\\boldsymbol{\\theta}_0) \\approx \\ell_n(\\hat{\\boldsymbol{\\theta}}) + 0 + \\frac{1}{2} (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\mathbf{H}_n(\\hat{\\boldsymbol{\\theta}}) (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n   $$\n\n2. **Rearranging for Deviance**\n\n   $$\n   2[\\ell_n(\\hat{\\boldsymbol{\\theta}}) - \\ell_n(\\boldsymbol{\\theta}_0)] \\approx - (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\mathbf{H}_n(\\hat{\\boldsymbol{\\theta}}) (\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n   $$\n   Multiply and divide by $n$:\n   $$\n   = - \\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\left[ \\frac{1}{n} \\mathbf{H}_n(\\hat{\\boldsymbol{\\theta}}) \\right] \\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n   $$\n\n3. **Applying Asymptotic Results**\n\n   * $\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0) \\xrightarrow{d} Z \\sim N(\\mathbf{0}, \\mathbf{I}^{-1})$.\n   * $-\\frac{1}{n} \\mathbf{H}_n(\\hat{\\boldsymbol{\\theta}}) \\xrightarrow{p} \\mathbf{I}$ (Fisher Information).\n\n   Substituting these limits:\n   $$\n   -2 \\log \\Lambda_n \\approx Z^T \\mathbf{I} Z\n   $$\n   Let $Y = \\mathbf{I}^{1/2} Z$. Since $Z \\sim N(\\mathbf{0}, \\mathbf{I}^{-1})$, the variance of $Y$ is:\n   $$\n   \\text{Var}(Y) = \\mathbf{I}^{1/2} \\text{Var}(Z) (\\mathbf{I}^{1/2})^T = \\mathbf{I}^{1/2} \\mathbf{I}^{-1} \\mathbf{I}^{1/2} = \\mathbf{I}_p \\quad \\text{(Identity Matrix)}\n   $$\n   Thus, $Y \\sim N(\\mathbf{0}, \\mathbf{I}_p)$, and the quadratic form becomes:\n   $$\n   Z^T \\mathbf{I} Z = Y^T Y = \\sum_{j=1}^p Y_j^2 \\sim \\chi^2_p\n   $$\n:::\n\n## Hypothesis Testing: Likelihood Ratio Test\n\nConsider testing:\n$$\nH_0: \\theta \\in \\Theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\in \\Theta \\setminus \\Theta_0\n$$\nwhere $\\dim(\\Theta) = p$ and $\\dim(\\Theta_0) = p-m$.\n\nThe Likelihood Ratio Statistic is:\n$$\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta; x)}{\\sup_{\\theta \\in \\Theta} L(\\theta; x)} = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n$$\n\n::: {#thm-wilks}\n\n## Wilks' Theorem\nUnder regularity conditions, under $H_0$:\n$$\n-2 \\log \\Lambda \\xrightarrow{d} \\chi^2_m\n$$\nwhere $m$ is the difference in dimensions (number of restrictions).\n\n:::\n\n::: {#exm-normal-mean-lrt}\nLet $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$.\n$H_0: \\mu = \\mu_0$ vs $H_1: \\mu \\ne \\mu_0$.\nHere $p=2$ ($\\mu, \\sigma^2$) and under $H_0$, free parameters = 1 ($\\sigma^2$). So $m = 2-1 = 1$.\n\nThe statistic $\\Lambda$:\n$$\n\\Lambda = \\left( \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}_0^2} \\right)^{n/2} = \\left( \\frac{\\sum(x_i - \\bar{x})^2}{\\sum(x_i - \\mu_0)^2} \\right)^{n/2}\n$$\nIt can be shown that:\n$$\n\\Lambda = \\left( 1 + \\frac{(\\bar{x} - \\mu_0)^2}{\\hat{\\sigma}^2} \\right)^{-n/2}\n$$\nThe rejection region $-2 \\log \\Lambda > \\chi^2_{1, \\alpha}$ is equivalent to the t-test:\n$$\n\\left| \\frac{\\bar{x} - \\mu_0}{S/\\sqrt{n}} \\right| > t_{n-1, \\alpha/2}\n$$\n\n:::\n\n## Illustration of Wilks' Theorem\n\nWe can approximate the statistic using Taylor expansion. Consider the scalar case.\n$$\n-2 \\log \\Lambda = 2 [l(\\hat{\\theta}) - l(\\theta_0)]\n$$\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Approximation of Likelihood Ratio](mle_files/figure-html/fig-wilks-proof-1.png){#fig-wilks-proof fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\nUsing Taylor expansion around the MLE $\\hat{\\theta}$:\n$$\nl(\\theta_0) \\approx l(\\hat{\\theta}) + (\\theta_0 - \\hat{\\theta})l'(\\hat{\\theta}) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta})\n$$\nSince $l'(\\hat{\\theta}) = 0$:\n$$\n2[l(\\hat{\\theta}) - l(\\theta_0)] \\approx -(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta}) = (\\hat{\\theta} - \\theta_0)^2 [-\\frac{1}{n} l''(\\hat{\\theta})] \\cdot n\n$$\nAs $n \\to \\infty$, $-\\frac{1}{n}l'' \\to I(\\theta_0)$ and $\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\to N(0, 1/I)$.\nThus, the expression behaves like $Z^2 \\sim \\chi^2_1$.\n\n\n## An Example: Poisson Regression\n\nIn this example, we explore the Generalized Linear Model (GLM) for count data using the Poisson distribution. Let $Y_1, \\dots, Y_n$ be independent count variables where $Y_i \\sim \\text{Poisson}(\\lambda_i)$. The expected count $\\lambda_i$ is related to a vector of covariates $\\mathbf{x}_i$ and parameters $\\boldsymbol{\\beta}$ via the **canonical log link function**:\n$$ \\log(\\lambda_i) = \\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik} $$\n\n### Canonical Representation\n\nWe begin by expressing the log-likelihood in the canonical exponential family form. The probability mass function for the Poisson distribution is $P(Y_i=y_i) = \\frac{e^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!}$. The log-likelihood is:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\log(\\lambda_i) - \\lambda_i - \\log(y_i!) \\right) $$\n\nSubstituting the log link $\\log(\\lambda_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ and $\\lambda_i = e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}} \\right) + \\text{const} $$\nRearranging terms to isolate the parameters $\\beta_j$:\n$$ \\ell(\\boldsymbol{\\beta}; \\mathbf{y}) = \\sum_{j=0}^k \\beta_j \\underbrace{\\left( \\sum_{i=1}^n y_i x_{ij} \\right)}_{T_j(\\mathbf{y})} - \\underbrace{\\sum_{i=1}^n e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}}_{A(\\boldsymbol{\\beta})} + \\text{const} $$\n\nFrom this form, we identify:\n\n* **Sufficient Statistics:** $\\mathbf{T}(\\mathbf{y}) = \\mathbf{X}^\\top \\mathbf{y}$.\n* **Log-Partition Function:** $A(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n e^{\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}$.\n\n### The Score and Information\n\nNext, we derive the gradient and Hessian of the log-likelihood.\n\n* **Score Vector ($\\mathbf{U}$):** The gradient of $\\ell(\\boldsymbol{\\beta})$ is the difference between the observed sufficient statistics and their expectations (derived from $\\nabla A$).\n    $$ \\mathbf{U}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\ell = \\mathbf{T}(\\mathbf{y}) - \\nabla A(\\boldsymbol{\\beta}) = \\sum_{i=1}^n y_i \\mathbf{x}_i - \\sum_{i=1}^n \\lambda_i \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\lambda}) $$\n\n* **Fisher Information Matrix ($\\mathcal{J}$):** This is the negative Hessian of the log-likelihood, or equivalently the Hessian of the log-partition function $A(\\boldsymbol{\\beta})$.\n    $$ \\mathcal{J}(\\boldsymbol{\\beta}) = \\nabla^2 A(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\mathbf{x}_i \\frac{\\partial \\lambda_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\sum_{i=1}^n \\lambda_i \\mathbf{x}_i \\mathbf{x}_i^\\top $$\n    In matrix notation, $\\mathcal{J}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W} = \\text{diag}(\\lambda_i)$. Note that for the Poisson model, the variance equals the mean $\\lambda_i$.\n\n### Asymptotic Distributions (Theory)\n\na.  **Normality of the Score:**\n    The Score vector $\\mathbf{U}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (Y_i - \\lambda_i)\\mathbf{x}_i$ is a sum of independent mean-zero random vectors.\n\n    * Mean: $E[\\mathbf{U}] = \\mathbf{0}$.\n    * Variance: $\\text{Var}(\\mathbf{U}) = \\sum \\text{Var}(Y_i)\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathcal{J}(\\boldsymbol{\\beta})$.\n    By the **Multivariate Central Limit Theorem**, as $n \\to \\infty$:\n    $$ \\frac{1}{\\sqrt{n}}\\mathbf{U}(\\boldsymbol{\\beta}) \\xrightarrow{d} N(\\mathbf{0}, \\mathcal{I}(\\boldsymbol{\\beta})) $$\n    where $\\mathcal{I}$ is the limit of $\\frac{1}{n}\\mathcal{J}$.\n\nb.  **Normality of the Estimator:**\n    The MLE $\\hat{\\boldsymbol{\\beta}}$ satisfies $\\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$. Taking a first-order Taylor expansion around the true parameter $\\boldsymbol{\\beta}$:\n    $$ \\mathbf{0} = \\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{U}(\\boldsymbol{\\beta}) - \\mathcal{J}(\\boldsymbol{\\beta}) (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) $$\n    Rearranging gives $(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\approx \\mathcal{J}^{-1}(\\boldsymbol{\\beta}) \\mathbf{U}(\\boldsymbol{\\beta})$. Since $\\mathbf{U}$ is asymptotically normal, the linear transformation implies:\n    $$ \\sqrt{n}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\xrightarrow{d} N\\left(\\mathbf{0}, \\mathcal{I}^{-1}(\\boldsymbol{\\beta})\\right) $$\n\n### Numerical Application in R\n\nWe now implement the Newton-Raphson algorithm in R to estimate parameters for a Poisson regression model with a **continuous covariate**. Let $\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_i$.\n\n**a. Data Generation**\nWe simulate $n=100$ observations using a continuous predictor $x$ drawn from a Uniform distribution.\n\n```r\nset.seed(123)\nn <- 100\nx <- runif(n, 0, 1)\n\n# True parameters: Intercept=0.5, Slope=2.0\nbeta_true <- c(0.5, 2.0) \n\n# Generate response Y\nlambda_true <- exp(beta_true[1] + beta_true[2] * x)\ny <- rpois(n, lambda_true)\n\n# Design Matrix (intercept column + covariate)\nX <- cbind(1, x)\n\n```\n\n**b. Newton-Raphson Implementation**\nThe update rule is $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\mathcal{J}^{-1}\\mathbf{U}$. We implement this iteratively.\n\n```r\nnewton_raphson_poisson <- function(X, y, tol = 1e-6, max_iter = 100) {\n  beta <- rep(0, ncol(X)) # Start at 0\n  \n  for (i in 1:max_iter) {\n    # 1. Compute means and weights\n    eta <- X %*% beta\n    lambda <- as.vector(exp(eta))\n    \n    # 2. Compute Score U and Info J\n    U <- crossprod(X, y - lambda)\n    J <- crossprod(X * lambda, X) # Efficient t(X) %*% W %*% X\n    \n    # 3. Update beta\n    delta <- solve(J, U)\n    beta_new <- beta + as.vector(delta)\n    \n    # 4. Check convergence\n    diff <- sum((beta_new - beta)^2)\n    cat(sprintf(\"Iter %d: beta0=%.4f, beta1=%.4f, diff=%.6f\\n\", \n                i, beta_new[1], beta_new[2], diff))\n    \n    if (diff < tol) return(beta_new)\n    beta <- beta_new\n  }\n}\n\nbeta_mle <- newton_raphson_poisson(X, y)\n\n```\n\n**Output:**\n\n```\nIter 1: beta0=1.1718, beta1=0.6234, diff=1.761592\nIter 2: beta0=0.6970, beta1=1.5794, diff=1.139433\nIter 3: beta0=0.4996, beta1=2.0101, diff=0.224446\nIter 4: beta0=0.4725, beta1=2.0838, diff=0.006173\nIter 5: beta0=0.4722, beta1=2.0845, diff=0.000001\n\n```\n\n**c. Discussion & Verification**\nWe compare our manual estimates with R's built-in `glm` function.\n\n```r\ncat(\"Manual MLE: \", beta_mle, \"\\n\")\ncat(\"GLM Output: \", coef(glm(y ~ x, family = \"poisson\")))\n\n```\n\n**Observation:** The manual implementation converges to the exact same values as the built-in function ($\\hat{\\beta}_0 \\approx 0.47, \\hat{\\beta}_1 \\approx 2.08$). Notice that we started at $\\boldsymbol{\\beta}=(0,0)$. In the first iteration, the algorithm took a large step because the log-likelihood surface is steep. The quadratic convergence of Newton-Raphson is evident in the rapid decrease of the difference term (from 1.13 to 0.22 to 0.006).\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}