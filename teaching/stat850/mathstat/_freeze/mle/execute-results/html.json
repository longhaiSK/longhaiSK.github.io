{
  "hash": "128dabbe52019c31e4a9b23648a30bc2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation and Likelihood Theory\"\nengine: knitr\nformat: \n  html: default\n  pdf: default\n---\n\n\n\n\n\n\n\n\n\n\n\n## Definitions\n\n1.  **Likelihood Function**\n    Let $f(x|\\theta)$ be the probability density function (or mass function). The likelihood function is:\n    $$\n    L(\\theta; x) = f(x|\\theta)\n    $$\n\n2.  **Log-likelihood**\n    $$\n    l(\\theta; x) = \\log L(\\theta; x) = \\log f(x|\\theta)\n    $$\n\n3.  **Score Function**\n    The score function is the derivative of the log-likelihood with respect to the parameter $\\theta$:\n    $$\n    S(\\theta; x) = \\frac{\\partial}{\\partial \\theta} l(\\theta; x) = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta; x)\n    $$\n\n4.  **Maximum Likelihood Estimator (MLE)**\n    The MLE is the value that maximizes the likelihood function:\n    $$\n    \\hat{\\theta}_{\\text{MLE}}(x) = \\operatorname{\\text{argmax}}_{\\theta} L(\\theta; x) = \\operatorname{\\text{argmax}}_{\\theta} l(\\theta; x)\n    $$\n    An approach to finding $\\hat{\\theta}$ is to solve the score equation:\n    $$\n    \\forall_{\\theta}, \\quad S(\\theta; x) = 0\n    $$\n\n::: {#exm-uniform-mle}\n\n## Uniform Distribution MLE\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{\\text{Unif}}(0, \\theta)$.\n\nThe likelihood function is:\n$$\nL(\\theta; x) = \\prod_{i=1}^{n} f(x_i | \\theta) = \\frac{1}{\\theta^n} I(X_{(n)} < \\theta)\n$$\nwhere $X_{(n)} = \\max\\{X_1, \\dots, X_n\\}$.\n\nTo maximize this function, we observe that $L(\\theta)$ decreases as $\\theta$ increases, but $\\theta$ must be at least $X_{(n)}$. Therefore:\n$$\n\\hat{\\theta}_{\\text{MLE}}(x) = X_{(n)}\n$$\n\n**Properties of this estimator:**\nThe CDF of $X_{(n)}$ is:\n$$\nP(X_{(n)} \\le x) = [P(X_1 \\le x)]^n = \\left(\\frac{x}{\\theta}\\right)^n \\quad \\text{for } 0 < x < \\theta\n$$\nThe PDF is $f_{X_{(n)}}(x) = n \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta}$.\n\nThe expected value is:\n$$\nE(X_{(n)}) = \\int_{0}^{\\theta} x \\cdot \\frac{n x^{n-1}}{\\theta^n} dx = \\frac{n}{\\theta^n} \\int_{0}^{\\theta} x^n dx = \\frac{n}{\\theta^n} \\left[ \\frac{x^{n+1}}{n+1} \\right]_0^{\\theta} = \\frac{n}{n+1}\\theta < \\theta\n$$\nThus, it is a biased estimator.\n\n:::\n\n::: {#exm-normal-mle}\n\n## Normal Distribution MLE\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)$. Let $\\theta = (\\mu, \\sigma^2)$.\n\nThe likelihood is:\n$$\nL(\\theta; x) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\nThe log-likelihood is:\n$$\nl(\\theta; x) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2}\n$$\n\n**Score Functions:**\n\n1.  With respect to $\\mu$:\n    $$\n    \\frac{\\partial l}{\\partial \\mu} = \\frac{2\\sum(x_i - \\mu)}{2\\sigma^2} = \\frac{\\sum(x_i - \\mu)}{\\sigma^2} = 0 \\implies \\hat{\\mu}_{\\text{MLE}} = \\bar{x}\n    $$\n\n2.  With respect to $\\sigma^2$:\n    $$\n    \\frac{\\partial l}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\sum(x_i - \\mu)^2}{2(\\sigma^2)^2} = 0\n    $$\n    Solving for $\\sigma^2$:\n    $$\n    \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\sum(x_i - \\hat{\\mu})^2}{n} = \\frac{\\sum(x_i - \\bar{x})^2}{n}\n    $$\n**Bias:**\n    * $E(S^2) = \\sigma^2$ (Unbiased)\n    * $E(\\hat{\\sigma}^2_{\\text{MLE}}) = E\\left(\\frac{n-1}{n} S^2\\right) = \\frac{n-1}{n} \\sigma^2 \\ne \\sigma^2$ (Biased)\n:::\n\n## Properties of Score and Fisher Information\n\n::: {#def-fisher-information}\n\n### Definition of Fisher Information\nSome properties about the Score function $S(\\theta; x)$:\n\n1.  **Mean:** $E[S(\\theta; x) | \\theta] = 0$.\n   2.  **Variance/Covariance:**\n    $$\n    \\text{Cov}(S_i(\\theta; x), S_j(\\theta; x)) = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n    $$\n    The Fisher Information matrix $I(\\theta)$ is defined as:\n    $$\n    I(\\theta) = \\text{Cov}(S(\\theta; x)) = E[S(\\theta; x) S(\\theta; x)^T] = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta^2} \\right]\n    $$\n    Note: $J(\\theta, x) = -\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_k} l(\\theta; x)$ is the Observed Fisher Information. $I(\\theta) = E[J(\\theta, x)]$.\n\n:::\n\n::: {#thm-score-properties}\n\n## Properties of Score Function\nGiven the support of $X$ is free of $\\theta$:\n\n1.  $E_X[S(\\theta; x)] = 0$\n   2.  $\\text{Cov}(S(\\theta; x)) = I(\\theta)$\n\n:::\n\n::: {.proof}\n**Proof of Mean 0:**\n$$\n\\begin{aligned}\nE[S(\\theta; X)] &= \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx \\quad \\text{(assuming regularity conditions allow interchange)} \\\\\n&= \\frac{\\partial}{\\partial \\theta} (1) = 0\n\\end{aligned}\n$$\n\n**Proof of Variance:**\nDifferentiating $\\int f(x|\\theta) dx = 1$ twice with respect to $\\theta$ leads to the identity:\n$$\n\\text{Var}(S(\\theta)) = E\\left[ \\left(\\frac{\\partial l}{\\partial \\theta}\\right)^2 \\right] = -E\\left[ \\frac{\\partial^2 l}{\\partial \\theta^2} \\right]\n$$\n\n:::\n\n::: {#rem-fisher-iid}\nIf $X_1, \\dots, X_n$ are i.i.d with $f(x|\\theta)$, then:\n$$\nl(\\theta; x) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n$$\nThe score function is the sum of individual score functions. Since variance of a sum of independent variables is the sum of variances:\n$$\nI_n(\\theta) = n I_1(\\theta)\n$$\nwhere $I_1(\\theta) = E\\left[ -\\frac{\\partial^2}{\\partial \\theta^2} \\log f(X_1|\\theta) \\right]$.\n\n:::\n\n## Cramer-Rao Lower Bound (CRLB)\n\n::: {#thm-crlb}\n\n## Cramer-Rao Lower Bound\nLet $W(X)$ be any estimator with $m(\\theta) = E[W(X)]$. Under regularity conditions (support of X independent of $\\theta$),\n$$\n\\text{Var}(W(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n**Particular Case:** If $W(X)$ is an unbiased estimator of $\\theta$ (i.e., $m(\\theta) = \\theta, m'(\\theta)=1$), then:\n$$\n\\text{Var}(W(X)) \\ge \\frac{1}{I(\\theta)}\n$$\n\n:::\n\n::: {.proof}\nLet $Z = S(\\theta; X)$. We know $E[Z] = 0$ and $\\text{Var}(Z) = I(\\theta)$.\nUsing the Covariance inequality:\n$$\n[\\text{Cov}(W, Z)]^2 \\le \\text{Var}(W) \\text{Var}(Z)\n$$\nEvaluate the covariance:\n$$\n\\begin{aligned}\n\\text{Cov}(W, Z) &= E[W(X) S(\\theta; X)] - E[W]E[S] \\\\\n&= \\int w(x) \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx - 0 \\\\\n&= \\int w(x) \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int w(x) f(x|\\theta) dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} m(\\theta) = m'(\\theta)\n\\end{aligned}\n$$\nTherefore:\n$$\n[m'(\\theta)]^2 \\le \\text{Var}(W) \\cdot I(\\theta) \\implies \\text{Var}(W) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n$$\n\n:::\n\n::: {#exm-exponential-crlb}\nLet $X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Exp}(\\theta)$, where $f(x|\\theta) = \\frac{1}{\\theta} e^{-x/\\theta}$.\nLog-likelihood:\n$$\nl(\\theta; x) = -n \\log \\theta - \\frac{1}{\\theta} \\sum x_i\n$$\nScore function:\n$$\nS(\\theta; x) = -\\frac{n}{\\theta} + \\frac{\\sum x_i}{\\theta^2}\n$$\nSetting $S=0 \\implies \\hat{\\theta}_{\\text{MLE}} = \\bar{x}$.\n\nFisher Information:\n$$\nI(\\theta) = \\text{Var}(S) = \\frac{1}{\\theta^4} \\text{Var}(\\sum X_i) = \\frac{1}{\\theta^4} n \\theta^2 = \\frac{n}{\\theta^2}\n$$\nAlternatively using the second derivative:\n$$\nS'(\\theta) = \\frac{n}{\\theta^2} - \\frac{2\\sum x_i}{\\theta^3}\n$$\n$$\n-E[S'] = -\\left( \\frac{n}{\\theta^2} - \\frac{2 n \\theta}{\\theta^3} \\right) = -\\left( \\frac{n}{\\theta^2} - \\frac{2n}{\\theta^2} \\right) = \\frac{n}{\\theta^2}\n$$\n\nVariance of MLE:\n$$\n\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\theta^2}{n}\n$$\nCRLB for unbiased estimator:\n$$\n\\text{CRLB} = \\frac{1}{I(\\theta)} = \\frac{1}{n/\\theta^2} = \\frac{\\theta^2}{n}\n$$\nSince $\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{CRLB}$, the MLE is efficient.\n\n:::\n\n## Asymptotic Properties of MLE\n\nThere are three main asymptotic properties:\n\n1.  **Consistency:** $\\hat{\\theta}_n \\xrightarrow{p} \\theta_0$\n   2.  **Asymptotic Normality:** $\\hat{\\theta}_n \\sim N(\\theta_0, 1/I(\\theta_0))$ roughly.\n   3.  **Efficiency:** Variance achieves CRLB asymptotically.\n\n## Review of Convergence\n\n1.  **LLN (Law of Large Numbers):** $\\bar{X} \\xrightarrow{p} E(X)$.\n   2.  **CLT (Central Limit Theorem):** $\\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2)$.\n   3.  **Slutsky's Theorem:** If $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{p} a$ (constant), then:\n    * $X_n + Y_n \\xrightarrow{d} X + a$\n    * $X_n Y_n \\xrightarrow{d} aX$\n    * $X_n / Y_n \\xrightarrow{d} X/a$\n\n::: {#exm-asymptotic-normal-variance}\nLet $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$.\n$\\hat{\\sigma}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}$.\nWe can show:\n$$\n\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) \\xrightarrow{d} N(0, 2\\sigma^4)\n$$\nUsing CLT on $Y_i = (X_i - \\mu)^2$ and Slutsky's theorem.\n\n:::\n\n## Consistency\n\n::: {#thm-consistency-mle}\n\n## Consistency\nUnder regularity conditions, let $\\theta_0$ be the true parameter. Then $\\hat{\\theta}_n \\xrightarrow{p} \\theta_0$.\n\n:::\n\n**Proof Idea:**\n$\\hat{\\theta}_n$ maximizes $\\frac{1}{n} l(\\theta; x)$.\nBy LLN, $\\frac{1}{n} l(\\theta; x) \\xrightarrow{p} E[\\log f(X|\\theta)]$.\nWe compare the expected log-likelihood at $\\theta$ vs $\\theta_0$:\n$$\nE\\left[ \\log \\frac{f(X|\\theta)}{f(X|\\theta_0)} \\right] \\le \\log E\\left[ \\frac{f(X|\\theta)}{f(X|\\theta_0)} \\right] \\quad \\text{(Jensen's Inequality)}\n$$\n$$\n= \\log \\int f(x|\\theta_0) \\frac{f(x|\\theta)}{f(x|\\theta_0)} dx = \\log \\int f(x|\\theta) dx = \\log(1) = 0\n$$\nThus $E[\\log f(X|\\theta)]$ is maximized at $\\theta = \\theta_0$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Log-likelihood function maximized at MLE](mle_files/figure-html/fig-likelihood-curve-1.png){#fig-likelihood-curve fig-align='center' width=576 style=\"width: 60% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Asymptotic Normality\n\n::: {#thm-asymptotic-normality}\n\n## Asymptotic Normality of MLE\nUnder regularity conditions:\n\n1.  Support of $f(x|\\theta)$ does not depend on $\\theta$.\n   2.  Likelihood is twice continuously differentiable.\n   3.  Fisher Information exists and is positive.\n\nThen:\n$$\n\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} N\\left(0, \\frac{1}{I_1(\\theta_0)}\\right)\n$$\n\n:::\n\n::: {.proof}\n**Taylor Expansion Method:**\nExpand the score function $l'(\\theta)$ around the true parameter $\\theta_0$:\n$$\nl'(\\hat{\\theta}_n) = l'(\\theta_0) + (\\hat{\\theta}_n - \\theta_0) l''(\\theta_n^*)\n$$\nwhere $\\theta_n^*$ lies between $\\hat{\\theta}_n$ and $\\theta_0$.\nSince $\\hat{\\theta}_n$ is the MLE, $l'(\\hat{\\theta}_n) = 0$.\n$$\n0 = l'(\\theta_0) + (\\hat{\\theta}_n - \\theta_0) l''(\\theta_n^*)\n$$\nRearranging:\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) = \\frac{-\\frac{1}{\\sqrt{n}} l'(\\theta_0)}{\\frac{1}{n} l''(\\theta_n^*)}\n$$\nWe analyze the numerator and denominator:\n\n1.  **Numerator:** $E[l'(\\theta_0)] = 0$, $\\text{Var}(l'(\\theta_0)) = n I_1(\\theta_0)$.\n    By CLT: $\\frac{1}{\\sqrt{n}} l'(\\theta_0) \\xrightarrow{d} N(0, I_1(\\theta_0))$.\n\n2.  **Denominator:** By LLN and Consistency, $\\frac{1}{n} l''(\\theta_n^*) \\xrightarrow{p} E[l''(\\theta_0)] = -I_1(\\theta_0)$.\n\nCombining via Slutsky's Theorem:\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} \\frac{N(0, I_1(\\theta_0))}{I_1(\\theta_0)} \\sim N\\left(0, \\frac{1}{I_1(\\theta_0)}\\right)\n$$\n\n:::\n\n::: {#rem-vector-case}\nFor a vector parameter $\\theta \\in \\mathbb{R}^p$:\n$$\n\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} N_p(0, I(\\theta_0)^{-1})\n$$\nwhere $I(\\theta_0)$ is the Fisher Information Matrix.\n\n:::\n\n## Hypothesis Testing: Likelihood Ratio Test\n\nConsider testing:\n$$\nH_0: \\theta \\in \\Theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\in \\Theta \\setminus \\Theta_0\n$$\nwhere $\\dim(\\Theta) = p$ and $\\dim(\\Theta_0) = p-m$.\n\nThe Likelihood Ratio Statistic is:\n$$\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta; x)}{\\sup_{\\theta \\in \\Theta} L(\\theta; x)} = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n$$\n\n::: {#thm-wilks}\n\n## Wilks' Theorem\nUnder regularity conditions, under $H_0$:\n$$\n-2 \\log \\Lambda \\xrightarrow{d} \\chi^2_m\n$$\nwhere $m$ is the difference in dimensions (number of restrictions).\n\n:::\n\n::: {#exm-normal-mean-lrt}\nLet $X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)$.\n$H_0: \\mu = \\mu_0$ vs $H_1: \\mu \\ne \\mu_0$.\nHere $p=2$ ($\\mu, \\sigma^2$) and under $H_0$, free parameters = 1 ($\\sigma^2$). So $m = 2-1 = 1$.\n\nThe statistic $\\Lambda$:\n$$\n\\Lambda = \\left( \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}_0^2} \\right)^{n/2} = \\left( \\frac{\\sum(x_i - \\bar{x})^2}{\\sum(x_i - \\mu_0)^2} \\right)^{n/2}\n$$\nIt can be shown that:\n$$\n\\Lambda = \\left( 1 + \\frac{(\\bar{x} - \\mu_0)^2}{\\hat{\\sigma}^2} \\right)^{-n/2}\n$$\nThe rejection region $-2 \\log \\Lambda > \\chi^2_{1, \\alpha}$ is equivalent to the t-test:\n$$\n\\left| \\frac{\\bar{x} - \\mu_0}{S/\\sqrt{n}} \\right| > t_{n-1, \\alpha/2}\n$$\n\n:::\n\n## Illustration of Wilks' Theorem\n\nWe can approximate the statistic using Taylor expansion. Consider the scalar case.\n$$\n-2 \\log \\Lambda = 2 [l(\\hat{\\theta}) - l(\\theta_0)]\n$$\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Approximation of Likelihood Ratio](mle_files/figure-html/fig-wilks-proof-1.png){#fig-wilks-proof fig-align='center' width=576 style=\"width: 50% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nUsing Taylor expansion around the MLE $\\hat{\\theta}$:\n$$\nl(\\theta_0) \\approx l(\\hat{\\theta}) + (\\theta_0 - \\hat{\\theta})l'(\\hat{\\theta}) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta})\n$$\nSince $l'(\\hat{\\theta}) = 0$:\n$$\n2[l(\\hat{\\theta}) - l(\\theta_0)] \\approx -(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta}) = (\\hat{\\theta} - \\theta_0)^2 [-\\frac{1}{n} l''(\\hat{\\theta})] \\cdot n\n$$\nAs $n \\to \\infty$, $-\\frac{1}{n}l'' \\to I(\\theta_0)$ and $\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\to N(0, 1/I)$.\nThus, the expression behaves like $Z^2 \\sim \\chi^2_1$.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}