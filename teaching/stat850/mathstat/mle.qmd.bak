
---
title: "Maximum Likelihood Estimation"
engine: knitr
format: 
  html: default
  pdf: default
---

## Definitions and Concepts

### Likelihood and MLE

::: {#def-likelihood-mle}
### Maximum Likelihood Estimation

1.  **Likelihood Function:**
    Let $f(\mathbf{x}|\boldsymbol{\theta})$ be the joint probability density function of the data $\mathbf{X}$. When viewed as a function of the parameter $\boldsymbol{\theta}$ given fixed data $\mathbf{x}$, it is called the likelihood function:
    $$
    L(\boldsymbol{\theta}; \mathbf{x}) = f(\mathbf{x}|\boldsymbol{\theta})
    $$

2.  **Log-likelihood:**
    It is usually easier to maximize the natural logarithm of the likelihood:
    $$
    \ell(\boldsymbol{\theta}; \mathbf{x}) = \log L(\boldsymbol{\theta}; \mathbf{x})
    $$

3.  **Maximum Likelihood Estimator (MLE):**
    The MLE $\hat{\boldsymbol{\theta}}_{\text{MLE}}$ is the value that maximizes the likelihood function:
    $$
    \hat{\boldsymbol{\theta}}_{\text{MLE}}(\mathbf{x}) = \operatorname*{argmax}_{\boldsymbol{\theta} \in \Theta} \ell(\boldsymbol{\theta}; \mathbf{x})
    $$

4.  **Score Function ($\mathbf{U}$):**
    The score function is defined as the gradient of the log-likelihood with respect to the parameter vector. Finding the MLE often involves solving the score equation:
    $$
    \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) = \nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta}; \mathbf{x}) = \mathbf{0}
    $$
:::

## Examples

::: {#exm-normal-mle-score}

### Normal Distribution: MLE and Score Convergence

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We define $\boldsymbol{\theta} = (\mu, \sigma^2)^T$.

**1. Log-Likelihood**
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
$$

**2. Score Function and MLE**
The Score vector $\mathbf{U}(\boldsymbol{\theta})$ has two components:

* **Component 1 (Mean):**
    $$
    U_\mu = \frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)
    $$
    Setting $U_\mu = 0$ yields $\hat{\mu}_{\text{MLE}} = \bar{x}$.

* **Component 2 (Variance):**
    $$
    U_{\sigma^2} = \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2
    $$
    Setting $U_{\sigma^2} = 0$ yields $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum (x_i - \hat{\mu})^2$.

**3. Asymptotic Normality of the Score**
We now demonstrate that $\mathbf{U}(\boldsymbol{\theta})$ follows a Normal distribution $N(\mathbf{0}, \mathbf{I}(\boldsymbol{\theta}))$.

* **For $U_\mu$:**
    $$
    U_\mu = \frac{n}{\sigma^2} (\bar{X} - \mu)
    $$
    Since $\bar{X} \sim N(\mu, \sigma^2/n)$, $U_\mu$ is a linear transformation of a Normal variable. It is **exactly Normal**:
    $$
    E[U_\mu] = 0, \quad \text{Var}(U_\mu) = \left(\frac{n}{\sigma^2}\right)^2 \text{Var}(\bar{X}) = \frac{n^2}{\sigma^4} \frac{\sigma^2}{n} = \frac{n}{\sigma^2}
    $$
    Thus, $U_\mu \sim N(0, \frac{n}{\sigma^2})$.

* **For $U_{\sigma^2}$:**
    We rewrite $U_{\sigma^2}$ as a sum of i.i.d. variables. Let $Z_i = ((X_i - \mu)/\sigma)^2$. Note that $Z_i \sim \chi^2_1$, with variance 2.
    $$
    U_{\sigma^2} = \frac{1}{2\sigma^2} \sum_{i=1}^n \left( \left(\frac{X_i - \mu}{\sigma}\right)^2 - 1 \right) = \frac{1}{2\sigma^2} \sum_{i=1}^n (Z_i - 1)
    $$
    Since this is a sum of i.i.d. random variables with mean 0, the **Central Limit Theorem** applies:
    $$
    U_{\sigma^2} \xrightarrow{d} \text{Normal}
    $$
    The limiting variance is:
    $$
    \text{Var}(U_{\sigma^2}) = \frac{1}{4\sigma^4} \sum \text{Var}(Z_i) = \frac{1}{4\sigma^4} (n \times 2) = \frac{n}{2\sigma^4}
    $$

**Conclusion:**
The covariance matrix of the Score approaches the Fisher Information matrix:
$$
\text{Var}(\mathbf{U}) = \begin{bmatrix} \frac{n}{\sigma^2} & 0 \\ 0 & \frac{n}{2\sigma^4} \end{bmatrix} = \mathbf{I}(\boldsymbol{\theta})
$$
:::

::: {#exm-uniform-mle-boundary}

### Uniform Distribution (Boundary Case)

Let $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{\text{Unif}}(0, \theta)$. The likelihood is:
$$
L(\theta; \mathbf{x}) = \frac{1}{\theta^n} I(x_{(n)} \le \theta)
$$
This function strictly decreases for $\theta \ge x_{(n)}$ and is zero otherwise. Thus:
$$
\hat{\theta}_{\text{MLE}} = x_{(n)}
$$
Note that the Score equation approach fails here because the support depends on $\theta$, making the log-likelihood discontinuous at the boundary.
:::

## Review of Convergence Theorems for Probability



::: {#thm-lln}
### Weak Law of Large Numbers (WLLN)
Let $X_1, \dots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \mu$ and finite variance $\text{Var}(X_i) = \sigma^2 < \infty$.

Then, the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ converges in probability to $\mu$:
$$
\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$
Formal definition: For any $\epsilon > 0$, $\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$.
:::



::: {#thm-clt}
### Central Limit Theorem for IID Cases
Let $X_1, \dots, X_n$ be i.i.d. random variables with mean $E[X_i] = \mu$ and finite variance $0 < \text{Var}(X_i) = \sigma^2 < \infty$.

Then, the random variable $\sqrt{n}(\bar{X}_n - \mu)$ converges in distribution to a normal distribution with mean 0 and variance $\sigma^2$:
$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)
$$
:::

::: {#rem-clt-history}
### Historical Note: The Evolution of the CLT

While the standard i.i.d. theorem is named after **Jarl Waldemar Lindeberg** and **Paul Lévy**, they were the "closers" of a discovery process that involved many others:

1.  **Abraham de Moivre (1733):** The first to discover the "Normal curve" as an approximation to the Binomial distribution (specifically for the symmetric case $p=1/2$).

2.  **Pierre-Simon Laplace (1812):** Generalized de Moivre's work to arbitrary $p$ (the De Moivre-Laplace Theorem) and intuitively grasped that sums of independent errors tend toward normality, though his proofs lacked modern rigor.

3.  **Pafnuty Chebyshev (1887):** Attempted to prove the general CLT using the "Method of Moments." While his proof had gaps, it established the moment-based approach later completed by his student, Markov.

4.  **Aleksandr Lyapunov (1901):** Provided the first rigorous proof for independent variables using **Characteristic Functions**. However, he required a slightly stricter condition (existence of $(2+\delta)$ moments) than necessary.

5.  **Lindeberg (1922) & Lévy (1925):**
    * **Lindeberg** proved the general CLT for independent variables under the famous "Lindeberg Condition." notably using a direct "replacement method" (swapping distributions), **not** characteristic functions.
    * **Lévy** rigorously proved the i.i.d. case and investigated stable laws using Characteristic Functions, establishing them as the standard tool for such proofs.

6.  **William Feller (1935):** Proved the converse: if the individual random variables are asymptotically negligible (no single term dominates the sum), then the Lindeberg condition is not just sufficient, but **necessary** for the sum to converge to a Normal distribution. Hence, the general theorem is often called the **Lindeberg-Feller Theorem**.
:::

::: {#thm-lindeberg-feller}
### Lindeberg-Feller CLT (For Non-Identical Distributions)
This variation is crucial for regression analysis (e.g., OLS properties with fixed regressors) where variables are independent but **not** identically distributed.

Let $X_1, \dots, X_n$ be independent random variables with $E[X_i] = \mu_i$ and $\text{Var}(X_i) = \sigma_i^2$.
Define the **average variance** $\tilde{\sigma}_n^2$:
$$
\tilde{\sigma}_n^2 = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
$$

If the **Lindeberg Condition** holds:
For every $\epsilon > 0$,
$$
\lim_{n \to \infty} \frac{1}{n \tilde{\sigma}_n^2} \sum_{i=1}^n E\left[ (X_i - \mu_i)^2 \cdot I\left( |X_i - \mu_i| > \epsilon \sqrt{n \tilde{\sigma}_n^2} \right) \right] = 0
$$

Then the standardized sum converges to a standard normal:
$$
\frac{\sum_{i=1}^n (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} \xrightarrow{d} N(0, 1)
$$
:::

::: {#cor-lindeberg-feller-approx}
### Approximating Distribution for Sample Mean (Non-i.i.d.)

Under the conditions of the Lindeberg-Feller CLT, we can derive the asymptotic distribution for the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

Let $\bar{\mu}_n = \frac{1}{n}\sum_{i=1}^n \mu_i$ be the average mean. Note that the denominator in the CLT is simply $\sqrt{n} \tilde{\sigma}_n$.

The standardized sum converges to $N(0,1)$:
$$
\frac{\sum (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} = \frac{n(\bar{X}_n - \bar{\mu}_n)}{\sqrt{n} \tilde{\sigma}_n} = \frac{\sqrt{n}(\bar{X}_n - \bar{\mu}_n)}{\tilde{\sigma}_n} \xrightarrow{d} N(0, 1)
$$

This implies the following **approximate distributions** for large $n$:

1.  **For the Sample Mean:**
    $$
    \bar{X}_n \overset{\cdot}{\sim} N\left(\bar{\mu}_n, \frac{\tilde{\sigma}_n^2}{n}\right)
    $$

2.  **For the Scaled Difference (Root-n consistency):**
    $$
    \sqrt{n}(\bar{X}_n - \bar{\mu}_n) \overset{\cdot}{\sim} N\left(0, \tilde{\sigma}_n^2\right)
    $$

*Note: If all $X_i$ share the same mean $\mu$, simply replace $\bar{\mu}_n$ with $\mu$.*
:::

::: {#thm-slutsky}
### Slutsky's Theorem
Let $X_n$ and $Y_n$ be sequences of random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant, then:

1.  **Sum:** $X_n + Y_n \xrightarrow{d} X + c$
2.  **Product:** $X_n Y_n \xrightarrow{d} cX$
3.  **Quotient:** $X_n / Y_n \xrightarrow{d} X/c$ (provided $c \ne 0$)
:::

::: {#thm-generalized-slutsky}
### Generalized Slutsky's Theorem (Continuous Mapping)

The arithmetic operations in Slutsky's theorem are special cases of a broader property.

Let $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant.
Let $g: \mathbb{R}^2 \to \mathbb{R}^k$ be a function that is **continuous** at every point $(x, c)$ where $x$ is in the support of $X$.

Then:
$$
g(X_n, Y_n) \xrightarrow{d} g(X, c)
$$

This implies that for any "well-behaved" algebraic combination (polynomials, exponentials, etc.) of a sequence converging in distribution and a sequence converging in probability to a constant, the limit behaves as if the constant were substituted directly.
:::

::: {#exm-asymptotic-normal-variance}
### Asymptotic Normality of Sample Variance

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We wish to derive the asymptotic distribution of the MLE for variance, 

$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2.$$

**1. Algebraic Expansion**

We rewrite the estimator by adding and subtracting the true mean $\mu$:
$$
\sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n ((X_i - \mu) - (\bar{X} - \mu))^2
$$
$$
= \sum_{i=1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu)\sum_{i=1}^n(X_i - \mu) + n(\bar{X} - \mu)^2
$$
Since $\sum(X_i - \mu) = n(\bar{X} - \mu)$:
$$
= \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2
$$
Dividing by $n$, we get:
$$
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar{X} - \mu)^2
$$

**2. Scaling by $\sqrt{n}$**

We rearrange to look at the pivotal quantity $\sqrt{n}(\hat{\sigma}^2 - \sigma^2)$:
$$
\sqrt{n}(\hat{\sigma}^2 - \sigma^2) = \sqrt{n}\left( \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \sigma^2 \right) - \sqrt{n}(\bar{X} - \mu)^2
$$

**3. Applying Convergence Theorems**


* **Term 1 (CLT):**
* 
    Let $W_i = (X_i - \mu)^2$. Since $X_i \sim N(\mu, \sigma^2)$, $W_i/\sigma^2 \sim \chi^2_1$.
    moments of $W_i$: $E[W_i] = \sigma^2$ and $\text{Var}(W_i) = 2\sigma^4$.
    By the standard CLT:
    $$
    \sqrt{n}(\bar{W} - E[W]) = \sqrt{n}\left( \frac{1}{n}\sum (X_i - \mu)^2 - \sigma^2 \right) \xrightarrow{d} N(0, 2\sigma^4)
    $$

* **Term 2 (Slutsky):**
* 
    Consider the term $\sqrt{n}(\bar{X} - \mu)^2$. We can rewrite this as:
    $$
    \underbrace{\sqrt{n}(\bar{X} - \mu)}_{\xrightarrow{d} N(0, \sigma^2)} \cdot \underbrace{(\bar{X} - \mu)}_{\xrightarrow{p} 0 \text{ by LLN}}
    $$
    By Slutsky's Theorem, the product converges to $Z \cdot 0 = 0$.

**Conclusion:**

Combining the terms:
$$
\sqrt{n}(\hat{\sigma}^2 - \sigma^2) \xrightarrow{d} N(0, 2\sigma^4) - 0 = N(0, 2\sigma^4)
$$
:::


## Asymptotic Theory for MLE

There are three main asymptotic properties:

1.  **Consistency:** $\hat{\theta}_n \xrightarrow{p} \theta_0$
   2.  **Asymptotic Normality:** $\hat{\theta}_n \sim N(\theta_0, 1/I(\theta_0))$ roughly.
   3.  **Efficiency:** Variance achieves CRLB asymptotically.


## Consistency

::: {#thm-consistency-mle}

## Consistency
Under regularity conditions, let $\theta_0$ be the true parameter. Then $\hat{\theta}_n \xrightarrow{p} \theta_0$.

:::

**Proof Idea:**
$\hat{\theta}_n$ maximizes $\frac{1}{n} l(\theta; x)$.
By LLN, $\frac{1}{n} l(\theta; x) \xrightarrow{p} E[\log f(X|\theta)]$.
We compare the expected log-likelihood at $\theta$ vs $\theta_0$:
$$
E\left[ \log \frac{f(X|\theta)}{f(X|\theta_0)} \right] \le \log E\left[ \frac{f(X|\theta)}{f(X|\theta_0)} \right] \quad \text{(Jensen's Inequality)}
$$
$$
= \log \int f(x|\theta_0) \frac{f(x|\theta)}{f(x|\theta_0)} dx = \log \int f(x|\theta) dx = \log(1) = 0
$$
Thus $E[\log f(X|\theta)]$ is maximized at $\theta = \theta_0$.

```{tikz}
%| label: fig-likelihood-curve
%| fig-cap: "Log-likelihood function maximized at MLE"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 60% !important;"'

\begin{tikzpicture}
\draw[->] (0,0) -- (6,0) node[right] {$\theta$};
\draw[->] (0,0) -- (0,4) node[above] {$l_n(\theta; x)$};
\draw[thick] (0.5,0.5) .. controls (3,5) and (5,1) .. (5.5,0.5);
\draw[dashed] (3,0) node[below] {$\hat{\theta}_n$} -- (3,3.2);
\filldraw (3,3.2) circle (2pt);
\draw (3,3.2) node[above] {Max};
\draw (2,0) node[below] {$\theta_0 - \delta$} -- (2,0.1);
\draw (4,0) node[below] {$\theta_0 + \delta$} -- (4,0.1);
\end{tikzpicture}

```

## Asymptotic Normality

::: {#thm-asymptotic-normality}

## Asymptotic Normality of MLE
Under regularity conditions:

1.  Support of $f(x|\theta)$ does not depend on $\theta$.
   2.  Likelihood is twice continuously differentiable.
   3.  Fisher Information exists and is positive.

Then:
$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} N\left(0, \frac{1}{I_1(\theta_0)}\right)
$$

:::

::: {.proof}
**Taylor Expansion Method:**
Expand the score function $l'(\theta)$ around the true parameter $\theta_0$:
$$
l'(\hat{\theta}_n) = l'(\theta_0) + (\hat{\theta}_n - \theta_0) l''(\theta_n^*)
$$
where $\theta_n^*$ lies between $\hat{\theta}_n$ and $\theta_0$.
Since $\hat{\theta}_n$ is the MLE, $l'(\hat{\theta}_n) = 0$.
$$
0 = l'(\theta_0) + (\hat{\theta}_n - \theta_0) l''(\theta_n^*)
$$
Rearranging:
$$
\sqrt{n}(\hat{\theta}_n - \theta_0) = \frac{-\frac{1}{\sqrt{n}} l'(\theta_0)}{\frac{1}{n} l''(\theta_n^*)}
$$
We analyze the numerator and denominator:

1.  **Numerator:** $E[l'(\theta_0)] = 0$, $\text{Var}(l'(\theta_0)) = n I_1(\theta_0)$.
    By CLT: $\frac{1}{\sqrt{n}} l'(\theta_0) \xrightarrow{d} N(0, I_1(\theta_0))$.

2.  **Denominator:** By LLN and Consistency, $\frac{1}{n} l''(\theta_n^*) \xrightarrow{p} E[l''(\theta_0)] = -I_1(\theta_0)$.

Combining via Slutsky's Theorem:
$$
\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \frac{N(0, I_1(\theta_0))}{I_1(\theta_0)} \sim N\left(0, \frac{1}{I_1(\theta_0)}\right)
$$

:::

::: {#rem-vector-case}
For a vector parameter $\theta \in \mathbb{R}^p$:
$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} N_p(0, I(\theta_0)^{-1})
$$
where $I(\theta_0)$ is the Fisher Information Matrix.

:::

## Hypothesis Testing: Likelihood Ratio Test

Consider testing:
$$
H_0: \theta \in \Theta_0 \quad \text{vs} \quad H_1: \theta \in \Theta \setminus \Theta_0
$$
where $\dim(\Theta) = p$ and $\dim(\Theta_0) = p-m$.

The Likelihood Ratio Statistic is:
$$
\Lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta; x)}{\sup_{\theta \in \Theta} L(\theta; x)} = \frac{L(\hat{\theta}_0)}{L(\hat{\theta})}
$$

::: {#thm-wilks}

## Wilks' Theorem
Under regularity conditions, under $H_0$:
$$
-2 \log \Lambda \xrightarrow{d} \chi^2_m
$$
where $m$ is the difference in dimensions (number of restrictions).

:::

::: {#exm-normal-mean-lrt}
Let $X_1, \dots, X_n \sim N(\mu, \sigma^2)$.
$H_0: \mu = \mu_0$ vs $H_1: \mu \ne \mu_0$.
Here $p=2$ ($\mu, \sigma^2$) and under $H_0$, free parameters = 1 ($\sigma^2$). So $m = 2-1 = 1$.

The statistic $\Lambda$:
$$
\Lambda = \left( \frac{\hat{\sigma}^2}{\hat{\sigma}_0^2} \right)^{n/2} = \left( \frac{\sum(x_i - \bar{x})^2}{\sum(x_i - \mu_0)^2} \right)^{n/2}
$$
It can be shown that:
$$
\Lambda = \left( 1 + \frac{(\bar{x} - \mu_0)^2}{\hat{\sigma}^2} \right)^{-n/2}
$$
The rejection region $-2 \log \Lambda > \chi^2_{1, \alpha}$ is equivalent to the t-test:
$$
\left| \frac{\bar{x} - \mu_0}{S/\sqrt{n}} \right| > t_{n-1, \alpha/2}
$$

:::

## Illustration of Wilks' Theorem

We can approximate the statistic using Taylor expansion. Consider the scalar case.
$$
-2 \log \Lambda = 2 [l(\hat{\theta}) - l(\theta_0)]
$$

```{tikz}
%| label: fig-wilks-proof
%| fig-cap: "Approximation of Likelihood Ratio"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 50% !important;"'

\begin{tikzpicture}
\draw[->] (-0.5,0) -- (4,0) node[right] {$x$};
\draw[->] (0,-1) -- (0,3) node[above] {$y$};
\draw[thick, domain=-0.5:3.5] plot (\x, {ln(1+\x)});
\node at (2.5, 1.5) {$\log(1+x)$};
\draw[dashed] (0,0) -- (3,3) node[right] {$y=x$};
\draw (1,0) node[below] {$\tau^*$} -- (1, {ln(2)});
\draw (2,0) node[below] {$\tau$} -- (2, {ln(3)});
\end{tikzpicture}

```

Using Taylor expansion around the MLE $\hat{\theta}$:
$$
l(\theta_0) \approx l(\hat{\theta}) + (\theta_0 - \hat{\theta})l'(\hat{\theta}) + \frac{1}{2}(\theta_0 - \hat{\theta})^2 l''(\hat{\theta})
$$
Since $l'(\hat{\theta}) = 0$:
$$
2[l(\hat{\theta}) - l(\theta_0)] \approx -(\theta_0 - \hat{\theta})^2 l''(\hat{\theta}) = (\hat{\theta} - \theta_0)^2 [-\frac{1}{n} l''(\hat{\theta})] \cdot n
$$
As $n \to \infty$, $-\frac{1}{n}l'' \to I(\theta_0)$ and $\sqrt{n}(\hat{\theta}-\theta_0) \to N(0, 1/I)$.
Thus, the expression behaves like $Z^2 \sim \chi^2_1$.


## An Example: Poisson Regression





In this example, we explore the Generalized Linear Model (GLM) for count data using the Poisson distribution. Let $Y_1, \dots, Y_n$ be independent count variables where $Y_i \sim \text{Poisson}(\lambda_i)$. The expected count $\lambda_i$ is related to a vector of covariates $\mathbf{x}_i$ and parameters $\boldsymbol{\beta}$ via the **canonical log link function**:
$$ \log(\lambda_i) = \eta_i = \mathbf{x}_i^\top \boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} $$

### Canonical Representation

We begin by expressing the log-likelihood in the canonical exponential family form. The probability mass function for the Poisson distribution is $P(Y_i=y_i) = \frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}$. The log-likelihood is:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i \log(\lambda_i) - \lambda_i - \log(y_i!) \right) $$

Substituting the log link $\log(\lambda_i) = \mathbf{x}_i^\top \boldsymbol{\beta}$ and $\lambda_i = e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i (\mathbf{x}_i^\top \boldsymbol{\beta}) - e^{\mathbf{x}_i^\top \boldsymbol{\beta}} \right) + \text{const} $$
Rearranging terms to isolate the parameters $\beta_j$:
$$ \ell(\boldsymbol{\beta}; \mathbf{y}) = \sum_{j=0}^k \beta_j \underbrace{\left( \sum_{i=1}^n y_i x_{ij} \right)}_{T_j(\mathbf{y})} - \underbrace{\sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}_{A(\boldsymbol{\beta})} + \text{const} $$

From this form, we identify:

* **Sufficient Statistics:** $\mathbf{T}(\mathbf{y}) = \mathbf{X}^\top \mathbf{y}$.
* **Log-Partition Function:** $A(\boldsymbol{\beta}) = \sum_{i=1}^n \lambda_i = \sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$.

### The Score and Information

Next, we derive the gradient and Hessian of the log-likelihood.

* **Score Vector ($\mathbf{U}$):** The gradient of $\ell(\boldsymbol{\beta})$ is the difference between the observed sufficient statistics and their expectations (derived from $\nabla A$).
    $$ \mathbf{U}(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell = \mathbf{T}(\mathbf{y}) - \nabla A(\boldsymbol{\beta}) = \sum_{i=1}^n y_i \mathbf{x}_i - \sum_{i=1}^n \lambda_i \mathbf{x}_i = \mathbf{X}^\top (\mathbf{y} - \boldsymbol{\lambda}) $$

* **Fisher Information Matrix ($\mathcal{J}$):** This is the negative Hessian of the log-likelihood, or equivalently the Hessian of the log-partition function $A(\boldsymbol{\beta})$.
    $$ \mathcal{J}(\boldsymbol{\beta}) = \nabla^2 A(\boldsymbol{\beta}) = \sum_{i=1}^n \mathbf{x}_i \frac{\partial \lambda_i}{\partial \boldsymbol{\beta}^\top} = \sum_{i=1}^n \lambda_i \mathbf{x}_i \mathbf{x}_i^\top $$
    In matrix notation, $\mathcal{J}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}$, where $\mathbf{W} = \text{diag}(\lambda_i)$. Note that for the Poisson model, the variance equals the mean $\lambda_i$.

### Asymptotic Distributions (Theory)

a.  **Normality of the Score:**
    The Score vector $\mathbf{U}(\boldsymbol{\beta}) = \sum_{i=1}^n (Y_i - \lambda_i)\mathbf{x}_i$ is a sum of independent mean-zero random vectors.

    * Mean: $E[\mathbf{U}] = \mathbf{0}$.
    * Variance: $\text{Var}(\mathbf{U}) = \sum \text{Var}(Y_i)\mathbf{x}_i\mathbf{x}_i^\top = \mathcal{J}(\boldsymbol{\beta})$.
    By the **Multivariate Central Limit Theorem**, as $n \to \infty$:
    $$ \frac{1}{\sqrt{n}}\mathbf{U}(\boldsymbol{\beta}) \xrightarrow{d} N(\mathbf{0}, \mathcal{I}(\boldsymbol{\beta})) $$
    where $\mathcal{I}$ is the limit of $\frac{1}{n}\mathcal{J}$.

b.  **Normality of the Estimator:**
    The MLE $\hat{\boldsymbol{\beta}}$ satisfies $\mathbf{U}(\hat{\boldsymbol{\beta}}) = \mathbf{0}$. Taking a first-order Taylor expansion around the true parameter $\boldsymbol{\beta}$:
    $$ \mathbf{0} = \mathbf{U}(\hat{\boldsymbol{\beta}}) \approx \mathbf{U}(\boldsymbol{\beta}) - \mathcal{J}(\boldsymbol{\beta}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) $$
    Rearranging gives $(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \approx \mathcal{J}^{-1}(\boldsymbol{\beta}) \mathbf{U}(\boldsymbol{\beta})$. Since $\mathbf{U}$ is asymptotically normal, the linear transformation implies:
    $$ \sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \xrightarrow{d} N\left(\mathbf{0}, \mathcal{I}^{-1}(\boldsymbol{\beta})\right) $$

### Numerical Application in R

We now implement the Newton-Raphson algorithm in R to estimate parameters for a Poisson regression model with a **continuous covariate**. Let $\log(\lambda_i) = \beta_0 + \beta_1 x_i$.

**a. Data Generation**
We simulate $n=100$ observations using a continuous predictor $x$ drawn from a Uniform distribution.

```r
set.seed(123)
n <- 100
x <- runif(n, 0, 1)

# True parameters: Intercept=0.5, Slope=2.0
beta_true <- c(0.5, 2.0) 

# Generate response Y
lambda_true <- exp(beta_true[1] + beta_true[2] * x)
y <- rpois(n, lambda_true)

# Design Matrix (intercept column + covariate)
X <- cbind(1, x)

```

**b. Newton-Raphson Implementation**
The update rule is $\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \mathcal{J}^{-1}\mathbf{U}$. We implement this iteratively.

```r
newton_raphson_poisson <- function(X, y, tol = 1e-6, max_iter = 100) {
  beta <- rep(0, ncol(X)) # Start at 0
  
  for (i in 1:max_iter) {
    # 1. Compute means and weights
    eta <- X %*% beta
    lambda <- as.vector(exp(eta))
    
    # 2. Compute Score U and Info J
    U <- crossprod(X, y - lambda)
    J <- crossprod(X * lambda, X) # Efficient t(X) %*% W %*% X
    
    # 3. Update beta
    delta <- solve(J, U)
    beta_new <- beta + as.vector(delta)
    
    # 4. Check convergence
    diff <- sum((beta_new - beta)^2)
    cat(sprintf("Iter %d: beta0=%.4f, beta1=%.4f, diff=%.6f\n", 
                i, beta_new[1], beta_new[2], diff))
    
    if (diff < tol) return(beta_new)
    beta <- beta_new
  }
}

beta_mle <- newton_raphson_poisson(X, y)

```

**Output:**

```
Iter 1: beta0=1.1718, beta1=0.6234, diff=1.761592
Iter 2: beta0=0.6970, beta1=1.5794, diff=1.139433
Iter 3: beta0=0.4996, beta1=2.0101, diff=0.224446
Iter 4: beta0=0.4725, beta1=2.0838, diff=0.006173
Iter 5: beta0=0.4722, beta1=2.0845, diff=0.000001

```

**c. Discussion & Verification**
We compare our manual estimates with R's built-in `glm` function.

```r
cat("Manual MLE: ", beta_mle, "\n")
cat("GLM Output: ", coef(glm(y ~ x, family = "poisson")))

```

**Observation:** The manual implementation converges to the exact same values as the built-in function ($\hat{\beta}_0 \approx 0.47, \hat{\beta}_1 \approx 2.08$). Notice that we started at $\boldsymbol{\beta}=(0,0)$. In the first iteration, the algorithm took a large step because the log-likelihood surface is steep. The quadratic convergence of Newton-Raphson is evident in the rapid decrease of the difference term (from 1.13 to 0.22 to 0.006).

