---
title: "Introduction to Statistical Inference"
date: "January 05, 2014"
format: html
number-sections: true
---

## Course Overview

**Goal:** Compare different statistical methods.

**Textbook:** *Essentials of Statistical Inference*.
We will cover Chapters 1 to 8.

**Assessment:**
* Lab questions 
* Presentation skills 
* Term Test 1 & Term Test 2 (The higher mark is considered) 
* Final Exam 

---

## Statistical Inference Setup

We begin with observations (units) $X_1, X_2, \dots, X_n$. These may be vectors. We regard these observations as a realization of random variables.

::: {#def-population-distribution}
### Population Distribution
We assume that $X_1, X_2, \dots, X_n \sim f(x)$.
The function $f(x)$ is called the **population distribution**.
:::

### Assumptions and Scope

For simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). However, there are non-i.i.d. examples:
* **Spatial data:** Involves spatial correlation.
* **Time series:** Observations depend on time.

In **Parametric Statistics**, we assume $f(x)$ is of a known analytic form but involves unknown parameters.

::: {#exm-normal-distribution}
### Parametric Model
Consider the Normal distribution:
$$f(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
Here, the parameter space is $\Theta = \{ (\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma \in [0, +\infty) \}$.
The goal is to learn aspects of the unknown $\theta$ from observations $X_1, \dots, X_n$.
:::

## Probability vs. Statistics

There is a fundamental distinction between probability and statistics regarding the parameter $\theta$:

* **Probability:** $\theta$ is known. We ask questions about the data $X$.
* **Statistics:** $\theta$ is unknown. We use data $X_1, \dots, X_n$ to infer $\theta$ (from sample to population).

## Types of Statistical Inference

We can categorize inference into four main types:

::: {#def-point-estimation}
### Point Estimation
We use a single number to capture the parameter.
$$\hat{\theta} = \theta(X_1, \dots, X_n)$$
:::

::: {#def-interval-estimation}
### Interval Estimation
We construct an interval that likely contains the true parameter.
$$\theta \in (L(X_1, \dots, X_n), U(X_1, \dots, X_n))$$
The true parameter is within this interval.
:::

::: {#def-hypothesis-testing}
### Hypothesis Testing
We test a specific theory about the parameter.
$$H_0: \theta = \theta_0 \quad \text{vs} \quad H_1: \theta \neq \theta_0$$
(Or one-sided alternatives like $\theta > \theta_0$) .
:::

::: {#def-predictive-inference}
### Predictive Inference
Given observed data $(X_1, Y_1), \dots, (X_n, Y_n)$, we want to predict a new observation $Y_{n+1}$ given $X_{n+1}$.
This is often the primary goal in **Machine Learning**.
:::

## Standard Paradigms for Inference

There are two primary frameworks for how to perform these inferences.

### Bayesian Inference

In the Bayesian framework, we treat the parameter $\theta$ as a random variable.

1.  **Prior:** We assign a prior distribution $\pi(\theta)$.
2.  **Data Model:** We have the likelihood $f(D|\theta)$.
3.  **Posterior:** We compute the posterior distribution using Bayes' theorem:
    $$f(\theta|D) = \frac{\pi(\theta)f(D|\theta)}{\int \pi(\theta)f(D|\theta) d\theta}$$

::: {#exm-predictive-density}
### Bayesian Prediction
The predictive density for a new observation $x_{n+1}$ is obtained by integrating over the posterior:
$$f(x_{n+1}|x) = \int f(x_{n+1}|\theta) \pi(\theta|x) d\theta$$
:::

### Frequentist Inference (Fisher)

Developed largely by Fisher (c. 1920).
* **Repeated Sampling Principle:** Inference is based on the performance of methods under hypothetical repeated sampling of the data.
* Uses **Sampling Distributions**.