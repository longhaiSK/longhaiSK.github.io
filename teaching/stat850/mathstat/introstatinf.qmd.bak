---
title: "Introduction to Statistical Inference"
date: "January 05, 2014"
format: html
number-sections: true
---

## Course Overview

[cite_start]**Goal:** Compare different statistical methods[cite: 1].

[cite_start]**Textbook:** *Essentials of Statistical Inference*[cite: 1].
[cite_start]We will cover Chapters 1 to 8[cite: 1].

**Assessment:**
* [cite_start]Lab questions [cite: 1]
* [cite_start]Presentation skills [cite: 1]
* [cite_start]Term Test 1 & Term Test 2 (The higher mark is considered) [cite: 1]
* [cite_start]Final Exam [cite: 1]

---

## Statistical Inference Setup

[cite_start]We begin with observations (units) $X_1, X_2, \dots, X_n$[cite: 1]. [cite_start]These may be vectors[cite: 1]. [cite_start]We regard these observations as a realization of random variables[cite: 1].

::: {#def-population-distribution}
### Population Distribution
[cite_start]We assume that $X_1, X_2, \dots, X_n \sim f(x)$[cite: 8, 9, 10, 11].
[cite_start]The function $f(x)$ is called the **population distribution**[cite: 12, 14].
:::

### Assumptions and Scope

[cite_start]For simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.)[cite: 4, 5, 6]. However, there are non-i.i.d. examples:
* [cite_start]**Spatial data:** Involves spatial correlation[cite: 17].
* [cite_start]**Time series:** Observations depend on time[cite: 17].

[cite_start]In **Parametric Statistics**, we assume $f(x)$ is of a known analytic form but involves unknown parameters[cite: 21, 22, 25, 27].

::: {#exm-normal-distribution}
### Parametric Model
Consider the Normal distribution:
$$f(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
[cite_start]Here, the parameter space is $\Theta = \{ (\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma \in [0, +\infty) \}$[cite: 26, 28, 29, 36].
[cite_start]The goal is to learn aspects of the unknown $\theta$ from observations $X_1, \dots, X_n$[cite: 38, 40, 42].
:::

## Probability vs. Statistics

There is a fundamental distinction between probability and statistics regarding the parameter $\theta$:

* **Probability:** $\theta$ is known. [cite_start]We ask questions about the data $X$[cite: 44, 45, 46].
* **Statistics:** $\theta$ is unknown. [cite_start]We use data $X_1, \dots, X_n$ to infer $\theta$ (from sample to population)[cite: 47, 48, 49].

## Types of Statistical Inference

[cite_start]We can categorize inference into four main types[cite: 50]:

::: {#def-point-estimation}
### Point Estimation
[cite_start]We use a single number to capture the parameter[cite: 52, 55, 57].
$$\hat{\theta} = \theta(X_1, \dots, X_n)$$
:::

::: {#def-interval-estimation}
### Interval Estimation
[cite_start]We construct an interval that likely contains the true parameter[cite: 60].
$$\theta \in (L(X_1, \dots, X_n), U(X_1, \dots, X_n))$$
[cite_start]The true parameter is within this interval[cite: 63, 64].
:::

::: {#def-hypothesis-testing}
### Hypothesis Testing
[cite_start]We test a specific theory about the parameter[cite: 65].
$$H_0: \theta = \theta_0 \quad \text{vs} \quad H_1: \theta \neq \theta_0$$
(Or one-sided alternatives like $\theta > \theta_0$) [cite_start][cite: 66, 67, 69, 70].
:::

::: {#def-predictive-inference}
### Predictive Inference
[cite_start]Given observed data $(X_1, Y_1), \dots, (X_n, Y_n)$, we want to predict a new observation $Y_{n+1}$ given $X_{n+1}$[cite: 71, 75, 79, 80].
[cite_start]This is often the primary goal in **Machine Learning**[cite: 83].
:::

## Standard Paradigms for Inference

[cite_start]There are two primary frameworks for how to perform these inferences[cite: 90].

### Bayesian Inference

[cite_start]In the Bayesian framework, we treat the parameter $\theta$ as a random variable[cite: 94].

1.  [cite_start]**Prior:** We assign a prior distribution $\pi(\theta)$[cite: 95].
2.  [cite_start]**Data Model:** We have the likelihood $f(D|\theta)$[cite: 96].
3.  [cite_start]**Posterior:** We compute the posterior distribution using Bayes' theorem[cite: 97, 99]:
    $$f(\theta|D) = \frac{\pi(\theta)f(D|\theta)}{\int \pi(\theta)f(D|\theta) d\theta}$$

::: {#exm-predictive-density}
### Bayesian Prediction
[cite_start]The predictive density for a new observation $x_{n+1}$ is obtained by integrating over the posterior[cite: 81, 82]:
$$f(x_{n+1}|x) = \int f(x_{n+1}|\theta) \pi(\theta|x) d\theta$$
:::

### Frequentist Inference (Fisher)

[cite_start]Developed largely by Fisher (c. 1920)[cite: 100].
* [cite_start]**Repeated Sampling Principle:** Inference is based on the performance of methods under hypothetical repeated sampling of the data[cite: 104].
* [cite_start]Uses **Sampling Distributions**[cite: 110].