# Bayesian Methods

## Fundamental Elements of Bayesian Inference

The foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes' Theorem (or Law).

::: {#def-posterior}
### Posterior Distribution

Suppose we have a parameter $\theta$ with a prior distribution denoted by $\pi(\theta)$. If we observe data $x$ drawn from a distribution with probability density function (pdf) $f(x; \theta)$, then the **posterior density** of $\theta$ given the data $x$ is defined as:

$$
\pi(\theta|x) = \frac{\pi(\theta) f(x;\theta)}{m(x)}
$$

where $m(x)$ is the **marginal distribution** (or marginal likelihood) of the data, calculated as:
$$
m(x) = \int_{\Theta} \pi(\theta) f(x;\theta) d\theta
$$

In this context, $m(x)$ acts as a normalizing constant. Since it depends only on the data $x$ and not on the parameter $\theta$, it ensures that the posterior density integrates to 1 but does not influence the **shape** of the posterior distribution.

Thus, we often state the proportional relationship:

$$
\pi(\theta|x) \propto \pi(\theta) f(x;\theta)
$$
:::



::: {#exm-binomial-beta}
### Binomial-beta Conjugacy

Consider an experiment where $x|\theta \sim \text{Bin}(n, \theta)$. The likelihood function is:

$$
f(x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
$$

Suppose we choose a Beta distribution as the prior for $\theta$, such that $\theta \sim \text{Beta}(a, b)$. The prior density is:

$$
\pi(\theta) = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
$$

where $B(a,b)$ is the Beta function defined as $\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d\theta$.

To find the posterior, we multiply the prior and the likelihood:

$$
\pi(\theta|x) \propto \theta^{a-1}(1-\theta)^{b-1} \cdot \theta^x (1-\theta)^{n-x}
$$

Combining terms with the same base:

$$
\pi(\theta|x) \propto \theta^{a+x-1} (1-\theta)^{b+n-x-1}
$$

We can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:

$$
\theta|x \sim \text{Beta}(a+x, b+n-x)
$$

**Properties of the Posterior:**

* The posterior mean is:
    $$E(\theta|x) = \frac{a+x}{a+b+n}$$
    As $n \to \infty$, this approximates the maximum likelihood estimate $\frac{x}{n}$.

* The posterior variance is:
    $$\text{Var}(\theta|x) = \frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}$$
    For large $n$, this approximates $\frac{x(n-x)}{n^3} = \frac{\hat{p}(1-\hat{p})}{n}$.

**Numerical Illustration:**

Suppose we are estimating a probability $\theta$.

* **Prior:** $\theta \sim \text{Beta}(2, 2)$ (Mean = 0.5).
* **Data:** 10 trials, 8 successes ($n=10, x=8$).
* **Posterior:** $\theta|x \sim \text{Beta}(2+8, 2+2) = \text{Beta}(10, 4)$ (Mean $\approx$ 0.71).

The plot below shows the prior (dashed) and posterior (solid) densities.

```{r}
#| label: fig-beta-conjugacy
#| fig-cap: "Prior vs Posterior for Beta-Binomial Example"
#| echo: true

theta <- seq(0, 1, length.out = 200)

# Prior: Beta(2, 2)
prior <- dbeta(theta, shape1 = 2, shape2 = 2)

# Posterior: Beta(10, 4)
posterior <- dbeta(theta, shape1 = 10, shape2 = 4)

plot(theta, posterior, type = 'l', lwd = 2, col = "blue",
     xlab = expression(theta), ylab = "Density",
     main = "Beta Prior vs Posterior", ylim = c(0, max(c(prior, posterior))))
lines(theta, prior, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior Beta(2,2)", "Posterior Beta(10,4)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-normal-normal}
### Normal-normal Conjugacy (known Variance)

Let $X_1, X_2, \dots, X_n$ be independent and identically distributed (i.i.d.) variables such that $X_i \sim N(\mu, \sigma^2)$, where $\sigma^2$ is known.

We assign a Normal prior to the mean $\mu$: $\mu \sim N(\mu_0, \sigma_0^2)$.

To find the posterior $\pi(\mu|x_1, \dots, x_n)$, let $x = (x_1, \dots, x_n)$. The posterior is proportional to:

$$
\pi(\mu|x) \propto \pi(\mu) \cdot f(x|\mu)
$$

$$
\propto \exp\left\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\right\} \cdot \exp\left\{-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\right\}
$$



**Posterior Precision:**

It is often more convenient to work with **precision** (the inverse of variance). Let:

* $\tau_0 = 1/\sigma_0^2$ (Prior precision)
* $\tau = 1/\sigma^2$ (Data precision)
* $\tau_1 = 1/\sigma_1^2$ (Posterior precision)

The relationship is additive:

$$
\tau_1 = \tau_0 + n\tau
$$

$$
\text{Posterior Precision} = \text{Prior Precision} + \text{Precision of Data}
$$

The posterior mean $\mu_1$ is a weighted average of the prior mean and the sample mean:

$$
\mu_1 = \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}
$$

So, the posterior distribution is:

$$
\mu|x_1, \dots, x_n \sim N\left( \frac{\mu_0 \tau_0 + n\bar{x}\tau}{\tau_0 + n\tau}, \frac{1}{\tau_0 + n\tau} \right)
$$

**Numerical Illustration:**

Suppose we estimate a mean height $\mu$.

* **Known Variance:** $\sigma^2 = 100$ ($\tau = 0.01$).
* **Prior:** $\mu \sim N(175, 25)$ (Precision $\tau_0 = 0.04$).
* **Data:** $n=10, \bar{x}=180$. (Total data precision $n\tau = 0.1$).
* **Posterior:**
  * Precision $\tau_1 = 0.04 + 0.1 = 0.14$.
  * Variance $\sigma_1^2 \approx 7.14$.
  * Mean $\mu_1 = \frac{175(0.04) + 180(0.1)}{0.14} \approx 178.6$.

The plot below illustrates the prior (dashed) and posterior (solid) normal densities.

```{r}
#| label: fig-normal-conjugacy
#| fig-cap: "Prior vs Posterior for Normal-Normal Example"
#| echo: true

mu_vals <- seq(150, 200, length.out = 200)

# Prior: N(175, 25) -> SD = 5
prior_norm <- dnorm(mu_vals, mean = 175, sd = 5)

# Posterior: N(178.6, 7.14) -> SD = Sqrt(7.14) Approx 2.67
posterior_norm <- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))

plot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = "blue",
     xlab = expression(mu), ylab = "Density",
     main = "Normal Prior vs Posterior",
     ylim = c(0, max(c(prior_norm, posterior_norm))))
lines(mu_vals, prior_norm, col = "red", lty = 2, lwd = 2)
legend("topleft", legend = c("Prior N(175, 25)", "Posterior N(178.6, 7.14)"),
       col = c("red", "blue"), lty = c(2, 1), lwd = 2)
```
:::

::: {#exm-discrete-posterior}
### Discrete Posterior Calculation

Consider the following table where we calculate the posterior probabilities for a discrete parameter space.

Let the parameter $\theta$ take values $\{1, 2, 3\}$ with prior probabilities $\pi(\theta)$. Let the data $x$ take values $\{0, 1, 2, \dots\}$.

Given:

* Prior $\pi(\theta)$: $\pi(1)=1/3, \pi(2)=1/3, \pi(3)=1/3$.
* Likelihood $\pi(x|\theta)$:
    * If $\theta=1$, $x \sim \text{Uniform on } \{0, 1\}$ (Prob = 1/2).
    * If $\theta=2$, $x \sim \text{Uniform on } \{0, 1, 2\}$ (Prob = 1/3).
    * If $\theta=3$, $x \sim \text{Uniform on } \{0, 1, 2, 3\}$ (Prob = 1/4).

Suppose we observe $x=2$. The calculation of the posterior probabilities is summarized in the table below:

| | $\theta=1$ | $\theta=2$ | $\theta=3$ | Sum |
|:---|:---:|:---:|:---:|:---:|
| **Prior** $\pi(\theta)$ | $1/3$ | $1/3$ | $1/3$ | $1$ |
| **Likelihood** $\pi(x=2|\theta)$ | $0$ | $1/3$ | $1/4$ | - |
| **Product** $\pi(\theta)\pi(x|\theta)$ | $0$ | $1/9$ | $1/12$ | $7/36$ |
| **Posterior** $\pi(\theta|x)$ | $0$ | $4/7$ | $3/7$ | $1$ |

The marginal sum (evidence) is calculated as $0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36$. The posterior values are obtained by dividing the product row by this sum.
:::

::: {#exm-Normal-with-Unknown-Mean-and-Variance}
### Normal with Unknown Mean and Variance

Consider $X_1, \dots, X_n \sim N(\mu, 1/\tau)$, where both $\mu$ and the precision $\tau$ are unknown.

We use a **Normal-Gamma** conjugate prior:

1.  $\tau \sim \text{Gamma}(\alpha, \beta)$
    $$\pi(\tau) \propto \tau^{\alpha-1} e^{-\beta\tau}$$

2.  $\mu|\tau \sim N(\nu, 1/(k\tau))$
    $$\pi(\mu|\tau) \propto \tau^{1/2} e^{-\frac{k\tau}{2}(\mu-\nu)^2}$$

The joint prior is the product of the conditional and the marginal:
$$
\pi(\mu, \tau) \propto \tau^{\alpha - 1/2} \exp\left\{ -\tau \left( \beta + \frac{k}{2}(\mu - \nu)^2 \right) \right\}
$$

**Derivation of the Posterior:**

First, we write the likelihood in terms of the sufficient statistics $\bar{x}$ and $S_{xx} = \sum (x_i - \bar{x})^2$:
$$
L(\mu, \tau|x) \propto \tau^{n/2} \exp\left\{ -\frac{\tau}{2} \left[ S_{xx} + n(\bar{x}-\mu)^2 \right] \right\}
$$

Multiplying the prior by the likelihood gives the joint posterior:
$$
\begin{aligned}
\pi(\mu, \tau | x) &\propto \tau^{\alpha - 1/2} e^{-\beta\tau} e^{-\frac{k\tau}{2}(\mu-\nu)^2} \cdot \tau^{n/2} e^{-\frac{\tau}{2}S_{xx}} e^{-\frac{n\tau}{2}(\mu-\bar{x})^2} \\
&\propto \tau^{\alpha + n/2 - 1/2} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{1}{2}\left( k(\mu-\nu)^2 + n(\mu-\bar{x})^2 \right) \right] \right\}
\end{aligned}
$$

Next, we complete the square for the terms involving $\mu$ inside the brackets. It can be shown that:
$$
k(\mu-\nu)^2 + n(\mu-\bar{x})^2 = (k+n)\left(\mu - \frac{k\nu+n\bar{x}}{k+n}\right)^2 + \frac{nk}{n+k}(\bar{x}-\nu)^2
$$

Substituting this back into the joint density and grouping terms that do not depend on $\mu$:
$$
\pi(\mu, \tau | x) \propto \underbrace{\tau^{\alpha + n/2 - 1} \exp\left\{ -\tau \left[ \beta + \frac{S_{xx}}{2} + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2 \right] \right\}}_{\text{Marginal of } \tau} \cdot \underbrace{\tau^{1/2} \exp\left\{ -\frac{(k+n)\tau}{2} \left( \mu - \frac{k\nu+n\bar{x}}{k+n} \right)^2 \right\}}_{\text{Conditional of } \mu|\tau}
$$

**Results:**

By inspecting the factored equation above, we identify the updated parameters:

* **Marginal Posterior of $\tau$:**
    The first part corresponds to a Gamma kernel $\tau^{\alpha' - 1} e^{-\beta'\tau}$.
    $$\tau|x \sim \text{Gamma}(\alpha', \beta')$$
    where $\alpha' = \alpha + n/2$ and $\beta' = \beta + \frac{1}{2}\sum(x_i-\bar{x})^2 + \frac{nk}{2(n+k)}(\bar{x}-\nu)^2$.

* **Conditional Posterior of $\mu$:**
    The second part corresponds to a Normal kernel with precision $k'\tau$.
    $$\mu|\tau, x \sim N(\nu', 1/(k'\tau))$$
    where $k' = k + n$ and $\nu' = \frac{k\nu + n\bar{x}}{k+n}$.
:::

## Paradigm to Find Bayes Rules

The general form of Bayes rule is derived by minimizing risk.

::: {#def-risk}
### Risk Function and Bayes Risk

* **Risk Function:** $R(\theta, d) = \int_{X} L(\theta, d(x)) f(x;\theta) dx$
* **Bayes Risk:** The expected risk with respect to the prior.
    $$r(\pi, d) = \int_{\Theta} R(\theta, d) \pi(\theta) d\theta$$
:::

::: {#thm-bayes-rule-minimization}
### Minimization of Bayes Risk

Minimizing the Bayes risk $r(\pi, d)$ is equivalent to minimizing the posterior expected loss for each observed $x$. That is, the Bayes rule $d(x)$ satisfies:
$$
d(x) = \underset{a}{\arg\min} \ E_{\theta|x} [ L(\theta, a) ]
$$
:::

::: {.proof}
We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function $R(\theta, d)$:

$$
\begin{aligned}
r(\pi, d) &= \int_{\Theta} R(\theta, d) \pi(\theta) d\theta \\
&= \int_{\Theta} \left[ \int_{X} L(\theta, d(x)) f(x|\theta) dx \right] \pi(\theta) d\theta
\end{aligned}
$$

Assuming the conditions for Fubini's Theorem are met, we switch the order of integration:

$$
r(\pi, d) = \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) f(x|\theta) \pi(\theta) d\theta \right] dx
$$

Recall that the joint density can be factored as $f(x, \theta) = f(x|\theta)\pi(\theta) = \pi(\theta|x)m(x)$, where $m(x)$ is the marginal density of the data. Substituting this into the inner integral:

$$
\begin{aligned}
r(\pi, d) &= \int_{X} \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) m(x) d\theta \right] dx \\
&= \int_{X} m(x) \left[ \int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta \right] dx
\end{aligned}
$$

Since the marginal density $m(x)$ is non-negative, minimizing the total integral $r(\pi, d)$ with respect to the decision rule $d(\cdot)$ is equivalent to minimizing the term inside the brackets for every $x$ (specifically where $m(x) > 0$).

The term inside the brackets is the **Posterior Expected Loss**:

$$
\int_{\Theta} L(\theta, d(x)) \pi(\theta|x) d\theta = E_{\theta|x} [ L(\theta, d(x)) ]
$$


:::



:::{.callout-important}
Therefore, to minimize the Bayes risk, one just need to  choose $d(x)$ to minimize the posterior expected loss for each $x$.
:::

The following diagram summarizes the general workflow for deriving a Bayes estimator:

```{tikz fig-bayes-workflow}
%| fig-cap: "Workflow for Finding the Bayes Rule"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 80% !important;"'
%| engine.opts:
%|   extra.preamble: "\\usetikzlibrary{shapes.geometric, arrows}"

\begin{tikzpicture}[node distance=2cm, auto]
    % Styles
    \tikzstyle{block} = [rectangle, draw, fill=blue!10, text width=8em, text centered, rounded corners, minimum height=3em]
    \tikzstyle{input} = [trapezium, trapezium left angle=70, trapezium right angle=110, draw, fill=green!10, text width=6em, text centered, minimum height=3em]
    \tikzstyle{line} = [draw, -latex, thick]

    % Nodes
    \node [input] (inputs) {Prior $\pi(\theta)$ \\ Likelihood $f(x|\theta)$ \\ Data $x$};
    \node [block, below of=inputs, node distance=3cm] (posterior) {Compute Posterior \\ $\pi(\theta|x)$};
    \node [input, right of=posterior, node distance=5cm] (loss) {Loss Function \\ $L(\theta, d)$};
    \node [block, below of=posterior, node distance=3cm] (exp_loss) {Compute Posterior Expected Loss \\ $E_{\theta|x}[L(\theta, d)]$};
    \node [block, below of=exp_loss, node distance=3cm] (minimize) {Minimize w.r.t $d$ \\ $\arg\min_d E[L]$};
    \node [block, below of=minimize, node distance=3cm, fill=red!10] (bayesrule) {Bayes Rule \\ $d(x)$};

    % Paths
    \path [line] (inputs) -- (posterior);
    \path [line] (posterior) -- (exp_loss);
    \path [line] (loss) |- (exp_loss);
    \path [line] (exp_loss) -- (minimize);
    \path [line] (minimize) -- (bayesrule);

\end{tikzpicture}
```


## Common Loss Functions and Bayes Estimators

### Squared Error Loss (point Estimate)

$$L(\theta, a) = (\theta - a)^2$$

To find the optimal estimator $d(x)$, we minimize the posterior expected loss $E_{\theta|x}[(\theta - d(x))^2]$. Taking the derivative with respect to $d$ and setting it to 0:

$$-2 E_{\theta|x}(\theta - d) = 0 \implies d(x) = E(\theta|x)$$

**Result:** The Bayes rule under squared error loss is the **posterior mean**.

### Absolute Error Loss

$$L(\theta, d) = |\theta - d|$$

To find the Bayes rule, we minimize the posterior expected loss:

$$
\psi(d) = E_{\theta|x} [ |\theta - d| ] = \int_{-\infty}^{\infty} |\theta - d| \, dF(\theta|x)
$$

where $F(\theta|x)$ is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point $d$:

$$
\psi(d) = \int_{-\infty}^{d} (d - \theta) \, dF(\theta|x) + \int_{d}^{\infty} (\theta - d) \, dF(\theta|x)
$$

We find the minimum by analyzing the rate of change of $\psi(d)$ with respect to $d$. Differentiating (or taking the subgradient for non-differentiable points):

$$
\frac{d}{dd} \psi(d) = \int_{-\infty}^{d} 1 \, dF(\theta|x) - \int_{d}^{\infty} 1 \, dF(\theta|x) = P(\theta \le d|x) - P(\theta > d|x)
$$

Setting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:

$$
P(\theta \le d|x) = P(\theta > d|x)
$$

Since the total probability is 1, this condition simplifies to finding $d$ such that the cumulative probability is $1/2$.

**General Case (Discrete or Mixed Distributions)**

In cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to $F(d) = 0.5$ may not exist. To generalize, the Bayes rule is defined as any **median** $m$ of the posterior distribution.

A median is formally defined as any value $m$ that satisfies the following two conditions simultaneously:

* $P(\theta \le m|x) \ge \frac{1}{2}$
* $P(\theta \ge m|x) \ge \frac{1}{2}$

**Result:** The Bayes rule under absolute error loss is the **posterior median**.

### Hypothesis Testing (0-1 Loss)

Consider the hypothesis test $H_0: \theta \in \Theta_0$ versus $H_1: \theta \in \Theta_1$. We define the decision space as $\mathcal{A} = \{0, 1\}$, where $a=0$ means accepting $H_0$ and $a=1$ means rejecting $H_0$ (accepting $H_1$).

**Case 1: 0-1 Loss**

The standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one:
$$L(\theta, a) = \begin{cases} 0 & \text{if } \theta \in \Theta_0, a=0 \ (\text{Correct } H_0) \\ 1 & \text{if } \theta \in \Theta_0, a=1 \ (\text{Type I Error}) \\ 1 & \text{if } \theta \in \Theta_1, a=0 \ (\text{Type II Error}) \\ 0 & \text{if } \theta \in \Theta_1, a=1 \ (\text{Correct } H_1) \end{cases}$$

To find the Bayes rule, we minimize the **posterior expected loss** for a given $x$, denoted as $E_{\theta|x}[L(\theta, a)]$.

* **Expected Loss for choosing $a=0$ (Accept $H_0$):**
    $$
    E_{\theta|x}[L(\theta, 0)] = 0 \cdot P(\theta \in \Theta_0|x) + 1 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_1|x)
    $$

* **Expected Loss for choosing $a=1$ (Reject $H_0$):**
    $$
    E_{\theta|x}[L(\theta, 1)] = 1 \cdot P(\theta \in \Theta_0|x) + 0 \cdot P(\theta \in \Theta_1|x) = P(\theta \in \Theta_0|x)
    $$

The Bayes rule selects the action with the smaller expected loss. Thus, we choose $a=1$ if:
$$
P(\theta \in \Theta_0|x) \le P(\theta \in \Theta_1|x)
$$
This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.

**Case 2: General Loss (Asymmetric Costs)**

In many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let $c_1$ be the cost of a Type I error and $c_2$ be the cost of a Type II error. Usually, we normalize one cost to 1.

Suppose the loss function is:
$$L(\theta, a) = \begin{cases} 0 & \text{if } \theta \in \Theta_0, a=0 \\ c & \text{if } \theta \in \Theta_0, a=1 \ (\text{Cost of Type I Error}) \\ 1 & \text{if } \theta \in \Theta_1, a=0 \ (\text{Cost of Type II Error}) \\ 0 & \text{if } \theta \in \Theta_1, a=1 \end{cases}$$

We again calculate the posterior expected loss:

* **Expected Loss for $a=0$:**
    $$E[L(\theta, 0)|x] = 0 \cdot P(\Theta_0|x) + 1 \cdot P(\Theta_1|x) = P(\Theta_1|x)$$

* **Expected Loss for $a=1$:**
    $$E[L(\theta, 1)|x] = c \cdot P(\Theta_0|x) + 0 \cdot P(\Theta_1|x) = c P(\Theta_0|x)$$

We reject $H_0$ ($a=1$) if the expected loss of doing so is lower:
$$
c P(\Theta_0|x) \le P(\Theta_1|x)
$$

Since $P(\Theta_1|x) = 1 - P(\Theta_0|x)$, we can rewrite this condition as:
$$
c P(\Theta_0|x) \le 1 - P(\Theta_0|x) \implies (1+c) P(\Theta_0|x) \le 1
$$
$$
P(\Theta_0|x) \le \frac{1}{1+c}
$$

**Result:** With asymmetric costs, we accept $H_1$ only if the posterior probability of the null hypothesis is sufficiently small (below the threshold $\frac{1}{1+c}$). If the cost of false rejection $c$ is high, we require stronger evidence against $H_0$.

### Classification Prediction (categorical Parameter)

In classification problems, the parameter of interest is a discrete class label $\theta$ (often denoted as $y$) taking values in a set of categories $\{1, 2, \dots, K\}$. The goal is to predict the true class label based on observed features $x$.

We typically employ the **0-1 loss function**, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:

$$L(\theta, \hat{\theta}) = \begin{cases} 0 & \text{if } \hat{\theta} = \theta \ (\text{Correct Classification}) \\ 1 & \text{if } \hat{\theta} \neq \theta \ (\text{Misclassification}) \end{cases}$$

To find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.

$$
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta} L(\theta, \hat{\theta}) \pi(\theta|x)
$$

Since the loss is 1 only when the predicted class $\hat{\theta}$ differs from the true class $\theta$, this sum simplifies to:

$$
E_{\theta|x}[L(\theta, \hat{\theta})] = \sum_{\theta \neq \hat{\theta}} 1 \cdot \pi(\theta|x) = P(\theta \neq \hat{\theta} | x) = 1 - P(\theta = \hat{\theta} | x)
$$

Minimizing the misclassification rate $1 - P(\theta = \hat{\theta} | x)$ is mathematically equivalent to maximizing the probability of being correct, $P(\theta = \hat{\theta} | x)$.

**Result:** The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the **Maximum A Posteriori (MAP)** estimator, in the context of machine learning and pattern recognition, this decision rule is known as the **Bayes Optimal Classifier**.

$$
\hat{\theta}_{\text{Bayes}}(x) = \underset{k \in \{1, \dots, K\}}{\arg\max} \ P(\theta = k | x)
$$


### Interval Estimation and Highest Posterior Density (HPD)

In interval estimation, our goal is typically to find a set $C(x)$ with a specified probability coverage $1-\alpha$ (i.e., $P(\theta \in C(x)|x) = 1-\alpha$) that minimizes the "size" or length of the interval.

**Loss Function for HPD:**
The HPD interval can be formally derived as the Bayes rule under a loss function that linearly combines the size of the interval and the error of non-coverage:

$$
L(\theta, C) = \text{Length}(C) + k \cdot I(\theta \notin C)
$$

where $k$ is a positive constant representing the penalty for failing to include the true parameter $\theta$. Minimizing the posterior expected loss leads to including all values of $\theta$ where the posterior density $\pi(\theta|x)$ exceeds $1/k$. By adjusting $k$, we control the credibility level $1-\alpha$.

**Justification for HPD:**
To minimize the length of the interval for a fixed probability mass (or equivalently, minimize this loss function), we should include the values of $\theta$ that have the highest probability density. If we include a value with low density while excluding a value with higher density, we could swap them to increase the probability mass without increasing the interval length (or conversely, shrink the length while maintaining the mass).

Therefore, the **Highest Posterior Density (HPD)** interval is defined as:
$$C_{HPD} = \{ \theta : \pi(\theta|x) \ge k_\alpha \}$$
where $k_\alpha$ is a threshold chosen such that the posterior probability of the set is $1-\alpha$.

**Comparison with Equal-Tailed Intervals:**

* **Equal-Tailed Interval:** We simply cut off $\alpha/2$ probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.
* **HPD Interval:** This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.

The plot below illustrates a skewed posterior distribution (Gamma). Notice how the **HPD Interval (Blue)** is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the **Equal-Tailed Interval (Red)**.

```{r}
#| label: fig-hpd-vs-equal
#| fig-cap: "Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution"
#| echo: true

# Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)
x_vals <- seq(0, 15, length.out = 1000)
y_vals <- dgamma(x_vals, shape = 2, rate = 0.5)

# Target Coverage
alpha <- 0.10
target_prob <- 1 - alpha

# 1. Equal-tailed Interval (quantiles)
eq_lower <- qgamma(alpha/2, shape = 2, rate = 0.5)
eq_upper <- qgamma(1 - alpha/2, shape = 2, rate = 0.5)

# 2. HPD Interval (density Threshold Optimization)
# We Look for a Density Threshold K Such That the Area Above K Is 0.90
find_hpd <- function(dist_vals, density_vals, probability) {
  # Sort density values
  ord <- order(density_vals, decreasing = TRUE)
  sorted_dens <- density_vals[ord]
  sorted_dist <- dist_vals[ord]
  
  # Accumulate probability (approximation)
  dx <- diff(dist_vals)[1]
  cum_prob <- cumsum(sorted_dens * dx)
  
  # Find cutoff index
  cutoff_idx <- which(cum_prob >= probability)[1]
  
  # Get the subset of x values
  hpd_set <- sorted_dist[1:cutoff_idx]
  return(c(min(hpd_set), max(hpd_set)))
}

hpd_bounds <- find_hpd(x_vals, y_vals, target_prob)
hpd_lower <- hpd_bounds[1]
hpd_upper <- hpd_bounds[2]

# Plotting
plot(x_vals, y_vals, type = 'l', lwd = 2, col = "black",
     main = "90% Credible Intervals (Skewed Posterior)",
     xlab = expression(theta), ylab = "Density",
     ylim = c(0, max(y_vals) * 1.2))

# Shade HPD
polygon(c(x_vals[x_vals >= hpd_lower & x_vals <= hpd_upper], hpd_upper, hpd_lower),
        c(y_vals[x_vals >= hpd_lower & x_vals <= hpd_upper], 0, 0),
        col = rgb(0, 0, 1, 0.2), border = NA)

# Draw Equal-tailed Lines (red)
abline(v = c(eq_lower, eq_upper), col = "red", lwd = 2, lty = 2)
# Draw HPD Lines (blue)
abline(v = c(hpd_lower, hpd_upper), col = "blue", lwd = 2, lty = 1)

legend("topright", 
       legend = c("Posterior Density", 
                  paste0("Equal-Tailed (Len: ", round(eq_upper - eq_lower, 2), ")"), 
                  paste0("HPD (Len: ", round(hpd_upper - hpd_lower, 2), ")")),
       col = c("black", "red", "blue"), 
       lty = c(1, 2, 1), lwd = 2,
       fill = c(NA, NA, rgb(0, 0, 1, 0.2)), border = NA)

```


## Constant Risk Bayes Estimator Is Minimax

A decision rule $d(x)$ is **minimax** if it minimizes the maximum possible risk: $\sup_\theta R(\theta, d)$.

::: {#thm-minimax-constant}
### Constant Risk Bayes Estimator Is Minimax

Let $\delta^\pi$ be a Bayes estimator with respect to a prior $\pi$. If the risk function of $\delta^\pi$ is constant on the parameter space $\Theta$, such that $R(\theta, \delta^\pi) = c$ for all $\theta \in \Theta$, then $\delta^\pi$ is a minimax estimator.
:::

::: {.proof}
Let $\delta^\pi$ be the Bayes estimator with constant risk $c$. First, we compute its Bayes risk $r(\pi, \delta^\pi)$. Since the risk is constant:

$$
r(\pi, \delta^\pi) = \int_\Theta R(\theta, \delta^\pi) \pi(\theta) d\theta = \int_\Theta c \, \pi(\theta) d\theta = c
$$

Now, let $\delta'$ be any arbitrary estimator. By the definition of a Bayes estimator, $\delta^\pi$ minimizes the Bayes risk among all estimators:

$$
r(\pi, \delta^\pi) \le r(\pi, \delta')
$$

Next, we observe that the Bayes risk of $\delta'$ is the expectation of its risk function with respect to the prior $\pi$. An average cannot exceed the maximum value of the function being averaged (the supremum):

$$
r(\pi, \delta') = \int_\Theta R(\theta, \delta') \pi(\theta) d\theta \le \sup_{\theta \in \Theta} R(\theta, \delta') \cdot \int_\Theta \pi(\theta) d\theta = \sup_{\theta \in \Theta} R(\theta, \delta')
$$

Combining these inequalities, we have:

$$
\sup_{\theta \in \Theta} R(\theta, \delta^\pi) = c = r(\pi, \delta^\pi) \le r(\pi, \delta') \le \sup_{\theta \in \Theta} R(\theta, \delta')
$$

Since $\sup_\theta R(\theta, \delta^\pi) \le \sup_\theta R(\theta, \delta')$ holds for any estimator $\delta'$, $\delta^\pi$ minimizes the maximum risk. Therefore, it is minimax.
:::

The plot below visualizes the logic of the proof. The red line represents the **Constant Risk Bayes Estimator** ($\delta^\pi$), which has a constant height $c$. The blue curve represents an **Arbitrary Estimator** ($\delta'$).

Because $\delta^\pi$ minimizes the *weighted average* risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk ($\sup R$) strictly greater than or equal to $c$. Thus, the constant risk estimator has the lowest possible maximum.

```{r}
#| label: fig-minimax-proof
#| fig-cap: "Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk."
#| echo: true

# Define Parameter Space Theta
theta <- seq(0, 1, length.out = 200)

# 1. Constant Risk Bayes Estimator (risk = C)
c_val <- 0.5
risk_bayes <- rep(c_val, length(theta))

# 2. Arbitrary Alternative Estimator
# This Function Is Chosen Such That It Dips Below C but Rises Above It Elsewhere
risk_alt <- c_val + 0.2 * sin(2 * pi * theta) - 0.05

# Plotting
plot(theta, risk_alt, type = 'l', lwd = 2, col = "blue",
     ylim = c(0, 1), ylab = "Risk R(theta, d)", xlab = expression(theta),
     main = "Geometry of the Minimax Theorem")

# Add Constant Risk Line
lines(theta, risk_bayes, col = "red", lwd = 2)

# Mark the Maximum of the Alternative
max_alt <- max(risk_alt)
max_theta <- theta[which.max(risk_alt)]
points(max_theta, max_alt, pch = 19, col = "blue")
text(max_theta, max_alt, labels = expression(sup~R(theta, delta^"'")), pos = 3, col = "blue")

# Label the Constant Risk
text(0.1, c_val, labels = expression(R(theta, delta^pi) == c), pos = 3, col = "red")

# Add Legend
legend("bottomright", legend = c("Arbitrary Estimator", "Constant Risk Bayes Est."),
       col = c("blue", "red"), lwd = 2, lty = 1)
```

::: {#exm-binomial-minimax}
### Binomial Minimax Estimator

Let $X \sim \text{Bin}(n, \theta)$ and $\theta \sim \text{Beta}(a, b)$.
The squared error loss is $L(\theta, d) = (\theta - d)^2$.
The Bayes estimator is the posterior mean:
$$d(x) = \frac{a+x}{a+b+n}$$

We calculate the risk $R(\theta, d)$:

$$
R(\theta, d) = E_x \left[ \left( \theta - \frac{a+x}{a+b+n} \right)^2 \right]
$$

Let $c = a+b+n$.
$$R(\theta, d) = \frac{1}{c^2} E \left[ (c\theta - a - x)^2 \right]$$

Using the bias-variance decomposition and knowing $E(x) = n\theta$ and $E(x^2) = (n\theta)^2 + n\theta(1-\theta)$, we expand the risk function. To make the risk constant (independent of $\theta$), we set the coefficients of $\theta$ and $\theta^2$ to zero.

Solving the resulting system of equations yields:
$$a = b = \frac{\sqrt{n}}{2}$$

Thus, the minimax estimator is:
$$d(x) = \frac{x + \sqrt{n}/2}{n + \sqrt{n}}$$

This differs from the standard MLE $\hat{p} = x/n$ and the uniform prior Bayes estimator ($a=b=1$).
:::

# Stein's Paradox and the James-stein Estimator

In high-dimensional estimation ($p \ge 3$), the Maximum Likelihood Estimator (MLE) is inadmissible under squared error loss. The **James-Stein Estimator** dominates the MLE, meaning it achieves lower risk for all values of $\theta$.

::: {#exm-anova-js}
### Practical Application: One-Way ANOVA and "Borrowing Strength"

Consider a One-Way ANOVA setting where we wish to estimate the means of $p$ different independent groups (e.g., the true batting averages of $p=10$ baseball players, or the efficacy of $p=5$ different hospital treatments).

* **Model:** Let $X_i \sim N(\theta_i, \sigma^2)$ be the observed sample mean for group $i$, for $i = 1, \dots, p$.
* **Goal:** Estimate the vector of true means $\boldsymbol{\theta} = (\theta_1, \dots, \theta_p)$ simultaneously. The loss is the sum of squared errors: $L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}) = \sum (\theta_i - \hat{\theta}_i)^2$.

**The MLE Approach (Total Separation):**
The standard estimator is $\hat{\theta}_i^{MLE} = X_i$. This estimates each group entirely independently, using only data from that specific group. If a specific player has a lucky streak, their estimate is very high; if they are unlucky, it is very low.

**The James-Stein Approach (Shrinkage / Pooling):**
In this context, the James-Stein estimator (specifically the variation shrinking toward the grand mean $\bar{X}$) is:
$$
\hat{\theta}_i^{JS} = \bar{X} + \left( 1 - \frac{(p-3)\sigma^2}{\sum (X_i - \bar{X})^2} \right) (X_i - \bar{X})
$$

**Why is this better?**
Even though the groups might be physically independent (e.g., distinct hospitals), the James-Stein estimator **"borrows strength"** from the ensemble.

1.  **Noise Reduction:** Extreme observations $X_i$ are likely to contain more positive noise than signal. Shrinking them toward the global average $\bar{X}$ reduces this variance.
2.  **Stein's Paradox:** While $\hat{\theta}_i^{JS}$ introduces bias (estimates are pulled toward the center), the reduction in variance is so significant that the **Total Risk** (sum of squared errors over all groups) is strictly lower than that of the MLE, provided $p \ge 3$.

Thus, estimating the groups *together* yields a more accurate global picture than estimating them *separately*, even if the groups are independent.
:::

Consider the setting:

* Data: $X \sim N_p(\theta, I)$
* Prior: $\theta \sim N_p(0, \tau^2 I)$
* Estimator: $d^{JS}(x) = \left( 1 - \frac{p-2}{||x||^2} \right) x$

We can derive the Bayes Risk $r(\pi, d^{JS})$ of this estimator using two equivalent methods: minimizing the expected frequentist risk, or minimizing the expected posterior loss.

::: {#thm-js-bayes-risk}
### Bayes Risk of James-stein Estimator

For $p \ge 3$, the Bayes risk of the James-Stein estimator $d^{JS}$ with respect to the prior $\theta \sim N(0, \tau^2 I)$ is:

$$
r(\pi, d^{JS}) = \frac{p\tau^2 + 2}{\tau^2 + 1}
$$
:::

::: {.proof}
**Method 1: Integration over the Prior (Frequentist Risk approach)** 

The Bayes risk is defined as $r(\pi, d) = E_\pi [ R(\theta, d) ]$.

First, recall the frequentist risk of the James-Stein estimator for a fixed $\theta$. Using Stein's Lemma, the risk is given by:
$$
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
$$

To find the Bayes risk, we take the expectation of this risk with respect to the prior $\pi(\theta)$:
$$
r(\pi, d^{JS}) = \int R(\theta, d^{JS}) \pi(\theta) d\theta = p - (p-2)^2 E_\pi \left[ E_\theta \left( \frac{1}{||X||^2} \right) \right]
$$

By the law of iterated expectations, $E_\pi [ E_\theta (\cdot) ]$ is equivalent to the expectation with respect to the marginal distribution of $X$, denoted as $m(x)$.
Under the conjugate prior, the marginal distribution is $X \sim N(0, (1+\tau^2)I)$.

Consequently, the quantity $\frac{||X||^2}{1+\tau^2}$ follows a Chi-squared distribution with $p$ degrees of freedom ($\chi^2_p$). The expectation of the inverse chi-square is:
$$
E \left[ \frac{1}{||X||^2} \right] = \frac{1}{1+\tau^2} E \left[ \frac{1}{\chi^2_p} \right] = \frac{1}{1+\tau^2} \cdot \frac{1}{p-2}
$$

Substituting this back into the risk equation:
$$
\begin{aligned}
r(\pi, d^{JS}) &= p - (p-2)^2 \cdot \frac{1}{(p-2)(1+\tau^2)} \\
&= p - \frac{p-2}{1+\tau^2} \\
&= \frac{p(1+\tau^2) - (p-2)}{1+\tau^2} \\
&= \frac{p\tau^2 + p - p + 2}{1+\tau^2} = \frac{p\tau^2 + 2}{\tau^2 + 1}
\end{aligned}
$$
:::

::: {.proof}
**Method 2: Integration over the Marginal (Posterior Loss approach)** 

Alternatively, we can compute the Bayes risk by first finding the posterior expected loss for a given $x$, and then averaging over the marginal distribution of $x$:
$$
r(\pi, d) = E_m [ E_{\theta|x} [ L(\theta, d(x)) ] ]
$$

**Step 1: Posterior Expected Loss**

The posterior distribution of $\theta$ given $x$ is:
$$
\theta | x \sim N \left( \frac{\tau^2}{1+\tau^2}x, \frac{\tau^2}{1+\tau^2}I \right)
$$

The expected squared error loss can be decomposed into the variance (trace) and the squared bias:
$$
E_{\theta|x} [ ||\theta - d^{JS}(x)||^2 ] = \text{tr}(\text{Var}(\theta|x)) + || E[\theta|x] - d^{JS}(x) ||^2
$$

* **Trace term:**
    $$\text{tr} \left( \frac{\tau^2}{1+\tau^2} I_p \right) = \frac{p\tau^2}{1+\tau^2}$$

* **Squared Bias term:**
    Let $B = \frac{1}{1+\tau^2}$. Then $E[\theta|x] = (1-B)x$.
    The estimator is $d^{JS}(x) = (1 - \frac{p-2}{||x||^2})x$.
    The difference is:
    $$
    E[\theta|x] - d^{JS}(x) = \left( (1-B) - \left( 1 - \frac{p-2}{||x||^2} \right) \right) x = \left( \frac{p-2}{||x||^2} - B \right) x
    $$
    Squaring the norm gives:
    $$
    \left( \frac{p-2}{||x||^2} - B \right)^2 ||x||^2 = \frac{(p-2)^2}{||x||^2} - 2B(p-2) + B^2 ||x||^2
    $$

**Step 2: Expectation with respect to Marginal $X$** 

We now take the expectation $E_m[\cdot]$ of the posterior loss. Recall $X \sim N(0, (1+\tau^2)I)$, so $E[||X||^2] = p(1+\tau^2)$ and $E[1/||X||^2] = \frac{1}{(p-2)(1+\tau^2)}$.

* **Expectation of Trace term:** Constant, remains $\frac{p\tau^2}{1+\tau^2}$.
* **Expectation of Bias term:**
    $$
    \begin{aligned}
    E_m \left[ \frac{(p-2)^2}{||X||^2} - \frac{2(p-2)}{1+\tau^2} + \frac{||X||^2}{(1+\tau^2)^2} \right] &= (p-2)^2 \frac{1}{(p-2)(1+\tau^2)} - \frac{2(p-2)}{1+\tau^2} + \frac{p(1+\tau^2)}{(1+\tau^2)^2} \\
    &= \frac{p-2}{1+\tau^2} - \frac{2p-4}{1+\tau^2} + \frac{p}{1+\tau^2} \\
    &= \frac{p - 2 - 2p + 4 + p}{1+\tau^2} \\
    &= \frac{2}{1+\tau^2}
    \end{aligned}
    $$

**Step 3: Combine Terms** 

$$
r(\pi, d^{JS}) = \underbrace{\frac{p\tau^2}{1+\tau^2}}_{\text{Variance Part}} + \underbrace{\frac{2}{1+\tau^2}}_{\text{Bias Part}} = \frac{p\tau^2 + 2}{\tau^2 + 1}
$$

Both methods yield the same result.
:::

::: {#thm-mle-inadmissible}
### Inadmissibility of the MLE in High Dimensions (Stein's Phenomenon)

Let $X \sim N_p(\theta, I)$ be a $p$-dimensional random vector with $p \ge 3$. Under the squared error loss function $L(\theta, d) = ||\theta - d||^2$, the standard Maximum Likelihood Estimator $d^0(X) = X$ is **inadmissible**.
:::

::: {.proof}
To show that $d^0(X) = X$ is inadmissible, we must find another estimator that dominates it (i.e., has equal or lower risk for all $\theta$, and strictly lower risk for at least one $\theta$).

First, consider the risk of the standard estimator $d^0$. Since $X_i \sim N(\theta_i, 1)$ are independent:

$$
R(\theta, d^0) = E_\theta [ ||X - \theta||^2 ] = \sum_{i=1}^p E [ (X_i - \theta_i)^2 ] = \sum_{i=1}^p \text{Var}(X_i) = p
$$

Now consider the James-Stein estimator $d^{JS}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$. As established in the derivation of the Bayes Risk in @thm-js-bayes-risk, the frequentist risk function of $d^{JS}$ is:

$$
R(\theta, d^{JS}) = p - (p-2)^2 E_\theta \left[ \frac{1}{||X||^2} \right]
$$

Since the random variable $||X||^2$ is non-negative and not identically infinity, the expectation $E_\theta [ 1/||X||^2 ]$ is strictly positive for all $\theta$. Therefore:

$$
R(\theta, d^{JS}) < p = R(\theta, d^0) \quad \text{for all } \theta \in \mathbb{R}^p
$$

Because $d^{JS}$ achieves a strictly lower risk than $d^0$ everywhere in the parameter space, $d^0$ is dominated by $d^{JS}$ and is thus inadmissible.
:::

## Empirical Bayes

The James-Stein estimator can be motivated via an Empirical Bayes approach.

**Model:**

1.  $X_i | \mu_i \sim N(\mu_i, 1)$
2.  Prior: $\mu_i \sim N(0, \tau^2)$

The posterior mean for $\mu_i$ (if $\tau^2$ were known) is:
$$E(\mu_i|x_i) = \frac{\tau^2}{1+\tau^2} x_i = \left( 1 - \frac{1}{1+\tau^2} \right) x_i$$

The marginal distribution of $X_i$ is $N(0, 1+\tau^2)$.
Consequently, $S = \sum X_i^2 \sim (1+\tau^2) \chi^2_p$.

We can estimate the unknown shrinkage factor $B = \frac{1}{1+\tau^2}$ using the data.
Since $E[ \frac{p-2}{S} ] = \frac{1}{1+\tau^2}$, we replace the theoretical shrinkage factor with its unbiased estimate:
$$\hat{B} = \frac{p-2}{||X||^2}$$

This recovers the James-Stein rule:
$$\delta^{EB}(X) = \left( 1 - \frac{p-2}{||X||^2} \right) X$$

::: {#ex-baseball}
### Baseball Example (efron & Morris)

We illustrate Stein estimation using baseball batting averages.
Let $y_i$ be the number of hits for player $i$ in their first $n=45$ at-bats.
Let $\hat{p}_i = y_i/n$ be the observed average.

To apply the Normal model, we use a variance-stabilizing transformation:
$$X_i = \sqrt{n} \arcsin(2\hat{p}_i - 1)$$
Under this transformation, $X_i \approx N(\mu_i, 1)$.

Using the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value $\mu_0$).
Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages $\hat{p}_i$.
:::

## Predictive Distributions

A key feature of Bayesian analysis is the predictive distribution for a future observation $x^*$.

$$f(x^*|x) = \int f(x^*|\theta) \pi(\theta|x) d\theta$$

::: {#ex-predictive-normal}
### Normal-normal Predictive Distribution

If $x_1, \dots, x_n \sim N(\mu, \sigma^2)$ (with $\sigma^2$ known) and $\mu \sim N(\mu_0, \sigma_0^2)$, the predictive distribution for a new observation $x^*$ is:

$$x^*|x \sim N(\mu_1, \sigma^2 + \sigma_1^2)$$

where $\mu_1$ and $\sigma_1^2$ are the posterior mean and variance of $\mu$. The predictive variance includes both the inherent sampling uncertainty ($\sigma^2$) and the uncertainty about the parameter ($\sigma_1^2$).
:::

## Hierarchical Modeling and MCMC

When analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).

**Hierarchical Structure:**

1.  **Data:** $X_i | \mu_i \sim f(x_i|\mu_i)$
2.  **Parameters:** $\mu_i | \theta \sim \pi(\mu_i|\theta)$
3.  **Hyperparameters:** $\theta \sim \pi(\theta)$

**Gibbs Sampling:**
To estimate the posterior $f(\mu, \theta | x)$, we sample iteratively from the **full conditional distributions**:

1.  Sample $\mu_i$ from $f(\mu_i | x, \theta)$.
2.  Sample $\theta$ from $f(\theta | \mu, x)$.

::: {#ex-hierarchical-baseball}
### Baseball Example with Hierarchical Model

* $Y_i \sim \text{Bin}(n_i, p_i)$
* Logit transform: $\mu_i = \text{logit}(p_i)$
* $\mu_i \sim N(\theta, \tau^2)$
* Priors on $\theta$ and $\tau^2$.

Since the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use **Metropolis-Hastings** steps within the Gibbs sampler.

**Algorithm:**

1.  Initialize parameters $\mu^{(0)}, \theta^{(0)}, \tau^{(0)}$.
2.  Propose new values based on a candidate distribution.
3.  Accept or reject based on the acceptance probability ratio (Likelihood $\times$ Prior ratio).
4.  Repeat until convergence.

The marginal posterior density for a specific parameter (e.g., $f(\mu_j|x)$) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.
:::
