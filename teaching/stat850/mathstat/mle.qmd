
---
title: "Maximum Likelihood Estimation"
engine: knitr
format: 
  html: default
  pdf: default
---

## Definitions and Concepts

### Likelihood and MLE

::: {#def-likelihood-mle}

### Maximum Likelihood Estimation

1.  **Likelihood Function:**
    Let $f(\mathbf{x}|\boldsymbol{\theta})$ be the joint probability density function of the data $\mathbf{X}$. When viewed as a function of the parameter $\boldsymbol{\theta}$ given fixed data $\mathbf{x}$, it is called the likelihood function:
    $$
    L(\boldsymbol{\theta}; \mathbf{x}) = f(\mathbf{x}|\boldsymbol{\theta})
    $$

2.  **Log-likelihood:**
    It is usually easier to maximize the natural logarithm of the likelihood:
    $$
    \ell(\boldsymbol{\theta}; \mathbf{x}) = \log L(\boldsymbol{\theta}; \mathbf{x})
    $$

3.  **Maximum Likelihood Estimator (MLE):**
    The MLE $\hat{\boldsymbol{\theta}}_{\text{MLE}}$ is the value that maximizes the likelihood function:
    $$
    \hat{\boldsymbol{\theta}}_{\text{MLE}}(\mathbf{x}) = \operatorname*{argmax}_{\boldsymbol{\theta} \in \Theta} \ell(\boldsymbol{\theta}; \mathbf{x})
    $$

4.  **Score Function ($\mathbf{U}$):**
    The score function is defined as the gradient of the log-likelihood with respect to the parameter vector. Finding the MLE often involves solving the score equation:
    $$
    \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) = \nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta}; \mathbf{x}) = \mathbf{0}
    $$

:::

## Examples

::: {#exm-normal-mle-score}

### Normal Distribution: MLE and Score Convergence

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We define $\boldsymbol{\theta} = (\mu, \sigma^2)^T$.

1. Log-Likelihood
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
$$

2. Score Function and MLE
The Score vector $\mathbf{U}(\boldsymbol{\theta})$ has two components:

* **Component 1 (Mean):**
    $$
    U_\mu = \frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)
    $$
    Setting $U_\mu = 0$ yields $\hat{\mu}_{\text{MLE}} = \bar{x}$.

* **Component 2 (Variance):**
    $$
    U_{\sigma^2} = \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2
    $$
    Setting $U_{\sigma^2} = 0$ yields $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum (x_i - \hat{\mu})^2$.

3. Asymptotic Normality of the Score
We now demonstrate that $\mathbf{U}(\boldsymbol{\theta})$ follows a Normal distribution $N(\mathbf{0}, \mathbf{I}(\boldsymbol{\theta}))$.

* **For $U_\mu$:**
    $$
    U_\mu = \frac{n}{\sigma^2} (\bar{X} - \mu)
    $$
    Since $\bar{X} \sim N(\mu, \sigma^2/n)$, $U_\mu$ is a linear transformation of a Normal variable. It is **exactly Normal**:
    $$
    E[U_\mu] = 0, \quad \text{Var}(U_\mu) = \left(\frac{n}{\sigma^2}\right)^2 \text{Var}(\bar{X}) = \frac{n^2}{\sigma^4} \frac{\sigma^2}{n} = \frac{n}{\sigma^2}
    $$
    Thus, $U_\mu \sim N(0, \frac{n}{\sigma^2})$.

* **For $U_{\sigma^2}$:**
    We rewrite $U_{\sigma^2}$ as a sum of i.i.d. variables. Let $Z_i = ((X_i - \mu)/\sigma)^2$. Note that $Z_i \sim \chi^2_1$, with variance 2.
    $$
    U_{\sigma^2} = \frac{1}{2\sigma^2} \sum_{i=1}^n \left( \left(\frac{X_i - \mu}{\sigma}\right)^2 - 1 \right) = \frac{1}{2\sigma^2} \sum_{i=1}^n (Z_i - 1)
    $$
    Since this is a sum of i.i.d. random variables with mean 0, the **Central Limit Theorem** applies:
    $$
    U_{\sigma^2} \xrightarrow{d} \text{Normal}
    $$
    The limiting variance is:
    $$
    \text{Var}(U_{\sigma^2}) = \frac{1}{4\sigma^4} \sum \text{Var}(Z_i) = \frac{1}{4\sigma^4} (n \times 2) = \frac{n}{2\sigma^4}
    $$

**Conclusion:**
The covariance matrix of the Score approaches the Fisher Information matrix:
$$
\text{Var}(\mathbf{U}) = \begin{bmatrix} \frac{n}{\sigma^2} & 0 \\ 0 & \frac{n}{2\sigma^4} \end{bmatrix} = \mathbf{I}(\boldsymbol{\theta})
$$

:::

::: {#exm-uniform-mle-boundary}

### Uniform Distribution (Boundary Case)

Let $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{\text{Unif}}(0, \theta)$. The likelihood is:
$$
L(\theta; \mathbf{x}) = \frac{1}{\theta^n} I(x_{(n)} \le \theta)
$$
This function strictly decreases for $\theta \ge x_{(n)}$ and is zero otherwise. Thus:
$$
\hat{\theta}_{\text{MLE}} = x_{(n)}
$$
Note that the Score equation approach fails here because the support depends on $\theta$, making the log-likelihood discontinuous at the boundary.

:::

## Review of Convergence Theorems for Probability



::: {#thm-lln}

### Weak Law of Large Numbers (WLLN)
Let $X_1, \dots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \mu$ and finite variance $\text{Var}(X_i) = \sigma^2 < \infty$.

Then, the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ converges in probability to $\mu$:
$$
\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$
Formal definition: For any $\epsilon > 0$, $\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$.

:::



::: {#thm-clt}

### Central Limit Theorem for IID Cases
Let $X_1, \dots, X_n$ be i.i.d. random variables with mean $E[X_i] = \mu$ and finite variance $0 < \text{Var}(X_i) = \sigma^2 < \infty$.

Then, the random variable $\sqrt{n}(\bar{X}_n - \mu)$ converges in distribution to a normal distribution with mean 0 and variance $\sigma^2$:
$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)
$$

:::

::: {#rem-clt-history}

### Historical Note: The Evolution of the CLT

While the standard i.i.d. theorem is named after **Jarl Waldemar Lindeberg** and **Paul Lévy**, they were the "closers" of a discovery process that involved many others:

1.  **Abraham de Moivre (1733):** The first to discover the "Normal curve" as an approximation to the Binomial distribution (specifically for the symmetric case $p=1/2$).

2.  **Pierre-Simon Laplace (1812):** Generalized de Moivre's work to arbitrary $p$ (the De Moivre-Laplace Theorem) and intuitively grasped that sums of independent errors tend toward normality, though his proofs lacked modern rigor.

3.  **Pafnuty Chebyshev (1887):** Attempted to prove the general CLT using the "Method of Moments." While his proof had gaps, it established the moment-based approach later completed by his student, Markov.

4.  **Aleksandr Lyapunov (1901):** Provided the first rigorous proof for independent variables using **Characteristic Functions**. However, he required a slightly stricter condition (existence of $(2+\delta)$ moments) than necessary.

5.  **Lindeberg (1922) & Lévy (1925):**
    * **Lindeberg** proved the general CLT for independent variables under the famous "Lindeberg Condition." notably using a direct "replacement method" (swapping distributions), **not** characteristic functions.
    * **Lévy** rigorously proved the i.i.d. case and investigated stable laws using Characteristic Functions, establishing them as the standard tool for such proofs.

6.  **William Feller (1935):** Proved the converse: if the individual random variables are asymptotically negligible (no single term dominates the sum), then the Lindeberg condition is not just sufficient, but **necessary** for the sum to converge to a Normal distribution. Hence, the general theorem is often called the **Lindeberg-Feller Theorem**.

:::

::: {#thm-lindeberg-feller}

### Lindeberg-Feller CLT (For Non-Identical Distributions)
This variation is crucial for regression analysis (e.g., OLS properties with fixed regressors) where variables are independent but **not** identically distributed.

Let $X_1, \dots, X_n$ be independent random variables with $E[X_i] = \mu_i$ and $\text{Var}(X_i) = \sigma_i^2$.
Define the **average variance** $\tilde{\sigma}_n^2$:
$$
\tilde{\sigma}_n^2 = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
$$

If the **Lindeberg Condition** holds:
For every $\epsilon > 0$,
$$
\lim_{n \to \infty} \frac{1}{n \tilde{\sigma}_n^2} \sum_{i=1}^n E\left[ (X_i - \mu_i)^2 \cdot I\left( |X_i - \mu_i| > \epsilon \sqrt{n \tilde{\sigma}_n^2} \right) \right] = 0
$$

Then the standardized sum converges to a standard normal:
$$
\frac{\sum_{i=1}^n (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} \xrightarrow{d} N(0, 1)
$$

:::

::: {#cor-lindeberg-feller-approx}

### Approximating Distribution for Sample Mean (Non-i.i.d.)

Under the conditions of the Lindeberg-Feller CLT, we can derive the asymptotic distribution for the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

Let $\bar{\mu}_n = \frac{1}{n}\sum_{i=1}^n \mu_i$ be the average mean. Note that the denominator in the CLT is simply $\sqrt{n} \tilde{\sigma}_n$.

The standardized sum converges to $N(0,1)$:
$$
\frac{\sum (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} = \frac{n(\bar{X}_n - \bar{\mu}_n)}{\sqrt{n} \tilde{\sigma}_n} = \frac{\sqrt{n}(\bar{X}_n - \bar{\mu}_n)}{\tilde{\sigma}_n} \xrightarrow{d} N(0, 1)
$$

This implies the following **approximate distributions** for large $n$:

1.  **For the Sample Mean:**
    $$
    \bar{X}_n \overset{\cdot}{\sim} N\left(\bar{\mu}_n, \frac{\tilde{\sigma}_n^2}{n}\right)
    $$

2.  **For the Scaled Difference (Root-n consistency):**
    $$
    \sqrt{n}(\bar{X}_n - \bar{\mu}_n) \overset{\cdot}{\sim} N\left(0, \tilde{\sigma}_n^2\right)
    $$

*Note: If all $X_i$ share the same mean $\mu$, simply replace $\bar{\mu}_n$ with $\mu$.*

:::

::: {#thm-slutsky}

### Slutsky's Theorem
Let $X_n$ and $Y_n$ be sequences of random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant, then:

1.  **Sum:** $X_n + Y_n \xrightarrow{d} X + c$
2.  **Product:** $X_n Y_n \xrightarrow{d} cX$
3.  **Quotient:** $X_n / Y_n \xrightarrow{d} X/c$ (provided $c \ne 0$)

:::

::: {#thm-generalized-slutsky}

### Generalized Slutsky's Theorem (Continuous Mapping)

The arithmetic operations in Slutsky's theorem are special cases of a broader property.

Let $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant.
Let $g: \mathbb{R}^2 \to \mathbb{R}^k$ be a function that is **continuous** at every point $(x, c)$ where $x$ is in the support of $X$.

Then:
$$
g(X_n, Y_n) \xrightarrow{d} g(X, c)
$$

This implies that for any "well-behaved" algebraic combination (polynomials, exponentials, etc.) of a sequence converging in distribution and a sequence converging in probability to a constant, the limit behaves as if the constant were substituted directly.

:::

::: {#exm-asymptotic-normal-variance}

### Asymptotic Normality of Sample Variance

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We wish to derive the asymptotic distribution of the MLE for variance, 

$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2.$$

1. Algebraic Expansion

    We rewrite the estimator by adding and subtracting the true mean $\mu$:
    $$
    \sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n ((X_i - \mu) - (\bar{X} - \mu))^2
    $$
    $$
    = \sum_{i=1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu)\sum_{i=1}^n(X_i - \mu) + n(\bar{X} - \mu)^2
    $$
    Since $\sum(X_i - \mu) = n(\bar{X} - \mu)$:
    $$
    = \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2
    $$
    Dividing by $n$, we get:
    $$
    \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar{X} - \mu)^2
    $$

2. Scaling by $\sqrt{n}$

    We rearrange to look at the pivotal quantity $\sqrt{n}(\hat{\sigma}^2 - \sigma^2)$:
    $$
    \sqrt{n}(\hat{\sigma}^2 - \sigma^2) = \sqrt{n}\left( \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \sigma^2 \right) - \sqrt{n}(\bar{X} - \mu)^2
    $$

3. Applying Convergence Theorems


    * **Term 1 (CLT):**
    
        Let $W_i = (X_i - \mu)^2$. Since $X_i \sim N(\mu, \sigma^2)$, $W_i/\sigma^2 \sim \chi^2_1$.
        moments of $W_i$: $E[W_i] = \sigma^2$ and $\text{Var}(W_i) = 2\sigma^4$.
        By the standard CLT:
        $$
        \sqrt{n}(\bar{W} - E[W]) = \sqrt{n}\left( \frac{1}{n}\sum (X_i - \mu)^2 - \sigma^2 \right) \xrightarrow{d} N(0, 2\sigma^4)
        $$

    * **Term 2 (Slutsky):**
    
        Consider the term $\sqrt{n}(\bar{X} - \mu)^2$. We can rewrite this as:
        $$
        \underbrace{\sqrt{n}(\bar{X} - \mu)}_{\xrightarrow{d} N(0, \sigma^2)} \cdot \underbrace{(\bar{X} - \mu)}_{\xrightarrow{p} 0 \text{ by LLN}}
        $$
        By Slutsky's Theorem, the product converges to $Z \cdot 0 = 0$.

**Conclusion:**

Combining the terms:
$$
\sqrt{n}(\hat{\sigma}^2 - \sigma^2) \xrightarrow{d} N(0, 2\sigma^4) - 0 = N(0, 2\sigma^4)
$$

:::

## Asymptotic Theory of Maximum Likelihood

### Consistency of the MLE

Consistency establishes that as the sample size grows, the estimator converges in probability to the true parameter value.

::: {#thm-mle-consistency}
#### Consistency of MLE
Let $X_1, \dots, X_n$ be i.i.d. with density $f(x|\boldsymbol{\theta})$. Let $\boldsymbol{\theta}_0$ be the true parameter. Under regularity conditions (specifically identifiability and compactness), the Maximum Likelihood Estimator $\hat{\boldsymbol{\theta}}_n$ satisfies:
$$
\hat{\boldsymbol{\theta}}_n \xrightarrow{p} \boldsymbol{\theta}_0 \quad \text{as } n \to \infty
$$
:::

::: {.proof}
1. **The Objective Function**

   The MLE maximizes the average log-likelihood:
   $$
   M_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \log f(X_i|\boldsymbol{\theta})
   $$
   By the **Weak Law of Large Numbers (WLLN)**, for any fixed $\boldsymbol{\theta}$, $M_n(\boldsymbol{\theta})$ converges in probability to its expectation:
   $$
   M_n(\boldsymbol{\theta}) \xrightarrow{p} M(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}_0} [\log f(X|\boldsymbol{\theta})]
   $$

2. **Identifying the Maximum (Information Inequality)**

   We compare the expected log-likelihood at an arbitrary $\boldsymbol{\theta}$ versus the true $\boldsymbol{\theta}_0$. Consider the difference:
   $$
   M(\boldsymbol{\theta}) - M(\boldsymbol{\theta}_0) = E_{\boldsymbol{\theta}_0} \left[ \log f(X|\boldsymbol{\theta}) - \log f(X|\boldsymbol{\theta}_0) \right] = E_{\boldsymbol{\theta}_0} \left[ \log \left( \frac{f(X|\boldsymbol{\theta})}{f(X|\boldsymbol{\theta}_0)} \right) \right]
   $$
   By **Jensen's Inequality** (since $\log(x)$ is strictly concave), $E[\log Y] \le \log E[Y]$.
   $$
   E_{\boldsymbol{\theta}_0} \left[ \log \left( \frac{f(X|\boldsymbol{\theta})}{f(X|\boldsymbol{\theta}_0)} \right) \right] \le \log E_{\boldsymbol{\theta}_0} \left[ \frac{f(X|\boldsymbol{\theta})}{f(X|\boldsymbol{\theta}_0)} \right]
   $$
   Evaluating the expectation on the right:
   $$
   E_{\boldsymbol{\theta}_0} \left[ \frac{f(X|\boldsymbol{\theta})}{f(X|\boldsymbol{\theta}_0)} \right] = \int \left( \frac{f(x|\boldsymbol{\theta})}{f(x|\boldsymbol{\theta}_0)} \right) f(x|\boldsymbol{\theta}_0) \, dx = \int f(x|\boldsymbol{\theta}) \, dx = 1
   $$
   Therefore:
   $$
   M(\boldsymbol{\theta}) - M(\boldsymbol{\theta}_0) \le \log(1) = 0 \implies M(\boldsymbol{\theta}) \le M(\boldsymbol{\theta}_0)
   $$
   Assuming the model is **identifiable** (i.e., $f(x|\boldsymbol{\theta}) = f(x|\boldsymbol{\theta}_0) \iff \boldsymbol{\theta} = \boldsymbol{\theta}_0$), the maximum is unique at $\boldsymbol{\theta}_0$. Since $\hat{\boldsymbol{\theta}}_n$ maximizes $M_n(\boldsymbol{\theta})$, which converges to $M(\boldsymbol{\theta})$, $\hat{\boldsymbol{\theta}}_n$ must converge to $\boldsymbol{\theta}_0$.
:::

```{tikz}
%| label: fig-likelihood-consistency
%| fig-cap: "Consistency: As n increases, the log-likelihood function concentrates around the true parameter."
%| echo: false
%| fig-align: "center"

\begin{tikzpicture}
\draw[->] (0,0) -- (6,0) node[right] {$\theta$};
\draw[->] (0,0) -- (0,4) node[above] {$M_n(\theta)$};

% True Limit Curve
\draw[thick, blue] (0.5,0.5) .. controls (3,4.5) and (5,1) .. (5.5,0.5);
\node[blue] at (5.5, 0.8) {$M(\theta)$};

% Finite Sample Curve (wiggly)
\draw[dashed, red] (0.5,0.3) .. controls (1,1) and (2.8,4.2) .. (3, 4) .. controls (3.2, 4.2) and (5,0.8) .. (5.5,0.3);
\node[red] at (5.5, 0.2) {$M_n(\theta)$};

% Maxima
\draw[dashed] (3,0) node[below] {$\theta_0$} -- (3,3.7);
\filldraw (3,3.7) circle (2pt);

\end{tikzpicture}
```

### Asymptotic Normality of the Score Vector

The Score function acts as the "engine" for the normality of the MLE. We treat the I.I.D. and non-I.I.D. cases separately.

#### Case A: The I.I.D. Case

::: {#thm-score-normality-iid}
#### Normality of Score (I.I.D.)
Let $X_1, \dots, X_n$ be i.i.d. with Fisher Information $\mathbf{I}_1(\boldsymbol{\theta})$. The scaled total score vector converges to a Normal distribution:
$$
\frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
$$
:::

::: {.proof}
The total score is the sum of independent score contributions:
$$
\mathbf{U}_n(\boldsymbol{\theta}_0) = \sum_{i=1}^n \nabla \log f(X_i|\boldsymbol{\theta}_0) = \sum_{i=1}^n \mathbf{u}_i
$$
From **Bartlett's Identities**, for each observation $i$:

1. **Mean**

   $E[\mathbf{u}_i] = \mathbf{0}$.

2. **Variance**

   $\text{Var}(\mathbf{u}_i) = E[\mathbf{u}_i \mathbf{u}_i^T] = \mathbf{I}_1(\boldsymbol{\theta}_0)$.

Since the terms $\mathbf{u}_i$ are i.i.d. with finite variance, we apply the **Multivariate Central Limit Theorem (Lindeberg-Lévy)**:
$$
\frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{u}_i \xrightarrow{d} N(\mathbf{0}, \text{Var}(\mathbf{u}_i)) = N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
$$
:::

#### Case B: The Non-I.I.D. Case (Regression Setting)

::: {#thm-score-normality-noniid}
#### Normality of Score (Independent, Non-Identical)
Let $X_1, \dots, X_n$ be independent but not necessarily identically distributed. Let $\mathbf{I}_n(\boldsymbol{\theta}_0) = \sum_{i=1}^n \text{Var}(\mathbf{u}_i)$ be the total information.

Define the **Average Fisher Information Matrix**:
$$
\bar{\mathbf{I}}_n(\boldsymbol{\theta}_0) = \frac{1}{n} \mathbf{I}_n(\boldsymbol{\theta}_0) = \frac{1}{n} \sum_{i=1}^n E[\mathbf{u}_i \mathbf{u}_i^T]
$$

If the **Lindeberg Condition** is satisfied for the sequence of score vectors, then:
$$
\left( \mathbf{I}_n(\boldsymbol{\theta}_0) \right)^{-1/2} \mathbf{U}_n(\boldsymbol{\theta}_0) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}_p)
$$
Or, more intuitively, if $\bar{\mathbf{I}}_n \to \mathbf{I}_{\infty}$ (positive definite):
$$
\frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}_{\infty})
$$
:::

::: {.proof}
This is a direct application of the **Lindeberg-Feller Central Limit Theorem**.
The total score $\mathbf{U}_n = \sum \mathbf{u}_i$ is a sum of independent random vectors with mean $\mathbf{0}$ and variances $\text{Var}(\mathbf{u}_i)$.
Provided that no single observation's score contribution dominates the sum (the Lindeberg condition), the standardized sum converges to a standard Normal distribution.
:::

### symptotic Normality of the MLE

We now transfer the normality from the Score function to the estimator $\hat{\boldsymbol{\theta}}$ using a Taylor expansion (the Delta Method logic).

::: {#thm-mle-normality}
#### Asymptotic Normality of MLE
Under regularity conditions, the MLE $\hat{\boldsymbol{\theta}}_n$ satisfies:
$$
\sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \xrightarrow{d} N\left(\mathbf{0}, [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1}\right)
$$
:::

::: {.proof}
1. **Taylor Expansion of Score Equation**

   We expand the Score function $\mathbf{U}_n(\boldsymbol{\theta})$ around the true parameter $\boldsymbol{\theta}_0$. Since $\hat{\boldsymbol{\theta}}_n$ is the MLE, $\mathbf{U}_n(\hat{\boldsymbol{\theta}}_n) = \mathbf{0}$.
   $$
   \mathbf{0} = \mathbf{U}_n(\hat{\boldsymbol{\theta}}_n) \approx \mathbf{U}_n(\boldsymbol{\theta}_0) + \nabla \mathbf{U}_n(\boldsymbol{\theta}_0) (\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$
   Note that $\nabla \mathbf{U}_n(\boldsymbol{\theta}) = \mathbf{H}_n(\boldsymbol{\theta})$ is the Hessian of the log-likelihood.
   $$
   -\mathbf{U}_n(\boldsymbol{\theta}_0) \approx \mathbf{H}_n(\boldsymbol{\theta}_0) (\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$

2. **Scaling and Inversion**

   Multiply by $\frac{1}{\sqrt{n}}$ and introduce $n$ to the Hessian term:
   $$
   -\frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \approx \left( \frac{1}{n} \mathbf{H}_n(\boldsymbol{\theta}_0) \right) \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$
   Rearranging for the estimator:
   $$
   \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \approx \left[ -\frac{1}{n} \mathbf{H}_n(\boldsymbol{\theta}_0) \right]^{-1} \left[ \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \right]
   $$

3. **Convergence of Components**

   * **Numerator (Score):** From the I.I.D. Score Normality theorem:
       $$
       \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \xrightarrow{d} Z \sim N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
       $$
   * **Denominator (Hessian):** By the **WLLN**, the average observed information converges to the expected information:
       $$
       -\frac{1}{n} \mathbf{H}_n(\boldsymbol{\theta}_0) \xrightarrow{p} -E[\nabla^2 \log f] = \mathbf{I}_1(\boldsymbol{\theta}_0)
       $$

4. **Slutsky's Theorem**

   Combining these via Slutsky's Theorem (matrix inverse is a continuous mapping):
   $$
   \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \xrightarrow{d} [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1} Z \sim N\left( \mathbf{0}, \mathbf{I}_1^{-1} \mathbf{I}_1 \mathbf{I}_1^{-1} \right) = N\left( \mathbf{0}, [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1} \right)
   $$
:::

###Wilks' Theorem (Likelihood Ratio Test)

This theorem connects the Likelihood Ratio statistic to the Chi-squared distribution.

::: {#thm-wilks}
#### Wilks' Theorem
Consider testing $H_0: \boldsymbol{\theta} = \boldsymbol{\theta}_0$ against $H_1: \boldsymbol{\theta} \ne \boldsymbol{\theta}_0$.
Let $\Lambda_n$ be the likelihood ratio:
$$
\Lambda_n = \frac{L(\boldsymbol{\theta}_0)}{L(\hat{\boldsymbol{\theta}}_{\text{MLE}})}
$$
Then the Deviance statistic ($-2 \log \Lambda$) converges to a Chi-squared distribution:
$$
-2 \log \Lambda_n = 2 [\ell_n(\hat{\boldsymbol{\theta}}) - \ell_n(\boldsymbol{\theta}_0)] \xrightarrow{d} \chi^2_p
$$
where $p$ is the dimension of $\boldsymbol{\theta}$.
:::

::: {.proof}
1. **Taylor Expansion of Log-Likelihood**

   Expand $\ell_n(\boldsymbol{\theta}_0)$ around the MLE $\hat{\boldsymbol{\theta}}$. Note that the first derivative term vanishes because $\mathbf{U}_n(\hat{\boldsymbol{\theta}}) = \mathbf{0}$.
   $$
   \ell_n(\boldsymbol{\theta}_0) \approx \ell_n(\hat{\boldsymbol{\theta}}) + (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})^T \mathbf{U}_n(\hat{\boldsymbol{\theta}}) + \frac{1}{2} (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})
   $$
   $$
   \ell_n(\boldsymbol{\theta}_0) \approx \ell_n(\hat{\boldsymbol{\theta}}) + 0 + \frac{1}{2} (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$

2. **Rearranging for Deviance**

   $$
   2[\ell_n(\hat{\boldsymbol{\theta}}) - \ell_n(\boldsymbol{\theta}_0)] \approx - (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$
   Multiply and divide by $n$:
   $$
   = - \sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \left[ \frac{1}{n} \mathbf{H}_n(\hat{\boldsymbol{\theta}}) \right] \sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$

3. **Applying Asymptotic Results**

   * $\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \xrightarrow{d} Z \sim N(\mathbf{0}, \mathbf{I}^{-1})$.
   * $-\frac{1}{n} \mathbf{H}_n(\hat{\boldsymbol{\theta}}) \xrightarrow{p} \mathbf{I}$ (Fisher Information).

   Substituting these limits:
   $$
   -2 \log \Lambda_n \approx Z^T \mathbf{I} Z
   $$
   Let $Y = \mathbf{I}^{1/2} Z$. Since $Z \sim N(\mathbf{0}, \mathbf{I}^{-1})$, the variance of $Y$ is:
   $$
   \text{Var}(Y) = \mathbf{I}^{1/2} \text{Var}(Z) (\mathbf{I}^{1/2})^T = \mathbf{I}^{1/2} \mathbf{I}^{-1} \mathbf{I}^{1/2} = \mathbf{I}_p \quad \text{(Identity Matrix)}
   $$
   Thus, $Y \sim N(\mathbf{0}, \mathbf{I}_p)$, and the quadratic form becomes:
   $$
   Z^T \mathbf{I} Z = Y^T Y = \sum_{j=1}^p Y_j^2 \sim \chi^2_p
   $$
:::

## Hypothesis Testing: Likelihood Ratio Test

Consider testing:
$$
H_0: \theta \in \Theta_0 \quad \text{vs} \quad H_1: \theta \in \Theta \setminus \Theta_0
$$
where $\dim(\Theta) = p$ and $\dim(\Theta_0) = p-m$.

The Likelihood Ratio Statistic is:
$$
\Lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta; x)}{\sup_{\theta \in \Theta} L(\theta; x)} = \frac{L(\hat{\theta}_0)}{L(\hat{\theta})}
$$

::: {#thm-wilks}

## Wilks' Theorem
Under regularity conditions, under $H_0$:
$$
-2 \log \Lambda \xrightarrow{d} \chi^2_m
$$
where $m$ is the difference in dimensions (number of restrictions).

:::

::: {#exm-normal-mean-lrt}
Let $X_1, \dots, X_n \sim N(\mu, \sigma^2)$.
$H_0: \mu = \mu_0$ vs $H_1: \mu \ne \mu_0$.
Here $p=2$ ($\mu, \sigma^2$) and under $H_0$, free parameters = 1 ($\sigma^2$). So $m = 2-1 = 1$.

The statistic $\Lambda$:
$$
\Lambda = \left( \frac{\hat{\sigma}^2}{\hat{\sigma}_0^2} \right)^{n/2} = \left( \frac{\sum(x_i - \bar{x})^2}{\sum(x_i - \mu_0)^2} \right)^{n/2}
$$
It can be shown that:
$$
\Lambda = \left( 1 + \frac{(\bar{x} - \mu_0)^2}{\hat{\sigma}^2} \right)^{-n/2}
$$
The rejection region $-2 \log \Lambda > \chi^2_{1, \alpha}$ is equivalent to the t-test:
$$
\left| \frac{\bar{x} - \mu_0}{S/\sqrt{n}} \right| > t_{n-1, \alpha/2}
$$

:::

## Illustration of Wilks' Theorem

We can approximate the statistic using Taylor expansion. Consider the scalar case.
$$
-2 \log \Lambda = 2 [l(\hat{\theta}) - l(\theta_0)]
$$

```{tikz}
%| label: fig-wilks-proof
%| fig-cap: "Approximation of Likelihood Ratio"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 50% !important;"'

\begin{tikzpicture}
\draw[->] (-0.5,0) -- (4,0) node[right] {$x$};
\draw[->] (0,-1) -- (0,3) node[above] {$y$};
\draw[thick, domain=-0.5:3.5] plot (\x, {ln(1+\x)});
\node at (2.5, 1.5) {$\log(1+x)$};
\draw[dashed] (0,0) -- (3,3) node[right] {$y=x$};
\draw (1,0) node[below] {$\tau^*$} -- (1, {ln(2)});
\draw (2,0) node[below] {$\tau$} -- (2, {ln(3)});
\end{tikzpicture}

```

Using Taylor expansion around the MLE $\hat{\theta}$:
$$
l(\theta_0) \approx l(\hat{\theta}) + (\theta_0 - \hat{\theta})l'(\hat{\theta}) + \frac{1}{2}(\theta_0 - \hat{\theta})^2 l''(\hat{\theta})
$$
Since $l'(\hat{\theta}) = 0$:
$$
2[l(\hat{\theta}) - l(\theta_0)] \approx -(\theta_0 - \hat{\theta})^2 l''(\hat{\theta}) = (\hat{\theta} - \theta_0)^2 [-\frac{1}{n} l''(\hat{\theta})] \cdot n
$$
As $n \to \infty$, $-\frac{1}{n}l'' \to I(\theta_0)$ and $\sqrt{n}(\hat{\theta}-\theta_0) \to N(0, 1/I)$.
Thus, the expression behaves like $Z^2 \sim \chi^2_1$.


## An Example: Poisson Regression

In this example, we explore the Generalized Linear Model (GLM) for count data using the Poisson distribution. Let $Y_1, \dots, Y_n$ be independent count variables where $Y_i \sim \text{Poisson}(\lambda_i)$. The expected count $\lambda_i$ is related to a vector of covariates $\mathbf{x}_i$ and parameters $\boldsymbol{\beta}$ via the **canonical log link function**:
$$ \log(\lambda_i) = \eta_i = \mathbf{x}_i^\top \boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} $$

### Canonical Representation

We begin by expressing the log-likelihood in the canonical exponential family form. The probability mass function for the Poisson distribution is $P(Y_i=y_i) = \frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}$. The log-likelihood is:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i \log(\lambda_i) - \lambda_i - \log(y_i!) \right) $$

Substituting the log link $\log(\lambda_i) = \mathbf{x}_i^\top \boldsymbol{\beta}$ and $\lambda_i = e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i (\mathbf{x}_i^\top \boldsymbol{\beta}) - e^{\mathbf{x}_i^\top \boldsymbol{\beta}} \right) + \text{const} $$
Rearranging terms to isolate the parameters $\beta_j$:
$$ \ell(\boldsymbol{\beta}; \mathbf{y}) = \sum_{j=0}^k \beta_j \underbrace{\left( \sum_{i=1}^n y_i x_{ij} \right)}_{T_j(\mathbf{y})} - \underbrace{\sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}_{A(\boldsymbol{\beta})} + \text{const} $$

From this form, we identify:

* **Sufficient Statistics:** $\mathbf{T}(\mathbf{y}) = \mathbf{X}^\top \mathbf{y}$.
* **Log-Partition Function:** $A(\boldsymbol{\beta}) = \sum_{i=1}^n \lambda_i = \sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$.

### The Score and Information

Next, we derive the gradient and Hessian of the log-likelihood.

* **Score Vector ($\mathbf{U}$):** The gradient of $\ell(\boldsymbol{\beta})$ is the difference between the observed sufficient statistics and their expectations (derived from $\nabla A$).
    $$ \mathbf{U}(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell = \mathbf{T}(\mathbf{y}) - \nabla A(\boldsymbol{\beta}) = \sum_{i=1}^n y_i \mathbf{x}_i - \sum_{i=1}^n \lambda_i \mathbf{x}_i = \mathbf{X}^\top (\mathbf{y} - \boldsymbol{\lambda}) $$

* **Fisher Information Matrix ($\mathcal{J}$):** This is the negative Hessian of the log-likelihood, or equivalently the Hessian of the log-partition function $A(\boldsymbol{\beta})$.
    $$ \mathcal{J}(\boldsymbol{\beta}) = \nabla^2 A(\boldsymbol{\beta}) = \sum_{i=1}^n \mathbf{x}_i \frac{\partial \lambda_i}{\partial \boldsymbol{\beta}^\top} = \sum_{i=1}^n \lambda_i \mathbf{x}_i \mathbf{x}_i^\top $$
    In matrix notation, $\mathcal{J}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}$, where $\mathbf{W} = \text{diag}(\lambda_i)$. Note that for the Poisson model, the variance equals the mean $\lambda_i$.

### Asymptotic Distributions (Theory)

a.  **Normality of the Score:**
    The Score vector $\mathbf{U}(\boldsymbol{\beta}) = \sum_{i=1}^n (Y_i - \lambda_i)\mathbf{x}_i$ is a sum of independent mean-zero random vectors.

    * Mean: $E[\mathbf{U}] = \mathbf{0}$.
    * Variance: $\text{Var}(\mathbf{U}) = \sum \text{Var}(Y_i)\mathbf{x}_i\mathbf{x}_i^\top = \mathcal{J}(\boldsymbol{\beta})$.
    By the **Multivariate Central Limit Theorem**, as $n \to \infty$:
    $$ \frac{1}{\sqrt{n}}\mathbf{U}(\boldsymbol{\beta}) \xrightarrow{d} N(\mathbf{0}, \mathcal{I}(\boldsymbol{\beta})) $$
    where $\mathcal{I}$ is the limit of $\frac{1}{n}\mathcal{J}$.

b.  **Normality of the Estimator:**
    The MLE $\hat{\boldsymbol{\beta}}$ satisfies $\mathbf{U}(\hat{\boldsymbol{\beta}}) = \mathbf{0}$. Taking a first-order Taylor expansion around the true parameter $\boldsymbol{\beta}$:
    $$ \mathbf{0} = \mathbf{U}(\hat{\boldsymbol{\beta}}) \approx \mathbf{U}(\boldsymbol{\beta}) - \mathcal{J}(\boldsymbol{\beta}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) $$
    Rearranging gives $(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \approx \mathcal{J}^{-1}(\boldsymbol{\beta}) \mathbf{U}(\boldsymbol{\beta})$. Since $\mathbf{U}$ is asymptotically normal, the linear transformation implies:
    $$ \sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \xrightarrow{d} N\left(\mathbf{0}, \mathcal{I}^{-1}(\boldsymbol{\beta})\right) $$

### Numerical Application in R

We now implement the Newton-Raphson algorithm in R to estimate parameters for a Poisson regression model with a **continuous covariate**. Let $\log(\lambda_i) = \beta_0 + \beta_1 x_i$.

**a. Data Generation**
We simulate $n=100$ observations using a continuous predictor $x$ drawn from a Uniform distribution.

```r
set.seed(123)
n <- 100
x <- runif(n, 0, 1)

# True parameters: Intercept=0.5, Slope=2.0
beta_true <- c(0.5, 2.0) 

# Generate response Y
lambda_true <- exp(beta_true[1] + beta_true[2] * x)
y <- rpois(n, lambda_true)

# Design Matrix (intercept column + covariate)
X <- cbind(1, x)

```

**b. Newton-Raphson Implementation**
The update rule is $\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \mathcal{J}^{-1}\mathbf{U}$. We implement this iteratively.

```r
newton_raphson_poisson <- function(X, y, tol = 1e-6, max_iter = 100) {
  beta <- rep(0, ncol(X)) # Start at 0
  
  for (i in 1:max_iter) {
    # 1. Compute means and weights
    eta <- X %*% beta
    lambda <- as.vector(exp(eta))
    
    # 2. Compute Score U and Info J
    U <- crossprod(X, y - lambda)
    J <- crossprod(X * lambda, X) # Efficient t(X) %*% W %*% X
    
    # 3. Update beta
    delta <- solve(J, U)
    beta_new <- beta + as.vector(delta)
    
    # 4. Check convergence
    diff <- sum((beta_new - beta)^2)
    cat(sprintf("Iter %d: beta0=%.4f, beta1=%.4f, diff=%.6f\n", 
                i, beta_new[1], beta_new[2], diff))
    
    if (diff < tol) return(beta_new)
    beta <- beta_new
  }
}

beta_mle <- newton_raphson_poisson(X, y)

```

**Output:**

```
Iter 1: beta0=1.1718, beta1=0.6234, diff=1.761592
Iter 2: beta0=0.6970, beta1=1.5794, diff=1.139433
Iter 3: beta0=0.4996, beta1=2.0101, diff=0.224446
Iter 4: beta0=0.4725, beta1=2.0838, diff=0.006173
Iter 5: beta0=0.4722, beta1=2.0845, diff=0.000001

```

**c. Discussion & Verification**
We compare our manual estimates with R's built-in `glm` function.

```r
cat("Manual MLE: ", beta_mle, "\n")
cat("GLM Output: ", coef(glm(y ~ x, family = "poisson")))

```

**Observation:** The manual implementation converges to the exact same values as the built-in function ($\hat{\beta}_0 \approx 0.47, \hat{\beta}_1 \approx 2.08$). Notice that we started at $\boldsymbol{\beta}=(0,0)$. In the first iteration, the algorithm took a large step because the log-likelihood surface is steep. The quadratic convergence of Newton-Raphson is evident in the rapid decrease of the difference term (from 1.13 to 0.22 to 0.006).

