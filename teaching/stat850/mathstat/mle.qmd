
---
title: "Maximum Likelihood Estimation"
engine: knitr
format: 
  html: default
  pdf: default
---

## Definitions and Concepts

### Likelihood and MLE

::: {#def-likelihood-mle}

### Maximum Likelihood Estimation

1.  **Likelihood Function:**
    Let $f(\mathbf{x}|\boldsymbol{\theta})$ be the joint probability density function of the data $\mathbf{X}$. When viewed as a function of the parameter $\boldsymbol{\theta}$ given fixed data $\mathbf{x}$, it is called the likelihood function:
    $$
    L(\boldsymbol{\theta}; \mathbf{x}) = f(\mathbf{x}|\boldsymbol{\theta})
    $$

2.  **Log-likelihood:**
    It is usually easier to maximize the natural logarithm of the likelihood:
    $$
    \ell(\boldsymbol{\theta}; \mathbf{x}) = \log L(\boldsymbol{\theta}; \mathbf{x})
    $$

3.  **Maximum Likelihood Estimator (MLE):**
    The MLE $\hat{\boldsymbol{\theta}}_{\text{MLE}}$ is the value that maximizes the likelihood function:
    $$
    \hat{\boldsymbol{\theta}}_{\text{MLE}}(\mathbf{x}) = \operatorname*{argmax}_{\boldsymbol{\theta} \in \Theta} \ell(\boldsymbol{\theta}; \mathbf{x})
    $$

4.  **Score Function ($\mathbf{U}$):**
    The score function is defined as the gradient of the log-likelihood with respect to the parameter vector. Finding the MLE often involves solving the score equation:
    $$
    \mathbf{U}(\boldsymbol{\theta}; \mathbf{x}) = \nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta}; \mathbf{x}) = \mathbf{0}
    $$

:::

## Examples

::: {#exm-normal-mle-score}

### Normal Distribution: MLE and Score Convergence

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We define $\boldsymbol{\theta} = (\mu, \sigma^2)^T$.

1. Log-Likelihood
$$
\ell(\boldsymbol{\theta}; \mathbf{x}) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
$$

2. Score Function and MLE
The Score vector $\mathbf{U}(\boldsymbol{\theta})$ has two components:

* **Component 1 (Mean):**
    $$
    U_\mu = \frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)
    $$
    Setting $U_\mu = 0$ yields $\hat{\mu}_{\text{MLE}} = \bar{x}$.

* **Component 2 (Variance):**
    $$
    U_{\sigma^2} = \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2
    $$
    Setting $U_{\sigma^2} = 0$ yields $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum (x_i - \hat{\mu})^2$.

3. Asymptotic Normality of the Score
We now demonstrate that $\mathbf{U}(\boldsymbol{\theta})$ follows a Normal distribution $N(\mathbf{0}, \mathbf{I}(\boldsymbol{\theta}))$.

* **For $U_\mu$:**
    $$
    U_\mu = \frac{n}{\sigma^2} (\bar{X} - \mu)
    $$
    Since $\bar{X} \sim N(\mu, \sigma^2/n)$, $U_\mu$ is a linear transformation of a Normal variable. It is **exactly Normal**:
    $$
    E[U_\mu] = 0, \quad \text{Var}(U_\mu) = \left(\frac{n}{\sigma^2}\right)^2 \text{Var}(\bar{X}) = \frac{n^2}{\sigma^4} \frac{\sigma^2}{n} = \frac{n}{\sigma^2}
    $$
    Thus, $U_\mu \sim N(0, \frac{n}{\sigma^2})$.

* **For $U_{\sigma^2}$:**
    We rewrite $U_{\sigma^2}$ as a sum of i.i.d. variables. Let $Z_i = ((X_i - \mu)/\sigma)^2$. Note that $Z_i \sim \chi^2_1$, with variance 2.
    $$
    U_{\sigma^2} = \frac{1}{2\sigma^2} \sum_{i=1}^n \left( \left(\frac{X_i - \mu}{\sigma}\right)^2 - 1 \right) = \frac{1}{2\sigma^2} \sum_{i=1}^n (Z_i - 1)
    $$
    Since this is a sum of i.i.d. random variables with mean 0, the **Central Limit Theorem** applies:
    $$
    U_{\sigma^2} \xrightarrow{d} \text{Normal}
    $$
    The limiting variance is:
    $$
    \text{Var}(U_{\sigma^2}) = \frac{1}{4\sigma^4} \sum \text{Var}(Z_i) = \frac{1}{4\sigma^4} (n \times 2) = \frac{n}{2\sigma^4}
    $$

**Conclusion:**
The covariance matrix of the Score approaches the Fisher Information matrix:
$$
\text{Var}(\mathbf{U}) = \begin{bmatrix} \frac{n}{\sigma^2} & 0 \\ 0 & \frac{n}{2\sigma^4} \end{bmatrix} = \mathbf{I}(\boldsymbol{\theta})
$$

:::

::: {#exm-uniform-mle-boundary}

### Uniform Distribution (Boundary Case)

Let $X_1, \dots, X_n \overset{iid}{\sim} \operatorname{\text{Unif}}(0, \theta)$. The likelihood is:
$$
L(\theta; \mathbf{x}) = \frac{1}{\theta^n} I(x_{(n)} \le \theta)
$$
This function strictly decreases for $\theta \ge x_{(n)}$ and is zero otherwise. Thus:
$$
\hat{\theta}_{\text{MLE}} = x_{(n)}
$$
Note that the Score equation approach fails here because the support depends on $\theta$, making the log-likelihood discontinuous at the boundary.

:::

## Review of Convergence Theorems for Probability



::: {#thm-lln}

### Weak Law of Large Numbers (WLLN)
Let $X_1, \dots, X_n$ be independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \mu$ and finite variance $\text{Var}(X_i) = \sigma^2 < \infty$.

Then, the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ converges in probability to $\mu$:
$$
\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$
Formal definition: For any $\epsilon > 0$, $\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$.

:::



::: {#thm-clt}

### Central Limit Theorem for IID Cases
Let $X_1, \dots, X_n$ be i.i.d. random variables with mean $E[X_i] = \mu$ and finite variance $0 < \text{Var}(X_i) = \sigma^2 < \infty$.

Then, the random variable $\sqrt{n}(\bar{X}_n - \mu)$ converges in distribution to a normal distribution with mean 0 and variance $\sigma^2$:
$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)
$$

:::

::: {#rem-clt-history}

### Historical Note: The Evolution of the CLT

While the standard i.i.d. theorem is named after **Jarl Waldemar Lindeberg** and **Paul Lévy**, they were the "closers" of a discovery process that involved many others:

1.  **Abraham de Moivre (1733):** The first to discover the "Normal curve" as an approximation to the Binomial distribution (specifically for the symmetric case $p=1/2$).

2.  **Pierre-Simon Laplace (1812):** Generalized de Moivre's work to arbitrary $p$ (the De Moivre-Laplace Theorem) and intuitively grasped that sums of independent errors tend toward normality, though his proofs lacked modern rigor.

3.  **Pafnuty Chebyshev (1887):** Attempted to prove the general CLT using the "Method of Moments." While his proof had gaps, it established the moment-based approach later completed by his student, Markov.

4.  **Aleksandr Lyapunov (1901):** Provided the first rigorous proof for independent variables using **Characteristic Functions**. However, he required a slightly stricter condition (existence of $(2+\delta)$ moments) than necessary.

5.  **Lindeberg (1922) & Lévy (1925):**
    * **Lindeberg** proved the general CLT for independent variables under the famous "Lindeberg Condition." notably using a direct "replacement method" (swapping distributions), **not** characteristic functions.
    * **Lévy** rigorously proved the i.i.d. case and investigated stable laws using Characteristic Functions, establishing them as the standard tool for such proofs.

6.  **William Feller (1935):** Proved the converse: if the individual random variables are asymptotically negligible (no single term dominates the sum), then the Lindeberg condition is not just sufficient, but **necessary** for the sum to converge to a Normal distribution. Hence, the general theorem is often called the **Lindeberg-Feller Theorem**.

:::

::: {#thm-lindeberg-feller}

### Lindeberg-Feller CLT (For Non-Identical Distributions)
This variation is crucial for regression analysis (e.g., OLS properties with fixed regressors) where variables are independent but **not** identically distributed.

Let $X_1, \dots, X_n$ be independent random variables with $E[X_i] = \mu_i$ and $\text{Var}(X_i) = \sigma_i^2$.
Define the **average variance** $\tilde{\sigma}_n^2$:
$$
\tilde{\sigma}_n^2 = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
$$

If the **Lindeberg Condition** holds:
For every $\epsilon > 0$,
$$
\lim_{n \to \infty} \frac{1}{n \tilde{\sigma}_n^2} \sum_{i=1}^n E\left[ (X_i - \mu_i)^2 \cdot I\left( |X_i - \mu_i| > \epsilon \sqrt{n \tilde{\sigma}_n^2} \right) \right] = 0
$$

Then the standardized sum converges to a standard normal:
$$
\frac{\sum_{i=1}^n (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} \xrightarrow{d} N(0, 1)
$$

:::

::: {#cor-lindeberg-feller-approx}

### Approximating Distribution for Sample Mean (Non-i.i.d.)

Under the conditions of the Lindeberg-Feller CLT, we can derive the asymptotic distribution for the sample mean $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

Let $\bar{\mu}_n = \frac{1}{n}\sum_{i=1}^n \mu_i$ be the average mean. Note that the denominator in the CLT is simply $\sqrt{n} \tilde{\sigma}_n$.

The standardized sum converges to $N(0,1)$:
$$
\frac{\sum (X_i - \mu_i)}{\sqrt{n \tilde{\sigma}_n^2}} = \frac{n(\bar{X}_n - \bar{\mu}_n)}{\sqrt{n} \tilde{\sigma}_n} = \frac{\sqrt{n}(\bar{X}_n - \bar{\mu}_n)}{\tilde{\sigma}_n} \xrightarrow{d} N(0, 1)
$$

This implies the following **approximate distributions** for large $n$:

1.  **For the Sample Mean:**
    $$
    \bar{X}_n \overset{\cdot}{\sim} N\left(\bar{\mu}_n, \frac{\tilde{\sigma}_n^2}{n}\right)
    $$

2.  **For the Scaled Difference (Root-n consistency):**
    $$
    \sqrt{n}(\bar{X}_n - \bar{\mu}_n) \overset{\cdot}{\sim} N\left(0, \tilde{\sigma}_n^2\right)
    $$

*Note: If all $X_i$ share the same mean $\mu$, simply replace $\bar{\mu}_n$ with $\mu$.*

:::

::: {#thm-slutsky}

### Slutsky's Theorem
Let $X_n$ and $Y_n$ be sequences of random variables. If $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant, then:

1.  **Sum:** $X_n + Y_n \xrightarrow{d} X + c$
2.  **Product:** $X_n Y_n \xrightarrow{d} cX$
3.  **Quotient:** $X_n / Y_n \xrightarrow{d} X/c$ (provided $c \ne 0$)

:::

::: {#thm-generalized-slutsky}

### Generalized Slutsky's Theorem (Continuous Mapping)

The arithmetic operations in Slutsky's theorem are special cases of a broader property.

Let $X_n \xrightarrow{d} X$ and $Y_n \xrightarrow{p} c$, where $c$ is a constant.
Let $g: \mathbb{R}^2 \to \mathbb{R}^k$ be a function that is **continuous** at every point $(x, c)$ where $x$ is in the support of $X$.

Then:
$$
g(X_n, Y_n) \xrightarrow{d} g(X, c)
$$

This implies that for any "well-behaved" algebraic combination (polynomials, exponentials, etc.) of a sequence converging in distribution and a sequence converging in probability to a constant, the limit behaves as if the constant were substituted directly.

:::

::: {#exm-asymptotic-normal-variance}

### Asymptotic Normality of Sample Variance

Let $X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. We wish to derive the asymptotic distribution of the MLE for variance, 

$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2.$$

1. Algebraic Expansion

    We rewrite the estimator by adding and subtracting the true mean $\mu$:
    $$
    \sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n ((X_i - \mu) - (\bar{X} - \mu))^2
    $$
    $$
    = \sum_{i=1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu)\sum_{i=1}^n(X_i - \mu) + n(\bar{X} - \mu)^2
    $$
    Since $\sum(X_i - \mu) = n(\bar{X} - \mu)$:
    $$
    = \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2
    $$
    Dividing by $n$, we get:
    $$
    \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar{X} - \mu)^2
    $$

2. Scaling by $\sqrt{n}$

    We rearrange to look at the pivotal quantity $\sqrt{n}(\hat{\sigma}^2 - \sigma^2)$:
    $$
    \sqrt{n}(\hat{\sigma}^2 - \sigma^2) = \sqrt{n}\left( \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \sigma^2 \right) - \sqrt{n}(\bar{X} - \mu)^2
    $$

3. Applying Convergence Theorems


    * **Term 1 (CLT):**
    
        Let $W_i = (X_i - \mu)^2$. Since $X_i \sim N(\mu, \sigma^2)$, $W_i/\sigma^2 \sim \chi^2_1$.
        moments of $W_i$: $E[W_i] = \sigma^2$ and $\text{Var}(W_i) = 2\sigma^4$.
        By the standard CLT:
        $$
        \sqrt{n}(\bar{W} - E[W]) = \sqrt{n}\left( \frac{1}{n}\sum (X_i - \mu)^2 - \sigma^2 \right) \xrightarrow{d} N(0, 2\sigma^4)
        $$

    * **Term 2 (Slutsky):**
    
        Consider the term $\sqrt{n}(\bar{X} - \mu)^2$. We can rewrite this as:
        $$
        \underbrace{\sqrt{n}(\bar{X} - \mu)}_{\xrightarrow{d} N(0, \sigma^2)} \cdot \underbrace{(\bar{X} - \mu)}_{\xrightarrow{p} 0 \text{ by LLN}}
        $$
        By Slutsky's Theorem, the product converges to $Z \cdot 0 = 0$.

**Conclusion:**

Combining the terms:
$$
\sqrt{n}(\hat{\sigma}^2 - \sigma^2) \xrightarrow{d} N(0, 2\sigma^4) - 0 = N(0, 2\sigma^4)
$$

:::

## Asymptotic Theory of Maximum Likelihood

### Consistency of the MLE

#### IID Cases

Consistency establishes that as the sample size grows, the estimator converges in probability to the true parameter value.

::: {#thm-mle-consistency}

#### Consistency of MLE
Let $Y_1, \dots, Y_n$ be i.i.d. with density $f(y|\boldsymbol{\theta})$. Let $\boldsymbol{\theta}_0$ be the true parameter. Under regularity conditions (specifically identifiability and compactness), the Maximum Likelihood Estimator $\hat{\boldsymbol{\theta}}_n$ satisfies:
$$
\hat{\boldsymbol{\theta}}_n \xrightarrow{p} \boldsymbol{\theta}_0 \quad \text{as } n \to \infty
$$
:::

::: {.proof}
1. **The Objective Function**

   The MLE maximizes the average log-likelihood:
   $$
   M_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \log f(Y_i|\boldsymbol{\theta})
   $$
   By the **Weak Law of Large Numbers (WLLN)**, for any fixed $\boldsymbol{\theta}$, $M_n(\boldsymbol{\theta})$ converges in probability to its expectation:
   $$
   M_n(\boldsymbol{\theta}) \xrightarrow{p} M(\boldsymbol{\theta}) = E_{\boldsymbol{\theta}_0} [\log f(Y|\boldsymbol{\theta})]
   $$

2. **Identifying the Maximum (Information Inequality)**

   We compare the expected log-likelihood at an arbitrary $\boldsymbol{\theta}$ versus the true $\boldsymbol{\theta}_0$. Consider the difference:
   $$
   M(\boldsymbol{\theta}) - M(\boldsymbol{\theta}_0) = E_{\boldsymbol{\theta}_0} \left[ \log f(Y|\boldsymbol{\theta}) - \log f(Y|\boldsymbol{\theta}_0) \right] = E_{\boldsymbol{\theta}_0} \left[ \log \left( \frac{f(Y|\boldsymbol{\theta})}{f(Y|\boldsymbol{\theta}_0)} \right) \right]
   $$
   By **Jensen's Inequality** (since $\log(y)$ is strictly concave), $E[\log Z] \le \log E[Z]$.
   $$
   E_{\boldsymbol{\theta}_0} \left[ \log \left( \frac{f(Y|\boldsymbol{\theta})}{f(Y|\boldsymbol{\theta}_0)} \right) \right] \le \log E_{\boldsymbol{\theta}_0} \left[ \frac{f(Y|\boldsymbol{\theta})}{f(Y|\boldsymbol{\theta}_0)} \right]
   $$
   Evaluating the expectation on the right:
   $$
   E_{\boldsymbol{\theta}_0} \left[ \frac{f(Y|\boldsymbol{\theta})}{f(Y|\boldsymbol{\theta}_0)} \right] = \int \left( \frac{f(y|\boldsymbol{\theta})}{f(y|\boldsymbol{\theta}_0)} \right) f(y|\boldsymbol{\theta}_0) \, dy = \int f(y|\boldsymbol{\theta}) \, dy = 1
   $$
   Therefore:
   $$
   M(\boldsymbol{\theta}) - M(\boldsymbol{\theta}_0) \le \log(1) = 0 \implies M(\boldsymbol{\theta}) \le M(\boldsymbol{\theta}_0)
   $$
   Assuming the model is **identifiable** (i.e., $f(y|\boldsymbol{\theta}) = f(y|\boldsymbol{\theta}_0) \iff \boldsymbol{\theta} = \boldsymbol{\theta}_0$), the maximum is unique at $\boldsymbol{\theta}_0$. Since $\hat{\boldsymbol{\theta}}_n$ maximizes $M_n(\boldsymbol{\theta})$, which converges to $M(\boldsymbol{\theta})$, $\hat{\boldsymbol{\theta}}_n$ must converge to $\boldsymbol{\theta}_0$.
:::

```{tikz}
%| label: fig-likelihood-consistency
%| fig-cap: "Consistency: As n increases, the log-likelihood function concentrates around the true parameter."
%| echo: false
%| fig-align: "center"

\begin{tikzpicture}
\draw[->] (0,0) -- (6,0) node[right] {$\theta$};
\draw[->] (0,0) -- (0,4) node[above] {$M_n(\theta)$};

% True Limit Curve
\draw[thick, blue] (0.5,0.5) .. controls (3,4.5) and (5,1) .. (5.5,0.5);
\node[blue] at (5.5, 0.8) {$M(\theta)$};

% Finite Sample Curve (wiggly)
\draw[dashed, red] (0.5,0.3) .. controls (1,1) and (2.8,4.2) .. (3, 4) .. controls (3.2, 4.2) and (5,0.8) .. (5.5,0.3);
\node[red] at (5.5, 0.2) {$M_n(\theta)$};

% Maxima
\draw[dashed] (3,0) node[below] {$\theta_0$} -- (3,3.7);
\filldraw (3,3.7) circle (2pt);

\end{tikzpicture}

```

#### Non-IID Cases

In regression settings, observations are independent but not identically distributed (i.n.i.d.). We generalize the consistency result by requiring that the "average" information accumulates sufficiently.

::: {#thm-mle-consistency-general}
#### Consistency of MLE (General Case)
Let $Y_1, \dots, Y_n$ be independent observations with densities $f_i(y|\boldsymbol{\theta})$ (e.g., depending on covariates $x_i$). Let $\boldsymbol{\theta}_0$ be the true parameter.

Under the following conditions:
1.  **Parameter Space:** Compact parameter space $\Theta$.
2.  **Identification:** For any $\boldsymbol{\theta} \neq \boldsymbol{\theta}_0$, the average Kullback-Leibler divergence is strictly positive in the limit:
    $$\liminf_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \text{KL}(f_i(\cdot|\boldsymbol{\theta}_0) || f_i(\cdot|\boldsymbol{\theta})) > 0$$
3.  **Uniform Convergence:** The log-likelihood satisfies a Uniform Law of Large Numbers (ULLN).

Then $\hat{\boldsymbol{\theta}}_n \xrightarrow{p} \boldsymbol{\theta}_0$.
:::

::: {.proof}
1. **The Objective Function**

   The MLE maximizes the average log-likelihood:
   $$
   M_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \log f_i(Y_i|\boldsymbol{\theta})
   $$
   Unlike the i.i.d. case, the terms in the sum have different expectations. We rely on a **WLLN for Independent Variables** (e.g., Chebyshev's WLLN). Provided the variances are bounded, $M_n(\boldsymbol{\theta})$ converges in probability to the **average expectation**:
   $$
   M_n(\boldsymbol{\theta}) \xrightarrow{p} \bar{M}_n(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n E_{\boldsymbol{\theta}_0} [\log f_i(Y_i|\boldsymbol{\theta})]
   $$
   *Note: For consistency of the maximizer, we strictly require **Uniform Convergence** over $\Theta$, not just pointwise convergence.*

2. **Identifying the Maximum (Average KL Divergence)**

   We compare the limit function at $\boldsymbol{\theta}$ versus $\boldsymbol{\theta}_0$. Consider the average difference:
   $$
   \bar{M}_n(\boldsymbol{\theta}) - \bar{M}_n(\boldsymbol{\theta}_0) = \frac{1}{n} \sum_{i=1}^n E_{\boldsymbol{\theta}_0} \left[ \log \left( \frac{f_i(Y_i|\boldsymbol{\theta})}{f_i(Y_i|\boldsymbol{\theta}_0)} \right) \right]
   $$
   By Jensen's Inequality applied to *each* term in the sum:
   $$
   E_{\boldsymbol{\theta}_0} \left[ \log \frac{f_i(Y_i|\boldsymbol{\theta})}{f_i(Y_i|\boldsymbol{\theta}_0)} \right] \le \log E_{\boldsymbol{\theta}_0} \left[ \frac{f_i(Y_i|\boldsymbol{\theta})}{f_i(Y_i|\boldsymbol{\theta}_0)} \right] = \log(1) = 0
   $$
   Summing these inequalities implies that $\bar{M}_n(\boldsymbol{\theta})$ is uniquely maximized at $\boldsymbol{\theta}_0$ (provided the identification condition holds). Since the sample function $M_n(\boldsymbol{\theta})$ converges uniformly to this maximized limit function, the argmax $\hat{\boldsymbol{\theta}}_n$ converges to $\boldsymbol{\theta}_0$.
:::

### Asymptotic Normality of the Score Vector

The Score function acts as the "engine" for the normality of the MLE. We treat the I.I.D. and non-I.I.D. cases separately.

#### IID Cases

::: {#thm-score-normality-iid}
#### Normality of Score (I.I.D.)
Let $Y_1, \dots, Y_n$ be i.i.d. with density $f(y|\boldsymbol{\theta})$.
Define the **Fisher Information matrix** for a single observation as the expected outer product of the score:
$$
\mathbf{I}_1(\boldsymbol{\theta}) = E\left[ \left( \nabla_{\boldsymbol{\theta}} \log f(Y_1|\boldsymbol{\theta}) \right) \left( \nabla_{\boldsymbol{\theta}} \log f(Y_1|\boldsymbol{\theta}) \right)^T \right]
$$
Then, the scaled total score vector converges to a Normal distribution:
$$
\frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0; \mathbf{Y}) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
$$
:::

::: {.proof}
The total score is the sum of independent score contributions:
$$
\mathbf{U}_n(\boldsymbol{\theta}_0; \mathbf{Y}) = \sum_{i=1}^n \nabla \log f(Y_i|\boldsymbol{\theta}_0) = \sum_{i=1}^n \mathbf{u}_i
$$
From **Bartlett's Identities**, for each observation $i$:

1. **Mean**

   $E[\mathbf{u}_i] = \mathbf{0}$.

2. **Variance**

   $\text{Var}(\mathbf{u}_i) = E[\mathbf{u}_i \mathbf{u}_i^T] = \mathbf{I}_1(\boldsymbol{\theta}_0)$.

Since the terms $\mathbf{u}_i$ are i.i.d. with finite variance, we apply the **Multivariate Central Limit Theorem (Lindeberg-Lévy)**:
$$
\frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{u}_i \xrightarrow{d} N(\mathbf{0}, \text{Var}(\mathbf{u}_i)) = N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
$$
:::
#### Case B: The Non-I.I.D. Case (Regression Setting)

::: {#thm-score-normality-noniid}
#### Normality of Score (Independent, Non-Identical)
Let $Y_1, \dots, Y_n$ be independent but not necessarily identically distributed (e.g., due to different covariates). Let $\mathbf{I}_n(\boldsymbol{\theta}_0) = \sum_{i=1}^n \text{Var}(\mathbf{u}_i)$ be the total information.

Define the **Average Fisher Information Matrix**:
$$
\bar{\mathbf{I}}_n(\boldsymbol{\theta}_0) = \frac{1}{n} \mathbf{I}_n(\boldsymbol{\theta}_0) = \frac{1}{n} \sum_{i=1}^n E[\mathbf{u}_i \mathbf{u}_i^T]
$$

If the **Lindeberg Condition** is satisfied for the sequence of score vectors, then the standardized score converges to a standard Normal:
$$
\left( \mathbf{I}_n(\boldsymbol{\theta}_0) \right)^{-1/2} \mathbf{U}_n(\boldsymbol{\theta}_0; \mathbf{Y}) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}_p)
$$

**Approximating Distribution:**
If the average information converges to a positive definite limit $\bar{\mathbf{I}}_n \to \mathbf{I}_{\infty}$, we have the following approximation for the scaled average score $\sqrt{n} \bar{\mathbf{U}}_n(\boldsymbol{\theta}_0; \mathbf{Y})$:
$$
\sqrt{n} \bar{\mathbf{U}}_n(\boldsymbol{\theta}_0; \mathbf{Y}) = \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0; \mathbf{Y}) \overset{\cdot}{\sim} N(\mathbf{0}, \bar{\mathbf{I}}_n(\boldsymbol{\theta}_0))
$$
:::

::: {.proof}
This is a direct application of the **Lindeberg-Feller Central Limit Theorem**.
The total score $\mathbf{U}_n(\boldsymbol{\theta}_0; \mathbf{Y}) = \sum \mathbf{u}_i$ is a sum of independent random vectors with mean $\mathbf{0}$ and variances $\text{Var}(\mathbf{u}_i)$.
Provided that no single observation's score contribution dominates the sum (the Lindeberg condition), the standardized sum converges to a standard Normal distribution.
:::

### Asymptotic Normality of the MLE

We now transfer the normality from the Score function to the estimator $\hat{\boldsymbol{\theta}}$ using a Taylor expansion (the Delta Method logic).

::: {#def-regularity-conditions}
#### Regularity Conditions for Asymptotic Normality
The following conditions are required to ensure the validity of the Taylor expansion and the convergence of the remainder term:

1.  **Interiority:** The true parameter $\boldsymbol{\theta}_0$ lies in the interior of the parameter space $\Theta$.
2.  **Smoothness:** The log-likelihood function $\log f(y|\boldsymbol{\theta})$ is three times continuously differentiable with respect to $\boldsymbol{\theta}$.
3.  **Boundedness:** The third derivatives are bounded by an integrable function $M(y)$ (to control the Taylor remainder).
4.  **Positive Information:** The Fisher Information Matrix $\mathbf{I}_1(\boldsymbol{\theta}_0)$ exists and is non-singular (positive definite).
5.  **Interchangeability:** Differentiation and integration can be interchanged for the density $f(y|\boldsymbol{\theta})$.
:::

::: {#thm-mle-normality}
#### Asymptotic Normality of MLE
Under the regularity conditions above, the MLE $\hat{\boldsymbol{\theta}}_n$ satisfies:
$$
\sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \xrightarrow{d} N\left(\mathbf{0}, [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1}\right)
$$
:::

::: {.proof}
1. **Taylor Expansion of Score Equation**

   We expand the Score function $\mathbf{U}_n(\boldsymbol{\theta})$ around the true parameter $\boldsymbol{\theta}_0$. Since $\hat{\boldsymbol{\theta}}_n$ is the MLE, $\mathbf{U}_n(\hat{\boldsymbol{\theta}}_n) = \mathbf{0}$.
   
   Define the **Observed Information Matrix** as the negative Hessian:
   $$
   \mathbf{J}_n(\boldsymbol{\theta}) = -\nabla^2 \ell_n(\boldsymbol{\theta})
   $$

   ::: {.callout-note appearance="simple"}
   **The Linear Link (Score vs. Parameter Error)**
   The fundamental approximation linking the random Score vector to the estimation error is:
   $$
   \underbrace{\mathbf{U}_n(\hat{\boldsymbol{\theta}}_n)}_{=\mathbf{0}} \approx \mathbf{U}_n(\boldsymbol{\theta}_0) - \mathbf{J}_n(\boldsymbol{\theta}_0) (\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$
   $$
   \implies \quad \mathbf{U}_n(\boldsymbol{\theta}_0) \approx \mathbf{J}_n(\boldsymbol{\theta}_0) (\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$
   :::

2. **Scaling and Inversion**

   Multiply the link equation by $\frac{1}{\sqrt{n}}$ and introduce $n$ to the Observed Information term to stabilize the limits:
   $$
   \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \approx \left( \frac{1}{n} \mathbf{J}_n(\boldsymbol{\theta}_0) \right) \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0)
   $$
   Rearranging to solve for the estimator's distribution:
   $$
   \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \approx \left[ \frac{1}{n} \mathbf{J}_n(\boldsymbol{\theta}_0) \right]^{-1} \left[ \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \right]
   $$

3. **Convergence of Components**

   * **Numerator (Score):** From the I.I.D. Score Normality theorem:
       $$
       \frac{1}{\sqrt{n}} \mathbf{U}_n(\boldsymbol{\theta}_0) \xrightarrow{d} Z \sim N(\mathbf{0}, \mathbf{I}_1(\boldsymbol{\theta}_0))
       $$
   * **Denominator (Observed Info):** By the **WLLN**, the average observed information converges to the expected information:
       $$
       \frac{1}{n} \mathbf{J}_n(\boldsymbol{\theta}_0) = -\frac{1}{n} \sum_{i=1}^n \nabla^2 \log f(Y_i|\boldsymbol{\theta}_0) \xrightarrow{p} -E[\nabla^2 \log f] = \mathbf{I}_1(\boldsymbol{\theta}_0)
       $$

4. **Slutsky's Theorem**

   Combining these via Slutsky's Theorem (matrix inverse is a continuous mapping):
   $$
   \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \xrightarrow{d} [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1} Z
   $$
   The variance of this limiting distribution is:
   $$
   \text{Var}([\mathbf{I}_1]^{-1} Z) = \mathbf{I}_1^{-1} \text{Var}(Z) (\mathbf{I}_1^{-1})^T = \mathbf{I}_1^{-1} \mathbf{I}_1 \mathbf{I}_1^{-1} = \mathbf{I}_1^{-1}
   $$
   Thus:
   $$
   \sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}_0) \xrightarrow{d} N\left( \mathbf{0}, [\mathbf{I}_1(\boldsymbol{\theta}_0)]^{-1} \right)
   $$
:::

```{r}
#| label: fig-score-approx
#| fig-cap: "Linear Approximation of the Score Function: The vertical purple segment represents the Score at the true parameter ($\\Delta U$), while the horizontal green segment shows the estimation error ($\\hat{\\theta}_n - \\theta_0$). The red dashed line indicates the linear approximation ($-J(\\theta_0)$)."
#| warning: false
#| message: false

library(ggplot2)
library(dplyr)

# --- 1. Settings and Data Generation (Cauchy) ---
set.seed(42)
n_sim <- 10
n_obs <- 10
theta_0 <- 10 
gamma <- 1    

# Generate Datasets
datasets <- matrix(rcauchy(n_sim * n_obs, location = theta_0, scale = gamma), 
                   nrow = n_sim, ncol = n_obs)

# Define Score and Hessian Functions
score_cauchy <- function(y, theta) {
  sum( (2 * (y - theta)) / (1 + (y - theta)^2) )
}
hessian_cauchy <- function(y, theta) {
  u <- y - theta
  sum( (2 * (u^2 - 1)) / (1 + u^2)^2 )
}

# --- 2. Analyze Main Dataset ---
y_main <- datasets[1, ]
# Find MLE numerically
mle_res <- optimize(function(th) sum(dcauchy(y_main, location = th, log = TRUE)),
                    interval = c(theta_0 - 5, theta_0 + 5), maximum = TRUE)
theta_hat <- mle_res$maximum

# Calculate Geometry at theta_0
u_true <- score_cauchy(y_main, theta_0)
slope_true <- hessian_cauchy(y_main, theta_0)

# --- 3. Prepare Plot Data ---
theta_seq <- seq(8, 12, length.out = 300)

# Curves Data
plot_data <- data.frame()
for(i in 1:n_sim) {
  y_curr <- datasets[i, ]
  u_vals <- sapply(theta_seq, function(th) score_cauchy(y_curr, th))
  plot_data <- rbind(plot_data, data.frame(theta = theta_seq, U = u_vals, sim_id = as.factor(i), is_main = (i == 1)))
}

# Linear Approximation Data (Tangent Line)
tangent_data <- data.frame(
  theta = theta_seq,
  U_lin = u_true + slope_true * (theta_seq - theta_0)
)

# --- 4. Plotting ---
ggplot() +
  # Axes Reference
  geom_hline(yintercept = 0, color = "black", size = 0.5) +
  geom_vline(xintercept = theta_0, color = "black", linetype = "dotted") +
  
  # A. Background Curves (Grey)
  geom_line(data = subset(plot_data, !is_main), 
            aes(x = theta, y = U, group = sim_id), 
            color = "grey80", alpha = 0.5) +
  
  # B. Main Curve (Blue) & Tangent (Red Dashed)
  geom_line(data = subset(plot_data, is_main), aes(x = theta, y = U), 
            color = "blue", size = 1.2) +
  geom_line(data = tangent_data, aes(x = theta, y = U_lin), 
            color = "red", linetype = "dashed", size = 1) +
  
  # C. THICK SEGMENTS for Delta U and Delta Theta
  # Vertical Segment (Delta U): From axis to curve at theta_0
  # Color: Purple
  annotate("segment", x = theta_0, xend = theta_0, y = 0, yend = u_true,
           color = "purple", size = 2, alpha = 0.8) +
  
  # Horizontal Segment (Delta Theta): From theta_0 to theta_hat along the axis
  # Color: Dark Green
  annotate("segment", x = theta_0, xend = theta_hat, y = 0, yend = 0,
           color = "darkgreen", size = 2, alpha = 0.8) +
  
  # D. Points
  geom_point(aes(x = theta_0, y = u_true), color = "red", size = 3) +
  geom_point(aes(x = theta_hat, y = 0), color = "darkgreen", size = 3) +
  
  # E. Labels (Delta labels)
  annotate("text", x = theta_0 + 0.15, y = u_true / 2, 
           label = "Delta~U", parse = TRUE, color = "purple", fontface = "bold", hjust = 0) +
  
  annotate("text", x = (theta_0 + theta_hat)/2, y = -1.0, 
           label = "hat(theta)[n] - theta[0]", parse = TRUE, color = "darkgreen", fontface = "bold") +
  
  # F. Parameter Labels (Revised Position)
  annotate("text", x = theta_hat, y = 1, label = "hat(theta)[n]", parse = TRUE, color = "darkgreen", size = 5) +
  annotate("text", x = theta_0, y = 1, label = "theta[0]", parse = TRUE, color = "red", size = 5) +

  # G. Styling
  coord_cartesian(ylim = c(-10, 10), xlim = c(8.5, 11.5)) +
  labs(title = expression(paste("Score Function Approximation (Cauchy)")),
       subtitle = expression(paste("Linear Link: ", Delta, "U ", approx, " -J ", Delta, "theta")),
       x = expression(theta), y = expression(U(theta))) +
  theme_minimal()
```

### Wilks' Theorem (Likelihood Ratio Test)

This theorem connects the Likelihood Ratio statistic to the Chi-squared distribution.

::: {#thm-wilks}
#### Wilks' Theorem
Consider testing $H_0: \boldsymbol{\theta} = \boldsymbol{\theta}_0$ against $H_1: \boldsymbol{\theta} \ne \boldsymbol{\theta}_0$.
Let $\Lambda_n$ be the likelihood ratio:
$$
\Lambda_n = \frac{L(\boldsymbol{\theta}_0)}{L(\hat{\boldsymbol{\theta}}_{\text{MLE}})}
$$
Then the Deviance statistic ($-2 \log \Lambda$) converges to a Chi-squared distribution:
$$
-2 \log \Lambda_n = 2 [\ell_n(\hat{\boldsymbol{\theta}}) - \ell_n(\boldsymbol{\theta}_0)] \xrightarrow{d} \chi^2_p
$$
where $p$ is the dimension of $\boldsymbol{\theta}$.
:::

::: {.proof}
1. **Taylor Expansion of Log-Likelihood**

   Expand $\ell_n(\boldsymbol{\theta}_0)$ around the MLE $\hat{\boldsymbol{\theta}}$. Note that the first derivative term vanishes because $\mathbf{U}_n(\hat{\boldsymbol{\theta}}) = \mathbf{0}$.
   $$
   \ell_n(\boldsymbol{\theta}_0) \approx \ell_n(\hat{\boldsymbol{\theta}}) + (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})^T \mathbf{U}_n(\hat{\boldsymbol{\theta}}) + \frac{1}{2} (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})
   $$
   $$
   \ell_n(\boldsymbol{\theta}_0) \approx \ell_n(\hat{\boldsymbol{\theta}}) + 0 + \frac{1}{2} (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$

2. **Rearranging for Deviance**

   $$
   2[\ell_n(\hat{\boldsymbol{\theta}}) - \ell_n(\boldsymbol{\theta}_0)] \approx - (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \mathbf{H}_n(\hat{\boldsymbol{\theta}}) (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$
   Multiply and divide by $n$:
   $$
   = - \sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^T \left[ \frac{1}{n} \mathbf{H}_n(\hat{\boldsymbol{\theta}}) \right] \sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)
   $$

3. **Applying Asymptotic Results**

   * $\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \xrightarrow{d} Z \sim N(\mathbf{0}, \mathbf{I}^{-1})$.
   * $-\frac{1}{n} \mathbf{H}_n(\hat{\boldsymbol{\theta}}) \xrightarrow{p} \mathbf{I}$ (Fisher Information).

   Substituting these limits:
   $$
   -2 \log \Lambda_n \approx Z^T \mathbf{I} Z
   $$
   Let $Y = \mathbf{I}^{1/2} Z$. Since $Z \sim N(\mathbf{0}, \mathbf{I}^{-1})$, the variance of $Y$ is:
   $$
   \text{Var}(Y) = \mathbf{I}^{1/2} \text{Var}(Z) (\mathbf{I}^{1/2})^T = \mathbf{I}^{1/2} \mathbf{I}^{-1} \mathbf{I}^{1/2} = \mathbf{I}_p \quad \text{(Identity Matrix)}
   $$
   Thus, $Y \sim N(\mathbf{0}, \mathbf{I}_p)$, and the quadratic form becomes:
   $$
   Z^T \mathbf{I} Z = Y^T Y = \sum_{j=1}^p Y_j^2 \sim \chi^2_p
   $$
:::

## Hypothesis Testing: Likelihood Ratio Test

Consider testing:
$$
H_0: \theta \in \Theta_0 \quad \text{vs} \quad H_1: \theta \in \Theta \setminus \Theta_0
$$
where $\dim(\Theta) = p$ and $\dim(\Theta_0) = p-m$.

The Likelihood Ratio Statistic is:
$$
\Lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta; x)}{\sup_{\theta \in \Theta} L(\theta; x)} = \frac{L(\hat{\theta}_0)}{L(\hat{\theta})}
$$

::: {#thm-wilks}

## Wilks' Theorem
Under regularity conditions, under $H_0$:
$$
-2 \log \Lambda \xrightarrow{d} \chi^2_m
$$
where $m$ is the difference in dimensions (number of restrictions).

:::

::: {#exm-normal-mean-lrt}
Let $X_1, \dots, X_n \sim N(\mu, \sigma^2)$.
$H_0: \mu = \mu_0$ vs $H_1: \mu \ne \mu_0$.
Here $p=2$ ($\mu, \sigma^2$) and under $H_0$, free parameters = 1 ($\sigma^2$). So $m = 2-1 = 1$.

The statistic $\Lambda$:
$$
\Lambda = \left( \frac{\hat{\sigma}^2}{\hat{\sigma}_0^2} \right)^{n/2} = \left( \frac{\sum(x_i - \bar{x})^2}{\sum(x_i - \mu_0)^2} \right)^{n/2}
$$
It can be shown that:
$$
\Lambda = \left( 1 + \frac{(\bar{x} - \mu_0)^2}{\hat{\sigma}^2} \right)^{-n/2}
$$
The rejection region $-2 \log \Lambda > \chi^2_{1, \alpha}$ is equivalent to the t-test:
$$
\left| \frac{\bar{x} - \mu_0}{S/\sqrt{n}} \right| > t_{n-1, \alpha/2}
$$

:::

## Illustration of Wilks' Theorem

We can approximate the statistic using Taylor expansion. Consider the scalar case.
$$
-2 \log \Lambda = 2 [l(\hat{\theta}) - l(\theta_0)]
$$

```{tikz}
%| label: fig-wilks-proof
%| fig-cap: "Approximation of Likelihood Ratio"
%| echo: false
%| fig-align: "center"
%| out-extra: 'style="width: 50% !important;"'

\begin{tikzpicture}
\draw[->] (-0.5,0) -- (4,0) node[right] {$x$};
\draw[->] (0,-1) -- (0,3) node[above] {$y$};
\draw[thick, domain=-0.5:3.5] plot (\x, {ln(1+\x)});
\node at (2.5, 1.5) {$\log(1+x)$};
\draw[dashed] (0,0) -- (3,3) node[right] {$y=x$};
\draw (1,0) node[below] {$\tau^*$} -- (1, {ln(2)});
\draw (2,0) node[below] {$\tau$} -- (2, {ln(3)});
\end{tikzpicture}

```

Using Taylor expansion around the MLE $\hat{\theta}$:
$$
l(\theta_0) \approx l(\hat{\theta}) + (\theta_0 - \hat{\theta})l'(\hat{\theta}) + \frac{1}{2}(\theta_0 - \hat{\theta})^2 l''(\hat{\theta})
$$
Since $l'(\hat{\theta}) = 0$:
$$
2[l(\hat{\theta}) - l(\theta_0)] \approx -(\theta_0 - \hat{\theta})^2 l''(\hat{\theta}) = (\hat{\theta} - \theta_0)^2 [-\frac{1}{n} l''(\hat{\theta})] \cdot n
$$
As $n \to \infty$, $-\frac{1}{n}l'' \to I(\theta_0)$ and $\sqrt{n}(\hat{\theta}-\theta_0) \to N(0, 1/I)$.
Thus, the expression behaves like $Z^2 \sim \chi^2_1$.


## An Example: Poisson Regression

In this example, we explore the Generalized Linear Model (GLM) for count data using the Poisson distribution. Let $Y_1, \dots, Y_n$ be independent count variables where $Y_i \sim \text{Poisson}(\lambda_i)$. The expected count $\lambda_i$ is related to a vector of covariates $\mathbf{x}_i$ and parameters $\boldsymbol{\beta}$ via the **canonical log link function**:
$$ \log(\lambda_i) = \eta_i = \mathbf{x}_i^\top \boldsymbol{\beta} = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik} $$

### Canonical Representation

We begin by expressing the log-likelihood in the canonical exponential family form. The probability mass function for the Poisson distribution is $P(Y_i=y_i) = \frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}$. The log-likelihood is:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i \log(\lambda_i) - \lambda_i - \log(y_i!) \right) $$

Substituting the log link $\log(\lambda_i) = \mathbf{x}_i^\top \boldsymbol{\beta}$ and $\lambda_i = e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$:
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left( y_i (\mathbf{x}_i^\top \boldsymbol{\beta}) - e^{\mathbf{x}_i^\top \boldsymbol{\beta}} \right) + \text{const} $$
Rearranging terms to isolate the parameters $\beta_j$:
$$ \ell(\boldsymbol{\beta}; \mathbf{y}) = \sum_{j=0}^k \beta_j \underbrace{\left( \sum_{i=1}^n y_i x_{ij} \right)}_{T_j(\mathbf{y})} - \underbrace{\sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}_{A(\boldsymbol{\beta})} + \text{const} $$

From this form, we identify:

* **Sufficient Statistics:** $\mathbf{T}(\mathbf{y}) = \mathbf{X}^\top \mathbf{y}$.
* **Log-Partition Function:** $A(\boldsymbol{\beta}) = \sum_{i=1}^n \lambda_i = \sum_{i=1}^n e^{\mathbf{x}_i^\top \boldsymbol{\beta}}$.

### The Score and Information

Next, we derive the gradient and Hessian of the log-likelihood.

* **Score Vector ($\mathbf{U}$):** The gradient of $\ell(\boldsymbol{\beta})$ is the difference between the observed sufficient statistics and their expectations (derived from $\nabla A$).
    $$ \mathbf{U}(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell = \mathbf{T}(\mathbf{y}) - \nabla A(\boldsymbol{\beta}) = \sum_{i=1}^n y_i \mathbf{x}_i - \sum_{i=1}^n \lambda_i \mathbf{x}_i = \mathbf{X}^\top (\mathbf{y} - \boldsymbol{\lambda}) $$

* **Fisher Information Matrix ($\mathcal{J}$):** This is the negative Hessian of the log-likelihood, or equivalently the Hessian of the log-partition function $A(\boldsymbol{\beta})$.
    $$ \mathcal{J}(\boldsymbol{\beta}) = \nabla^2 A(\boldsymbol{\beta}) = \sum_{i=1}^n \mathbf{x}_i \frac{\partial \lambda_i}{\partial \boldsymbol{\beta}^\top} = \sum_{i=1}^n \lambda_i \mathbf{x}_i \mathbf{x}_i^\top $$
    In matrix notation, $\mathcal{J}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}$, where $\mathbf{W} = \text{diag}(\lambda_i)$. Note that for the Poisson model, the variance equals the mean $\lambda_i$.

### Asymptotic Distributions (Theory)

a.  **Normality of the Score:**
    The Score vector $\mathbf{U}(\boldsymbol{\beta}) = \sum_{i=1}^n (Y_i - \lambda_i)\mathbf{x}_i$ is a sum of independent mean-zero random vectors.

    * Mean: $E[\mathbf{U}] = \mathbf{0}$.
    * Variance: $\text{Var}(\mathbf{U}) = \sum \text{Var}(Y_i)\mathbf{x}_i\mathbf{x}_i^\top = \mathcal{J}(\boldsymbol{\beta})$.
    By the **Multivariate Central Limit Theorem**, as $n \to \infty$:
    $$ \frac{1}{\sqrt{n}}\mathbf{U}(\boldsymbol{\beta}) \xrightarrow{d} N(\mathbf{0}, \mathcal{I}(\boldsymbol{\beta})) $$
    where $\mathcal{I}$ is the limit of $\frac{1}{n}\mathcal{J}$.

b.  **Normality of the Estimator:**
    The MLE $\hat{\boldsymbol{\beta}}$ satisfies $\mathbf{U}(\hat{\boldsymbol{\beta}}) = \mathbf{0}$. Taking a first-order Taylor expansion around the true parameter $\boldsymbol{\beta}$:
    $$ \mathbf{0} = \mathbf{U}(\hat{\boldsymbol{\beta}}) \approx \mathbf{U}(\boldsymbol{\beta}) - \mathcal{J}(\boldsymbol{\beta}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) $$
    Rearranging gives $(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \approx \mathcal{J}^{-1}(\boldsymbol{\beta}) \mathbf{U}(\boldsymbol{\beta})$. Since $\mathbf{U}$ is asymptotically normal, the linear transformation implies:
    $$ \sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}) \xrightarrow{d} N\left(\mathbf{0}, \mathcal{I}^{-1}(\boldsymbol{\beta})\right) $$

### Numerical Application in R

We now implement the Newton-Raphson algorithm in R to estimate parameters for a Poisson regression model with a **continuous covariate**. Let $\log(\lambda_i) = \beta_0 + \beta_1 x_i$.

**a. Data Generation**
We simulate $n=100$ observations using a continuous predictor $x$ drawn from a Uniform distribution.

```r
set.seed(123)
n <- 100
x <- runif(n, 0, 1)

# True parameters: Intercept=0.5, Slope=2.0
beta_true <- c(0.5, 2.0) 

# Generate response Y
lambda_true <- exp(beta_true[1] + beta_true[2] * x)
y <- rpois(n, lambda_true)

# Design Matrix (intercept column + covariate)
X <- cbind(1, x)

```

**b. Newton-Raphson Implementation**
The update rule is $\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \mathcal{J}^{-1}\mathbf{U}$. We implement this iteratively.

```r
newton_raphson_poisson <- function(X, y, tol = 1e-6, max_iter = 100) {
  beta <- rep(0, ncol(X)) # Start at 0
  
  for (i in 1:max_iter) {
    # 1. Compute means and weights
    eta <- X %*% beta
    lambda <- as.vector(exp(eta))
    
    # 2. Compute Score U and Info J
    U <- crossprod(X, y - lambda)
    J <- crossprod(X * lambda, X) # Efficient t(X) %*% W %*% X
    
    # 3. Update beta
    delta <- solve(J, U)
    beta_new <- beta + as.vector(delta)
    
    # 4. Check convergence
    diff <- sum((beta_new - beta)^2)
    cat(sprintf("Iter %d: beta0=%.4f, beta1=%.4f, diff=%.6f\n", 
                i, beta_new[1], beta_new[2], diff))
    
    if (diff < tol) return(beta_new)
    beta <- beta_new
  }
}

beta_mle <- newton_raphson_poisson(X, y)

```

**Output:**

```
Iter 1: beta0=1.1718, beta1=0.6234, diff=1.761592
Iter 2: beta0=0.6970, beta1=1.5794, diff=1.139433
Iter 3: beta0=0.4996, beta1=2.0101, diff=0.224446
Iter 4: beta0=0.4725, beta1=2.0838, diff=0.006173
Iter 5: beta0=0.4722, beta1=2.0845, diff=0.000001

```

**c. Discussion & Verification**
We compare our manual estimates with R's built-in `glm` function.

```r
cat("Manual MLE: ", beta_mle, "\n")
cat("GLM Output: ", coef(glm(y ~ x, family = "poisson")))

```

**Observation:** The manual implementation converges to the exact same values as the built-in function ($\hat{\beta}_0 \approx 0.47, \hat{\beta}_1 \approx 2.08$). Notice that we started at $\boldsymbol{\beta}=(0,0)$. In the first iteration, the algorithm took a large step because the log-likelihood surface is steep. The quadratic convergence of Newton-Raphson is evident in the rapid decrease of the difference term (from 1.13 to 0.22 to 0.006).

