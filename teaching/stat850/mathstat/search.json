[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface\nThis is a concise course about statistical inference.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Inference",
    "section": "Key Features",
    "text": "Key Features\n\nUse simulation and graphs to illustrate the concepts in probability theory and statistical inference\nRigourous derivation of the key theorems in statistical inference",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introstatinf.html",
    "href": "introstatinf.html",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "1.1 Population Model (Data Model)\nWe begin with observations (units) \\(X_1, X_2, \\dots, X_n\\). These may be vectors. We regard these observations as a realization of random variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#statistical-inference-setup",
    "href": "introstatinf.html#statistical-inference-setup",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\n1.1.1 Assumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model: Normal) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).\n\n\nExample 1.2 (Parametric Model: Bernoulli) Consider a sequence of binary outcomes (e.g., Success/Failure) where each \\(X_i \\in \\{0, 1\\}\\). We assume \\(X_i \\sim \\text{Bernoulli}(\\theta)\\). The probability mass function is: \\[f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}\\] Here, the parameter space is \\(\\Theta = [0, 1]\\), where \\(\\theta\\) represents the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probability-vs.-statistics",
    "href": "introstatinf.html#probability-vs.-statistics",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probability vs. Statistics",
    "text": "1.2 Probability vs. Statistics\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\). We can visualize this using a “shooting target” analogy:\n\n\\(\\theta\\) (The Center): The true, unknown bullseye location.\n\\(x\\) (The Shots): The observed holes on the target board.\nProbability (Deductive): The center \\(\\theta\\) is known. We predict where the shots \\(x\\) will land.\nStatistics (Inductive): The shots \\(x\\) are observed on the board. The center \\(\\theta\\) is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n\n\n\n\n\n\n\nFigure 1.1: Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#types-of-statistical-inference",
    "href": "introstatinf.html#types-of-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Types of Statistical Inference",
    "text": "1.4 Types of Statistical Inference\nWe can categorize inference into four main types:\n\nDefinition 1.3 (Point Estimation) We use a single number to capture the parameter. \\[\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\]\n\n\nExample 1.4 (Estimating Average Height) We want to estimate the average height (\\(\\mu\\)) of all students in a university. We measure 100 students and calculate the sample mean \\(\\bar{x} = 170\\) cm. Our point estimate is \\(\\hat{\\mu} = 170\\).\n\n\nDefinition 1.4 (Interval Estimation) We construct an interval that likely contains the true parameter. \\[\\theta \\in (L(X_1, \\dots, X_n), U(X_1, \\dots, X_n))\\] The true parameter is within this interval.\n\n\nExample 1.5 (Confidence Interval for Height) Using the same height data, we calculate a 95% Confidence Interval. We state: “We are 95% confident that the true average height is between 168 cm and 172 cm.”\n\n\nDefinition 1.5 (Hypothesis Testing) We test a specific theory about the parameter. \\[H_0: \\theta = \\theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\neq \\theta_0\\] (Or one-sided alternatives like \\(\\theta &gt; \\theta_0\\)).\n\n\nExample 1.6 (Testing Soda Volume) A manufacturer claims their soda bottles contain exactly 500ml (\\(H_0: \\mu = 500\\)). We measure a sample and find an average of 495ml. We perform a test to see if this difference is significant enough to reject the manufacturer’s claim.\n\n\nDefinition 1.6 (Predictive Inference) Given observed data \\((X_1, Y_1), \\dots, (X_n, Y_n)\\), we want to predict a new observation \\(Y_{n+1}\\) given \\(X_{n+1}\\). This is often the primary goal in Machine Learning.\n\n\nExample 1.7 (Predicting House Prices) Based on data about house sizes (\\(X\\)) and prices (\\(Y\\)) from the last year, we want to predict the selling price (\\(Y_{n+1}\\)) of a specific new house that is 2000 sq ft (\\(X_{n+1}\\)).\n\nThere are two primary frameworks for how to perform these inferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#standard-paradigms-for-inference",
    "href": "introstatinf.html#standard-paradigms-for-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 Standard Paradigms for Inference",
    "text": "1.5 Standard Paradigms for Inference\nThere are two primary frameworks for how to perform these inferences.\n\n1.5.1 Frequentist Inference (Fisher)\nDeveloped largely by Fisher (c. 1920).\n\nConcept: The parameter \\(\\theta\\) is a fixed, unknown constant. The data \\(X\\) are random.\nRepeated Sampling Principle: Inference is based on the performance of methods (estimators) under hypothetical repeated sampling of the data.\nSampling Distribution: We analyze how the estimator \\(\\hat{\\theta}\\) behaves over many different datasets generated from the same population.\n\n\n\n\n\n\n\n\n\nFigure 1.3: Frequentist Concept: The Sampling Distribution. The parameter \\(\\theta\\) is fixed (vertical line). The curve represents how the estimator \\(\\hat{\\theta}\\) varies across many hypothetical samples.\n\n\n\n\n\n\n1.5.1.1 Bernoulli Example: Sampling Distribution\nFor our Bernoulli data (\\(x = \\{1, 0, 1\\}\\), \\(n=3\\)), the estimator \\(\\bar{X}\\) is a scaled Binomial variable. If the true parameter were \\(\\theta = 0.5\\), the exact distribution and its Normal approximation would be:\n\n\nCode\ntrue_theta &lt;- 0.5\nn &lt;- 3\nk_vals &lt;- 0:n\nx_bar_vals &lt;- k_vals / n\nprobs &lt;- dbinom(k_vals, size=n, prob=true_theta)\n\ndf_exact &lt;- data.frame(x_bar = x_bar_vals, prob = probs)\n\nx_grid &lt;- seq(-0.2, 1.2, length.out=200)\nsd_approx &lt;- sqrt(true_theta * (1 - true_theta) / n)\nnorm_dens &lt;- dnorm(x_grid, mean=true_theta, sd=sd_approx)\ndf_approx &lt;- data.frame(x = x_grid, density = norm_dens)\n\n# Identify extreme regions for p-value shading (Two-sided: x &gt;= 2/3 and x &lt;= 1/3)\n# Note: 1/3 is symmetric to 2/3 around the mean of 0.5\ndf_shade &lt;- subset(df_approx, x &gt;= 2/3 | x &lt;= 1/3)\n\nggplot() +\n  # 1. Exact Discrete Bars\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob), \n               size=5, color=\"darkgreen\", alpha=0.6) +\n  \n  # 2. Shaded P-value Regions (Normal Approx)\n  geom_area(data=df_shade, aes(x=x, y=density * (1/n)), \n            fill=\"red\", alpha=0.3) +\n  \n  # 3. Normal Approximation Curve\n  geom_line(data=df_approx, aes(x=x, y=density * (1/n)), \n            color=\"red\", size=1.2, linetype=\"dashed\") +\n  \n  # 4. Vertical Line at Observed Mean\n  geom_vline(xintercept = 2/3, color = \"blue\", size = 1, linetype = \"solid\") +\n  annotate(\"text\", x = 2/3, y = -0.02, label = \"Observed\\nbar(x) == 2/3\", \n           parse = TRUE, color = \"blue\", vjust = 1, size = 3.5) +\n  \n  labs(title = expression(paste(\"Sampling Distribution: Transformed Binomial vs. Normal\")),\n       subtitle = expression(paste(\"Testing \", H[0]: theta == 0.5, \" vs \", H[1]: theta != 0.5)),\n       x = expression(bar(x)), y = \"Probability Mass\") +\n  theme_minimal() +\n  # Increase bottom margin for annotation\n  theme(plot.margin = margin(t=10, r=10, b=30, l=10))\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution of \\(\\bar{X}\\) for \\(n=3\\) assuming true \\(\\theta=0.5\\). The vertical line shows the observed \\(\\bar{x}=2/3\\). The shaded red areas represent the p-value (probability of observing a result as extreme as 2/3 under the Null).\n\n\n\n\n\n\n\n1.5.1.2 Key Questions in Statistical Inference\n\nPoint Estimation\n\nConstruction: How do we construct an estimator for an unknown parameter \\(\\theta\\)? (e.g., Method of Moments, Maximum Likelihood Estimation).\nEvaluation: Which estimator is “better”? How do we compare them? (e.g., Unbiasedness, Minimum Variance, Mean Squared Error, Consistency).\nDistribution: What is the sampling distribution of the estimator? Is it exact or asymptotic (approximate)?\n\n\n\nHypothesis Testing\n\nConstruction: How do we construct a test statistic to decide between hypotheses? (e.g., Likelihood Ratio Test, Wald Test, Score Test).\nDecision Rule: How do we determine the critical region or rejection rule?\nErrors: How do we control the probability of making errors? (Type I vs. Type II errors, Power of the test).\n\n\n\nInterval Estimation\n\nConstruction: How do we construct a confidence interval (or credible interval) that covers the true parameter with high probability?\nDuality: How does interval estimation relate to hypothesis testing? (e.g., Inverting a test statistic).\n\n\n\nPrediction\n\nFuture Observations: How do we account for both the uncertainty in the parameter estimate and the random variation of the new data point?\n\n\n\n\n\n1.5.2 Bayesian Inference\nIn the Bayesian framework, we treat the parameter \\(\\theta\\) as a random variable representing our knowledge/uncertainty.\n\nPrior: We assign a prior distribution \\(\\pi(\\theta)\\) reflecting beliefs before seeing data.\nData Model: We have the likelihood \\(f(x_1, \\ldots, x_n|\\theta)\\).\nPosterior: We compute the posterior distribution using Bayes’ theorem: \\[f(\\theta|x_1, \\ldots, x_n) = \\frac{\\pi(\\theta)f(x_1, \\ldots, x_n|\\theta)}{\\int \\pi(\\theta)f(x_1, \\ldots, x_n|\\theta) d\\theta}\\]\n\nIn this framework, inference is based entirely on the Posterior Distribution, which combines the Prior and the Likelihood.\n\n1.5.2.1 Bernoulli Example: Bayesian Update\nUsing \\(x = \\{1, 0, 1\\}\\) and a weakly informative \\(\\text{Beta}(2,2)\\) prior:\n\nPrior: \\(\\theta \\sim \\text{Beta}(2,2)\\)\nLikelihood: \\(L(\\theta) \\propto \\theta^2(1-\\theta)^1\\)\nPosterior: \\(\\theta|x \\sim \\text{Beta}(2+2, 2+1) = \\text{Beta}(4,3)\\)\n\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nprior &lt;- dbeta(theta_grid, 2, 2)\nposterior &lt;- dbeta(theta_grid, 4, 3)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"Posterior\" = \"blue\", \"Prior\" = \"gray\")) +\n  labs(title = \"Bayesian Updating: Prior vs Posterior\",\n       x = expression(theta), y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Bayesian Updating for Bernoulli Data {1, 0, 1}.\n\n\n\n\n\n\n\n\n1.5.3 Bayesian Prediction\nThe predictive density for a new observation \\(x_{n+1}\\) is obtained by integrating over the posterior: \\[f(x_{n+1}|x) = \\int f(x_{n+1}|\\theta) \\pi(\\theta|x) d\\theta\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "2  Decision Theory",
    "section": "",
    "text": "2.1 Formulation of Decision Theory\nIn decision theory, we formalize the process of making decisions under uncertainty using the following components:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#decision-rules-and-risk-functions",
    "href": "decision.html#decision-rules-and-risk-functions",
    "title": "2  Decision Theory",
    "section": "2.2 Decision Rules and Risk Functions",
    "text": "2.2 Decision Rules and Risk Functions\n\n2.2.1 Decision Rule\nA decision rule is a function \\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\). It dictates the action \\(d(x)\\) we take when we observe data \\(x\\).\n\n\n2.2.2 Risk Function\nThe risk function is the expected loss for a given decision rule \\(d\\) as a function of the parameter \\(\\theta\\).\n\\[R(\\theta, d) = E_\\theta[L(\\theta, d(X))]\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#comparing-decision-rules",
    "href": "decision.html#comparing-decision-rules",
    "title": "2  Decision Theory",
    "section": "2.5 Comparing Decision Rules",
    "text": "2.5 Comparing Decision Rules\nWe typically do not have a single rule \\(d\\) that is better than all other rules for all \\(\\theta\\).\n\n\\(d\\) strictly dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nAdmissibility: A decision rule \\(d\\) is admissible if it is not dominated by any other rule. If it is dominated, it is inadmissible.\n\n\n2.5.1 Minimax Principle\nA rule \\(d\\) is Minimax if it minimizes the maximum possible risk.\n\\[\\sup_{\\theta} R(\\theta, d) \\le \\sup_{\\theta} R(\\theta, d') \\quad \\text{for all } d' \\in \\mathcal{D}\\]\nVisualize the risk functions of two rules, \\(d_1\\) and \\(d_2\\). \\(d_2\\) might have a higher risk in some areas but a lower “peak” risk, making it Minimax.\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Minimax: Rule d2 has a lower maximum risk than d1, making it the Minimax rule, even though d1 is better for some values of theta.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#bayes-decision-rules",
    "href": "decision.html#bayes-decision-rules",
    "title": "Decision Theory",
    "section": "2.5 Bayes Decision Rules",
    "text": "2.5 Bayes Decision Rules\nWe specify a prior distribution \\(\\pi(\\theta)\\) on the parameter space \\(\\Theta\\).\n\n2.5.1 Bayes Risk\nThe Bayes risk of a rule \\(d\\) with respect to prior \\(\\pi\\) is the weighted average of the frequentist risk:\n\\[r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n2.5.2 Bayes Rule Definition\nA decision rule \\(d_\\pi\\) is a Bayes rule with respect to \\(\\pi\\) if it minimizes the Bayes risk:\n\\[r(\\pi, d_\\pi) = \\inf_{d' \\in \\mathcal{D}} r(\\pi, d')\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#randomized-decision-rules-risk-sets",
    "href": "decision.html#randomized-decision-rules-risk-sets",
    "title": "Decision Theory",
    "section": "2.6 Randomized Decision Rules & Risk Sets",
    "text": "2.6 Randomized Decision Rules & Risk Sets\n\n2.6.1 Randomized Rules\nA randomized decision rule chooses between deterministic rules \\(d_1, d_2, \\dots\\) with probabilities \\(p_1, p_2, \\dots\\). The risk of a randomized rule \\(d^* = \\sum p_i d_i\\) is the linear combination of their risks: \\[R(\\theta, d^*) = \\sum p_i R(\\theta, d_i)\\]\n\n\n2.6.2 Geometric Interpretation (Finite Parameter Space)\nIf \\(\\Theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_k\\}\\) is finite, we can plot the risk vector \\((R(\\theta_1, d), \\dots, R(\\theta_k, d))\\) in \\(\\mathbb{R}^k\\).\nRisk Set (\\(S\\)): The set of all possible risk vectors. Lemma: The Risk Set \\(S\\) is convex. This is because we can form randomized rules that lie on the line segment connecting any two deterministic rules.\n\n\n\n\n\n\n\n\nFigure 2.3: The Risk Set S. Points d1 and d2 are deterministic rules. The line connecting them represents randomized rules. The Minimax rule lies on the line R1=R2 (if accessible). The Bayes rule is found by bringing a tangent line with slope -pi1/pi2 toward the origin.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#example-the-great-aunts-necklace",
    "href": "decision.html#example-the-great-aunts-necklace",
    "title": "2  Decision Theory",
    "section": "2.6 Example: The “Great Aunt’s Necklace”",
    "text": "2.6 Example: The “Great Aunt’s Necklace”\nScenario: Two boxes. One contains a real necklace (\\(\\theta=1\\)), the other an imitation (\\(\\theta=2\\)). Loss is 1 if we choose the wrong box, 0 otherwise.\n\n\\(\\theta \\in \\{1, 2\\}\\)\nAction \\(a \\in \\{1, 2\\}\\) (Choose Box 1 or Box 2)\n\nData (\\(X\\)): Great Aunt’s judgment. \\(X \\in \\{1, 2\\}\\) (Aunt says Box 1 or Box 2). Probabilities: * If \\(\\theta=1\\) (Real in Box 1): Aunt is senile. \\(P(X=1)=1, P(X=2)=0\\). * If \\(\\theta=2\\) (Real in Box 2): Aunt guesses. \\(P(X=1)=0.5, P(X=2)=0.5\\).\nDecision Rules: 1. \\(d_1\\): Always choose Box 1. 2. \\(d_2\\): Always choose Box 2. 3. \\(d_3(x) = x\\): Follow Aunt’s advice. 4. \\(d_4(x) = 3-x\\): Do opposite of Aunt.\nRisk Calculation:\n\n\n\nRule\n\\(R(\\theta=1)\\)\n\\(R(\\theta=2)\\)\nCoordinates \\((R_1, R_2)\\)\n\n\n\n\n\\(d_1\\)\n0\n1\n(0, 1)\n\n\n\\(d_2\\)\n1\n0\n(1, 0)\n\n\n\\(d_3\\)\n0\n0.5\n(0, 0.5)\n\n\n\\(d_4\\)\n1\n0.5\n(1, 0.5)\n\n\n\nGeometry and Minimax: The risk set is the convex hull of these four points.\n\n\n\n\n\n\n\n\nFigure 2.3: Risk Set for the Necklace Example. The set is the quadrilateral defined by d1, d2, d3, d4. The Minimax rule is the intersection of the line R1=R2 and the lower boundary segment connecting d3 and d2.\n\n\n\n\n\nCalculation of Minimax Rule: The lower boundary connects \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\). Equation of line: \\(R_2 - 0 = \\frac{0.5 - 0}{0 - 1} (R_1 - 1) \\Rightarrow R_2 = -0.5(R_1 - 1)\\). For Minimax, set \\(R_1 = R_2 = R\\). \\(R = -0.5R + 0.5 \\Rightarrow 1.5R = 0.5 \\Rightarrow R = 1/3\\). The minimax rule is a randomized combination: \\(d^* = \\frac{2}{3}d_3 + \\frac{1}{3}d_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes",
    "href": "decision.html#theorems-relating-minimax-and-bayes",
    "title": "2  Decision Theory",
    "section": "2.7 Theorems relating Minimax and Bayes",
    "text": "2.7 Theorems relating Minimax and Bayes\nTheorem 2.1: If a sequence of Bayes rules \\(\\delta_n\\) has Bayes risk converging to \\(C\\), and \\(R(\\theta, \\delta_0) \\le C\\) for all \\(\\theta\\), then \\(\\delta_0\\) is Minimax.\nTheorem 2.2 (Equalizer Rule): An Extended Bayes rule that is an equalizer rule (constant risk across all \\(\\theta\\)) must be Minimax.\nTheorem 2.3: Assume that the parameter space \\(\\Theta\\) is finite, and that the prior \\(\\pi(\\theta)\\) gives positive probability to each \\(\\theta_i\\). Then, a Bayes rule with respect to \\(\\pi\\) is admissible.\nTheorem 2.4: If a Bayes rule is unique, it is admissible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-decision-theory",
    "href": "decision.html#formulation-of-decision-theory",
    "title": "2  Decision Theory",
    "section": "",
    "text": "Parameter Space (\\(\\Theta\\)): The set of all possible states of nature or values that the parameter can take. \\(\\theta \\in \\Theta\\) (e.g., mean, variance).\nSample Space (\\(\\mathcal{X}\\)): The space where the data \\(X\\) lies. Example: \\(X = (X_1, X_2, \\dots, X_n)\\) where \\(X_i \\in \\mathbb{R}\\). So \\(\\mathcal{X} \\in \\mathbb{R}^n\\).\nFamily of Probability Distributions: \\(\\{P_\\theta(x) : \\theta \\in \\Theta\\}\\). This describes how likely we are to see the data \\(X\\) given a specific parameter \\(\\theta\\).\n\nIf \\(X\\) is continuous: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Density Function).\nIf \\(X\\) is discrete: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Mass Function).\n\nAction Space (\\(\\mathcal{A}\\)): The set of all actions or decisions available to the experimenter.\nLoss Function: \\(L: \\Theta \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\). \\(L(\\theta, a)\\) specifies the loss incurred if the true parameter is \\(\\theta\\) and we take action \\(a\\). Generally, \\(L(\\theta, a) \\ge 0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "3  Bayesian Methods",
    "section": "",
    "text": "3.1 Fundamental Elements of Bayesian Inference\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#examples",
    "href": "bayesian.html#examples",
    "title": "3  Bayesian Inference",
    "section": "3.2 Examples",
    "text": "3.2 Examples\n\n3.2.1 1. Binomial-Beta\n\n\\(X|\\theta \\sim \\text{Bin}(n, \\theta) \\Rightarrow f(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\\)\nPrior \\(\\theta \\sim \\text{Beta}(a, b) \\Rightarrow \\pi(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\)\n\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x} = \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nSo, \\(\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\\).\nMoments:\n\nMean: \\(E(\\theta|x) = \\frac{a+x}{a+b+n} \\approx \\frac{x}{n}\\) (for small \\(n\\))\nVariance: \\(\\text{Var}(\\theta|x) = \\frac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}\\)\n\n\n\n3.2.2 2. Normal-Normal (Known Variance)\n\n\\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is known.\nPrior \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\n\nLet \\(\\tau_0 = 1/\\sigma_0^2\\) (prior precision), \\(\\tau = 1/\\sigma^2\\) (data precision). The posterior precision is \\(\\tau_1 = \\tau_0 + n\\tau\\).\nPosterior: \\[\n\\mu|x \\sim N\\left( \\frac{\\tau_0 \\mu_0 + n\\tau \\bar{x}}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nThis shows the posterior mean is a weighted average of the prior mean and the sample mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "href": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "title": "3  Bayesian Inference",
    "section": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)",
    "text": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)\nMinimizing \\(E_{\\theta|x}[(\\theta - d)^2]\\) leads to: \\[\nd(x) = E(\\theta|x) \\quad \\text{(Posterior Mean)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "href": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "title": "3  Bayesian Inference",
    "section": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)",
    "text": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) leads to: \\[\n\\int_{-\\infty}^d \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = 0.5\n\\] So, \\(d(x) = \\text{Median of } \\pi(\\theta|x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#loss-hypothesis-testing",
    "href": "bayesian.html#loss-hypothesis-testing",
    "title": "3  Bayesian Inference",
    "section": "4.3 3. 0-1 Loss (Hypothesis Testing)",
    "text": "4.3 3. 0-1 Loss (Hypothesis Testing)\n\nLoss is 1 if error, 0 if correct.\nTesting \\(\\Theta_0\\) vs \\(\\Theta_1\\).\nBayes Rule: Choose class with highest posterior probability.\n\nReject \\(H_0\\) if \\(P(\\theta \\in \\Theta_1 | x) &gt; P(\\theta \\in \\Theta_0 | x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#interval-estimation",
    "href": "bayesian.html#interval-estimation",
    "title": "3  Bayesian Inference",
    "section": "4.4 4. Interval Estimation",
    "text": "4.4 4. Interval Estimation\nWe want an interval \\(A = (d-\\delta, d+\\delta)\\) minimizing risk (maximizing coverage probability \\(1-\\alpha\\)).\nHighest Posterior Density (HPD) Interval: The set \\(C = \\{ \\theta : \\pi(\\theta|x) \\ge k \\}\\) where \\(P(\\theta \\in C|x) = 1-\\alpha\\). This is the shortest interval for a given confidence level if the posterior is unimodal.\n\n\nCode\nlibrary(ggplot2)\n\nx &lt;- seq(0, 15, length.out = 1000)\ny &lt;- dgamma(x, shape = 3, rate = 0.5)\ndf &lt;- data.frame(x = x, y = y)\n\n# Approximate HPD cutoff (visual)\nhpd_level &lt;- 0.05\ncutoff &lt;- 0.08 # Chosen for visual representation of the cut\n\nggplot(df, aes(x, y)) +\n  geom_line(size = 1) +\n  geom_area(data = subset(df, y &gt; cutoff), fill = \"skyblue\", alpha = 0.5) +\n  geom_hline(yintercept = cutoff, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = 10, y = cutoff + 0.02, label = \"HPD Cutoff line\", color = \"red\") +\n  labs(title = \"Highest Posterior Density (HPD) Interval\", \n       subtitle = \"Points with density higher than the red line form the HPD set\",\n       x = \"Theta\", y = \"Posterior Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of Highest Posterior Density (HPD) Interval vs Equi-tailed Interval on a skewed posterior.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#baseball-example-efron-morris",
    "href": "bayesian.html#baseball-example-efron-morris",
    "title": "3  Bayesian Methods",
    "section": "3.13 3.8 Baseball Example (Efron & Morris)",
    "text": "3.13 3.8 Baseball Example (Efron & Morris)\nWe illustrate Stein estimation using baseball batting averages. Let \\(y_i\\) be the number of hits for player \\(i\\) in their first \\(n=45\\) at-bats. Let \\(\\hat{p}_i = y_i/n\\) be the observed average.\nTo apply the Normal model, we use a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\] Under this transformation, \\(X_i \\approx N(\\mu_i, 1)\\).\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value \\(\\mu_0\\)). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages \\(\\hat{p}_i\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes",
    "href": "bayesian.html#empirical-bayes",
    "title": "3  Bayesian Methods",
    "section": "3.4 Empirical Bayes",
    "text": "3.4 Empirical Bayes\nThe James-Stein estimator provides a natural entry point into the concept of Empirical Bayes (EB). While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.\n\n3.4.1 The General Empirical Bayes Framework\nIn a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to “learn” the prior.\nThe workflow typically follows these steps:\n\nHierarchical Model: We assume the data \\(X\\) comes from a distribution \\(f(x|\\theta)\\), and the parameter \\(\\theta\\) comes from a prior \\(\\pi(\\theta|\\eta)\\) controlled by hyperparameters \\(\\eta\\).\nMarginal Likelihood (Evidence): We integrate out the parameter \\(\\theta\\) to obtain the marginal distribution of the data given the hyperparameters: \\[m(x|\\eta) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\\]\nEstimation of Hyperparameters: Instead of fixing \\(\\eta\\), we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments: \\[\\hat{\\eta} = \\underset{\\eta}{\\arg\\max} \\ m(x|\\eta)\\]\nPosterior Inference: We proceed with standard Bayesian inference, but we substitute the estimated estimate \\(\\hat{\\eta}\\) into the posterior: \\[\\pi(\\theta|x, \\hat{\\eta}) \\propto f(x|\\theta) \\pi(\\theta|\\hat{\\eta})\\]\n\nDiscussion:\n\n“Borrowing Strength”: EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.\nThe Critique: A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate \\(\\theta\\)) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a “hyperprior” on \\(\\eta\\) and integrate it out.\n\n\n\n3.4.2 Deriving James-Stein as Empirical Bayes\nWe can derive the James-Stein rule explicitly using this framework.\nModel:\n\nLikelihood: \\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\). Here, \\(\\tau^2\\) is the unknown hyperparameter.\n\nStep 1: The Ideal Bayes Estimator\nIf we knew \\(\\tau^2\\), the posterior distribution of \\(\\mu_i\\) would be Normal with mean: \\[E(\\mu_i|x_i, \\tau^2) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\] We define the shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\).\nStep 2: Marginal Estimation\nSince \\(\\mu_i\\) and \\(X_i-\\mu_i\\) are independent normals, the marginal distribution of the data is: \\[X_i \\sim N(0, 1+\\tau^2)\\] Consequently, the sum of squares \\(S = ||X||^2 = \\sum X_i^2\\) follows a scaled Chi-squared distribution: \\[S \\sim (1+\\tau^2) \\chi^2_p\\]\nStep 3: Estimating the Shrinkage Factor\nWe need to estimate \\(B = \\frac{1}{1+\\tau^2}\\). Note that the expected value of an inverse Chi-square variable is \\(E[1/\\chi^2_p] = \\frac{1}{p-2}\\). Therefore: \\[E \\left[ \\frac{p-2}{S} \\right] = \\frac{p-2}{1+\\tau^2} E\\left[\\frac{1}{\\chi^2_p}\\right] = \\frac{p-2}{1+\\tau^2} \\cdot \\frac{1}{p-2} = \\frac{1}{1+\\tau^2} = B\\]\nThus, \\(\\hat{B} = \\frac{p-2}{||X||^2}\\) is an unbiased estimator of the optimal shrinkage factor.\nStep 4: The Empirical Bayes Rule\nPlugging \\(\\hat{B}\\) into the ideal Bayes estimator recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\hat{B} \\right) X = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-models",
    "href": "bayesian.html#hierarchical-models",
    "title": "3  Bayesian Inference",
    "section": "7.2 Hierarchical Models",
    "text": "7.2 Hierarchical Models\nWe assume a multistage structure:\n\nData model: \\(X|\\theta \\sim f(x|\\theta)\\)\nParameter model: \\(\\theta|\\lambda \\sim \\pi(\\theta|\\lambda)\\)\nHyperparameter model: \\(\\lambda \\sim h(\\lambda)\\)\n\nComputation: Since analytical solutions are often impossible, we use Markov Chain Monte Carlo (MCMC).\n\n7.2.1 Gibbs Sampling\nTo sample from the joint posterior \\(f(\\theta, \\lambda | x)\\), we sample iteratively from the full conditional distributions:\n\nDraw \\(\\theta^{(k+1)} \\sim f(\\theta | \\lambda^{(k)}, x)\\)\nDraw \\(\\lambda^{(k+1)} \\sim f(\\lambda | \\theta^{(k+1)}, x)\\)\n\n\n\n7.2.2 Metropolis-Hastings\nIf a conditional distribution is hard to sample from directly:\n\nPropose \\(\\theta^*\\) from a proposal density \\(q(\\theta^* | \\theta^{(t)})\\).\nCalculate acceptance ratio \\(\\alpha = \\min \\left( 1, \\frac{f(\\theta^*|x)q(\\theta^{(t)}|\\theta^*)}{f(\\theta^{(t)}|x)q(\\theta^*|\\theta^{(t)})} \\right)\\).\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "decision.html#examples-of-decision-problems",
    "href": "decision.html#examples-of-decision-problems",
    "title": "2  Decision Theory",
    "section": "2.3 Examples of Decision Problems",
    "text": "2.3 Examples of Decision Problems\n\n2.3.1 Example 1: Hypothesis Testing\nWe want to test \\(H_0\\) vs \\(H_1\\).\n\nAction Space: \\(\\mathcal{A} = \\{0, 1\\}\\) (0=“Accept \\(H_0\\)”, 1=“Reject \\(H_0\\)”).\nLoss Function (0-1 Loss): 0 if correct, 1 if wrong.\nRisk Function:\n\nIf \\(\\theta \\in H_0\\): \\(R(\\theta, d) = P(\\text{Type I Error})\\).\nIf \\(\\theta \\in H_1\\): \\(R(\\theta, d) = P(\\text{Type II Error})\\).\n\n\n\n\n2.3.2 Example 2: Point Estimation\nWe want to estimate a parameter \\(\\theta\\).\n\nAction Space: \\(\\mathcal{A} = \\Theta\\).\nLoss Function (Squared Error): \\(L(\\theta, a) = (\\theta - a)^2\\).\nRisk Function (MSE): \\(R(\\theta, d) = \\text{Var}(\\bar{x}) + \\text{Bias}^2\\).\n\n\n\n2.3.3 Example 3: Interval Estimation\nWe want to estimate a range for the parameter.\n\nAction Space: \\(\\mathcal{A} = \\{(l, u) : l \\in \\mathbb{R}, u \\in \\mathbb{R}, l \\le u\\}\\).\n\n\n\n2.3.4 Example 4: The Duchess and the Emerald Necklace\nScenario: You are the Duchess of Omnium. You have two necklaces: a priceless Real one and a valueless Imitation. They are indistinguishable to you. One is in the Left Drawer (Box 1), the other is in the Right Drawer (Box 2).\nThe Data (Great Aunt): You consult your Great Aunt. She inspects the Left Drawer first, then the Right.\n\nIf the Real necklace is in the Left (\\(\\theta=1\\)): She identifies it correctly. (Infallible).\nIf the Real necklace is in the Right (\\(\\theta=2\\)): She sees the fake first, gets confused, and guesses randomly (\\(50/50\\)).\n\n\n2.3.4.1 Formulation\n\nParameter Space: \\(\\Theta = \\{1, 2\\}\\) (1=Real Left, 2=Real Right).\nAction Space: \\(\\mathcal{A} = \\{1, 2\\}\\) (1=Wear Left, 2=Wear Right).\nLoss Function: 0 if correct, 1 if wrong.\n\n\n\n2.3.4.2 Risk Calculation for Deterministic Rules\nWe consider four deterministic rules \\(d(X)\\). We calculate the risk (\\(R_1\\) for \\(\\theta=1\\) and \\(R_2\\) for \\(\\theta=2\\)) for each.\nRule \\(d_1\\) (Always Left)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 1\\)\n\n\n\nRule \\(d_2\\) (Always Right)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0\\)\n\n\n\nRule \\(d_3\\) (Follow Aunt)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)\n\n\n\nRule \\(d_4\\) (Do Opposite)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#example-the-duchess-and-the-emerald-necklace",
    "href": "decision.html#example-the-duchess-and-the-emerald-necklace",
    "title": "2  Decision Theory",
    "section": "2.5 Example: The Duchess and the Emerald Necklace",
    "text": "2.5 Example: The Duchess and the Emerald Necklace\nScenario: You are the Duchess of Omnium. You have two necklaces: a priceless Real one and a valueless Imitation. They are indistinguishable to you. One is in the Left Drawer (Box 1), the other is in the Right Drawer (Box 2).\nThe Data (Great Aunt): You consult your Great Aunt. She inspects the Left Drawer first, then the Right.\n\nIf the Real necklace is in the Left (\\(\\theta=1\\)): She identifies it correctly. (Infallible).\nIf the Real necklace is in the Right (\\(\\theta=2\\)): She sees the fake first, gets confused, and guesses randomly (\\(50/50\\)).\n\n\n2.5.1 Formulation\n\nParameter Space: \\(\\Theta = \\{1, 2\\}\\) (1=Real Left, 2=Real Right).\nAction Space: \\(\\mathcal{A} = \\{1, 2\\}\\) (1=Wear Left, 2=Wear Right).\nData: \\(X \\in \\{1, 2\\}\\) (Aunt says Left/Right).\nLoss Function: 0 if correct, 1 if wrong.\n\n\n\n2.5.2 Probability of Data Given Parameter (\\(P_\\theta(X)\\))\n\n\n\n\nState \\(\\theta=1\\)\n\nState \\(\\theta=2\\)\n\n\n\n\n\nOutcome\n\\(X=1\\)\n\\(X=2\\)\n\\(X=1\\)\n\\(X=2\\)\n\n\nProbability\n1\n0\n0.5\n0.5\n\n\n\n\n\n2.5.3 Decision Rules\nWe consider four deterministic rules \\(d(X)\\).\n\n\n\nRule\nDescription\nAction if \\(X=1\\)\nAction if \\(X=2\\)\n\n\n\n\n\\(d_1\\)\nAlways Left\n1\n1\n\n\n\\(d_2\\)\nAlways Right\n2\n2\n\n\n\\(d_3\\)\nFollow Aunt\n1\n2\n\n\n\\(d_4\\)\nDo Opposite\n2\n1\n\n\n\n\n\n2.5.4 Risk Calculation Tables\nFor each rule, we calculate the risk (\\(R_1\\) for \\(\\theta=1\\) and \\(R_2\\) for \\(\\theta=2\\)).\n\n2.5.4.1 Rule \\(d_1\\) (Always Left)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_1(X)\\)\n1\n1\n\n\n\n\nLoss \\(L(1, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n0\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_1(X)\\)\n1\n1\n\n\n\n\nLoss \\(L(2, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0.5\n0.5\n\\(R_2 = 1\\)\n\n\n\n\n\n2.5.4.2 Rule \\(d_2\\) (Always Right)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_2(X)\\)\n2\n2\n\n\n\n\nLoss \\(L(1, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_2(X)\\)\n2\n2\n\n\n\n\nLoss \\(L(2, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0\n0\n\\(R_2 = 0\\)\n\n\n\n\n\n2.5.4.3 Rule \\(d_3\\) (Follow Aunt)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_3(X)\\)\n1\n2\n\n\n\n\nLoss \\(L(1, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n0\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_3(X)\\)\n1\n2\n\n\n\n\nLoss \\(L(2, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0.5\n0\n\\(R_2 = 0.5\\)\n\n\n\n\n\n2.5.4.4 Rule \\(d_4\\) (Do Opposite)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_4(X)\\)\n2\n1\n\n\n\n\nLoss \\(L(1, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_4(X)\\)\n2\n1\n\n\n\n\nLoss \\(L(2, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0\n0.5\n\\(R_2 = 0.5\\)\n\n\n\n\n\n\n2.5.5 Application of Geometric Analysis to Necklace Problem\nWe plot the risks calculated above:\n\n\\(d_1\\): \\((0, 1)\\)\n\\(d_2\\): \\((1, 0)\\)\n\\(d_3\\): \\((0, 0.5)\\)\n\\(d_4\\): \\((1, 0.5)\\)\n\nFrom the plot below, we can see that the admissible boundary connects \\(d_3\\) and \\(d_2\\). The Minimax rule is a randomized mixture of these two.\n\n\n\n\n\n\n\n\nFigure 2.2: Risk Set for the Necklace Problem. The Minimax rule is found at the intersection of y=x and the lower boundary. The orange and green lines represent Bayes risks for different priors.\n\n\n\n\n\n\n\n2.5.6 Finding the Minimax Weights\nThe minimax rule lies on the boundary connecting \\(d_3\\) and \\(d_2\\). \\[R(\\delta^*) = p R(d_3) + (1-p) R(d_2) = p \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} + (1-p) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-p \\\\ 0.5p \\end{pmatrix}\\] Setting \\(R_1 = R_2\\): \\[1-p = 0.5p \\implies 1 = 1.5p \\implies p = 2/3\\] The Minimax strategy is to choose \\(d_3\\) with probability \\(2/3\\) and \\(d_2\\) with probability \\(1/3\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-the-decision-problem",
    "href": "decision.html#formulation-of-the-decision-problem",
    "title": "2  Decision Theory",
    "section": "2.5 Formulation of the Decision Problem",
    "text": "2.5 Formulation of the Decision Problem\n\n2.5.1 Parameter Space (\\(\\Theta\\))\nThe set of possible states of nature regarding the location of the real necklace:\n\n\\(\\theta_1\\): The Real necklace is in the Left Drawer.\n\\(\\theta_2\\): The Real necklace is in the Right Drawer.\n\n\n\n2.5.2 Action Space (\\(\\mathcal{A}\\))\nThe set of possible actions available to the Duchess:\n\n\\(a_1\\): Wear the Left necklace.\n\\(a_2\\): Wear the Right necklace.\n\n\n\n2.5.3 The Data (\\(X\\))\nThe data consists of the Great Aunt’s judgment after inspecting the necklaces:\n\n\\(X \\in \\{1, 2\\}\\)\n\\(X=1\\): Aunt says “Left is Real”.\n\\(X=2\\): Aunt says “Right is Real”.\n\n\n\n2.5.4 Probability of Data Given Parameter (\\(P_\\theta(X)\\))\nThe probability of the Aunt’s advice changes depending on the true state of nature (\\(\\theta\\)).\n\n\n\nState\n\\(P(X=1 \\mid \\theta)\\)\n\\(P(X=2 \\mid \\theta)\\)\n\n\n\n\n\\(\\theta_1\\) (Real Left)\n1\n0\n\n\n\\(\\theta_2\\) (Real Right)\n0.5\n0.5\n\n\n\n\n\n2.5.5 Loss Function (\\(L\\))\nThe loss is defined as 0 for a correct choice and 1 (representing £1M) for an incorrect choice.\n\n\n\nState  Action\n\\(a_1\\) (Wear Left)\n\\(a_2\\) (Wear Right)\n\n\n\n\n\\(\\theta_1\\) (Real Left)\n0\n1\n\n\n\\(\\theta_2\\) (Real Right)\n1\n0\n\n\n\n\n\n2.5.6 Decision Rules\nThere are four possible deterministic decision rules (\\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\)).\n\n\n\n\n\n\n\n\n\nRule\nDescription\nAction if \\(X=1\\) (Left)\nAction if \\(X=2\\) (Right)\n\n\n\n\n\\(d_1\\)\nAlways Left\n\\(a_1\\)\n\\(a_1\\)\n\n\n\\(d_2\\)\nAlways Right\n\\(a_2\\)\n\\(a_2\\)\n\n\n\\(d_3\\)\nFollow Aunt\n\\(a_1\\)\n\\(a_2\\)\n\n\n\\(d_4\\)\nDo Opposite\n\\(a_2\\)\n\\(a_1\\)\n\n\n\n\n\n2.5.7 Risk Calculation\nBelow are the risk calculation tables for each decision rule.\n\n2.5.7.1 Rule \\(d_1\\) (Always Left)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n0\n0\n\\(R(\\theta_1) = (0 \\times 1) + (0 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 0\\)\n\n\n\\(\\theta_2\\)\nLoss\n1\n1\n\\(R(\\theta_2) = (1 \\times 0.5) + (1 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 1\\)\n\n\n\n\n\n2.5.7.2 Rule \\(d_2\\) (Always Right)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n1\n1\n\\(R(\\theta_1) = (1 \\times 1) + (1 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 1\\)\n\n\n\\(\\theta_2\\)\nLoss\n0\n0\n\\(R(\\theta_2) = (0 \\times 0.5) + (0 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0\\)\n\n\n\n\n\n2.5.7.3 Rule \\(d_3\\) (Follow Aunt)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n0\n1\n\\(R(\\theta_1) = (0 \\times 1) + (1 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 0\\)\n\n\n\\(\\theta_2\\)\nLoss\n1\n0\n\\(R(\\theta_2) = (1 \\times 0.5) + (0 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0.5\\)\n\n\n\n\n\n2.5.7.4 Rule \\(d_4\\) (Do Opposite)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n1\n0\n\\(R(\\theta_1) = (1 \\times 1) + (0 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 1\\)\n\n\n\\(\\theta_2\\)\nLoss\n0\n1\n\\(R(\\theta_2) = (0 \\times 0.5) + (1 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0.5\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#geometric-interpretation-and-principles",
    "href": "decision.html#geometric-interpretation-and-principles",
    "title": "2  Decision Theory",
    "section": "2.4 Geometric Interpretation and Principles",
    "text": "2.4 Geometric Interpretation and Principles\nWhen the parameter space is finite (e.g., \\(\\Theta = \\{1, 2\\}\\)), the risk function \\(R(\\theta, d)\\) can be represented as a point in \\(\\mathbb{R}^2\\) with coordinates \\((R_1, R_2) = (R(\\theta_1, d), R(\\theta_2, d))\\). This geometric perspective allows us to visualize fundamental concepts like Admissibility, Minimax, and Bayes rules.\n\n2.4.1 The Risk Set (\\(S\\))\nThe Risk Set \\(S\\) represents the collection of risk vectors for all possible decision rules.\n\nDeterministic Rules: These form the vertices (corners) of the set.\nRandomized Rules: If we mix two rules \\(d_1\\) and \\(d_2\\) with probabilities \\(p\\) and \\((1-p)\\), the resulting risk lies on the straight line segment connecting \\(R(d_1)\\) and \\(R(d_2)\\).\nConvexity: Because we can randomize between any rules, the Risk Set \\(S\\) is the convex hull of the deterministic rules. It typically forms a polygon filled with all possible randomized strategies.\n\n\n\n2.4.2 Admissibility (The Efficient Frontier)\nWe always prefer a lower risk.\n\nA rule \\(d\\) dominates rule \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\). Geometrically, this means \\(d\\) is to the “south-west” of \\(d'\\).\nA rule is Admissible if it is not dominated.\nThe set of admissible rules forms the lower-left boundary of the Risk Set \\(S\\). This is often called the “efficient frontier.”\n\n\n\n2.4.3 Finding the Minimax Rule\nThe Minimax principle seeks to minimize the maximum possible risk: \\(\\min_d [\\max(R_1, R_2)]\\).\n\nGeometrically, points of constant maximum risk (\\(\\max(R_1, R_2) = k\\)) form “L-shaped” right angles centered on the line \\(R_1 = R_2\\).\nTo find the Minimax rule, we find the intersection of the Risk Set \\(S\\) with the \\(45^\\circ\\) line \\(y = x\\) (\\(R_1 = R_2\\)).\nThe intersection point on the lower boundary of \\(S\\) is the Minimax rule.\n\n\n\n2.4.4 Finding the Bayes Rule\nA Bayes rule minimizes the weighted sum of risks for a given prior \\(\\pi = (\\pi_1, \\pi_2)\\).\n\nWe minimize Bayes Risk \\(r(\\pi, d) = \\pi_1 R_1 + \\pi_2 R_2\\).\nThe equation \\(\\pi_1 R_1 + \\pi_2 R_2 = c\\) defines a family of parallel lines with slope \\(m = -\\frac{\\pi_1}{\\pi_2}\\).\nTo find the Bayes rule, imagine taking a line with slope \\(-\\frac{\\pi_1}{\\pi_2}\\) and moving it from the origin outward until it just touches the Risk Set \\(S\\).\nThe point(s) of tangency are the Bayes rules for that prior.\n\n\n\n\n\n\n\n\n\nFigure 2.1: Geometric Representation of Decision Principles. The gray polygon is the Risk Set. The blue line segment represents the Admissible Rules. The red point is the Minimax Rule (intersection with R1=R2). The green point is a Bayes Rule, found by the tangent line determined by the prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#principles-for-choosing-a-decision-rule",
    "href": "decision.html#principles-for-choosing-a-decision-rule",
    "title": "2  Decision Theory",
    "section": "2.4 Principles for Choosing a Decision Rule",
    "text": "2.4 Principles for Choosing a Decision Rule\nSince no single rule minimizes risk for all \\(\\theta\\), we rely on several principles to order and select decision rules.\n\n2.4.1 Admissibility\nA decision rule \\(d\\) is admissible if it is not “dominated” by any other rule.\n\nDomination: A rule \\(d\\) dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nInadmissibility: If a rule is dominated, it is inadmissible and can be discarded (we can do better or equal in every possible state).\n\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Domination: Rule A (Red) is inadmissible because Rule B (Blue) has lower risk for all values of theta.\n\n\n\n\n\n\n\n2.4.2 Minimax Principle\nThe Minimax principle is a conservative approach that guards against the worst-case scenario. It selects the rule that minimizes the maximum risk. \\[ \\min_{d} \\left[ \\sup_{\\theta} R(\\theta, d) \\right] \\]\nIn the plot below, while Rule B has lower risk in the center, it has a very high maximum risk. Rule A is “flatter” and has a lower maximum value, making it the Minimax choice.\n\n\n\n\n\n\n\n\nFigure 2.2: Illustration of Minimax: Rule A has a lower peak risk than Rule B, making Rule A the Minimax choice.\n\n\n\n\n\n\n\n2.4.3 Bayes Decision Rules\nThe Bayes principle incorporates prior knowledge. If we assign a probability distribution (prior) \\(\\pi(\\theta)\\) to the parameter, we can calculate the Bayes Risk, which is the weighted average of the risk function. We choose the rule that minimizes this average. \\[ r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#geometric-interpretation-risk-sets",
    "href": "decision.html#geometric-interpretation-risk-sets",
    "title": "2  Decision Theory",
    "section": "2.5 Geometric Interpretation (Risk Sets)",
    "text": "2.5 Geometric Interpretation (Risk Sets)\nFor finite parameter spaces (e.g., \\(\\Theta = \\{1, 2\\}\\)), we can visualize the problem in 2D space where the axes are \\(R_1 = R(\\theta_1)\\) and \\(R_2 = R(\\theta_2)\\).\n\n2.5.1 The Risk Set (\\(S\\))\nThe set of all possible risk vectors is called the Risk Set \\(S\\). * Deterministic Rules: These are the vertices of the set. * Randomized Rules: By choosing rule \\(d_i\\) with probability \\(p\\) and \\(d_j\\) with probability \\(1-p\\), we can achieve any risk on the line segment connecting them. * Convexity: The Risk Set is the convex hull of the deterministic rules.\n\n\n2.5.2 Visualizing Admissibility\nThe admissible rules lie on the lower-left boundary of the set. Any point to the “north-east” of another point is dominated (inadmissible).\n\n\n2.5.3 Visualizing Minimax\nThe Minimax rule is found by intersecting the Risk Set with the line \\(y=x\\) (\\(R_1 = R_2\\)). * We look for the point in \\(S\\) that touches the \\(45^\\circ\\) line at the lowest value. * If the set is entirely below the line, we minimize \\(R_2\\). If entirely above, we minimize \\(R_1\\).\n\n\n2.5.4 Visualizing Bayes Rules\nA Bayes rule minimizes \\(\\pi_1 R_1 + \\pi_2 R_2 = k\\). This equation represents a line with slope \\(m = -\\pi_1 / \\pi_2\\). * To find the Bayes rule, we find the tangent line to the Risk Set \\(S\\) with slope \\(-\\pi_1 / \\pi_2\\).\n\n\n\n\n\n\n\n\nFigure 2.3: Geometric Interpretation: The gray polygon is the Risk Set S. The blue boundary represents admissible rules. The red point is the Minimax rule. The green line represents a Bayes rule for a specific prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "href": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "title": "2  Decision Theory",
    "section": "2.6 Revisiting the Necklace Example: Geometric Solution",
    "text": "2.6 Revisiting the Necklace Example: Geometric Solution\nWe now apply the geometric interpretation to the Necklace problem using the risks calculated in Section 2.3.4.\n\n\\(d_1\\): \\((0, 1)\\)\n\\(d_2\\): \\((1, 0)\\)\n\\(d_3\\): \\((0, 0.5)\\)\n\\(d_4\\): \\((1, 0.5)\\)\n\n\n2.6.1 Analysis\n\nAdmissibility:\n\n\\(d_4\\) has risk \\((1, 0.5)\\). \\(d_3\\) has risk \\((0, 0.5)\\). Since \\(0 &lt; 1\\), \\(d_3\\) strictly dominates \\(d_4\\). Thus \\(d_4\\) is inadmissible.\nThe efficient frontier connects \\(d_3\\) and \\(d_2\\).\n\nMinimax Solution: The Minimax rule lies on the segment connecting \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\).\n\nLet the randomized rule be \\(\\delta^* = p d_3 + (1-p) d_2\\).\n\\(R(\\delta^*) = p \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} + (1-p) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-p \\\\ 0.5p \\end{pmatrix}\\).\nSet \\(R_1 = R_2\\): \\(1-p = 0.5p \\Rightarrow 1 = 1.5p \\Rightarrow p = 2/3\\).\nResult: The Minimax rule is to choose \\(d_3\\) with probability \\(2/3\\) and \\(d_2\\) with probability \\(1/3\\).\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Necklace Problem Solution. The Minimax rule (red diamond) is the specific randomized combination of d3 and d2 that equalizes the risk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "href": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "title": "2  Decision Theory",
    "section": "2.7 Theorems Relating Minimax and Bayes Rules",
    "text": "2.7 Theorems Relating Minimax and Bayes Rules\nIn practice, finding a Minimax rule directly is mathematically difficult. A standard strategy is to “guess” a Least Favorable Prior \\(\\pi\\)—defined as the prior distribution that maximizes the minimum Bayes risk (i.e., the prior against which it is hardest to defend)—find the corresponding Bayes rule, and then check if it satisfies specific conditions to confirm it is Minimax.\n\n2.7.1 Equalizer Rules\n\nTheorem 2.1 (The Equalizer Rule Strategy) If \\(\\delta^*\\) is a Bayes rule with respect to some prior \\(\\pi\\), and if \\(\\delta^*\\) is an equalizer rule (meaning \\(R(\\theta, \\delta^*) = C\\) for some constant \\(C\\) for all \\(\\theta \\in \\Theta\\)), then \\(\\delta^*\\) is Minimax.\n\n\nProof. \n\nBayes Risk Definition: Since \\(\\delta^*\\) is an equalizer rule with risk \\(C\\), its Bayes risk with respect to \\(\\pi\\) is: \\[r(\\pi, \\delta^*) = \\int_\\Theta R(\\theta, \\delta^*) \\pi(\\theta) d\\theta = \\int_\\Theta C \\pi(\\theta) d\\theta = C \\cdot 1 = C\\]\nMinimax Contradiction: Suppose, for the sake of contradiction, that \\(\\delta^*\\) is not Minimax. This implies there exists another rule \\(\\delta'\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta') &lt; \\sup_{\\theta} R(\\theta, \\delta^*)\\] Since \\(R(\\theta, \\delta^*) = C\\) for all \\(\\theta\\), the supremum is \\(C\\). Thus: \\[\\sup_{\\theta} R(\\theta, \\delta') &lt; C\\]\nInequality: This implies that for all \\(\\theta\\), \\(R(\\theta, \\delta') &lt; C\\).\nBayes Risk Comparison: Now, consider the Bayes risk of this alternative rule \\(\\delta'\\): \\[r(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta\\] Since \\(R(\\theta, \\delta') &lt; C\\) for all \\(\\theta\\), it follows that: \\[r(\\pi, \\delta') &lt; \\int_\\Theta C \\pi(\\theta) d\\theta = C\\]\nConclusion: We have established that \\(r(\\pi, \\delta') &lt; C\\). However, we established in step 1 that \\(r(\\pi, \\delta^*) = C\\). This yields \\(r(\\pi, \\delta') &lt; r(\\pi, \\delta^*)\\). This contradicts the assumption that \\(\\delta^*\\) is a Bayes rule (since a Bayes rule must minimize the Bayes risk). Therefore, no such \\(\\delta'\\) exists, and \\(\\delta^*\\) is Minimax. \\(\\blacksquare\\)\n\n\n\n\n2.7.2 Limits of Bayes Rules\nSometimes the Minimax rule corresponds to an “improper” prior (a prior that does not integrate to 1, like a uniform distribution on the real line). We approach these via a limiting sequence.\n\nTheorem 2.2 (Limits of Bayes Rules) Let \\(\\{\\delta_n\\}\\) be a sequence of Bayes rules with respect to priors \\(\\{\\pi_n\\}\\). Let \\(r(\\pi_n, \\delta_n)\\) be the associated Bayes risks. If there exists a rule \\(\\delta_0\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\] Then \\(\\delta_0\\) is Minimax.\n\n\nProof. \n\nDefine Limit: Let \\(V = \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\). We are given that \\(\\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\).\nContradiction Setup: Suppose \\(\\delta_0\\) is not Minimax. Then there exists a rule \\(\\delta^*\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta^*) &lt; \\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\] Let \\(\\sup_{\\theta} R(\\theta, \\delta^*) = V - \\epsilon\\) for some \\(\\epsilon &gt; 0\\).\nBayes Risk Bound: For any prior \\(\\pi_n\\), the Bayes risk of \\(\\delta^*\\) cannot exceed its maximum risk: \\[r(\\pi_n, \\delta^*) = \\int R(\\theta, \\delta^*) \\pi_n(\\theta) d\\theta \\le \\int (V - \\epsilon) \\pi_n(\\theta) d\\theta = V - \\epsilon\\]\nOptimality of \\(\\delta_n\\): Since \\(\\delta_n\\) is the Bayes rule for \\(\\pi_n\\), it minimizes Bayes risk. Thus: \\[r(\\pi_n, \\delta_n) \\le r(\\pi_n, \\delta^*)\\]\nCombining Inequalities: Combining steps 3 and 4: \\[r(\\pi_n, \\delta_n) \\le V - \\epsilon\\]\nTaking Limits: Taking the limit as \\(n \\to \\infty\\): \\[\\lim_{n \\to \\infty} r(\\pi_n, \\delta_n) \\le V - \\epsilon\\] \\[V \\le V - \\epsilon\\] This is a contradiction since \\(\\epsilon &gt; 0\\). Therefore, \\(\\delta_0\\) must be Minimax. \\(\\blacksquare\\)\n\n\n\n\n2.7.3 Admissibility of Bayes Rules\nBayes rules are generally good candidates for admissibility. If a rule is Bayes, it is likely efficient, provided the prior doesn’t ignore parts of the parameter space.\n\nTheorem 2.3 (Admissibility of Bayes Rules (Finite Support)) If the parameter space \\(\\Theta\\) is finite (or countable) and the prior \\(\\pi\\) assigns positive probability to every \\(\\theta \\in \\Theta\\) (i.e., \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\)), then any Bayes rule \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) that dominates it. By definition of domination:\n\n\\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\).\n\\(R(\\theta_k, \\delta') &lt; R(\\theta_k, \\delta_\\pi)\\) for at least one \\(\\theta_k\\).\n\nBayes Risk Difference: Consider the difference in Bayes risk: \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') = \\sum_{\\theta \\in \\Theta} \\pi(\\theta) [R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\]\nStrict Positivity:\n\nSince \\(\\delta'\\) dominates \\(\\delta_\\pi\\), each term \\([R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\) is non-negative (\\(\\ge 0\\)).\nAt \\(\\theta_k\\), the term is strictly positive (\\(&gt; 0\\)).\nWe assumed the prior has full support, so \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\).\n\nSummation: A sum of non-negative terms where at least one term is strictly positive must be strictly positive. \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') &gt; 0 \\implies r(\\pi, \\delta') &lt; r(\\pi, \\delta_\\pi)\\]\nConclusion: This contradicts the definition that \\(\\delta_\\pi\\) is a Bayes rule (which must minimize Bayes risk). Therefore, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)\n\n\n\n\n2.7.4 Admissibility of Unique Bayes Rules\nIf the Bayes rule is unique, we can drop the requirement that the parameter space be discrete or finite.\n\nTheorem 2.4 (Admissibility of Unique Bayes Rules) Let \\(\\delta_\\pi\\) be a Bayes rule with respect to \\(\\pi\\). If \\(\\delta_\\pi\\) is the unique Bayes rule (up to risk equivalence), then \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) such that: \\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\), with strict inequality for some set of \\(\\theta\\).\nBayes Risk Inequality: Taking the expectation with respect to \\(\\pi\\): \\[r(\\pi, \\delta') = \\int R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\int R(\\theta, \\delta_\\pi) \\pi(\\theta) d\\theta = r(\\pi, \\delta_\\pi)\\]\nMinimality: Since \\(\\delta_\\pi\\) is Bayes, it minimizes the risk, so \\(r(\\pi, \\delta_\\pi) \\le r(\\pi, \\delta')\\). Combining these gives \\(r(\\pi, \\delta') = r(\\pi, \\delta_\\pi)\\).\nUniqueness: This implies that \\(\\delta'\\) is also a Bayes rule. However, we assumed that \\(\\delta_\\pi\\) is the unique Bayes rule. Therefore, \\(\\delta'\\) must be equal to \\(\\delta_\\pi\\) (in terms of risk functions).\nConclusion: If \\(\\delta'\\) and \\(\\delta_\\pi\\) have identical risk functions, then \\(\\delta'\\) cannot strictly dominate \\(\\delta_\\pi\\). This contradicts the assumption of inadmissibility. Thus, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#risk-set-for-finite-parameter-space",
    "href": "decision.html#risk-set-for-finite-parameter-space",
    "title": "2  Decision Theory",
    "section": "2.5 Risk Set for Finite Parameter Space",
    "text": "2.5 Risk Set for Finite Parameter Space\nFor finite parameter spaces (e.g., \\(\\Theta = \\{1, 2\\}\\)), we can visualize the problem in 2D space where the axes are \\(R_1 = R(\\theta_1)\\) and \\(R_2 = R(\\theta_2)\\).\n\n2.5.1 The Risk Set (\\(S\\))\nThe set of all possible risk vectors is called the Risk Set \\(S\\).\n\nDeterministic Rules: These are the vertices of the set.\nRandomized Rules: By choosing rule \\(d_i\\) with probability \\(p\\) and \\(d_j\\) with probability \\(1-p\\), we can achieve any risk on the line segment connecting them.\nConvexity: The Risk Set is the convex hull of the deterministic rules.\n\n\n\n2.5.2 Visualizing Admissibility\nThe admissible rules lie on the lower-left boundary of the set. Any point to the “north-east” of another point is dominated (inadmissible).\n\n\n2.5.3 Visualizing Minimax\nThe Minimax rule is found by intersecting the Risk Set with the line \\(y=x\\) (\\(R_1 = R_2\\)).\n\nWe look for the point in \\(S\\) that touches the \\(45^\\circ\\) line at the lowest value.\nIf the set is entirely below the line, we minimize \\(R_2\\). If entirely above, we minimize \\(R_1\\).\n\n\n\n2.5.4 Visualizing Bayes Rules\nA Bayes rule minimizes \\(\\pi_1 R_1 + \\pi_2 R_2 = k\\). This equation represents a line with slope \\(m = -\\pi_1 / \\pi_2\\).\n\nTo find the Bayes rule, we find the tangent line to the Risk Set \\(S\\) with slope \\(-\\pi_1 / \\pi_2\\).\n\n\n\n\n\n\n\n\n\nFigure 2.3: Geometric Interpretation: The gray polygon is the Risk Set S. The blue boundary represents admissible rules. The red point is the Minimax rule. The green line represents a Bayes rule for a specific prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#the-likelihood-function",
    "href": "introstatinf.html#the-likelihood-function",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 The Likelihood Function",
    "text": "1.5 The Likelihood Function\nThe bridge between probability and statistics is the Likelihood Function.\n\nDefinition 1.2 (Likelihood Function) Let \\(f(x_1, \\dots, x_n; \\theta)\\) be the joint probability density (or mass) function of the data given the parameter \\(\\theta\\). When we view this function as a function of \\(\\theta\\) for fixed observed data \\(x_1, \\dots, x_n\\), we call it the likelihood function, denoted \\(L(\\theta)\\). \\[L(\\theta) = f(x_1, \\dots, x_n; \\theta)\\]\n\n\nExample: Lady Tasting Tea\nFor our Tea Tasting data, the likelihood is proportional to the Binomial probability: \\[L(\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}\\]\n\nn=10 (k=7)n=40 (k=28)\n\n\nHere, \\(L(\\theta) = \\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\).\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n120 \\(\\times 0^{7} \\times 1^{3}\\)\n0.0000\n\n\n0.2\n120 \\(\\times 0.2^{7} \\times 0.8^{3}\\)\n0.0008\n\n\n0.4\n120 \\(\\times 0.4^{7} \\times 0.6^{3}\\)\n0.0425\n\n\n0.6\n120 \\(\\times 0.6^{7} \\times 0.4^{3}\\)\n0.2150\n\n\n0.7\n120 \\(\\times 0.7^{7} \\times 0.3^{3}\\)\n0.2668 (Max)\n\n\n0.8\n120 \\(\\times 0.8^{7} \\times 0.2^{3}\\)\n0.2013\n\n\n1.0\n120 \\(\\times 1^{7} \\times 0^{3}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_small, k_small) * theta^k_small * (1 - theta)^(n_small-k_small) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_small/n_small, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_small/n_small, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_small/n_small), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_small, \", k=\", k_small, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.2: Likelihood Function (n= 10 )\n\n\n\n\n\n\n\nHere, \\(L(\\theta) = \\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\). Notice how the likelihood becomes narrower (more peaked) with more data, even though the peak remains at 0.7.\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n5.5868535^{9} \\(\\times 0^{28} \\times 1^{12}\\)\n0.0000\n\n\n0.2\n5.5868535^{9} \\(\\times 0.2^{28} \\times 0.8^{12}\\)\n0.0000\n\n\n0.4\n5.5868535^{9} \\(\\times 0.4^{28} \\times 0.6^{12}\\)\n0.0001\n\n\n0.6\n5.5868535^{9} \\(\\times 0.6^{28} \\times 0.4^{12}\\)\n0.0576\n\n\n0.7\n5.5868535^{9} \\(\\times 0.7^{28} \\times 0.3^{12}\\)\n0.1366 (Max)\n\n\n0.8\n5.5868535^{9} \\(\\times 0.8^{28} \\times 0.2^{12}\\)\n0.0443\n\n\n1.0\n5.5868535^{9} \\(\\times 1^{28} \\times 0^{12}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_large, k_large) * theta^k_large * (1 - theta)^(n_large-k_large) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_large/n_large, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_large/n_large, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_large/n_large), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_large, \", k=\", k_large, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.3: Likelihood Function (n= 40 )\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nIs an estimator like \\(\\bar x\\), which is called Maximum Likelihood Estimator (MLE), a good estimator in general?\nWhat do you discover from actually observing the two likelihood unctions of different sample size \\(n\\)?\nIs the likelihood function central to all inference problems?\nWhat are the essential ‘parameters’ of the likelihood function?\n\nThere are two primary frameworks for “How” to perform these inferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#frequentist-inference-fisher",
    "href": "introstatinf.html#frequentist-inference-fisher",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Frequentist Inference (Fisher)",
    "text": "1.6 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#bayesian-inference",
    "href": "introstatinf.html#bayesian-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.7 Bayesian Inference",
    "text": "1.7 Bayesian Inference\n\nConcept: \\(\\theta\\) is regarded as a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\nExample: Bayesian Analysis of the Lady Tasting Tea\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: \\(\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)\\)\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: \\(\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)\\).\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.7.1 Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not or general inference?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?\nPrior: How to choose our prior? What’s its effect on our inference?\nModel Comparison and Assessment: How to assess a Bayesian model?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#motivating-example-the-lady-tasting-tea",
    "href": "introstatinf.html#motivating-example-the-lady-tasting-tea",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 Motivating Example: The Lady Tasting Tea",
    "text": "1.3 Motivating Example: The Lady Tasting Tea\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare \\(n\\) cups of tea.\n\nRandom Variable: Let \\(X_i=1\\) if she identifies the cup correctly, and \\(0\\) otherwise.\nParameter: Let \\(\\theta\\) be the probability that she correctly identifies a cup.\nThe Data: Suppose we observe that she identifies 70% of cups correctly (\\(\\bar{x} = 0.7\\)).\n\n\nSmall Sample (n=10)Large Sample (n=40)\n\n\nWe observe 7 out of 10 correct (\\(k=7\\)). \\[\\bar{x} = 0.7\\]\n\n\nWe observe 28 out of 40 correct (\\(k=28\\)). \\[\\bar{x} = 0.7\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#paradigms-of-inference",
    "href": "introstatinf.html#paradigms-of-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Paradigms of Inference",
    "text": "1.6 Paradigms of Inference\nThere are two primary frameworks for “How” to perform these inferences.\n\n1.6.1 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\n1.6.1.1 Application: Frequentist Test of the Tea Lady\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1.2 Methodologies & Challenges (Frequentist)\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE Construction: How do we find the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for complex models where no closed-form solution exists?\nComparing Estimator Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? If exact derivation is impossible, how do we use Asymptotic Theory to prove \\(\\hat{\\theta} \\overset{d}{\\to} N(\\theta, I^{-1}(\\theta))\\)?\nConfidence Intervals: How do we construct Confidence Intervals for parameters in general multiparameter models?\nHypothesis Testing: How do we derive powerful tests (like the Likelihood Ratio Test) and determine their critical values?\n\n\n\n\n1.6.2 Bayesian Inference\n\nConcept: \\(\\theta\\) is a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\n1.6.2.1 Application: Bayesian Analysis of the Tea Lady\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: $(1+7, 1+3) = (8, 4)`.\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: $(1+28, 1+12) = (29, 13)`. The posterior is taller and narrower, indicating higher certainty.\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.2.2 Methodologies & Challenges (Bayesian)\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#key-questions-in-statistical-inference",
    "href": "introstatinf.html#key-questions-in-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Key Questions in Statistical Inference",
    "text": "1.4 Key Questions in Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "href": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 A Motivating Example: The Lady Tasting Tea",
    "text": "1.3 A Motivating Example: The Lady Tasting Tea\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare \\(n\\) cups of tea.\n\nRandom Variable: Let \\(X_i=1\\) if she identifies the cup correctly, and \\(0\\) otherwise.\nParameter: Let \\(\\theta\\) be the probability that she correctly identifies a cup.\nThe Data: Suppose we observe that she identifies 70% of cups correctly (\\(\\bar{x} = 0.7\\)), which is a summary of the observed vector of \\(x_i\\), for example,\n\n\\[x=(0,1,1,0, 1,1,0,1,1,1)\\]\n\nSmall Sample (n=10)Large Sample (n=40)\n\n\nWe observe 7 out of 10 correct (\\(k=7\\)). \\[\\bar{x} = 0.7\\]\n\n\nWe observe 28 out of 40 correct (\\(k=28\\)). \\[\\bar{x} = 0.7\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#questions-answered-with-statistical-inference",
    "href": "introstatinf.html#questions-answered-with-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Questions Answered with Statistical Inference",
    "text": "1.4 Questions Answered with Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nModel Assessment\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nExample: Can we use a reduced model? What level of complexity of \\(f(x; \\theta)\\) is necessary?\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#two-paradigms-of-inference",
    "href": "introstatinf.html#two-paradigms-of-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Two Paradigms of Inference",
    "text": "1.6 Two Paradigms of Inference\nThere are two primary frameworks for “How” to perform these inferences.\n\n1.6.1 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?\n\n\n\n\n1.6.2 Bayesian Inference\n\nConcept: \\(\\theta\\) is a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\nExample: Bayesian Analysis of the Lady Tasting Tea\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: \\(\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)\\)\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: \\(\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)\\).\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.2.1 Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not or general inference?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?\nPrior: How to choose our prior? What’s its effect on our inference?\nModel Comparison and Assessment: How to assess a Bayesian model?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "href": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Questions to Answer in Statistical Inference",
    "text": "1.4 Questions to Answer in Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nModel Assessment\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nExample: Can we use a reduced model? What level of complexity of \\(f(x; \\theta)\\) is necessary?\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#population-model-data-model",
    "href": "introstatinf.html#population-model-data-model",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\nAssumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). The assumption of identical distribution can be relaxed to regression settings in which the distributions of \\(x_i\\)’s are independent but dependent on covariate \\(x_i\\).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model: Normal) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).\n\n\nExample 1.2 (Parametric Model: Bernoulli) Consider a sequence of binary outcomes (e.g., Success/Failure) where each \\(X_i \\in \\{0, 1\\}\\). We assume \\(X_i \\sim \\text{Bernoulli}(\\theta)\\). The probability mass function is: \\[f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}\\] Here, the parameter space is \\(\\Theta = [0, 1]\\), where \\(\\theta\\) represents the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "href": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probabilistic Model vs. Statistical Inference",
    "text": "1.2 Probabilistic Model vs. Statistical Inference\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\). We can visualize this using a “shooting target” analogy:\n\n\\(\\theta\\) (The Center): The true, unknown bullseye location.\n\\(x\\) (The Shots): The observed holes on the target board.\nProbability (Deductive): The center \\(\\theta\\) is known. We predict where the shots \\(x\\) will land.\nStatistics (Inductive): The shots \\(x\\) are observed on the board. The center \\(\\theta\\) is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n\n\n\n\n\n\n\nFigure 1.1: Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#frequentist-inference",
    "href": "introstatinf.html#frequentist-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Frequentist Inference",
    "text": "1.6 Frequentist Inference\n\nConcept: \\(\\theta\\) is unknown but fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Statistical Inference",
    "section": "Audience",
    "text": "Audience\nThis course requires a strong command of multivariate calculus, alongside a rigorous foundation in intermediate probability theory including asymptotic theorey for probability. Students should also possess prior exposure to applied statistical methods and familiar with basic statistical concepts such as p-value and confidence internal.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "bayesian.html#fundamental-elements-of-bayesian-inference",
    "href": "bayesian.html#fundamental-elements-of-bayesian-inference",
    "title": "3  Bayesian Methods",
    "section": "",
    "text": "Definition 3.1 (Posterior Distribution) Suppose we have a parameter \\(\\theta\\) with a prior distribution denoted by \\(\\pi(\\theta)\\). If we observe data \\(x\\) drawn from a distribution with probability density function (pdf) \\(f(x; \\theta)\\), then the posterior density of \\(\\theta\\) given the data \\(x\\) is defined as:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{m(x)}\n\\]\nwhere \\(m(x)\\) is the marginal distribution (or marginal likelihood) of the data, calculated as: \\[\nm(x) = \\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta\n\\]\nIn this context, \\(m(x)\\) acts as a normalizing constant. Since it depends only on the data \\(x\\) and not on the parameter \\(\\theta\\), it ensures that the posterior density integrates to 1 but does not influence the shape of the posterior distribution.\nThus, we often state the proportional relationship:\n\\[\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n\\]\n\n\nExample 3.1 (Binomial-beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\nNumerical Illustration:\nSuppose we are estimating a probability \\(\\theta\\).\n\nPrior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (Mean = 0.5).\nData: 10 trials, 8 successes (\\(n=10, x=8\\)).\nPosterior: \\(\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)\\) (Mean \\(\\approx\\) 0.71).\n\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\nCode\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Prior: Beta(2, 2)\nprior &lt;- dbeta(theta, shape1 = 2, shape2 = 2)\n\n# Posterior: Beta(10, 4)\nposterior &lt;- dbeta(theta, shape1 = 10, shape2 = 4)\n\nplot(theta, posterior, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = \"Beta Prior vs Posterior\", ylim = c(0, max(c(prior, posterior))))\nlines(theta, prior, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,4)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 3.1: Prior vs Posterior for Beta-Binomial Example\n\n\n\n\n\n\n\nExample 3.2 (Normal-normal Conjugacy (known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nNumerical Illustration:\nSuppose we estimate a mean height \\(\\mu\\).\n\nKnown Variance: \\(\\sigma^2 = 100\\) (\\(\\tau = 0.01\\)).\nPrior: \\(\\mu \\sim N(175, 25)\\) (Precision \\(\\tau_0 = 0.04\\)).\nData: \\(n=10, \\bar{x}=180\\). (Total data precision \\(n\\tau = 0.1\\)).\nPosterior:\n\nPrecision \\(\\tau_1 = 0.04 + 0.1 = 0.14\\).\nVariance \\(\\sigma_1^2 \\approx 7.14\\).\nMean \\(\\mu_1 = \\frac{175(0.04) + 180(0.1)}{0.14} \\approx 178.6\\).\n\n\nThe plot below illustrates the prior (dashed) and posterior (solid) normal densities.\n\n\nCode\nmu_vals &lt;- seq(150, 200, length.out = 200)\n\n# Prior: N(175, 25) -&gt; SD = 5\nprior_norm &lt;- dnorm(mu_vals, mean = 175, sd = 5)\n\n# Posterior: N(178.6, 7.14) -&gt; SD = Sqrt(7.14) Approx 2.67\nposterior_norm &lt;- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))\n\nplot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Density\",\n     main = \"Normal Prior vs Posterior\",\n     ylim = c(0, max(c(prior_norm, posterior_norm))))\nlines(mu_vals, prior_norm, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior N(175, 25)\", \"Posterior N(178.6, 7.14)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 3.2: Prior vs Posterior for Normal-Normal Example\n\n\n\n\n\n\n\nExample 3.3 (Discrete Posterior Calculation) Consider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). The calculation of the posterior probabilities is summarized in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta=1\\)\n\\(\\theta=2\\)\n\\(\\theta=3\\)\nSum\n\n\n\n\nPrior \\(\\pi(\\theta)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1\\)\n\n\nLikelihood \\(\\pi(x=2|\\theta)\\)\n\\(0\\)\n\\(1/3\\)\n\\(1/4\\)\n-\n\n\nProduct \\(\\pi(\\theta)\\pi(x|\\theta)\\)\n\\(0\\)\n\\(1/9\\)\n\\(1/12\\)\n\\(7/36\\)\n\n\nPosterior \\(\\pi(\\theta|x)\\)\n\\(0\\)\n\\(4/7\\)\n\\(3/7\\)\n\\(1\\)\n\n\n\nThe marginal sum (evidence) is calculated as \\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\\). The posterior values are obtained by dividing the product row by this sum.\n\n\nExample 3.4 (Normal with Unknown Mean and Variance) Consider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is the product of the conditional and the marginal: \\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nDerivation of the Posterior:\nFirst, we write the likelihood in terms of the sufficient statistics \\(\\bar{x}\\) and \\(S_{xx} = \\sum (x_i - \\bar{x})^2\\): \\[\nL(\\mu, \\tau|x) \\propto \\tau^{n/2} \\exp\\left\\{ -\\frac{\\tau}{2} \\left[ S_{xx} + n(\\bar{x}-\\mu)^2 \\right] \\right\\}\n\\]\nMultiplying the prior by the likelihood gives the joint posterior: \\[\n\\begin{aligned}\n\\pi(\\mu, \\tau | x) &\\propto \\tau^{\\alpha - 1/2} e^{-\\beta\\tau} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2} \\cdot \\tau^{n/2} e^{-\\frac{\\tau}{2}S_{xx}} e^{-\\frac{n\\tau}{2}(\\mu-\\bar{x})^2} \\\\\n&\\propto \\tau^{\\alpha + n/2 - 1/2} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{1}{2}\\left( k(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 \\right) \\right] \\right\\}\n\\end{aligned}\n\\]\nNext, we complete the square for the terms involving \\(\\mu\\) inside the brackets. It can be shown that: \\[\nk(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 = (k+n)\\left(\\mu - \\frac{k\\nu+n\\bar{x}}{k+n}\\right)^2 + \\frac{nk}{n+k}(\\bar{x}-\\nu)^2\n\\]\nSubstituting this back into the joint density and grouping terms that do not depend on \\(\\mu\\): \\[\n\\pi(\\mu, \\tau | x) \\propto \\underbrace{\\tau^{\\alpha + n/2 - 1} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2 \\right] \\right\\}}_{\\text{Marginal of } \\tau} \\cdot \\underbrace{\\tau^{1/2} \\exp\\left\\{ -\\frac{(k+n)\\tau}{2} \\left( \\mu - \\frac{k\\nu+n\\bar{x}}{k+n} \\right)^2 \\right\\}}_{\\text{Conditional of } \\mu|\\tau}\n\\]\nResults:\nBy inspecting the factored equation above, we identify the updated parameters:\n\nMarginal Posterior of \\(\\tau\\): The first part corresponds to a Gamma kernel \\(\\tau^{\\alpha' - 1} e^{-\\beta'\\tau}\\). \\[\\tau|x \\sim \\text{Gamma}(\\alpha', \\beta')\\] where \\(\\alpha' = \\alpha + n/2\\) and \\(\\beta' = \\beta + \\frac{1}{2}\\sum(x_i-\\bar{x})^2 + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2\\).\nConditional Posterior of \\(\\mu\\): The second part corresponds to a Normal kernel with precision \\(k'\\tau\\). \\[\\mu|\\tau, x \\sim N(\\nu', 1/(k'\\tau))\\] where \\(k' = k + n\\) and \\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-1-binomial-beta-conjugacy",
    "href": "bayesian.html#example-1-binomial-beta-conjugacy",
    "title": "3  Bayesian Methods",
    "section": "3.2 Example 1: Binomial-Beta Conjugacy",
    "text": "3.2 Example 1: Binomial-Beta Conjugacy\nConsider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-2-normal-normal-conjugacy-known-variance",
    "href": "bayesian.html#example-2-normal-normal-conjugacy-known-variance",
    "title": "3  Bayesian Methods",
    "section": "3.3 Example 2: Normal-Normal Conjugacy (Known Variance)",
    "text": "3.3 Example 2: Normal-Normal Conjugacy (Known Variance)\nLet \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#discrete-example",
    "href": "bayesian.html#discrete-example",
    "title": "3  Bayesian Methods",
    "section": "3.4 3.2 Discrete Example",
    "text": "3.4 3.2 Discrete Example\nConsider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#normal-with-unknown-mean-and-variance",
    "href": "bayesian.html#normal-with-unknown-mean-and-variance",
    "title": "3  Bayesian Methods",
    "section": "3.2 Normal with Unknown Mean and Variance",
    "text": "3.2 Normal with Unknown Mean and Variance\nConsider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is:\n\\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nMultiplying by the likelihood leads to a posterior of the same form (Conjugate), with updated parameters:\n\n\\(\\alpha' = \\alpha + n/2\\)\n\\(k' = k + n\\)\n\\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\)\n\\(\\beta' = \\beta + \\frac{1}{2} \\frac{nk}{n+k}(\\bar{x}-\\nu)^2 + \\frac{1}{2}\\sum (x_i - \\bar{x})^2\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#decision-theory-and-bayes-rules",
    "href": "bayesian.html#decision-theory-and-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.2 Decision Theory and Bayes Rules",
    "text": "3.2 Decision Theory and Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) satisfies: \\[\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n\\]\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\nTherefore, to minimize the Bayes risk, one must choose \\(d(x)\\) to minimize the posterior expected loss for each \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#common-loss-functions-and-estimators",
    "href": "bayesian.html#common-loss-functions-and-estimators",
    "title": "3  Bayesian Methods",
    "section": "3.3 Common Loss Functions and Estimators",
    "text": "3.3 Common Loss Functions and Estimators\n\n\n3.3.1 Squared Error Loss (point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule is the posterior mean.\n\n\n\n\n3.3.2 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) requires solving:\n\\[\\int_{-\\infty}^{d} \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = \\frac{1}{2}\\]\nResult: The Bayes rule is the posterior median.\n\n\n\n\n3.3.3 Hypothesis Testing (0-1 Loss)\nTesting \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\).\n\\[L(\\theta, a) = \\begin{cases} 1 & \\text{if error} \\\\ 0 & \\text{if correct} \\end{cases}\\]\nThe Bayes rule selects the hypothesis with the higher posterior probability.\n\\[d(x) = 1 \\iff P(\\theta \\in \\Theta_1 | x) \\ge P(\\theta \\in \\Theta_0 | x)\\]\n\n\n\nDefinition 3.3 (Highest Posterior Density (HPD) Interval) In interval estimation, we prescribe a set \\(A = (d-\\delta, d+\\delta)\\) and minimize the loss associated with \\(\\theta\\) falling outside this interval.\nThe Bayes rule \\(d(x)\\) is the center of the interval with the highest probability coverage. This leads to the Highest Posterior Density (HPD) interval.\nIn practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the Equal-Tailed Interval, where we cut off \\(\\alpha/2\\) probability from each tail.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#minimax-estimation",
    "href": "bayesian.html#minimax-estimation",
    "title": "3  Bayesian Methods",
    "section": "3.4 Minimax Estimation",
    "text": "3.4 Minimax Estimation\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Theorem) If a Bayes rule \\(d^\\pi\\) has constant risk (i.e., \\(R(\\theta, d^\\pi) = c\\) for all \\(\\theta\\)), then \\(d^\\pi\\) is a minimax estimator.\n\n\n\n3.4.1 Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-binomial-minimax-estimator",
    "href": "bayesian.html#example-binomial-minimax-estimator",
    "title": "3  Bayesian Methods",
    "section": "3.9 Example: Binomial Minimax Estimator",
    "text": "3.9 Example: Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#stein-estimation-and-shrinkage",
    "href": "bayesian.html#stein-estimation-and-shrinkage",
    "title": "3  Bayesian Methods",
    "section": "3.5 Stein Estimation and Shrinkage",
    "text": "3.5 Stein Estimation and Shrinkage\nConsider estimating a multivariate mean vector \\(\\mu = (\\mu_1, \\dots, \\mu_p)\\) given independent observations \\(X_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).The standard estimator is the MLE: \\(d^0(X) = X\\). The loss function is the sum of squared errors: \\(L(\\mu, d) = ||\\mu - d||^2 = \\sum (\\mu_i - d_i)^2\\).\n\nTheorem 3.3 (Stein’s Paradox) When \\(p \\ge 3\\), the estimator \\(d^0(X)\\) is inadmissible. There exists an estimator that strictly dominates it (has lower risk everywhere).\n\nConsider the class of shrinkage estimators: \\[d^a(X) = \\left( 1 - \\frac{a}{||X||^2} \\right) X\\] where \\(X = (X_1, \\dots, X_p)^T\\).\nWhen \\(a &gt; 0\\), this estimator “shrinks” the data vector toward the origin \\((0, \\dots, 0)\\).\n\nLemma 3.1 (Stein’s Lemma) If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\n\n\nProof. Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#steins-lemma",
    "href": "bayesian.html#steins-lemma",
    "title": "3  Bayesian Methods",
    "section": "3.11 Stein’s Lemma",
    "text": "3.11 Stein’s Lemma\nTo prove dominance, we use Stein’s Lemma. If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\nUsing this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#predictive-distributions",
    "href": "bayesian.html#predictive-distributions",
    "title": "3  Bayesian Methods",
    "section": "3.7 Predictive Distributions",
    "text": "3.7 Predictive Distributions\nA key feature of Bayesian analysis is the ability to make inference about future observations, rather than just the model parameters. The posterior predictive distribution describes the probability of observing a new data point \\(y^*\\) given the observed data \\(y\\).\n\nDefinition 3.3 (Posterior Predictive Distribution) Let \\(f(y^*|\\theta)\\) be the sampling distribution of a future observation \\(y^*\\) given parameter \\(\\theta\\), and let \\(\\pi(\\theta|y)\\) be the posterior distribution of \\(\\theta\\) given observed data \\(y\\). The posterior predictive density is obtained by marginalizing over the parameter \\(\\theta\\):\n\\[\nf(y^*|y) = \\int_\\Theta f(y^*|\\theta) \\pi(\\theta|y) \\, d\\theta\n\\]\n\nThis distribution incorporates two distinct sources of uncertainty:\n\nSampling Uncertainty (Aleatoric): The inherent variability of the data generation process, represented by the variance in \\(f(y^*|\\theta)\\).\nParameter Uncertainty (Epistemic): The uncertainty regarding the true value of \\(\\theta\\), represented by the variance in the posterior \\(\\pi(\\theta|y)\\).\n\nAs sample size \\(n \\to \\infty\\), the parameter uncertainty vanishes (the posterior approaches a point mass), and the predictive distribution converges to the true data-generating distribution.\n\nExample 3.8 (Normal-Normal Predictive Distribution) Consider a case where the data \\(y_1, \\dots, y_n\\) are independent and normally distributed with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\):\n\\[\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\\]\nAssume a conjugate prior for the mean: \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\). The posterior distribution is \\(\\mu|y \\sim N(\\mu_n, \\sigma_n^2)\\), where \\(\\mu_n\\) and \\(\\sigma_n^2\\) are the updated posterior hyperparameters.\nThe predictive distribution for a new observation \\(y^*\\) is derived as:\n\\[\n\\begin{aligned}\nf(y^*|y) &= \\int_{-\\infty}^{\\infty} f(y^*|\\mu) \\pi(\\mu|y) \\, d\\mu \\\\\n&= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y^*-\\mu)^2}{2\\sigma^2}} \\times \\frac{1}{\\sqrt{2\\pi\\sigma_n^2}} e^{-\\frac{(\\mu-\\mu_n)^2}{2\\sigma_n^2}} \\, d\\mu\n\\end{aligned}\n\\]\nThis convolution of two Gaussians results in a new Gaussian distribution:\n\\[\ny^* | y \\sim N(\\mu_n, \\sigma^2 + \\sigma_n^2)\n\\]\nHere, the total predictive variance is the sum of the data variance (\\(\\sigma^2\\)) and the posterior uncertainty about the mean (\\(\\sigma_n^2\\)).\n\n\n3.7.1 Application: Bayesian Predictive Distributions of \\(Y_i\\) in Baseball Data\nA key advantage of the fully Bayesian approach is the ability to generate a predictive distribution for the season outcome.\nIn the previous section, we estimated \\(\\hat{p}_{Bayes}\\) by transforming the posterior mean of \\(\\mu\\) (the “Plug-in” estimator): \\[ \\hat{p}_{plug-in} = g^{-1}(E[\\mu | \\text{data}]) \\]\nHowever, because the inverse transformation \\(g^{-1}\\) is non-linear, this plug-in estimator is biased. A more robust approach is to compute the Predicted Season Outcome by averaging over the entire posterior distribution:\n\nFor each MCMC sample \\(t\\), convert \\(\\mu_i^{(t)}\\) to probability: \\(p_i^{(t)} = g^{-1}(\\mu_i^{(t)})\\).\nPredict the season home runs: \\(Y_i^{(t)} = N_i p_i^{(t)}\\).\nAverage these predictions to get the expected season count: \\(\\hat{Y}_i = \\frac{1}{T} \\sum Y_i^{(t)}\\).\nConvert back to a rate: \\(\\hat{p}_{pred} = \\hat{Y}_i / N_i\\).\n\nThis quantity \\(\\hat{p}_{pred}\\) approximates \\(E[p | \\text{data}]\\), the true posterior mean of the probability.\n\n\nCode\n# 1. Prepare Variables from Previous Chunks\n# We assume 'baseball_data', 'mu_js', and 'mu_brms' exist from previous steps.\n# 'mu_brms' is the posterior mean of mu from the brms model.\n\n# Inverse Transformation Function\ninv_trans &lt;- function(mu, n) { 0.5 * (sin(mu / sqrt(n)) + 1) }\n\n# 2. Calculate Method 3: Bayes (Plug-in)\n# Transform the posterior mean of mu directly\np_hat_plugin &lt;- inv_trans(mu_brms, baseball_data$Pre_AtBats)\n\n# 3. Calculate Method 4: Bayes (Predictive)\n# We simulate the posterior integration. We approximate the posterior variance \n# using the shrinkage estimate derived in the James-Stein step.\np_hat_pred &lt;- numeric(17)\nshrinkage_est &lt;- 1 - 14/sum((baseball_data$x - mean(baseball_data$x))^2)\n\nfor(i in 1:17) {\n  # A. Simulate posterior samples for mu_i\n  # Mean = mu_brms[i], Variance approx (1 - shrinkage)\n  sim_mus &lt;- rnorm(5000, mean = mu_brms[i], sd = sqrt(shrinkage_est)) \n  \n  # B. Convert samples to probability p using Pre-season N (scaling factor)\n  sim_ps &lt;- inv_trans(sim_mus, baseball_data$Pre_AtBats[i])\n  \n  # C. Predict Season Home Runs (Expected Y given p and Season N)\n  sim_Ys &lt;- sim_ps * baseball_data$Season_AtBats[i]\n  \n  # D. Average Y to get Expected Season Count\n  Y_hat &lt;- mean(sim_Ys)\n  \n  # E. Calculate p_hat_predictive\n  p_hat_pred[i] &lt;- Y_hat / baseball_data$Season_AtBats[i]\n}\n\n# 4. Combine Estimates into Data Frame\ndf_four_methods &lt;- data.frame(\n  Player = baseball_data$Player,\n  p_mle = inv_trans(baseball_data$x, baseball_data$Pre_AtBats),\n  p_js  = inv_trans(mu_js, baseball_data$Pre_AtBats),\n  p_plugin = p_hat_plugin,\n  p_pred   = p_hat_pred,\n  p_true   = baseball_data$p_season,\n  x_i      = baseball_data$x # for sorting\n)\n\n# 5. Sort by pre-season performance (x_i)\ndf_four_sorted &lt;- df_four_methods[order(df_four_methods$x_i), ]\n\n\n\n\n3.7.2 Comparison of Four Estimates (Probability Scale)\nThe table below compares the estimated season strike rates. Note the difference between \\(\\hat{p}_{Plug-in}\\) (transforming the mean \\(\\mu\\)) and \\(\\hat{p}_{Pred}\\) (the predictive average).\n\n\nCode\ndf_display_four &lt;- df_four_sorted\ndf_display_four[, 2:7] &lt;- round(df_display_four[, 2:7], 3)\n\nknitr::kable(df_display_four[, c(\"Player\", \"p_mle\", \"p_js\", \"p_plugin\", \"p_pred\", \"p_true\")],\n             row.names = FALSE,\n             col.names = c(\"Player\", \"MLE\", \"James-Stein\", \"Bayes (Plug-in)\", \"Bayes (Pred)\", \"True Rate\"),\n             align = \"c\",\n             caption = \"Comparison of Probability Estimates (Sorted by Pre-season Performance)\")\n\n\n\n\nTable 3.4: Comparison of Probability Estimates (Sorted by Pre-season Performance)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\nMLE\nJames-Stein\nBayes (Plug-in)\nBayes (Pred)\nTrue Rate\n\n\n\n\n13\n0.028\n0.048\n0.071\n0.073\n0.050\n\n\n5\n0.043\n0.058\n0.074\n0.075\n0.074\n\n\n3\n0.054\n0.069\n0.083\n0.085\n0.088\n\n\n7\n0.033\n0.045\n0.058\n0.060\n0.069\n\n\n10\n0.033\n0.045\n0.058\n0.060\n0.063\n\n\n12\n0.045\n0.058\n0.070\n0.072\n0.068\n\n\n4\n0.083\n0.094\n0.106\n0.107\n0.071\n\n\n11\n0.061\n0.068\n0.075\n0.077\n0.057\n\n\n9\n0.038\n0.043\n0.048\n0.052\n0.067\n\n\n14\n0.078\n0.078\n0.077\n0.080\n0.053\n\n\n6\n0.095\n0.087\n0.081\n0.083\n0.079\n\n\n17\n0.103\n0.088\n0.074\n0.077\n0.061\n\n\n16\n0.053\n0.037\n0.025\n0.030\n0.042\n\n\n15\n0.071\n0.052\n0.037\n0.041\n0.051\n\n\n1\n0.121\n0.098\n0.079\n0.081\n0.138\n\n\n2\n0.153\n0.117\n0.088\n0.090\n0.103\n\n\n8\n0.185\n0.129\n0.085\n0.088\n0.066\n\n\n\n\n\n\n\n\n\n\n3.7.3 Error Analysis of the Four Methods\nWe calculate the Sum of Squared Errors (SSE) for all four approaches on the probability scale.\n\n\nCode\n# Calculate Squared Errors\nerr_mle    &lt;- (df_four_sorted$p_mle - df_four_sorted$p_true)^2\nerr_js     &lt;- (df_four_sorted$p_js - df_four_sorted$p_true)^2\nerr_plugin &lt;- (df_four_sorted$p_plugin - df_four_sorted$p_true)^2\nerr_pred   &lt;- (df_four_sorted$p_pred - df_four_sorted$p_true)^2\n\n# Sum of Squared Errors\nsse_mle    &lt;- sum(err_mle)\nsse_js     &lt;- sum(err_js)\nsse_plugin &lt;- sum(err_plugin)\nsse_pred   &lt;- sum(err_pred)\n\n# Plotting\ny_max &lt;- max(c(err_mle, err_js, err_plugin, err_pred))\n\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(p) - p[true])^2),\n     main = \"Error Comparison (Probability Scale)\",\n     ylim = c(0, y_max))\n\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, err_plugin, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, err_pred, type = \"b\", pch = 15, col = \"darkgreen\")\n\ngrid()\nlegend(\"topleft\", \n       title = \"Sum Squared Error\",\n       legend = c(paste0(\"MLE: \", round(sse_mle, 4)), \n                  paste0(\"JS: \", round(sse_js, 4)), \n                  paste0(\"Bayes (Plug-in): \", round(sse_plugin, 4)),\n                  paste0(\"Bayes (Pred): \", round(sse_pred, 4))),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\"), \n       pch = c(1, 19, 17, 15), \n       lty = c(2, 1, 1, 1),\n       cex = 0.8)\n\n\n\n\n\n\n\n\nFigure 3.10: Squared Error Comparison: Four Methods\n\n\n\n\n\n\n\n3.7.4 Assessment using Relative Standardized Error\nWe now evaluate the methods using a relative error metric that penalizes deviations based on the rarity of the event. The error is standardized by the distance of the true probability to the nearest boundary.\n\\[ \\text{Metric}_i = \\frac{|p_i^{\\text{true}} - \\hat{p}_i|}{\\min(p_i^{\\text{true}}, 1 - p_i^{\\text{true}})} \\]\n\n\nCode\n# 1. Define the Error Metric Function\ncalc_metric &lt;- function(p_hat, p_true) {\n  # min(p, 1-p) determines the distance to the nearest boundary\n  denom &lt;- pmin(p_true, 1 - p_true)\n  abs(p_hat - p_true) / denom\n}\n\n# 2. Calculate Errors for each method\nmetric_mle    &lt;- calc_metric(df_four_sorted$p_mle, df_four_sorted$p_true)\nmetric_js     &lt;- calc_metric(df_four_sorted$p_js, df_four_sorted$p_true)\nmetric_plugin &lt;- calc_metric(df_four_sorted$p_plugin, df_four_sorted$p_true)\nmetric_pred   &lt;- calc_metric(df_four_sorted$p_pred, df_four_sorted$p_true)\n\n# 3. Calculate Sum of Errors for the Legend\nsum_mle    &lt;- sum(metric_mle)\nsum_js     &lt;- sum(metric_js)\nsum_plugin &lt;- sum(metric_plugin)\nsum_pred   &lt;- sum(metric_pred)\n\n# 4. Plotting\ny_max &lt;- max(c(metric_mle, metric_js, metric_plugin, metric_pred)) * 1.2\n\nplot(1:17, metric_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = \"Relative Standardized Error\",\n     main = \"Assessment of Estimation Methods\",\n     ylim = c(0, y_max))\n\nlines(1:17, metric_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, metric_plugin, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, metric_pred, type = \"b\", pch = 15, col = \"darkgreen\")\n\ngrid()\n\n# 5. Combined Legend (Top Left)\nlegend(\"topleft\", \n       title = \"Method [Sum of Relative Errors]\",\n       legend = c(paste0(\"MLE [\", round(sum_mle, 3), \"]\"), \n                  paste0(\"James-Stein [\", round(sum_js, 3), \"]\"), \n                  paste0(\"Bayes (Plug-in) [\", round(sum_plugin, 3), \"]\"),\n                  paste0(\"Bayes (Pred) [\", round(sum_pred, 3), \"]\")),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\"), \n       pch = c(1, 19, 17, 15), \n       lty = c(2, 1, 1, 1),\n       cex = 0.85, \n       bg = \"white\")\n\n\n\n\n\n\n\n\nFigure 3.11: Relative Error Assessment: Four Methods",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-modeling-and-mcmc",
    "href": "bayesian.html#hierarchical-modeling-and-mcmc",
    "title": "3  Bayesian Methods",
    "section": "3.5 Hierarchical Modeling and MCMC",
    "text": "3.5 Hierarchical Modeling and MCMC\nIn complex Bayesian settings where the posterior distribution cannot be derived analytically, we utilize hierarchical structures to represent levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate the resulting distributions.\n\n3.5.1 Hierarchical Model Structure\nA hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:\n\\[\n\\begin{aligned}\n\\text{Level 1 (Data Likelihood):} & \\quad X_i | \\mu_i, \\sigma^2 \\sim f(x_i | \\mu_i, \\sigma^2) \\\\\n\\text{Level 2 (Parameters):} & \\quad \\mu_i | \\theta, \\tau^2 \\sim \\pi(\\mu_i | \\theta, \\tau^2) \\\\\n\\text{Level 3 (Hyperparameters):} & \\quad \\theta, \\tau^2 \\sim \\pi(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nThe goal is to compute the joint posterior distribution of all unobserved parameters given the data \\(X = \\{X_1, \\dots, X_n\\}\\):\n\\[\np(\\boldsymbol{\\mu}, \\theta, \\tau^2 | X) \\propto \\left[ \\prod_{i=1}^n f(x_i | \\mu_i, \\sigma^2) \\pi(\\mu_i | \\theta, \\tau^2) \\right] \\pi(\\theta, \\tau^2)\n\\]\n\n\n3.5.2 Graphical Model Representation (Tree Structure)\nThe following tree diagram illustrates the conditional dependencies. Note that the parameters \\(\\mu_i\\) are conditionally independent given the hyperparameter \\(\\theta\\), which facilitates “borrowing strength” across groups.\n\n\n\n\n\n\n\n\nFigure 3.6: Hierarchical Tree Structure\n\n\n\n\n\n\n\n3.5.3 MCMC Estimation\nIn hierarchical models, the joint posterior distribution \\(p(\\boldsymbol{\\mu}, \\theta | X)\\) often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use Markov Chain Monte Carlo (MCMC) to draw sequence of samples \\(\\{\\boldsymbol{\\mu}^{(t)}, \\theta^{(t)}\\}\\) that converge to the target posterior distribution.\n\n3.5.3.1 Gibbs Sampling Algorithm\n\nGibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the full conditional distributions. To sample from a target distribution \\(p(\\theta_1, \\theta_2, \\dots, \\theta_k)\\), the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:\n\\[\n\\begin{aligned}\n\\theta_1^{(t+1)} &\\sim p(\\theta_1 | \\theta_2^{(t)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n\\theta_2^{(t+1)} &\\sim p(\\theta_2 | \\theta_1^{(t+1)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n&\\vdots \\\\\n\\theta_k^{(t+1)} &\\sim p(\\theta_k | \\theta_1^{(t+1)}, \\theta_2^{(t+1)}, \\dots, \\theta_{k-1}^{(t+1)})\n\\end{aligned}\n\\]\n\n\nExample 3.7 (Gibbs Sampling for Groups of Normal Data) The Model\nTo apply the general Gibbs sampling framework \\(\\theta_1, \\theta_2, \\dots, \\theta_k\\) to our specific hierarchical model, we identify the variables as follows:\n\nData Observations (\\(X_i\\)): These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school \\(i\\)). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These represent the latent means for each specific group or cluster. In the update step, \\(\\mu_i\\) acts as the first block of variables. It is updated by “compromising” between the local data \\(X_i\\) and the global characteristic \\(\\theta\\).\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all \\(\\mu_i\\) values, effectively “pooling” information from all groups to estimate the overall population center.\n\nGibbs Update in Hierarchical Models\nIn the hierarchical tree structure provided earlier, let our parameter vector be \\((\\mu_i, \\theta)\\). The “orthogonality” of the updates becomes clear when we derive the full conditionals for a Gaussian case:\n\nCase \\(\\theta_1 = \\mu_i\\): Sample \\(\\mu_i^{(t+1)}\\) from \\(p(\\mu_i | X_i, \\theta^{(t)})\\). This is a normal distribution with: \\[\n\\mu_i^{(t+1)} \\sim N\\left( \\frac{\\tau^2 X_i + \\sigma^2 \\theta^{(t)}}{\\sigma^2 + \\tau^2}, \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\right)\n\\]\nCase \\(\\theta_2 = \\theta\\): Sample \\(\\theta^{(t+1)}\\) from \\(p(\\theta | \\boldsymbol{\\mu}^{(t+1)})\\). Assuming a flat prior \\(\\pi(\\theta) \\propto 1\\): \\[\n\\theta^{(t+1)} \\sim N\\left( \\frac{1}{n} \\sum_{i=1}^n \\mu_i^{(t+1)}, \\frac{\\tau^2}{n} \\right)\n\\]\n\n\nVisual Characteristic: Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.\n\n\n3.5.3.2 Metropolis-Hastings (MH) Sampling\nWhen the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step \\(t\\):\n\nPropose: Draw a candidate state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* | \\theta^{(t)})\\).\nAccept/Reject: Calculate the acceptance probability: \\[\n\\alpha = \\min \\left( 1, \\frac{p(\\theta^* | X) q(\\theta^{(t)} | \\theta^*)}{p(\\theta^{(t)} | X) q(\\theta^* | \\theta^{(t)})} \\right)\n\\]\nSet \\(\\theta^{(t+1)} = \\theta^*\\) with probability \\(\\alpha\\); otherwise, set \\(\\theta^{(t+1)} = \\theta^{(t)}\\).\n\nVisual Characteristic: MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.\n\n\nCode\nset.seed(123)\nrho &lt;- 0.8\nlog_target &lt;- function(x, y) { -0.5 * (x^2 - 2*rho*x*y + y^2) / (1 - rho^2) }\n\n# Gibbs Path (Step-wise update)\ngx &lt;- -2; gy &lt;- -2\ngx_path &lt;- gx; gy_path &lt;- gy\nfor(i in 1:25) {\n  gx &lt;- rnorm(1, rho * gy, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx, gx); gy_path &lt;- c(gy_path, gy, gy) # Horizontal move\n  gy &lt;- rnorm(1, rho * gx, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx); gy_path &lt;- c(gy_path, gy) # Vertical move\n}\n\n# MH Path (Random Walk)\nmx &lt;- numeric(50); my &lt;- numeric(50)\nmx[1] &lt;- -2; my[1] &lt;- -2\nfor(i in 2:50) {\n  px &lt;- mx[i-1] + rnorm(1, 0, 0.4); py &lt;- my[i-1] + rnorm(1, 0, 0.4)\n  acc &lt;- exp(log_target(px, py) - log_target(mx[i-1], my[i-1]))\n  if(runif(1) &lt; acc) { mx[i] &lt;- px; my[i] &lt;- py } else { mx[i] &lt;- mx[i-1]; my[i] &lt;- my[i-1] }\n}\n\npar(mfrow = c(1, 2))\nt_seq &lt;- seq(-3, 3, length=50); z &lt;- outer(t_seq, t_seq, function(x,y) exp(log_target(x,y)))\nplot(gx_path, gy_path, type=\"l\", col=\"blue\", main=\"Gibbs (Orthogonal Steps)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\nplot(mx, my, type=\"l\", col=\"red\", main=\"Metropolis-Hastings (Random Walk)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\n\n\n\n\n\n\n\n\nFigure 3.7: Comparison of Sampling Paths",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#discrete-parameter-space",
    "href": "bayesian.html#discrete-parameter-space",
    "title": "3  Bayesian Methods",
    "section": "3.2 Discrete Parameter Space",
    "text": "3.2 Discrete Parameter Space\n\n\n3.2.1 Discrete Posterior Calculation\nConsider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#fundamental-elements-of-bayesian-inference-1",
    "href": "bayesian.html#fundamental-elements-of-bayesian-inference-1",
    "title": "3  Bayesian Methods",
    "section": "4.1 Fundamental Elements of Bayesian Inference",
    "text": "4.1 Fundamental Elements of Bayesian Inference\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).\n\nTheorem 4.1 (Bayes’ Theorem) Suppose we have a parameter \\(\\theta\\) with a prior distribution denoted by \\(\\pi(\\theta)\\). If we observe data \\(x\\) drawn from a distribution with probability density function (pdf) \\(f(x; \\theta)\\), then the posterior density of \\(\\theta\\) given the data \\(x\\) is defined as:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{\\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta}\n\\]\nIn this equation:\n\n\\(\\pi(\\theta)\\) is the prior.\n\\(f(x;\\theta)\\) is the likelihood.\nThe denominator is the marginal distribution of \\(x\\), often represented as a normalizing constant \\(c(x)\\) which is free of \\(\\theta\\).\n\nThus, we can state the proportional relationship:\n\\[\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n\\]\n\n\nExample 4.1 (Binomial-Beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\n\n\nExample 4.2 (Normal-Normal Conjugacy (Known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#numerical-illustrations-of-conjugacy",
    "href": "bayesian.html#numerical-illustrations-of-conjugacy",
    "title": "3  Bayesian Methods",
    "section": "4.2 Numerical Illustrations of Conjugacy",
    "text": "4.2 Numerical Illustrations of Conjugacy\nThe following examples visualize the shift in belief from prior to posterior using specific numerical values.\n\nExample 4.7 (Numerical Illustration: Binomial-Beta) Suppose we are estimating a probability \\(\\theta\\). * Prior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (Mean = 0.5). * Data: 10 trials, 8 successes (\\(n=10, x=8\\)). * Posterior: \\(\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)\\) (Mean \\(\\approx\\) 0.71).\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\nCode\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Prior: Beta(2, 2)\nprior &lt;- dbeta(theta, shape1 = 2, shape2 = 2)\n\n# Posterior: Beta(10, 4)\nposterior &lt;- dbeta(theta, shape1 = 10, shape2 = 4)\n\nplot(theta, posterior, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = \"Beta Prior vs Posterior\", ylim = c(0, max(c(prior, posterior))))\nlines(theta, prior, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,4)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 4.1: Prior vs Posterior for Beta-Binomial Example\n\n\n\n\n\n\nExample 4.3 (Binomial-Beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\n\n\nExample 4.4 (Normal-Normal Conjugacy (Known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\n\n\nExample 4.5 (Discrete Posterior Calculation) Consider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)\n\n\n\nExample 4.6 (Normal with Unknown Mean and Variance) Consider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is:\n\\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nMultiplying by the likelihood leads to a posterior of the same form (Conjugate), with updated parameters:\n\n\\(\\alpha' = \\alpha + n/2\\)\n\\(k' = k + n\\)\n\\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\)\n\\(\\beta' = \\beta + \\frac{1}{2} \\frac{nk}{n+k}(\\bar{x}-\\nu)^2 + \\frac{1}{2}\\sum (x_i - \\bar{x})^2\\)\n\n\n\n4.3 Decision Theory and Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 4.1 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\nMinimizing the Bayes Risk is equivalent to minimizing the expected loss for each \\(x\\). The quantity to minimize is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\n\n4.3.1 Common Loss Functions and Estimators\n\n\n\n4.3.2 Squared Error Loss (Point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule is the posterior mean.\n\n\n\n\n4.3.3 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) requires solving:\n\\[\\int_{-\\infty}^{d} \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = \\frac{1}{2}\\]\nResult: The Bayes rule is the posterior median.\n\n\n\n\n4.3.4 Hypothesis Testing (0-1 Loss)\nTesting \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\).\n\\[L(\\theta, a) = \\begin{cases} 1 & \\text{if error} \\\\ 0 & \\text{if correct} \\end{cases}\\]\nThe Bayes rule selects the hypothesis with the higher posterior probability.\n\\[d(x) = 1 \\iff P(\\theta \\in \\Theta_1 | x) \\ge P(\\theta \\in \\Theta_0 | x)\\]\n\n\n\nDefinition 4.2 (Highest Posterior Density (HPD) Interval) In interval estimation, we prescribe a set \\(A = (d-\\delta, d+\\delta)\\) and minimize the loss associated with \\(\\theta\\) falling outside this interval.\nThe Bayes rule \\(d(x)\\) is the center of the interval with the highest probability coverage. This leads to the Highest Posterior Density (HPD) interval.\nIn practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the Equal-Tailed Interval, where we cut off \\(\\alpha/2\\) probability from each tail.\n\n\n\n4.4 Minimax Estimation\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 4.2 (Constant Risk Theorem) If a Bayes rule \\(d^\\pi\\) has constant risk (i.e., \\(R(\\theta, d^\\pi) = c\\) for all \\(\\theta\\)), then \\(d^\\pi\\) is a minimax estimator.\n\n\n\n4.4.1 Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).\n\n\n\n\n4.5 Stein Estimation and Shrinkage\nConsider estimating a multivariate mean vector \\(\\mu = (\\mu_1, \\dots, \\mu_p)\\) given independent observations \\(X_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nThe standard estimator is the MLE: \\(d^0(X) = X\\). The loss function is the sum of squared errors: \\(L(\\mu, d) = ||\\mu - d||^2 = \\sum (\\mu_i - d_i)^2\\).\n\nTheorem 4.3 (Stein’s Result) When \\(p \\ge 3\\), the estimator \\(d^0(X)\\) is inadmissible. There exists an estimator that strictly dominates it (has lower risk everywhere).\n\nConsider the class of shrinkage estimators: \\[d^a(X) = \\left( 1 - \\frac{a}{||X||^2} \\right) X\\] where \\(X = (X_1, \\dots, X_p)^T\\).\nWhen \\(a &gt; 0\\), this estimator “shrinks” the data vector toward the origin \\((0, \\dots, 0)\\).\n\nLemma 4.1 (Stein’s Lemma) If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\n\n\nProof. Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\n\n\n\n4.6 Empirical Bayes\nThe James-Stein estimator can be motivated via an Empirical Bayes approach.\nModel:\n\n\\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\)\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\)\n\nThe posterior mean for \\(\\mu_i\\) (if \\(\\tau^2\\) were known) is: \\[E(\\mu_i|x_i) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\]\nThe marginal distribution of \\(X_i\\) is \\(N(0, 1+\\tau^2)\\). Consequently, \\(S = \\sum X_i^2 \\sim (1+\\tau^2) \\chi^2_p\\).\nWe can estimate the unknown shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\) using the data. Since \\(E[ \\frac{p-2}{S} ] = \\frac{1}{1+\\tau^2}\\), we replace the theoretical shrinkage factor with its unbiased estimate: \\[\\hat{B} = \\frac{p-2}{||X||^2}\\]\nThis recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\n\n\n4.6.1 Baseball Example (Efron & Morris)\nWe illustrate Stein estimation using baseball batting averages. Let \\(y_i\\) be the number of hits for player \\(i\\) in their first \\(n=45\\) at-bats. Let \\(\\hat{p}_i = y_i/n\\) be the observed average.\nTo apply the Normal model, we use a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\] Under this transformation, \\(X_i \\approx N(\\mu_i, 1)\\).\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value \\(\\mu_0\\)). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages \\(\\hat{p}_i\\).\n\n\n\n\n4.7 Predictive Distributions\nA key feature of Bayesian analysis is the predictive distribution for a future observation \\(x^*\\).\n\\[f(x^*|x) = \\int f(x^*|\\theta) \\pi(\\theta|x) d\\theta\\]\n\n\n4.7.1 Normal-Normal Predictive Distribution\nIf \\(x_1, \\dots, x_n \\sim N(\\mu, \\sigma^2)\\) (with \\(\\sigma^2\\) known) and \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\), the predictive distribution for a new observation \\(x^*\\) is:\n\\[x^*|x \\sim N(\\mu_1, \\sigma^2 + \\sigma_1^2)\\]\nwhere \\(\\mu_1\\) and \\(\\sigma_1^2\\) are the posterior mean and variance of \\(\\mu\\). The predictive variance includes both the inherent sampling uncertainty (\\(\\sigma^2\\)) and the uncertainty about the parameter (\\(\\sigma_1^2\\)).\n\n\n\n\n4.8 Hierarchical Modeling and MCMC\nWhen analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).\nHierarchical Structure:\n\nData: \\(X_i | \\mu_i \\sim f(x_i|\\mu_i)\\)\nParameters: \\(\\mu_i | \\theta \\sim \\pi(\\mu_i|\\theta)\\)\nHyperparameters: \\(\\theta \\sim \\pi(\\theta)\\)\n\nGibbs Sampling: To estimate the posterior \\(f(\\mu, \\theta | x)\\), we sample iteratively from the full conditional distributions:\n\nSample \\(\\mu_i\\) from \\(f(\\mu_i | x, \\theta)\\).\nSample \\(\\theta\\) from \\(f(\\theta | \\mu, x)\\).\n\n\n\n4.8.1 Baseball Example with Hierarchical Model\n\n\\(Y_i \\sim \\text{Bin}(n_i, p_i)\\)\nLogit transform: \\(\\mu_i = \\text{logit}(p_i)\\)\n\\(\\mu_i \\sim N(\\theta, \\tau^2)\\)\nPriors on \\(\\theta\\) and \\(\\tau^2\\).\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use Metropolis-Hastings steps within the Gibbs sampler.\nAlgorithm:\n\nInitialize parameters \\(\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}\\).\nPropose new values based on a candidate distribution.\nAccept or reject based on the acceptance probability ratio (Likelihood \\(\\times\\) Prior ratio).\nRepeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., \\(f(\\mu_j|x)\\)) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#paradigm-to-find-bayes-rules",
    "href": "bayesian.html#paradigm-to-find-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.2 Paradigm to Find Bayes Rules",
    "text": "3.2 Paradigm to Find Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) satisfies: \\[\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n\\]\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nTherefore, to minimize the Bayes risk, one just need to choose \\(d(x)\\) to minimize the posterior expected loss for each \\(x\\).\n\n\nThe following diagram summarizes the general workflow for deriving a Bayes estimator:\n\n\n\n\n\n\n\n\nFigure 3.3: Workflow for Finding the Bayes Rule",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#common-loss-functions-and-bayes-estimators",
    "href": "bayesian.html#common-loss-functions-and-bayes-estimators",
    "title": "3  Bayesian Methods",
    "section": "3.3 Common Loss Functions and Bayes Estimators",
    "text": "3.3 Common Loss Functions and Bayes Estimators\n\n3.3.1 Squared Error Loss (point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize the posterior expected loss \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting it to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule under squared error loss is the posterior mean.\n\n\n3.3.2 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nTo find the Bayes rule, we minimize the posterior expected loss:\n\\[\n\\psi(d) = E_{\\theta|x} [ |\\theta - d| ] = \\int_{-\\infty}^{\\infty} |\\theta - d| \\, dF(\\theta|x)\n\\]\nwhere \\(F(\\theta|x)\\) is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point \\(d\\):\n\\[\n\\psi(d) = \\int_{-\\infty}^{d} (d - \\theta) \\, dF(\\theta|x) + \\int_{d}^{\\infty} (\\theta - d) \\, dF(\\theta|x)\n\\]\nWe find the minimum by analyzing the rate of change of \\(\\psi(d)\\) with respect to \\(d\\). Differentiating (or taking the subgradient for non-differentiable points):\n\\[\n\\frac{d}{dd} \\psi(d) = \\int_{-\\infty}^{d} 1 \\, dF(\\theta|x) - \\int_{d}^{\\infty} 1 \\, dF(\\theta|x) = P(\\theta \\le d|x) - P(\\theta &gt; d|x)\n\\]\nSetting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:\n\\[\nP(\\theta \\le d|x) = P(\\theta &gt; d|x)\n\\]\nSince the total probability is 1, this condition simplifies to finding \\(d\\) such that the cumulative probability is \\(1/2\\).\nGeneral Case (Discrete or Mixed Distributions)\nIn cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to \\(F(d) = 0.5\\) may not exist. To generalize, the Bayes rule is defined as any median \\(m\\) of the posterior distribution.\nA median is formally defined as any value \\(m\\) that satisfies the following two conditions simultaneously:\n\n\\(P(\\theta \\le m|x) \\ge \\frac{1}{2}\\)\n\\(P(\\theta \\ge m|x) \\ge \\frac{1}{2}\\)\n\nResult: The Bayes rule under absolute error loss is the posterior median.\n\n\n3.3.3 Hypothesis Testing (0-1 Loss)\nConsider the hypothesis test \\(H_0: \\theta \\in \\Theta_0\\) versus \\(H_1: \\theta \\in \\Theta_1\\). We define the decision space as \\(\\mathcal{A} = \\{0, 1\\}\\), where \\(a=0\\) means accepting \\(H_0\\) and \\(a=1\\) means rejecting \\(H_0\\) (accepting \\(H_1\\)).\nCase 1: 0-1 Loss\nThe standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\ (\\text{Correct } H_0) \\\\ 1 & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\ (\\text{Correct } H_1) \\end{cases}\\]\nTo find the Bayes rule, we minimize the posterior expected loss for a given \\(x\\), denoted as \\(E_{\\theta|x}[L(\\theta, a)]\\).\n\nExpected Loss for choosing \\(a=0\\) (Accept \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 0)] = 0 \\cdot P(\\theta \\in \\Theta_0|x) + 1 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_1|x)\n  \\]\nExpected Loss for choosing \\(a=1\\) (Reject \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 1)] = 1 \\cdot P(\\theta \\in \\Theta_0|x) + 0 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_0|x)\n  \\]\n\nThe Bayes rule selects the action with the smaller expected loss. Thus, we choose \\(a=1\\) if: \\[\nP(\\theta \\in \\Theta_0|x) \\le P(\\theta \\in \\Theta_1|x)\n\\] This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.\nCase 2: General Loss (Asymmetric Costs)\nIn many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let \\(c_1\\) be the cost of a Type I error and \\(c_2\\) be the cost of a Type II error. Usually, we normalize one cost to 1.\nSuppose the loss function is: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\\\ c & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Cost of Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Cost of Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\end{cases}\\]\nWe again calculate the posterior expected loss:\n\nExpected Loss for \\(a=0\\): \\[E[L(\\theta, 0)|x] = 0 \\cdot P(\\Theta_0|x) + 1 \\cdot P(\\Theta_1|x) = P(\\Theta_1|x)\\]\nExpected Loss for \\(a=1\\): \\[E[L(\\theta, 1)|x] = c \\cdot P(\\Theta_0|x) + 0 \\cdot P(\\Theta_1|x) = c P(\\Theta_0|x)\\]\n\nWe reject \\(H_0\\) (\\(a=1\\)) if the expected loss of doing so is lower: \\[\nc P(\\Theta_0|x) \\le P(\\Theta_1|x)\n\\]\nSince \\(P(\\Theta_1|x) = 1 - P(\\Theta_0|x)\\), we can rewrite this condition as: \\[\nc P(\\Theta_0|x) \\le 1 - P(\\Theta_0|x) \\implies (1+c) P(\\Theta_0|x) \\le 1\n\\] \\[\nP(\\Theta_0|x) \\le \\frac{1}{1+c}\n\\]\nResult: With asymmetric costs, we accept \\(H_1\\) only if the posterior probability of the null hypothesis is sufficiently small (below the threshold \\(\\frac{1}{1+c}\\)). If the cost of false rejection \\(c\\) is high, we require stronger evidence against \\(H_0\\).\n\n\n3.3.4 Classification Prediction (categorical Parameter)\nIn classification problems, the parameter of interest is a discrete class label \\(\\theta\\) (often denoted as \\(y\\)) taking values in a set of categories \\(\\{1, 2, \\dots, K\\}\\). The goal is to predict the true class label based on observed features \\(x\\).\nWe typically employ the 0-1 loss function, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:\n\\[L(\\theta, \\hat{\\theta}) = \\begin{cases} 0 & \\text{if } \\hat{\\theta} = \\theta \\ (\\text{Correct Classification}) \\\\ 1 & \\text{if } \\hat{\\theta} \\neq \\theta \\ (\\text{Misclassification}) \\end{cases}\\]\nTo find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta} L(\\theta, \\hat{\\theta}) \\pi(\\theta|x)\n\\]\nSince the loss is 1 only when the predicted class \\(\\hat{\\theta}\\) differs from the true class \\(\\theta\\), this sum simplifies to:\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta \\neq \\hat{\\theta}} 1 \\cdot \\pi(\\theta|x) = P(\\theta \\neq \\hat{\\theta} | x) = 1 - P(\\theta = \\hat{\\theta} | x)\n\\]\nMinimizing the misclassification rate \\(1 - P(\\theta = \\hat{\\theta} | x)\\) is mathematically equivalent to maximizing the probability of being correct, \\(P(\\theta = \\hat{\\theta} | x)\\).\nResult: The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the Maximum A Posteriori (MAP) estimator, in the context of machine learning and pattern recognition, this decision rule is known as the Bayes Optimal Classifier.\n\\[\n\\hat{\\theta}_{\\text{Bayes}}(x) = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg\\max} \\ P(\\theta = k | x)\n\\]\n\n\n3.3.5 Interval Estimation and Highest Posterior Density (HPD)\nIn interval estimation, our goal is typically to find a set \\(C(x)\\) with a specified probability coverage \\(1-\\alpha\\) (i.e., \\(P(\\theta \\in C(x)|x) = 1-\\alpha\\)) that minimizes the “size” or length of the interval.\nLoss Function for HPD: The HPD interval can be formally derived as the Bayes rule under a loss function that linearly combines the size of the interval and the error of non-coverage:\n\\[\nL(\\theta, C) = \\text{Length}(C) + k \\cdot I(\\theta \\notin C)\n\\]\nwhere \\(k\\) is a positive constant representing the penalty for failing to include the true parameter \\(\\theta\\). Minimizing the posterior expected loss leads to including all values of \\(\\theta\\) where the posterior density \\(\\pi(\\theta|x)\\) exceeds \\(1/k\\). By adjusting \\(k\\), we control the credibility level \\(1-\\alpha\\).\nJustification for HPD: To minimize the length of the interval for a fixed probability mass (or equivalently, minimize this loss function), we should include the values of \\(\\theta\\) that have the highest probability density. If we include a value with low density while excluding a value with higher density, we could swap them to increase the probability mass without increasing the interval length (or conversely, shrink the length while maintaining the mass).\nTherefore, the Highest Posterior Density (HPD) interval is defined as: \\[C_{HPD} = \\{ \\theta : \\pi(\\theta|x) \\ge k_\\alpha \\}\\] where \\(k_\\alpha\\) is a threshold chosen such that the posterior probability of the set is \\(1-\\alpha\\).\nComparison with Equal-Tailed Intervals:\n\nEqual-Tailed Interval: We simply cut off \\(\\alpha/2\\) probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.\nHPD Interval: This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.\n\nThe plot below illustrates a skewed posterior distribution (Gamma). Notice how the HPD Interval (Blue) is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the Equal-Tailed Interval (Red).\n\n\nCode\n# Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)\nx_vals &lt;- seq(0, 15, length.out = 1000)\ny_vals &lt;- dgamma(x_vals, shape = 2, rate = 0.5)\n\n# Target Coverage\nalpha &lt;- 0.10\ntarget_prob &lt;- 1 - alpha\n\n# 1. Equal-tailed Interval (quantiles)\neq_lower &lt;- qgamma(alpha/2, shape = 2, rate = 0.5)\neq_upper &lt;- qgamma(1 - alpha/2, shape = 2, rate = 0.5)\n\n# 2. HPD Interval (density Threshold Optimization)\n# We Look for a Density Threshold K Such That the Area Above K Is 0.90\nfind_hpd &lt;- function(dist_vals, density_vals, probability) {\n  # Sort density values\n  ord &lt;- order(density_vals, decreasing = TRUE)\n  sorted_dens &lt;- density_vals[ord]\n  sorted_dist &lt;- dist_vals[ord]\n  \n  # Accumulate probability (approximation)\n  dx &lt;- diff(dist_vals)[1]\n  cum_prob &lt;- cumsum(sorted_dens * dx)\n  \n  # Find cutoff index\n  cutoff_idx &lt;- which(cum_prob &gt;= probability)[1]\n  \n  # Get the subset of x values\n  hpd_set &lt;- sorted_dist[1:cutoff_idx]\n  return(c(min(hpd_set), max(hpd_set)))\n}\n\nhpd_bounds &lt;- find_hpd(x_vals, y_vals, target_prob)\nhpd_lower &lt;- hpd_bounds[1]\nhpd_upper &lt;- hpd_bounds[2]\n\n# Plotting\nplot(x_vals, y_vals, type = 'l', lwd = 2, col = \"black\",\n     main = \"90% Credible Intervals (Skewed Posterior)\",\n     xlab = expression(theta), ylab = \"Density\",\n     ylim = c(0, max(y_vals) * 1.2))\n\n# Shade HPD\npolygon(c(x_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], hpd_upper, hpd_lower),\n        c(y_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], 0, 0),\n        col = rgb(0, 0, 1, 0.2), border = NA)\n\n# Draw Equal-tailed Lines (red)\nabline(v = c(eq_lower, eq_upper), col = \"red\", lwd = 2, lty = 2)\n# Draw HPD Lines (blue)\nabline(v = c(hpd_lower, hpd_upper), col = \"blue\", lwd = 2, lty = 1)\n\nlegend(\"topright\", \n       legend = c(\"Posterior Density\", \n                  paste0(\"Equal-Tailed (Len: \", round(eq_upper - eq_lower, 2), \")\"), \n                  paste0(\"HPD (Len: \", round(hpd_upper - hpd_lower, 2), \")\")),\n       col = c(\"black\", \"red\", \"blue\"), \n       lty = c(1, 2, 1), lwd = 2,\n       fill = c(NA, NA, rgb(0, 0, 1, 0.2)), border = NA)\n\n\n\n\n\n\n\n\nFigure 3.4: Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#finding-minimax-estimators-with-bayes-rules",
    "href": "bayesian.html#finding-minimax-estimators-with-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.4 Finding Minimax Estimators with Bayes Rules",
    "text": "3.4 Finding Minimax Estimators with Bayes Rules\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Bayes Estimator is Minimax) Let \\(\\delta^\\pi\\) be a Bayes estimator with respect to a prior \\(\\pi\\). If the risk function of \\(\\delta^\\pi\\) is constant on the parameter space \\(\\Theta\\), such that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), then \\(\\delta^\\pi\\) is a minimax estimator.\n\n\nProof. Let \\(\\delta^\\pi\\) be the Bayes estimator with constant risk \\(c\\). First, we compute its Bayes risk \\(r(\\pi, \\delta^\\pi)\\). Since the risk is constant:\n\\[\nr(\\pi, \\delta^\\pi) = \\int_\\Theta R(\\theta, \\delta^\\pi) \\pi(\\theta) d\\theta = \\int_\\Theta c \\, \\pi(\\theta) d\\theta = c\n\\]\nNow, let \\(\\delta'\\) be any arbitrary estimator. By the definition of a Bayes estimator, \\(\\delta^\\pi\\) minimizes the Bayes risk among all estimators:\n\\[\nr(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta')\n\\]\nNext, we observe that the Bayes risk of \\(\\delta'\\) is the expectation of its risk function with respect to the prior \\(\\pi\\). An average cannot exceed the maximum value of the function being averaged (the supremum):\n\\[\nr(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') \\cdot \\int_\\Theta \\pi(\\theta) d\\theta = \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nCombining these inequalities, we have:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta^\\pi) = c = r(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta') \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nSince \\(\\sup_\\theta R(\\theta, \\delta^\\pi) \\le \\sup_\\theta R(\\theta, \\delta')\\) holds for any estimator \\(\\delta'\\), \\(\\delta^\\pi\\) minimizes the maximum risk. Therefore, it is minimax.\n\nThe plot below visualizes the logic of the proof. The red line represents the Constant Risk Bayes Estimator (\\(\\delta^\\pi\\)), which has a constant height \\(c\\). The blue curve represents an Arbitrary Estimator (\\(\\delta'\\)).\nBecause \\(\\delta^\\pi\\) minimizes the weighted average risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk (\\(\\sup R\\)) strictly greater than or equal to \\(c\\). Thus, the constant risk estimator has the lowest possible maximum.\n\n\nCode\n# Define parameter space theta\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# 1. Constant Risk Bayes Estimator (Risk = c)\nc_val &lt;- 0.5\nrisk_bayes &lt;- rep(c_val, length(theta))\n\n# 2. Arbitrary Alternative Estimator\n# This function is chosen such that it dips below c but rises above it elsewhere\nrisk_alt &lt;- c_val + 0.2 * sin(2 * pi * theta) - 0.05\n\n# Plotting\nplot(theta, risk_alt, type = 'l', lwd = 2, col = \"blue\",\n     ylim = c(0, 1), ylab = \"Risk R(theta, d)\", xlab = expression(theta),\n     main = \"Geometry of the Minimax Theorem\")\n\n# Add Constant Risk Line\nlines(theta, risk_bayes, col = \"red\", lwd = 2)\n\n# Mark the Maximum of the Alternative\nmax_alt &lt;- max(risk_alt)\nmax_theta &lt;- theta[which.max(risk_alt)]\npoints(max_theta, max_alt, pch = 19, col = \"blue\")\ntext(max_theta, max_alt, labels = expression(sup~R(theta, delta^\"'\")), pos = 3, col = \"blue\")\n\n# Label the Constant Risk\ntext(0.1, c_val, labels = expression(R(theta, delta^pi) == c), pos = 3, col = \"red\")\n\n# Add Legend\nlegend(\"bottomright\", legend = c(\"Arbitrary Estimator\", \"Constant Risk Bayes Est.\"),\n       col = c(\"blue\", \"red\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nFigure 3.4: Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk.\n\n\n\n\n\n\nExample 3.5 (Binomial Minimax Estimator) Let \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#constant-risk-bayes-estimator-is-minimax",
    "href": "bayesian.html#constant-risk-bayes-estimator-is-minimax",
    "title": "3  Bayesian Methods",
    "section": "3.4 Constant Risk Bayes Estimator Is Minimax",
    "text": "3.4 Constant Risk Bayes Estimator Is Minimax\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Bayes Estimator Is Minimax) Let \\(\\delta^\\pi\\) be a Bayes estimator with respect to a prior \\(\\pi\\). If the risk function of \\(\\delta^\\pi\\) is constant on the parameter space \\(\\Theta\\), such that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), then \\(\\delta^\\pi\\) is a minimax estimator.\n\n\nProof. Let \\(\\delta^\\pi\\) be the Bayes estimator with constant risk \\(c\\). First, we compute its Bayes risk \\(r(\\pi, \\delta^\\pi)\\). Since the risk is constant:\n\\[\nr(\\pi, \\delta^\\pi) = \\int_\\Theta R(\\theta, \\delta^\\pi) \\pi(\\theta) d\\theta = \\int_\\Theta c \\, \\pi(\\theta) d\\theta = c\n\\]\nNow, let \\(\\delta'\\) be any arbitrary estimator. By the definition of a Bayes estimator, \\(\\delta^\\pi\\) minimizes the Bayes risk among all estimators:\n\\[\nr(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta')\n\\]\nNext, we observe that the Bayes risk of \\(\\delta'\\) is the expectation of its risk function with respect to the prior \\(\\pi\\). An average cannot exceed the maximum value of the function being averaged (the supremum):\n\\[\nr(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') \\cdot \\int_\\Theta \\pi(\\theta) d\\theta = \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nCombining these inequalities, we have:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta^\\pi) = c = r(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta') \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nSince \\(\\sup_\\theta R(\\theta, \\delta^\\pi) \\le \\sup_\\theta R(\\theta, \\delta')\\) holds for any estimator \\(\\delta'\\), \\(\\delta^\\pi\\) minimizes the maximum risk. Therefore, it is minimax.\n\nThe plot below visualizes the logic of the proof. The red line represents the Constant Risk Bayes Estimator (\\(\\delta^\\pi\\)), which has a constant height \\(c\\). The blue curve represents an Arbitrary Estimator (\\(\\delta'\\)).\nBecause \\(\\delta^\\pi\\) minimizes the weighted average risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk (\\(\\sup R\\)) strictly greater than or equal to \\(c\\). Thus, the constant risk estimator has the lowest possible maximum.\n\n\nCode\n# Define Parameter Space Theta\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# 1. Constant Risk Bayes Estimator (risk = C)\nc_val &lt;- 0.5\nrisk_bayes &lt;- rep(c_val, length(theta))\n\n# 2. Arbitrary Alternative Estimator\n# This Function Is Chosen Such That It Dips Below C but Rises Above It Elsewhere\nrisk_alt &lt;- c_val + 0.2 * sin(2 * pi * theta) - 0.05\n\n# Plotting\nplot(theta, risk_alt, type = 'l', lwd = 2, col = \"blue\",\n     ylim = c(0, 1), ylab = \"Risk R(theta, d)\", xlab = expression(theta),\n     main = \"Geometry of the Minimax Theorem\")\n\n# Add Constant Risk Line\nlines(theta, risk_bayes, col = \"red\", lwd = 2)\n\n# Mark the Maximum of the Alternative\nmax_alt &lt;- max(risk_alt)\nmax_theta &lt;- theta[which.max(risk_alt)]\npoints(max_theta, max_alt, pch = 19, col = \"blue\")\ntext(max_theta, max_alt, labels = expression(sup~R(theta, delta^\"'\")), pos = 3, col = \"blue\")\n\n# Label the Constant Risk\ntext(0.1, c_val, labels = expression(R(theta, delta^pi) == c), pos = 3, col = \"red\")\n\n# Add Legend\nlegend(\"bottomright\", legend = c(\"Arbitrary Estimator\", \"Constant Risk Bayes Est.\"),\n       col = c(\"blue\", \"red\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nFigure 3.5: Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk.\n\n\n\n\n\n\nExample 3.5 (Binomial Minimax Estimator) Let \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#steins-paradox-and-the-james-stein-estimator",
    "href": "bayesian.html#steins-paradox-and-the-james-stein-estimator",
    "title": "3  Bayesian Methods",
    "section": "3.3 Stein’s Paradox and the James-stein Estimator",
    "text": "3.3 Stein’s Paradox and the James-stein Estimator\nIn high-dimensional estimation (\\(p \\ge 3\\)), the Maximum Likelihood Estimator (MLE) is inadmissible under squared error loss. The James-Stein Estimator dominates the MLE, meaning it achieves lower risk for all values of \\(\\theta\\).\nConsider the setting:\n\nData: \\(X \\sim N_p(\\theta, I)\\)\nPrior: \\(\\theta \\sim N_p(0, \\tau^2 I)\\)\nEstimator: \\(d^{JS}(x) = \\left( 1 - \\frac{p-2}{||x||^2} \\right) x\\)\n\nWe can derive the Bayes Risk \\(r(\\pi, d^{JS})\\) of this estimator using two equivalent methods: minimizing the expected frequentist risk, or minimizing the expected posterior loss.\n\nTheorem 3.3 (Bayes Risk of James-stein Estimator) For \\(p \\ge 3\\), the Bayes risk of the James-Stein estimator \\(d^{JS}\\) with respect to the prior \\(\\theta \\sim N(0, \\tau^2 I)\\) is:\n\\[\nr(\\pi, d^{JS}) = \\frac{p\\tau^2 + 2}{\\tau^2 + 1}\n\\]\n\n\nProof. Method 1: Integration over the Prior (Frequentist Risk approach)\nThe Bayes risk is defined as \\(r(\\pi, d) = E_\\pi [ R(\\theta, d) ]\\).\nFirst, recall the frequentist risk of the James-Stein estimator for a fixed \\(\\theta\\). Using Stein’s Lemma, the risk is given by: \\[\nR(\\theta, d^{JS}) = p - (p-2)^2 E_\\theta \\left[ \\frac{1}{||X||^2} \\right]\n\\]\nTo find the Bayes risk, we take the expectation of this risk with respect to the prior \\(\\pi(\\theta)\\): \\[\nr(\\pi, d^{JS}) = \\int R(\\theta, d^{JS}) \\pi(\\theta) d\\theta = p - (p-2)^2 E_\\pi \\left[ E_\\theta \\left( \\frac{1}{||X||^2} \\right) \\right]\n\\]\nBy the law of iterated expectations, \\(E_\\pi [ E_\\theta (\\cdot) ]\\) is equivalent to the expectation with respect to the marginal distribution of \\(X\\), denoted as \\(m(x)\\). Under the conjugate prior, the marginal distribution is \\(X \\sim N(0, (1+\\tau^2)I)\\).\nConsequently, the quantity \\(\\frac{||X||^2}{1+\\tau^2}\\) follows a Chi-squared distribution with \\(p\\) degrees of freedom (\\(\\chi^2_p\\)). The expectation of the inverse chi-square is: \\[\nE \\left[ \\frac{1}{||X||^2} \\right] = \\frac{1}{1+\\tau^2} E \\left[ \\frac{1}{\\chi^2_p} \\right] = \\frac{1}{1+\\tau^2} \\cdot \\frac{1}{p-2}\n\\]\nSubstituting this back into the risk equation: \\[\n\\begin{aligned}\nr(\\pi, d^{JS}) &= p - (p-2)^2 \\cdot \\frac{1}{(p-2)(1+\\tau^2)} \\\\\n&= p - \\frac{p-2}{1+\\tau^2} \\\\\n&= \\frac{p(1+\\tau^2) - (p-2)}{1+\\tau^2} \\\\\n&= \\frac{p\\tau^2 + p - p + 2}{1+\\tau^2} = \\frac{p\\tau^2 + 2}{\\tau^2 + 1}\n\\end{aligned}\n\\]\n\n\nProof. Method 2: Integration over the Marginal (Posterior Loss approach)\nAlternatively, we can compute the Bayes risk by first finding the posterior expected loss for a given \\(x\\), and then averaging over the marginal distribution of \\(x\\): \\[\nr(\\pi, d) = E_m [ E_{\\theta|x} [ L(\\theta, d(x)) ] ]\n\\]\nStep 1: Posterior Expected Loss\nThe posterior distribution of \\(\\theta\\) given \\(x\\) is: \\[\n\\theta | x \\sim N \\left( \\frac{\\tau^2}{1+\\tau^2}x, \\frac{\\tau^2}{1+\\tau^2}I \\right)\n\\]\nThe expected squared error loss can be decomposed into the variance (trace) and the squared bias: \\[\nE_{\\theta|x} [ ||\\theta - d^{JS}(x)||^2 ] = \\text{tr}(\\text{Var}(\\theta|x)) + || E[\\theta|x] - d^{JS}(x) ||^2\n\\]\n\nTrace term: \\[\\text{tr} \\left( \\frac{\\tau^2}{1+\\tau^2} I_p \\right) = \\frac{p\\tau^2}{1+\\tau^2}\\]\nSquared Bias term: Let \\(B = \\frac{1}{1+\\tau^2}\\). Then \\(E[\\theta|x] = (1-B)x\\). The estimator is \\(d^{JS}(x) = (1 - \\frac{p-2}{||x||^2})x\\). The difference is: \\[\n  E[\\theta|x] - d^{JS}(x) = \\left( (1-B) - \\left( 1 - \\frac{p-2}{||x||^2} \\right) \\right) x = \\left( \\frac{p-2}{||x||^2} - B \\right) x\n  \\] Squaring the norm gives: \\[\n  \\left( \\frac{p-2}{||x||^2} - B \\right)^2 ||x||^2 = \\frac{(p-2)^2}{||x||^2} - 2B(p-2) + B^2 ||x||^2\n  \\]\n\nStep 2: Expectation with respect to Marginal \\(X\\)\nWe now take the expectation \\(E_m[\\cdot]\\) of the posterior loss. Recall \\(X \\sim N(0, (1+\\tau^2)I)\\), so \\(E[||X||^2] = p(1+\\tau^2)\\) and \\(E[1/||X||^2] = \\frac{1}{(p-2)(1+\\tau^2)}\\).\n\nExpectation of Trace term: Constant, remains \\(\\frac{p\\tau^2}{1+\\tau^2}\\).\nExpectation of Bias term: \\[\n  \\begin{aligned}\n  E_m \\left[ \\frac{(p-2)^2}{||X||^2} - \\frac{2(p-2)}{1+\\tau^2} + \\frac{||X||^2}{(1+\\tau^2)^2} \\right] &= (p-2)^2 \\frac{1}{(p-2)(1+\\tau^2)} - \\frac{2(p-2)}{1+\\tau^2} + \\frac{p(1+\\tau^2)}{(1+\\tau^2)^2} \\\\\n  &= \\frac{p-2}{1+\\tau^2} - \\frac{2p-4}{1+\\tau^2} + \\frac{p}{1+\\tau^2} \\\\\n  &= \\frac{p - 2 - 2p + 4 + p}{1+\\tau^2} \\\\\n  &= \\frac{2}{1+\\tau^2}\n  \\end{aligned}\n  \\]\n\nStep 3: Combine Terms\n\\[\nr(\\pi, d^{JS}) = \\underbrace{\\frac{p\\tau^2}{1+\\tau^2}}_{\\text{Variance Part}} + \\underbrace{\\frac{2}{1+\\tau^2}}_{\\text{Bias Part}} = \\frac{p\\tau^2 + 2}{\\tau^2 + 1}\n\\]\nBoth methods yield the same result.\n\n\nTheorem 3.4 (Inadmissibility of the MLE in High Dimensions (stein’s Phenomenon)) Let \\(X \\sim N_p(\\theta, I)\\) be a \\(p\\)-dimensional random vector with \\(p \\ge 3\\). Under the squared error loss function \\(L(\\theta, d) = ||\\theta - d||^2\\), the standard Maximum Likelihood Estimator \\(d^0(X) = X\\) is inadmissible.\n\n\nProof. To show that \\(d^0(X) = X\\) is inadmissible, we must find another estimator that dominates it (i.e., has equal or lower risk for all \\(\\theta\\), and strictly lower risk for at least one \\(\\theta\\)).\nFirst, consider the risk of the standard estimator \\(d^0\\). Since \\(X_i \\sim N(\\theta_i, 1)\\) are independent:\n\\[\nR(\\theta, d^0) = E_\\theta [ ||X - \\theta||^2 ] = \\sum_{i=1}^p E [ (X_i - \\theta_i)^2 ] = \\sum_{i=1}^p \\text{Var}(X_i) = p\n\\]\nNow consider the James-Stein estimator \\(d^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\). As established in the derivation of the Bayes Risk in Theorem 3.3, the frequentist risk function of \\(d^{JS}\\) is:\n\\[\nR(\\theta, d^{JS}) = p - (p-2)^2 E_\\theta \\left[ \\frac{1}{||X||^2} \\right]\n\\]\nSince the random variable \\(||X||^2\\) is non-negative and not identically infinity, the expectation \\(E_\\theta [ 1/||X||^2 ]\\) is strictly positive for all \\(\\theta\\). Therefore:\n\\[\nR(\\theta, d^{JS}) &lt; p = R(\\theta, d^0) \\quad \\text{for all } \\theta \\in \\mathbb{R}^p\n\\]\nBecause \\(d^{JS}\\) achieves a strictly lower risk than \\(d^0\\) everywhere in the parameter space, \\(d^0\\) is dominated by \\(d^{JS}\\) and is thus inadmissible.\n\n\n3.3.1 Practical Application: One-way ANOVA and “borrowing Strength”\n\nExample 3.6 Consider a One-Way ANOVA setting where we wish to estimate the means of \\(p\\) different independent groups (e.g., the true batting averages of \\(p=10\\) baseball players, or the efficacy of \\(p=5\\) different hospital treatments).\n\nModel: Let \\(X_i \\sim N(\\theta_i, \\sigma^2)\\) be the observed sample mean for group \\(i\\), for \\(i = 1, \\dots, p\\).\nGoal: Estimate the vector of true means \\(\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_p)\\) simultaneously. The loss is the sum of squared errors: \\(L(\\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\theta}}) = \\sum (\\theta_i - \\hat{\\theta}_i)^2\\).\n\nThe MLE Approach (Total Separation): The standard estimator is \\(\\hat{\\theta}_i^{\\text{MLE}} = X_i\\). This estimates each group entirely independently, using only data from that specific group. If a specific player has a lucky streak, their estimate is very high; if they are unlucky, it is very low.\nThe James-Stein Approach (Shrinkage / Pooling): In this context, the James-Stein estimator (specifically the variation shrinking toward the grand mean \\(\\bar{X}\\)) is: \\[\n\\hat{\\theta}_i^{JS} = \\bar{X} + \\left( 1 - \\frac{(p-3)\\sigma^2}{\\sum (X_i - \\bar{X})^2} \\right) (X_i - \\bar{X})\n\\]\nWhy is this better? Even though the groups might be physically independent (e.g., distinct hospitals), the James-Stein estimator “borrows strength” from the ensemble.\n\nNoise Reduction: Extreme observations \\(X_i\\) are likely to contain more positive noise than signal. Shrinking them toward the global average \\(\\bar{X}\\) reduces this variance.\nStein’s Paradox: While \\(\\hat{\\theta}_i^{JS}\\) introduces bias (estimates are pulled toward the center), the reduction in variance is so significant that the Total Risk (sum of squared errors over all groups) is strictly lower than that of the MLE, provided \\(p \\ge 3\\).\n\nThus, estimating the groups together yields a more accurate global picture than estimating them separately, even if the groups are independent.\n\n\n\n3.3.2 Why is this Paradoxical?\nThe result that \\(d^{JS}\\) dominates \\(d^0\\) is called Stein’s Paradox because it defies intuition in several ways:\n\nIndependence Irrelevance: The result holds even if the components \\(X_i\\) are completely unrelated (e.g., \\(X_1\\) is the price of tea in China, \\(X_2\\) is the temperature in Saskatoon, and \\(X_3\\) is the weight of a local cat). It seems absurd that combining unrelated data improves the estimate of each, but the combined risk is indeed lower.\nNo “Free Lunch”: The James-Stein estimator does not improve every individual component \\(\\theta_i\\) simultaneously for every realization. Instead, it minimizes the total risk \\(\\sum E(\\hat{\\theta}_i - \\theta_i)^2\\). It sacrifices accuracy on outliers (by biasing them) to gain significant stability on the bulk of the data.\nDestruction of Symmetry: The MLE is invariant under translation and rotation. The James-Stein estimator breaks this symmetry by shrinking toward an arbitrary point (usually the origin or the grand mean), yet it yields a better objective performance.\n\n\n\n3.3.3 What We Learned\n\nBias-Variance Tradeoff: This is the most famous example where introducing bias (shrinkage) leads to a massive reduction in variance, thereby reducing the overall Mean Squared Error (MSE). Unbiasedness is not always a virtue in estimation.\nInadmissibility in High Dimensions: Intuitions formed in 1D or 2D (where MLE is admissible) fail in higher dimensions (\\(p \\ge 3\\)). The volume of space grows so fast that “standard” diffuse priors or MLEs become inefficient.\nHierarchical Modeling: Stein’s result provides the theoretical foundation for Hierarchical Bayesian Models. When we assume parameters come from a common distribution (e.g., \\(\\theta_i \\sim N(\\mu, \\tau^2)\\)), we naturally derive shrinkage estimators that “borrow strength” across groups, formalized as Empirical Bayes or fully Bayesian methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#baseball-example-with-hierarchical-model",
    "href": "bayesian.html#baseball-example-with-hierarchical-model",
    "title": "3  Bayesian Methods",
    "section": "4.4 Baseball Example with Hierarchical Model",
    "text": "4.4 Baseball Example with Hierarchical Model\n\n4.4.1 Empirical Bayes Method with Variance Stability Transformation (Efron & Morris, 1975)\nIn their seminal paper Data Analysis Using Stein’s Estimator and its Generalizations, Efron and Morris applied this method to predict the batting averages of 18 Major League Baseball players.\n\nData: They used the batting average of each player after their first 45 at-bats (\\(n=45\\)) as the initial observation \\(y_i\\).\nGoal: Predict the batting average for the remainder of the season.\n\nTransformation: Since batting averages are binomial proportions \\(\\hat{p}_i\\), their variance depends on the true mean \\(p_i(1-p_i)/n\\). To apply the homoscedastic Normal model (\\(X_i \\sim N(\\mu_i, 1)\\)), they applied a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\]\nThe Result: The Maximum Likelihood Estimator would simply predict that a player’s future performance will match their first 45 at-bats. The James-Stein (Empirical Bayes) estimator shrunk all individual averages toward the Grand Mean of all 18 players.\n\nPlayers with unusually high initial averages (e.g., Clemente, who started at .400) were predicted to regress downward.\nPlayers with unusually low starts were predicted to improve.\n\nOutcome: The James-Stein estimator reduced the total squared prediction error by roughly a factor of 3 compared to the MLE. This famously demonstrated that even for real-world independent parameters (different human beings), pooling information improves predictive accuracy.\n\n\\(Y_i \\sim \\text{Bin}(n_i, p_i)\\)\nLogit transform: \\(\\mu_i = \\text{logit}(p_i)\\)\n\\(\\mu_i \\sim N(\\theta, \\tau^2)\\)\nPriors on \\(\\theta\\) and \\(\\tau^2\\).\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use Metropolis-Hastings steps within the Gibbs sampler.\nAlgorithm:\n\nInitialize parameters \\(\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}\\).\nPropose new values based on a candidate distribution.\nAccept or reject based on the acceptance probability ratio (Likelihood \\(\\times\\) Prior ratio).\nRepeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., \\(f(\\mu_j|x)\\)) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-model-structure",
    "href": "bayesian.html#hierarchical-model-structure",
    "title": "3  Bayesian Methods",
    "section": "4.1 Hierarchical Model Structure",
    "text": "4.1 Hierarchical Model Structure\nA hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:\n\\[\n\\begin{aligned}\n\\text{Level 1 (Data Likelihood):} & \\quad X_i | \\mu_i, \\sigma^2 \\sim f(x_i | \\mu_i, \\sigma^2) \\\\\n\\text{Level 2 (Parameters):} & \\quad \\mu_i | \\theta, \\tau^2 \\sim \\pi(\\mu_i | \\theta, \\tau^2) \\\\\n\\text{Level 3 (Hyperparameters):} & \\quad \\theta, \\tau^2 \\sim \\pi(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nThe goal is to compute the joint posterior distribution of all unobserved parameters given the data \\(X = \\{X_1, \\dots, X_n\\}\\):\n\\[\np(\\boldsymbol{\\mu}, \\theta, \\tau^2 | X) \\propto \\left[ \\prod_{i=1}^n f(x_i | \\mu_i, \\sigma^2) \\pi(\\mu_i | \\theta, \\tau^2) \\right] \\pi(\\theta, \\tau^2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#graphical-model-representation-tree-structure",
    "href": "bayesian.html#graphical-model-representation-tree-structure",
    "title": "3  Bayesian Methods",
    "section": "4.2 Graphical Model Representation (Tree Structure)",
    "text": "4.2 Graphical Model Representation (Tree Structure)\nThe following tree diagram illustrates the conditional dependencies. Note that the parameters \\(\\mu_i\\) are conditionally independent given the hyperparameter \\(\\theta\\), which facilitates “borrowing strength” across groups.\n\n\n\n\n\n\n\n\nFigure 4.1: Hierarchical Tree Structure",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#mcmc-estimation",
    "href": "bayesian.html#mcmc-estimation",
    "title": "3  Bayesian Methods",
    "section": "4.3 MCMC Estimation",
    "text": "4.3 MCMC Estimation\nIn hierarchical models, the joint posterior distribution \\(p(\\boldsymbol{\\mu}, \\theta | X)\\) often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use Markov Chain Monte Carlo (MCMC) to draw sequence of samples \\(\\{\\boldsymbol{\\mu}^{(t)}, \\theta^{(t)}\\}\\) that converge to the target posterior distribution.\n\n\n4.3.1 Gibbs Sampling Algorithm\nGibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the full conditional distributions. To sample from a target distribution \\(p(\\theta_1, \\theta_2, \\dots, \\theta_k)\\), the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:\n\\[\n\\begin{aligned}\n\\theta_1^{(t+1)} &\\sim p(\\theta_1 | \\theta_2^{(t)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n\\theta_2^{(t+1)} &\\sim p(\\theta_2 | \\theta_1^{(t+1)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n&\\vdots \\\\\n\\theta_k^{(t+1)} &\\sim p(\\theta_k | \\theta_1^{(t+1)}, \\theta_2^{(t+1)}, \\dots, \\theta_{k-1}^{(t+1)})\n\\end{aligned}\n\\]\n\n\n\nExample 4.1 (Gibbs Sampling for Groups of Normal Data) The Model\nTo apply the general Gibbs sampling framework \\(\\theta_1, \\theta_2, \\dots, \\theta_k\\) to our specific hierarchical model, we identify the variables as follows:\n\nData Observations (\\(X_i\\)): These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school \\(i\\)). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These represent the latent means for each specific group or cluster. In the update step, \\(\\mu_i\\) acts as the first block of variables. It is updated by “compromising” between the local data \\(X_i\\) and the global characteristic \\(\\theta\\).\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all \\(\\mu_i\\) values, effectively “pooling” information from all groups to estimate the overall population center.\n\nGibbs Update in Hierarchical Models\nIn the hierarchical tree structure provided earlier, let our parameter vector be \\((\\mu_i, \\theta)\\). The “orthogonality” of the updates becomes clear when we derive the full conditionals for a Gaussian case:\n\nCase \\(\\theta_1 = \\mu_i\\): Sample \\(\\mu_i^{(t+1)}\\) from \\(p(\\mu_i | X_i, \\theta^{(t)})\\). This is a normal distribution with: \\[\n\\mu_i^{(t+1)} \\sim N\\left( \\frac{\\tau^2 X_i + \\sigma^2 \\theta^{(t)}}{\\sigma^2 + \\tau^2}, \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\right)\n\\]\nCase \\(\\theta_2 = \\theta\\): Sample \\(\\theta^{(t+1)}\\) from \\(p(\\theta | \\boldsymbol{\\mu}^{(t+1)})\\). Assuming a flat prior \\(\\pi(\\theta) \\propto 1\\): \\[\n\\theta^{(t+1)} \\sim N\\left( \\frac{1}{n} \\sum_{i=1}^n \\mu_i^{(t+1)}, \\frac{\\tau^2}{n} \\right)\n\\]\n\n\nVisual Characteristic: Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.\n\n4.3.2 Metropolis-Hastings (MH) Sampling\nWhen the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step \\(t\\):\n\nPropose: Draw a candidate state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* | \\theta^{(t)})\\).\nAccept/Reject: Calculate the acceptance probability: \\[\n\\alpha = \\min \\left( 1, \\frac{p(\\theta^* | X) q(\\theta^{(t)} | \\theta^*)}{p(\\theta^{(t)} | X) q(\\theta^* | \\theta^{(t)})} \\right)\n\\]\nSet \\(\\theta^{(t+1)} = \\theta^*\\) with probability \\(\\alpha\\); otherwise, set \\(\\theta^{(t+1)} = \\theta^{(t)}\\).\n\nVisual Characteristic: MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.\n\n\nCode\nset.seed(123)\nrho &lt;- 0.8\nlog_target &lt;- function(x, y) { -0.5 * (x^2 - 2*rho*x*y + y^2) / (1 - rho^2) }\n\n# Gibbs Path (Step-wise update)\ngx &lt;- -2; gy &lt;- -2\ngx_path &lt;- gx; gy_path &lt;- gy\nfor(i in 1:25) {\n  gx &lt;- rnorm(1, rho * gy, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx, gx); gy_path &lt;- c(gy_path, gy, gy) # Horizontal move\n  gy &lt;- rnorm(1, rho * gx, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx); gy_path &lt;- c(gy_path, gy) # Vertical move\n}\n\n# MH Path (Random Walk)\nmx &lt;- numeric(50); my &lt;- numeric(50)\nmx[1] &lt;- -2; my[1] &lt;- -2\nfor(i in 2:50) {\n  px &lt;- mx[i-1] + rnorm(1, 0, 0.4); py &lt;- my[i-1] + rnorm(1, 0, 0.4)\n  acc &lt;- exp(log_target(px, py) - log_target(mx[i-1], my[i-1]))\n  if(runif(1) &lt; acc) { mx[i] &lt;- px; my[i] &lt;- py } else { mx[i] &lt;- mx[i-1]; my[i] &lt;- my[i-1] }\n}\n\npar(mfrow = c(1, 2))\nt_seq &lt;- seq(-3, 3, length=50); z &lt;- outer(t_seq, t_seq, function(x,y) exp(log_target(x,y)))\nplot(gx_path, gy_path, type=\"l\", col=\"blue\", main=\"Gibbs (Orthogonal Steps)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\nplot(mx, my, type=\"l\", col=\"red\", main=\"Metropolis-Hastings (Random Walk)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\n\n\n\n\n\n\n\n\nFigure 4.2: Comparison of Sampling Paths",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#metropolis-hastings-mh-sampling",
    "href": "bayesian.html#metropolis-hastings-mh-sampling",
    "title": "3  Bayesian Methods",
    "section": "4.4 Metropolis-Hastings (MH) Sampling",
    "text": "4.4 Metropolis-Hastings (MH) Sampling\nWhen the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step \\(t\\):\n\nPropose: Draw a candidate state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* | \\theta^{(t)})\\).\nAccept/Reject: Calculate the acceptance probability: \\[\n\\alpha = \\min \\left( 1, \\frac{p(\\theta^* | X) q(\\theta^{(t)} | \\theta^*)}{p(\\theta^{(t)} | X) q(\\theta^* | \\theta^{(t)})} \\right)\n\\]\nSet \\(\\theta^{(t+1)} = \\theta^*\\) with probability \\(\\alpha\\); otherwise, set \\(\\theta^{(t+1)} = \\theta^{(t)}\\).\n\nVisual Characteristic: MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.\n\n\nCode\nset.seed(123)\nrho &lt;- 0.8\nlog_target &lt;- function(x, y) { -0.5 * (x^2 - 2*rho*x*y + y^2) / (1 - rho^2) }\n\n# Gibbs Path (Step-wise update)\ngx &lt;- -2; gy &lt;- -2\ngx_path &lt;- gx; gy_path &lt;- gy\nfor(i in 1:25) {\n  gx &lt;- rnorm(1, rho * gy, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx, gx); gy_path &lt;- c(gy_path, gy, gy) # Horizontal move\n  gy &lt;- rnorm(1, rho * gx, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx); gy_path &lt;- c(gy_path, gy) # Vertical move\n}\n\n# MH Path (Random Walk)\nmx &lt;- numeric(50); my &lt;- numeric(50)\nmx[1] &lt;- -2; my[1] &lt;- -2\nfor(i in 2:50) {\n  px &lt;- mx[i-1] + rnorm(1, 0, 0.4); py &lt;- my[i-1] + rnorm(1, 0, 0.4)\n  acc &lt;- exp(log_target(px, py) - log_target(mx[i-1], my[i-1]))\n  if(runif(1) &lt; acc) { mx[i] &lt;- px; my[i] &lt;- py } else { mx[i] &lt;- mx[i-1]; my[i] &lt;- my[i-1] }\n}\n\npar(mfrow = c(1, 2))\nt_seq &lt;- seq(-3, 3, length=50); z &lt;- outer(t_seq, t_seq, function(x,y) exp(log_target(x,y)))\nplot(gx_path, gy_path, type=\"l\", col=\"blue\", main=\"Gibbs (Orthogonal Steps)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\nplot(mx, my, type=\"l\", col=\"red\", main=\"Metropolis-Hastings (Random Walk)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\n\n\n\n\n\n\n\n\nFigure 4.2: Comparison of Sampling Paths",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#final-example-baseball-example-with-hierarchical-model",
    "href": "bayesian.html#final-example-baseball-example-with-hierarchical-model",
    "title": "3  Bayesian Methods",
    "section": "4.8 Final Example: Baseball Example with Hierarchical Model",
    "text": "4.8 Final Example: Baseball Example with Hierarchical Model\n\n4.8.1 Empirical Bayes Method with Variance Stability Transformation (Efron & Morris, 1975)\nIn their seminal paper Data Analysis Using Stein’s Estimator and its Generalizations, Efron and Morris applied this method to predict the batting averages of 18 Major League Baseball players.\n\nData: They used the batting average of each player after their first 45 at-bats (\\(n=45\\)) as the initial observation \\(y_i\\).\nGoal: Predict the batting average for the remainder of the season.\n\nTransformation: Since batting averages are binomial proportions \\(\\hat{p}_i\\), their variance depends on the true mean \\(p_i(1-p_i)/n\\). To apply the homoscedastic Normal model (\\(X_i \\sim N(\\mu_i, 1)\\)), they applied a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\]\nThe Result: The Maximum Likelihood Estimator would simply predict that a player’s future performance will match their first 45 at-bats. The James-Stein (Empirical Bayes) estimator shrunk all individual averages toward the Grand Mean of all 18 players.\n\nPlayers with unusually high initial averages (e.g., Clemente, who started at .400) were predicted to regress downward.\nPlayers with unusually low starts were predicted to improve.\n\nOutcome: The James-Stein estimator reduced the total squared prediction error by roughly a factor of 3 compared to the MLE. This famously demonstrated that even for real-world independent parameters (different human beings), pooling information improves predictive accuracy.\n\n\\(Y_i \\sim \\text{Bin}(n_i, p_i)\\)\nLogit transform: \\(\\mu_i = \\text{logit}(p_i)\\)\n\\(\\mu_i \\sim N(\\theta, \\tau^2)\\)\nPriors on \\(\\theta\\) and \\(\\tau^2\\).\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use Metropolis-Hastings steps within the Gibbs sampler.\nAlgorithm:\n\nInitialize parameters \\(\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}\\).\nPropose new values based on a candidate distribution.\nAccept or reject based on the acceptance probability ratio (Likelihood \\(\\times\\) Prior ratio).\nRepeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., \\(f(\\mu_j|x)\\)) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#case-study-1998-major-league-baseball-home-run-race",
    "href": "bayesian.html#case-study-1998-major-league-baseball-home-run-race",
    "title": "3  Bayesian Methods",
    "section": "3.6 Case Study: 1998 Major League Baseball Home Run Race",
    "text": "3.6 Case Study: 1998 Major League Baseball Home Run Race\nIn 1998, the baseball world was captivated by Mark McGwire and Sammy Sosa as they chased Roger Maris’ 1961 record of 61 home runs in a single season. While McGwire and Sosa finished with 70 and 66 home runs respectively, we consider whether such performance could have been predicted using pre-season exhibition data.\nFor a set of \\(i = 1, \\dots, 17\\) players (including McGwire and Sosa), we observe their batting records in pre-season exhibition matches. Our goal is to estimate each player’s home run “strike rate” for the competitive season.\n\n3.6.1 Transforming Data\nWe utilize the pre-season home runs (\\(y_i\\)) and at-bats (\\(n_i\\)) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance \\(\\sigma^2 = 1\\).\n\\[\nx_i = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{y_i}{n_i} - 1 \\right)\n\\]\nThe goal is to estimate the latent parameter \\(\\mu_i\\) for each player and compare it to the “true” regular season performance.\n\n\n3.6.2 True Season Parameter (\\(\\mu_i\\) or \\(p_i^{Season}\\))\nTo validate our estimates, we define the “true” parameter value \\(\\mu_i\\) using the player’s performance over the full competitive season. Let \\(Y_i\\) be the total home runs and \\(N_i\\) be the total at-bats in the regular season. The true transformed rate is calculated as:\n\\[\n\\mu_i^{\\text{season}} = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{Y_i}{N_i} - 1 \\right)\n\\]\nNote that while we use the season-long probability (\\(Y_i/N_i\\)), we scale it by the pre-season sample size (\\(\\sqrt{n_i}\\)). This ensures that \\(\\mu_i^{\\text{season}}\\) is on the same scale as our observations \\(x_i\\), allowing for direct comparison of the estimation error.\n\n\nCode\nlibrary(ggplot2)\nlibrary(brms)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# 1. Input Raw Data\nni &lt;- c(58, 59, 74, 84, 69, 63, 60, 54, 53, 60, 66, 66, 72, 64, 42, 38, 58)\nyi &lt;- c(7, 9, 4, 7, 3, 6, 2, 10, 2, 2, 4, 3, 2, 5, 3, 2, 6)\nNi &lt;- c(509, 643, 633, 645, 606, 555, 619, 609, 552, 540, 561, 440, 585, 531, 454, 504, 244)\nYi &lt;- c(70, 66, 56, 46, 45, 44, 43, 40, 37, 34, 32, 30, 29, 28, 23, 21, 15)\n\n# 2. Calculate Derived Values\np_pre   &lt;- yi / ni                        # Pre-season Probability\nx       &lt;- sqrt(ni) * asin(2 * p_pre - 1) # Transformed Pre-season (x_i)\n\np_season &lt;- Yi / Ni                       # Season Probability\ntrue_mu  &lt;- sqrt(ni) * asin(2 * p_season - 1) # Transformed Season (mu_i)\n\n# 3. Create Main Data Frame\nbaseball_data &lt;- data.frame(\n  Player = 1:17,\n  Pre_HR = yi,\n  Pre_AtBats = ni,\n  p_pre = round(p_pre, 3),\n  x = x,\n  sei = 1, # Known standard error for transformed data\n  Season_HR = Yi,\n  Season_AtBats = Ni,\n  p_season = p_season,\n  true_mu = true_mu\n)\n\n# 4. Display the Data\nknitr::kable(baseball_data, \n             col.names = c(\"Player\", \"$y_i$\", \"$n_i$\", \"$p_i^{\\\\text{pre}}$\", \"$x_i$\", \"SE\", \n                           \"$Y_i$\", \"$N_i$\", \"$p_i^{\\\\text{seas}}$\", \"$\\\\mu_i$\"),\n             align = \"c\",\n             digits = 3,\n             caption = \"1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data\")\n\n\n\n1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(y_i\\)\n\\(n_i\\)\n\\(p_i^{\\text{pre}}\\)\n\\(x_i\\)\nSE\n\\(Y_i\\)\n\\(N_i\\)\n\\(p_i^{\\text{seas}}\\)\n\\(\\mu_i\\)\n\n\n\n\n1\n7\n58\n0.121\n-6.559\n1\n70\n509\n0.138\n-6.176\n\n\n2\n9\n59\n0.153\n-5.901\n1\n66\n643\n0.103\n-7.055\n\n\n3\n4\n74\n0.054\n-9.476\n1\n56\n633\n0.088\n-8.317\n\n\n4\n7\n84\n0.083\n-9.029\n1\n46\n645\n0.071\n-9.441\n\n\n5\n3\n69\n0.043\n-9.558\n1\n45\n606\n0.074\n-8.463\n\n\n6\n6\n63\n0.095\n-7.488\n1\n44\n555\n0.079\n-7.937\n\n\n7\n2\n60\n0.033\n-9.323\n1\n43\n619\n0.069\n-8.035\n\n\n8\n10\n54\n0.185\n-5.005\n1\n40\n609\n0.066\n-7.734\n\n\n9\n2\n53\n0.038\n-8.589\n1\n37\n552\n0.067\n-7.622\n\n\n10\n2\n60\n0.033\n-9.323\n1\n34\n540\n0.063\n-8.238\n\n\n11\n4\n66\n0.061\n-8.720\n1\n32\n561\n0.057\n-8.843\n\n\n12\n3\n66\n0.045\n-9.270\n1\n30\n440\n0.068\n-8.469\n\n\n13\n2\n72\n0.028\n-10.487\n1\n29\n585\n0.050\n-9.518\n\n\n14\n5\n64\n0.078\n-8.034\n1\n28\n531\n0.053\n-8.859\n\n\n15\n3\n42\n0.071\n-6.673\n1\n23\n454\n0.051\n-7.237\n\n\n16\n2\n38\n0.053\n-6.829\n1\n21\n504\n0.042\n-7.149\n\n\n17\n6\n58\n0.103\n-6.975\n1\n15\n244\n0.061\n-8.146\n\n\n\n\n\nIn this analysis, we model the home run strike rates of 17 Major League Baseball players using pre-season exhibition data from 1998. We apply five statistical methods ranging from simple independent estimation to advanced Bayesian decision theory.\n\n\n3.6.3 Methods for Estimating \\(\\mu_i\\) (Transformed Scale)\n\n3.6.3.1 Method 1: Simple Estimation (MLE)\nThe Maximum Likelihood Estimator (MLE) assumes each player’s performance is independent. It relies solely on the observed pre-season data.\n\\[ \\hat{\\mu}_i^{MLE} = X_i \\]\n\n\nCode\n# Simple Estimate is just the data itself\nmu_mle &lt;- baseball_data$x\n\n# MSE Calculation (Transformed Scale)\nmse_mle &lt;- mean((mu_mle - baseball_data$true_mu)^2)\n\n\n\n\n3.6.3.2 Method 2: Empirical Bayes (James-Stein)\nThe James-Stein estimator introduces a global mean \\(\\bar{X}\\) and shrinks individual estimates toward it. This assumes the players come from a common population distribution.\n\\[ \\hat{\\mu}_i^{JS} = \\bar{X} + \\left( 1 - \\frac{k-3}{\\sum (X_i - \\bar{X})^2} \\right) (X_i - \\bar{X}) \\]\nwhere \\(k=17\\) is the number of players.\n\n\nCode\ntheta_hat &lt;- mean(baseball_data$x)\nS &lt;- sum((baseball_data$x - theta_hat)^2)\nshrinkage_factor &lt;- 1 - (14 / S)\n\nmu_js &lt;- theta_hat + shrinkage_factor * (baseball_data$x - theta_hat)\n\n# MSE Calculation (Transformed Scale)\nmse_js &lt;- mean((mu_js - baseball_data$true_mu)^2)\n\n\n\n\n3.6.3.3 Method 3: Fully Bayesian MCMC (brms)\nWe use a hierarchical Bayesian model where parameters are treated as random variables. We implement this using brms.\n\\[\n\\begin{aligned}\nX_i &\\sim N(\\mu_i, 1) \\\\\n\\mu_i &\\sim N(\\theta, \\tau^2) \\\\\n\\theta &\\sim N(0, 10) \\\\\n\\tau &\\sim \\text{Cauchy}(0, 2)\n\\end{aligned}\n\\]\n\n\nCode\n# Fit Random Intercept Model: x | se(1) ~ 1 + (1|Player)\nfit_brms &lt;- brm(\n  formula = x | se(sei, sigma = TRUE) ~ 1 + (1 | Player),\n  data = baseball_data,\n  prior = c(\n    prior(normal(0, 10), class = \"Intercept\"),\n    prior(cauchy(0, 2), class = \"sd\")\n  ),\n  chains = 2, iter = 4000, warmup = 1000, seed = 123,\n  refresh = 0\n)\n\n# Extract Point Estimates (Posterior Means)\npost_means &lt;- fitted(fit_brms)[, \"Estimate\"]\nmu_brms &lt;- post_means\n\n# MSE Calculation (Transformed Scale)\nmse_brms &lt;- mean((mu_brms - baseball_data$true_mu)^2)\n\n\n\n\n\n3.6.4 Comparison of Estimates of \\(\\mu_i\\)\nFull Comparison of Estimates (Transformed Scale)\nThe following table presents the transformed data (\\(x_i\\)) and the true season parameter (\\(\\mu_i\\)) alongside the estimates from the three methods. The rows are sorted by \\(x_i\\) to visualize how the shrinkage methods (James-Stein and Bayesian) pull the estimates away from the extremes and toward the population mean compared to the raw MLE.\n\n\nCode\n# 1. Compile all estimates into a single data frame\ndf_estimates &lt;- data.frame(\n  Player = 1:17,\n  ni = baseball_data$Pre_AtBats,       \n  x_i = baseball_data$x,               # MLE Estimate (Raw Data)\n  mu_js = mu_js,                       # James-Stein Estimate\n  mu_bayes = mu_brms,                  # Fully Bayesian Estimate\n  mu_true = baseball_data$true_mu      # True Season Parameter\n)\n\n# 2. Sort by x_i (ascending)\ndf_sorted &lt;- df_estimates[order(df_estimates$x_i), ]\n\n# 3. Display the table\ndf_display_mu &lt;- df_sorted\ndf_display_mu[, 3:6] &lt;- round(df_display_mu[, 3:6], 3)\n\nknitr::kable(df_display_mu[, c(\"Player\", \"x_i\", \"mu_js\", \"mu_bayes\", \"mu_true\")],\n             row.names = FALSE,\n             col.names = c(\"Player\", \"$x_i$ (MLE)\", \"$\\\\hat{\\\\mu}_{JS}$\", \n                           \"$\\\\hat{\\\\mu}_{Bayes}$\", \"$\\\\mu_{true}$\"),\n             align = \"c\",\n             caption = \"Comparison of Estimates (Sorted by Pre-season $x_i$)\")\n\n\n\n\nTable 3.1: Comparison of Estimates (Sorted by Pre-season \\(x_i\\))\n\n\n\n\nComparison of Estimates (Sorted by Pre-season \\(x_i\\))\n\n\n\n\n\n\n\n\n\nPlayer\n\\(x_i\\) (MLE)\n\\(\\hat{\\mu}_{JS}\\)\n\\(\\hat{\\mu}_{Bayes}\\)\n\\(\\mu_{true}\\)\n\n\n\n\n13\n-10.487\n-9.589\n-8.746\n-9.518\n\n\n5\n-9.558\n-9.006\n-8.478\n-8.463\n\n\n3\n-9.476\n-8.954\n-8.470\n-8.317\n\n\n7\n-9.323\n-8.858\n-8.412\n-8.035\n\n\n10\n-9.323\n-8.858\n-8.415\n-8.238\n\n\n12\n-9.270\n-8.825\n-8.412\n-8.469\n\n\n4\n-9.029\n-8.673\n-8.331\n-9.441\n\n\n11\n-8.720\n-8.479\n-8.260\n-8.843\n\n\n9\n-8.589\n-8.397\n-8.206\n-7.622\n\n\n14\n-8.034\n-8.048\n-8.054\n-8.859\n\n\n6\n-7.488\n-7.705\n-7.897\n-7.937\n\n\n17\n-6.975\n-7.384\n-7.754\n-8.146\n\n\n16\n-6.829\n-7.292\n-7.714\n-7.149\n\n\n15\n-6.673\n-7.194\n-7.663\n-7.237\n\n\n1\n-6.559\n-7.122\n-7.628\n-6.176\n\n\n2\n-5.901\n-6.709\n-7.441\n-7.055\n\n\n8\n-5.005\n-6.146\n-7.186\n-7.734\n\n\n\n\n\n\n\n\nPlots of Errors (Sorted by \\(x_i\\))\nThis plot displays the Squared Error for each player. The x-axis represents the players sorted from lowest pre-season performance to highest.\n\n\nCode\n# Calculate Squared Errors using the SORTED dataframe\nerr_mle  &lt;- (df_sorted$x_i - df_sorted$mu_true)^2\nerr_js   &lt;- (df_sorted$mu_js - df_sorted$mu_true)^2\nerr_brms &lt;- (df_sorted$mu_bayes - df_sorted$mu_true)^2\n\n# Determine Y-axis range\ny_max &lt;- max(c(err_mle, err_js, err_brms))\n\n# Plot MLE Errors (Baseline)\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(mu) - mu[true])^2),\n     main = \"Estimation Error Comparison (Sorted)\",\n     ylim = c(0, y_max))\n\n# Add James-Stein Errors\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\n\n# Add Bayesian (brms) Errors\nlines(1:17, err_brms, type = \"b\", pch = 17, col = \"red\")\n\n# Add Grid and Legend\ngrid()\nlegend(\"topleft\", \n       title = \"Mean Squared Error\",\n       legend = c(paste0(\"MLE: \", round(mse_mle, 3)), \n                  paste0(\"JS: \", round(mse_js, 3)), \n                  paste0(\"Bayes: \", round(mse_brms, 3))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 3.8: Squared Error by Sorted Player Index (Transformed Scale)\n\n\n\n\n\n\n\n3.6.5 Methods for Estimating \\(p_i\\) directly\n\n3.6.5.1 Method 1-3: Converting \\(\\hat \\mu_i\\) back to \\(p_i\\)\nThe first three methods (MLE, James-Stein, and Normal-Normal Bayes) estimated the parameter \\(\\mu_i\\) on the transformed scale. To obtain the probability estimates \\(\\hat{p}_i\\), we apply the inverse of the variance-stabilizing transformation:\n\\[ \\hat{p}_i = \\frac{1}{2} \\left( \\sin\\left( \\frac{\\hat{\\mu}_i}{\\sqrt{n_i}} \\right) + 1 \\right) \\]\nwhere \\(\\hat{\\mu}_i\\) corresponds to the estimate derived from Method 1, 2, or 3, and \\(n_i\\) is the number of pre-season at-bats for player \\(i\\).\n\n\n3.6.5.2 Method 4: Hierarchical Logistic Regression (Logit-Normal)\nIn this fourth method, we model the probability \\(p_i\\) directly using a hierarchical structure on the log-odds scale, rather than transforming the data.\nWe assume the count \\(y_i\\) follows a Binomial distribution. The log-odds (logit) of the success rate \\(p_i\\) are drawn from a common Normal distribution with unknown mean \\(\\mu_0\\) and standard deviation \\(\\tau_0\\).\n\\[\n\\begin{aligned}\ny_i | p_i &\\sim \\text{Binomial}(n_i, p_i) \\\\\n\\text{logit}(p_i) &\\sim N(\\mu_0, \\tau_0^2) \\\\\n\\mu_0 &\\sim N(0, 10) \\\\\n\\tau_0 &\\sim \\text{Cauchy}(0, 2)\n\\end{aligned}\n\\]\nWe implement this in brms using the binomial family with a logit link. The individual point estimate \\(\\hat{p}_i\\) is the posterior mean of \\(p_i\\). Note that because the inverse-logit function is non-linear, the posterior mean of \\(p_i\\) is not simply the inverse-logit of the posterior mean of the random effect; brms handles this integration automatically via the fitted() function.\n\n\nCode\n# 1. Fit Hierarchical Logistic Regression\n# Formula: y | trials(n) ~ 1 + (1 | Player)\n# This estimates a global intercept (mu_0) and random intercepts for each player (logit(p_i))\nfit_logit &lt;- brm(\n  formula = Pre_HR | trials(Pre_AtBats) ~ 1 + (1 | Player),\n  data = baseball_data,\n  family = binomial(link = \"logit\"),\n  prior = c(\n    prior(normal(0, 5), class = \"Intercept\"),\n    prior(cauchy(0, 2), class = \"sd\")\n  ),\n  chains = 2, iter = 4000, warmup = 1000, seed = 123,\n  refresh = 0\n)\n\n# 2. Extract Posterior Means of p_i\n# fitted() returns the posterior expectations of the response (Expected Count). \nfitted_counts &lt;- fitted(fit_logit) \np_hat_logit &lt;- fitted_counts[, \"Estimate\"] / baseball_data$Pre_AtBats\n\n\n\n\n3.6.5.3 Method 5: Optimal Bayes Estimator (Weighted Median)\nWhile the posterior mean (Method 4) minimizes the Mean Squared Error (MSE), it is not necessarily optimal for the Relative Standardized Error metric we defined earlier: \\[L(p, \\hat{p}) = \\frac{|p - \\hat{p}|}{\\min(p, 1-p)}\\]\nThis is a form of weighted absolute error loss, where the weight is \\(w(p) = \\frac{1}{\\min(p, 1-p)}\\). Theoretical derivation shows that the estimator minimizing the expected posterior loss for this function is the Weighted Posterior Median.\nWe compute this by extracting the full posterior samples from the Logit-Normal model (Method 4) and calculating the weighted median for each player.\n\n\nCode\n# 1. Extract Posterior Samples (N_samples x 17 players)\n# posterior_epred gives samples of the expected count (N * p)\npost_counts &lt;- posterior_epred(fit_logit) \n\n# Convert to probability scale by dividing by trials\np_samples &lt;- sweep(post_counts, 2, baseball_data$Pre_AtBats, \"/\")\n\n# 2. Define Function for Weighted Median\n# Finds the value 'q' such that sum(weights where x &lt;= q) &gt;= 0.5 * total_weight\nget_weighted_median &lt;- function(samples) {\n  # Calculate weights based on the loss function denominator\n  # Avoid division by exact zero (unlikely but safer)\n  denom &lt;- pmin(samples, 1 - samples)\n  denom[denom &lt; 1e-6] &lt;- 1e-6 \n  weights &lt;- 1 / denom\n  \n  # Normalize weights\n  weights_norm &lt;- weights / sum(weights)\n  \n  # Sort samples and weights\n  ord &lt;- order(samples)\n  samp_sorted &lt;- samples[ord]\n  w_sorted &lt;- weights_norm[ord]\n  \n  # Find cutoff\n  cum_w &lt;- cumsum(w_sorted)\n  idx &lt;- which(cum_w &gt;= 0.5)[1]\n  \n  return(samp_sorted[idx])\n}\n\n# 3. Apply to all players\np_hat_optimal &lt;- apply(p_samples, 2, get_weighted_median)\n\n\n\n\n3.6.5.4 Comparison of All Five Estimates (Probability Scale)\nWe now compare all five methods: MLE, James-Stein (transformed), Bayes Normal-Normal (transformed), Hierarchical Logit-Normal (Posterior Mean), and Optimal Bayes (Weighted Median).\n1. MSE Comparison\n\n\nCode\n# 1. Prepare Estimates from Previous Steps\ninv_trans &lt;- function(mu, n) { 0.5 * (sin(mu / sqrt(n)) + 1) }\n\n# Convert transformed estimates back to probability scale\np_mle    &lt;- inv_trans(baseball_data$x, baseball_data$Pre_AtBats)\np_js     &lt;- inv_trans(mu_js, baseball_data$Pre_AtBats)\np_normal &lt;- inv_trans(mu_brms, baseball_data$Pre_AtBats) \n# p_hat_logit (Method 4) and p_hat_optimal (Method 5) are already calculated\n\n# 2. Combine into DataFrame\ndf_compare &lt;- data.frame(\n  Player = baseball_data$Player,\n  x_i    = baseball_data$x, # For sorting\n  p_true = baseball_data$p_season,\n  p_mle  = p_mle,\n  p_js   = p_js,\n  p_norm = p_normal,\n  p_logit = p_hat_logit,\n  p_opt   = p_hat_optimal\n)\n\n# Sort by initial performance\ndf_compare_sorted &lt;- df_compare[order(df_compare$x_i), ]\n\n# 3. Calculate MSE\nmse_p_mle   &lt;- mean((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)\nmse_p_js    &lt;- mean((df_compare_sorted$p_js - df_compare_sorted$p_true)^2)\nmse_p_norm  &lt;- mean((df_compare_sorted$p_norm - df_compare_sorted$p_true)^2)\nmse_p_logit &lt;- mean((df_compare_sorted$p_logit - df_compare_sorted$p_true)^2)\nmse_p_opt   &lt;- mean((df_compare_sorted$p_opt - df_compare_sorted$p_true)^2)\n\n# 4. Plot MSE\ny_max &lt;- max((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)\n\nplot(1:17, (df_compare_sorted$p_mle - df_compare_sorted$p_true)^2, \n     type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season)\",\n     ylab = \"Squared Error\",\n     main = \"Squared Error by Method\",\n     ylim = c(0, y_max))\n\nlines(1:17, (df_compare_sorted$p_js - df_compare_sorted$p_true)^2, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, (df_compare_sorted$p_norm - df_compare_sorted$p_true)^2, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, (df_compare_sorted$p_logit - df_compare_sorted$p_true)^2, type = \"b\", pch = 15, col = \"darkgreen\")\nlines(1:17, (df_compare_sorted$p_opt - df_compare_sorted$p_true)^2, type = \"b\", pch = 18, col = \"purple\")\n\ngrid()\nlegend(\"topleft\",\n       legend = c(paste0(\"MLE [MSE: \", round(mse_p_mle, 4), \"]\"),\n                  paste0(\"JS [MSE: \", round(mse_p_js, 4), \"]\"),\n                  paste0(\"Normal-Bayes [MSE: \", round(mse_p_norm, 4), \"]\"),\n                  paste0(\"Logit-Normal [MSE: \", round(mse_p_logit, 4), \"]\"),\n                  paste0(\"Optimal-Bayes [MSE: \", round(mse_p_opt, 4), \"]\")),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\", \"purple\"),\n       pch = c(1, 19, 17, 15, 18),\n       lty = c(2, 1, 1, 1, 1),\n       cex = 0.75,\n       bg = \"white\")\n\n\n\n\n\n\n\n\n\n2. Relative Standardized Error\nWe also evaluate the methods using the relative error metric that penalizes deviations based on the rarity of the event: \\[ \\text{Metric}_i = \\frac{|p_i^{\\text{true}} - \\hat{p}_i|}{\\min(p_i^{\\text{true}}, 1 - p_i^{\\text{true}})} \\]\n\n\nCode\n# 1. Define Metric\ncalc_metric &lt;- function(p_hat, p_true) {\n  denom &lt;- pmin(p_true, 1 - p_true)\n  abs(p_hat - p_true) / denom\n}\n\n# 2. Calculate Metric\nrel_mle   &lt;- calc_metric(df_compare_sorted$p_mle, df_compare_sorted$p_true)\nrel_js    &lt;- calc_metric(df_compare_sorted$p_js, df_compare_sorted$p_true)\nrel_norm  &lt;- calc_metric(df_compare_sorted$p_norm, df_compare_sorted$p_true)\nrel_logit &lt;- calc_metric(df_compare_sorted$p_logit, df_compare_sorted$p_true)\nrel_opt   &lt;- calc_metric(df_compare_sorted$p_opt, df_compare_sorted$p_true)\n\n# 3. Sum of Errors\nsum_rel_mle   &lt;- sum(rel_mle)\nsum_rel_js    &lt;- sum(rel_js)\nsum_rel_norm  &lt;- sum(rel_norm)\nsum_rel_logit &lt;- sum(rel_logit)\nsum_rel_opt   &lt;- sum(rel_opt)\n\n# 4. Plot\ny_max_rel &lt;- max(c(rel_mle, rel_js, rel_norm, rel_logit, rel_opt)) * 1.1\n\nplot(1:17, rel_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season)\", \n     ylab = \"Relative Standardized Error\",\n     main = \"Assessment of Estimation Methods\",\n     ylim = c(0, y_max_rel))\n\nlines(1:17, rel_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, rel_norm, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, rel_logit, type = \"b\", pch = 15, col = \"darkgreen\")\nlines(1:17, rel_opt, type = \"b\", pch = 18, col = \"purple\")\n\ngrid()\n\nlegend(\"topleft\", \n       title = \"Method [Sum Relative Error]\",\n       legend = c(paste0(\"MLE [\", round(sum_rel_mle, 3), \"]\"), \n                  paste0(\"James-Stein [\", round(sum_rel_js, 3), \"]\"), \n                  paste0(\"Normal-Bayes [\", round(sum_rel_norm, 3), \"]\"),\n                  paste0(\"Logit-Normal [\", round(sum_rel_logit, 3), \"]\"),\n                  paste0(\"Optimal-Bayes [\", round(sum_rel_opt, 3), \"]\")),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\", \"purple\"), \n       pch = c(1, 19, 17, 15, 18), \n       lty = c(2, 1, 1, 1, 1),\n       cex = 0.75, \n       bg = \"white\")\n\n\n\n\n\n\n\n\nFigure 3.9: Relative Error Assessment: Five Methods",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-structure-for-strike-rates",
    "href": "bayesian.html#hierarchical-structure-for-strike-rates",
    "title": "3  Bayesian Methods",
    "section": "4.5 Hierarchical Structure for Strike Rates",
    "text": "4.5 Hierarchical Structure for Strike Rates\nBecause the strike rates \\(p_i\\) likely share similarities across professional players, we can use a hierarchical model to “borrow strength” across the 17 players. To facilitate the use of Normal distributions in our MCMC (Gibbs) framework, we apply a logit transformation to the probabilities:\n\\[\n\\mu_i = \\text{logit}(p_i) = \\log\\left( \\frac{p_i}{1 - p_i} \\right)\n\\]\n\nExample 4.2 (Hierarchical Components for Baseball Example) In this specific context, the general variables \\(\\theta_1, \\theta_2\\) from the Gibbs sampler are mapped as follows:\n\nData (\\(X_i\\)): Here, our data consists of the pairs \\((Y_i, n_i)\\) for each of the 17 players.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These are the individual logit-strike rates for each player. A high \\(\\mu_i\\) corresponds to a high probability \\(p_i\\) of hitting a home run.\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the “league average” logit-strike rate for top-tier home run hitters. It determines the center of the distribution from which the individual \\(\\mu_i\\) are drawn.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#data-preparation",
    "href": "bayesian.html#data-preparation",
    "title": "3  Bayesian Methods",
    "section": "4.5 Data Preparation",
    "text": "4.5 Data Preparation\nWe utilize the pre-season home runs (\\(y_i\\)) and at-bats (\\(n_i\\)) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance \\(\\sigma^2 = 1\\).\n\\[\nX_i = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{y_i}{n_i} - 1 \\right)\n\\]\nThe goal is to estimate the latent parameter \\(\\mu_i\\) for each player and compare it to the “true” regular season performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#method-1-simple-estimation-mle",
    "href": "bayesian.html#method-1-simple-estimation-mle",
    "title": "3  Bayesian Methods",
    "section": "4.5 Method 1: Simple Estimation (MLE)",
    "text": "4.5 Method 1: Simple Estimation (MLE)\nThe Maximum Likelihood Estimator (MLE) assumes each player’s performance is independent. It relies solely on the observed pre-season data.\n\\[\n\\hat{\\mu}_i^{MLE} = X_i\n\\]\n\n\nCode\n# Simple Estimate is just the data itself\nmu_mle &lt;- df_baseball$x\n\n# MSE Calculation\nmse_mle &lt;- sum((mu_mle - df_baseball$true_mu)^2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#method-2-empirical-bayes-james-stein",
    "href": "bayesian.html#method-2-empirical-bayes-james-stein",
    "title": "3  Bayesian Methods",
    "section": "4.6 Method 2: Empirical Bayes (James-Stein)",
    "text": "4.6 Method 2: Empirical Bayes (James-Stein)\nThe James-Stein estimator introduces a global mean \\(\\bar{X}\\) and shrinks individual estimates toward it. This assumes the players come from a common population distribution.\n\nDefinition 4.1 (James-Stein Estimator) The estimator is defined as a convex combination of the individual observation and the global mean:\n\\[\n\\hat{\\mu}_i^{JS} = \\bar{X} + \\left( 1 - \\frac{k-3}{\\sum (X_i - \\bar{X})^2} \\right) (X_i - \\bar{X})\n\\]\nwhere \\(k=17\\) is the number of players.\n\n\n\nCode\ntheta_hat &lt;- mean(x)\nS &lt;- sum((x - theta_hat)^2)\nshrinkage_factor &lt;- 1 - (14 / S)\n\nmu_js &lt;- theta_hat + shrinkage_factor * (x - theta_hat)\n\n# MSE Calculation\nmse_js &lt;- sum((mu_js - df_baseball$true_mu)^2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#method-3-fully-bayesian-mcmc-brms",
    "href": "bayesian.html#method-3-fully-bayesian-mcmc-brms",
    "title": "3  Bayesian Methods",
    "section": "4.7 Method 3: Fully Bayesian MCMC (brms)",
    "text": "4.7 Method 3: Fully Bayesian MCMC (brms)\nWe use a hierarchical Bayesian model where parameters are treated as random variables. We implement this using brms.\nModel Specification: \\[\n\\begin{aligned}\nX_i &\\sim N(\\mu_i, 1) \\\\\n\\mu_i &\\sim N(\\theta, \\tau^2) \\\\\n\\theta &\\sim N(0, 10) \\\\\n\\tau &\\sim \\text{Cauchy}(0, 2)\n\\end{aligned}\n\\]\n\n\nCode\n# Fit Random Intercept Model: x | se(1) ~ 1 + (1|Player)\nfit_brms &lt;- brm(\n  formula = x | se(sei, sigma = TRUE) ~ 1 + (1 | Player),\n  data = df_baseball,\n  prior = c(\n    prior(normal(0, 10), class = \"Intercept\"),\n    prior(cauchy(0, 2), class = \"sd\")\n  ),\n  chains = 2, iter = 4000, warmup = 1000, seed = 123,\n  refresh = 0\n)\n\n# Extract Point Estimates (Posterior Means)\npost_means &lt;- fitted(fit_brms)[, \"Estimate\"]\nmu_brms &lt;- post_means\n\n# MSE Calculation\nmse_brms &lt;- sum((mu_brms - df_baseball$true_mu)^2)\n\n\n\n4.7.1 Comparison of Estimation of \\(\\mu_i\\)\n\n\nCode\n# Calculate Squared Errors for each method\nerr_mle &lt;- (df_baseball$x - df_baseball$true_mu)^2\nerr_js  &lt;- (mu_js - df_baseball$true_mu)^2\nerr_brms &lt;- (mu_brms - df_baseball$true_mu)^2\n\n# Determine Y-axis range\ny_max &lt;- max(c(err_mle, err_js, err_brms))\n\n# Plot MLE Errors (Baseline)\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index\", \n     ylab = expression(Squared~Error~~(hat(mu) - mu[true])^2),\n     main = \"Estimation Error Comparison by Player\",\n     ylim = c(0, y_max))\n\n# Add James-Stein Errors\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\n\n# Add Bayesian (brms) Errors\nlines(1:17, err_brms, type = \"b\", pch = 17, col = \"red\")\n\n# Add Grid and Legend with compact labels\ngrid()\nlegend(\"topright\", \n       title = \"MSE\",\n       legend = c(paste0(\"MLE: \", round(mse_mle, 3)), \n                  paste0(\"JS: \", round(mse_js, 3)), \n                  paste0(\"Bayes: \", round(mse_brms, 3))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 4.3: Squared Error by Player Index\n\n\n\n\n\n\n\n4.7.2 Error Analysis on Probability Scale\nWe apply the inverse transformation to convert the estimated \\(\\mu\\) values back to probabilities (\\(p\\)). We then calculate the sum of squared errors relative to the true season strike rates.\n\n\nCode\n# 1. Define Inverse Transformation\ninv_trans &lt;- function(mu, n) {\n  0.5 * (sin(mu / sqrt(n)) + 1)\n}\n\n# 2. Convert Estimates to Probability Scale\np_hat_mle  &lt;- inv_trans(df_baseball$x, ni)       # Equivalent to p_pre (yi/ni)\np_hat_js   &lt;- inv_trans(mu_js, ni)\np_hat_brms &lt;- inv_trans(mu_brms, ni)\n\n# 3. Define True Season Probability\np_true &lt;- df_baseball$p_season\n\n# 4. Calculate Squared Errors (Probability Scale)\nerr_p_mle  &lt;- (p_hat_mle - p_true)^2\nerr_p_js   &lt;- (p_hat_js - p_true)^2\nerr_p_brms &lt;- (p_hat_brms - p_true)^2\n\n# 5. Calculate Sum of Squared Errors (SSE)\nsse_p_mle  &lt;- sum(err_p_mle)\nsse_p_js   &lt;- sum(err_p_js)\nsse_p_brms &lt;- sum(err_p_brms)\n\n# 6. Plotting\ny_max_p &lt;- max(c(err_p_mle, err_p_js, err_p_brms))\n\nplot(1:17, err_p_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index\", \n     ylab = expression(Squared~Error~~(hat(p) - p[true])^2),\n     main = \"Error Comparison (Probability Scale)\",\n     ylim = c(0, y_max_p))\n\nlines(1:17, err_p_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, err_p_brms, type = \"b\", pch = 17, col = \"red\")\n\ngrid()\nlegend(\"topright\", \n       title = \"Sum Squared Error\",\n       legend = c(paste0(\"MLE: \", round(sse_p_mle, 4)), \n                  paste0(\"JS: \", round(sse_p_js, 4)), \n                  paste0(\"Bayes: \", round(sse_p_brms, 4))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 4.4: Squared Error Comparison on Probability Scale",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#method-4-rao-blackwellized-density-estimation",
    "href": "bayesian.html#method-4-rao-blackwellized-density-estimation",
    "title": "3  Bayesian Methods",
    "section": "4.8 Method 4: Rao-Blackwellized Density Estimation",
    "text": "4.8 Method 4: Rao-Blackwellized Density Estimation\nStandard histograms of MCMC samples can be rough. Rao-Blackwellization improves density estimation by exploiting the conditional independence structure.\n\nTheorem 4.1 (Rao-Blackwellized Density) Instead of kernel density estimation on samples of \\(\\mu_i\\), we average the conditional densities. Given samples \\((\\theta^{(t)}, \\tau^{(t)})\\), the conditional posterior for \\(\\mu_i\\) is Normal:\n\\[\np(\\mu_i | X) \\approx \\frac{1}{T} \\sum_{t=1}^T N\\left(\\mu_i \\mid m_i^{(t)}, v^{(t)}\\right)\n\\]\nwhere: \\[\n\\begin{aligned}\nw^{(t)} &= \\frac{(\\tau^{(t)})^2}{(\\tau^{(t)})^2 + 1} \\\\\nm_i^{(t)} &= w^{(t)} X_i + (1 - w^{(t)}) \\theta^{(t)} \\\\\nv^{(t)} &= w^{(t)}\n\\end{aligned}\n\\]\n\n\n\nCode\n# Extract Posterior Samples of Hyperparameters\n# b_Intercept = theta, sd_Player__Intercept = tau\nsamples &lt;- as_draws_df(fit_brms)\ntheta_mc &lt;- samples$b_Intercept\ntau_mc   &lt;- samples$sd_Player__Intercept\n\n# Function to compute RB density for a specific player index i\nget_rb_density &lt;- function(player_idx, x_val, grid_points) {\n  # Calculate weights for every MCMC sample\n  w &lt;- (tau_mc^2) / (tau_mc^2 + 1)\n  \n  # Calculate conditional means and variances\n  cond_mean &lt;- w * x_val + (1 - w) * theta_mc\n  cond_var  &lt;- w # Since sigma^2 = 1, variance is just w\n  \n  # Average densities over the MCMC samples\n  dens_vals &lt;- numeric(length(grid_points))\n  for(k in seq_along(grid_points)) {\n    dens_vals[k] &lt;- mean(dnorm(grid_points[k], mean = cond_mean, sd = sqrt(cond_var)))\n  }\n  return(data.frame(x = grid_points, density = dens_vals))\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#comparison-and-results",
    "href": "bayesian.html#comparison-and-results",
    "title": "3  Bayesian Methods",
    "section": "4.9 Comparison and Results",
    "text": "4.9 Comparison and Results\n\n4.9.1 1. MSE Performance\nThe following table compares the sum of squared errors for the three point-estimation methods.\n\n\n\n\nTable 4.1: Comparison of Estimation Error (Sum of Squared Differences)\n\n\n\n\n\n\nMethod\nMSE\n\n\n\n\nSimple (MLE)\n19.679\n\n\nJames-Stein\n8.066\n\n\nFully Bayesian (brms)\n6.570\n\n\n\n\n\n\n\n\n\n\n4.9.2 2. Shrinkage Plot\nThis plot visualizes how the methods adjust the estimates. The MLE lies on the 1:1 line (no adjustment). The Bayesian and James-Stein estimates are pulled horizontally toward the global mean.\n\n\nCode\nplot(df_baseball$true_mu, df_baseball$x, \n     xlab = \"True Season Rate (Transformed)\", \n     ylab = \"Estimated Rate\", \n     main = \"Shrinkage of Estimators\",\n     pch = 1, col = \"black\", xlim = c(-8, -2), ylim = c(-8, -2))\nabline(0, 1, lty = 2, col = \"gray\")\n\n# Add James-Stein\npoints(df_baseball$true_mu, mu_js, pch = 19, col = \"blue\")\n\n# Add brms (Fully Bayesian)\npoints(df_baseball$true_mu, mu_brms, pch = 17, col = \"red\")\n\nlegend(\"topleft\", legend = c(\"MLE (Data)\", \"James-Stein\", \"Bayesian (brms)\"),\n       col = c(\"black\", \"blue\", \"red\"), pch = c(1, 19, 17))\n\n# Draw arrows for player 1 (McGwire) to show shrinkage\nidx &lt;- 1\narrows(df_baseball$true_mu[idx], df_baseball$x[idx], \n       df_baseball$true_mu[idx], mu_brms[idx], \n       length = 0.1, col = \"red\")\n\n\n\n\n\n\n\n\nFigure 4.3: Shrinkage Effect: Estimated vs True Strike Rate\n\n\n\n\n\n\n\n4.9.3 3. Posterior Density Comparison (Player 1: McGwire)\nHere we compare the raw histogram of posterior samples for Mark McGwire against the smooth Rao-Blackwellized density.\n\n\nCode\n# Grid for plotting\ngrid &lt;- seq(-6, -2, length.out = 200)\n\n# Calculate RB Density for Player 1\nrb_dens &lt;- get_rb_density(1, x[1], grid)\n\n# Standard Histogram from brms samples\nmu1_samples &lt;- as_draws_df(fit_brms)$`r_Player[1,Intercept]` + samples$b_Intercept\n\nhist(mu1_samples, probability = TRUE, breaks = 30, col = \"lightgray\", border = \"white\",\n     main = \"Posterior Density: Player 1\", xlab = expression(mu[1]))\nlines(rb_dens$x, rb_dens$density, col = \"darkblue\", lwd = 2)\nabline(v = x[1], col = \"red\", lty = 2, lwd = 2)\nlegend(\"topright\", legend = c(\"Rao-Blackwell Density\", \"MCMC Histogram\", \"Observed Data\"),\n       col = c(\"darkblue\", \"lightgray\", \"red\"), lty = c(1, 1, 2), lwd = c(2, 10, 2))\n\n\n\n\n\n\n\n\nFigure 4.4: Posterior Density for Player 1 (McGwire)\n\n\n\n\n\nBecause the strike rates \\(p_i\\) likely share similarities across professional players, we can use a hierarchical model to “borrow strength” across the 17 players. To facilitate the use of Normal distributions in our MCMC (Gibbs) framework, we apply a logit transformation to the probabilities:\n\\[\n\\mu_i = \\text{logit}(p_i) = \\log\\left( \\frac{p_i}{1 - p_i} \\right)\n\\]\n\nExample 4.2 (Hierarchical Components for Baseball Example) In this specific context, the general variables \\(\\theta_1, \\theta_2\\) from the Gibbs sampler are mapped as follows:\n\nData (\\(X_i\\)): Here, our data consists of the pairs \\((Y_i, n_i)\\) for each of the 17 players.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These are the individual logit-strike rates for each player. A high \\(\\mu_i\\) corresponds to a high probability \\(p_i\\) of hitting a home run.\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the “league average” logit-strike rate for top-tier home run hitters. It determines the center of the distribution from which the individual \\(\\mu_i\\) are drawn.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#transforming-data",
    "href": "bayesian.html#transforming-data",
    "title": "3  Bayesian Methods",
    "section": "4.5 Transforming Data",
    "text": "4.5 Transforming Data\nWe utilize the pre-season home runs (\\(y_i\\)) and at-bats (\\(n_i\\)) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance \\(\\sigma^2 = 1\\).\n\\[\nX_i = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{y_i}{n_i} - 1 \\right)\n\\]\nThe goal is to estimate the latent parameter \\(\\mu_i\\) for each player and compare it to the “true” regular season performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#comparison-of-estimates-of-mu_i",
    "href": "bayesian.html#comparison-of-estimates-of-mu_i",
    "title": "3  Bayesian Methods",
    "section": "4.5 Comparison of Estimates of \\(\\mu_i\\)",
    "text": "4.5 Comparison of Estimates of \\(\\mu_i\\)\nFull Comparison of Estimates (Transformed Scale)\nThe following table presents the transformed data (\\(x_i\\)) and the true season parameter (\\(\\mu_i\\)) alongside the estimates from the three methods. The rows are sorted by \\(x_i\\) to visualize how the shrinkage methods (James-Stein and Bayesian) pull the estimates away from the extremes and toward the population mean compared to the raw MLE.\n\n\nCode\n# 1. Compile all estimates into a single data frame\n# Using 'baseball_data' as the source\ndf_estimates &lt;- data.frame(\n  Player = 1:17,\n  ni = baseball_data$Pre_AtBats,       # Use column names from baseball_data\n  x_i = baseball_data$x,               # MLE Estimate (Raw Data)\n  mu_js = mu_js,                       # James-Stein Estimate\n  mu_bayes = mu_brms,                  # Fully Bayesian Estimate\n  mu_true = baseball_data$true_mu      # True Season Parameter\n)\n\n# 2. Sort by x_i (ascending)\ndf_sorted &lt;- df_estimates[order(df_estimates$x_i), ]\n\n# 3. Create a display version (rounding values)\ndf_display_mu &lt;- df_sorted\ndf_display_mu[, 3:6] &lt;- round(df_display_mu[, 3:6], 3)\n\n# 4. Display the table\nknitr::kable(df_display_mu[, c(\"Player\", \"x_i\", \"mu_js\", \"mu_bayes\", \"mu_true\")],\n             row.names = FALSE,\n             col.names = c(\"Player\", \"$x_i$ (MLE)\", \"$\\\\hat{\\\\mu}_{JS}$\", \n                           \"$\\\\hat{\\\\mu}_{Bayes}$\", \"$\\\\mu_{true}$\"),\n             align = \"c\",\n             caption = \"Comparison of Estimates (Sorted by Pre-season $x_i$)\")\n\n\n\n\nTable 4.2: Comparison of Estimates (Sorted by Pre-season \\(x_i\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(x_i\\) (MLE)\n\\(\\hat{\\mu}_{JS}\\)\n\\(\\hat{\\mu}_{Bayes}\\)\n\\(\\mu_{true}\\)\n\n\n\n\n13\n-10.487\n-9.589\n-8.746\n-9.518\n\n\n5\n-9.558\n-9.006\n-8.478\n-8.463\n\n\n3\n-9.476\n-8.954\n-8.470\n-8.317\n\n\n7\n-9.323\n-8.858\n-8.412\n-8.035\n\n\n10\n-9.323\n-8.858\n-8.415\n-8.238\n\n\n12\n-9.270\n-8.825\n-8.412\n-8.469\n\n\n4\n-9.029\n-8.673\n-8.331\n-9.441\n\n\n11\n-8.720\n-8.479\n-8.260\n-8.843\n\n\n9\n-8.589\n-8.397\n-8.206\n-7.622\n\n\n14\n-8.034\n-8.048\n-8.054\n-8.859\n\n\n6\n-7.488\n-7.705\n-7.897\n-7.937\n\n\n17\n-6.975\n-7.384\n-7.754\n-8.146\n\n\n16\n-6.829\n-7.292\n-7.714\n-7.149\n\n\n15\n-6.673\n-7.194\n-7.663\n-7.237\n\n\n1\n-6.559\n-7.122\n-7.628\n-6.176\n\n\n2\n-5.901\n-6.709\n-7.441\n-7.055\n\n\n8\n-5.005\n-6.146\n-7.186\n-7.734\n\n\n\n\n\n\n\n\nPlots of Errors (Sorted by \\(x_i\\))\nThis plot displays the Squared Error for each player. The x-axis represents the players sorted from lowest pre-season performance to highest.\n\n\nCode\n# Calculate Squared Errors using the SORTED dataframe\nerr_mle  &lt;- (df_sorted$x_i - df_sorted$mu_true)^2\nerr_js   &lt;- (df_sorted$mu_js - df_sorted$mu_true)^2\nerr_brms &lt;- (df_sorted$mu_bayes - df_sorted$mu_true)^2\n\n# Total MSE for Legend\nmse_mle  &lt;- mean(err_mle) \nmse_js   &lt;- mean(err_js)\nmse_brms &lt;- mean(err_brms)\n\n# Determine Y-axis range\ny_max &lt;- max(c(err_mle, err_js, err_brms))\n\n# Plot MLE Errors (Baseline)\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(mu) - mu[true])^2),\n     main = \"Estimation Error Comparison (Sorted)\",\n     ylim = c(0, y_max))\n\n# Add James-Stein Errors\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\n\n# Add Bayesian (brms) Errors\nlines(1:17, err_brms, type = \"b\", pch = 17, col = \"red\")\n\n# Add Grid and Legend\ngrid()\nlegend(\"topleft\", \n       title = \"Mean Squared Error\",\n       legend = c(paste0(\"MLE: \", round(mse_mle, 3)), \n                  paste0(\"JS: \", round(mse_js, 3)), \n                  paste0(\"Bayes: \", round(mse_brms, 3))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 4.3: Squared Error by Sorted Player Index (Transformed Scale)\n\n\n\n\n\n\n4.5.1 Error Analysis on Probability Scale\n\n4.5.1.1 Recovering Estimated Probabilities (\\(\\hat{p}_i\\))\nOnce we have obtained an estimate \\(\\hat{\\mu}_i\\) (whether via James-Stein or Bayesian MCMC), we often want to interpret the result as a probability (strike rate). We invert the variance-stabilizing transformation to recover the estimated probability \\(\\hat{p}_i^{\\text{season}}\\):\n\\[\n\\hat{p}_i = \\frac{1}{2} \\left( \\sin\\left( \\frac{\\hat{\\mu}_i}{\\sqrt{n_i}} \\right) + 1 \\right)\n\\]\nThis inverse transformation maps the unbounded normal parameter back to the \\((0, 1)\\) interval, providing the predicted probability that player \\(i\\) hits a home run in any given at-bat during the regular season.\nWe apply the inverse transformation to convert the estimated \\(\\mu\\) values back to probabilities (\\(p\\)). We then calculate the sum of squared errors relative to the true season strike rates.\nFull Comparison of Estimates (Probability Scale)\n\n\nCode\n# 1. Define Inverse Transformation\ninv_trans &lt;- function(mu, n) {\n  0.5 * (sin(mu / sqrt(n)) + 1)\n}\n\n# 2. Convert Sorted Estimates to Probability Scale\n# We use the 'ni' column from df_sorted to ensure the correct N is used for each player\ndf_prob_sorted &lt;- data.frame(\n  Player = df_sorted$Player,\n  p_mle   = inv_trans(df_sorted$x_i, df_sorted$ni),\n  p_js    = inv_trans(df_sorted$mu_js, df_sorted$ni),\n  p_bayes = inv_trans(df_sorted$mu_bayes, df_sorted$ni),\n  p_true  = inv_trans(df_sorted$mu_true, df_sorted$ni)\n)\n\n# 3. Create display table\ndf_display_p &lt;- df_prob_sorted\ndf_display_p[, 2:5] &lt;- round(df_display_p[, 2:5], 3)\n\nknitr::kable(df_display_p,\n             row.names = FALSE,\n             col.names = c(\"Player\", \"$\\\\hat{p}_{MLE}$\", \"$\\\\hat{p}_{JS}$\", \n                           \"$\\\\hat{p}_{Bayes}$\", \"$p_{true}$\"),\n             align = \"c\",\n             caption = \"Comparison of Estimated Strike Rates (Sorted by Pre-season Performance)\")\n\n\n\n\nTable 4.3: Comparison of Estimated Strike Rates (Sorted by Pre-season Performance)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(\\hat{p}_{MLE}\\)\n\\(\\hat{p}_{JS}\\)\n\\(\\hat{p}_{Bayes}\\)\n\\(p_{true}\\)\n\n\n\n\n13\n0.028\n0.048\n0.071\n0.050\n\n\n5\n0.043\n0.058\n0.074\n0.074\n\n\n3\n0.054\n0.069\n0.083\n0.088\n\n\n7\n0.033\n0.045\n0.058\n0.069\n\n\n10\n0.033\n0.045\n0.058\n0.063\n\n\n12\n0.045\n0.058\n0.070\n0.068\n\n\n4\n0.083\n0.094\n0.106\n0.071\n\n\n11\n0.061\n0.068\n0.075\n0.057\n\n\n9\n0.038\n0.043\n0.048\n0.067\n\n\n14\n0.078\n0.078\n0.077\n0.053\n\n\n6\n0.095\n0.087\n0.081\n0.079\n\n\n17\n0.103\n0.088\n0.074\n0.061\n\n\n16\n0.053\n0.037\n0.025\n0.042\n\n\n15\n0.071\n0.052\n0.037\n0.051\n\n\n1\n0.121\n0.098\n0.079\n0.138\n\n\n2\n0.153\n0.117\n0.088\n0.103\n\n\n8\n0.185\n0.129\n0.085\n0.066\n\n\n\n\n\n\n\n\nPlots of Errors (Probability Scale)\n\n\nCode\n# 1. Calculate Squared Errors using the SORTED probability dataframe\nerr_p_mle  &lt;- (df_prob_sorted$p_mle - df_prob_sorted$p_true)^2\nerr_p_js   &lt;- (df_prob_sorted$p_js - df_prob_sorted$p_true)^2\nerr_p_brms &lt;- (df_prob_sorted$p_bayes - df_prob_sorted$p_true)^2\n\n# 2. Calculate Sum of Squared Errors (SSE)\nsse_p_mle  &lt;- sum(err_p_mle)\nsse_p_js   &lt;- sum(err_p_js)\nsse_p_brms &lt;- sum(err_p_brms)\n\n# 3. Plotting\ny_max_p &lt;- max(c(err_p_mle, err_p_js, err_p_brms))\n\nplot(1:17, err_p_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(p) - p[true])^2),\n     main = \"Error Comparison on Probability Scale (Sorted)\",\n     ylim = c(0, y_max_p))\n\nlines(1:17, err_p_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, err_p_brms, type = \"b\", pch = 17, col = \"red\")\n\ngrid()\nlegend(\"topleft\", \n       title = \"Sum Squared Error\",\n       legend = c(paste0(\"MLE: \", round(sse_p_mle, 4)), \n                  paste0(\"JS: \", round(sse_p_js, 4)), \n                  paste0(\"Bayes: \", round(sse_p_brms, 4))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 4.4: Squared Error Comparison on Probability Scale (Sorted)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#error-analysis-on-probability-scale",
    "href": "bayesian.html#error-analysis-on-probability-scale",
    "title": "3  Bayesian Methods",
    "section": "4.6 Error Analysis on Probability Scale",
    "text": "4.6 Error Analysis on Probability Scale\n\n4.6.1 Recovering Estimated Probabilities (\\(\\hat{p}_i\\))\nOnce we have obtained an estimate \\(\\hat{\\mu}_i\\) (whether via James-Stein or Bayesian MCMC), we often want to interpret the result as a probability (strike rate). We invert the variance-stabilizing transformation to recover the estimated probability \\(\\hat{p}_i^{\\text{season}}\\):\n\\[\n\\hat{p}_i = \\frac{1}{2} \\left( \\sin\\left( \\frac{\\hat{\\mu}_i}{\\sqrt{n_i}} \\right) + 1 \\right)\n\\]\nThis inverse transformation maps the unbounded normal parameter back to the \\((0, 1)\\) interval, providing the predicted probability that player \\(i\\) hits a home run in any given at-bat during the regular season.\nWe apply the inverse transformation to convert the estimated \\(\\mu\\) values back to probabilities (\\(p\\)). We then calculate the sum of squared errors relative to the true season strike rates.\nFull Comparison of Estimates (Probability Scale)\n\n\nCode\n# 1. Define Inverse Transformation\ninv_trans &lt;- function(mu, n) {\n  0.5 * (sin(mu / sqrt(n)) + 1)\n}\n\n# 2. Convert Sorted Estimates to Probability Scale\n# We use the 'ni' column from df_sorted to ensure the correct N is used for each player\ndf_prob_sorted &lt;- data.frame(\n  Player = df_sorted$Player,\n  p_mle   = inv_trans(df_sorted$x_i, df_sorted$ni),\n  p_js    = inv_trans(df_sorted$mu_js, df_sorted$ni),\n  p_bayes = inv_trans(df_sorted$mu_bayes, df_sorted$ni),\n  p_true  = inv_trans(df_sorted$mu_true, df_sorted$ni)\n)\n\n# 3. Create display table\ndf_display_p &lt;- df_prob_sorted\ndf_display_p[, 2:5] &lt;- round(df_display_p[, 2:5], 3)\n\nknitr::kable(df_display_p,\n             row.names = FALSE,\n             col.names = c(\"Player\", \"$\\\\hat{p}_{MLE}$\", \"$\\\\hat{p}_{JS}$\", \n                           \"$\\\\hat{p}_{Bayes}$\", \"$p_{true}$\"),\n             align = \"c\",\n             caption = \"Comparison of Estimated Strike Rates (Sorted by Pre-season Performance)\")\n\n\n\n\nTable 4.3: Comparison of Estimated Strike Rates (Sorted by Pre-season Performance)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(\\hat{p}_{MLE}\\)\n\\(\\hat{p}_{JS}\\)\n\\(\\hat{p}_{Bayes}\\)\n\\(p_{true}\\)\n\n\n\n\n13\n0.028\n0.048\n0.071\n0.050\n\n\n5\n0.043\n0.058\n0.074\n0.074\n\n\n3\n0.054\n0.069\n0.083\n0.088\n\n\n7\n0.033\n0.045\n0.058\n0.069\n\n\n10\n0.033\n0.045\n0.058\n0.063\n\n\n12\n0.045\n0.058\n0.070\n0.068\n\n\n4\n0.083\n0.094\n0.106\n0.071\n\n\n11\n0.061\n0.068\n0.075\n0.057\n\n\n9\n0.038\n0.043\n0.048\n0.067\n\n\n14\n0.078\n0.078\n0.077\n0.053\n\n\n6\n0.095\n0.087\n0.081\n0.079\n\n\n17\n0.103\n0.088\n0.074\n0.061\n\n\n16\n0.053\n0.037\n0.025\n0.042\n\n\n15\n0.071\n0.052\n0.037\n0.051\n\n\n1\n0.121\n0.098\n0.079\n0.138\n\n\n2\n0.153\n0.117\n0.088\n0.103\n\n\n8\n0.185\n0.129\n0.085\n0.066\n\n\n\n\n\n\n\n\nPlots of Errors (Probability Scale)\n\n\nCode\n# 1. Calculate Squared Errors using the SORTED probability dataframe\nerr_p_mle  &lt;- (df_prob_sorted$p_mle - df_prob_sorted$p_true)^2\nerr_p_js   &lt;- (df_prob_sorted$p_js - df_prob_sorted$p_true)^2\nerr_p_brms &lt;- (df_prob_sorted$p_bayes - df_prob_sorted$p_true)^2\n\n# 2. Calculate Sum of Squared Errors (SSE)\nsse_p_mle  &lt;- sum(err_p_mle)\nsse_p_js   &lt;- sum(err_p_js)\nsse_p_brms &lt;- sum(err_p_brms)\n\n# 3. Plotting\ny_max_p &lt;- max(c(err_p_mle, err_p_js, err_p_brms))\n\nplot(1:17, err_p_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(p) - p[true])^2),\n     main = \"Error Comparison on Probability Scale (Sorted)\",\n     ylim = c(0, y_max_p))\n\nlines(1:17, err_p_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, err_p_brms, type = \"b\", pch = 17, col = \"red\")\n\ngrid()\nlegend(\"topleft\", \n       title = \"Sum Squared Error\",\n       legend = c(paste0(\"MLE: \", round(sse_p_mle, 4)), \n                  paste0(\"JS: \", round(sse_p_js, 4)), \n                  paste0(\"Bayes: \", round(sse_p_brms, 4))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 4.4: Squared Error Comparison on Probability Scale (Sorted)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#predictive-distributions-and-posterior-means",
    "href": "bayesian.html#predictive-distributions-and-posterior-means",
    "title": "3  Bayesian Methods",
    "section": "4.6 Predictive Distributions and Posterior Means",
    "text": "4.6 Predictive Distributions and Posterior Means\nA key advantage of the fully Bayesian approach is the ability to generate a predictive distribution for the season outcome.\n\nDefinition 4.2 (Bayesian Predictive Estimate (Posterior Mean of \\(p\\))) In the previous section, we estimated \\(\\hat{p}_{Bayes}\\) by transforming the posterior mean of \\(\\mu\\) (the “Plug-in” estimator): \\[ \\hat{p}_{plug-in} = g^{-1}(E[\\mu | \\text{data}]) \\]\nHowever, because the inverse transformation \\(g^{-1}\\) is non-linear, this plug-in estimator is biased. A more robust approach is to compute the Predicted Season Outcome by averaging over the entire posterior distribution:\n\nFor each MCMC sample \\(t\\), convert \\(\\mu_i^{(t)}\\) to probability: \\(p_i^{(t)} = g^{-1}(\\mu_i^{(t)})\\).\nPredict the season home runs: \\(Y_i^{(t)} = N_i p_i^{(t)}\\).\nAverage these predictions to get the expected season count: \\(\\hat{Y}_i = \\frac{1}{T} \\sum Y_i^{(t)}\\).\nConvert back to a rate: \\(\\hat{p}_{pred} = \\hat{Y}_i / N_i\\).\n\nThis quantity \\(\\hat{p}_{pred}\\) approximates \\(E[p | \\text{data}]\\), the true posterior mean of the probability.\n\n\n\n\nTable 4.4\n\n\n\nCode\n# 1. Prepare Variables from Previous Chunks\n# We assume 'baseball_data', 'mu_js', and 'mu_brms' exist from previous steps.\n# 'mu_brms' is the posterior mean of mu from the brms model.\n\n# Inverse Transformation Function\ninv_trans &lt;- function(mu, n) { 0.5 * (sin(mu / sqrt(n)) + 1) }\n\n# 2. Calculate Method 3: Bayes (Plug-in)\n# Transform the posterior mean of mu directly\np_hat_plugin &lt;- inv_trans(mu_brms, baseball_data$Pre_AtBats)\n\n# 3. Calculate Method 4: Bayes (Predictive)\n# We simulate the posterior integration. We approximate the posterior variance \n# using the shrinkage estimate derived in the James-Stein step.\np_hat_pred &lt;- numeric(17)\nshrinkage_est &lt;- 1 - 14/sum((baseball_data$x - mean(baseball_data$x))^2)\n\nfor(i in 1:17) {\n  # A. Simulate posterior samples for mu_i\n  # Mean = mu_brms[i], Variance approx (1 - shrinkage)\n  sim_mus &lt;- rnorm(5000, mean = mu_brms[i], sd = sqrt(shrinkage_est)) \n  \n  # B. Convert samples to probability p using Pre-season N (scaling factor)\n  sim_ps &lt;- inv_trans(sim_mus, baseball_data$Pre_AtBats[i])\n  \n  # C. Predict Season Home Runs (Expected Y given p and Season N)\n  sim_Ys &lt;- sim_ps * baseball_data$Season_AtBats[i]\n  \n  # D. Average Y to get Expected Season Count\n  Y_hat &lt;- mean(sim_Ys)\n  \n  # E. Calculate p_hat_predictive\n  p_hat_pred[i] &lt;- Y_hat / baseball_data$Season_AtBats[i]\n}\n\n# 4. Combine Estimates into Data Frame\ndf_four_methods &lt;- data.frame(\n  Player = baseball_data$Player,\n  p_mle = inv_trans(baseball_data$x, baseball_data$Pre_AtBats),\n  p_js  = inv_trans(mu_js, baseball_data$Pre_AtBats),\n  p_plugin = p_hat_plugin,\n  p_pred   = p_hat_pred,\n  p_true   = baseball_data$p_season,\n  x_i      = baseball_data$x # for sorting\n)\n\n# 5. Sort by pre-season performance (x_i)\ndf_four_sorted &lt;- df_four_methods[order(df_four_methods$x_i), ]\n\n\n\n\n\n4.6.1 Comparison of Four Estimates (Probability Scale)\nThe table below compares the estimated season strike rates. Note the difference between \\(\\hat{p}_{Plug-in}\\) (transforming the mean \\(\\mu\\)) and \\(\\hat{p}_{Pred}\\) (the predictive average).\n\n\nCode\ndf_display_four &lt;- df_four_sorted\ndf_display_four[, 2:7] &lt;- round(df_display_four[, 2:7], 3)\n\nknitr::kable(df_display_four[, c(\"Player\", \"p_mle\", \"p_js\", \"p_plugin\", \"p_pred\", \"p_true\")],\n             row.names = FALSE,\n             col.names = c(\"Player\", \"MLE\", \"James-Stein\", \"Bayes (Plug-in)\", \"Bayes (Pred)\", \"True Rate\"),\n             align = \"c\",\n             caption = \"Comparison of Probability Estimates (Sorted by Pre-season Performance)\")\n\n\n\n\nTable 4.5: Comparison of Probability Estimates (Sorted by Pre-season Performance)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\nMLE\nJames-Stein\nBayes (Plug-in)\nBayes (Pred)\nTrue Rate\n\n\n\n\n13\n0.028\n0.048\n0.071\n0.073\n0.050\n\n\n5\n0.043\n0.058\n0.074\n0.075\n0.074\n\n\n3\n0.054\n0.069\n0.083\n0.085\n0.088\n\n\n7\n0.033\n0.045\n0.058\n0.060\n0.069\n\n\n10\n0.033\n0.045\n0.058\n0.060\n0.063\n\n\n12\n0.045\n0.058\n0.070\n0.072\n0.068\n\n\n4\n0.083\n0.094\n0.106\n0.107\n0.071\n\n\n11\n0.061\n0.068\n0.075\n0.077\n0.057\n\n\n9\n0.038\n0.043\n0.048\n0.052\n0.067\n\n\n14\n0.078\n0.078\n0.077\n0.080\n0.053\n\n\n6\n0.095\n0.087\n0.081\n0.083\n0.079\n\n\n17\n0.103\n0.088\n0.074\n0.077\n0.061\n\n\n16\n0.053\n0.037\n0.025\n0.030\n0.042\n\n\n15\n0.071\n0.052\n0.037\n0.041\n0.051\n\n\n1\n0.121\n0.098\n0.079\n0.081\n0.138\n\n\n2\n0.153\n0.117\n0.088\n0.090\n0.103\n\n\n8\n0.185\n0.129\n0.085\n0.088\n0.066\n\n\n\n\n\n\n\n\n\n\n4.6.2 Error Analysis of the Four Methods\nWe calculate the Sum of Squared Errors (SSE) for all four approaches on the probability scale.\n\n\nCode\n# Calculate Squared Errors\nerr_mle    &lt;- (df_four_sorted$p_mle - df_four_sorted$p_true)^2\nerr_js     &lt;- (df_four_sorted$p_js - df_four_sorted$p_true)^2\nerr_plugin &lt;- (df_four_sorted$p_plugin - df_four_sorted$p_true)^2\nerr_pred   &lt;- (df_four_sorted$p_pred - df_four_sorted$p_true)^2\n\n# Sum of Squared Errors\nsse_mle    &lt;- sum(err_mle)\nsse_js     &lt;- sum(err_js)\nsse_plugin &lt;- sum(err_plugin)\nsse_pred   &lt;- sum(err_pred)\n\n# Plotting\ny_max &lt;- max(c(err_mle, err_js, err_plugin, err_pred))\n\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(p) - p[true])^2),\n     main = \"Error Comparison (Probability Scale)\",\n     ylim = c(0, y_max))\n\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, err_plugin, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, err_pred, type = \"b\", pch = 15, col = \"darkgreen\")\n\ngrid()\nlegend(\"topright\", \n       title = \"Sum Squared Error\",\n       legend = c(paste0(\"MLE: \", round(sse_mle, 4)), \n                  paste0(\"JS: \", round(sse_js, 4)), \n                  paste0(\"Bayes (Plug-in): \", round(sse_plugin, 4)),\n                  paste0(\"Bayes (Pred): \", round(sse_pred, 4))),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\"), \n       pch = c(1, 19, 17, 15), \n       lty = c(2, 1, 1, 1),\n       cex = 0.8)\n\n\n\n\n\n\n\n\nFigure 4.5: Squared Error Comparison: Four Methods",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#bayes-rules",
    "href": "bayesian.html#bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.2 Bayes Rules",
    "text": "3.2 Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) satisfies: \\[\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n\\]\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nTherefore, to minimize the Bayes risk, one just need to choose \\(d(x)\\) to minimize the posterior expected loss for each \\(x\\).\n\n\nThe following diagram summarizes the general workflow for deriving a Bayes estimator:\n\n\n\n\n\n\n\n\nFigure 3.3: Workflow for Finding the Bayes Rule\n\n\n\n\n\n\n3.2.1 Common Loss Functions and Bayes Estimators\n\n3.2.1.1 Squared Error Loss (point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize the posterior expected loss \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting it to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule under squared error loss is the posterior mean.\n\n\n3.2.1.2 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nTo find the Bayes rule, we minimize the posterior expected loss:\n\\[\n\\psi(d) = E_{\\theta|x} [ |\\theta - d| ] = \\int_{-\\infty}^{\\infty} |\\theta - d| \\, dF(\\theta|x)\n\\]\nwhere \\(F(\\theta|x)\\) is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point \\(d\\):\n\\[\n\\psi(d) = \\int_{-\\infty}^{d} (d - \\theta) \\, dF(\\theta|x) + \\int_{d}^{\\infty} (\\theta - d) \\, dF(\\theta|x)\n\\]\nWe find the minimum by analyzing the rate of change of \\(\\psi(d)\\) with respect to \\(d\\). Differentiating (or taking the subgradient for non-differentiable points):\n\\[\n\\frac{d}{dd} \\psi(d) = \\int_{-\\infty}^{d} 1 \\, dF(\\theta|x) - \\int_{d}^{\\infty} 1 \\, dF(\\theta|x) = P(\\theta \\le d|x) - P(\\theta &gt; d|x)\n\\]\nSetting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:\n\\[\nP(\\theta \\le d|x) = P(\\theta &gt; d|x)\n\\]\nSince the total probability is 1, this condition simplifies to finding \\(d\\) such that the cumulative probability is \\(1/2\\).\nGeneral Case (Discrete or Mixed Distributions)\nIn cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to \\(F(d) = 0.5\\) may not exist. To generalize, the Bayes rule is defined as any median \\(m\\) of the posterior distribution.\nA median is formally defined as any value \\(m\\) that satisfies the following two conditions simultaneously:\n\n\\(P(\\theta \\le m|x) \\ge \\frac{1}{2}\\)\n\\(P(\\theta \\ge m|x) \\ge \\frac{1}{2}\\)\n\nResult: The Bayes rule under absolute error loss is the posterior median.\n\n\n3.2.1.3 Hypothesis Testing (0-1 Loss)\nConsider the hypothesis test \\(H_0: \\theta \\in \\Theta_0\\) versus \\(H_1: \\theta \\in \\Theta_1\\). We define the decision space as \\(\\mathcal{A} = \\{0, 1\\}\\), where \\(a=0\\) means accepting \\(H_0\\) and \\(a=1\\) means rejecting \\(H_0\\) (accepting \\(H_1\\)).\nCase 1: 0-1 Loss\nThe standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\ (\\text{Correct } H_0) \\\\ 1 & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\ (\\text{Correct } H_1) \\end{cases}\\]\nTo find the Bayes rule, we minimize the posterior expected loss for a given \\(x\\), denoted as \\(E_{\\theta|x}[L(\\theta, a)]\\).\n\nExpected Loss for choosing \\(a=0\\) (Accept \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 0)] = 0 \\cdot P(\\theta \\in \\Theta_0|x) + 1 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_1|x)\n  \\]\nExpected Loss for choosing \\(a=1\\) (Reject \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 1)] = 1 \\cdot P(\\theta \\in \\Theta_0|x) + 0 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_0|x)\n  \\]\n\nThe Bayes rule selects the action with the smaller expected loss. Thus, we choose \\(a=1\\) if: \\[\nP(\\theta \\in \\Theta_0|x) \\le P(\\theta \\in \\Theta_1|x)\n\\] This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.\nCase 2: General Loss (Asymmetric Costs)\nIn many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let \\(c_1\\) be the cost of a Type I error and \\(c_2\\) be the cost of a Type II error. Usually, we normalize one cost to 1.\nSuppose the loss function is: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\\\ c & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Cost of Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Cost of Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\end{cases}\\]\nWe again calculate the posterior expected loss:\n\nExpected Loss for \\(a=0\\): \\[E[L(\\theta, 0)|x] = 0 \\cdot P(\\Theta_0|x) + 1 \\cdot P(\\Theta_1|x) = P(\\Theta_1|x)\\]\nExpected Loss for \\(a=1\\): \\[E[L(\\theta, 1)|x] = c \\cdot P(\\Theta_0|x) + 0 \\cdot P(\\Theta_1|x) = c P(\\Theta_0|x)\\]\n\nWe reject \\(H_0\\) (\\(a=1\\)) if the expected loss of doing so is lower: \\[\nc P(\\Theta_0|x) \\le P(\\Theta_1|x)\n\\]\nSince \\(P(\\Theta_1|x) = 1 - P(\\Theta_0|x)\\), we can rewrite this condition as: \\[\nc P(\\Theta_0|x) \\le 1 - P(\\Theta_0|x) \\implies (1+c) P(\\Theta_0|x) \\le 1\n\\] \\[\nP(\\Theta_0|x) \\le \\frac{1}{1+c}\n\\]\nResult: With asymmetric costs, we accept \\(H_1\\) only if the posterior probability of the null hypothesis is sufficiently small (below the threshold \\(\\frac{1}{1+c}\\)). If the cost of false rejection \\(c\\) is high, we require stronger evidence against \\(H_0\\).\n\n\n3.2.1.4 Classification Prediction (categorical Parameter)\nIn classification problems, the parameter of interest is a discrete class label \\(\\theta\\) (often denoted as \\(y\\)) taking values in a set of categories \\(\\{1, 2, \\dots, K\\}\\). The goal is to predict the true class label based on observed features \\(x\\).\nWe typically employ the 0-1 loss function, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:\n\\[L(\\theta, \\hat{\\theta}) = \\begin{cases} 0 & \\text{if } \\hat{\\theta} = \\theta \\ (\\text{Correct Classification}) \\\\ 1 & \\text{if } \\hat{\\theta} \\neq \\theta \\ (\\text{Misclassification}) \\end{cases}\\]\nTo find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta} L(\\theta, \\hat{\\theta}) \\pi(\\theta|x)\n\\]\nSince the loss is 1 only when the predicted class \\(\\hat{\\theta}\\) differs from the true class \\(\\theta\\), this sum simplifies to:\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta \\neq \\hat{\\theta}} 1 \\cdot \\pi(\\theta|x) = P(\\theta \\neq \\hat{\\theta} | x) = 1 - P(\\theta = \\hat{\\theta} | x)\n\\]\nMinimizing the misclassification rate \\(1 - P(\\theta = \\hat{\\theta} | x)\\) is mathematically equivalent to maximizing the probability of being correct, \\(P(\\theta = \\hat{\\theta} | x)\\).\nResult: The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the Maximum A Posteriori (MAP) estimator, in the context of machine learning and pattern recognition, this decision rule is known as the Bayes Optimal Classifier.\n\\[\n\\hat{\\theta}_{\\text{Bayes}}(x) = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg\\max} \\ P(\\theta = k | x)\n\\]\n\n\n3.2.1.5 Interval Estimation and Highest Posterior Density (HPD)\nWe can motivate the choice of a Credible Interval by defining a specific loss function for interval estimation. Suppose we seek an estimate \\(d\\) and specify a tolerance \\(\\delta &gt; 0\\). We define the loss function as:\n\\[\nL(\\theta, d) = \\begin{cases}\n0 & \\text{if } |\\theta - d| \\le \\delta \\\\\n1 & \\text{if } |\\theta - d| &gt; \\delta\n\\end{cases}\n\\]\nThe Expected Posterior Loss is the posterior probability that \\(\\theta\\) lies outside the interval \\((d - \\delta, d + \\delta)\\). Therefore, minimizing the loss is equivalent to finding the interval of fixed length \\(2\\delta\\) that maximizes the posterior probability:\n\\[ P(d - \\delta \\le \\theta \\le d + \\delta \\mid x) \\]\nIn practice, we typically reverse this formulation: instead of fixing the length \\(2\\delta\\), we fix the coverage probability \\(1-\\alpha\\) (e.g., 0.95) and seek the interval with the shortest possible length. This results in the Highest Posterior Density (HPD) interval, defined as the region where the posterior density exceeds a certain threshold \\(c\\):\n\\[ C_{HPD} = \\{ \\theta : \\pi(\\theta \\mid x) \\ge c \\} \\]\nThis HPD interval is optimal because it includes the “most likely” values of \\(\\theta\\) and, for a unimodal distribution, provides the narrowest interval for a given confidence level.\nComparison with Equal-Tailed Intervals:\n\nEqual-Tailed Interval: We simply cut off \\(\\alpha/2\\) probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.\nHPD Interval: This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.\n\nThe plot below illustrates a skewed posterior distribution (Gamma). Notice how the HPD Interval (Blue) is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the Equal-Tailed Interval (Red).\n\n\nCode\n## Define a Skewed Distribution: Gamma(shape=2, Rate=0.5)\nx_vals &lt;- seq(0, 15, length.out = 1000)\ny_vals &lt;- dgamma(x_vals, shape = 2, rate = 0.5)\n\n## Target Coverage\nalpha &lt;- 0.10\ntarget_prob &lt;- 1 - alpha\n\n## 1. Equal-tailed Interval (quantiles)\neq_lower &lt;- qgamma(alpha/2, shape = 2, rate = 0.5)\neq_upper &lt;- qgamma(1 - alpha/2, shape = 2, rate = 0.5)\n\n## 2. HPD Interval (density Threshold Optimization)\n## We Look for a Density Threshold K Such That the Area Above K Is 0.90\nfind_hpd &lt;- function(dist_vals, density_vals, probability) {\n  ## Sort density values\n  ord &lt;- order(density_vals, decreasing = TRUE)\n  sorted_dens &lt;- density_vals[ord]\n  sorted_dist &lt;- dist_vals[ord]\n  \n  ## Accumulate probability (approximation)\n  dx &lt;- diff(dist_vals)[1]\n  cum_prob &lt;- cumsum(sorted_dens * dx)\n  \n  ## Find cutoff index\n  cutoff_idx &lt;- which(cum_prob &gt;= probability)[1]\n  \n  ## Get the subset of x values\n  hpd_set &lt;- sorted_dist[1:cutoff_idx]\n  return(c(min(hpd_set), max(hpd_set)))\n}\n\nhpd_bounds &lt;- find_hpd(x_vals, y_vals, target_prob)\nhpd_lower &lt;- hpd_bounds[1]\nhpd_upper &lt;- hpd_bounds[2]\n\n## Plotting\nplot(x_vals, y_vals, type = 'l', lwd = 2, col = \"black\",\n     main = \"90% Credible Intervals (Skewed Posterior)\",\n     xlab = expression(theta), ylab = \"Density\",\n     ylim = c(0, max(y_vals) * 1.2))\n\n## Shade HPD\npolygon(c(x_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], hpd_upper, hpd_lower),\n        c(y_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], 0, 0),\n        col = rgb(0, 0, 1, 0.2), border = NA)\n\n## Draw Equal-tailed Lines (red)\nabline(v = c(eq_lower, eq_upper), col = \"red\", lwd = 2, lty = 2)\n## Draw HPD Lines (blue)\nabline(v = c(hpd_lower, hpd_upper), col = \"blue\", lwd = 2, lty = 1)\n\nlegend(\"topright\", \n       legend = c(\"Posterior Density\", \n                  paste0(\"Equal-Tailed (Len: \", round(eq_upper - eq_lower, 2), \")\"), \n                  paste0(\"HPD (Len: \", round(hpd_upper - hpd_lower, 2), \")\")),\n       col = c(\"black\", \"red\", \"blue\"), \n       lty = c(1, 2, 1), lwd = 2,\n       fill = c(NA, NA, rgb(0, 0, 1, 0.2)), border = NA)\n\n\n\n\n\n\n\n\nFigure 3.4: Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution\n\n\n\n\n\n\n\n\n3.2.2 Constant Risk Bayes Estimator Is Minimax\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Bayes Estimator Is Minimax) Let \\(\\delta^\\pi\\) be a Bayes estimator with respect to a prior \\(\\pi\\). If the risk function of \\(\\delta^\\pi\\) is constant on the parameter space \\(\\Theta\\), such that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), then \\(\\delta^\\pi\\) is a minimax estimator.\n\n\nProof. Let \\(\\delta^\\pi\\) be the Bayes estimator with constant risk \\(c\\). First, we compute its Bayes risk \\(r(\\pi, \\delta^\\pi)\\). Since the risk is constant:\n\\[\nr(\\pi, \\delta^\\pi) = \\int_\\Theta R(\\theta, \\delta^\\pi) \\pi(\\theta) d\\theta = \\int_\\Theta c \\, \\pi(\\theta) d\\theta = c\n\\]\nNow, let \\(\\delta'\\) be any arbitrary estimator. By the definition of a Bayes estimator, \\(\\delta^\\pi\\) minimizes the Bayes risk among all estimators:\n\\[\nr(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta')\n\\]\nNext, we observe that the Bayes risk of \\(\\delta'\\) is the expectation of its risk function with respect to the prior \\(\\pi\\). An average cannot exceed the maximum value of the function being averaged (the supremum):\n\\[\nr(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') \\cdot \\int_\\Theta \\pi(\\theta) d\\theta = \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nCombining these inequalities, we have:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta^\\pi) = c = r(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta') \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nSince \\(\\sup_\\theta R(\\theta, \\delta^\\pi) \\le \\sup_\\theta R(\\theta, \\delta')\\) holds for any estimator \\(\\delta'\\), \\(\\delta^\\pi\\) minimizes the maximum risk. Therefore, it is minimax.\n\nThe plot below visualizes the logic of the proof. The red line represents the Constant Risk Bayes Estimator (\\(\\delta^\\pi\\)), which has a constant height \\(c\\). The blue curve represents an Arbitrary Estimator (\\(\\delta'\\)).\nBecause \\(\\delta^\\pi\\) minimizes the weighted average risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk (\\(\\sup R\\)) strictly greater than or equal to \\(c\\). Thus, the constant risk estimator has the lowest possible maximum.\n\n\nCode\n# Define Parameter Space Theta\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# 1. Constant Risk Bayes Estimator (risk = C)\nc_val &lt;- 0.5\nrisk_bayes &lt;- rep(c_val, length(theta))\n\n# 2. Arbitrary Alternative Estimator\n# This Function Is Chosen Such That It Dips Below C but Rises Above It Elsewhere\nrisk_alt &lt;- c_val + 0.2 * sin(2 * pi * theta) - 0.05\n\n# Plotting\nplot(theta, risk_alt, type = 'l', lwd = 2, col = \"blue\",\n     ylim = c(0, 1), ylab = \"Risk R(theta, d)\", xlab = expression(theta),\n     main = \"Geometry of the Minimax Theorem\")\n\n# Add Constant Risk Line\nlines(theta, risk_bayes, col = \"red\", lwd = 2)\n\n# Mark the Maximum of the Alternative\nmax_alt &lt;- max(risk_alt)\nmax_theta &lt;- theta[which.max(risk_alt)]\npoints(max_theta, max_alt, pch = 19, col = \"blue\")\ntext(max_theta, max_alt, labels = expression(sup~R(theta, delta^\"'\")), pos = 3, col = \"blue\")\n\n# Label the Constant Risk\ntext(0.1, c_val, labels = expression(R(theta, delta^pi) == c), pos = 3, col = \"red\")\n\n# Add Legend\nlegend(\"bottomright\", legend = c(\"Arbitrary Estimator\", \"Constant Risk Bayes Est.\"),\n       col = c(\"blue\", \"red\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nFigure 3.5: Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk.\n\n\n\n\n\n\nExample 3.5 (Binomial Minimax Estimator) Let \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#bayesian-predictive-distributions",
    "href": "bayesian.html#bayesian-predictive-distributions",
    "title": "3  Bayesian Methods",
    "section": "3.7 Bayesian Predictive Distributions",
    "text": "3.7 Bayesian Predictive Distributions\nA key feature of Bayesian analysis is the ability to make inference about future observations, rather than just the model parameters. The posterior predictive distribution describes the probability of observing a new data point \\(y^*\\) given the observed data \\(y\\).\n\nDefinition 3.3 (Posterior Predictive Distribution) Let \\(f(y^*|\\theta)\\) be the sampling distribution of a future observation \\(y^*\\) given parameter \\(\\theta\\), and let \\(\\pi(\\theta|y)\\) be the posterior distribution of \\(\\theta\\) given observed data \\(y\\). The posterior predictive density is obtained by marginalizing over the parameter \\(\\theta\\):\n\\[\nf(y^*|y) = \\int_\\Theta f(y^*|\\theta) \\pi(\\theta|y) \\, d\\theta\n\\]\n\nThis distribution incorporates two distinct sources of uncertainty:\n\nSampling Uncertainty (Aleatoric): The inherent variability of the data generation process, represented by the variance in \\(f(y^*|\\theta)\\).\nParameter Uncertainty (Epistemic): The uncertainty regarding the true value of \\(\\theta\\), represented by the variance in the posterior \\(\\pi(\\theta|y)\\).\n\nAs sample size \\(n \\to \\infty\\), the parameter uncertainty vanishes (the posterior approaches a point mass), and the predictive distribution converges to the true data-generating distribution.\n\nExample 3.8 (Normal-Normal Predictive Distribution) Consider a case where the data \\(y_1, \\dots, y_n\\) are independent and normally distributed with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\):\n\\[\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\\]\nAssume a conjugate prior for the mean: \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\). The posterior distribution is \\(\\mu|y \\sim N(\\mu_n, \\sigma_n^2)\\), where \\(\\mu_n\\) and \\(\\sigma_n^2\\) are the updated posterior hyperparameters.\nThe predictive distribution for a new observation \\(y^*\\) is derived as:\n\\[\n\\begin{aligned}\nf(y^*|y) &= \\int_{-\\infty}^{\\infty} f(y^*|\\mu) \\pi(\\mu|y) \\, d\\mu \\\\\n&= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y^*-\\mu)^2}{2\\sigma^2}} \\times \\frac{1}{\\sqrt{2\\pi\\sigma_n^2}} e^{-\\frac{(\\mu-\\mu_n)^2}{2\\sigma_n^2}} \\, d\\mu\n\\end{aligned}\n\\]\nThis convolution of two Gaussians results in a new Gaussian distribution:\n\\[\ny^* | y \\sim N(\\mu_n, \\sigma^2 + \\sigma_n^2)\n\\]\nHere, the total predictive variance is the sum of the data variance (\\(\\sigma^2\\)) and the posterior uncertainty about the mean (\\(\\sigma_n^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes-rule",
    "href": "bayesian.html#empirical-bayes-rule",
    "title": "3  Bayesian Methods",
    "section": "3.4 Empirical Bayes Rule",
    "text": "3.4 Empirical Bayes Rule\nThe James-Stein estimator provides a natural entry point into the concept of Empirical Bayes (EB). While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.\n\n3.4.1 The General Empirical Bayes Framework\nIn a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to “learn” the prior.\nThe workflow typically follows these steps:\n\nHierarchical Model: We assume the data \\(X\\) comes from a distribution \\(f(x|\\theta)\\), and the parameter \\(\\theta\\) comes from a prior \\(\\pi(\\theta|\\eta)\\) controlled by hyperparameters \\(\\eta\\).\nMarginal Likelihood (Evidence): We integrate out the parameter \\(\\theta\\) to obtain the marginal distribution of the data given the hyperparameters: \\[m(x|\\eta) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\\]\nEstimation of Hyperparameters: Instead of fixing \\(\\eta\\), we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments: \\[\\hat{\\eta} = \\underset{\\eta}{\\arg\\max} \\ m(x|\\eta)\\]\nPosterior Inference: We proceed with standard Bayesian inference, but we substitute the estimated estimate \\(\\hat{\\eta}\\) into the posterior: \\[\\pi(\\theta|x, \\hat{\\eta}) \\propto f(x|\\theta) \\pi(\\theta|\\hat{\\eta})\\]\n\nDiscussion:\n\n“Borrowing Strength”: EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.\nThe Critique: A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate \\(\\theta\\)) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a “hyperprior” on \\(\\eta\\) and integrate it out.\n\n\n\n3.4.2 Deriving James-Stein as Empirical Bayes\nWe can derive the James-Stein rule explicitly using this framework.\nModel:\n\nLikelihood: \\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\). Here, \\(\\tau^2\\) is the unknown hyperparameter.\n\nStep 1: The Ideal Bayes Estimator\nIf we knew \\(\\tau^2\\), the posterior distribution of \\(\\mu_i\\) would be Normal with mean: \\[E(\\mu_i|x_i, \\tau^2) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\] We define the shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\).\nStep 2: Marginal Estimation\nSince \\(\\mu_i\\) and \\(X_i-\\mu_i\\) are independent normals, the marginal distribution of the data is: \\[X_i \\sim N(0, 1+\\tau^2)\\] Consequently, the sum of squares \\(S = ||X||^2 = \\sum X_i^2\\) follows a scaled Chi-squared distribution: \\[S \\sim (1+\\tau^2) \\chi^2_p\\]\nStep 3: Estimating the Shrinkage Factor\nWe need to estimate \\(B = \\frac{1}{1+\\tau^2}\\). Note that the expected value of an inverse Chi-square variable is \\(E[1/\\chi^2_p] = \\frac{1}{p-2}\\). Therefore: \\[E \\left[ \\frac{p-2}{S} \\right] = \\frac{p-2}{1+\\tau^2} E\\left[\\frac{1}{\\chi^2_p}\\right] = \\frac{p-2}{1+\\tau^2} \\cdot \\frac{1}{p-2} = \\frac{1}{1+\\tau^2} = B\\]\nThus, \\(\\hat{B} = \\frac{p-2}{||X||^2}\\) is an unbiased estimator of the optimal shrinkage factor.\nStep 4: The Empirical Bayes Rule\nPlugging \\(\\hat{B}\\) into the ideal Bayes estimator recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\hat{B} \\right) X = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes-rules",
    "href": "bayesian.html#empirical-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.4 Empirical Bayes Rules",
    "text": "3.4 Empirical Bayes Rules\nThe James-Stein estimator provides a natural entry point into the concept of Empirical Bayes (EB). While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.\n\n3.4.1 The General Empirical Bayes Framework\nIn a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to “learn” the prior.\nThe workflow typically follows these steps:\n\nHierarchical Model: We assume the data \\(X\\) comes from a distribution \\(f(x|\\theta)\\), and the parameter \\(\\theta\\) comes from a prior \\(\\pi(\\theta|\\eta)\\) controlled by hyperparameters \\(\\eta\\).\nMarginal Likelihood (Evidence): We integrate out the parameter \\(\\theta\\) to obtain the marginal distribution of the data given the hyperparameters: \\[m(x|\\eta) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\\]\nEstimation of Hyperparameters: Instead of fixing \\(\\eta\\), we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments: \\[\\hat{\\eta} = \\underset{\\eta}{\\arg\\max} \\ m(x|\\eta)\\]\nPosterior Inference: We proceed with standard Bayesian inference, but we substitute the estimated estimate \\(\\hat{\\eta}\\) into the posterior: \\[\\pi(\\theta|x, \\hat{\\eta}) \\propto f(x|\\theta) \\pi(\\theta|\\hat{\\eta})\\]\n\nDiscussion:\n\n“Borrowing Strength”: EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.\nThe Critique: A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate \\(\\theta\\)) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a “hyperprior” on \\(\\eta\\) and integrate it out.\n\n\n\n3.4.2 Deriving James-Stein as Empirical Bayes\nWe can derive the James-Stein rule explicitly using this framework.\nModel:\n\nLikelihood: \\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\). Here, \\(\\tau^2\\) is the unknown hyperparameter.\n\nStep 1: The Ideal Bayes Estimator\nIf we knew \\(\\tau^2\\), the posterior distribution of \\(\\mu_i\\) would be Normal with mean: \\[E(\\mu_i|x_i, \\tau^2) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\] We define the shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\).\nStep 2: Marginal Estimation\nSince \\(\\mu_i\\) and \\(X_i-\\mu_i\\) are independent normals, the marginal distribution of the data is: \\[X_i \\sim N(0, 1+\\tau^2)\\] Consequently, the sum of squares \\(S = ||X||^2 = \\sum X_i^2\\) follows a scaled Chi-squared distribution: \\[S \\sim (1+\\tau^2) \\chi^2_p\\]\nStep 3: Estimating the Shrinkage Factor\nWe need to estimate \\(B = \\frac{1}{1+\\tau^2}\\). Note that the expected value of an inverse Chi-square variable is \\(E[1/\\chi^2_p] = \\frac{1}{p-2}\\). Therefore: \\[E \\left[ \\frac{p-2}{S} \\right] = \\frac{p-2}{1+\\tau^2} E\\left[\\frac{1}{\\chi^2_p}\\right] = \\frac{p-2}{1+\\tau^2} \\cdot \\frac{1}{p-2} = \\frac{1}{1+\\tau^2} = B\\]\nThus, \\(\\hat{B} = \\frac{p-2}{||X||^2}\\) is an unbiased estimator of the optimal shrinkage factor.\nStep 4: The Empirical Bayes Rule\nPlugging \\(\\hat{B}\\) into the ideal Bayes estimator recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\hat{B} \\right) X = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-modeling-via-mcmc",
    "href": "bayesian.html#hierarchical-modeling-via-mcmc",
    "title": "3  Bayesian Methods",
    "section": "3.5 Hierarchical Modeling via MCMC",
    "text": "3.5 Hierarchical Modeling via MCMC\nIn complex Bayesian settings where the posterior distribution cannot be derived analytically, we utilize hierarchical structures to represent levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate the resulting distributions.\n\n3.5.1 Hierarchical Model Structure\nA hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:\n\\[\n\\begin{aligned}\n\\text{Level 1 (Data Likelihood):} & \\quad X_i | \\mu_i, \\sigma^2 \\sim f(x_i | \\mu_i, \\sigma^2) \\\\\n\\text{Level 2 (Parameters):} & \\quad \\mu_i | \\theta, \\tau^2 \\sim \\pi(\\mu_i | \\theta, \\tau^2) \\\\\n\\text{Level 3 (Hyperparameters):} & \\quad \\theta, \\tau^2 \\sim \\pi(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nThe goal is to compute the joint posterior distribution of all unobserved parameters given the data \\(X = \\{X_1, \\dots, X_n\\}\\):\n\\[\np(\\boldsymbol{\\mu}, \\theta, \\tau^2 | X) \\propto \\left[ \\prod_{i=1}^n f(x_i | \\mu_i, \\sigma^2) \\pi(\\mu_i | \\theta, \\tau^2) \\right] \\pi(\\theta, \\tau^2)\n\\]\n\n\n3.5.2 Graphical Model Representation (Tree Structure)\nThe following tree diagram illustrates the conditional dependencies. Note that the parameters \\(\\mu_i\\) are conditionally independent given the hyperparameter \\(\\theta\\), which facilitates “borrowing strength” across groups.\n\n\n\n\n\n\n\n\nFigure 3.6: Hierarchical Tree Structure\n\n\n\n\n\n\n\n3.5.3 MCMC Estimation\nIn hierarchical models, the joint posterior distribution \\(p(\\boldsymbol{\\mu}, \\theta | X)\\) often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use Markov Chain Monte Carlo (MCMC) to draw sequence of samples \\(\\{\\boldsymbol{\\mu}^{(t)}, \\theta^{(t)}\\}\\) that converge to the target posterior distribution.\n\n3.5.3.1 Gibbs Sampling Algorithm\n\nGibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the full conditional distributions. To sample from a target distribution \\(p(\\theta_1, \\theta_2, \\dots, \\theta_k)\\), the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:\n\\[\n\\begin{aligned}\n\\theta_1^{(t+1)} &\\sim p(\\theta_1 | \\theta_2^{(t)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n\\theta_2^{(t+1)} &\\sim p(\\theta_2 | \\theta_1^{(t+1)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n&\\vdots \\\\\n\\theta_k^{(t+1)} &\\sim p(\\theta_k | \\theta_1^{(t+1)}, \\theta_2^{(t+1)}, \\dots, \\theta_{k-1}^{(t+1)})\n\\end{aligned}\n\\]\n\n\nExample 3.7 (Gibbs Sampling for Groups of Normal Data) The Model\nTo apply the general Gibbs sampling framework \\(\\theta_1, \\theta_2, \\dots, \\theta_k\\) to our specific hierarchical model, we identify the variables as follows:\n\nData Observations (\\(X_i\\)): These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school \\(i\\)). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These represent the latent means for each specific group or cluster. In the update step, \\(\\mu_i\\) acts as the first block of variables. It is updated by “compromising” between the local data \\(X_i\\) and the global characteristic \\(\\theta\\).\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all \\(\\mu_i\\) values, effectively “pooling” information from all groups to estimate the overall population center.\n\nGibbs Update in Hierarchical Models\nIn the hierarchical tree structure provided earlier, let our parameter vector be \\((\\mu_i, \\theta)\\). The “orthogonality” of the updates becomes clear when we derive the full conditionals for a Gaussian case:\n\nCase \\(\\theta_1 = \\mu_i\\): Sample \\(\\mu_i^{(t+1)}\\) from \\(p(\\mu_i | X_i, \\theta^{(t)})\\). This is a normal distribution with: \\[\n\\mu_i^{(t+1)} \\sim N\\left( \\frac{\\tau^2 X_i + \\sigma^2 \\theta^{(t)}}{\\sigma^2 + \\tau^2}, \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\right)\n\\]\nCase \\(\\theta_2 = \\theta\\): Sample \\(\\theta^{(t+1)}\\) from \\(p(\\theta | \\boldsymbol{\\mu}^{(t+1)})\\). Assuming a flat prior \\(\\pi(\\theta) \\propto 1\\): \\[\n\\theta^{(t+1)} \\sim N\\left( \\frac{1}{n} \\sum_{i=1}^n \\mu_i^{(t+1)}, \\frac{\\tau^2}{n} \\right)\n\\]\n\n\nVisual Characteristic: Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.\n\n\n3.5.3.2 Metropolis-Hastings (MH) Sampling\nWhen the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step \\(t\\):\n\nPropose: Draw a candidate state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* | \\theta^{(t)})\\).\nAccept/Reject: Calculate the acceptance probability: \\[\n\\alpha = \\min \\left( 1, \\frac{p(\\theta^* | X) q(\\theta^{(t)} | \\theta^*)}{p(\\theta^{(t)} | X) q(\\theta^* | \\theta^{(t)})} \\right)\n\\]\nSet \\(\\theta^{(t+1)} = \\theta^*\\) with probability \\(\\alpha\\); otherwise, set \\(\\theta^{(t+1)} = \\theta^{(t)}\\).\n\nVisual Characteristic: MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.\n\n\nCode\nset.seed(123)\nrho &lt;- 0.8\nlog_target &lt;- function(x, y) { -0.5 * (x^2 - 2*rho*x*y + y^2) / (1 - rho^2) }\n\n# Gibbs Path (Step-wise update)\ngx &lt;- -2; gy &lt;- -2\ngx_path &lt;- gx; gy_path &lt;- gy\nfor(i in 1:25) {\n  gx &lt;- rnorm(1, rho * gy, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx, gx); gy_path &lt;- c(gy_path, gy, gy) # Horizontal move\n  gy &lt;- rnorm(1, rho * gx, sqrt(1 - rho^2))\n  gx_path &lt;- c(gx_path, gx); gy_path &lt;- c(gy_path, gy) # Vertical move\n}\n\n# MH Path (Random Walk)\nmx &lt;- numeric(50); my &lt;- numeric(50)\nmx[1] &lt;- -2; my[1] &lt;- -2\nfor(i in 2:50) {\n  px &lt;- mx[i-1] + rnorm(1, 0, 0.4); py &lt;- my[i-1] + rnorm(1, 0, 0.4)\n  acc &lt;- exp(log_target(px, py) - log_target(mx[i-1], my[i-1]))\n  if(runif(1) &lt; acc) { mx[i] &lt;- px; my[i] &lt;- py } else { mx[i] &lt;- mx[i-1]; my[i] &lt;- my[i-1] }\n}\n\npar(mfrow = c(1, 2))\nt_seq &lt;- seq(-3, 3, length=50); z &lt;- outer(t_seq, t_seq, function(x,y) exp(log_target(x,y)))\nplot(gx_path, gy_path, type=\"l\", col=\"blue\", main=\"Gibbs (Orthogonal Steps)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\nplot(mx, my, type=\"l\", col=\"red\", main=\"Metropolis-Hastings (Random Walk)\", xlab=expression(theta[1]), ylab=expression(theta[2]))\ncontour(t_seq, t_seq, z, add=TRUE, col=\"gray\")\n\n\n\n\n\n\n\n\nFigure 3.7: Comparison of Sampling Paths",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  }
]