[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Inference",
    "section": "Key Features",
    "text": "Key Features",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introstatinf.html",
    "href": "introstatinf.html",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "1.1 Statistical Inference Setup\nWe begin with observations (units) \\(X_1, X_2, \\dots, X_n\\). These may be vectors. We regard these observations as a realization of random variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#statistical-inference-setup",
    "href": "introstatinf.html#statistical-inference-setup",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\n1.1.1 Assumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\nNote on Simplicity:\nThe assumption that observations are identically distributed is made primarily for notational simplicity . There is no fundamental issue in extending this framework to regression settings, where the distribution depends on covariates (e.g., \\(Y_i | X_i\\)), or to other complex structures.\n\nExample 1.1 (Parametric Model) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probability-vs.-statistics",
    "href": "introstatinf.html#probability-vs.-statistics",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probability vs. Statistics",
    "text": "1.2 Probability vs. Statistics\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\):\n\nProbability: \\(\\theta\\) is known. We ask questions about the data \\(X\\).\nStatistics: \\(\\theta\\) is unknown. We use data \\(X_1, \\dots, X_n\\) to infer \\(\\theta\\) (from sample to population).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#types-of-statistical-inference",
    "href": "introstatinf.html#types-of-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 Types of Statistical Inference",
    "text": "1.3 Types of Statistical Inference\nWe can categorize inference into four main types:\n\nDefinition 1.2 (Point Estimation) We use a single number to capture the parameter. \\[\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\]\n\n\nDefinition 1.3 (Interval Estimation) We construct an interval that likely contains the true parameter. \\[\\theta \\in (L(X_1, \\dots, X_n), U(X_1, \\dots, X_n))\\] The true parameter is within this interval.\n\n\nDefinition 1.4 (Hypothesis Testing) We test a specific theory about the parameter. \\[H_0: \\theta = \\theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\neq \\theta_0\\] (Or one-sided alternatives like \\(\\theta &gt; \\theta_0\\)) .\n\n\nDefinition 1.5 (Predictive Inference) Given observed data \\((X_1, Y_1), \\dots, (X_n, Y_n)\\), we want to predict a new observation \\(Y_{n+1}\\) given \\(X_{n+1}\\). This is often the primary goal in Machine Learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#standard-paradigms-for-inference",
    "href": "introstatinf.html#standard-paradigms-for-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Standard Paradigms for Inference",
    "text": "1.4 Standard Paradigms for Inference\nThere are two primary frameworks for how to perform these inferences.\n\n1.4.1 Bayesian Inference\nIn the Bayesian framework, we treat the parameter \\(\\theta\\) as a random variable.\n\nPrior: We assign a prior distribution \\(\\pi(\\theta)\\).\nData Model: We have the likelihood \\(f(D|\\theta)\\).\nPosterior: We compute the posterior distribution using Bayes’ theorem: \\[f(\\theta|D) = \\frac{\\pi(\\theta)f(D|\\theta)}{\\int \\pi(\\theta)f(D|\\theta) d\\theta}\\]\n\n\nExample 1.2 (Bayesian Prediction) The predictive density for a new observation \\(x_{n+1}\\) is obtained by integrating over the posterior: \\[f(x_{n+1}|x) = \\int f(x_{n+1}|\\theta) \\pi(\\theta|x) d\\theta\\]\n\n\n\n1.4.2 Frequentist Inference (Fisher)\nDeveloped largely by Fisher (c. 1920).\n\nRepeated Sampling Principle: Inference is based on the performance of methods under hypothetical repeated sampling of the data.\nUses Sampling Distributions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "2  Decision Theory",
    "section": "",
    "text": "2.1 Formulation of Decision Theory\nIn decision theory, we formalize the process of making decisions under uncertainty using the following components:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#decision-rules-and-risk-functions",
    "href": "decision.html#decision-rules-and-risk-functions",
    "title": "2  Decision Theory",
    "section": "2.2 Decision Rules and Risk Functions",
    "text": "2.2 Decision Rules and Risk Functions\n\n2.2.1 Decision Rule\nA decision rule is a function \\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\). It dictates the action \\(d(x)\\) we take when we observe data \\(x\\).\n\n\n2.2.2 Risk Function\nThe risk function is the expected loss for a given decision rule \\(d\\) as a function of the parameter \\(\\theta\\).\n\\[R(\\theta, d) = E_\\theta[L(\\theta, d(X))]\\]\n\nContinuous Case: \\(R(\\theta, d) = \\int_{\\mathcal{X}} L(\\theta, d(x)) f(x, \\theta) dx\\)\nDiscrete Case: \\(R(\\theta, d) = \\sum_{x \\in \\mathcal{X}} L(\\theta, d(x)) P_\\theta(x)\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#comparing-decision-rules",
    "href": "decision.html#comparing-decision-rules",
    "title": "2  Decision Theory",
    "section": "2.4 Comparing Decision Rules",
    "text": "2.4 Comparing Decision Rules\nWe typically do not have a single rule \\(d\\) that is better than all other rules for all \\(\\theta\\).\n\n\\(d\\) strictly dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nAdmissibility: A decision rule \\(d\\) is admissible if it is not dominated by any other rule. If it is dominated, it is inadmissible.\n\n\n2.4.1 Minimax Principle\nA rule \\(d\\) is Minimax if it minimizes the maximum possible risk.\n\\[\\sup_{\\theta} R(\\theta, d) \\le \\sup_{\\theta} R(\\theta, d') \\quad \\text{for all } d' \\in \\mathcal{D}\\]\nVisualize the risk functions of two rules, \\(d_1\\) and \\(d_2\\). \\(d_2\\) might have a higher risk in some areas but a lower “peak” risk, making it Minimax.\n\n\n\n\n\n\n\n\nFigure 2.2: Illustration of Minimax: Rule d2 has a lower maximum risk than d1, making it the Minimax rule, even though d1 is better for some values of theta.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#bayes-decision-rules",
    "href": "decision.html#bayes-decision-rules",
    "title": "2  Decision Theory",
    "section": "2.5 Bayes Decision Rules",
    "text": "2.5 Bayes Decision Rules\nWe specify a prior distribution \\(\\pi(\\theta)\\) on the parameter space \\(\\Theta\\).\n\n2.5.1 Bayes Risk\nThe Bayes risk of a rule \\(d\\) with respect to prior \\(\\pi\\) is the weighted average of the frequentist risk:\n\\[r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n2.5.2 Bayes Rule Definition\nA decision rule \\(d_\\pi\\) is a Bayes rule with respect to \\(\\pi\\) if it minimizes the Bayes risk:\n\\[r(\\pi, d_\\pi) = \\inf_{d' \\in \\mathcal{D}} r(\\pi, d')\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#randomized-decision-rules-risk-sets",
    "href": "decision.html#randomized-decision-rules-risk-sets",
    "title": "2  Decision Theory",
    "section": "2.6 Randomized Decision Rules & Risk Sets",
    "text": "2.6 Randomized Decision Rules & Risk Sets\n\n2.6.1 Randomized Rules\nA randomized decision rule chooses between deterministic rules \\(d_1, d_2, \\dots\\) with probabilities \\(p_1, p_2, \\dots\\). The risk of a randomized rule \\(d^* = \\sum p_i d_i\\) is the linear combination of their risks: \\[R(\\theta, d^*) = \\sum p_i R(\\theta, d_i)\\]\n\n\n2.6.2 Geometric Interpretation (Finite Parameter Space)\nIf \\(\\Theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_k\\}\\) is finite, we can plot the risk vector \\((R(\\theta_1, d), \\dots, R(\\theta_k, d))\\) in \\(\\mathbb{R}^k\\).\nRisk Set (\\(S\\)): The set of all possible risk vectors. Lemma: The Risk Set \\(S\\) is convex. This is because we can form randomized rules that lie on the line segment connecting any two deterministic rules.\n\n\n\n\n\n\n\n\nFigure 2.3: The Risk Set S. Points d1 and d2 are deterministic rules. The line connecting them represents randomized rules. The Minimax rule lies on the line R1=R2 (if accessible). The Bayes rule is found by bringing a tangent line with slope -pi1/pi2 toward the origin.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#example-the-great-aunts-necklace",
    "href": "decision.html#example-the-great-aunts-necklace",
    "title": "2  Decision Theory",
    "section": "2.6 Example: The “Great Aunt’s Necklace”",
    "text": "2.6 Example: The “Great Aunt’s Necklace”\nScenario: Two boxes. One contains a real necklace (\\(\\theta=1\\)), the other an imitation (\\(\\theta=2\\)). Loss is 1 if we choose the wrong box, 0 otherwise.\n\n\\(\\theta \\in \\{1, 2\\}\\)\nAction \\(a \\in \\{1, 2\\}\\) (Choose Box 1 or Box 2)\n\nData (\\(X\\)): Great Aunt’s judgment. \\(X \\in \\{1, 2\\}\\) (Aunt says Box 1 or Box 2). Probabilities: * If \\(\\theta=1\\) (Real in Box 1): Aunt is senile. \\(P(X=1)=1, P(X=2)=0\\). * If \\(\\theta=2\\) (Real in Box 2): Aunt guesses. \\(P(X=1)=0.5, P(X=2)=0.5\\).\nDecision Rules: 1. \\(d_1\\): Always choose Box 1. 2. \\(d_2\\): Always choose Box 2. 3. \\(d_3(x) = x\\): Follow Aunt’s advice. 4. \\(d_4(x) = 3-x\\): Do opposite of Aunt.\nRisk Calculation:\n\n\n\nRule\n\\(R(\\theta=1)\\)\n\\(R(\\theta=2)\\)\nCoordinates \\((R_1, R_2)\\)\n\n\n\n\n\\(d_1\\)\n0\n1\n(0, 1)\n\n\n\\(d_2\\)\n1\n0\n(1, 0)\n\n\n\\(d_3\\)\n0\n0.5\n(0, 0.5)\n\n\n\\(d_4\\)\n1\n0.5\n(1, 0.5)\n\n\n\nGeometry and Minimax: The risk set is the convex hull of these four points.\n\n\n\n\n\n\n\n\nFigure 2.3: Risk Set for the Necklace Example. The set is the quadrilateral defined by d1, d2, d3, d4. The Minimax rule is the intersection of the line R1=R2 and the lower boundary segment connecting d3 and d2.\n\n\n\n\n\nCalculation of Minimax Rule: The lower boundary connects \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\). Equation of line: \\(R_2 - 0 = \\frac{0.5 - 0}{0 - 1} (R_1 - 1) \\Rightarrow R_2 = -0.5(R_1 - 1)\\). For Minimax, set \\(R_1 = R_2 = R\\). \\(R = -0.5R + 0.5 \\Rightarrow 1.5R = 0.5 \\Rightarrow R = 1/3\\). The minimax rule is a randomized combination: \\(d^* = \\frac{2}{3}d_3 + \\frac{1}{3}d_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes",
    "href": "decision.html#theorems-relating-minimax-and-bayes",
    "title": "2  Decision Theory",
    "section": "2.7 Theorems relating Minimax and Bayes",
    "text": "2.7 Theorems relating Minimax and Bayes\nTheorem 2.1: If a sequence of Bayes rules \\(\\delta_n\\) (w.r.t priors \\(\\pi_n\\)) has Bayes risk converging to \\(C\\), and \\(R(\\theta, \\delta_0) \\le C\\) for all \\(\\theta\\), then \\(\\delta_0\\) is Minimax.\nTheorem 2.2 (Equalizer Rule): An “Extended Bayes” rule that is an equalizer rule (constant risk across \\(\\theta\\)) must be Minimax. Proof sketch: If it were not minimax, there would exist a rule with strictly lower maximum risk. This would imply it has lower Bayes risk for the specific prior, leading to a contradiction with the assumption that the original rule was Bayes.\nTheorem 2.3\nAssume that the parameter space \\(\\Theta = \\{\\theta_1, \\dots, \\theta_t\\}\\) is finite, and that the prior \\(\\pi(\\theta)\\) gives positive probability to each \\(\\theta_i\\). Then, a Bayes rule with respect to \\(\\pi\\) is admissible.\nTheorem 2.4\nIf a Bayes rule is unique, it is admissible.\nTheorem 2.5\nLet \\(\\Theta\\) be a subset of the real line. 1. Assume that the risk functions \\(R(\\theta, d)\\) are continuous in \\(\\theta\\) for all decision rules \\(d\\). 2. Suppose that for any \\(\\theta\\) and any \\(\\epsilon &gt; 0\\), the interval \\((\\theta - \\epsilon, \\theta + \\epsilon)\\) has positive probability under the prior \\(\\pi(\\theta)\\).\nThen, a Bayes rule with respect to \\(\\pi\\) is admissible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-decision-theory",
    "href": "decision.html#formulation-of-decision-theory",
    "title": "2  Decision Theory",
    "section": "",
    "text": "Parameter Space (\\(\\Theta\\)): The set of all possible states of nature or values that the parameter can take. \\(\\theta \\in \\Theta\\) (e.g., mean, variance).\nSample Space (\\(\\mathcal{X}\\)): The space where the data \\(X\\) lies. Example: \\(X = (X_1, X_2, \\dots, X_n)\\) where \\(X_i \\in \\mathbb{R}\\). So \\(\\mathcal{X} \\in \\mathbb{R}^n\\).\nFamily of Probability Distributions: \\(\\{P_\\theta(x) : \\theta \\in \\Theta\\}\\). This describes how likely we are to see the data \\(X\\) given a specific parameter \\(\\theta\\).\n\nIf \\(X\\) is continuous: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Density Function).\nIf \\(X\\) is discrete: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Mass Function).\n\nAction Space (\\(\\mathcal{A}\\)): The set of all actions or decisions available to the experimenter.\nLoss Function: \\(L: \\Theta \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\). \\(L(\\theta, a)\\) specifies the loss incurred if the true parameter is \\(\\theta\\) and we take action \\(a\\). Generally, \\(L(\\theta, a) \\ge 0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "3  Bayesian Methods",
    "section": "",
    "text": "3.1 Bayes Theorem\nSuppose \\(\\theta \\sim \\pi(\\theta)\\) and \\(X \\sim f(x; \\theta)\\). The posterior density of \\(\\theta\\) given \\(X\\) is:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{\\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta} \\propto \\pi(\\theta) \\cdot L(\\theta; x)\n\\]\nwhere \\(L(\\theta; x)\\) is the likelihood.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#examples",
    "href": "bayesian.html#examples",
    "title": "3  Bayesian Methods",
    "section": "3.2 Examples",
    "text": "3.2 Examples\n\n3.2.1 1. Binomial-Beta\n\n\\(X|\\theta \\sim \\text{Bin}(n, \\theta) \\Rightarrow f(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\\)\nPrior \\(\\theta \\sim \\text{Beta}(a, b) \\Rightarrow \\pi(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\)\n\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x} = \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nSo, \\(\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\\).\nMoments: * Mean: \\(E(\\theta|x) = \\frac{a+x}{a+b+n} \\approx \\frac{x}{n}\\) (for small \\(n\\)) * Variance: \\(\\text{Var}(\\theta|x) = \\frac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}\\)\n\n\n3.2.2 2. Normal-Normal (Known Variance)\n\n\\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is known.\nPrior \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\n\nLet \\(\\tau_0 = 1/\\sigma_0^2\\) (prior precision), \\(\\tau = 1/\\sigma^2\\) (data precision). The posterior precision is \\(\\tau_1 = \\tau_0 + n\\tau\\).\nPosterior: \\[\n\\mu|x \\sim N\\left( \\frac{\\tau_0 \\mu_0 + n\\tau \\bar{x}}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nThis shows the posterior mean is a weighted average of the prior mean and the sample mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "href": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "title": "3  Bayesian Methods",
    "section": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)",
    "text": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)\nMinimizing \\(E_{\\theta|x}[(\\theta - d)^2]\\) leads to: \\[\nd(x) = E(\\theta|x) \\quad \\text{(Posterior Mean)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "href": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "title": "3  Bayesian Methods",
    "section": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)",
    "text": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) leads to: \\[\n\\int_{-\\infty}^d \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = 0.5\n\\] So, \\(d(x) = \\text{Median of } \\pi(\\theta|x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#loss-hypothesis-testing",
    "href": "bayesian.html#loss-hypothesis-testing",
    "title": "3  Bayesian Methods",
    "section": "4.3 3. 0-1 Loss (Hypothesis Testing)",
    "text": "4.3 3. 0-1 Loss (Hypothesis Testing)\n\nLoss is 1 if error, 0 if correct.\nTesting \\(\\Theta_0\\) vs \\(\\Theta_1\\).\nBayes Rule: Choose class with highest posterior probability.\n\nReject \\(H_0\\) if \\(P(\\theta \\in \\Theta_1 | x) &gt; P(\\theta \\in \\Theta_0 | x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#interval-estimation",
    "href": "bayesian.html#interval-estimation",
    "title": "3  Bayesian Methods",
    "section": "4.4 4. Interval Estimation",
    "text": "4.4 4. Interval Estimation\nWe want an interval \\(A = (d-\\delta, d+\\delta)\\) minimizing risk (maximizing coverage probability \\(1-\\alpha\\)).\nHighest Posterior Density (HPD) Interval: The set \\(C = \\{ \\theta : \\pi(\\theta|x) \\ge k \\}\\) where \\(P(\\theta \\in C|x) = 1-\\alpha\\). This is the shortest interval for a given confidence level if the posterior is unimodal.\n\n\nCode\nlibrary(ggplot2)\n\nx &lt;- seq(0, 15, length.out = 1000)\ny &lt;- dgamma(x, shape = 3, rate = 0.5)\ndf &lt;- data.frame(x = x, y = y)\n\n# Approximate HPD cutoff (visual)\nhpd_level &lt;- 0.05\ncutoff &lt;- 0.08 # Chosen for visual representation of the cut\n\nggplot(df, aes(x, y)) +\n  geom_line(size = 1) +\n  geom_area(data = subset(df, y &gt; cutoff), fill = \"skyblue\", alpha = 0.5) +\n  geom_hline(yintercept = cutoff, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = 10, y = cutoff + 0.02, label = \"HPD Cutoff line\", color = \"red\") +\n  labs(title = \"Highest Posterior Density (HPD) Interval\", \n       subtitle = \"Points with density higher than the red line form the HPD set\",\n       x = \"Theta\", y = \"Posterior Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of Highest Posterior Density (HPD) Interval vs Equi-tailed Interval on a skewed posterior.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#baseball-example-efron-morris",
    "href": "bayesian.html#baseball-example-efron-morris",
    "title": "3  Bayesian Methods",
    "section": "6.1 Baseball Example (Efron & Morris)",
    "text": "6.1 Baseball Example (Efron & Morris)\nWe observe batting averages for \\(p=18\\) players. * MLE: Individual batting averages. * JS: Shrinks individual averages toward the global average.\n\n\nCode\n# Creating mock data similar to the baseball example\nplayer &lt;- 1:10\nMLE &lt;- c(0.400, 0.370, 0.350, 0.300, 0.280, 0.250, 0.220, 0.200, 0.150, 0.100)\nGrandMean &lt;- mean(MLE)\nshrinkage_factor &lt;- 0.6 # c = 1 - (p-2)/S\nJS &lt;- GrandMean + shrinkage_factor * (MLE - GrandMean)\n\ndf_base &lt;- data.frame(player, MLE, JS)\n\nggplot(df_base) +\n  geom_point(aes(x = MLE, y = 1), color = \"red\", size = 3) +\n  geom_point(aes(x = JS, y = 1), color = \"blue\", size = 3) +\n  geom_segment(aes(x = MLE, y = 1, xend = JS, yend = 1), arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_vline(xintercept = GrandMean, linetype = \"dashed\") +\n  annotate(\"text\", x = GrandMean, y = 1.1, label = \"Grand Mean\") +\n  ylim(0.9, 1.2) +\n  labs(title = \"James-Stein Shrinkage Effect\", \n       subtitle = \"Red: MLE, Blue: JS Estimator\",\n       x = \"Batting Average\") +\n  theme_void() +\n  theme(axis.title.x = element_text(), axis.text.x = element_text())\n\n\n\n\n\n\n\n\nFigure 6.1: Visualizing James-Stein Shrinkage (Mock Data based on Baseball Example). The arrows show MLEs being pulled toward the Grand Mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes",
    "href": "bayesian.html#empirical-bayes",
    "title": "3  Bayesian Methods",
    "section": "7.1 Empirical Bayes",
    "text": "7.1 Empirical Bayes\nInstead of fixing hyperparameters \\((\\mu_0, \\sigma_0^2)\\), we estimate them from the marginal distribution of the data. \\[\nm(x) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\n\\] We estimate \\(\\eta\\) by maximizing \\(m(x)\\) (Type-II MLE) or method of moments.\nExample: If \\(X_i \\sim N(\\mu_i, 1)\\) and \\(\\mu_i \\sim N(0, \\tau^2)\\), then marginally \\(X_i \\sim N(0, 1+\\tau^2)\\). We can use \\(S = \\sum X_i^2\\) to estimate \\(\\tau^2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-models",
    "href": "bayesian.html#hierarchical-models",
    "title": "3  Bayesian Methods",
    "section": "7.2 Hierarchical Models",
    "text": "7.2 Hierarchical Models\nWe assume a multistage structure: 1. Data model: \\(X|\\theta \\sim f(x|\\theta)\\) 2. Parameter model: \\(\\theta|\\lambda \\sim \\pi(\\theta|\\lambda)\\) 3. Hyperparameter model: \\(\\lambda \\sim h(\\lambda)\\)\nComputation: Since analytical solutions are often impossible, we use Markov Chain Monte Carlo (MCMC).\n\n7.2.1 Gibbs Sampling\nTo sample from the joint posterior \\(f(\\theta, \\lambda | x)\\), we sample iteratively from the full conditional distributions: 1. Draw \\(\\theta^{(k+1)} \\sim f(\\theta | \\lambda^{(k)}, x)\\) 2. Draw \\(\\lambda^{(k+1)} \\sim f(\\lambda | \\theta^{(k+1)}, x)\\)\n\n\n7.2.2 Metropolis-Hastings\nIf a conditional distribution is hard to sample from directly: 1. Propose \\(\\theta^*\\) from a proposal density \\(q(\\theta^* | \\theta^{(t)})\\). 2. Calculate acceptance ratio \\(\\alpha = \\min \\left( 1, \\frac{f(\\theta^*|x)q(\\theta^{(t)}|\\theta^*)}{f(\\theta^{(t)}|x)q(\\theta^*|\\theta^{(t)})} \\right)\\). 3. Accept \\(\\theta^*\\) with probability \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "decision.html#examples-of-decision-problems",
    "href": "decision.html#examples-of-decision-problems",
    "title": "2  Decision Theory",
    "section": "2.3 Examples of Decision Problems",
    "text": "2.3 Examples of Decision Problems\nThis section applies the definitions above to common statistical problems.\n\n2.3.1 Example 1: Hypothesis Testing\nWe want to test \\(H_0\\) vs \\(H_1\\).\n\nAction Space: \\(\\mathcal{A} = \\{0, 1\\}\\) where \\(0\\) means “Accept \\(H_0\\)” and \\(1\\) means “Reject \\(H_0\\)”.\nLoss Function (0-1 Loss): \\[\n  L(\\theta, a) =\n  \\begin{cases}\n  0 & \\text{if decision is correct} \\\\\n  1 & \\text{if decision is wrong}\n  \\end{cases}\n  \\] Specifically:\n\nIf \\(\\theta \\in H_0\\) and \\(a=1\\) (Type I Error), Loss = 1.\nIf \\(\\theta \\in H_1\\) and \\(a=0\\) (Type II Error), Loss = 1.\n\nDecision Rule: A common rule is \\(d(x) = 1\\) if \\(\\bar{x} &gt; c\\), else \\(0\\).\nRisk Function:\n\nIf \\(\\theta \\in H_0\\): \\(R(\\theta, d) = 1 \\cdot P_\\theta(d(X)=1) + 0 = P(\\text{Type I Error})\\).\nIf \\(\\theta \\in H_1\\): \\(R(\\theta, d) = 1 \\cdot P_\\theta(d(X)=0) + 0 = P(\\text{Type II Error})\\).\n\n\n\n\n2.3.2 Example 2: Point Estimation\nWe want to estimate a parameter \\(\\theta\\).\n\nAction Space: \\(\\mathcal{A} = \\Theta\\). An action is an estimate of \\(\\theta\\).\nLoss Function:\n\nSquared Error: \\(L(\\theta, a) = (\\theta - a)^2\\)\nAbsolute Error: \\(L(\\theta, a) = |\\theta - a|\\)\n\nDecision Rule: A common rule is \\(d(x) = \\bar{x}\\) (Sample mean).\nRisk Function (for Squared Error): \\[R(\\theta, d) = E_\\theta[(\\bar{x} - \\theta)^2] = \\text{MSE} = \\text{Var}(\\bar{x}) + \\text{Bias}^2\\]\n\n\n\n2.3.3 Example 3: Interval Estimation\nWe want to estimate a range for the parameter.\n\nAction Space: \\(\\mathcal{A} = \\{(l, u) : l \\in \\mathbb{R}, u \\in \\mathbb{R}, l \\le u\\}\\).\n\n\n\n2.3.4 Example 4: The “Great Aunt’s Necklace”\nScenario: Two boxes. One contains a real necklace (\\(\\theta=1\\)), the other an imitation (\\(\\theta=2\\)). Loss is 1 if we choose the wrong box, 0 otherwise.\n\nParameter Space: \\(\\theta \\in \\{1, 2\\}\\)\nAction Space: \\(a \\in \\{1, 2\\}\\) (Choose Box 1 or Box 2)\n\nData (\\(X\\)): Great Aunt’s judgment. \\(X \\in \\{1, 2\\}\\) (Aunt says Box 1 or Box 2).\n\nProbabilities:\n\nIf \\(\\theta=1\\) (Real in Box 1): Aunt is senile. \\(P(X=1)=1, P(X=2)=0\\).\nIf \\(\\theta=2\\) (Real in Box 2): Aunt guesses. \\(P(X=1)=0.5, P(X=2)=0.5\\).\n\n\nDecision Rules:\n\n\\(d_1\\): Always choose Box 1.\n\\(d_2\\): Always choose Box 2.\n\\(d_3(x) = x\\): Follow Aunt’s advice.\n\\(d_4(x) = 3-x\\): Do opposite of Aunt.\n\nRisk Calculation:\n\n\n\nRule\n\\(R(\\theta=1)\\)\n\\(R(\\theta=2)\\)\nCoordinates \\((R_1, R_2)\\)\n\n\n\n\n\\(d_1\\)\n0\n1\n(0, 1)\n\n\n\\(d_2\\)\n1\n0\n(1, 0)\n\n\n\\(d_3\\)\n0\n0.5\n(0, 0.5)\n\n\n\\(d_4\\)\n1\n0.5\n(1, 0.5)\n\n\n\nGeometry of the Risk Set: The risk set is the convex hull of these four points.\n\n\n\n\n\n\n\n\nFigure 2.1: Risk Set for the Necklace Example. The set is the quadrilateral defined by d1, d2, d3, d4. The Minimax rule is the intersection of the line R1=R2 and the lower boundary segment connecting d3 and d2.\n\n\n\n\n\nCalculation of Minimax Rule: The lower boundary connects \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\). Equation of line: \\(R_2 - 0 = \\frac{0.5 - 0}{0 - 1} (R_1 - 1) \\Rightarrow R_2 = -0.5(R_1 - 1)\\). For Minimax, set \\(R_1 = R_2 = R\\). \\(R = -0.5R + 0.5 \\Rightarrow 1.5R = 0.5 \\Rightarrow R = 1/3\\). The minimax rule is a randomized combination: \\(d^* = \\frac{2}{3}d_3 + \\frac{1}{3}d_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  }
]