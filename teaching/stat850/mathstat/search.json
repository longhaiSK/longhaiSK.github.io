[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Inference",
    "section": "Key Features",
    "text": "Key Features",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introstatinf.html",
    "href": "introstatinf.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Introduction to Statistical Inference",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#statistical-inference-setup",
    "href": "introstatinf.html#statistical-inference-setup",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Statistical Inference Setup",
    "text": "1.2 Statistical Inference Setup\nWe begin with observations (units) \\(X_1, X_2, \\dots, X_n\\). These may be vectors. We regard these observations as a realization of random variables.\n\nDefinition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\n1.2.1 Assumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). However, there are non-i.i.d. examples: * Spatial data: Involves spatial correlation. * Time series: Observations depend on time.\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probability-vs.-statistics",
    "href": "introstatinf.html#probability-vs.-statistics",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 Probability vs. Statistics",
    "text": "1.3 Probability vs. Statistics\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\):\n\nProbability: \\(\\theta\\) is known. We ask questions about the data \\(X\\).\nStatistics: \\(\\theta\\) is unknown. We use data \\(X_1, \\dots, X_n\\) to infer \\(\\theta\\) (from sample to population).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#types-of-statistical-inference",
    "href": "introstatinf.html#types-of-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Types of Statistical Inference",
    "text": "1.4 Types of Statistical Inference\nWe can categorize inference into four main types:\n\nDefinition 1.2 (Point Estimation) We use a single number to capture the parameter. \\[\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\]\n\n\nDefinition 1.3 (Interval Estimation) We construct an interval that likely contains the true parameter. \\[\\theta \\in (L(X_1, \\dots, X_n), U(X_1, \\dots, X_n))\\] The true parameter is within this interval.\n\n\nDefinition 1.4 (Hypothesis Testing) We test a specific theory about the parameter. \\[H_0: \\theta = \\theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\neq \\theta_0\\] (Or one-sided alternatives like \\(\\theta &gt; \\theta_0\\)) .\n\n\nDefinition 1.5 (Predictive Inference) Given observed data \\((X_1, Y_1), \\dots, (X_n, Y_n)\\), we want to predict a new observation \\(Y_{n+1}\\) given \\(X_{n+1}\\). This is often the primary goal in Machine Learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#standard-paradigms-for-inference",
    "href": "introstatinf.html#standard-paradigms-for-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 Standard Paradigms for Inference",
    "text": "1.5 Standard Paradigms for Inference\nThere are two primary frameworks for how to perform these inferences.\n\n1.5.1 Bayesian Inference\nIn the Bayesian framework, we treat the parameter \\(\\theta\\) as a random variable.\n\nPrior: We assign a prior distribution \\(\\pi(\\theta)\\).\nData Model: We have the likelihood \\(f(D|\\theta)\\).\nPosterior: We compute the posterior distribution using Bayes’ theorem: \\[f(\\theta|D) = \\frac{\\pi(\\theta)f(D|\\theta)}{\\int \\pi(\\theta)f(D|\\theta) d\\theta}\\]\n\n\nExample 1.2 (Bayesian Prediction) The predictive density for a new observation \\(x_{n+1}\\) is obtained by integrating over the posterior: \\[f(x_{n+1}|x) = \\int f(x_{n+1}|\\theta) \\pi(\\theta|x) d\\theta\\]\n\n\n\n1.5.2 Frequentist Inference (Fisher)\nDeveloped largely by Fisher (c. 1920). * Repeated Sampling Principle: Inference is based on the performance of methods under hypothetical repeated sampling of the data. * Uses Sampling Distributions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  }
]