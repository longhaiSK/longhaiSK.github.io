[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface\nThis is a concise course about statistical inference.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Statistical Inference",
    "section": "",
    "text": "Key Features\n\nUse simulation and graphs to illustrate the concepts in probability theory and statistical inference\nRigourous derivation of the key theorems in statistical inference\n\n\n\nAudience\nThis course requires a strong command of multivariate calculus, alongside a rigorous foundation in intermediate probability theory including asymptotic theorey for probability. Students should also possess prior exposure to applied statistical methods and familiar with basic statistical concepts such as p-value and confidence internal.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introstatinf.html",
    "href": "introstatinf.html",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "1.1 Population Model (Data Model)\nWe begin with observations (units) \\(X_1, X_2, \\dots, X_n\\). These may be vectors. We regard these observations as a realization of random variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#population-model-data-model",
    "href": "introstatinf.html#population-model-data-model",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\nAssumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). The assumption of identical distribution can be relaxed to regression settings in which the distributions of \\(x_i\\)’s are independent but dependent on covariate \\(x_i\\).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model: Normal) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).\n\n\nExample 1.2 (Parametric Model: Bernoulli) Consider a sequence of binary outcomes (e.g., Success/Failure) where each \\(X_i \\in \\{0, 1\\}\\). We assume \\(X_i \\sim \\text{Bernoulli}(\\theta)\\). The probability mass function is: \\[f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}\\] Here, the parameter space is \\(\\Theta = [0, 1]\\), where \\(\\theta\\) represents the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "href": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probabilistic Model vs. Statistical Inference",
    "text": "1.2 Probabilistic Model vs. Statistical Inference\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\). We can visualize this using a “shooting target” analogy:\n\n\\(\\theta\\) (The Center): The true, unknown bullseye location.\n\\(x\\) (The Shots): The observed holes on the target board.\nProbability (Deductive): The center \\(\\theta\\) is known. We predict where the shots \\(x\\) will land.\nStatistics (Inductive): The shots \\(x\\) are observed on the board. The center \\(\\theta\\) is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n\nCode\n# --- Setup ---\nset.seed(2024)\nn_sim &lt;- 100\n\n# Helper function to draw circles in base R\ndraw_circle &lt;- function(x, y, r, col, lty = 1, lwd = 1) {\n  theta &lt;- seq(0, 2 * pi, length.out = 200)\n  lines(x + r * cos(theta), y + r * sin(theta), col = col, lty = lty, lwd = lwd)\n}\n\n# Layout: 1 row, 2 columns\n# mar: c(bottom, left, top, right) - decreasing margins to trim space\npar(mfrow = c(1, 2), mar = c(3, 3, 3, 1))\n\n# ---------------------------------------------------------\n# PLOT 1: PROBABILITY (The Generator)\n# ---------------------------------------------------------\n# 1. Generate Data\nx_prob &lt;- rnorm(n_sim)\ny_prob &lt;- rnorm(n_sim)\n\n# 2. Setup Canvas (asp=1 ensures circles look like circles)\nplot(NA, xlim = c(-4, 4), ylim = c(-4, 4), asp = 1,\n     xlab = \"\", ylab = \"\", \n     main = \"PROBABILITY\\n(Model Known -&gt; Data Random)\")\n\n# 3. Draw Model (True Center & Contours)\npoints(0, 0, pch = 19, col = \"blue\", cex = 2)\nfor(r in 1:3) draw_circle(0, 0, r, col = \"blue\", lwd = 1.5)\n\n# 4. Draw Generated Data\npoints(x_prob, y_prob, pch = 16, col = adjustcolor(\"darkblue\", alpha = 0.5))\n\n# 5. Legend\nlegend(\"bottomleft\", legend = c(\"True Model\", \"Generated Data\"),\n       col = c(\"blue\", \"darkblue\"), pch = c(19, 16),\n       lty = c(1, 0), bty = \"n\", cex = 0.8)\n\n# ---------------------------------------------------------\n# PLOT 2: STATISTICS (The Inference)\n# ---------------------------------------------------------\n# 1. Generate NEW Observed Data\nx_stat &lt;- rnorm(n_sim)\ny_stat &lt;- rnorm(n_sim)\nx_bar &lt;- mean(x_stat)\ny_bar &lt;- mean(y_stat)\n\n# 2. Setup Canvas\nplot(NA, xlim = c(-4, 4), ylim = c(-4, 4), asp = 1,\n     xlab = \"\", ylab = \"\", \n     main = \"STATISTICS\\n(Data Observed -&gt; Model Unknown)\")\n\n# 3. Draw Observed Data\npoints(x_stat, y_stat, pch = 16, col = adjustcolor(\"black\", alpha = 0.5))\n\n# 4. Draw Hypothesis 1 (Good Fit - Centered at Sample Mean)\npoints(x_bar, y_bar, pch = 19, col = \"darkgreen\", cex = 1.5)\nfor(r in 1:3) draw_circle(x_bar, y_bar, r, col = \"darkgreen\", lty = 2, lwd = 1.5)\n\n# 5. Draw Hypothesis 2 (Bad Fit - Shifted)\npoints(x_bar + 1.5, y_bar + 1.5, pch = 19, col = \"red\", cex = 1.5)\nfor(r in 1:3) draw_circle(x_bar + 1.5, y_bar + 1.5, r, col = \"red\", lty = 2, lwd = 1.5)\n\n# 6. Legend\nlegend(\"bottomleft\", \n       legend = c(\"Observed Data\", \"H1 (Good Fit)\", \"H2 (Bad Fit)\"),\n       col = c(\"black\", \"darkgreen\", \"red\"), \n       pch = c(16, 19, 19), lty = c(0, 2, 2), \n       bty = \"n\", cex = 0.8)\n\n\n\n\n\n\n\n\nFigure 1.1: Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "href": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 A Motivating Example: The Lady Tasting Tea",
    "text": "1.3 A Motivating Example: The Lady Tasting Tea\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare \\(n\\) cups of tea.\n\nRandom Variable: Let \\(X_i=1\\) if she identifies the cup correctly, and \\(0\\) otherwise.\nParameter: Let \\(\\theta\\) be the probability that she correctly identifies a cup.\nThe Data: Suppose we observe that she identifies 70% of cups correctly (\\(\\bar{x} = 0.7\\)), which is a summary of the observed vector of \\(x_i\\), for example,\n\n\\[x=(0,1,1,0, 1,1,0,1,1,1)\\]\n\nSmall Sample (n=10)Large Sample (n=40)\n\n\nWe observe 7 out of 10 correct (\\(k=7\\)). \\[\\bar{x} = 0.7\\]\n\n\nWe observe 28 out of 40 correct (\\(k=28\\)). \\[\\bar{x} = 0.7\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "href": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Questions to Answer in Statistical Inference",
    "text": "1.4 Questions to Answer in Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nModel Assessment\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nExample: Can we use a reduced model? What level of complexity of \\(f(x; \\theta)\\) is necessary?\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#the-likelihood-function",
    "href": "introstatinf.html#the-likelihood-function",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 The Likelihood Function",
    "text": "1.5 The Likelihood Function\nThe bridge between probability and statistics is the Likelihood Function.\n\nDefinition 1.2 (Likelihood Function) Let \\(f(x_1, \\dots, x_n; \\theta)\\) be the joint probability density (or mass) function of the data given the parameter \\(\\theta\\). When we view this function as a function of \\(\\theta\\) for fixed observed data \\(x_1, \\dots, x_n\\), we call it the likelihood function, denoted \\(L(\\theta)\\). \\[L(\\theta) = f(x_1, \\dots, x_n; \\theta)\\]\n\n\nExample: Lady Tasting Tea\nFor our Tea Tasting data, the likelihood is proportional to the Binomial probability: \\[L(\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}\\]\n\nn=10 (k=7)n=40 (k=28)\n\n\nHere, \\(L(\\theta) = \\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\).\n\n\nCode\n# Calculate values for table\ntheta_pts &lt;- c(0, 0.2, 0.4, 0.6, 0.7, 0.8, 1.0)\nlik_pts &lt;- choose(n_small, k_small) * theta_pts^k_small * (1-theta_pts)^(n_small-k_small)\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n120 \\(\\times 0^{7} \\times 1^{3}\\)\n0.0000\n\n\n0.2\n120 \\(\\times 0.2^{7} \\times 0.8^{3}\\)\n0.0008\n\n\n0.4\n120 \\(\\times 0.4^{7} \\times 0.6^{3}\\)\n0.0425\n\n\n0.6\n120 \\(\\times 0.6^{7} \\times 0.4^{3}\\)\n0.2150\n\n\n0.7\n120 \\(\\times 0.7^{7} \\times 0.3^{3}\\)\n0.2668 (Max)\n\n\n0.8\n120 \\(\\times 0.8^{7} \\times 0.2^{3}\\)\n0.2013\n\n\n1.0\n120 \\(\\times 1^{7} \\times 0^{3}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_small, k_small) * theta^k_small * (1 - theta)^(n_small-k_small) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_small/n_small, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_small/n_small, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_small/n_small), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_small, \", k=\", k_small, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.2: Likelihood Function (n= 10 )\n\n\n\n\n\n\n\nHere, \\(L(\\theta) = \\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\). Notice how the likelihood becomes narrower (more peaked) with more data, even though the peak remains at 0.7.\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n5.5868535^{9} \\(\\times 0^{28} \\times 1^{12}\\)\n0.0000\n\n\n0.2\n5.5868535^{9} \\(\\times 0.2^{28} \\times 0.8^{12}\\)\n0.0000\n\n\n0.4\n5.5868535^{9} \\(\\times 0.4^{28} \\times 0.6^{12}\\)\n0.0001\n\n\n0.6\n5.5868535^{9} \\(\\times 0.6^{28} \\times 0.4^{12}\\)\n0.0576\n\n\n0.7\n5.5868535^{9} \\(\\times 0.7^{28} \\times 0.3^{12}\\)\n0.1366 (Max)\n\n\n0.8\n5.5868535^{9} \\(\\times 0.8^{28} \\times 0.2^{12}\\)\n0.0443\n\n\n1.0\n5.5868535^{9} \\(\\times 1^{28} \\times 0^{12}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_large, k_large) * theta^k_large * (1 - theta)^(n_large-k_large) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_large/n_large, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_large/n_large, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_large/n_large), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_large, \", k=\", k_large, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.3: Likelihood Function (n= 40 )\n\n\n\n\n\n\n\n\n\nQuestions\n\nIs an estimator like \\(\\bar x\\), which is called Maximum Likelihood Estimator (MLE), a good estimator in general?\nWhat do you discover from actually observing the two likelihood unctions of different sample size \\(n\\)?\nIs the likelihood function central to all inference problems?\nWhat are the essential ‘parameters’ of the likelihood function?\n\nThere are two primary frameworks for “How” to perform these inferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#frequentist-inference",
    "href": "introstatinf.html#frequentist-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Frequentist Inference",
    "text": "1.6 Frequentist Inference\n\nConcept: \\(\\theta\\) is unknown but fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#bayesian-inference",
    "href": "introstatinf.html#bayesian-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.7 Bayesian Inference",
    "text": "1.7 Bayesian Inference\n\nConcept: \\(\\theta\\) is regarded as a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\nExample: Bayesian Analysis of the Lady Tasting Tea\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: \\(\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)\\)\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: \\(\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)\\).\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.7.1 Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not or general inference?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?\nPrior: How to choose our prior? What’s its effect on our inference?\nModel Comparison and Assessment: How to assess a Bayesian model?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "2  Decision Theory",
    "section": "",
    "text": "2.1 Formulation of Decision Theory\nIn decision theory, we formalize the process of making decisions under uncertainty using the following components:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-decision-theory",
    "href": "decision.html#formulation-of-decision-theory",
    "title": "2  Decision Theory",
    "section": "",
    "text": "Parameter Space (\\(\\Theta\\)): The set of all possible states of nature or values that the parameter can take. \\(\\theta \\in \\Theta\\) (e.g., mean, variance).\nSample Space (\\(\\mathcal{X}\\)): The space where the data \\(X\\) lies. Example: \\(X = (X_1, X_2, \\dots, X_n)\\) where \\(X_i \\in \\mathbb{R}\\). So \\(\\mathcal{X} \\in \\mathbb{R}^n\\).\nFamily of Probability Distributions: \\(\\{P_\\theta(x) : \\theta \\in \\Theta\\}\\). This describes how likely we are to see the data \\(X\\) given a specific parameter \\(\\theta\\).\n\nIf \\(X\\) is continuous: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Density Function).\nIf \\(X\\) is discrete: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Mass Function).\n\nAction Space (\\(\\mathcal{A}\\)): The set of all actions or decisions available to the experimenter.\nLoss Function: \\(L: \\Theta \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\). \\(L(\\theta, a)\\) specifies the loss incurred if the true parameter is \\(\\theta\\) and we take action \\(a\\). Generally, \\(L(\\theta, a) \\ge 0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#decision-rules-and-risk-functions",
    "href": "decision.html#decision-rules-and-risk-functions",
    "title": "2  Decision Theory",
    "section": "2.2 Decision Rules and Risk Functions",
    "text": "2.2 Decision Rules and Risk Functions\n\n2.2.1 Decision Rule\nA decision rule is a function \\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\). It dictates the action \\(d(x)\\) we take when we observe data \\(x\\).\n\n\n2.2.2 Risk Function\nThe risk function is the expected loss for a given decision rule \\(d\\) as a function of the parameter \\(\\theta\\).\n\\[R(\\theta, d) = E_\\theta[L(\\theta, d(X))]\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#examples-of-decision-problems",
    "href": "decision.html#examples-of-decision-problems",
    "title": "2  Decision Theory",
    "section": "2.3 Examples of Decision Problems",
    "text": "2.3 Examples of Decision Problems\n\n2.3.1 Example 1: Hypothesis Testing\nWe want to test \\(H_0\\) vs \\(H_1\\).\n\nAction Space: \\(\\mathcal{A} = \\{0, 1\\}\\) (0=“Accept \\(H_0\\)”, 1=“Reject \\(H_0\\)”).\nLoss Function (0-1 Loss): 0 if correct, 1 if wrong.\nRisk Function:\n\nIf \\(\\theta \\in H_0\\): \\(R(\\theta, d) = P(\\text{Type I Error})\\).\nIf \\(\\theta \\in H_1\\): \\(R(\\theta, d) = P(\\text{Type II Error})\\).\n\n\n\n\n2.3.2 Example 2: Point Estimation\nWe want to estimate a parameter \\(\\theta\\).\n\nAction Space: \\(\\mathcal{A} = \\Theta\\).\nLoss Function (Squared Error): \\(L(\\theta, a) = (\\theta - a)^2\\).\nRisk Function (MSE): \\(R(\\theta, d) = \\text{Var}(\\bar{x}) + \\text{Bias}^2\\).\n\n\n\n2.3.3 Example 3: Interval Estimation\nWe want to estimate a range for the parameter.\n\nAction Space: \\(\\mathcal{A} = \\{(l, u) : l \\in \\mathbb{R}, u \\in \\mathbb{R}, l \\le u\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#sec-necklace",
    "href": "decision.html#sec-necklace",
    "title": "2  Decision Theory",
    "section": "2.4 The Duchess and the Emerald Necklace",
    "text": "2.4 The Duchess and the Emerald Necklace\nScenario: You are the Duchess of Omnium. You have two necklaces: a priceless Real one and a valueless Imitation. They are indistinguishable to you. One is in the Left Drawer (Box 1), the other is in the Right Drawer (Box 2).\nThe Data (Great Aunt): You consult your Great Aunt. She inspects the Left Drawer first, then the Right.\n\nIf the Real necklace is in the Left (\\(\\theta=1\\)): She identifies it correctly. (Infallible).\nIf the Real necklace is in the Right (\\(\\theta=2\\)): She sees the fake first, gets confused, and guesses randomly (\\(50/50\\)).\n\n\n2.4.1 Formulation\n\nParameter Space: \\(\\Theta = \\{1, 2\\}\\) (1=Real Left, 2=Real Right).\nAction Space: \\(\\mathcal{A} = \\{1, 2\\}\\) (1=Wear Left, 2=Wear Right).\nLoss Function: 0 if correct, 1 if wrong.\n\n\n\n2.4.2 Risk Calculation for Deterministic Rules\nWe consider four deterministic rules \\(d(X)\\). We calculate the risk (\\(R_1\\) for \\(\\theta=1\\) and \\(R_2\\) for \\(\\theta=2\\)) for each.\nRule \\(d_1\\) (Always Left)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 1\\)\n\n\n\nRule \\(d_2\\) (Always Right)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0\\)\n\n\n\nRule \\(d_3\\) (Follow Aunt)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)\n\n\n\nRule \\(d_4\\) (Do Opposite)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#principles-for-choosing-a-decision-rule",
    "href": "decision.html#principles-for-choosing-a-decision-rule",
    "title": "2  Decision Theory",
    "section": "2.5 Principles for Choosing a Decision Rule",
    "text": "2.5 Principles for Choosing a Decision Rule\nSince no single rule minimizes risk for all \\(\\theta\\), we rely on several principles to order and select decision rules.\n\n2.5.1 Admissibility\nA decision rule \\(d\\) is admissible if it is not “dominated” by any other rule.\n\nDomination: A rule \\(d\\) dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nInadmissibility: If a rule is dominated, it is inadmissible and can be discarded (we can do better or equal in every possible state).\n\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Domination: Rule A (Red) is inadmissible because Rule B (Blue) has lower risk for all values of theta.\n\n\n\n\n\n\n\n2.5.2 Minimax Principle\nThe Minimax principle is a conservative approach that guards against the worst-case scenario. It selects the rule that minimizes the maximum risk. \\[ \\min_{d} \\left[ \\sup_{\\theta} R(\\theta, d) \\right] \\]\nIn the plot below, while Rule B has lower risk in the center, it has a very high maximum risk. Rule A is “flatter” and has a lower maximum value, making it the Minimax choice.\n\n\n\n\n\n\n\n\nFigure 2.2: Illustration of Minimax: Rule A has a lower peak risk than Rule B, making Rule A the Minimax choice.\n\n\n\n\n\n\n\n2.5.3 Bayes Decision Rules\nThe Bayes principle incorporates prior knowledge. If we assign a probability distribution (prior) \\(\\pi(\\theta)\\) to the parameter, we can calculate the Bayes Risk, which is the weighted average of the risk function. We choose the rule that minimizes this average. \\[ r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#risk-set-for-finite-parameter-space",
    "href": "decision.html#risk-set-for-finite-parameter-space",
    "title": "2  Decision Theory",
    "section": "2.6 Risk Set for Finite Parameter Space",
    "text": "2.6 Risk Set for Finite Parameter Space\nFor finite parameter spaces (e.g., \\(\\Theta = \\{1, 2\\}\\)), we can visualize the problem in 2D space where the axes are \\(R_1 = R(\\theta_1)\\) and \\(R_2 = R(\\theta_2)\\).\n\n2.6.1 The Risk Set (\\(S\\))\nThe set of all possible risk vectors is called the Risk Set \\(S\\).\n\nDeterministic Rules: These are the vertices of the set.\nRandomized Rules: By choosing rule \\(d_i\\) with probability \\(p\\) and \\(d_j\\) with probability \\(1-p\\), we can achieve any risk on the line segment connecting them.\nConvexity: The Risk Set is the convex hull of the deterministic rules.\n\n\n\n2.6.2 Visualizing Admissibility\nThe admissible rules lie on the lower-left boundary of the set. Any point to the “north-east” of another point is dominated (inadmissible).\n\n\n2.6.3 Visualizing Minimax\nThe Minimax rule is found by intersecting the Risk Set with the line \\(y=x\\) (\\(R_1 = R_2\\)).\n\nWe look for the point in \\(S\\) that touches the \\(45^\\circ\\) line at the lowest value.\nIf the set is entirely below the line, we minimize \\(R_2\\). If entirely above, we minimize \\(R_1\\).\n\n\n\n2.6.4 Visualizing Bayes Rules\nA Bayes rule minimizes \\(\\pi_1 R_1 + \\pi_2 R_2 = k\\). This equation represents a line with slope \\(m = -\\pi_1 / \\pi_2\\).\n\nTo find the Bayes rule, we find the tangent line to the Risk Set \\(S\\) with slope \\(-\\pi_1 / \\pi_2\\).\n\n\n\n\n\n\n\n\n\nFigure 2.3: Geometric Interpretation: The gray polygon is the Risk Set S. The blue boundary represents admissible rules. The red point is the Minimax rule. The green line represents a Bayes rule for a specific prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "href": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "title": "2  Decision Theory",
    "section": "2.7 Revisiting the Necklace Example: Geometric Solution",
    "text": "2.7 Revisiting the Necklace Example: Geometric Solution\nWe now apply the geometric interpretation to the Necklace problem using the risks calculated in Section 2.4.\n\n\\(d_1\\): \\((0, 1)\\)\n\\(d_2\\): \\((1, 0)\\)\n\\(d_3\\): \\((0, 0.5)\\)\n\\(d_4\\): \\((1, 0.5)\\)\n\n\n2.7.1 Analysis\n\nAdmissibility:\n\n\\(d_4\\) has risk \\((1, 0.5)\\). \\(d_3\\) has risk \\((0, 0.5)\\). Since \\(0 &lt; 1\\), \\(d_3\\) strictly dominates \\(d_4\\). Thus \\(d_4\\) is inadmissible.\nThe efficient frontier connects \\(d_3\\) and \\(d_2\\).\n\nMinimax Solution: The Minimax rule lies on the segment connecting \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\).\n\nLet the randomized rule be \\(\\delta^* = p d_3 + (1-p) d_2\\).\n\\(R(\\delta^*) = p \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} + (1-p) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-p \\\\ 0.5p \\end{pmatrix}\\).\nSet \\(R_1 = R_2\\): \\(1-p = 0.5p \\Rightarrow 1 = 1.5p \\Rightarrow p = 2/3\\).\nResult: The Minimax rule is to choose \\(d_3\\) with probability \\(2/3\\) and \\(d_2\\) with probability \\(1/3\\).\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Necklace Problem Solution. The Minimax rule (red diamond) is the specific randomized combination of d3 and d2 that equalizes the risk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "href": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "title": "2  Decision Theory",
    "section": "2.8 Theorems Relating Minimax and Bayes Rules",
    "text": "2.8 Theorems Relating Minimax and Bayes Rules\nIn practice, finding a Minimax rule directly is mathematically difficult. A standard strategy is to “guess” a Least Favorable Prior \\(\\pi\\)—defined as the prior distribution that maximizes the minimum Bayes risk (i.e., the prior against which it is hardest to defend)—find the corresponding Bayes rule, and then check if it satisfies specific conditions to confirm it is Minimax.\n\n2.8.1 Constant Risk Bayes Rule Is Minimax (Proof by Contradiction)\n\nTheorem 2.1 (Constant Risk Bayes Rule Is Minimax) Let \\(\\delta^\\pi\\) be a Bayes estimator with respect to a prior \\(\\pi\\). If the risk function of \\(\\delta^\\pi\\) is constant on the parameter space \\(\\Theta\\), such that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), then \\(\\delta^\\pi\\) is a minimax estimator.\n\n\nProof. Assume, for the sake of contradiction, that \\(\\delta^\\pi\\) is not a minimax estimator.\nBy definition, if \\(\\delta^\\pi\\) is not minimax, there must exist some other estimator \\(\\delta'\\) that has a strictly smaller maximum risk. That is:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') &lt; \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta^\\pi)\n\\]\nSince we are given that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), its supremum is simply \\(c\\). Therefore, our assumption implies:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') &lt; c\n\\]\nNow, consider the Bayes risk of \\(\\delta'\\) with respect to the prior \\(\\pi\\). The Bayes risk is the weighted average of the risk function:\n\\[\nr(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta\n\\]\nSince \\(R(\\theta, \\delta') \\le \\sup_{\\theta} R(\\theta, \\delta')\\) for all \\(\\theta\\), and we assumed this supremum is strictly less than \\(c\\), it follows that:\n\\[\nr(\\pi, \\delta') \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') &lt; c\n\\]\nHowever, we know that \\(c\\) is the Bayes risk of \\(\\delta^\\pi\\):\n\\[\nr(\\pi, \\delta^\\pi) = \\int_\\Theta c \\, \\pi(\\theta) d\\theta = c\n\\]\nSubstituting this into our inequality, we get:\n\\[\nr(\\pi, \\delta') &lt; r(\\pi, \\delta^\\pi)\n\\]\nThis result contradicts the fact that \\(\\delta^\\pi\\) is a Bayes estimator. By definition, a Bayes estimator must minimize the Bayes risk, meaning \\(r(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta)\\) for any estimator \\(\\delta\\).\nBecause our assumption that \\(\\delta^\\pi\\) is not minimax leads to a contradiction of the Bayes optimality of \\(\\delta^\\pi\\), the assumption must be false. Thus, \\(\\delta^\\pi\\) must be minimax.\n\nThe plot below visualizes this logic. If an estimator \\(\\delta'\\) (Blue) were to be “better” in a minimax sense than \\(\\delta^\\pi\\) (Red), its entire curve would have to stay below the maximum value \\(c\\). However, if it stays below \\(c\\) everywhere, its average (Bayes risk) would necessarily be lower than \\(c\\), which is impossible if \\(\\delta^\\pi\\) is the Bayes estimator.\n\n\nCode\n# Define Parameter Space Theta\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# 1. Constant Risk Bayes Estimator (risk = C)\nc_val &lt;- 0.6\nrisk_bayes &lt;- rep(c_val, length(theta))\n\n# 2. An estimator that would contradict Bayes optimality \n# (Always below the constant risk line)\nrisk_contradiction &lt;- 0.5 + 0.05 * cos(2 * pi * theta)\n\n# Plotting\nplot(theta, risk_bayes, type = 'l', lwd = 3, col = \"red\",\n     ylim = c(0, 1), ylab = \"Risk R(theta, d)\", xlab = expression(theta),\n     main = \"Proof by Contradiction Geometry\")\n\n# Add the \"Better\" Estimator (which is impossible)\nlines(theta, risk_contradiction, col = \"blue\", lwd = 2, lty = 2)\n\n# Shaded area showing the \"Impossible\" Bayes Risk improvement\npolygon(c(theta, rev(theta)), c(risk_contradiction, rev(risk_bayes)), \n        col = rgb(0, 0, 1, 0.1), border = NA)\n\n# Add Legend\nlegend(\"topright\", \n       legend = c(\"Constant Risk Bayes (c)\", \"Hypothetical 'Better' Est.\"),\n       col = c(\"red\", \"blue\"), lwd = 2, lty = c(1, 2))\n\n\n\n\n\n\n\n\nFigure 2.5: Visualizing the Contradiction: If the blue curve’s maximum were below the red line, its average risk would be lower than the Bayes risk of the red estimator.\n\n\n\n\n\n\n\n2.8.2 Minimaxity via Limiting Bayes Risks\nSometimes the Minimax rule corresponds to an “improper” prior (a prior that does not integrate to 1, like a uniform distribution on the real line). We approach these via a limiting sequence.\n\nTheorem 2.2 (Minimaxity of Limit-Attaining Rules) Let \\(\\{\\delta_n\\}\\) be a sequence of Bayes rules with respect to priors \\(\\{\\pi_n\\}\\). Let \\(r(\\pi_n, \\delta_n)\\) be the associated Bayes risks. If there exists a rule \\(\\delta_0\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\] Then \\(\\delta_0\\) is Minimax.\n\n\nProof. \n\nDefine Limit: Let \\(V = \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\). We are given that \\(\\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\).\nContradiction Setup: Suppose \\(\\delta_0\\) is not Minimax. Then there exists a rule \\(\\delta^*\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta^*) &lt; \\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\] Let \\(\\sup_{\\theta} R(\\theta, \\delta^*) = V - \\epsilon\\) for some \\(\\epsilon &gt; 0\\).\nBounded Risk of \\(\\delta^*\\): The Bayes risk of \\(\\delta^*\\) is bounded by its maximum risk: \\[r(\\pi_n, \\delta^*) = \\int R(\\theta, \\delta^*) \\pi_n(\\theta) d\\theta \\le V - \\epsilon\\] Therefore, \\(\\lim_{n \\to \\infty} r(\\pi_n, \\delta^*) \\le V - \\epsilon\\).\nOptimality of \\(\\delta_n\\): Since \\(\\delta_n\\) is the Bayes rule for \\(\\pi_n\\), it minimizes Bayes risk. This creates the inequality pair shown in the figure (Orange \\(\\le\\) Blue): \\[r(\\pi_n, \\delta_n) \\le r(\\pi_n, \\delta^*)\\]\nThe Contradiction: Combining the inequalities, we get: \\[\\lim_{n \\to \\infty} r(\\pi_n, \\delta_n) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta^*) \\le V - \\epsilon\\] This implies \\(V \\le V - \\epsilon\\), which is impossible. Thus \\(\\delta_0\\) must be Minimax. \\(\\blacksquare\\)\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Visual Proof: We examine the Bayes risks at two steps, \\(n=j\\) (squares) and \\(n=j+1\\) (circles). In both steps, the optimal risk \\(r(\\pi_n, \\delta_n)\\) (orange) must be lower than the hypothetical risk \\(r(\\pi_n, \\delta^*)\\) (blue). Even as the sequence rises (j+1 is higher than j), the blue points are capped by the bound \\(V-\\epsilon\\). This ‘traps’ the orange points, making it impossible for them to ever reach the Limit V.\n\n\n\n\n\n\n\n2.8.3 Procedure: Verifying Minimaxity\nThe theorem above provides a practical recipe for identifying Minimax rules, particularly in unbounded parameter spaces (where a standard Least Favorable Prior often does not exist). The procedure is often used “backwards”—we guess a rule and then construct a sequence to prove it is Minimax.\n\nPropose a Candidate Rule (\\(\\delta_0\\)): Identify a rule that intuitively seems robust. Typically, we look for an Equalizer Rule, which is a rule with constant risk (\\(R(\\theta, \\delta_0) = C\\) for all \\(\\theta\\)). If the risk is constant, then \\(\\sup_\\theta R(\\theta, \\delta_0) = C\\).\nConstruct a Sequence of Priors (\\(\\pi_n\\)): Choose a sequence of priors that becomes increasingly “diffuse” or “flat” as \\(n \\to \\infty\\) (e.g., Uniform on \\([-n, n]\\) or Normal with variance \\(n\\)). These approximate the “improper” prior corresponding to the candidate rule.\nCompute Bayes Risks (\\(r_n\\)): Calculate the Bayes risk \\(r(\\pi_n, \\delta_n)\\) for each prior in the sequence. Note that you do not necessarily need the formula for the Bayes rule \\(\\delta_n\\) itself, only its associated risk.\nVerify the Condition: Check if the limit of the Bayes risks approaches the maximum risk of your candidate: \\[ \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n) = \\sup_{\\theta} R(\\theta, \\delta_0) \\] If this holds, \\(\\delta_0\\) is Minimax.\n\n\nExample 2.1 (The Normal Mean) Consider a single observation \\(X \\sim N(\\theta, 1)\\) with squared error loss \\(L(\\theta, \\delta) = (\\theta - \\delta)^2\\). We suspect the sample mean (in this case, just \\(X\\) itself) is the Minimax estimator.\nStep 1: Candidate Rule\nLet \\(\\delta_0(X) = X\\). The risk is the variance of the estimator: \\[ R(\\theta, \\delta_0) = E[(\\theta - X)^2] = \\text{Var}(X) = 1 \\] Since the risk is constant (1) for all \\(\\theta\\), \\(\\sup_\\theta R(\\theta, \\delta_0) = 1\\).\nStep 2: Sequence of Priors\nWe choose a sequence of Normal priors \\(\\pi_n \\sim N(0, n)\\). As \\(n\\) increases, the variance increases, making the prior flatter over the real line.\nStep 3: Bayes Risks\nFor a Normal prior \\(\\theta \\sim N(0, \\tau^2)\\) and data \\(X \\sim N(\\theta, \\sigma^2)\\), the Bayes risk is known to be: \\[ r(\\pi, \\delta_\\pi) = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\] Substituting our values (\\(\\sigma^2=1, \\tau^2=n\\)): \\[ r(\\pi_n, \\delta_n) = \\frac{1 \\cdot n}{1 + n} = \\frac{n}{n+1} \\]\nStep 4: Verification\nWe take the limit of the sequence of Bayes risks: \\[ \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n) = \\lim_{n \\to \\infty} \\frac{n}{n+1} = 1 \\] Comparing this to our candidate: \\[ \\sup_{\\theta} R(\\theta, \\delta_0) = 1 \\le 1 \\] The condition holds. Therefore, \\(\\delta_0(X) = X\\) is the Minimax estimator for \\(\\theta\\).\n\n\n\n2.8.4 Bayes Rule as a Working Horse to Find a Minimax Rule\n\n2.8.4.1 The Minimax Theorem (Saddle Point)\nThis theorem connects the search for a Minimax rule to the search for a Least Favorable Prior. It justifies the strategy of “finding the worst prior and solving it.”\n\nTheorem 2.3 (The Minimax Theorem) Let \\(\\mathcal{D}\\) be the set of all decision rules and \\(\\Pi\\) be the set of all prior distributions. Let \\(r(\\pi, \\delta)\\) denote the Bayes risk.\nThe Minimax value equals the Maximin Bayes value: \\[ \\inf_{\\delta \\in \\mathcal{D}} \\sup_{\\pi \\in \\Pi} r(\\pi, \\delta) = \\sup_{\\pi \\in \\Pi} \\inf_{\\delta \\in \\mathcal{D}} r(\\pi, \\delta) \\]\nFurthermore, a pair \\((\\delta_0, \\pi_0)\\) is a Saddle Point if for all \\(\\delta \\in \\mathcal{D}\\) and \\(\\pi \\in \\Pi\\): \\[ r(\\pi_0, \\delta) \\ge r(\\pi_0, \\delta_0) \\ge r(\\pi, \\delta_0) \\] If such a saddle point exists, then:\n\n\\(\\delta_0\\) is a Minimax rule.\n\\(\\pi_0\\) is a Least Favorable Prior.\n\n\n\nProof. Goal: We wish to show that if \\((\\delta_0, \\pi_0)\\) is a saddle point, then \\(\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\sup_{\\theta} R(\\theta, \\delta)\\) for any other rule \\(\\delta\\).\n1. Interpret the Saddle Point Inequalities: The condition is given as two simultaneous inequalities: \\[\n\\begin{aligned}\n(A) \\quad & r(\\pi_0, \\delta_0) \\le r(\\pi_0, \\delta) \\quad \\text{for all } \\delta \\\\\n(B) \\quad & r(\\pi, \\delta_0) \\le r(\\pi_0, \\delta_0) \\quad \\text{for all } \\pi\n\\end{aligned}\n\\]\n2. Analyze Inequality (A): Since \\(r(\\pi_0, \\delta_0) \\le r(\\pi_0, \\delta)\\) for all \\(\\delta\\), \\(\\delta_0\\) minimizes the Bayes risk with respect to \\(\\pi_0\\).\n\nTherefore, \\(\\delta_0\\) is the Bayes rule for \\(\\pi_0\\).\n\n3. Analyze Inequality (B): Since \\(r(\\pi, \\delta_0) \\le r(\\pi_0, \\delta_0)\\) for all \\(\\pi\\), the prior \\(\\pi_0\\) maximizes the average risk of \\(\\delta_0\\).\n\nSince the supremum over all priors includes point-mass priors (which yield the risk at a single \\(\\theta\\)), maximizing over \\(\\pi\\) is equivalent to maximizing over \\(\\theta\\): \\[ \\sup_{\\pi} r(\\pi, \\delta_0) = \\sup_{\\theta} R(\\theta, \\delta_0) \\]\nTherefore, Inequality (B) implies: \\[ \\sup_{\\theta} R(\\theta, \\delta_0) = r(\\pi_0, \\delta_0) \\]\n\n4. Combine to Prove Minimaxity: Let \\(\\delta^*\\) be any arbitrary decision rule. We compute its worst-case risk: \\[\n\\begin{aligned}\n\\sup_{\\theta} R(\\theta, \\delta^*) &= \\sup_{\\pi} r(\\pi, \\delta^*) & \\text{(Max risk = Max average risk)} \\\\\n&\\ge r(\\pi_0, \\delta^*) & \\text{(Supremum $\\ge$ specific value)} \\\\\n&\\ge r(\\pi_0, \\delta_0) & \\text{(From Inequality A: $\\delta_0$ is Bayes for $\\pi_0$)} \\\\\n&= \\sup_{\\theta} R(\\theta, \\delta_0) & \\text{(From Step 3)}\n\\end{aligned}\n\\]\n5. Conclusion: We have shown that for any \\(\\delta^*\\): \\[ \\sup_{\\theta} R(\\theta, \\delta^*) \\ge \\sup_{\\theta} R(\\theta, \\delta_0) \\] Thus, \\(\\delta_0\\) minimizes the maximum risk. \\(\\delta_0\\) is Minimax. \\(\\blacksquare\\)\n\n\n\n2.8.4.2 Alternating Optimization on the Risk Surface\nThe Minimax solution can be found computationally by iteratively optimizing one variable while holding the other fixed.\n\nFix Prior \\(\\pi\\), Minimize Risk: We search the valley bottom for the current \\(\\pi\\).\nFix Rule \\(\\delta\\), Maximize Risk: We search the hill top for the current \\(\\delta\\).\n\nThis creates a “zigzag” path on the surface that converges to the saddle point.\n\n\n\n\n\n\n\n\nFigure 2.7: The ‘Wiggle Mountain’ of Risk. The surface represents Bayes Risk \\(r(\\pi, \\delta)\\). The white zigzag line shows the iterative algorithm: starting from an arbitrary prior, we alternate between finding the best \\(\\delta\\) (moving along the valley) and the worst \\(\\pi\\) (climbing the hill). This path spirals inward, converging to the red Saddle Point (Minimax solution) in the center.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#admissibility-of-bayes-rules",
    "href": "decision.html#admissibility-of-bayes-rules",
    "title": "2  Decision Theory",
    "section": "2.9 Admissibility of Bayes Rules",
    "text": "2.9 Admissibility of Bayes Rules\nBayes rules are generally good candidates for admissibility. If a rule is Bayes, it is likely efficient, provided the prior doesn’t ignore parts of the parameter space.\n\nTheorem 2.4 (Admissibility of Bayes Rules (Finite Support)) If the parameter space \\(\\Theta\\) is finite (or countable) and the prior \\(\\pi\\) assigns positive probability to every \\(\\theta \\in \\Theta\\) (i.e., \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\)), then any Bayes rule \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) that dominates it. By definition of domination:\n\n\\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\).\n\\(R(\\theta_k, \\delta') &lt; R(\\theta_k, \\delta_\\pi)\\) for at least one \\(\\theta_k\\).\n\nBayes Risk Difference: Consider the difference in Bayes risk: \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') = \\sum_{\\theta \\in \\Theta} \\pi(\\theta) [R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\]\nStrict Positivity:\n\nSince \\(\\delta'\\) dominates \\(\\delta_\\pi\\), each term \\([R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\) is non-negative (\\(\\ge 0\\)).\nAt \\(\\theta_k\\), the term is strictly positive (\\(&gt; 0\\)).\nWe assumed the prior has full support, so \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\).\n\nSummation: A sum of non-negative terms where at least one term is strictly positive must be strictly positive. \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') &gt; 0 \\implies r(\\pi, \\delta') &lt; r(\\pi, \\delta_\\pi)\\]\nConclusion: This contradicts the definition that \\(\\delta_\\pi\\) is a Bayes rule (which must minimize Bayes risk). Therefore, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)\n\n\n\n2.9.1 Admissibility of Unique Bayes Rules\nIf the Bayes rule is unique, we can drop the requirement that the parameter space be discrete or finite.\n\nTheorem 2.5 (Admissibility of Unique Bayes Rules) Let \\(\\delta_\\pi\\) be a Bayes rule with respect to \\(\\pi\\). If \\(\\delta_\\pi\\) is the unique Bayes rule (up to risk equivalence), then \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) such that: \\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\), with strict inequality for some set of \\(\\theta\\).\nBayes Risk Inequality: Taking the expectation with respect to \\(\\pi\\): \\[r(\\pi, \\delta') = \\int R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\int R(\\theta, \\delta_\\pi) \\pi(\\theta) d\\theta = r(\\pi, \\delta_\\pi)\\]\nMinimality: Since \\(\\delta_\\pi\\) is Bayes, it minimizes the risk, so \\(r(\\pi, \\delta_\\pi) \\le r(\\pi, \\delta')\\). Combining these gives \\(r(\\pi, \\delta') = r(\\pi, \\delta_\\pi)\\).\nUniqueness: This implies that \\(\\delta'\\) is also a Bayes rule. However, we assumed that \\(\\delta_\\pi\\) is the unique Bayes rule. Therefore, \\(\\delta'\\) must be equal to \\(\\delta_\\pi\\) (in terms of risk functions).\nConclusion: If \\(\\delta'\\) and \\(\\delta_\\pi\\) have identical risk functions, then \\(\\delta'\\) cannot strictly dominate \\(\\delta_\\pi\\). This contradicts the assumption of inadmissibility. Thus, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "3  Bayesian Inference",
    "section": "",
    "text": "3.1 Posterior Distributions\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#posterior-distributions",
    "href": "bayesian.html#posterior-distributions",
    "title": "3  Bayesian Inference",
    "section": "",
    "text": "Definition 3.1 (Posterior Distribution) Suppose we have a parameter \\(\\theta\\) with a prior distribution denoted by \\(\\pi(\\theta)\\). If we observe data \\(x\\) drawn from a distribution with probability density function (pdf) \\(f(x; \\theta)\\), then the posterior density of \\(\\theta\\) given the data \\(x\\) is defined as:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{m(x)}\n\\]\nwhere \\(m(x)\\) is the marginal distribution (or marginal likelihood) of the data, calculated as: \\[\nm(x) = \\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta\n\\]\nIn this context, \\(m(x)\\) acts as a normalizing constant. Since it depends only on the data \\(x\\) and not on the parameter \\(\\theta\\), it ensures that the posterior density integrates to 1 but does not influence the shape of the posterior distribution.\nThus, we often state the proportional relationship:\n\\[\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n\\]\n\n\n3.1.1 Discrete Posterior Calculation\n\nExample 3.1 (Discrete Posterior Calculation) Consider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\n\nLikelihood \\(\\pi(x|\\theta)\\):\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). The calculation of the posterior probabilities is summarized in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta=1\\)\n\\(\\theta=2\\)\n\\(\\theta=3\\)\nSum\n\n\n\n\nPrior \\(\\pi(\\theta)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1\\)\n\n\nLikelihood \\(\\pi(x=2|\\theta)\\)\n\\(0\\)\n\\(1/3\\)\n\\(1/4\\)\n-\n\n\nProduct \\(\\pi(\\theta)\\pi(x|\\theta)\\)\n\\(0\\)\n\\(1/9\\)\n\\(1/12\\)\n\\(7/36\\)\n\n\nPosterior \\(\\pi(\\theta|x)\\)\n\\(0\\)\n\\(4/7\\)\n\\(3/7\\)\n\\(1\\)\n\n\n\nThe marginal sum (evidence) is calculated as \\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\\). The posterior values are obtained by dividing the product row by this sum.\n\n\n\n3.1.2 Binomial-beta Conjugacy\n\nExample 3.2 (Binomial-beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E^{\\theta|X}[\\theta] = \\frac{a+X}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{X}{n}\\).\nThe posterior variance is: \\[\\text{Var}^{\\theta|X}(\\theta) = \\frac{(a+X)(n+b-X)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{X(n-X)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\nNumerical Illustration:\nSuppose we are estimating a probability \\(\\theta\\).\n\nPrior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (Mean = 0.5).\n\nData: 10 trials, 8 successes (\\(n=10, x=8\\)).\n\nPosterior: \\(\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)\\) (Mean \\(\\approx\\) 0.71).\n\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\n\n\n\n\n\n\n\nFigure 3.1: Prior vs Posterior for Beta-Binomial Example\n\n\n\n\n\n\n\n3.1.3 Normal-normal Conjugacy (known Variance)\n\nExample 3.3 (Normal-normal Conjugacy (known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nNumerical Illustration:\nSuppose we estimate a mean height \\(\\mu\\).\n\nKnown Variance: \\(\\sigma^2 = 100\\) (\\(\\tau = 0.01\\)).\n\nPrior: \\(\\mu \\sim N(175, 25)\\) (Precision \\(\\tau_0 = 0.04\\)).\n\nData: \\(n=10, \\bar{x}=180\\). (Total data precision \\(n\\tau = 0.1\\)).\n\nPosterior:\nPrecision \\(\\tau_1 = 0.04 + 0.1 = 0.14\\).\nVariance \\(\\sigma_1^2 \\approx 7.14\\).\nMean \\(\\mu_1 = \\frac{175(0.04) + 180(0.1)}{0.14} \\approx 178.6\\).\n\n\nFigure 3.2 illustrates the prior (dashed) and posterior (solid) normal densities.\n\n\n\nCode\nmu_vals &lt;- seq(150, 200, length.out = 200)\n\n# Prior: N(175, 25) -&gt; SD = 5\nprior_norm &lt;- dnorm(mu_vals, mean = 175, sd = 5)\n\n# Posterior: N(178.6, 7.14) -&gt; SD = Sqrt(7.14) Approx 2.67\nposterior_norm &lt;- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))\n\nplot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Density\",\n     main = \"Normal Prior vs Posterior\",\n     ylim = c(0, max(c(prior_norm, posterior_norm))))\nlines(mu_vals, prior_norm, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior N(175, 25)\", \"Posterior N(178.6, 7.14)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 3.2: Prior vs Posterior for Normal-Normal Example\n\n\n\n\n\n\n\n3.1.4 Normal with Unknown Mean and Variance\n\nExample 3.4 (Normal with Unknown Mean and Variance) Consider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior with parameters \\(\\mu_0, \\tau_0, \\alpha_0, w_0\\):\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha_0/2, \\alpha_0 w_0/2)\\) \\[ \\pi(\\tau) \\propto \\tau^{\\alpha_0/2 - 1} \\exp\\left\\{ -\\frac{\\alpha_0 w_0}{2} \\tau \\right\\} \\]\n\\(\\mu|\\tau \\sim N(\\mu_0, 1/(\\tau_0\\tau))\\) \\[ \\pi(\\mu|\\tau) \\propto \\tau^{1/2} \\exp\\left\\{ -\\frac{\\tau_0\\tau}{2}(\\mu-\\mu_0)^2 \\right\\} \\]\n\nThe joint prior is: \\[\n\\pi(\\mu, \\tau) \\propto \\tau^{(\\alpha_0+1)/2 - 1} \\exp\\left\\{ -\\frac{\\tau}{2} \\left( \\alpha_0 w_0 + \\tau_0(\\mu - \\mu_0)^2 \\right) \\right\\}\n\\]\nThe Likelihood:\nTo derive the Maximum Likelihood Estimators (MLEs), we work with the log-likelihood function \\(l(\\mu, \\tau) = \\log L(\\mu, \\tau)\\):\n\\[\n\\begin{aligned}\nl(\\mu, \\tau) &= \\log \\left( \\tau^{n/2} \\exp\\left\\{ -\\frac{\\tau}{2} [ S_{xx} + n(\\bar{x}-\\mu)^2 ] \\right\\} \\right) \\\\\n&= \\frac{n}{2} \\log \\tau - \\frac{\\tau}{2} \\left[ S_{xx} + n(\\bar{x}-\\mu)^2 \\right] + \\text{const}\n\\end{aligned}\n\\]\nMLE for \\(\\mu\\)\nDifferentiating \\(l(\\mu, \\tau)\\) with respect to \\(\\mu\\) and setting to zero:\n\\[\n\\frac{\\partial l}{\\partial \\mu} = n\\tau(\\bar{x} - \\mu) = 0 \\implies \\hat{\\mu}_{\\text{MLE}} = \\bar{x}\n\\]\nMLE for \\(\\sigma^2\\)\nDifferentiating \\(l(\\mu, \\tau)\\) with respect to \\(\\tau\\), setting to zero, and substituting \\(\\mu = \\bar{x}\\):\n\\[\n\\frac{\\partial l}{\\partial \\tau} = \\frac{n}{2\\tau} - \\frac{S_{xx}}{2} = 0 \\implies \\hat{\\tau}_{\\text{MLE}} = \\frac{n}{S_{xx}}\n\\]\nUsing the invariance property (\\(\\sigma^2 = 1/\\tau\\)):\n\\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{S_{xx}}{n} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n}\n\\]\nDerivation of the Posterior:\nMultiplying the prior by the likelihood gives the joint posterior density. We organize the terms to separate the marginal distribution of \\(\\tau\\) from the conditional distribution of \\(\\mu\\):\n\\[\n\\begin{aligned}\n\\pi(\\mu, \\tau | x) &\\propto \\underbrace{\\tau^{(\\alpha_0 + n)/2 - 1} \\exp\\left\\{ -\\frac{\\tau}{2} \\left[ \\alpha_0 w_0 + S_{xx} + \\frac{n\\tau_0}{n+\\tau_0}(\\bar{x}-\\mu_0)^2 \\right] \\right\\}}_{\\text{Marginal of } \\tau} \\\\\n&\\quad \\times \\underbrace{\\tau^{1/2} \\exp\\left\\{ -\\frac{(n+\\tau_0)\\tau}{2} \\left( \\mu - \\frac{\\tau_0\\mu_0+n\\bar{x}}{n+\\tau_0} \\right)^2 \\right\\}}_{\\text{Conditional of } \\mu|\\tau}\n\\end{aligned}\n\\]\nResults:\n\nConditional Posterior of \\(\\mu|\\tau, x\\): \\[ \\mu|\\tau, x \\sim N(\\mu', 1/(\\tau'\\tau)) \\] \\[ E^{\\mu|\\tau, X}[\\mu] = \\frac{\\tau_0\\mu_0 + n\\bar{x}}{\\tau_0 + n} \\] where \\[\\tau' = \\tau_0 + n\\] \\[\\mu' = \\frac{\\tau_0\\mu_0 + n\\bar{x}}{\\tau_0 + n}\\]\nMarginal Posterior of \\(\\tau|x\\): The marginal posterior is \\(\\tau|x \\sim \\text{Gamma}(\\alpha', \\beta')\\) with: \\[\n  \\alpha' = \\frac{\\alpha_0 + n}{2}, \\quad \\beta' = \\frac{\\alpha_0 w_0 + n\\hat{\\sigma}^2_{\\text{MLE}} + \\frac{n\\tau_0}{n+\\tau_0}(\\bar{x}-\\mu_0)^2}{2}\n  \\]\nUsing the approximation \\(E^{\\sigma^2|X}[\\sigma^2] \\approx 1/E^{\\tau|X}[\\tau] = \\beta'/\\alpha'\\), the posterior expectation of the variance is a weighted average of the prior variance, the data variance, and the discrepancy between the prior and data means:\n\\[\n  E^{\\sigma^2|X}[\\sigma^2] \\approx \\frac{\\alpha_0 w_0 + n\\hat{\\sigma}^2_{\\text{MLE}} + \\frac{1}{1/n+1/\\tau_0}(\\bar{x}-\\mu_0)^2}{\\alpha_0 + n}\n  \\]\nConditional Posterior of \\(\\tau|\\mu, x\\): If \\(\\mu\\) is considered known, the posterior for \\(\\tau\\) combines the prior \\(\\alpha_0, w_0\\) with the deviations from \\(\\mu\\). Note that the prior term \\(\\pi(\\mu|\\tau)\\) contributes an extra factor of \\(\\tau^{1/2}\\) to the shape.\n\\[ \\tau|\\mu, x \\sim \\text{Gamma}(\\alpha'', \\beta'') \\]\nWhere: \\[ \\alpha'' = \\frac{\\alpha_0 + n + 1}{2}, \\quad \\beta'' = \\frac{\\alpha_0 w_0 + \\sum_{i=1}^n (x_i-\\mu)^2 + \\tau_0(\\mu-\\mu_0)^2}{2} \\]\nThe approximate expectation of the variance is: \\[\n  E^{\\sigma^2|\\mu, X}[\\sigma^2] \\approx \\frac{\\alpha_0 w_0 + \\sum_{i=1}^n (x_i-\\mu)^2 + \\tau_0(\\mu-\\mu_0)^2}{\\alpha_0 + n + 1}\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#finding-bayes-rules-via-minimizing-posterior-expected-loss",
    "href": "bayesian.html#finding-bayes-rules-via-minimizing-posterior-expected-loss",
    "title": "3  Bayesian Inference",
    "section": "3.2 Finding Bayes Rules via Minimizing Posterior Expected Loss",
    "text": "3.2 Finding Bayes Rules via Minimizing Posterior Expected Loss\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk) Setup and Notation:\n\n\\(\\theta \\in \\Theta\\): parameter of interest (unknown state of nature)\n\n\\(x \\in X\\): observed data\n\n\\(\\pi(\\theta)\\): prior probability distribution over the parameter space\n\n\\(f(x;\\theta)\\): likelihood or sampling distribution of the data given the parameter\n\n\\(d: X \\to A\\): decision rule mapping observed data to an action/decision\n\n\\(\\mathcal{L}(\\theta, a)\\): loss function measuring the loss incurred when the true parameter is \\(\\theta\\) and action \\(a\\) is taken\n\n\nDefinition:\n\nRisk Function: For a given decision rule \\(d\\) and parameter value \\(\\theta\\), \\[R(\\theta, d) = \\int_{X} \\mathcal{L}(\\theta, d(x)) f(x;\\theta) dx = E^{X|\\theta} [ \\mathcal{L}(\\theta, d(X)) ]\\] is the expected loss with respect to the sampling distribution when the true parameter is \\(\\theta\\).\nBayes Risk: For a decision rule \\(d\\) and prior distribution \\(\\pi\\), \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta = E^\\theta [ R(\\theta, d) ]\\] is the expected risk averaging over both the parameter uncertainty (prior) and the data variability (likelihood).\nPosterior Bayes Loss: The minimum possible expected loss given observed data \\(x\\) is denoted as \\(\\rho^{\\text{Bayes}}(\\pi, x)\\). It represents the expected posterior loss of the Bayes rule: \\[\n\\rho^{\\text{Bayes}}(\\pi, x) = \\inf_{d} E^{\\theta|x} [ \\mathcal{L}(\\theta, d) ]\n\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) is defined as \\[\nd^{\\text{Bayes}}(x) = \\underset{a}{\\arg\\min} \\ E^{\\theta|X} [ \\mathcal{L}(\\theta, a) ]\n\\] The value of the minimum expected posterior loss is \\(\\rho^{\\text{Bayes}}(\\pi, x)\\).\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} \\mathcal{L}(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} \\mathcal{L}(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} \\mathcal{L}(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} \\mathcal{L}(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} \\mathcal{L}(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E^{\\theta|X} [ \\mathcal{L}(\\theta, d(X)) ]\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nTherefore, to minimize the Bayes risk, one effectively minimizes the posterior expected loss for each \\(x\\). This relationship relies on the key identity for the total expectation of the loss:\n\\[\n\\boxed{ r(\\pi, d) = E^X \\left[ E^{\\theta|X} \\big( \\mathcal{L}(\\theta, d(X)) \\big) \\right] = E^\\theta \\left[ E^{X|\\theta} \\big( \\mathcal{L}(\\theta, d(X)) \\big) \\right] }\n\\]\nIn the first expression, the outer expectation \\(E^X\\) is taken with respect to the marginal density of the data, \\(m(x)\\), defined as: \\[\nm(x) = \\int_{\\Theta} f(x|\\theta)\\pi(\\theta) d\\theta\n\\]\nIn the second expression, the outer expectation \\(E^\\theta\\) is taken with respect to the prior density \\(\\pi(\\theta)\\).\n\n\nThe following diagram summarizes the general workflow for deriving a Bayes estimator:\n\n\n\n\n\n\n\n\nFigure 3.3: Workflow for Finding the Bayes Rule",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#special-bayes-rules",
    "href": "bayesian.html#special-bayes-rules",
    "title": "3  Bayesian Inference",
    "section": "3.3 Special Bayes Rules",
    "text": "3.3 Special Bayes Rules\n\n3.3.1 Squared Error Loss (point Estimate)\n\\[\\mathcal{L}(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize the posterior expected loss \\(E^{\\theta|X}[(\\theta - d(X))^2]\\). Taking the derivative with respect to \\(d\\) and setting it to 0:\n\\[-2 E^{\\theta|X}(\\theta - d) = 0 \\implies d(X) = E^{\\theta|X}[\\theta]\\]\nResult: The Bayes rule under squared error loss is the posterior mean.\n\n\n3.3.2 Scale-Invariant Squared Error Loss\nConsider the loss function that penalizes relative errors rather than absolute errors. This is particularly useful when the magnitude of the parameter \\(\\theta\\) varies significantly, and an error of 1.0 is “worse” when \\(\\theta=1\\) than when \\(\\theta=1000\\).\n\\[\n\\mathcal{L}(\\theta, d) = \\left( \\frac{d - \\theta}{\\theta} \\right)^2 = \\left( \\frac{d}{\\theta} - 1 \\right)^2\n\\]\nTo find the Bayes rule, we minimize the posterior expected loss \\(E^{\\theta|X} [\\mathcal{L}(\\theta, d)]\\):\n\\[\nQ(d) = E^{\\theta|X} \\left[ \\frac{d^2}{\\theta^2} - \\frac{2d}{\\theta} + 1 \\right] = d^2 E^{\\theta|X}[\\theta^{-2}] - 2d E^{\\theta|X}[\\theta^{-1}] + 1\n\\]\nDifferentiating with respect to \\(d\\) and setting to zero:\n\\[\n\\frac{\\partial Q}{\\partial d} = 2d E^{\\theta|X}[\\theta^{-2}] - 2 E^{\\theta|X}[\\theta^{-1}] = 0\n\\]\nSolving for \\(d\\):\n\\[\nd(X) = \\frac{E^{\\theta|X}[\\theta^{-1}]}{E^{\\theta|X}[\\theta^{-2}]}\n\\]\nResult: The Bayes rule under scale-invariant squared error loss is the ratio of the posterior mean of \\(\\theta^{-1}\\) to the posterior mean of \\(\\theta^{-2}\\).\n\n\n3.3.3 Absolute Error Loss\n\\[\\mathcal{L}(\\theta, d) = |\\theta - d|\\]\nTo find the Bayes rule, we minimize the posterior expected loss:\n\\[\n\\psi(d) = E^{\\theta|X} [ |\\theta - d| ] = \\int_{-\\infty}^{\\infty} |\\theta - d| \\, dF(\\theta|x)\n\\]\nwhere \\(F(\\theta|x)\\) is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point \\(d\\):\n\\[\n\\psi(d) = \\int_{-\\infty}^{d} (d - \\theta) \\, dF(\\theta|x) + \\int_{d}^{\\infty} (\\theta - d) \\, dF(\\theta|x)\n\\]\nWe find the minimum by analyzing the rate of change of \\(\\psi(d)\\) with respect to \\(d\\). Differentiating (or taking the subgradient for non-differentiable points):\n\\[\n\\frac{\\partial}{\\partial d} \\psi(d) = \\int_{-\\infty}^{d} 1 \\, dF(\\theta|x) - \\int_{d}^{\\infty} 1 \\, dF(\\theta|x) = P(\\theta \\le d|x) - P(\\theta &gt; d|x)\n\\]\nSetting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:\n\\[\nP(\\theta \\le d|x) = P(\\theta &gt; d|x)\n\\]\nSince the total probability is 1, this condition simplifies to finding \\(d\\) such that the cumulative probability is \\(1/2\\).\nGeneral Case (Discrete or Mixed Distributions)\nIn cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to \\(F(d) = 0.5\\) may not exist. To generalize, the Bayes rule is defined as any median \\(m\\) of the posterior distribution.\nA median is formally defined as any value \\(m\\) that satisfies the following two conditions simultaneously:\n\n\\(P(\\theta \\le m|x) \\ge \\frac{1}{2}\\)\n\n\\(P(\\theta \\ge m|x) \\ge \\frac{1}{2}\\)\n\n\nResult: The Bayes rule under absolute error loss is the posterior median.\n\n\n3.3.4 Weighted Absolute Error Loss (min-normalization)\n\\[\\mathcal{L}(\\theta, d) = \\frac{|\\theta - d|}{\\min(\\theta, 1-\\theta)}\\]\nThis loss function penalizes errors extremely heavily when the true parameter \\(\\theta\\) is near the boundaries (0 or 1). Because the denominator approaches zero at the boundaries, the “cost” of an error becomes infinite, forcing the estimator to be very cautious (conservative) if the posterior has significant mass near 0 or 1.\nTo find the Bayes rule, we minimize the posterior expected loss. Let \\(\\pi(\\theta|x)\\) denote the posterior density.\n\\[\n\\psi(d) = E^{\\theta|X} \\left[ \\frac{|\\theta - d|}{\\min(\\theta, 1-\\theta)} \\right] = \\int \\frac{|\\theta - d|}{\\min(\\theta, 1-\\theta)} \\pi(\\theta|x) \\, d\\theta\n\\]\nLet \\(w(\\theta) = \\frac{1}{\\min(\\theta, 1-\\theta)}\\). We can view this integral as an expectation with respect to a weighted posterior density \\(\\pi^*(\\theta|x)\\):\n\\[\n\\pi^*(\\theta|x) \\propto w(\\theta) \\pi(\\theta|x) = \\frac{\\pi(\\theta|x)}{\\min(\\theta, 1-\\theta)}\n\\]\nResult: The Bayes rule is the median of the weighted posterior distribution \\(\\pi^*(\\theta|x)\\).\n\n\n3.3.4.0.1 Importance Sampling for Weighted Median\nGoal: Estimate the median of \\(\\pi^*(\\theta|x) \\propto w(\\theta)\\pi(\\theta|x)\\) using samples from \\(\\pi(\\theta|x)\\).\n\nSample: Generate \\(M\\) independent draws \\(\\theta_1, \\dots, \\theta_M\\) from the standard posterior \\(\\pi(\\theta|x)\\).\nWeight: For each \\(i = 1, \\dots, M\\), compute the importance weight: \\[ W_i = w(\\theta_i) = \\frac{1}{\\min(\\theta_i, 1-\\theta_i)} \\]\nSort: Reorder the samples such that \\(\\theta_{(1)} \\le \\theta_{(2)} \\le \\dots \\le \\theta_{(M)}\\). Permute the weights \\(W_{(1)}, \\dots, W_{(M)}\\) to match this ordering.\nAccumulate: Compute the cumulative weights: \\[ S_k = \\sum_{j=1}^k W_{(j)} \\quad \\text{for } k=1, \\dots, M \\]\nSelect: Find the smallest index \\(k^*\\) such that the cumulative weight exceeds half the total weight: \\[ k^* = \\min \\{ k : S_k \\ge 0.5 \\times S_M \\} \\]\nOutput: Return the estimator \\(\\hat{\\delta} = \\theta_{(k^*)}\\).\n\n\n\nNumerical Example: Beta(2, 10)\nWe compare the “exact” weighted median (found by numerical integration) with the Monte Carlo estimate for a skewed distribution.\n\n\nCode\n# 1. Setup\nset.seed(2025) \nM &lt;- 10\nalpha &lt;- 2\nbeta &lt;- 10\n\ntheta_samples &lt;- rbeta(M, alpha, beta)\nw &lt;- function(theta) { 1 / pmin(theta, 1 - theta) }\n\n# 2. Process\nweights &lt;- w(theta_samples)\nord &lt;- order(theta_samples)\nsorted_theta &lt;- theta_samples[ord]\nsorted_weights &lt;- weights[ord]\ncum_weights &lt;- cumsum(sorted_weights)\ntotal_weight &lt;- sum(sorted_weights)\nthreshold &lt;- 0.5 * total_weight\n\n# Find k\nk_idx &lt;- which(cum_weights &gt;= threshold)[1]\n\n# 3. Create Data Frame\nselection_table &lt;- data.frame(\n  idx = 1:M,\n  theta = sorted_theta,\n  weight = sorted_weights,\n  cum_weight = cum_weights,\n  check = ifelse(cum_weights &gt;= threshold, \"$\\\\ge$ Threshold\", \"$&lt;$ Threshold\"),\n  sel = ifelse(1:M == k_idx, \"$\\\\leftarrow k$ (Median)\", \"\")\n)\n\n# 4. Set LaTeX Column Names\n\n# Note: We use double backslashes \\\\ for LaTeX commands inside R strings\ncolnames(selection_table) &lt;- c(\n  \"$i$\",\n  \"$\\\\theta_{(i)}$\",        # Sorted Theta\n  \"$w_{(i)}$\",              # Sorted Weight\n  \"$\\\\sum_{j=1}^i w_{(j)}$\",# Cumulative Sum\n  \"Condition\",\n  \"Selection\"\n)\n\n# Print Context\ncat(\"Total Weight ($\\\\sum w_i$):\", total_weight, \"\\n\")\n\n\nTotal Weight ($\\sum w_i$): 48.91008 \n\n\nCode\ncat(\"Threshold ($0.5 \\\\times \\\\sum w_i$):\", threshold, \"\\n\\n\")\n\n\nThreshold ($0.5 \\times \\sum w_i$): 24.45504 \n\n\nCode\n# 5. Render Table with escape = FALSE\n\n# escape = FALSE is crucial; otherwise, it prints the dollar signs literally\nknitr::kable(selection_table, \n             digits = 4, \n             align = \"c\",\n             escape = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(\\theta_{(i)}\\)\n\\(w_{(i)}\\)\n\\(\\sum_{j=1}^i w_{(j)}\\)\nCondition\nSelection\n\n\n\n\n1\n0.1256\n7.9601\n7.9601\n\\(&lt;\\) Threshold\n\n\n\n2\n0.1462\n6.8412\n14.8013\n\\(&lt;\\) Threshold\n\n\n\n3\n0.1563\n6.3965\n21.1978\n\\(&lt;\\) Threshold\n\n\n\n4\n0.1714\n5.8329\n27.0307\n\\(\\ge\\) Threshold\n\\(\\leftarrow k\\) (Median)\n\n\n5\n0.2221\n4.5024\n31.5330\n\\(\\ge\\) Threshold\n\n\n\n6\n0.2265\n4.4144\n35.9475\n\\(\\ge\\) Threshold\n\n\n\n7\n0.2676\n3.7376\n39.6850\n\\(\\ge\\) Threshold\n\n\n\n8\n0.2840\n3.5214\n43.2064\n\\(\\ge\\) Threshold\n\n\n\n9\n0.2990\n3.3445\n46.5509\n\\(\\ge\\) Threshold\n\n\n\n10\n0.4239\n2.3592\n48.9101\n\\(\\ge\\) Threshold\n\n\n\n\n\n\nActual Weighted Median with 1000 draws of \\(\\theta\\)\n\n\nCode\n# 1. Setup Parameters\nset.seed(123)\nM &lt;- 1000\nalpha &lt;- 2\nbeta &lt;- 10\n\n# 2. Generate Samples and Weights\ntheta_samples &lt;- rbeta(M, alpha, beta)\n\n# Weight function: w(theta) = 1 / min(theta, 1-theta)\nw &lt;- function(theta) { 1 / pmin(theta, 1 - theta) }\nweights &lt;- w(theta_samples)\n\n# 3. Sort and Calculate Cumulative Weights\nord &lt;- order(theta_samples)\nsorted_theta &lt;- theta_samples[ord]\nsorted_weights &lt;- weights[ord]\n\ncum_weights &lt;- cumsum(sorted_weights)\ntotal_weight &lt;- sum(sorted_weights)\nthreshold &lt;- 0.5 * total_weight\n\n# 4. Find the Weighted Median Index k\nk_idx &lt;- which(cum_weights &gt;= threshold)[1]\nmc_weighted_median &lt;- sorted_theta[k_idx]\n\n# 5. Compare with Theoretical Value (calculated previously)\n\n# Re-calculating theoretical for completeness of this chunk\nweighted_dens_unnorm &lt;- function(theta) { w(theta) * dbeta(theta, alpha, beta) }\nC &lt;- integrate(weighted_dens_unnorm, 0, 1)$value\nweighted_cdf &lt;- function(q) { integrate(weighted_dens_unnorm, 0, q)$value / C }\ntheo_median &lt;- uniroot(function(x) weighted_cdf(x) - 0.5, c(0.001, 0.999))$root\n\n# 6. Display Results\nresults &lt;- data.frame(\n  \"Method\" = c(\"Theoretical (Integration)\", \"Monte Carlo (M=1000)\", \"Standard Median (Unweighted)\"),\n  \"Value\" = c(theo_median, mc_weighted_median, qbeta(0.5, alpha, beta))\n)\n\nknitr::kable(results, digits = 4, align = \"l\", caption = \"Weighted Median Estimation\")\n\n\n\nWeighted Median Estimation\n\n\nMethod\nValue\n\n\n\n\nTheoretical (Integration)\n0.0670\n\n\nMonte Carlo (M=1000)\n0.0692\n\n\nStandard Median (Unweighted)\n0.1480\n\n\n\n\n\n\n\n3.3.5 Hypothesis Testing (0-1 Loss)\nConsider the hypothesis test \\(H_0: \\theta \\in \\Theta_0\\) versus \\(H_1: \\theta \\in \\Theta_1\\). We define the decision space as \\(\\mathcal{A} = \\{0, 1\\}\\), where \\(a=0\\) means accepting \\(H_0\\) and \\(a=1\\) means rejecting \\(H_0\\) (accepting \\(H_1\\)).\nCase 1: 0-1 Loss\nThe standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one:\n\n\n\nTable 3.1: Standard 0-1 Loss Function\n\n\n\n\n\n\n\n\n\n\nState of Nature (\\(\\theta\\))\nAction \\(a=0\\) (Accept \\(H_0\\))\nAction \\(a=1\\) (Reject \\(H_0\\))\n\n\n\n\n\\(\\theta \\in \\Theta_0\\) (\\(H_0\\) True)\n\\(0\\) (Correct)\n\\(1\\) (Type I Error)\n\n\n\\(\\theta \\in \\Theta_1\\) (\\(H_1\\) True)\n\\(1\\) (Type II Error)\n\\(0\\) (Correct)\n\n\n\n\n\n\nTo find the Bayes rule, we minimize the posterior expected loss for a given \\(x\\), denoted as \\(E^{\\theta|X}[\\mathcal{L}(\\theta, a)]\\).\n\nExpected Loss for choosing \\(a=0\\) (Accept \\(H_0\\)): \\[\n  E^{\\theta|X}[\\mathcal{L}(\\theta, 0)] = 0 \\cdot P(\\theta \\in \\Theta_0|x) + 1 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_1|x)\n  \\]\nExpected Loss for choosing \\(a=1\\) (Reject \\(H_0\\)): \\[\n  E^{\\theta|X}[\\mathcal{L}(\\theta, 1)] = 1 \\cdot P(\\theta \\in \\Theta_0|x) + 0 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_0|x)\n  \\]\n\nThe Bayes rule selects the action with the smaller expected loss. Thus, we choose \\(a=1\\) if: \\[\nP(\\theta \\in \\Theta_0|x) \\le P(\\theta \\in \\Theta_1|x)\n\\] This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability. The optimal Bayes decision rule \\(d(x)\\) is given by:\n\\[\nd(x) = \\begin{cases}\n1 & \\text{if } P(\\Theta_0|x) \\le \\frac{1}{2} \\quad (\\text{Reject } H_0) \\\\\n0 & \\text{if } P(\\Theta_0|x) &gt; \\frac{1}{2} \\quad (\\text{Accept } H_0)\n\\end{cases}\n\\]\nCase 2: General Loss (Asymmetric Costs)\nIn many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let \\(c_1\\) be the cost of a Type I error and \\(c_2\\) be the cost of a Type II error. Usually, we normalize one cost to 1.\n\n\n\nTable 3.2: Loss Function with Type I Error Cost \\(c\\)\n\n\n\n\n\n\n\n\n\n\nState of Nature (\\(\\theta\\))\nAction \\(a=0\\) (Accept \\(H_0\\))\nAction \\(a=1\\) (Reject \\(H_0\\))\n\n\n\n\n\\(\\theta \\in \\Theta_0\\) (\\(H_0\\) True)\n\\(0\\)\n\\(c\\) (Type I Error)\n\n\n\\(\\theta \\in \\Theta_1\\) (\\(H_1\\) True)\n\\(1\\) (Type II Error)\n\\(0\\)\n\n\n\n\n\n\nWe again calculate the posterior expected loss:\n\nExpected Loss for \\(a=0\\): \\[E^{\\theta|X}[\\mathcal{L}(\\theta, 0)] = 0 \\cdot P(\\Theta_0|x) + 1 \\cdot P(\\Theta_1|x) = P(\\Theta_1|x)\\]\nExpected Loss for \\(a=1\\): \\[E^{\\theta|X}[\\mathcal{L}(\\theta, 1)] = c \\cdot P(\\Theta_0|x) + 0 \\cdot P(\\Theta_1|x) = c P(\\Theta_0|x)\\]\n\nWe reject \\(H_0\\) (\\(a=1\\)) if the expected loss of doing so is lower: \\[\nc P(\\Theta_0|x) \\le P(\\Theta_1|x)\n\\]\nSince \\(P(\\Theta_1|x) = 1 - P(\\Theta_0|x)\\), we can rewrite this condition as: \\[\nc P(\\Theta_0|x) \\le 1 - P(\\Theta_0|x) \\implies (1+c) P(\\Theta_0|x) \\le 1\n\\] \\[\nP(\\Theta_0|x) \\le \\frac{1}{1+c}\n\\]\nResult: With asymmetric costs, we accept \\(H_1\\) only if the posterior probability of the null hypothesis is sufficiently small (below the threshold \\(\\frac{1}{1+c}\\)). If the cost of false rejection \\(c\\) is high, we require stronger evidence against \\(H_0\\). The optimal Bayes decision rule \\(d(x)\\) is given by:\n\\[\nd(x) = \\begin{cases}\n1 & \\text{if } P(\\Theta_0|x) \\le \\frac{1}{1+c} \\quad (\\text{Reject } H_0) \\\\\n0 & \\text{if } P(\\Theta_0|x) &gt; \\frac{1}{1+c} \\quad (\\text{Accept } H_0)\n\\end{cases}\n\\]\n\n\n3.3.6 Classification Prediction\nIn classification problems, the parameter of interest is a discrete class label \\(y\\) taking values in a set of categories \\(\\{1, 2, \\dots, K\\}\\). The goal is to predict the true class label based on observed features \\(x\\).\nWe typically employ the 0-1 loss function, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:\n\\[\\mathcal{L}(y, \\hat{y}) = \\begin{cases} 0 & \\text{if } \\hat{y} = y \\ (\\text{Correct Classification}) \\\\ 1 & \\text{if } \\hat{y} \\neq y \\ (\\text{Misclassification}) \\end{cases}\\]\nTo find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.\n\\[\nE^{Y|X}[\\mathcal{L}(y, \\hat{y})] = \\sum_{y} \\mathcal{L}(y, \\hat{y}) P(y|x)\n\\]\nSince the loss is 1 only when the predicted class \\(\\hat{y}\\) differs from the true class \\(y\\), this sum simplifies to:\n\\[\nE^{Y|X}[\\mathcal{L}(y, \\hat{y})] = \\sum_{y \\neq \\hat{y}} 1 \\cdot P(y|x) = P(y \\neq \\hat{y} | x) = 1 - P(y = \\hat{y} | x)\n\\]\nMinimizing the misclassification rate \\(1 - P(y = \\hat{y} | x)\\) is mathematically equivalent to maximizing the probability of being correct, \\(P(y = \\hat{y} | x)\\).\nResult:\nThe Bayes rule for classification is to predict the class with the highest posterior predictive probability. In the context of machine learning and pattern recognition, this decision rule is known as the Bayes Optimal Classifier.\n\\[\n\\hat{y}_{\\text{Bayes}}(x) = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg\\max} \\ P(y = k | x)\n\\]\n\n\n3.3.7 Interval Estimation as a Decision Problem\nWe can motivate the choice of a Credible Interval by defining a specific loss function for interval estimation. We define the action space \\(\\mathcal{A}\\) as the set of all intervals of fixed radius \\(\\delta &gt; 0\\) centered at \\(d\\), i.e., \\(\\mathcal{A} = \\{ [d - \\delta, d + \\delta] \\mid d \\in \\mathbb{R} \\}\\).\nThe loss function is defined as: \\[\n\\mathcal{L}(\\theta, d) = \\begin{cases}\n0 & \\text{if } |\\theta - d| \\le \\delta \\quad (\\theta \\in [d-\\delta, d+\\delta]) \\\\\n1 & \\text{if } |\\theta - d| &gt; \\delta \\quad (\\theta \\notin [d-\\delta, d+\\delta])\n\\end{cases}\n\\]\nDerivation of the Bayes Rule\nWe minimize the Expected Posterior Loss, which is simply the probability that \\(\\theta\\) falls outside the interval: \\[\nE^{\\theta|X}[\\mathcal{L}(\\theta, d)] = 1 \\cdot P(|\\theta - d| &gt; \\delta | x) = 1 - P(d - \\delta \\le \\theta \\le d + \\delta | x)\n\\]\nMinimizing this loss is equivalent to maximizing the posterior probability mass contained within the interval. Thus, the Bayes estimator \\(d\\) is: \\[\nd_{\\text{Bayes}} = \\underset{d}{\\arg\\max} \\int_{d-\\delta}^{d+\\delta} \\pi(\\theta|x) \\, d\\theta\n\\]\nTo find the optimal \\(d\\), we differentiate the integral with respect to \\(d\\) and set it to zero: \\[\n\\frac{\\partial}{\\partial d} \\left( \\int_{d-\\delta}^{d+\\delta} \\pi(\\theta|x) \\, d\\theta \\right) = \\pi(d+\\delta|x) - \\pi(d-\\delta|x) = 0\n\\]\nThis yields the condition \\(\\pi(d+\\delta|x) = \\pi(d-\\delta|x)\\).\nThe optimal \\(d\\) centers the interval such that the posterior density heights at the two endpoints are equal. This is the defining characteristic of a Highest Posterior Density (HPD) interval.\n\n\n\n\n\n\n\n\nFigure 3.4: Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution\n\n\n\n\n\nComparison with Equal-Tailed Intervals:\n\nEqual-Tailed Interval: We simply cut off \\(\\alpha/2\\) probability from each tail of the distribution. This is easy to compute but may not be the shortest interval if the distribution is skewed.\n\nHPD Interval: This is the shortest possible interval for the given coverage. For unimodal distributions, the probability density at the two endpoints of the HPD interval is identical.\n\n\nThe plot below illustrates a skewed posterior distribution (Gamma). Notice how the HPD Interval (Blue) is shifted toward the mode (the peak) to capture the highest density values, resulting in a shorter interval length compared to the Equal-Tailed Interval (Red).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#finding-minimax-rules-with-bayes-rules",
    "href": "bayesian.html#finding-minimax-rules-with-bayes-rules",
    "title": "3  Bayesian Inference",
    "section": "3.4 Finding Minimax Rules with Bayes Rules",
    "text": "3.4 Finding Minimax Rules with Bayes Rules\nTheorem 2.1 states that if a Bayes estimator \\(\\delta^\\pi\\) (derived from a prior \\(\\pi\\)) yields a constant risk \\(R(\\theta, \\delta^\\pi) = c\\) across the entire parameter space \\(\\Theta\\), then that estimator is necessarily minimax.\nThis result is a cornerstone of decision theory because it provides a sufficient condition for minimaxity. While the minimax criterion focuses on the “worst-case scenario” by minimizing the maximum possible risk, the Bayes criterion focuses on the “average-case scenario” relative to a prior. When the risk is constant, these two perspectives align: the average risk equals the maximum risk, and no other estimator can achieve a lower maximum without also having a lower Bayes risk, which would contradict the optimality of the Bayes rule.\n\n3.4.1 Binomial Minimax Estimator\n\nExample 3.5 Let \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(\\mathcal{L}(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(X) = \\frac{a+X}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E^{X|\\theta} \\left[ \\left( \\theta - \\frac{a+X}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E^{X|\\theta} \\left[ (c\\theta - a - X)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E^{X|\\theta}[X] = n\\theta\\) and \\(E^{X|\\theta}[X^2] = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(X) = \\frac{X + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = X/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).\nAccording to Theorem 2.2, let \\(\\{\\delta_n\\}\\) be a sequence of Bayes rules with respect to priors \\(\\{\\pi_n\\}\\), and let \\(r(\\pi_n, \\delta_n)\\) be the associated Bayes risks. If there exists a rule \\(\\delta_0\\) such that \\[\n\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\n\\] then \\(\\delta_0\\) is a minimax estimator.\nWe can rewrite the Minimax estimator \\(d(X)\\) as a linear combination of the sample proportion (MLE) \\(\\hat{p} = X/n\\) and the prior mean \\(p_0 = 1/2\\):\n\\[\nd(X) = \\underbrace{\\left( \\frac{n}{n + \\sqrt{n}} \\right)}_{w} \\underbrace{\\left( \\frac{X}{n} \\right)}_{\\hat{p}} + \\underbrace{\\left( \\frac{\\sqrt{n}}{n + \\sqrt{n}} \\right)}_{1-w} \\underbrace{\\left( \\frac{1}{2} \\right)}_{p_0}\n\\]\n\\[\nd(X) = w \\hat{p} + (1-w) p_0\n\\]\nInterpretation:\n\n\\(p_0 = 0.5\\): The estimator shrinks the data toward a neutral prior mean of \\(0.5\\) (representing maximum uncertainty).\n\n\\(w = \\frac{n}{n+\\sqrt{n}}\\): The weight assigned to the data. As the sample size \\(n\\) increases, \\(w \\to 1\\), and the minimax estimator converges to the MLE.\n\n\n\n\n\n3.4.2 Exponential Minimax Estimation\nLet’s recall Theorem 2.2. If there exists a rule \\(\\delta_0\\) such that: \\[\n\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n),\n\\] where \\(\\{\\delta_n\\}\\) is a sequence of Bayes rules with respect to priors \\(\\{\\pi_n\\}\\) and \\(r(\\pi_n, \\delta_n)\\) is the associated Bayes risks. Then \\(\\delta_0\\) is Minimax. This theorem is frequently used when a minimax estimator corresponds to an “improper” prior (a prior that does not integrate to 1, such as a uniform distribution on an infinite interval). Since Bayes rules cannot be directly defined for improper priors in the standard risk framework, we approximate the improper prior with a sequence of proper priors \\(\\{\\pi_k\\}\\). If the risk of our proposed estimator \\(\\delta_0\\) acts as a ceiling that the Bayes risks approach from below, \\(\\delta_0\\) effectively guards against the “least favorable” conditions, satisfying the minimax criterion.\n\nExample 3.6 (Exponential Minimax Estimation) Let \\(X_1, \\dots, X_n\\) be a sample from an \\(\\text{Exp}(\\theta)\\) distribution with mean \\(\\theta\\). We consider the Scale-Invariant Loss Function:\n\\[\n\\mathcal{L}(\\theta, d) = \\left( \\frac{d}{\\theta} - 1 \\right)^2\n\\]\nLikelihood and MLE\nThe probability density function for a single observation is \\(f(x_i|\\theta) = \\frac{1}{\\theta} e^{-x_i/\\theta}\\). The likelihood function for the sample is:\n\\[\nL(\\theta|x) = \\theta^{-n} e^{-\\frac{1}{\\theta}\\sum_{i=1}^n x_i}\n\\]\nThe Maximum Likelihood Estimator is standard: \\(\\hat{\\theta}_{\\text{MLE}} = \\bar{X}\\).\nMinimax Estimation Setup\nWe propose the estimator \\(d_0(X) = \\frac{\\sum X_i}{n+1}\\). To show this is a minimax estimator, we consider a sequence of priors \\(\\pi_k\\) and examine the limit of their Bayes risks.\nPrior Density\nWe assume the prior \\(\\pi_k(\\theta)\\) follows an Inverse-Gamma distribution with shape \\(\\alpha_k\\) and scale \\(\\beta_k\\). The density is given by:\n\\[\n\\pi_k(\\theta) = \\frac{\\beta_k^{\\alpha_k}}{\\Gamma(\\alpha_k)} \\theta^{-\\alpha_k - 1} e^{-\\beta_k / \\theta}, \\quad \\theta &gt; 0\n\\]\nPosterior Analysis\nLet \\(T = \\sum X_i\\). The posterior density is proportional to:\n\\[\n\\pi(\\theta | x) \\propto \\left( \\theta^{-n} e^{-T/\\theta} \\right) \\cdot \\left( \\theta^{-\\alpha_k - 1} e^{-\\beta_k / \\theta} \\right) \\propto \\theta^{-(n + \\alpha_k) - 1} e^{-(T + \\beta_k)/\\theta}\n\\]\nThis is an Inverse-Gamma distribution with parameters \\(\\alpha^* = n + \\alpha_k\\) and \\(\\beta^* = T + \\beta_k\\).\nCalculation of the Bayes Estimator\nUsing the result derived in the Scale-Invariant Squared Error Loss section, the Bayes estimator is:\n\\[\nd_{\\pi_k}(X) = \\frac{E^{\\theta|X}[\\theta^{-1}]}{E^{\\theta|X}[\\theta^{-2}]}\n\\]\nFor an Inverse-Gamma\\((\\alpha^*, \\beta^*)\\) variable, the required moments are:\n\n\\(E^{\\theta|X}[\\theta^{-1}] = \\frac{\\alpha^*}{\\beta^*}\\)\n\n\\(E^{\\theta|X}[\\theta^{-2}] = \\frac{\\alpha^* (\\alpha^* + 1)}{(\\beta^*)^2}\\)\n\n\nSubstituting these into the estimator formula:\n\\[\nd_{\\pi_k}(X) = \\frac{ \\frac{\\alpha^*}{\\beta^*} }{ \\frac{\\alpha^* (\\alpha^* + 1)}{(\\beta^*)^2} } = \\frac{\\beta^*}{\\alpha^* + 1} = \\frac{T + \\beta_k}{n + \\alpha_k + 1}\n\\]\nBayes Risk Limit\nThe Bayes risk \\(r(\\pi_k, d_{\\pi_k})\\) is the expected value of the minimum posterior loss. Substituting \\(d_{\\pi_k}\\) back into the loss equation:\n\\[\nr(\\pi_k, d_{\\pi_k}) = 1 - \\frac{(E^{\\theta|X}[\\theta^{-1}])^2}{E^{\\theta|X}[\\theta^{-2}]} = 1 - \\frac{n + \\alpha_k}{n + \\alpha_k + 1} = \\frac{1}{n + \\alpha_k + 1}\n\\]\nTaking the limit as the prior parameters approach zero (\\(\\alpha_k \\to 0\\)):\n\\[\n\\lim_{k \\to \\infty} r(\\pi_k, d_{\\pi_k}) = \\frac{1}{n+1}\n\\]\nMinimax Verification\nWe compute the frequentist risk of our candidate estimator \\(d_0(X) = \\frac{T}{n+1}\\). Let \\(Y = T/\\theta \\sim \\text{Gamma}(n, 1)\\). Note that \\(E^{X|\\theta}[Y]=n\\) and \\(\\text{Var}^{X|\\theta}(Y)=n\\).\n\\[\n\\begin{aligned}\nR(\\theta, d_0) &= E^{X|\\theta} \\left[ \\left( \\frac{d_0}{\\theta} - 1 \\right)^2 \\right] = E^{X|\\theta} \\left[ \\left( \\frac{Y}{n+1} - 1 \\right)^2 \\right] \\\\\n&= \\text{Var}^{X|\\theta}\\left(\\frac{Y}{n+1}\\right) + \\left(E^{X|\\theta}\\left[\\frac{Y}{n+1}\\right]-1\\right)^2 \\\\\n&= \\frac{n}{(n+1)^2} + \\left( \\frac{n}{n+1} - 1 \\right)^2 \\\\\n&= \\frac{1}{n+1}\n\\end{aligned}\n\\]\nSince \\(R(\\theta, d_0) = \\lim_{k \\to \\infty} r(\\pi_k, d_{\\pi_k}) = \\frac{1}{n+1}\\) for all \\(\\theta\\), \\(d_0\\) is a minimax estimator.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#steins-paradox-and-the-james-stein-estimator",
    "href": "bayesian.html#steins-paradox-and-the-james-stein-estimator",
    "title": "3  Bayesian Inference",
    "section": "3.5 Stein’s Paradox and the James-stein Estimator",
    "text": "3.5 Stein’s Paradox and the James-stein Estimator\n\n3.5.1 The Problem of Estimating Normal Mean\nIn high-dimensional estimation (\\(p \\ge 3\\)), the Maximum Likelihood Estimator (MLE) is inadmissible under squared error loss. The James-Stein Estimator dominates the MLE, meaning it achieves lower risk for all values of \\(\\theta\\).\nConsider the setting:\n\nData: \\(X \\sim N_p(\\theta, I)\\)\n\nPrior: \\(\\theta \\sim N_p(0, \\sigma^2 I)\\)\n\nJame-Stein Estimator:\n\n\\[d^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\nThe James-Stein estimator improves upon the MLE by shrinking the individual observations toward a common mean (usually zero). The magnitude of this shrinkage depends on the total sum of squares of the observations.\n\nWhen the variance of \\(\\theta\\) is large, \\(||X||^2\\) tends to be large, resulting in less shrinkage.\n\nWhen the variance of \\(\\theta\\) is small, \\(||X||^2\\) is smaller, leading to a larger shrinkage factor.\n\n\nThe following R code simulates these two cases and displays them side-by-side with a shared y-axis for direct comparison.\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(latex2exp)\n\nset.seed(42)\np &lt;- 20\nsigma1 &lt;- 3\nsigma2 &lt;- 1\n\nsimulate_js &lt;- function(sigma, p_dim) {\n  theta &lt;- rnorm(p_dim, 0, sigma)\n  X &lt;- theta + rnorm(p_dim, 0, 1)\n  sum_x_sq &lt;- sum(X^2)\n  shrinkage_factor &lt;- max(0, 1 - (p_dim - 2) / sum_x_sq)\n  js_est &lt;- shrinkage_factor * X\n  \n  data.frame(\n    i = 1:p_dim,\n    X_i = X,\n    JS_i = js_est\n  )\n}\n\ndf_large &lt;- simulate_js(sigma1, p)\ndf_small &lt;- simulate_js(sigma2, p)\ny_lims &lt;- range(c(df_large$X_i, df_small$X_i))\n\ncreate_plot &lt;- function(df, title, y_limits) {\n  df_long &lt;- df %&gt;%\n    pivot_longer(cols = c(X_i, JS_i), names_to = \"Est\", values_to = \"Value\")\n  \n  ggplot(df_long, aes(x = i, y = Value, color = Est, shape = Est)) +\n    geom_point(size = 3, alpha = 0.8) +\n    scale_color_manual(\n      values = c(\"X_i\" = \"tomato\", \"JS_i\" = \"blue\"),\n      labels = unname(TeX(c(\"$\\\\hat{\\\\theta}_i^{JS}$\", \"$X_i$\")))\n    ) +\n    scale_shape_manual(\n      values = c(\"X_i\" = 4, \"JS_i\" = 16),\n      labels = unname(TeX(c(\"$\\\\hat{\\\\theta}_i^{JS}$\", \"$X_i$\")))\n    ) +\n    ylim(y_limits) +\n    theme_minimal() +\n    labs(\n      title = title,\n      x = TeX(\"Index $i$\"),\n      y = TeX(\"Value\"),\n      color = \"Estimator\",\n      shape = \"Estimator\"\n    )\n}\n\np1 &lt;- create_plot(df_large, TeX(\"Large $\\\\sigma^2$ (Less Shrinkage)\"), y_lims)\np2 &lt;- create_plot(df_small, TeX(\"Small $\\\\sigma^2$ (More Shrinkage)\"), y_lims)\n\n# Combine with patchwork, collect guides at the bottom\np1 + p2 + \n  plot_layout(guides = \"collect\") & \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 3.5: Visualization of JS Estimator\n\n\n\n\n\n\n\n3.5.2 The Maximum Likelihood Estimator\nSince the observations have the covariance matrix \\(I\\) (the identity matrix), the individual components \\(X_1, \\dots, X_p\\) are independent, with \\(X_i \\sim N(\\theta_i, 1)\\).\nThe joint likelihood function is the product of the individual probability density functions:\n\\[\nL(\\theta; x) = \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(X_i - \\theta_i)^2}{2} \\right)\n\\]\nTo find the estimator, we maximize the log-likelihood function \\(\\ell(\\theta)\\):\n\\[\n\\begin{aligned}\n\\ell(\\theta) &= \\ln \\left( \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(X_i - \\theta_i)^2}{2} \\right) \\right) \\\\\n&= \\sum_{i=1}^p \\left[ \\ln\\left(\\frac{1}{\\sqrt{2\\pi}}\\right) - \\frac{(X_i - \\theta_i)^2}{2} \\right]\n\\end{aligned}\n\\]\nMaximizing this sum is equivalent to minimizing the sum of squared errors \\(\\sum (X_i - \\theta_i)^2\\). We can solve for each component \\(\\theta_i\\) separately. Differentiating with respect to \\(\\theta_i\\):\n\\[\n\\frac{\\partial \\ell}{\\partial \\theta_i} = (X_i - \\theta_i)\n\\]\nSetting the derivative to zero gives the critical point:\n\\[\nX_i - \\hat{\\theta}_i = 0 \\implies \\hat{\\theta}_{i, \\text{MLE}} = X_i\n\\]\nSince this holds for every component \\(i = 1, \\dots, p\\), the Maximum Likelihood Estimator for the entire vector is simply the observation vector itself:\n\\[\nd^{\\text{MLE}}(X) = X\n\\]\n\n\n3.5.3 A Bayes Rule\nWe first derive the Bayes rule with respect to a specific conjugate prior. Instead of using matrix notation, we can look at the problem component-wise, as the observations are independent.\nConsider the model where we observe \\(p\\) independent components:\n\\[\nX_i | \\theta_i \\sim N(\\theta_i, 1), \\quad \\text{for } i = 1, \\ldots, p\n\\]\nWe place independent centered normal priors on each unknown parameter \\(\\theta_i\\):\n\\[\n\\theta_i \\sim N(0, \\sigma^2), \\quad \\text{for } i = 1, \\ldots, p\n\\]\nSince the components are independent, we can derive the Bayes rule for a single scalar component \\(X_i\\) estimating \\(\\theta_i\\). The total risk will simply be the sum of the component risks.\nFor a single component, the posterior distribution of \\(\\theta_i\\) given \\(X_i\\) is Normal, with parameters determined by the standard conjugate formulas:\n\nPosterior Precision (inverse variance): The posterior precision is the sum of the prior precision and the data precision. \\[\n\\frac{1}{v_{\\text{post}}} = \\frac{1}{\\sigma^2} + \\frac{1}{1} = \\frac{1 + \\sigma^2}{\\sigma^2}\n\\] Therefore, the posterior variance is: \\[\nv_{\\text{post}} = \\frac{\\sigma^2}{1 + \\sigma^2}\n\\]\nPosterior Mean (Bayes Estimator): The posterior mean is the precision-weighted average of the prior mean (0) and the data mean (\\(X_i\\)). \\[\n\\begin{aligned}\nE^{\\theta_i|X_i}[\\theta_i] &= v_{\\text{post}} \\left( \\frac{0}{\\sigma^2} + \\frac{X_i}{1} \\right) \\\\\n&= \\frac{\\sigma^2}{1 + \\sigma^2} X_i \\\\\n&= \\left( 1 - \\frac{1}{1 + \\sigma^2} \\right) X_i\n\\end{aligned}\n\\]\n\nSince this holds for all \\(i\\), the Bayes rule for the vector \\(\\theta\\) is applying this shrinkage factor to each component:\n\\[\nd^{\\text{Bayes}}(X) = \\left( 1 - \\frac{1}{1 + \\sigma^2} \\right) X\n\\]\n\n3.5.3.1 Bayes Risk of the Bayes Rule\nTo compute the Bayes risk, we sum the risks of the individual components. For squared error loss, the posterior expected loss for one component is simply the posterior variance derived above:\n\\[\nE^{\\theta_i|X_i} [ (\\theta_i - d^{\\text{Bayes}}(X_i))^2 ] = v_{\\text{post}} = \\frac{\\sigma^2}{1 + \\sigma^2}\n\\]\nThe total Bayes risk is the sum of these variances over \\(p\\) components:\n\\[\nr(\\pi, d^{\\text{Bayes}}) = \\sum_{i=1}^p \\frac{\\sigma^2}{1 + \\sigma^2} = \\frac{p\\sigma^2}{1 + \\sigma^2}\n\\]\n\n\n3.5.3.2 Minimaxity of the MLE\nThe James-Stein result is particularly striking when compared to the performance of the standard estimator.\n\nTheorem 3.2 (Minimaxity of the Maximum Likelihood Estimator) Let \\(X \\sim N_p(\\theta, I)\\). Under the squared error loss function \\(\\mathcal{L}(\\theta, d) = ||\\theta - d||^2\\), the standard Maximum Likelihood Estimator \\(d^0(X) = X\\) is a minimax rule. That is, it minimizes the maximum possible risk over the parameter space:\n\\[\n\\sup_{\\theta \\in \\mathbb{R}^p} R(\\theta, d^0) = \\inf_{d} \\sup_{\\theta \\in \\mathbb{R}^p} R(\\theta, d) = p\n\\]\n\n\nProof. \n\n\nClick to view proof by least favorable prior\n\nThe risk of the MLE is \\(R(\\theta, d^0) = p\\) for all \\(\\theta\\). Since it is a constant risk estimator, its maximum risk is simply \\(p\\).\nTo prove it is minimax, we show that \\(p\\) is the limit of the Bayes risks for a sequence of conjugate priors \\(\\theta_i \\sim N(0, \\sigma^2)\\). As derived above, the Bayes risk for the optimal Bayes estimator \\(d^{\\text{Bayes}}\\) is:\n\\[\nr(\\pi_{\\sigma^2}, d^{\\text{Bayes}}) = \\frac{p\\sigma^2}{1 + \\sigma^2}\n\\]\nAs \\(\\sigma^2 \\to \\infty\\) (the prior becomes “flat”), the Bayes risk approaches \\(p\\):\n\\[\n\\lim_{\\sigma^2 \\to \\infty} \\frac{p\\sigma^2}{1 + \\sigma^2} = p\n\\]\nBy the property that the maximum risk of an estimator is always at least the Bayes risk of any prior, and specifically greater than or equal to the limit of Bayes risks for a sequence of priors, we establish that no estimator can have a maximum risk lower than \\(p\\). Since \\(d^0\\) achieves this maximum risk, it is minimax.\n\n\n\n\n\n3.5.4 Stein’s Lemma\n\n\n\n\n\n\nNotation: The Divergence Operator\n\n\n\nThe symbol \\(\\nabla \\cdot g(X)\\) (read as “divergence of \\(g\\)”) is simply a shorthand notation for the sum of the partial derivatives:\n\\[\n\\nabla \\cdot g(X) \\equiv \\sum_{i=1}^p \\frac{\\partial g_i(X)}{\\partial X_i}\n\\]\nIt represents the total “outward flow” of the vector field \\(g\\) from a local point.\n\n\n\nLemma 3.1 (Stein’s Lemma) Let \\(X \\sim N_p(\\theta, I)\\) be a multivariate normal random vector, and let \\(g: \\mathbb{R}^p \\to \\mathbb{R}^p\\) be a continuously differentiable function such that \\(E^{X|\\theta}[| \\partial g_i / \\partial X_i |] &lt; \\infty\\). Then:\n\\[\nE^{X|\\theta} \\left[ (X - \\theta)^T g(X) \\right] = E^{X|\\theta} \\left[ \\nabla \\cdot g(X) \\right] = E^{X|\\theta} \\left[ \\sum_{i=1}^p \\frac{\\partial g_i(X)}{\\partial X_i} \\right]\n\\]\nThe term \\(\\nabla \\cdot g(X)\\) represents the divergence of the vector field \\(g\\), which intuitively measures the local rate of expansion or outward flux of the function \\(g\\) at the point \\(X\\); in this statistical context, it quantifies the aggregate sensitivity of the function components to changes in the data.\n\n\nProof. \n\nIt suffices to show the result for a single component in 1 dimension, as the multivariate case follows by summation due to independence. Let \\(X_i \\sim N(\\theta_i, 1)\\) and let \\(\\phi(t)\\) be the standard normal density function. The joint density is \\(f(x) = \\prod \\phi(x_j - \\theta_j)\\).\nConsider the expectation of the \\(i\\)-th term: \\[\nE^{X|\\theta} [ (X_i - \\theta_i) g_i(X) ] = \\int_{\\mathbb{R}^p} (x_i - \\theta_i) g_i(x) \\left( \\prod_{j=1}^p \\phi(x_j - \\theta_j) \\right) dx\n\\]\nFocusing on the integral with respect to \\(x_i\\): \\[\n\\int_{-\\infty}^{\\infty} (x_i - \\theta_i) \\phi(x_i - \\theta_i) g_i(x) dx_i\n\\]\nRecall that \\(\\phi'(z) = -z \\phi(z)\\). Therefore, \\((x_i - \\theta_i) \\phi(x_i - \\theta_i) = - \\frac{\\partial}{\\partial x_i} \\phi(x_i - \\theta_i)\\). We use integration by parts with: \\[\nu = g_i(x) \\quad \\text{and} \\quad dv = - \\frac{\\partial}{\\partial x_i} \\phi(x_i - \\theta_i) dx_i\n\\]\nThus: \\[\n\\int_{-\\infty}^{\\infty} g_i(x) (x_i - \\theta_i) \\phi(x_i - \\theta_i) dx_i = \\left[ -g_i(x) \\phi(x_i - \\theta_i) \\right]_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} \\frac{\\partial g_i(x)}{\\partial x_i} \\phi(x_i - \\theta_i) dx_i\n\\]\nAssuming \\(g(x)\\) does not grow exponentially fast, the boundary term vanishes. The remaining integral is the expectation of the partial derivative. Summing over all \\(i=1 \\dots p\\) gives the divergence \\(\\nabla \\cdot g(X)\\).\n\n\nIn high-dimensional statistics, Stein’s Lemma is often expressed using the inner product of the random vector and the function vector field, which highlights the alignment between the data and the transformation.\n\nCorollary 3.1 (Stein’s Lemma (Vector Form)) Let \\(X \\sim N_p(\\theta, I)\\) and \\(g: \\mathbb{R}^p \\to \\mathbb{R}^p\\) be a weakly differentiable function. Then:\n\\[\nE^{X|\\theta} [ X^T g(X) ] = \\theta^T E^{X|\\theta} [ g(X) ] + E^{X|\\theta} [ \\nabla \\cdot g(X) ]\n\\]\n\n\n\n\n\n\n\nRemark: Connection to Non-Central \\(\\chi^2\\) Moments\n\n\n\nThis identity provides an elegant way to derive the mean of a non-central chi-square distribution without performing complex integration.\nConsider the case where \\(g(X) = X\\). Here, \\(\\nabla \\cdot X = p\\). Plugging this into the vector form:\n\\[\nE^{X|\\theta} [ X^T X ] = \\theta^T E^{X|\\theta} [ X ] + E^{X|\\theta} [ p ]\n\\]\nSince \\(E^{X|\\theta} [ X ] = \\theta\\), we immediately obtain:\n\\[\nE^{X|\\theta} [ \\lVert X \\rVert^2 ] = \\lVert \\theta \\rVert^2 + p\n\\]\nThis is precisely the mean of a \\(\\chi^2_p(\\lambda)\\) distribution with non-centrality parameter \\(\\lambda = \\lVert \\theta \\rVert^2\\). Essentially, Stein’s Lemma decomposes the second moment into the signal component (\\(\\lVert \\theta \\rVert^2\\)) and the geometric noise component (\\(p\\)).\n\n\n\nLemma 3.2 (Stein’s Lemma for Radial Fields) Let \\(X \\sim N_p(\\theta, I)\\) and consider a radial vector field of the form \\(g(X) = c(\\lVert X \\rVert^2)X\\), where \\(c: \\mathbb{R} \\to \\mathbb{R}\\) is a differentiable scalar function. Then:\n\\[\nE^{X|\\theta} \\left[ (X - \\theta)^T g(X) \\right] = E^{X|\\theta} \\left[ p \\cdot c(\\lVert X \\rVert^2) + 2 \\lVert X \\rVert^2 \\cdot c'(\\lVert X \\rVert^2) \\right]\n\\]\nwhere \\(c'(z) = \\frac{d}{dz}c(z)\\).\n\n\nProof. \n\nWe apply the general Stein’s Lemma by calculating the divergence of the radial field \\(g(X) = c(\\lVert X \\rVert^2)X\\). Using the product rule for divergence:\n\\[\n\\nabla \\cdot (c(\\lVert X \\rVert^2)X) = c(\\lVert X \\rVert^2) (\\nabla \\cdot X) + X^T (\\nabla c(\\lVert X \\rVert^2))\n\\]\nStep 1: The geometric spread. The divergence of the identity map \\(X\\) in \\(p\\) dimensions is simply the sum of the partial derivatives of each component with respect to itself: \\[\n\\nabla \\cdot X = \\sum_{i=1}^p \\frac{\\partial X_i}{\\partial X_i} = p\n\\]\nStep 2: The radial stretch. To find \\(\\nabla c(\\lVert X \\rVert^2)\\), we use the chain rule. Let \\(h(X) = \\lVert X \\rVert^2 = \\sum X_i^2\\). Then \\(\\nabla h(X) = 2X\\). \\[\n\\nabla c(\\lVert X \\rVert^2) = c'(\\lVert X \\rVert^2) \\nabla (\\lVert X \\rVert^2) = 2 c'(\\lVert X \\rVert^2) X\n\\]\nSubstituting this back into the divergence formula: \\[\n\\begin{aligned}\n\\nabla \\cdot g(X) &= p \\cdot c(\\lVert X \\rVert^2) + X^T (2 c'(\\lVert X \\rVert^2) X) \\\\\n&= p \\cdot c(\\lVert X \\rVert^2) + 2 c'(\\lVert X \\rVert^2) \\lVert X \\rVert^2\n\\end{aligned}\n\\]\nTaking the expectation of both sides completes the proof.\nConnection to the \\(\\chi^2\\) Distribution\nThis version of the lemma is particularly useful because when \\(\\theta = 0\\), the quantity \\(\\lVert X \\rVert^2\\) follows a central \\(\\chi^2_p\\) distribution.\n\nVerifying the Mean: If we set \\(c(\\lVert X \\rVert^2) = 1\\), then \\(g(X) = X\\). The lemma gives \\(E^{X|\\theta=0}[\\lVert X \\rVert^2] = p + 2\\lVert X \\rVert^2(0) = p\\), which is the expected value of a \\(\\chi^2_p\\) variable.\n\nThe James-Stein Weight: If we set \\(c(\\lVert X \\rVert^2) = \\frac{1}{\\lVert X \\rVert^2}\\), then \\(c'(z) = -\\frac{1}{z^2}\\). \\[\n\\nabla \\cdot g(X) = \\frac{p}{\\lVert X \\rVert^2} + 2 \\lVert X \\rVert^2 \\left( -\\frac{1}{\\lVert X \\rVert^4} \\right) = \\frac{p-2}{\\lVert X \\rVert^2}\n\\] This explains why the \\(p-2\\) constant appears in the James-Stein estimator—it is the net result of \\(p\\) dimensions of “spreading” minus 2 dimensions of “radial thinning.”\n\n\n\n\n\nExample 3.7 (An Example for Verifying Stein’s Lemma) Let \\(X \\sim N(\\theta, 1)\\) be a univariate normal random variable with unit variance. Let \\(g(x) = x^2\\). Stein’s Lemma states that: \\[\nE^{X|\\theta} \\left[ (X-\\theta) g(X) \\right] = E^{X|\\theta} \\left[ g'(X) \\right]\n\\]\nStep 1: Calculate the Right-Hand Side (RHS) First, we find the derivative of \\(g(x)\\): \\[\ng'(x) = \\frac{d}{dx} (x^2) = 2x\n\\]\nNow, compute the expectation of the derivative: \\[\n\\text{RHS} = E^{X|\\theta} [g'(X)] = E^{X|\\theta} [2X] = 2 E^{X|\\theta} [X] = 2\\theta\n\\]\nStep 2: Calculate the Left-Hand Side (LHS) We evaluate the expectation of the cross-product term. Substitute \\(X = Z + \\theta\\), where \\(Z \\sim N(0, 1)\\) is a standard normal variable. Then \\(X - \\theta = Z\\).\n\\[\n\\begin{aligned}\n\\text{LHS} &= E^{X|\\theta} \\left[ (X-\\theta) X^2 \\right] \\\\\n&= E^{X|\\theta} \\left[ Z (Z + \\theta)^2 \\right] \\quad \\text{where } Z \\sim N(0,1) \\\\\n&= E^{X|\\theta} \\left[ Z (Z^2 + 2\\theta Z + \\theta^2) \\right] \\\\\n&= E^{X|\\theta} [Z^3] + 2\\theta E^{X|\\theta} [Z^2] + \\theta^2 E^{X|\\theta} [Z]\n\\end{aligned}\n\\]\nWe use the known moments of the standard normal distribution \\(Z\\):\n\n\\(E[Z] = 0\\) (mean)\n\n\\(E[Z^2] = 1\\) (variance)\n\n\\(E[Z^3] = 0\\) (skewness of symmetric distribution)\n\nSubstituting these values back: \\[\n\\text{LHS} = 0 + 2\\theta(1) + \\theta^2(0) = 2\\theta\n\\]\nConclusion We observe that: \\[\n\\text{LHS} = 2\\theta \\quad \\text{and} \\quad \\text{RHS} = 2\\theta\n\\] Thus, Stein’s Lemma holds for this specific case.\n\n\nExample 3.8 (A Radial Field Example Verifying Stein’s Lemma) Let \\(X \\sim N_p(\\theta, I)\\) and \\(g(X) = \\lVert X \\rVert^2 X\\). We verify the Radial Field Lemma: \\[E^{X|\\theta} \\left[ (X - \\theta)^T g(X) \\right] = E^{X|\\theta} \\left[ p \\cdot c(\\lVert X \\rVert^2) + 2 \\lVert X \\rVert^2 \\cdot c'(\\lVert X \\rVert^2) \\right]\\]\nHere, \\(c(z) = z\\), which implies \\(c'(z) = 1\\).\nRHS (Divergence): Using the radial formula: \\[\n\\begin{aligned}\n\\nabla \\cdot g(X) &= p(\\lVert X \\rVert^2) + 2\\lVert X \\rVert^2(1) \\\\\n&= (p+2)\\lVert X \\rVert^2\n\\end{aligned}\n\\] The expectation is \\((p+2)E^{X|\\theta}[\\lVert X \\rVert^2]\\). Since \\(\\lVert X \\rVert^2\\) is a non-central \\(\\chi^2_p\\) with non-centrality parameter \\(\\lVert \\theta \\rVert^2\\), we know \\(E^{X|\\theta}[\\lVert X \\rVert^2] = p + \\lVert \\theta \\rVert^2\\). Thus, \\(\\text{RHS} = (p+2)(p + \\lVert \\theta \\rVert^2)\\).\nLHS (Alignment): \\[\n\\begin{aligned}\nE^{X|\\theta} \\left[ (X-\\theta)^T (\\lVert X \\rVert^2 X) \\right] &= E^{X|\\theta} \\left[ \\lVert X \\rVert^2 (X^T X - \\theta^T X) \\right] \\\\\n&= E^{X|\\theta} \\left[ \\lVert X \\rVert^4 - \\theta^T X \\lVert X \\rVert^2 \\right]\n\\end{aligned}\n\\]\nTo simplify, let \\(X = \\theta + Z\\) where \\(Z \\sim N_p(0, I)\\). Recall the moments of the non-central chi-square distribution or expand the terms: \\(E[\\lVert X \\rVert^4] = p(p+2) + 2(p+2)\\lVert \\theta \\rVert^2 + \\lVert \\theta \\rVert^4\\). For the cross term \\(E[\\theta^T X \\lVert X \\rVert^2]\\), we find it equals \\(\\lVert \\theta \\rVert^4 + (p+2)\\lVert \\theta \\rVert^2\\).\nSubtracting these: \\[\n\\begin{aligned}\n\\text{LHS} &= [p(p+2) + 2(p+2)\\lVert \\theta \\rVert^2 + \\lVert \\theta \\rVert^4] - [\\lVert \\theta \\rVert^4 + (p+2)\\lVert \\theta \\rVert^2] \\\\\n&= p(p+2) + (p+2)\\lVert \\theta \\rVert^2 \\\\\n&= (p+2)(p + \\lVert \\theta \\rVert^2)\n\\end{aligned}\n\\]\nConclusion\nThe results match exactly. The alignment of the cubic radial field with the noise is perfectly predicted by the sum of its geometric expansion (\\(p \\lVert X \\rVert^2\\)) and its radial stretch (\\(2 \\lVert X \\rVert^2\\)).\n\n\n\n3.5.5 Inadmissibility of the MLE in High Dimensions (Stein’s Phenomenon)\n\nTheorem 3.3 Let \\(X \\sim N_p(\\theta, I)\\) be a \\(p\\)-dimensional random vector with \\(p \\ge 3\\). Under the squared error loss function \\(\\mathcal{L}(\\theta, d) = ||\\theta - d||^2\\), the standard Maximum Likelihood Estimator \\(d^0(X) = X\\) is inadmissible.\n\n\nProof (Proof of Inadmissibility). To show that \\(d^0(X) = X\\) is inadmissible, we compare its risk to that of the James-Stein estimator \\(d^{JS}(X)\\).\nLet \\(g(X) = c(\\lVert X \\rVert^2)X\\) where \\(c(\\lVert X \\rVert^2) = \\frac{p-2}{\\lVert X \\rVert^2}\\). We can write the James-Stein estimator as \\(d^{JS}(X) = X - g(X)\\).\nThe risk is the expected squared error loss:\n\\[\n\\begin{aligned}\nR(\\theta, d^{JS}) &= E^{X|\\theta} \\left[ || (X - \\theta) - g(X) ||^2 \\right] \\\\\n&= E^{X|\\theta} \\left[ ||X - \\theta||^2 \\right] - 2 E^{X|\\theta} \\left[ (X-\\theta)^T g(X) \\right] + E^{X|\\theta} \\left[ ||g(X)||^2 \\right]\n\\end{aligned}\n\\]\nThe first term is the risk of the MLE, which is \\(p\\).\nFor the second term, we apply Stein’s Lemma for Radial Fields (Lemma 3.2). We first compute the scalar function and its derivative: \\[c(z) = \\frac{p-2}{z} \\implies c'(z) = -\\frac{p-2}{z^2}\\]\nSubstituting these into the radial divergence formula from Lemma 3.2: \\[\n\\begin{aligned}\n\\nabla \\cdot g(X) &= p \\cdot c(\\lVert X \\rVert^2) + 2 \\lVert X \\rVert^2 \\cdot c'(\\lVert X \\rVert^2) \\\\\n&= p \\left( \\frac{p-2}{\\lVert X \\rVert^2} \\right) + 2 \\lVert X \\rVert^2 \\left( -\\frac{p-2}{\\lVert X \\rVert^4} \\right) \\\\\n&= \\frac{p(p-2)}{\\lVert X \\rVert^2} - \\frac{2(p-2)}{\\lVert X \\rVert^2} \\\\\n&= \\frac{(p-2)^2}{\\lVert X \\rVert^2}\n\\end{aligned}\n\\]\nApplying the lemma to the cross-term: \\[2 E^{X|\\theta} \\left[ (X-\\theta)^T g(X) \\right] = 2 E^{X|\\theta} [ \\nabla \\cdot g(X) ] = 2(p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right]\\]\nThe third term in the risk expansion is the squared magnitude of the shrinkage: \\[\\lVert g(X) \\rVert^2 = \\left\\lVert \\frac{p-2}{ \\lVert X \\rVert ^2} X \\right \\rVert^2 = \\frac{(p-2)^2}{\\lVert X \\rVert^4} \\lVert X \\rVert^2 = \\frac{(p-2)^2}{\\lVert X \\rVert^2}\\]\nSubstituting these results back into the risk equation: \\[\n\\begin{aligned}\nR(\\theta, d^{JS}) &= p - 2(p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right] + (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right] \\\\\n&= p - (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right]\n\\end{aligned}\n\\]\nSince \\(p \\ge 3\\), the constant \\((p-2)^2\\) is strictly positive. Because \\(1/\\lVert X \\rVert^2 &gt; 0\\) with probability 1, the risk of the James-Stein estimator is strictly less than \\(p\\) for all \\(\\theta \\in \\mathbb{R}^p\\).\nThus, \\(d^{JS}\\) dominates the MLE, proving the MLE is inadmissible.\n\n\n\n3.5.6 How much JS Estimator Improves over MLE\nThe exact risk function of the James-Stein estimator \\(d^{JS}(X)\\) under squared error loss for \\(X \\sim N_p(\\theta, I)\\) is given by:\n\\[R(\\theta, d^{JS}) = p - (p-2)^2 E \\left[ \\frac{1}{\\chi_p^2(\\|\\theta\\|^2/2)} \\right]\\]\nwhere \\(\\chi_p^2(\\|\\theta\\|^2/2)\\) is a non-central chi-square random variable with \\(p\\) degrees of freedom and non-centrality parameter \\(\\lambda = \\|\\theta\\|^2/2\\).\nUsing the approximation \\(E[1/\\chi_p^2(\\lambda)] \\approx 1/E[\\chi_p^2(\\lambda)] = 1/(p + \\|\\theta\\|^2)\\), we can see the approximate behavior of the risk:\n\\[R(\\theta, d^{JS}) \\approx p - \\frac{(p-2)^2}{p + \\|\\theta\\|^2}\\]\n\nAggressive Shrinkage near the Origin: When \\(\\|\\theta\\|^2\\) is small, the denominator \\(p + \\|\\theta\\|^2\\) is small, making the subtracted term large. This results in a risk substantially lower than the MLE risk of \\(p\\).\nDiminishing Improvement with Large Signal: As \\(\\|\\theta\\|^2\\) becomes large, the term \\(\\frac{(p-2)^2}{p + \\|\\theta\\|^2}\\) approaches zero. Consequently, the risk of the James-Stein estimator approaches \\(p\\), and the improvement over the MLE becomes negligible.\n\nRisk Ratio near the Origin: As the true parameter vector shrinks to zero (\\(\\|\\theta\\| \\to 0\\)), the ratio of the risks converges to a constant fraction. Using the exact expectation \\(E^{X|\\theta=0}[1/\\|X\\|^2] = 1/(p-2)\\):\n\\[\n\\lim_{\\|\\theta\\| \\to 0} \\frac{R(\\theta, d^{JS})}{R(\\theta, d^{\\text{MLE}})} = \\frac{p - (p-2)^2 \\left( \\frac{1}{p-2} \\right)}{p} = \\frac{p - (p-2)}{p} = \\frac{2}{p}\n\\]\nFor a dimension like \\(p=10\\), the James-Stein estimator incurs only \\(20\\%\\) of the risk of the MLE near the origin.\n\n\n\n\n\n\nIs \\(d^{JS}\\) Minimax?\n\n\n\n\n\nYes. Since the MLE is minimax with constant risk \\(p\\), the minimax risk value for this problem is \\(p\\).\nBecause \\(R(\\theta, d^{JS}) &lt; p\\) for all \\(\\theta\\) and \\(\\lim_{\\lVert \\theta \\rVert \\to \\infty} R(\\theta, d^{JS}) = p\\), the maximum risk of the James-Stein estimator is exactly \\(p\\). Therefore, \\(d^{JS}\\) achieves the minimax risk level and is a minimax estimator.\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(latex2exp)\n\n# Parameters\np &lt;- 17\ntheta_sq &lt;- seq(0, 50, length.out = 300)\n\n# Create Data Frame\ndf &lt;- data.frame(\n theta_sq = theta_sq,\n risk_mle = p,\n risk_js = p - (p - 2)^2 / (p - 2 + theta_sq)\n)\n\n# Calculate Ratio\ndf$ratio &lt;- df$risk_js / p\n\n# --- FIX: ADJUST SCALING FACTOR ---\n\n# We scale the Ratio so 1.0 sits at y = 12 (top of plot),\n\n# rather than y = 10, to separate it from the Risk curve.\ny_max_limit &lt;- p+1\nscale_factor &lt;- y_max_limit\n\n# Define Legend Labels\nlabels_map &lt;- c(\n \"MLE\" = TeX(r'($R(\\theta, d^{MLE}) = p$)'),\n \"JS\" = TeX(r'($R(\\theta, d^{JS})$)'),\n \"Ratio\" = TeX(r'(Ratio $\\frac{R(\\theta, d^{JS})}{R(\\theta, d^{MLE})}$)')\n)\n\nggplot(df, aes(x = theta_sq)) +\n # 1. MLE Risk Line (Red Dashed)\n geom_line(aes(y = risk_mle, color = \"MLE\", linetype = \"MLE\"), size = 1) +\n\n # 2. JS Risk Line (Blue Solid)\n geom_line(aes(y = risk_js, color = \"JS\", linetype = \"JS\"), size = 1.2) +\n\n # 3. Ratio Line (Green Dot-Dash)\n # We scale it up to occupy the full plot height\n geom_line(aes(y = ratio * scale_factor, color = \"Ratio\", linetype = \"Ratio\"), size = 1) +\n\n # Axes Configuration\n scale_y_continuous(\n  name = \"Risk\",\n  limits = c(0, y_max_limit),\n  # Secondary Axis: Divide back by the NEW scale factor\n  sec.axis = sec_axis(~ . / scale_factor, name = \"Risk Ratio\")\n ) +\n\n # Manual Colors and Linetypes\n scale_color_manual(\n  name = NULL,\n  values = c(\"MLE\" = \"firebrick\", \"JS\" = \"dodgerblue\", \"Ratio\" = \"forestgreen\"),\n  labels = labels_map\n ) +\n scale_linetype_manual(\n  name = NULL,\n  values = c(\"MLE\" = \"dashed\", \"JS\" = \"solid\", \"Ratio\" = \"dotdash\"),\n  labels = labels_map\n ) +\n\n # Theming\n labs(\n   title = TeX(sprintf(r'(Risk Comparison of Estimators ($p=%d$))', p)),  \n   x = TeX(r'($|| \\theta ||^2$ (Signal Strength))')\n ) +\n theme_minimal() +\n theme(\n  legend.position = c(0.95, 0.5),\n  legend.justification = c(1, 0.5),\n  legend.background = element_rect(fill = \"white\", color = \"gray80\"),\n  axis.title.y.right = element_text(color = \"forestgreen\"),\n  axis.text.y.right = element_text(color = \"forestgreen\")\n )\n\n\n\n\n\n\n\n\nFigure 3.6: Risk comparison of James-Stein vs MLE (p=10). Note: Ratio uses the right axis.\n\n\n\n\n\n\n\n3.5.7 Using Normalized Loss (Optional)\nWe consider the Normalized Squared Error Loss function, which penalizes errors relative to the magnitude of the true parameter vector: \\[\n\\mathcal{L}(\\theta, d) = \\frac{\\lVert d - \\theta \\rVert^2}{\\lVert \\theta \\rVert^2}, \\quad \\theta \\neq 0\n\\]\n\nRisk of the MLE (\\(d^{\\text{MLE}} = X\\))\n\nThe risk of the Maximum Likelihood Estimator is straightforward because the standard Mean Squared Error (MSE) of \\(X\\) is constant (\\(p\\)): \\[\nR(\\theta, d^{\\text{MLE}}) = E^{X|\\theta} \\left[ \\frac{\\lVert X - \\theta \\rVert^2}{\\lVert \\theta \\rVert^2} \\right] = \\frac{1}{\\lVert \\theta \\rVert^2} E^{X|\\theta} \\left[ \\lVert X - \\theta \\rVert^2 \\right]\n\\] Since \\(X \\sim N_p(\\theta, I)\\), we have \\(E^{X|\\theta}[\\lVert X - \\theta \\rVert^2] = p\\). \\[\nR(\\theta, d^{\\text{MLE}}) = \\frac{p}{\\lVert \\theta \\rVert^2}\n\\]\n\nRisk of the James-Stein Estimator (\\(d^{JS}\\))\n\nFor the James-Stein estimator \\(d^{JS} = \\left( 1 - \\frac{p-2}{\\lVert X \\rVert^2} \\right)X\\), we utilize the known result for its standard MSE risk: \\[\nE^{X|\\theta} [\\lVert d^{JS} - \\theta \\rVert^2] = p - (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right]\n\\] The risk under the normalized loss is simply this term scaled by \\(1/\\lVert \\theta \\rVert^2\\): \\[\nR(\\theta, d^{JS}) = \\frac{1}{\\lVert \\theta \\rVert^2} \\left( p - (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right] \\right)\n\\]\n\nComparison and Dominance\n\nWe compare the risks by taking the difference: \\[\nR(\\theta, d^{\\text{MLE}}) - R(\\theta, d^{JS}) = \\frac{1}{\\lVert \\theta \\rVert^2} (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right]\n\\] Since \\((p-2)^2 &gt; 0\\) (for \\(p \\ge 3\\)) and the expectation of a positive random variable is positive, this difference is strictly positive for all \\(\\theta \\neq 0\\). \\[\nR(\\theta, d^{JS}) &lt; R(\\theta, d^{\\text{MLE}})\n\\]\n\nGlobal Dominance: The James-Stein estimator dominates the MLE under this loss function as well, achieving lower risk everywhere in the parameter space.\n\nBehavior near \\(\\theta \\approx 0\\): As \\(\\lVert \\theta \\rVert \\to 0\\), both risks diverge to infinity. We analyze their relative performance by examining the ratio of the risks:\n\n\\[\n\\frac{R(\\theta, d^{JS})}{R(\\theta, d^{\\text{MLE}})} = \\frac{p - (p-2)^2 E^{X|\\theta} [1/\\lVert X \\rVert^2]}{p} = 1 - \\frac{(p-2)^2}{p} E^{X|\\theta} \\left[ \\frac{1}{\\lVert X \\rVert^2} \\right]\n\\]\nAt \\(\\theta = 0\\), \\(\\lVert X \\rVert^2 \\sim \\chi^2_p\\), and the expectation is \\(E^{X|\\theta=0}[1/\\lVert X \\rVert^2] = 1/(p-2)\\). Substituting this into the ratio:\n\\[\n\\lim_{\\lVert \\theta \\rVert \\to 0} \\frac{R(\\theta, d^{JS})}{R(\\theta, d^{\\text{MLE}})} = 1 - \\frac{(p-2)^2}{p(p-2)} = 1 - \\frac{p-2}{p} = \\frac{2}{p}\n\\]\nThus, near the origin, the James-Stein estimator reduces the risk by a factor of \\(p/2\\). For large dimensions (e.g., \\(p=10\\)), the JS estimator has only \\(20\\%\\) of the risk of the MLE.\n\n\n\n3.5.8 Bayes Risk of James-stein Estimator (Optional)\nWe can derive the Bayes Risk \\(r(\\pi, d^{JS})\\) of this estimator using two equivalent methods: minimizing the expected frequentist risk, or minimizing the expected posterior loss.\n\nTheorem 3.4 (Bayes Risk of James-stein Estimator) For \\(p \\ge 3\\), the Bayes risk of the James-Stein estimator \\(d^{JS}\\) with respect to the prior \\(\\theta \\sim N(0, \\sigma^2 I)\\) is:\n\\[\nr(\\pi, d^{JS}) = \\frac{p\\sigma^2 + 2}{\\sigma^2 + 1}\n\\]\n\n\nProof. \n\n\nMethod 1: Integration over the Prior (Frequentist Risk approach)\n\nThe Bayes risk is defined as \\(r(\\pi, d) = E^\\pi [ R(\\theta, d) ]\\).\nFirst, recall the frequentist risk of the James-Stein estimator for a fixed \\(\\theta\\). Using Stein’s Lemma, the risk is given by:\n\\[\nR(\\theta, d^{JS}) = p - (p-2)^2 E^{X|\\theta} \\left[ \\frac{1}{||X||^2} \\right]\n\\]\nTo find the Bayes risk, we take the expectation of this risk with respect to the prior \\(\\pi(\\theta)\\):\n\\[\nr(\\pi, d^{JS}) = \\int R(\\theta, d^{JS}) \\pi(\\theta) d\\theta = p - (p-2)^2 E^\\pi \\left[ E^{X|\\theta} \\left( \\frac{1}{||X||^2} \\right) \\right]\n\\]\nBy the law of iterated expectations, \\(E^\\pi [ E^{X|\\theta} (\\cdot) ]\\) is equivalent to the expectation with respect to the marginal distribution of \\(X\\), denoted as \\(m(x)\\). Under the conjugate prior, the marginal distribution is \\(X \\sim N(0, (1+\\sigma^2)I)\\).\nConsequently, the quantity \\(\\frac{||X||^2}{1+\\sigma^2}\\) follows a Chi-squared distribution with \\(p\\) degrees of freedom (\\(\\chi^2_p\\)). The expectation of the inverse chi-square is:\n\\[\nE^X \\left[ \\frac{1}{||X||^2} \\right] = \\frac{1}{1+\\sigma^2} E \\left[ \\frac{1}{\\chi^2_p} \\right] = \\frac{1}{1+\\sigma^2} \\cdot \\frac{1}{p-2}\n\\]\nSubstituting this back into the risk equation:\n\\[\n\\begin{aligned}\nr(\\pi, d^{JS}) &= p - (p-2)^2 \\cdot \\frac{1}{(p-2)(1+\\sigma^2)} \\\\\n&= p - \\frac{p-2}{1+\\sigma^2} \\\\\n&= \\frac{p(1+\\sigma^2) - (p-2)}{1+\\sigma^2} \\\\\n&= \\frac{p\\sigma^2 + p - p + 2}{1+\\sigma^2} = \\frac{p\\sigma^2 + 2}{\\sigma^2 + 1}\n\\end{aligned}\n\\]\n\n\n\nProof. \n\n\nMethod 2: Integration over the Marginal (Posterior Loss approach)\n\nAlternatively, we can compute the Bayes risk by first finding the posterior expected loss for a given \\(x\\), and then averaging over the marginal distribution of \\(x\\):\n\\[\nr(\\pi, d) = E^X [ E^{\\theta|X} [ \\mathcal{L}(\\theta, d(X)) ] ]\n\\]\nStep 1: Posterior Expected Loss\nThe posterior distribution of \\(\\theta\\) given \\(x\\) is:\n\\[\n\\theta | x \\sim N \\left( \\frac{\\sigma^2}{1+\\sigma^2}x, \\frac{\\sigma^2}{1+\\sigma^2}I \\right)\n\\]\nThe expected squared error loss can be decomposed into the variance (trace) and the squared bias:\n\\[\nE^{\\theta|X} [ ||\\theta - d^{JS}(X)||^2 ] = \\text{tr}(\\text{Var}^{\\theta|X}(\\theta)) + || E^{\\theta|X}[\\theta] - d^{JS}(X) ||^2\n\\]\n\nTrace term: \\[\\text{tr} \\left( \\frac{\\sigma^2}{1+\\sigma^2} I_p \\right) = \\frac{p\\sigma^2}{1+\\sigma^2}\\]\nSquared Bias term: Let \\(B = \\frac{1}{1+\\sigma^2}\\). Then \\(E^{\\theta|X}[\\theta] = (1-B)X\\). The estimator is \\(d^{JS}(X) = (1 - \\frac{p-2}{||X||^2})X\\). The difference is:\n\\[\nE^{\\theta|X}[\\theta] - d^{JS}(X) = \\left( (1-B) - \\left( 1 - \\frac{p-2}{||X||^2} \\right) \\right) X = \\left( \\frac{p-2}{||X||^2} - B \\right) X\n\\]\nSquaring the norm gives:\n\\[\n\\left( \\frac{p-2}{||X||^2} - B \\right)^2 ||X||^2 = \\frac{(p-2)^2}{||X||^2} - 2B(p-2) + B^2 ||X||^2\n\\]\n\nStep 2: Expectation with respect to Marginal \\(X\\)\nWe now take the expectation \\(E^X[\\cdot]\\) of the posterior loss. Recall \\(X \\sim N(0, (1+\\sigma^2)I)\\), so \\(E^X[||X||^2] = p(1+\\sigma^2)\\) and \\(E^X[1/||X||^2] = \\frac{1}{(p-2)(1+\\sigma^2)}\\).\n\nExpectation of Trace term: Constant, remains \\(\\frac{p\\sigma^2}{1+\\sigma^2}\\).\n\nExpectation of Bias term:\n\n\\[\n\\begin{aligned}\nE^X \\left[ \\frac{(p-2)^2}{||X||^2} - \\frac{2(p-2)}{1+\\sigma^2} + \\frac{||X||^2}{(1+\\sigma^2)^2} \\right] &= (p-2)^2 \\frac{1}{(p-2)(1+\\sigma^2)} - \\frac{2(p-2)}{1+\\sigma^2} + \\frac{p(1+\\sigma^2)}{(1+\\sigma^2)^2} \\\\\n&= \\frac{p-2}{1+\\sigma^2} - \\frac{2p-4}{1+\\sigma^2} + \\frac{p}{1+\\sigma^2} \\\\\n&= \\frac{p - 2 - 2p + 4 + p}{1+\\sigma^2} \\\\\n&= \\frac{2}{1+\\sigma^2}\n\\end{aligned}\n\\]\n\nStep 3: Combine Terms\n\\[\nr(\\pi, d^{JS}) = \\underbrace{\\frac{p\\sigma^2}{1+\\sigma^2}}_{\\text{Variance Part}} + \\underbrace{\\frac{2}{1+\\sigma^2}}_{\\text{Bias Part}} = \\frac{p\\sigma^2 + 2}{\\sigma^2 + 1}\n\\]\nBoth methods yield the same result.\n\n\n\n\n3.5.9 Practical Application: One-way ANOVA and “Borrowing Strength”\n\nExample 3.9 Consider a One-Way ANOVA setting where we wish to estimate the means of \\(p\\) different independent groups (e.g., the true batting averages of \\(p=10\\) baseball players, or the efficacy of \\(p=5\\) different hospital treatments).\n\nModel: Let \\(X_i \\sim N(\\theta_i, \\sigma^2)\\) be the observed sample mean for group \\(i\\), for \\(i = 1, \\dots, p\\).\n\nGoal: Estimate the vector of true means \\(\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_p)\\) simultaneously. The loss is the sum of squared errors: \\(L(\\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\theta}}) = \\sum (\\theta_i - \\hat{\\theta}_i)^2\\).\n\n\nThe MLE Approach (Total Separation): The standard estimator is \\(\\hat{\\theta}_i^{\\text{MLE}} = X_i\\). This estimates each group entirely independently, using only data from that specific group. If a specific player has a lucky streak, their estimate is very high; if they are unlucky, it is very low.\nThe James-Stein Approach (Shrinkage / Pooling): In this context, the James-Stein estimator (specifically the variation shrinking toward the grand mean \\(\\bar{X}\\)) is: \\[\n\\hat{\\theta}_i^{JS} = \\bar{X} + \\left( 1 - \\frac{(p-3)\\sigma^2}{\\sum (X_i - \\bar{X})^2} \\right) (X_i - \\bar{X})\n\\]\nWhy is this better? Even though the groups might be physically independent (e.g., distinct hospitals), the James-Stein estimator “borrows strength” from the ensemble.\n\nNoise Reduction: Extreme observations \\(X_i\\) are likely to contain more positive noise than signal. Shrinking them toward the global average \\(\\bar{X}\\) reduces this variance.\n\nStein’s Paradox: While \\(\\hat{\\theta}_i^{JS}\\) introduces bias (estimates are pulled toward the center), the reduction in variance is so significant that the Total Risk (sum of squared errors over all groups) is strictly lower than that of the MLE, provided \\(p \\ge 3\\).\n\n\nThus, estimating the groups together yields a more accurate global picture than estimating them separately, even if the groups are independent.\n\n\n\n3.5.10 Why Is This Paradoxical?\nThe result that \\(d^{JS}\\) dominates \\(d^0\\) is called Stein’s Paradox because it defies intuition in several ways:\n\nIndependence Irrelevance: The result holds even if the components \\(X_i\\) are completely unrelated (e.g., \\(X_1\\) is the price of tea in China, \\(X_2\\) is the temperature in Saskatoon, and \\(X_3\\) is the weight of a local cat). It seems absurd that combining unrelated data improves the estimate of each, but the combined risk is indeed lower.\n\nNo “Free Lunch”: The James-Stein estimator does not improve every individual component \\(\\theta_i\\) simultaneously for every realization. Instead, it minimizes the total risk \\(\\sum E(\\hat{\\theta}_i - \\theta_i)^2\\). It sacrifices accuracy on outliers (by biasing them) to gain significant stability on the bulk of the data.\n\nDestruction of Symmetry: The MLE is invariant under translation and rotation. The James-Stein estimator breaks this symmetry by shrinking toward an arbitrary point (usually the origin or the grand mean), yet it yields a better objective performance.\n\n\n\n3.5.11 What We Learned\n\nBias-Variance Tradeoff: This is the most famous example where introducing bias (shrinkage) leads to a massive reduction in variance, thereby reducing the overall Mean Squared Error (MSE). Unbiasedness is not always a virtue in estimation.\n\nInadmissibility in High Dimensions: Intuitions formed in 1D or 2D (where MLE is admissible) fail in higher dimensions (\\(p \\ge 3\\)). The volume of space grows so fast that “standard” diffuse priors or MLEs become inefficient.\n\nHierarchical Modeling: Stein’s result provides the theoretical foundation for Hierarchical Bayesian Models. When we assume parameters come from a common distribution (e.g., \\(\\theta_i \\sim N(\\mu, \\tau^2)\\)), we naturally derive shrinkage estimators that “borrow strength” across groups, formalized as Empirical Bayes or fully Bayesian methods.\n\n\n\n3.5.12 Bias-Variance Decomposition for James-Stein Estimator\nBased on the derivations in your handwritten notes, here are the corresponding LaTeX equations formatted for your Quarto document.\n\\[\nX = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n\\]\nThe Mean Squared Error (MSE) of the James-Stein estimator \\(d^{JS}(X)\\) with respect to the parameter \\(\\theta\\) is decomposed by adding and subtracting the expected value \\(u_{d^{JS}} = E^{X|\\theta}[d^{JS}(X)]\\):\n\\[\n\\begin{aligned}\nE^{X|\\theta} \\left[ \\| d^{JS}(X) - \\theta \\|^2 \\right] &= E^{X|\\theta} \\left[ \\| d^{JS} - u_{d^{JS}} + u_{d^{JS}} - \\theta \\|^2 \\right] \\\\\n&= V(d^{JS}(X)) + \\left[ Bias(d^{JS}(X)) \\right]^2\n\\end{aligned}\n\\]\nwhere the components are defined as:\n\nVariance Component \\[ V(d^{JS}(X)) = E^{X|\\theta} \\left[ \\| d^{JS}(X) - u_{d^{JS}} \\|^2 \\right] \\]\nBias Component \\[ Bias(d^{JS}(X)) = u_{d^{JS}} - \\theta \\]\n\n\nRemark. Note that while the ordinary MLE \\(X\\) is unbiased (\\(E^{X|\\theta}(X) - \\theta = 0\\)), the James-Stein estimator introduces a deliberate bias to significantly reduce the variance, resulting in a lower total MSE when the dimension is three or greater.\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(latex2exp)\n\n# --- 1. Parameters ---\ntheta &lt;- c(1, 1)\nsigma &lt;- 0.35 \nshrink_const &lt;- 0.5\nradii &lt;- c(0.6, 1.2) * sigma\n\n# --- 2. Helper Function ---\njs_map &lt;- function(x, y) {\n  sq_norm &lt;- x^2 + y^2\n  s &lt;- 1 - shrink_const / sq_norm\n  return(c(x * s, y * s))\n}\n\n# --- 3. Observation Data (8 Points) ---\nangles_obs &lt;- seq(0, 2 * pi, length.out = 9)[1:8]\nobs_radii &lt;- rep(radii, length.out = 8)\n\nx_obs &lt;- theta[1] + obs_radii * cos(angles_obs)\ny_obs &lt;- theta[2] + obs_radii * sin(angles_obs)\n\ndf_obs &lt;- data.frame(x1 = x_obs, x2 = y_obs)\n\n# Apply mapping to observations\ndf_obs_mapped &lt;- t(apply(df_obs, 1, function(row) js_map(row[1], row[2])))\ndf_obs$x1_js &lt;- df_obs_mapped[,1]\ndf_obs$x2_js &lt;- df_obs_mapped[,2]\n\n# Empirical mean of the 8 shrunken points (+)\nmean_js &lt;- colMeans(df_obs[, c(\"x1_js\", \"x2_js\")])\nmean_js_df &lt;- data.frame(x = mean_js[1], y = mean_js[2])\n\n# --- 4. Construct Contours via Mapping ---\nangles_contour &lt;- seq(0, 2 * pi, length.out = 200)\n\ncontour_list &lt;- lapply(radii, function(r) {\n  gray_x &lt;- theta[1] + r * cos(angles_contour)\n  gray_y &lt;- theta[2] + r * sin(angles_contour)\n  green_pts &lt;- t(mapply(js_map, gray_x, gray_y))\n  \n  data.frame(\n    gx = gray_x, gy = gray_y,\n    jx = green_pts[,1], jy = green_pts[,2],\n    radius_id = as.factor(r)\n  )\n})\ndf_contours &lt;- bind_rows(contour_list)\n\n# --- 5. Plotting ---\nggplot() +\n  coord_fixed(xlim = c(-0.1, 2.2), ylim = c(-0.1, 2.2)) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.title = element_text(hjust = 0.5, size = 16),\n    legend.position = \"right\"\n  ) +\n  \n  # Arrowed Axis Lines\n  geom_segment(aes(x = 0, y = 0, xend = 2.1, yend = 0), \n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 2.1), \n               arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  \n  # Axis Labels\n  annotate(\"text\", x = 2.2, y = 0, label = TeX(\"$x_1$\"), vjust = 1.5, size = 5) +\n  annotate(\"text\", x = 0, y = 2.2, label = TeX(\"$x_2$\"), hjust = 1.5, size = 5) +\n  annotate(\"text\", x = -0.05, y = -0.05, label = \"O\", size = 4) +\n\n  # Grey Lines from Origin to Original Points (x)\n  geom_segment(data = df_obs, aes(x = 0, y = 0, xend = x1, yend = x2),\n               color = \"grey85\", size = 0.5, alpha = 0.7) +\n\n  # Original Density Contours (Gray Dashed)\n  geom_path(data = df_contours, aes(x = gx, y = gy, group = radius_id),\n            color = \"grey70\", linetype = \"dashed\") +\n\n  # Mapped Density Contours (Green Dashed)\n  geom_path(data = df_contours, aes(x = jx, y = jy, group = radius_id),\n            color = \"palegreen4\", linetype = \"dashed\", size = 0.7) +\n\n  # Points for Legend\n  geom_point(aes(x = theta[1], y = theta[2], color = \"theta\", shape = \"theta\"), size = 4) +\n  \n  geom_point(data = df_obs, aes(x = x1, y = x2, color = \"x\", shape = \"x\"), \n             size = 2.5, stroke = 0.8) +\n\n  geom_point(data = df_obs, aes(x = x1_js, y = x2_js, color = \"djs\", shape = \"djs\"), \n             size = 2.5) +\n\n  geom_point(data = mean_js_df, aes(x = x, y = y, color = \"mean\", shape = \"mean\"), \n             size = 3, stroke = 1.2) +\n\n  # Manual Scale Definitions\n  scale_color_manual(name = \"Legend\",\n                     values = c(\"theta\" = \"red\", \"x\" = \"black\", \n                                \"djs\" = \"darkgreen\", \"mean\" = \"darkgreen\"),\n                     labels = unname(TeX(c(\"$d^{JS}(x)$\", \"$\\\\bar{d}^{JS}$\", \"$x$\", \"$\\\\theta$\")))) +\n  scale_shape_manual(name = \"Legend\",\n                     values = c(\"theta\" = 16, \"x\" = 1, \"djs\" = 17, \"mean\" = 3),\n                     labels = unname(TeX(c(\"$d^{JS}(x)$\", \"$\\\\bar{d}^{JS}$\", \"$x$\", \"$\\\\theta$\")))) +\n  \n  ggtitle(TeX(\"Bias-Variance Tradeoff\"))\n\n\n\n\n\n\n\n\nFigure 3.7: Variance-Bias Tradeoff: 8 Alternated Points with Origin Vectors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes-rules",
    "href": "bayesian.html#empirical-bayes-rules",
    "title": "3  Bayesian Inference",
    "section": "3.6 Empirical Bayes Rules",
    "text": "3.6 Empirical Bayes Rules\nThe James-Stein estimator provides a natural entry point into the concept of Empirical Bayes (EB). While the Stein estimator was originally derived using frequentist risk arguments, it can be intuitively understood as a Bayesian estimator where the parameters of the prior distribution are estimated from the data itself.\n\n3.6.1 The General Empirical Bayes Framework\nIn a standard Bayesian analysis, the hyperparameters of the prior are fixed based on subjective belief or external information. In contrast, Empirical Bayes uses the observed data to “learn” the prior.\nThe workflow typically follows these steps:\n\nHierarchical Model: We assume the data \\(X\\) comes from a distribution \\(f(x|\\theta)\\), and the parameter \\(\\theta\\) comes from a prior \\(\\pi(\\theta|\\eta)\\) controlled by hyperparameters \\(\\eta\\).\nMarginal Likelihood (Evidence): We integrate out the parameter \\(\\theta\\) to obtain the marginal distribution of the data given the hyperparameters: \\[m(x|\\eta) = \\int f(x|\\theta) \\pi(\\theta|\\eta) d\\theta\\]\nEstimation of Hyperparameters: Instead of fixing \\(\\eta\\), we estimate it by maximizing the marginal likelihood (Type-II Maximum Likelihood) or using method-of-moments: \\[\\hat{\\eta} = \\underset{\\eta}{\\arg\\max} \\ m(x|\\eta)\\]\nPosterior Inference: We proceed with standard Bayesian inference, but we substitute the estimated estimate \\(\\hat{\\eta}\\) into the posterior: \\[\\pi(\\theta|x, \\hat{\\eta}) \\propto f(x|\\theta) \\pi(\\theta|\\hat{\\eta})\\]\n\nDiscussion:\n\n“Borrowing Strength”: EB allows us to pool information across independent groups to estimate the common structure (the prior) governing them.\n\nThe Critique: A purist Bayesian might object that using the data twice (once to estimate the prior, once to estimate \\(\\theta\\)) underestimates the uncertainty. A fully Bayesian Hierarchical model would instead place a “hyperprior” on \\(\\eta\\) and integrate it out.\n\n\n\n\n3.6.2 Deriving James-Stein as Empirical Bayes\nThe James-Stein estimator can be viewed as an Empirical Bayes procedure, where the hyperparameters of the prior are estimated directly from the data rather than being specified a priori.\nModel:\n\nLikelihood: \\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\n\nPrior: \\(\\mu_i \\sim N(0, \\sigma^2)\\), where \\(\\sigma^2\\) is an unknown hyperparameter.\n\n\nStep 1: The Ideal Bayes Estimator\nIf \\(\\sigma^2\\) were known, the posterior distribution of \\(\\mu_i\\) would be Normal. The optimal estimator under squared error loss is the posterior mean: \\[E^{\\mu_i|X_i, \\sigma^2}[\\mu_i] = \\frac{\\sigma^2}{1+\\sigma^2} X_i = \\left( 1 - \\frac{1}{1+\\sigma^2} \\right) X_i\\] We define the shrinkage factor \\(B = \\frac{1}{1+\\sigma^2}\\).\nStep 2: Marginal Estimation\nThe marginal distribution of the data (integrating out \\(\\mu_i\\)) is: \\[X_i \\sim N(0, 1+\\sigma^2)\\] Consequently, the sum of squares \\(S = ||X||^2 = \\sum X_i^2\\) follows a scaled Chi-squared distribution: \\[S \\sim (1+\\sigma^2) \\chi^2_p\\]\nStep 3: Estimating the Shrinkage Factor\nWe need an estimator for \\(B = \\frac{1}{1+\\sigma^2}\\). From the properties of the inverse Chi-square distribution, we know \\(E^X[1/\\chi^2_p] = \\frac{1}{p-2}\\) for \\(p &gt; 2\\). Therefore: \\[E^X \\left[ \\frac{p-2}{S} \\right] = \\frac{p-2}{1+\\sigma^2} E^X\\left[\\frac{1}{\\chi^2_p}\\right] = \\frac{p-2}{1+\\sigma^2} \\cdot \\frac{1}{p-2} = \\frac{1}{1+\\sigma^2} = B\\]\nThus, \\(\\hat{B} = \\frac{p-2}{||X||^2}\\) is an unbiased estimator of the optimal shrinkage factor \\(B\\).\nStep 4: The Empirical Bayes Rule\nPlugging \\(\\hat{B}\\) into the ideal Bayes estimator recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\hat{B} \\right) X = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\nRemarks:\n\nAdaptive Shrinkage: The James-Stein estimator automatically adjusts the amount of shrinkage based on the observed total magnitude \\(\\lVert X \\rVert^2\\). If the data suggests the true means are spread far from zero, \\(\\lVert X \\rVert^2\\) will be large, \\(\\hat{B}\\) will be small, and we shrink less.\nUnbiasedness of B: Interestingly, while \\(\\hat{B}\\) is an unbiased estimator of the shrinkage factor, the resulting James-Stein estimator itself is biased toward the origin. This is a classic example of sacrificing unbiasedness to minimize total risk.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-modeling-via-mcmc",
    "href": "bayesian.html#hierarchical-modeling-via-mcmc",
    "title": "3  Bayesian Inference",
    "section": "3.7 Hierarchical Modeling via MCMC",
    "text": "3.7 Hierarchical Modeling via MCMC\nIn complex Bayesian settings where the posterior distribution cannot be derived analytically, we utilize hierarchical structures to represent levels of uncertainty and Markov Chain Monte Carlo (MCMC) to approximate the resulting distributions.\n\n3.7.1 Hierarchical Model Structure\nA hierarchical model decomposes a complex joint distribution into a series of conditional levels. The general mathematical form is:\n\\[\n\\begin{aligned}\n\\text{Level 1 (Data Likelihood):} & \\quad X_i | \\mu_i, \\sigma^2 \\sim f(x_i | \\mu_i, \\sigma^2) \\\\\n\\text{Level 2 (Parameters):} & \\quad \\mu_i | \\theta, \\tau^2 \\sim \\pi(\\mu_i | \\theta, \\tau^2) \\\\\n\\text{Level 3 (Hyperparameters):} & \\quad \\theta, \\tau^2 \\sim \\pi(\\theta, \\tau^2)\n\\end{aligned}\n\\]\nThe goal is to compute the joint posterior distribution of all unobserved parameters given the data \\(X = \\{X_1, \\dots, X_n\\}\\):\n\\[\np(\\boldsymbol{\\mu}, \\theta, \\tau^2 | X) \\propto \\left[ \\prod_{i=1}^n f(x_i | \\mu_i, \\sigma^2) \\pi(\\mu_i | \\theta, \\tau^2) \\right] \\pi(\\theta, \\tau^2)\n\\]\n\n\n3.7.2 Graphical Model Representation (tree Structure)\nThe following tree diagram illustrates the conditional dependencies. Note that the parameters \\(\\mu_i\\) are conditionally independent given the hyperparameter \\(\\theta\\), which facilitates “borrowing strength” across groups.\n\n\n\n\n\n\n\n\nFigure 3.8: Hierarchical Tree Structure\n\n\n\n\n\n\n\n3.7.3 MCMC Estimation\nIn hierarchical models, the joint posterior distribution \\(p(\\boldsymbol{\\mu}, \\theta | X)\\) often lacks a closed-form analytical solution due to the integration required for the normalizing constant. We use Markov Chain Monte Carlo (MCMC) to draw sequence of samples \\(\\{\\boldsymbol{\\mu}^{(t)}, \\theta^{(t)}\\}\\) that converge to the target posterior distribution.\n\n3.7.3.1 Gibbs Sampling Algorithm\n\nGibbs sampling is an algorithm for sampling from a multivariate distribution by sequentially sampling from the full conditional distributions. To sample from a target distribution \\(p(\\theta_1, \\theta_2, \\dots, \\theta_k)\\), the algorithm iterates through each variable, updating it conditioned on the current values of all other variables:\n\\[\n\\begin{aligned}\n\\theta_1^{(t+1)} &\\sim p(\\theta_1 | \\theta_2^{(t)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n\\theta_2^{(t+1)} &\\sim p(\\theta_2 | \\theta_1^{(t+1)}, \\theta_3^{(t)}, \\dots, \\theta_k^{(t)}) \\\\\n&\\vdots \\\\\n\\theta_k^{(t+1)} &\\sim p(\\theta_k | \\theta_1^{(t+1)}, \\theta_2^{(t+1)}, \\dots, \\theta_{k-1}^{(t+1)})\n\\end{aligned}\n\\]\n\n\nExample 3.10 (Gibbs Sampling for Groups of Normal Data) The Model\nTo apply the general Gibbs sampling framework \\(\\theta_1, \\theta_2, \\dots, \\theta_k\\) to our specific hierarchical model, we identify the variables as follows:\n\nData Observations (\\(X_i\\)): These are the known, measured values at the lowest level of the hierarchy (e.g., test scores of students in school \\(i\\)). In the Gibbs sampler, these remain fixed and condition the updates of the parameters.\nGroup-Level Parameters (\\(\\theta_1 = \\mu_i\\)): These represent the latent means for each specific group or cluster. In the update step, \\(\\mu_i\\) acts as the first block of variables. It is updated by “compromising” between the local data \\(X_i\\) and the global characteristic \\(\\theta\\).\nGlobal Hyperparameter (\\(\\theta_2 = \\theta\\)): This represents the common mean across all groups. It acts as the second block in the sampler. Its update depends on the current state of all \\(\\mu_i\\) values, effectively “pooling” information from all groups to estimate the overall population center.\n\nGibbs Update in Hierarchical Models\nIn the hierarchical tree structure provided earlier, let our parameter vector be \\((\\mu_i, \\theta)\\). The “orthogonality” of the updates becomes clear when we derive the full conditionals for a Gaussian case:\n\nCase \\(\\theta_1 = \\mu_i\\): Sample \\(\\mu_i^{(t+1)}\\) from \\(p(\\mu_i | X_i, \\theta^{(t)})\\). This is a normal distribution with: \\[\n\\mu_i^{(t+1)} \\sim N\\left( \\frac{\\tau^2 X_i + \\sigma^2 \\theta^{(t)}}{\\sigma^2 + \\tau^2}, \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\right)\n\\]\nCase \\(\\theta_2 = \\theta\\): Sample \\(\\theta^{(t+1)}\\) from \\(p(\\theta | \\boldsymbol{\\mu}^{(t+1)})\\). Assuming a flat prior \\(\\pi(\\theta) \\propto 1\\): \\[\n\\theta^{(t+1)} \\sim N\\left( \\frac{1}{n} \\sum_{i=1}^n \\mu_i^{(t+1)}, \\frac{\\tau^2}{n} \\right)\n\\]\n\n\nVisual Characteristic: Gibbs sampling moves along the coordinate axes because it updates one parameter at a time while holding others constant.\n\n\n3.7.3.2 Metropolis-hastings (MH) Sampling\nWhen the full conditional distributions are not easy to sample from, we use the Metropolis-Hastings algorithm. At each step \\(t\\):\n\nPropose: Draw a candidate state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* | \\theta^{(t)})\\).\nAccept/Reject: Calculate the acceptance probability: \\[\n\\alpha = \\min \\left( 1, \\frac{p(\\theta^* | X) q(\\theta^{(t)} | \\theta^*)}{p(\\theta^{(t)} | X) q(\\theta^* | \\theta^{(t)})} \\right)\n\\]\nSet \\(\\theta^{(t+1)} = \\theta^*\\) with probability \\(\\alpha\\); otherwise, set \\(\\theta^{(t+1)} = \\theta^{(t)}\\).\n\nVisual Characteristic: MH sampling moves in arbitrary directions and can “stay put” if a proposal is rejected, exploring the space via a random walk.\n\n\n\n\n\n\n\n\nFigure 3.9: Comparison of Sampling Paths",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#case-study-1998-major-league-baseball-home-run-race",
    "href": "bayesian.html#case-study-1998-major-league-baseball-home-run-race",
    "title": "3  Bayesian Inference",
    "section": "3.8 Case Study: 1998 Major League Baseball Home Run Race",
    "text": "3.8 Case Study: 1998 Major League Baseball Home Run Race\nIn 1998, the baseball world was captivated by Mark McGwire and Sammy Sosa as they chased Roger Maris’ 1961 record of 61 home runs in a single season. While McGwire and Sosa finished with 70 and 66 home runs respectively, we consider whether such performance could have been predicted using pre-season exhibition data.\nFor a set of \\(i = 1, \\dots, 17\\) players (including McGwire and Sosa), we observe their batting records in pre-season exhibition matches. Our goal is to estimate each player’s home run “strike rate” for the competitive season.\n\n3.8.1 Transforming Data\nWe utilize the pre-season home runs (\\(y_i\\)) and at-bats (\\(n_i\\)) for 17 players. The data is transformed using a variance-stabilizing transformation to approximate a normal distribution with known variance \\(\\sigma^2 = 1\\).\n\\[\nx_i = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{y_i}{n_i} - 1 \\right)\n\\]\nThe goal is to estimate the latent parameter \\(\\mu_i\\) for each player and compare it to the “true” regular season performance.\n\n\n3.8.2 True Season Parameter (\\(\\mu_i\\) or \\(p_i^{season}\\))\nTo validate our estimates, we define the “true” parameter value \\(\\mu_i\\) using the player’s performance over the full competitive season. Let \\(Y_i\\) be the total home runs and \\(N_i\\) be the total at-bats in the regular season. The true transformed rate is calculated as:\n\\[\n\\mu_i^{\\text{season}} = \\sqrt{n_i} \\arcsin\\left( 2 \\frac{Y_i}{N_i} - 1 \\right)\n\\]\nNote that while we use the season-long probability (\\(Y_i/N_i\\)), we scale it by the pre-season sample size (\\(\\sqrt{n_i}\\)). This ensures that \\(\\mu_i^{\\text{season}}\\) is on the same scale as our observations \\(x_i\\), allowing for direct comparison of the estimation error.\n\n\n\n1998 MLB Statistics: Raw Counts, Probabilities, and Transformed Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(y_i\\)\n\\(n_i\\)\n\\(p_i^{\\text{pre}}\\)\n\\(x_i\\)\n\\(Y_i\\)\n\\(N_i\\)\n\\(p_i^{\\text{seas}}\\)\n\\(\\mu_i\\)\n\n\n\n\n1\n7\n58\n0.121\n-6.559\n70\n509\n0.138\n-6.176\n\n\n2\n9\n59\n0.153\n-5.901\n66\n643\n0.103\n-7.055\n\n\n3\n4\n74\n0.054\n-9.476\n56\n633\n0.088\n-8.317\n\n\n4\n7\n84\n0.083\n-9.029\n46\n645\n0.071\n-9.441\n\n\n5\n3\n69\n0.043\n-9.558\n45\n606\n0.074\n-8.463\n\n\n6\n6\n63\n0.095\n-7.488\n44\n555\n0.079\n-7.937\n\n\n7\n2\n60\n0.033\n-9.323\n43\n619\n0.069\n-8.035\n\n\n8\n10\n54\n0.185\n-5.005\n40\n609\n0.066\n-7.734\n\n\n9\n2\n53\n0.038\n-8.589\n37\n552\n0.067\n-7.622\n\n\n10\n2\n60\n0.033\n-9.323\n34\n540\n0.063\n-8.238\n\n\n11\n4\n66\n0.061\n-8.720\n32\n561\n0.057\n-8.843\n\n\n12\n3\n66\n0.045\n-9.270\n30\n440\n0.068\n-8.469\n\n\n13\n2\n72\n0.028\n-10.487\n29\n585\n0.050\n-9.518\n\n\n14\n5\n64\n0.078\n-8.034\n28\n531\n0.053\n-8.859\n\n\n15\n3\n42\n0.071\n-6.673\n23\n454\n0.051\n-7.237\n\n\n16\n2\n38\n0.053\n-6.829\n21\n504\n0.042\n-7.149\n\n\n17\n6\n58\n0.103\n-6.975\n15\n244\n0.061\n-8.146\n\n\n\n\n\nIn this analysis, we model the home run strike rates of 17 Major League Baseball players using pre-season exhibition data from 1998. We apply five statistical methods ranging from simple independent estimation to advanced Bayesian decision theory.\n\n\n3.8.3 Methods for Estimating \\(\\mu_i\\) (transformed Scale)\n\n3.8.3.1 Method 1: Simple Estimation (MLE)\nThe Maximum Likelihood Estimator (MLE) assumes each player’s performance is independent. It relies solely on the observed pre-season data.\n\\[ \\hat{\\mu}_i^{MLE} = X_i \\]\n\n\nCode\n# Simple Estimate Is Just the Data Itself\nmu_mle &lt;- baseball_data$x\n\n# MSE Calculation (transformed Scale)\nmse_mle &lt;- mean((mu_mle - baseball_data$true_mu)^2)\n\n\n\n\n3.8.3.2 Method 2: Empirical Bayes (james-stein)\nThe James-Stein estimator introduces a global mean \\(\\bar{X}\\) and shrinks individual estimates toward it. This assumes the players come from a common population distribution.\n\\[ \\hat{\\mu}_i^{JS} = \\bar{X} + \\left( 1 - \\frac{k-3}{\\sum (X_i - \\bar{X})^2} \\right) (X_i - \\bar{X}) \\]\nwhere \\(k=17\\) is the number of players.\n\n\nCode\ntheta_hat &lt;- mean(baseball_data$x)\nS &lt;- sum((baseball_data$x - theta_hat)^2)\nshrinkage_factor &lt;- 1 - (14 / S)\n\nmu_js &lt;- theta_hat + shrinkage_factor * (baseball_data$x - theta_hat)\n\n# MSE Calculation (transformed Scale)\nmse_js &lt;- mean((mu_js - baseball_data$true_mu)^2)\n\n\n\n\n3.8.3.3 Method 3: Fully Bayesian MCMC (brms)\nWe use a hierarchical Bayesian model where parameters are treated as random variables. We implement this using brms.\n\\[\n\\begin{aligned}\nX_i &\\sim N(\\mu_i, 1) \\\\\n\\mu_i &\\sim N(\\theta, \\tau^2) \\\\\n\\theta &\\sim N(0, 10) \\\\\n\\tau &\\sim \\text{Cauchy}(0, 2)\n\\end{aligned}\n\\]\n\n\nCode\nbaseball_data$sei &lt;- rep(1, length(baseball_data$x))\n# Fit Random Intercept Model: X | Se(1) ~ 1 + (1|player)\nfit_brms &lt;- brm(\n  formula = x | se(sei, sigma = TRUE) ~ 1 + (1 | Player),\n  data = baseball_data,\n  prior = c(\n    prior(normal(0, 10), class = \"Intercept\"),\n    prior(cauchy(0, 2), class = \"sd\")\n  ),\n  chains = 2, iter = 4000, warmup = 1000, seed = 123,\n  refresh = 0\n)\n\n# Extract Point Estimates (posterior Means)\npost_means &lt;- fitted(fit_brms)[, \"Estimate\"]\nmu_brms &lt;- post_means\n\n# MSE Calculation (transformed Scale)\nmse_brms &lt;- mean((mu_brms - baseball_data$true_mu)^2)\n\n\n\n\n\n3.8.4 Comparison of Estimates of \\(\\mu_i\\)\nFull Comparison of Estimates (Transformed Scale)\nThe following table presents the transformed data (\\(x_i\\)) and the true season parameter (\\(\\mu_i\\)) alongside the estimates from the three methods. The rows are sorted by \\(x_i\\) to visualize how the shrinkage methods (James-Stein and Bayesian) pull the estimates away from the extremes and toward the population mean compared to the raw MLE.\n\n\n\n\nTable 3.3: Comparison of Estimates (Sorted by Pre-season \\(x_i\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\n\\(x_i\\) (MLE)\n\\(\\hat{\\mu}_{JS}\\)\n\\(\\hat{\\mu}_{Bayes}\\)\n\\(\\mu_{true}\\)\n\n\n\n\n13\n-10.487\n-9.589\n-8.746\n-9.518\n\n\n5\n-9.558\n-9.006\n-8.478\n-8.463\n\n\n3\n-9.476\n-8.954\n-8.470\n-8.317\n\n\n7\n-9.323\n-8.858\n-8.412\n-8.035\n\n\n10\n-9.323\n-8.858\n-8.415\n-8.238\n\n\n12\n-9.270\n-8.825\n-8.412\n-8.469\n\n\n4\n-9.029\n-8.673\n-8.331\n-9.441\n\n\n11\n-8.720\n-8.479\n-8.260\n-8.843\n\n\n9\n-8.589\n-8.397\n-8.206\n-7.622\n\n\n14\n-8.034\n-8.048\n-8.054\n-8.859\n\n\n6\n-7.488\n-7.705\n-7.897\n-7.937\n\n\n17\n-6.975\n-7.384\n-7.754\n-8.146\n\n\n16\n-6.829\n-7.292\n-7.714\n-7.149\n\n\n15\n-6.673\n-7.194\n-7.663\n-7.237\n\n\n1\n-6.559\n-7.122\n-7.628\n-6.176\n\n\n2\n-5.901\n-6.709\n-7.441\n-7.055\n\n\n8\n-5.005\n-6.146\n-7.186\n-7.734\n\n\n\n\n\n\n\n\nPlots of Squared Errors (Sorted by \\(x_i\\))\nThis plot displays the Squared Error for each player. The x-axis represents the players sorted from lowest pre-season performance to highest.\n\n\nCode\n# Calculate Squared Errors Using the SORTED Dataframe\nerr_mle  &lt;- (df_sorted$x_i - df_sorted$mu_true)^2\nerr_js   &lt;- (df_sorted$mu_js - df_sorted$mu_true)^2\nerr_brms &lt;- (df_sorted$mu_bayes - df_sorted$mu_true)^2\n\n# Determine Y-axis Range\ny_max &lt;- max(c(err_mle, err_js, err_brms))\n\n# Plot MLE Errors (baseline)\nplot(1:17, err_mle, type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season Performance)\", \n     ylab = expression(Squared~Error~~(hat(mu) - mu[true])^2),\n     main = \"Estimation Error Comparison (Sorted)\",\n     ylim = c(0, y_max))\n\n# Add James-stein Errors\nlines(1:17, err_js, type = \"b\", pch = 19, col = \"blue\")\n\n# Add Bayesian (brms) Errors\nlines(1:17, err_brms, type = \"b\", pch = 17, col = \"red\")\n\n# Add Grid and Legend\ngrid()\nlegend(\"topleft\", \n       title = \"Mean Squared Error\",\n       legend = c(paste0(\"MLE: \", round(mse_mle, 3)), \n                  paste0(\"JS: \", round(mse_js, 3)), \n                  paste0(\"Bayes: \", round(mse_brms, 3))),\n       col = c(\"black\", \"blue\", \"red\"), \n       pch = c(1, 19, 17), \n       lty = c(2, 1, 1))\n\n\n\n\n\n\n\n\nFigure 3.10: Squared Error by Sorted Player Index (Transformed Scale)\n\n\n\n\n\n\n\n3.8.5 Methods for Estimating \\(p_i\\) Directly\n\n3.8.5.1 Method 1-3: Converting \\(\\hat \\mu_i\\) Back to \\(p_i\\)\nThe first three methods (MLE, James-Stein, and Normal-Normal Bayes) estimated the parameter \\(\\mu_i\\) on the transformed scale. To obtain the probability estimates \\(\\hat{p}_i\\), we apply the inverse of the variance-stabilizing transformation:\n\\[ \\hat{p}_i = \\frac{1}{2} \\left( \\sin\\left( \\frac{\\hat{\\mu}_i}{\\sqrt{n_i}} \\right) + 1 \\right) \\]\nwhere \\(\\hat{\\mu}_i\\) corresponds to the estimate derived from Method 1, 2, or 3, and \\(n_i\\) is the number of pre-season at-bats for player \\(i\\).\n\n\n3.8.5.2 Method 4: Hierarchical Logistic Regression (logit-normal)\nIn this fourth method, we model the probability \\(p_i\\) directly using a hierarchical structure on the log-odds scale, rather than transforming the data.\nWe assume the count \\(y_i\\) follows a Binomial distribution. The log-odds (logit) of the success rate \\(p_i\\) are drawn from a common Normal distribution with unknown mean \\(\\mu_0\\) and standard deviation \\(\\tau_0\\).\n\\[\n\\begin{aligned}\ny_i | p_i &\\sim \\text{Binomial}(n_i, p_i) \\\\\n\\text{logit}(p_i) &\\sim N(\\mu_0, \\tau_0^2) \\\\\n\\mu_0 &\\sim N(0, 10) \\\\\n\\tau_0 &\\sim \\text{Cauchy}(0, 2)\n\\end{aligned}\n\\]\nWe implement this in brms using the binomial family with a logit link. The individual point estimate \\(\\hat{p}_i\\) is the posterior mean of \\(p_i\\). Note that because the inverse-logit function is non-linear, the posterior mean of \\(p_i\\) is not simply the inverse-logit of the posterior mean of the random effect; brms handles this integration automatically via the fitted() function.\n\n\nCode\n# 1. Fit Hierarchical Logistic Regression\n# We use the 'file' argument to cache the model on disk\nfit_logit &lt;- brm(\n  formula = Pre_HR | trials(Pre_AtBats) ~ 1 + (1 | Player),\n  data = baseball_data,\n  family = binomial(link = \"logit\"),\n  prior = c(\n    prior(normal(0, 5), class = \"Intercept\"),\n    prior(cauchy(0, 2), class = \"sd\")\n  ),\n  chains = 2, iter = 4000, warmup = 1000, seed = 123,\n  refresh = 0,\n  file = \"fit_logit_baseball\" \n)\n\n# 2. Extract specific players for the plots\nordered_players &lt;- baseball_data %&gt;% arrange(p_pre)\ntarget_players &lt;- ordered_players[c(1, 2, 16, 17), ]\n\n# 3. Extract Posterior Draws\n# summary = FALSE gives the full MCMC matrix (iter x players)\npost_draws &lt;- fitted(fit_logit, \n                     newdata = target_players, \n                     summary = FALSE)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# 1. Prepare data for plotting\nplot_list &lt;- list()\nfor(i in 1:nrow(target_players)) {\n  p_id &lt;- target_players$Player[i]\n  n_i &lt;- target_players$Pre_AtBats[i]\n  \n  plot_list[[i]] &lt;- data.frame(\n    Player = paste(\"Player\", p_id),\n    p_sample = post_draws[, i] / n_i,\n    p_pre = target_players$p_pre[i],\n    p_season = target_players$p_season[i]\n  )\n}\nplot_data &lt;- do.call(rbind, plot_list)\n\n# 2. Draw the faceted density plots\nggplot(plot_data, aes(x = p_sample)) +\n  geom_density(fill = \"steelblue\", alpha = 0.4, color = \"steelblue\") +\n  geom_vline(aes(xintercept = p_pre), \n             color = \"grey50\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(aes(xintercept = p_season), \n             color = \"black\", linetype = \"solid\", linewidth = 1) +\n  facet_wrap(~Player, scales = \"free\", ncol = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Posterior Distributions of HR Probabilities for Extreme Players\",\n    subtitle = \"Dashed Grey: Pre-season (Observed) | Solid Black: Remainder of Season (Actual)\",\n    x = expression(p[i]),\n    y = \"Posterior Density\"\n  ) +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    panel.spacing = unit(1, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.8.5.3 Method 5: Optimal Bayes Estimator w.r.t. Relative Absolute Error\nWhile the posterior mean (Method 4) minimizes the Mean Squared Error (MSE), it is not necessarily optimal for the Relative Standardized Error metric we defined earlier: \\[L(p, \\hat{p}) = \\frac{|p - \\hat{p}|}{\\min(p, 1-p)}\\]\nThis is a form of weighted absolute error loss, where the weight is \\(w(p) = \\frac{1}{\\min(p, 1-p)}\\). Theoretical derivation shows that the estimator minimizing the expected posterior loss for this function is the Weighted Posterior Median.\nWe compute this by extracting the full posterior samples from the Logit-Normal model (Method 4) and calculating the weighted median for each player.\n\n\nCode\n# 1. Extract Posterior Samples (n_samples X 17 Players)\n# Posterior_epred Gives Samples of the Expected Count (N * P)\npost_counts &lt;- posterior_epred(fit_logit) \n\n# Convert to Probability Scale by Dividing by Trials\np_samples &lt;- sweep(post_counts, 2, baseball_data$Pre_AtBats, \"/\")\n\n# 2. Extract Posterior Means (Method 4)\n# This provides the missing p_hat_logit variable\np_hat_logit &lt;- colMeans(p_samples)\n\n# 3. Define Function for Weighted Median\n# Finds the Value 'q' Such That Sum(weights Where X &lt;= Q) &gt;= 0.5 * Total_weight\nget_weighted_median &lt;- function(samples) {\n  # Calculate weights based on the loss function denominator\n  # Avoid division by exact zero (unlikely but safer)\n  denom &lt;- pmin(samples, 1 - samples)\n  denom[denom &lt; 1e-6] &lt;- 1e-6 \n  weights &lt;- 1 / denom\n  \n  # Normalize weights\n  weights_norm &lt;- weights / sum(weights)\n  \n  # Sort samples and weights\n  ord &lt;- order(samples)\n  samp_sorted &lt;- samples[ord]\n  w_sorted &lt;- weights_norm[ord]\n  \n  # Find cutoff\n  cum_w &lt;- cumsum(w_sorted)\n  idx &lt;- which(cum_w &gt;= 0.5)[1]\n  \n  return(samp_sorted[idx])\n}\n\n# 4. Apply to All Players (Method 5)\np_hat_optimal &lt;- apply(p_samples, 2, get_weighted_median)\n\n\n\n\n3.8.5.4 Comparison of All Five Estimates of \\(p_i\\)\nWe now compare all five methods: MLE, James-Stein (transformed), Bayes Normal-Normal (transformed), Hierarchical Logit-Normal (Posterior Mean), and Optimal Bayes (Weighted Median).\n\n\n\n\nTable 3.4: Comparison of Estimated Probabilities (\\(p_i\\)) across Five Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\nSeason Avg (\\(p_i\\))\nMLE\nJames-Stein\nNormal-Bayes\nLogit-Normal\nOptimal-Bayes\n\n\n\n\n13\n0.050\n0.028\n0.048\n0.071\n0.056\n0.048\n\n\n7\n0.069\n0.033\n0.045\n0.058\n0.060\n0.051\n\n\n10\n0.063\n0.033\n0.045\n0.058\n0.060\n0.051\n\n\n9\n0.067\n0.038\n0.043\n0.048\n0.062\n0.054\n\n\n5\n0.074\n0.043\n0.058\n0.074\n0.062\n0.055\n\n\n12\n0.068\n0.045\n0.058\n0.070\n0.063\n0.055\n\n\n16\n0.042\n0.053\n0.037\n0.025\n0.068\n0.060\n\n\n3\n0.088\n0.054\n0.069\n0.083\n0.066\n0.059\n\n\n11\n0.057\n0.061\n0.068\n0.075\n0.068\n0.062\n\n\n15\n0.051\n0.071\n0.052\n0.037\n0.073\n0.065\n\n\n14\n0.053\n0.078\n0.078\n0.077\n0.075\n0.067\n\n\n4\n0.071\n0.083\n0.094\n0.106\n0.078\n0.071\n\n\n6\n0.079\n0.095\n0.087\n0.081\n0.082\n0.073\n\n\n17\n0.061\n0.103\n0.088\n0.074\n0.084\n0.075\n\n\n1\n0.138\n0.121\n0.098\n0.079\n0.091\n0.079\n\n\n2\n0.103\n0.153\n0.117\n0.088\n0.105\n0.090\n\n\n8\n0.066\n0.185\n0.129\n0.085\n0.118\n0.098\n\n\n\n\n\n\n\n\n1. MSE Comparison\n\n\nCode\n# 3. Calculate MSE\nmse_p_mle   &lt;- mean((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)\nmse_p_js    &lt;- mean((df_compare_sorted$p_js - df_compare_sorted$p_true)^2)\nmse_p_norm  &lt;- mean((df_compare_sorted$p_norm - df_compare_sorted$p_true)^2)\nmse_p_logit &lt;- mean((df_compare_sorted$p_logit - df_compare_sorted$p_true)^2)\nmse_p_opt   &lt;- mean((df_compare_sorted$p_opt - df_compare_sorted$p_true)^2)\n\n# 4. Plot MSE\ny_max &lt;- max((df_compare_sorted$p_mle - df_compare_sorted$p_true)^2)\n\nplot(1:17, (df_compare_sorted$p_mle - df_compare_sorted$p_true)^2, \n     type = \"b\", pch = 1, col = \"black\", lty = 2,\n     xlab = \"Player Index (Sorted by Pre-season)\",\n     ylab = \"Squared Error\",\n     main = \"Squared Error by Method\",\n     ylim = c(0, y_max))\n\nlines(1:17, (df_compare_sorted$p_js - df_compare_sorted$p_true)^2, type = \"b\", pch = 19, col = \"blue\")\nlines(1:17, (df_compare_sorted$p_norm - df_compare_sorted$p_true)^2, type = \"b\", pch = 17, col = \"red\")\nlines(1:17, (df_compare_sorted$p_logit - df_compare_sorted$p_true)^2, type = \"b\", pch = 15, col = \"darkgreen\")\nlines(1:17, (df_compare_sorted$p_opt - df_compare_sorted$p_true)^2, type = \"b\", pch = 18, col = \"purple\")\n\ngrid()\nlegend(\"topleft\",\n       legend = c(paste0(\"MLE [MSE: \", round(mse_p_mle, 4), \"]\"),\n                  paste0(\"JS [MSE: \", round(mse_p_js, 4), \"]\"),\n                  paste0(\"Normal-Bayes [MSE: \", round(mse_p_norm, 4), \"]\"),\n                  paste0(\"Logit-Normal [MSE: \", round(mse_p_logit, 4), \"]\"),\n                  paste0(\"Optimal-Bayes [MSE: \", round(mse_p_opt, 4), \"]\")),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\", \"purple\"),\n       pch = c(1, 19, 17, 15, 18),\n       lty = c(2, 1, 1, 1, 1),\n       cex = 0.75,\n       bg = \"white\")\n\n\n\n\n\n\n\n\n\n2. Comparison of Relative Absolute Error\nWe also evaluate the methods using the relative error metric that penalizes deviations based on the rarity of the event: \\[ \\text{Metric}_i = \\frac{|p_i^{\\text{true}} - \\hat{p}_i|}{\\min(p_i^{\\text{true}}, 1 - p_i^{\\text{true}})} \\]\n\n\n\n\n\n\n\n\nFigure 3.11: Relative Error Assessment: Five Methods",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#bayesian-predictive-distributions",
    "href": "bayesian.html#bayesian-predictive-distributions",
    "title": "3  Bayesian Inference",
    "section": "3.9 Bayesian Predictive Distributions",
    "text": "3.9 Bayesian Predictive Distributions\nA key feature of Bayesian analysis is the ability to make inference about future observations, rather than just the model parameters. The posterior predictive distribution describes the probability of observing a new data point \\(y^*\\) given the observed data \\(y\\).\n\nDefinition 3.3 (Posterior Predictive Distribution) Let \\(f(y^*|\\theta)\\) be the sampling distribution of a future observation \\(y^*\\) given parameter \\(\\theta\\), and let \\(\\pi(\\theta|y)\\) be the posterior distribution of \\(\\theta\\) given observed data \\(y\\). The posterior predictive density is obtained by marginalizing over the parameter \\(\\theta\\):\n\\[\nf(y^*|y) = \\int_\\Theta f(y^*|\\theta) \\pi(\\theta|y) \\, d\\theta\n\\]\n\nThis distribution incorporates two distinct sources of uncertainty:\n\nSampling Uncertainty (Aleatoric): The inherent variability of the data generation process, represented by the variance in \\(f(y^*|\\theta)\\).\nParameter Uncertainty (Epistemic): The uncertainty regarding the true value of \\(\\theta\\), represented by the variance in the posterior \\(\\pi(\\theta|y)\\).\n\nAs sample size \\(n \\to \\infty\\), the parameter uncertainty vanishes (the posterior approaches a point mass), and the predictive distribution converges to the true data-generating distribution.\n\nExample 3.11 (Normal-normal Predictive Distribution) Consider a case where the data \\(y_1, \\dots, y_n\\) are independent and normally distributed with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\):\n\\[\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\\]\nAssume a conjugate prior for the mean: \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\). The posterior distribution is \\(\\mu|y \\sim N(\\mu_n, \\sigma_n^2)\\), where \\(\\mu_n\\) and \\(\\sigma_n^2\\) are the updated posterior hyperparameters.\nThe predictive distribution for a new observation \\(y^*\\) is derived as:\n\\[\n\\begin{aligned}\nf(y^*|y) &= \\int_{-\\infty}^{\\infty} f(y^*|\\mu) \\pi(\\mu|y) \\, d\\mu \\\\\n&= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y^*-\\mu)^2}{2\\sigma^2}} \\times \\frac{1}{\\sqrt{2\\pi\\sigma_n^2}} e^{-\\frac{(\\mu-\\mu_n)^2}{2\\sigma_n^2}} \\, d\\mu\n\\end{aligned}\n\\]\nThis convolution of two Gaussians results in a new Gaussian distribution:\n\\[\ny^* | y \\sim N(\\mu_n, \\sigma^2 + \\sigma_n^2)\n\\]\nHere, the total predictive variance is the sum of the data variance (\\(\\sigma^2\\)) and the posterior uncertainty about the mean (\\(\\sigma_n^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "sufficiency.html",
    "href": "sufficiency.html",
    "title": "4  Sufficient Statistic",
    "section": "",
    "text": "4.1 Sufficient Statistics\nFigure 4.1: Visualizing Parallel Log-Likelihoods",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sufficient Statistic</span>"
    ]
  },
  {
    "objectID": "sufficiency.html#sufficient-statistics",
    "href": "sufficiency.html#sufficient-statistics",
    "title": "4  Sufficient Statistic",
    "section": "",
    "text": "Definition 4.1 (Sufficient Statistic) A statistic \\(T=T(\\mathbf{X})\\) is sufficient for \\(\\theta\\) if one of the following three equivalent conditions holds:\n\nParallel Log-Likelihood\nFor any pair of data sets \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) such that \\(T(\\mathbf{x})=T(\\mathbf{y})\\), the difference in their log-likelihoods (\\(\\ell(\\theta;\\mathbf{x})=\\ln(f(\\mathbf{x};\\theta)))\\) is constant with respect to \\(\\theta\\): \\[\n\\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y}) \\quad \\text{for all } \\theta\n\\] where \\(c(\\mathbf{x},\\mathbf{y})\\) depends only on \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), not on \\(\\theta\\).\nFactorization of Likelihood\nThe likelihood function of \\(\\theta\\) given \\(\\mathbf{x}\\) can be expressed as: \\[\nL(\\theta; \\mathbf{x}) = h(\\mathbf{x})g(T(\\mathbf{x});\\theta)\n\\] where \\(h(\\mathbf{x})\\) is irrelevant to \\(\\theta\\).\nNon-informative Conditional Distribution of \\(\\mathbf{X}|T(\\mathbf{X})\\)\nThe conditional distribution of \\(\\mathbf{X}\\) given \\(T(\\mathbf{X})=t\\), denoted as \\(f(\\mathbf{x}|t, \\theta)\\), is independent of \\(\\theta\\). \\[\nf(\\mathbf{x}|T(\\mathbf{x})=t, \\theta) = f(\\mathbf{x}|t)\n\\]\n\n\n\n\nTheorem 4.1 (Factorization Theorem) The three conditions in the definitions of Definition 4.1 are equivalent.\n\n\n\nClick to view Complete Proof of Equivalence\n\n\nProof. Proof of Equivalence\nWe show the equivalence by proving the implications in a cycle or pairs: \\((2 \\Rightarrow 1)\\), \\((1 \\Rightarrow 2)\\), \\((2 \\Rightarrow 3)\\), and \\((3 \\Rightarrow 2)\\).\n\nFactorization \\(\\Rightarrow\\) Log-Likelihood Difference \\((2 \\Rightarrow 1)\\)\nAssume the Factorization Theorem holds: \\(L(\\theta; \\mathbf{x}) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)\\). Consider any pair \\(\\mathbf{x}, \\mathbf{y}\\) such that \\(T(\\mathbf{x}) = T(\\mathbf{y})\\). \\[\n\\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = [\\ln h(\\mathbf{x}) + \\ln g(T(\\mathbf{x}); \\theta)] - [\\ln h(\\mathbf{y}) + \\ln g(T(\\mathbf{y}); \\theta)]\n\\] Since \\(T(\\mathbf{x})=T(\\mathbf{y})\\), the terms \\(\\ln g(T(\\mathbf{x}); \\theta)\\) and \\(\\ln g(T(\\mathbf{y}); \\theta)\\) are identical and cancel out. \\[\n\\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = \\ln h(\\mathbf{x}) - \\ln h(\\mathbf{y})\n\\] This difference depends only on \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) (via \\(h\\)), and is independent of \\(\\theta\\). Thus, condition 1 holds.\nLog-Likelihood Difference \\(\\Rightarrow\\) Factorization \\((1 \\Rightarrow 2)\\)\nAssume Condition 1 holds. For any \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) with \\(T(\\mathbf{x})=T(\\mathbf{y})\\), \\(\\ell(\\theta; \\mathbf{x}) - \\ell(\\theta; \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y})\\). Exponentiating, we get \\(L(\\theta; \\mathbf{x}) = k(\\mathbf{x}, \\mathbf{y}) L(\\theta; \\mathbf{y})\\), where \\(k\\) is independent of \\(\\theta\\).\nFor each value \\(t\\) in the range of \\(T\\), select a fixed representative data point \\(\\mathbf{x}_t\\) such that \\(T(\\mathbf{x}_t) = t\\). For any data point \\(\\mathbf{x}\\), let \\(t = T(\\mathbf{x})\\). Using the relation above: \\[\nL(\\theta; \\mathbf{x}) = k(\\mathbf{x}, \\mathbf{x}_t) L(\\theta; \\mathbf{x}_t)\n\\] Define \\(h(\\mathbf{x}) = k(\\mathbf{x}, \\mathbf{x}_{T(\\mathbf{x})})\\) and \\(g(t; \\theta) = L(\\theta; \\mathbf{x}_t)\\). Then: \\[\nL(\\theta; \\mathbf{x}) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)\n\\] This is exactly the Factorization form.\nFactorization \\(\\Rightarrow\\) Conditional Distribution \\((2 \\Rightarrow 3)\\)\nAssume \\(f(\\mathbf{x}; \\theta) = h(\\mathbf{x})g(T(\\mathbf{x}); \\theta)\\). We derive the conditional distribution \\(P(\\mathbf{X}=\\mathbf{x} | T(\\mathbf{X})=t)\\). If \\(T(\\mathbf{x}) \\neq t\\), the probability is 0 (independent of \\(\\theta\\)). If \\(T(\\mathbf{x}) = t\\): \\[\nP(\\mathbf{X}=\\mathbf{x} | T(\\mathbf{X})=t) = \\frac{P(\\mathbf{X}=\\mathbf{x}, T(\\mathbf{X})=t)}{P(T(\\mathbf{X})=t)} = \\frac{f(\\mathbf{x}; \\theta)}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} f(\\mathbf{y}; \\theta)}\n\\] Substitute the factorization: \\[\n= \\frac{h(\\mathbf{x})g(t; \\theta)}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})g(t; \\theta)} = \\frac{h(\\mathbf{x})g(t; \\theta)}{g(t; \\theta) \\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})}\n\\] The term \\(g(t; \\theta)\\) cancels out: \\[\n= \\frac{h(\\mathbf{x})}{\\sum_{\\{\\mathbf{y} : T(\\mathbf{y})=t\\}} h(\\mathbf{y})}\n\\] This expression depends only on \\(\\mathbf{x}\\) and \\(h(\\cdot)\\), and is entirely free of \\(\\theta\\). Thus, Condition 3 holds.\nConditional Distribution \\(\\Rightarrow\\) Factorization \\((3 \\Rightarrow 2)\\)\nAssume \\(f(\\mathbf{x} | T(\\mathbf{x}); \\theta) = k(\\mathbf{x})\\), where \\(k(\\mathbf{x})\\) is independent of \\(\\theta\\). We can write the joint distribution as: \\[\nf(\\mathbf{x}; \\theta) = f(\\mathbf{x} | T(\\mathbf{x})=t; \\theta) \\cdot P(T(\\mathbf{X})=t; \\theta)\n\\] Substitute the assumption: \\[\nf(\\mathbf{x}; \\theta) = k(\\mathbf{x}) \\cdot P(T(\\mathbf{X})=T(\\mathbf{x}); \\theta)\n\\] Let \\(h(\\mathbf{x}) = k(\\mathbf{x})\\) and \\(g(t; \\theta) = P(T(\\mathbf{X})=t; \\theta)\\). Then: \\[\nf(\\mathbf{x}; \\theta) = h(\\mathbf{x}) g(T(\\mathbf{x}); \\theta)\n\\] This recovers the Factorization form.\n\n\n\n\nExample 4.1 (Uniform Distribution \\(U(\\theta-1, \\theta+1)\\)) Consider a random sample \\(\\mathbf{X} = (X_1, \\dots, X_n)\\) from a Uniform distribution with range \\((\\theta-1, \\theta+1)\\).\nThe density for a single observation is: \\[\nf(x_i|\\theta) = \\frac{1}{(\\theta+1) - (\\theta-1)} I(\\theta-1 &lt; x_i &lt; \\theta+1) = \\frac{1}{2} I(\\theta-1 &lt; x_i &lt; \\theta+1)\n\\]\nThe joint PDF (likelihood) for the vector \\(\\mathbf{x}\\) is: \\[\nL(\\theta; \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{2} I(\\theta-1 &lt; x_i &lt; \\theta+1)\n\\]\n\\[\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( \\min(x_i) &gt; \\theta-1 ) \\cdot I( \\max(x_i) &lt; \\theta+1 )\n\\]\nUsing order statistics notation where \\(X_{(1)} = \\min(X_i)\\) and \\(X_{(n)} = \\max(X_i)\\): \\[\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( \\theta &lt; X_{(1)} + 1 ) \\cdot I( \\theta &gt; X_{(n)} - 1 )\n\\]\n\\[\nL(\\theta; \\mathbf{x}) = 2^{-n} \\cdot I( X_{(n)} - 1 &lt; \\theta &lt; X_{(1)} + 1 )\n\\]\nBy the Factorization Theorem, we can define:\n\n\\(h(\\mathbf{x}) = 2^{-n}\\) (or simply 1, grouping constants into \\(g\\))\n\\(g(T(\\mathbf{x}), \\theta) = I( X_{(n)} - 1 &lt; \\theta &lt; X_{(1)} + 1 )\\)\n\nThus, the sufficient statistic is the pair of order statistics: \\[\nT(\\mathbf{X}) = (X_{(1)}, X_{(n)})\n\\]\n\n\nExample 4.2 (Gamma Distribution) Let \\(\\mathbf{X} = (X_1, \\dots, X_n)\\) be i.i.d. \\(\\Gamma(\\alpha, \\beta)\\). The pdf is:\n\\[\nf(x_i|\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}, \\quad x_i &gt; 0\n\\]\nThe joint likelihood is:\n\\[\nL(\\alpha, \\beta; \\mathbf{x}) = \\left( \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\right)^n \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n\\]\nBy the Factorization Theorem, we can identify the parts that depend on the data and the parameters: \\[\ng(T(\\mathbf{x}), \\mathbf{\\theta}) = \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha} \\exp\\left( -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right)\n\\]\nThus, the sufficient statistics are: \\[\nT(\\mathbf{X}) = \\left( \\prod_{i=1}^n X_i, \\sum_{i=1}^n X_i \\right)\n\\]\n\n\nExample 4.3 (Sufficient Statistic of Exponential Family) Many common distributions (Normal, Poisson, Gamma, Binomial) belong to the Exponential Family, which has a density in the form:\n\\[\nf(\\mathbf{x}|\\theta) = h(\\mathbf{x}) c(\\theta) \\exp\\left( \\sum_{j=1}^k \\pi_j(\\theta) t_j(\\mathbf{x}) \\right)\n\\]\nThen, by the Factorization Theorem, the statistic:\n\\[\nT(\\mathbf{X}) = \\left( \\sum_{i=1}^n t_1(x_i), \\dots, \\sum_{i=1}^n t_k(x_i) \\right)\n\\]\nis a sufficient statistic for \\(\\theta\\).\n\n\nExample 4.4 (Bernoulli as Exponential Family) Let \\(X_1, \\dots, X_n \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)\\). To find the sufficient statistic, we write the Joint PDF of the sample in the canonical Exponential Family form:\n\\[\nf(\\mathbf{x}|\\theta) = h(\\mathbf{x}) c(\\theta) \\exp\\left( \\sum_{j=1}^k \\pi_j(\\theta) T_j(\\mathbf{x}) \\right)\n\\]\n\nWrite the Joint PDF\n\\[\nf(\\mathbf{x}|p) = \\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}\n\\]\nConvert to Exponential Form\n\\[\n\\begin{aligned}\nf(\\mathbf{x}|p) &= \\exp\\left( \\sum_{i=1}^n \\left[ x_i \\ln p + (1-x_i) \\ln(1-p) \\right] \\right) \\\\\n&= \\exp\\left( \\sum_{i=1}^n \\left[ x_i \\ln p + \\ln(1-p) - x_i \\ln(1-p) \\right] \\right) \\\\\n&= \\exp\\left( \\sum_{i=1}^n \\ln(1-p) + \\sum_{i=1}^n x_i \\left[ \\ln p - \\ln(1-p) \\right] \\right)\n\\end{aligned}\n\\]\nFactor into Components We separate the terms to match the definition:\n\\[\nf(\\mathbf{x}|p) = \\underbrace{1}_{h(\\mathbf{x})} \\cdot \\underbrace{(1-p)^n}_{c(p)} \\cdot \\exp\\left( \\underbrace{\\ln\\left(\\frac{p}{1-p}\\right)}_{\\pi_1(p)} \\underbrace{\\sum_{i=1}^n x_i}_{T_1(\\mathbf{x})} \\right)\n\\]\n\nConclusion: By inspection of the exponent, the statistic coupled with the parameter \\(\\pi_1(p)\\) is the sufficient statistic:\n\\[\nT(\\mathbf{X}) = \\sum_{i=1}^n X_i\n\\]\n\n\nRemark 4.1 (Sufficient Statistic is the sufficient “Parameter” of Likelihood). There is a dual relationship between the sufficient statistic and the parameter \\(\\theta\\). Conventionally, we view \\(f(x|\\theta)\\) as a function of \\(x\\) parameterized by \\(\\theta\\).\nHowever, in Bayesian inference or likelihood theory, we often view the likelihood \\(L(\\theta; x)\\) as a function of \\(\\theta\\) determined by the observed data \\(x\\). The Factorization Theorem implies:\n\\[\nL(\\theta; \\mathbf{x}) \\propto g(T(\\mathbf{x})|\\theta)\n\\]\nThis suggests that \\(T(\\mathbf{x})\\) completely determines the shape of the likelihood function. In this specific sense, the sufficient statistic \\(T(\\mathbf{x})\\) acts as the “parameter” of the likelihood function itself.\nFor the exponential family that we will discuss below, this duality is explicit:\n\\[\n\\log L(\\theta; \\mathbf{x}) = \\text{const} + \\sum_{i=1}^k \\eta_i(\\theta) T_i(\\mathbf{x}) - n A(\\theta)\n\\]\nHere, \\(T_i(\\mathbf{x})\\) serves as the coefficient (or parameter) for the function \\(\\eta_i(\\theta)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sufficient Statistic</span>"
    ]
  },
  {
    "objectID": "sufficiency.html#minimal-sufficient-statistics",
    "href": "sufficiency.html#minimal-sufficient-statistics",
    "title": "4  Sufficient Statistic",
    "section": "4.2 Minimal Sufficient Statistics",
    "text": "4.2 Minimal Sufficient Statistics\n\nDefinition 4.2 (Minimal Sufficient Statistic (MSS)) A statistic \\(T(X)\\) is a Minimal Sufficient Statistic if:\n\nSufficiency: \\(T(X)\\) is a sufficient statistic for \\(\\theta\\).\nMinimality: For any other sufficient statistic \\(S(X)\\), \\(T(X)\\) is a function of \\(S(X)\\). \\[\n    T(X) = g(S(X))\n    \\] (This implies that \\(T(X)\\) provides the greatest possible data reduction without losing information about \\(\\theta\\). If \\(S(x) = S(y)\\), then it must be that \\(T(x) = T(y)\\)).\n\n\n\nTheorem 4.2 (MSS Condition Theorem) Let \\(T(X)\\) be a sufficient statistic. \\(T(X)\\) is a Minimal Sufficient Statistic (MSS) if and only if for any pair of data sets \\(x\\) and \\(y\\): \\[\n\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y) \\text{ for all } \\theta \\implies T(x) = T(y)\n\\] where \\(c(x, y)\\) is a constant independent of \\(\\theta\\).\n\n\n\nClick to view the Proof\n\n\nProof. Direction 1: Sufficiency (Implication holds \\(\\implies\\) \\(T\\) is MSS)\nAssume that for any \\(x, y\\), \\([\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)] \\implies T(x) = T(y)\\). We must show that \\(T\\) is a function of any sufficient statistic \\(U\\).\n\nLet \\(U(X)\\) be any sufficient statistic. Assume \\(U(x) = U(y)\\).\nBy the Factorization Theorem, the likelihoods are: \\[\n    L(\\theta; x) = h(x)g(U(x), \\theta)\n    \\] \\[\n    L(\\theta; y) = h(y)g(U(y), \\theta)\n    \\]\nSince \\(U(x) = U(y)\\), the factor \\(g(U(x), \\theta)\\) is identical to \\(g(U(y), \\theta)\\). Taking the log-ratio: \\[\n\\ell(\\theta; x) - \\ell(\\theta; y) = \\ln h(x) - \\ln h(y)\n\\] The term \\(\\ln h(x) - \\ln h(y)\\) depends only on \\(x\\) and \\(y\\), not on \\(\\theta\\). Let this be \\(c(x,y)\\). \\[\n\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\n\\]\nBy our main assumption, this condition implies \\(T(x) = T(y)\\).\nThus, we have shown that \\(U(x) = U(y) \\implies T(x) = T(y)\\). This means \\(T\\) is a function of \\(U\\). Since \\(U\\) is arbitrary, \\(T\\) is Minimal Sufficient.\n\nDirection 2: Necessity (\\(T\\) is MSS \\(\\implies\\) Implication holds)\nAssume \\(T\\) is Minimal Sufficient. We must prove that if \\(\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\\) for all \\(\\theta\\), then \\(T(x) = T(y)\\).\n\nDefine the Statistic \\(S(x)\\): Let \\(S(x)\\) be the set of all possible datasets \\(z\\) which give the same log-likelihood shape as \\(x\\): \\[\nS(x) = \\{z \\mid \\ell(\\theta; z) = \\ell(\\theta; x) + c_z \\text{ for all } \\theta \\}\n\\] This statistic \\(S(x)\\) represents the equivalence class of \\(x\\) under the parallel log-likelihood relationship. If the condition \\(\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\\) holds, then by definition \\(x\\) and \\(y\\) generate the same equivalence class, so \\(S(x) = S(y)\\).\nShow \\(S(x)\\) is Sufficient (Directly via Likelihood Ratio): To prove \\(S\\) is sufficient, we check the Likelihood Ratio Condition (Condition 2 from Section 1.1). Suppose \\(S(x) = S(y)\\). By the definition of \\(S\\), this implies: \\[\n\\ell(\\theta; x) - \\ell(\\theta; y) = c(x, y)\n\\] By the definition of sufficiency, \\(S(X)\\) is a sufficient statistic.\nUse Minimality of \\(T\\): Since \\(T\\) is a Minimal Sufficient Statistic, it is a function of any sufficient statistic. Therefore, \\(T\\) must be a function of \\(S\\). That is, \\(T(x) = f(S(x))\\).\nConclusion: Assume \\(\\ell(\\theta; x) = \\ell(\\theta; y) + c(x, y)\\). Then \\(S(x) = S(y)\\). Consequently, \\(T(x) = f(S(x)) = f(S(y)) = T(y)\\).\n\n\n\n\nExample 4.5 (Checking Minimality via Log-Likelihood Condition) Let \\(X_1, X_2, X_3 \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)\\). We determine the MSS by checking the implication from the MSS Condition Theorem: \\[\n\\text{Parallel Log-Likelihoods} \\implies T(x) = T(y)\n\\]\nStep 1: Establishing the MSS\nFirst, we find the condition under which two log-likelihoods are parallel. \\[\n\\ell(p; x) = (\\sum x_i) \\ln p + (n - \\sum x_i) \\ln(1-p)\n\\] The difference \\(\\ell(p; x) - \\ell(p; y)\\) depends on \\(p\\) only through the term \\((\\sum x_i - \\sum y_i) \\ln \\frac{p}{1-p}\\). For this difference to be constant (independent of \\(p\\)), the coefficient must be zero: \\[\n\\text{Parallel Log-Likelihoods} \\iff \\sum x_i = \\sum y_i\n\\] The statistic that corresponds exactly to this condition is \\(T(X) = \\sum X_i\\). Since \\(\\sum x_i = \\sum y_i\\) trivially implies \\(T(x) = T(y)\\), \\(T(X)\\) is the Minimal Sufficient Statistic.\nStep 2: Why \\(S(X) = (X_1, \\sum_{i=2}^3 X_i)\\) is NOT Minimal\nNow consider the “richer” statistic \\(S(X)\\). If \\(S\\) were minimal, the parallel condition must imply \\(S(x) = S(y)\\). We check: \\[\n\\sum x_i = \\sum y_i \\stackrel{?}{\\implies} (x_1, \\sum_{i=2}^3 x_i) = (y_1, \\sum_{i=2}^3 y_i)\n\\] Counter-Example:\nLet \\(x = (1, 0, 1)\\) and \\(y = (0, 1, 1)\\).\n\nCheck Parallel Condition:\n\\(\\sum x_i = 2\\) and \\(\\sum y_i = 2\\). The sums are equal, so the log-likelihoods are parallel.\nCheck Statistic Equality:\n\\[S(x) = (1, 1)\\] \\[S(y) = (0, 2)\\] \\[S(x) \\neq S(y)\\]\n\nConclusion: The parallel condition holds, but \\(S(x) \\neq S(y)\\). The implication fails. This proves that \\(S(X)\\) is not minimal—it retains “extra” information (the position of the first success) that is not relevant to the likelihood shape.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sufficient Statistic</span>"
    ]
  },
  {
    "objectID": "expofam.html",
    "href": "expofam.html",
    "title": "5  Exponential Families",
    "section": "",
    "text": "5.1 Exponential Families",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "expofam.html#sufficient-statistics",
    "href": "expofam.html#sufficient-statistics",
    "title": "5  Exponential Families",
    "section": "",
    "text": "Definition 5.1 (Sufficient Statistic (Likelihood Principle)) A statistic \\(T(\\mathbf{X})\\) is sufficient for \\(\\theta\\) if the likelihood function \\(L(\\theta; \\mathbf{x})\\) depends on the sample data \\(\\mathbf{x}\\) only through the value of the statistic \\(T(\\mathbf{x})\\).\nFormally, this means that for any two sample points \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) such that \\(T(\\mathbf{x}) = T(\\mathbf{y})\\), the ratio of their likelihoods is constant with respect to \\(\\theta\\):\n\\[\n\\frac{L(\\theta; \\mathbf{x})}{L(\\theta; \\mathbf{y})} = k(\\mathbf{x}, \\mathbf{y}) \\quad (\\text{independent of } \\theta)\n\\]\nThis implies that \\(L(\\theta; \\mathbf{x}) \\propto L(\\theta; T(\\mathbf{x}))\\).\n\n\nTheorem 5.1 (Factorization Theorem) Let \\(f(\\mathbf{x}|\\theta)\\) denote the joint probability density function (or probability mass function) of a sample \\(\\mathbf{X}\\). A statistic \\(T(\\mathbf{X})\\) is sufficient for \\(\\theta\\) if and only if the density can be factored as:\n\\[\nf(\\mathbf{x}|\\theta) = g(T(\\mathbf{x})|\\theta) h(\\mathbf{x})\n\\]\nwhere:\n\n\\(g(T(\\mathbf{x})|\\theta)\\) depends on the data \\(\\mathbf{x}\\) only through the statistic \\(T(\\mathbf{x})\\) and the parameter \\(\\theta\\).\n\\(h(\\mathbf{x})\\) does not depend on \\(\\theta\\).\n\n\n\nExample 5.1 (Sufficient Statistics for Normal Distribution) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). The joint density is given by:\n\\[\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n\\]\nExpanding the exponent:\n\\[\nf(\\mathbf{x}|\\theta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^n x_i^2 - 2\\mu \\sum_{i=1}^n x_i + n\\mu^2 \\right) \\right\\}\n\\]\nWe can rearrange this into the factorization form \\(g(T(\\mathbf{x})|\\theta) h(\\mathbf{x})\\):\n\\[\nf(\\mathbf{x}|\\theta) = \\underbrace{ (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\} }_{g(T(\\mathbf{x})|\\theta)} \\cdot \\underbrace{ 1 }_{h(\\mathbf{x})}\n\\]\nHere, the function \\(g\\) depends on the data only through the pair of statistics:\n\\[\nT(\\mathbf{x}) = \\left( \\sum_{i=1}^n X_i, \\sum_{i=1}^n X_i^2 \\right)\n\\]\nThus, \\(T(\\mathbf{X}) = (\\sum X_i, \\sum X_i^2)\\) is sufficient for \\(\\theta = (\\mu, \\sigma^2)\\).\nInvariance Property: It is a general property that any one-to-one (invertible) function of a sufficient statistic is also a sufficient statistic. We can define the sample mean and sample variance as:\n\\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad S^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^n X_i^2 - \\frac{1}{n} \\left(\\sum_{i=1}^n X_i\\right)^2 \\right)\n\\]\nSince the mapping from \\((\\sum X_i, \\sum X_i^2)\\) to \\((\\bar{X}, S^2)\\) is invertible (provided \\(n \\ge 2\\)), the statistic \\(T^*(\\mathbf{X}) = (\\bar{X}, S^2)\\) is also sufficient for \\(\\theta\\).\n\n\nRemark 5.1 (Sufficient Statistic as Parameter of Likelihood). There is a dual relationship between the sufficient statistic and the parameter \\(\\theta\\). Conventionally, we view \\(f(x|\\theta)\\) as a function of \\(x\\) parameterized by \\(\\theta\\).\nHowever, in Bayesian inference or likelihood theory, we often view the likelihood \\(L(\\theta; x)\\) as a function of \\(\\theta\\) determined by the observed data \\(x\\). The Factorization Theorem implies:\n\\[\nL(\\theta; \\mathbf{x}) \\propto g(T(\\mathbf{x})|\\theta)\n\\]\nThis suggests that \\(T(\\mathbf{x})\\) completely determines the shape of the likelihood function. In this specific sense, the sufficient statistic \\(T(\\mathbf{x})\\) acts as the “parameter” of the likelihood function itself.\nFor the exponential family that we will discuss below, this duality is explicit:\n\\[\n\\log L(\\theta; \\mathbf{x}) = \\text{const} + \\sum_{i=1}^k \\eta_i(\\theta) T_i(\\mathbf{x}) - n A(\\theta)\n\\]\nHere, \\(T_i(\\mathbf{x})\\) serves as the coefficient (or parameter) for the function \\(\\eta_i(\\theta)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "expofam.html#exponential-families",
    "href": "expofam.html#exponential-families",
    "title": "5  Exponential Families",
    "section": "",
    "text": "Definition 5.1 (Exponential Family) A family of probability density functions (or probability mass functions) \\(f(x|\\theta)\\) is said to be an Exponential Family if it can be written in the form:\n\\[\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n\\]\nwhere:\n\n\\(\\theta = (\\theta_1, \\dots, \\theta_d)\\) is the parameter vector.\n\\(k\\) is the number of terms in the exponent. Note that \\(d\\) may be less than \\(k\\).\nBy the Factorization Theorem, the vector \\(T(x) = (\\tau_1(x), \\dots, \\tau_k(x))\\) constitutes a sufficient statistic for \\(\\theta\\).\n\n\n\n5.1.1 Examples of Exponential Families\n\nExample 5.1 (Exponential Distribution) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)\\), where \\(\\theta\\) is the scale parameter. \\[\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{-x_i/\\theta} = \\theta^{-n} \\exp\\left\\{ -\\frac{1}{\\theta} \\sum_{i=1}^n x_i \\right\\}\n\\]\nHere we identify:\n\n\\(C(\\theta) = \\theta^{-n}\\)\n\\(h(\\mathbf{x}) = 1\\)\n\\(\\pi_1(\\theta) = -\\frac{1}{\\theta}\\)\n\\(\\tau_1(\\mathbf{x}) = \\sum_{i=1}^n x_i\\).\n\n\n\nExample 5.2 (Gamma Distribution) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Gamma}(\\alpha, \\beta)\\). \\[\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x_i^{\\alpha-1} e^{-x_i/\\beta}\n\\]\n\\[\n= [\\Gamma(\\alpha)\\beta^\\alpha]^{-n} \\left( \\prod_{i=1}^n x_i \\right)^{\\alpha-1} \\exp\\left\\{ -\\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n\\]\nRewriting in the canonical form:\n\\[\n= [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n\\]\nHere:\n\n\\(\\pi_1(\\theta) = \\alpha - 1\\), \\(\\tau_1(\\mathbf{x}) = \\sum \\log x_i\\)\n\\(\\pi_2(\\theta) = -\\frac{1}{\\beta}\\), \\(\\tau_2(\\mathbf{x}) = \\sum x_i\\).\n\n\n\nExample 5.3 (Beta Distribution) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Beta}(a, b)\\) with \\(\\theta = (a, b)\\). \\[\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{B(a, b)} x_i^{a-1} (1-x_i)^{b-1}\n\\]\n\\[\n= [B(a, b)]^{-n} \\exp\\left\\{ (a-1) \\sum_{i=1}^n \\log x_i + (b-1) \\sum_{i=1}^n \\log(1-x_i) \\right\\}\n\\]\nThis is an exponential family with \\(k=2\\).\n\n\nExample 5.4 (Normal Distribution) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\) with \\(\\theta = (\\mu, \\sigma^2)\\). \\[\nf(\\mathbf{x}|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right\\}\n\\]\n\\[\n= (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i - \\frac{n\\mu^2}{2\\sigma^2} \\right\\}\n\\]\n\\[\n= \\left[ (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} e^{-\\frac{n\\mu^2}{2\\sigma^2}} \\right] \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n\\]\nHere \\(d=2\\) and \\(k=2\\).\n\n\n\n5.1.2 Examples of Non-exponential Families\nA model is not in the exponential family if the support depends on the parameter.\n\nExample 5.5 (Uniform Distribution) Let \\(X \\sim U(0, \\theta)\\). \\[\nf(x|\\theta) = \\frac{1}{\\theta} I(0 &lt; x &lt; \\theta)\n\\]\nThis cannot be written in the required form because the indicator function \\(I(0 &lt; x &lt; \\theta)\\) cannot be factorized into separate functions of \\(x\\) and \\(\\theta\\) inside an exponential.\n\n\nExample 5.6 (Cauchy Distribution) Let \\(X \\sim \\text{Cauchy}(\\theta)\\). \\[\nf(x|\\theta) = \\frac{1}{\\pi [1 + (x-\\theta)^2]}\n\\]\nThis involves \\(\\log(1 + (x-\\theta)^2)\\) in the exponent, which cannot be separated into sums of products \\(\\pi_i(\\theta)\\tau_i(x)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "expofam.html#regular-families",
    "href": "expofam.html#regular-families",
    "title": "5  Exponential Families",
    "section": "5.2 Regular Families",
    "text": "5.2 Regular Families\nIn the context of exponential families and maximum likelihood estimation, we often require the statistical model to satisfy specific regularity conditions to ensure that standard asymptotic results hold.\n\nDefinition 5.2 (Regular Family) A family of probability density functions \\(f(x|\\theta)\\) is said to be a Regular Family (or “sufficiently well-behaved”) if the domain of \\(x\\) for which \\(f(x|\\theta) &gt; 0\\) (the support) does not depend on the parameter \\(\\theta\\).\nThis condition is necessary to satisfy the identity that allows differentiation under the integral sign:\n\\[\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx\n\\]\n\n\nWhy Regularity Matters\nIf a family is regular, we can differentiate the identity \\(\\int f(x|\\theta) dx = 1\\) with respect to \\(\\theta\\). This operation yields the fundamental properties of the Score Function \\(\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)\\):\n\nFirst Moment Identity: The expected value of the score function is zero.\n\n\\[E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = 0\\]\n\nSecond Moment Identity: The Fisher Information (variance of the score) is equal to the negative expected Hessian.\n\n\\[E_\\theta \\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_j \\partial \\theta_l} \\right] = -E_\\theta \\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_l} \\right]\\]\n\n\nAn Example of Non-regular Distribution\nThe Uniform Distribution \\(U(0, \\theta)\\) is a classic example of a non-regular family. Because the support \\((0, \\theta)\\) depends on \\(\\theta\\), the integral limits change with the parameter, preventing the direct interchange of differentiation and integration . Consequently, the standard identities for the score function do not hold for this distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "expofam.html#moments-of-sufficient-statistics-of-exponential-families",
    "href": "expofam.html#moments-of-sufficient-statistics-of-exponential-families",
    "title": "5  Exponential Families",
    "section": "5.3 Moments of Sufficient Statistics of Exponential Families",
    "text": "5.3 Moments of Sufficient Statistics of Exponential Families\n\n5.3.1 Means of Sufficient Statistics (General Case)\n\nTheorem 5.1 (Means of Sufficient Statistics (General Case)) For a random variable \\(X\\) belonging to an exponential family with density \\(f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\\}\\), the moments of the sufficient statistics \\(\\tau_i(X)\\) satisfy the system of equations: \\[\n\\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} E[\\tau_i(X)] = 0 \\quad \\text{for } j=1, \\dots, d\n\\]\n\n\nProof. For regular families, we can interchange differentiation and integration. Since \\(\\int f(x|\\theta) dx = 1\\), we have: \\[\n\\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx = 0 \\implies \\int \\frac{\\partial}{\\partial \\theta} f(x|\\theta) dx = 0\n\\]\nUsing the identity \\(\\frac{\\partial f}{\\partial \\theta} = f(x|\\theta) \\frac{\\partial \\log f}{\\partial \\theta}\\), we derive the fundamental moment property:\n\\[\nE\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right] = \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} f(x|\\theta) dx = 0\n\\]\nFor the exponential family, the log-likelihood is given by:\n\\[\n\\log f(x|\\theta) = \\log C(\\theta) + \\log h(x) + \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x)\n\\]\nTaking the derivative with respect to \\(\\theta_j\\):\n\\[\n\\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta_j} = \\frac{1}{C(\\theta)} \\frac{\\partial C(\\theta)}{\\partial \\theta_j} + \\sum_{i=1}^k \\frac{\\partial \\pi_i(\\theta)}{\\partial \\theta_j} \\tau_i(x)\n\\]\nTaking expectations and applying the condition \\(E[\\frac{\\partial}{\\partial \\theta} \\log f(X|\\theta)] = 0\\) yields the theorem statement.\n\n\nExample 5.7 (Moments of Normal Sufficient Statistics) Consider the Normal distribution \\(N(\\mu, \\sigma^2)\\) where \\(\\theta = (\\mu, \\sigma^2)\\). The density is: \\[\nf(\\mathbf{x}|\\theta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^n x_i \\right\\}\n\\]\nHere we identify the components :\n\n\\(C(\\mu, \\sigma^2) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)\\)\n\\(\\pi_1 = -\\frac{1}{2\\sigma^2}, \\quad \\tau_1(\\mathbf{x}) = \\sum x_i^2\\)\n\\(\\pi_2 = \\frac{\\mu}{\\sigma^2}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i\\)\n\nWe apply the theorem by differentiating with respect to \\(\\mu\\) and \\(\\sigma^2\\).\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n   \\frac{\\partial \\log C}{\\partial \\mu} = -\\frac{n\\mu}{\\sigma^2}\n\\]\n\\[\n   \\frac{\\partial \\pi_1}{\\partial \\mu} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\mu} = \\frac{1}{\\sigma^2}\n\\]\nThe theorem equation becomes:\n\\[\n   -\\frac{n\\mu}{\\sigma^2} + 0 \\cdot E[\\sum X_i^2] + \\frac{1}{\\sigma^2} E[\\sum X_i] = 0\n\\]\n\\[\n   \\implies E[\\sum_{i=1}^n X_i] = n\\mu\n\\]\nDifferentiate with respect to \\(\\sigma^2\\):\n\\[\n   \\frac{\\partial \\log C}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2}\n\\]\n\\[\n   \\frac{\\partial \\pi_1}{\\partial \\sigma^2} = \\frac{1}{2(\\sigma^2)^2}, \\quad \\frac{\\partial \\pi_2}{\\partial \\sigma^2} = -\\frac{\\mu}{(\\sigma^2)^2}\n\\]\nThe theorem equation becomes:\n\\[\n   \\left( -\\frac{n}{2\\sigma^2} + \\frac{n\\mu^2}{2(\\sigma^2)^2} \\right) + \\frac{1}{2(\\sigma^2)^2} E[\\sum X_i^2] - \\frac{\\mu}{(\\sigma^2)^2} E[\\sum X_i] = 0\n\\]\nMultiplying by \\(2(\\sigma^2)^2\\):\n\\[\n   -n\\sigma^2 + n\\mu^2 + E[\\sum X_i^2] - 2\\mu(n\\mu) = 0\n\\]\n\\[\n   E[\\sum X_i^2] = n\\sigma^2 + n\\mu^2\n\\]\nThis recovers the standard second moment \\(E[X^2] = \\sigma^2 + \\mu^2\\).\n\n\n\nExample 5.8 (Moments of Gamma Sufficient Statistics) Consider the Gamma distribution \\(\\text{Gamma}(\\alpha, \\beta)\\) with \\(\\theta = (\\alpha, \\beta)\\). The density is : \\[\nf(\\mathbf{x}|\\theta) = [\\Gamma(\\alpha)]^{-n} \\beta^{-n\\alpha} \\exp\\left\\{ (\\alpha-1) \\sum_{i=1}^n \\log x_i - \\frac{1}{\\beta} \\sum_{i=1}^n x_i \\right\\}\n\\]\nHere we identify:\n\n\\(\\log C(\\alpha, \\beta) = -n \\log \\Gamma(\\alpha) - n\\alpha \\log \\beta\\)\n\\(\\pi_1 = \\alpha - 1, \\quad \\tau_1(\\mathbf{x}) = \\sum \\log x_i\\)\n\\(\\pi_2 = -\\frac{1}{\\beta}, \\quad \\tau_2(\\mathbf{x}) = \\sum x_i\\)\n\n\nDifferentiate with respect to \\(\\beta\\):\n\\[\n   \\frac{\\partial \\log C}{\\partial \\beta} = -\\frac{n\\alpha}{\\beta}\n\\]\n\\[\n   \\frac{\\partial \\pi_1}{\\partial \\beta} = 0, \\quad \\frac{\\partial \\pi_2}{\\partial \\beta} = \\frac{1}{\\beta^2}\n\\]\nThe theorem yields:\n\\[\n   -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2} E[\\sum X_i] = 0 \\implies E[\\sum X_i] = n\\alpha\\beta\n\\]\nDifferentiate with respect to \\(\\alpha\\):\n\\[\n   \\frac{\\partial \\log C}{\\partial \\alpha} = -n \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} - n \\log \\beta = -n \\psi(\\alpha) - n \\log \\beta\n\\]\nwhere \\(\\psi(\\alpha)\\) is the digamma function.\n\\[\n   \\frac{\\partial \\pi_1}{\\partial \\alpha} = 1, \\quad \\frac{\\partial \\pi_2}{\\partial \\alpha} = 0\n\\]\nThe theorem yields:\n\\[\n   (-n \\psi(\\alpha) - n \\log \\beta) + 1 \\cdot E[\\sum \\log X_i] = 0\n\\]\n\\[\n\\implies E[\\sum_{i=1}^n \\log X_i] = n(\\psi(\\alpha) + \\log \\beta)\n\\]\n\n\n\n\n5.3.2 Natural Parameterization\n\nDefinition 5.3 (Natural Parameterization) Suppose an exponential family is given by: \\[\nf(x|\\theta) = C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) \\tau_i(x) \\right\\}\n\\]\nWe define the natural parameters \\(\\eta_i\\) as \\(\\eta_i = \\pi_i(\\theta)\\). Let \\(\\eta = (\\eta_1, \\dots, \\eta_k)\\). The density becomes:\n\\[\nf(x|\\eta) = C^*(\\eta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\eta_i \\tau_i(x) \\right\\}\n\\]\n\n\nDefinition 5.4 (Natural Parameter Space) The natural parameter space \\(\\mathcal{H}\\) is defined as: \\[\n\\mathcal{H} = \\{ \\eta = (\\pi_1(\\theta), \\dots, \\pi_k(\\theta)) : \\int h(x) e^{\\sum \\eta_i \\tau_i(x)} dx &lt; \\infty \\}\n\\]\nwhere the condition ensures \\(C(\\theta)\\) is finite.\n\n\nDefinition 5.5 (Full vs. Curved Exponential Families) Let \\(d\\) be the dimension of \\(\\theta\\) and \\(k\\) be the dimension of the sufficient statistic vector \\(\\tau(x)\\).\n\nIf \\(d = k\\), we say \\(f(x|\\theta)\\) is a Full Exponential Family.\nIf \\(d &lt; k\\), we say \\(f(x|\\theta)\\) is a Curved Exponential Family.\n\n\n\nExample 5.9 (Natural Parameterization of Normal Distribution) Consider the full Normal family \\(N(\\mu, \\sigma^2)\\) where both parameters are unknown (\\(d=2\\)). We previously identified the sufficient statistics \\(T_1(x) = \\sum x_i\\) and \\(T_2(x) = \\sum x_i^2\\) (\\(k=2\\)). Since \\(d=k\\), this is a Full Exponential Family.\nThe natural parameters \\(\\eta = (\\eta_1, \\eta_2)\\) are defined by the mapping:\n\\[\n\\eta_1 = \\frac{\\mu}{\\sigma^2}, \\quad \\eta_2 = -\\frac{1}{2\\sigma^2}\n\\]\nThe density can be rewritten purely in terms of \\(\\eta\\):\n\\[\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i + \\eta_2 \\sum_{i=1}^n x_i^2 - A(\\eta) \\right\\}\n\\] where the log-partition function \\(A(\\eta)\\) absorbs the normalizing constants.\n\n\nExample 5.10 (Natural Parameterization of Gamma Distribution) Consider the Gamma family \\(\\text{Gamma}(\\alpha, \\beta)\\) (\\(d=2\\)). We identified the sufficient statistics \\(T_1(x) = \\sum \\log x_i\\) and \\(T_2(x) = \\sum x_i\\) (\\(k=2\\)). Since \\(d=k\\), this is also a Full Exponential Family.\nThe natural parameters \\(\\eta = (\\eta_1, \\eta_2)\\) are derived from the canonical form :\n\\[\n\\eta_1 = \\alpha - 1, \\quad \\eta_2 = -\\frac{1}{\\beta}\n\\]\nThe density in natural parameterization is:\n\\[\nf(\\mathbf{x}|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n \\log x_i + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n\\]\n\n\nExample 5.11 (Curved Exponential Family (Natural Parameterization)) Consider the \\(N(\\theta, \\theta^2)\\) distribution (\\(d=1\\)). The density is: \\[\nf(x|\\theta) \\propto \\exp\\left\\{ -\\frac{1}{2\\theta^2} \\sum x_i^2 + \\frac{1}{\\theta} \\sum x_i \\right\\}\n\\]\nTo express this in the natural parameterization, we define \\(\\eta = (\\eta_1, \\eta_2)\\) as:\n\\[\n\\eta_1 = -\\frac{1}{2\\theta^2}, \\quad \\eta_2 = \\frac{1}{\\theta}\n\\]\nThe density becomes:\n\\[\nf(x|\\eta) \\propto \\exp\\left\\{ \\eta_1 \\sum_{i=1}^n x_i^2 + \\eta_2 \\sum_{i=1}^n x_i - A(\\eta) \\right\\}\n\\]\nHowever, the natural parameters \\(\\eta_1\\) and \\(\\eta_2\\) are not independent. They satisfy the constraint:\n\\[\n\\eta_1 = -\\frac{1}{2} \\eta_2^2\n\\]\nBecause the parameter space \\(\\mathcal{H}\\) forms a 1-dimensional non-linear curve (a parabola) within the 2-dimensional space of natural parameters, this is a Curved Exponential Family.\n\n\n\n5.3.3 Mean and Covariance of Natural Exponential Families\n\nTheorem 5.2 (Mean and Covariance of Natural Exponential Families) If the exponential family is in its canonical form (natural parameterization) where \\(\\pi_i(\\theta) = \\theta_i\\) for each \\(i\\), then the mean and covariance of the sufficient statistics are given by: \\[\nE_\\theta[\\tau_i(X)] = - \\frac{\\partial}{\\partial \\theta_i} \\log C(\\theta)\n\\]\n\\[\n\\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X)) = - \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log C(\\theta)\n\\]\n\n\nProof. We use the second-order regularity condition for the score function: \\[\nE_\\theta\\left[ \\frac{\\partial^2 \\log f(X|\\theta)}{\\partial \\theta_i \\partial \\theta_j} \\right] = - E_\\theta\\left[ \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_i} \\frac{\\partial \\log f(X|\\theta)}{\\partial \\theta_j} \\right]\n\\]\nIn the case of a natural exponential family, \\(\\pi_k(\\theta) = \\theta_k\\). The derivative of the log-likelihood simplifies to:\n\\[\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\frac{\\partial \\log C(\\theta)}{\\partial \\theta_i} + \\tau_i(x)\n\\]\nDifferentiating again with respect to \\(\\theta_j\\):\n\\[\n\\frac{\\partial^2 \\log f}{\\partial \\theta_i \\partial \\theta_j} = \\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\n\\]\nSince this second derivative is non-random (it does not depend on \\(x\\)), its expectation is simply itself.\nNow consider the right-hand side of the regularity condition. From the first moment property, we know \\(E[\\tau_i(X)] = - \\frac{\\partial \\log C}{\\partial \\theta_i}\\). Thus, the score function is:\n\\[\n\\frac{\\partial \\log f}{\\partial \\theta_i} = \\tau_i(X) - E[\\tau_i(X)]\n\\]\nSubstituting these into the identity:\n\\[\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - E\\left[ (\\tau_i(X) - E[\\tau_i(X)]) (\\tau_j(X) - E[\\tau_j(X)]) \\right]\n\\]\n\\[\n\\frac{\\partial^2 \\log C(\\theta)}{\\partial \\theta_i \\partial \\theta_j} = - \\text{Cov}_\\theta(\\tau_i(X), \\tau_j(X))\n\\]\nMultiplying by \\(-1\\) gives the result.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "expofam.html#distributions-of-sufficient-statistics",
    "href": "expofam.html#distributions-of-sufficient-statistics",
    "title": "5  Exponential Families",
    "section": "5.4 Distributions of Sufficient Statistics",
    "text": "5.4 Distributions of Sufficient Statistics\n\nLemma 5.1 (Joint Distribution of Sufficient Statistics) If \\(X\\) has a distribution in the exponential family \\(f(x|\\theta) = C(\\theta) h(x) \\exp\\{\\sum \\pi_i(\\theta) \\tau_i(x)\\}\\), then the joint distribution of the sufficient statistics \\(T = (\\tau_1(X), \\dots, \\tau_k(X))\\) is also in the exponential family with the same natural parameters.\n\n\nProof. Let \\(X\\) be discrete. The probability mass function of \\(T\\) is: \\[\nP(T_1 = y_1, \\dots, T_k = y_k | \\theta) = \\sum_{\\{x : \\tau(x) = y\\}} P(X=x|\\theta)\n\\]\n\\[\n= \\sum_{\\{x : \\tau(x) = y\\}} C(\\theta) h(x) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n\\]\nSince the exponential term and \\(C(\\theta)\\) depend only on \\(y\\) and \\(\\theta\\), they can be pulled out of the sum:\n\\[\n= C(\\theta) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\} \\left( \\sum_{\\{x : \\tau(x) = y\\}} h(x) \\right)\n\\]\nDefining \\(h^*(y) = \\sum_{\\{x : \\tau(x) = y\\}} h(x)\\), we get:\n\\[\nf_T(y|\\theta) = C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{i=1}^k \\pi_i(\\theta) y_i \\right\\}\n\\]\nwhich is of the exponential family form .\n\n\nLemma 5.2 (Marginal Distribution Lemma for Exponetial Families (MDL)) Let \\(S\\) be a subset of indices \\(\\{1, \\dots, k\\}\\). If \\(\\pi_i(\\theta)\\) are constant for all \\(i \\notin S\\), then the marginal distribution of the statistics \\(T_S = \\{ \\tau_j(X) : j \\in S \\}\\) is of the exponential family form with natural parameters \\(\\pi_j(\\theta)\\) for \\(j \\in S\\).\n\n\nProof. Let \\(T\\) be discrete. We sum over the variables not in \\(S\\) (denoted \\(T_{S^c}\\)): \\[\nP(T_S = y_S | \\theta) = \\sum_{y_{S^c}} C(\\theta) h^*(y) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j + \\sum_{l \\in S^c} \\pi_l(\\theta) y_l \\right\\}\n\\]\nSince \\(\\pi_l(\\theta)\\) is constant for \\(l \\in S^c\\), the term \\(\\exp\\{\\sum_{l \\in S^c} \\pi_l y_l\\}\\) does not depend on \\(\\theta\\). We can group it with \\(h^*(y)\\):\n\\[\n= C(\\theta) \\exp\\left\\{ \\sum_{j \\in S} \\pi_j(\\theta) y_j \\right\\} \\sum_{y_{S^c}} h^*(y) \\exp\\left\\{ \\sum_{l \\in S^c} \\pi_l y_l \\right\\}\n\\]\nThe sum becomes a new base measure \\(h^{**}(y_S)\\), yielding the exponential family form.\n\n\nExample 5.12 (Failure of MDL: The Coupled Bernoulli) Suppose we try to be clever and rewrite the Bernoulli joint density treating Heads (\\(T_1\\)) and Tails (\\(T_2\\)) as distinct sufficient statistics:\n\\[\nf(x|p) = \\exp\\left\\{ (\\log p) \\sum x_i + (\\log(1-p)) (n - \\sum x_i) \\right\\}\n\\]\nLet’s define separate natural parameters to make it look like a 2-parameter exponential family:\n\n\\(\\eta_1 = \\log p\\)\n\\(\\eta_2 = \\log (1-p)\\)\n\\(T_1 = \\sum x_i\\) (Heads)\n\\(T_2 = n - \\sum x_i\\) (Tails)\n\nThe density looks like: \\[\nf(t_1, t_2) \\propto h(t_1, t_2) \\exp( \\eta_1 T_1 + \\eta_2 T_2 )\n\\]\nAttempting to apply the Lemma: If we blindly followed the Lemma to find the marginal distribution of Heads (\\(T_1\\)), we would say: “Treat \\(\\eta_2\\) as constant, ignore the \\(T_2\\) part, and focus on \\(T_1\\).”\n\\[\n\\text{Hypothetical Marginal}(t_1) \\stackrel{?}{\\propto} h^*(t_1) \\exp( \\eta_1 t_1 ) = \\binom{n}{t_1} p^{t_1}\n\\]\nThe Result: \\(f(t_1) \\propto \\binom{n}{t_1} p^{t_1}\\). This is WRONG. It is missing the \\((1-p)^{n-t_1}\\) term. It suggests the probability of heads grows infinitely with \\(p\\) without being penalized by the probability of tails shrinking.\nWhy it Failed: The Marginal Distribution Lemma requires the natural parameters \\(\\eta_1\\) and \\(\\eta_2\\) to be variationally independent (the parameter space must contain a rectangle).\n\nParameter Constraint: \\(\\eta_1\\) and \\(\\eta_2\\) are coupled by the constraint \\(e^{\\eta_1} + e^{\\eta_2} = 1\\) (since \\(p + (1-p) = 1\\)). You cannot vary \\(\\eta_1\\) (change \\(p\\)) while holding \\(\\eta_2\\) constant.\nStatistic Constraint: \\(T_1\\) and \\(T_2\\) are perfectly collinear (\\(T_1 + T_2 = n\\)). The support of \\((T_1, T_2)\\) is a line segment, not a 2D grid. The base measure \\(h(t_1, t_2)\\) does not factor because \\(h(t_1, t_2)\\) is zero everywhere except on that line.\n\n\n\nExample 5.13 (Marginal Distributions of Normal Sufficient Statistics) Consider the Normal model \\(N(\\mu, \\sigma^2)\\) with sufficient statistics \\(T_1 = \\sum X_i^2\\) and \\(T_2 = \\sum X_i\\). The natural parameters are \\(\\pi_1 = -\\frac{1}{2\\sigma^2}\\) and \\(\\pi_2 = \\frac{\\mu}{\\sigma^2}\\).\n\nMarginal of \\(T_2\\) (fixing \\(\\sigma^2\\)) Suppose \\(\\sigma^2\\) is known (constant).\n\nLemma Condition: \\(\\pi_1 = -1/(2\\sigma^2)\\) is constant. The condition holds.\nImplied Form by MDL: The Marginal Distribution Lemma claims that \\(T_2\\) follows an exponential family with natural parameter \\(\\pi_2 = \\frac{\\mu}{\\sigma^2}\\): \\[\nf_{T_2}(t_2|\\theta) \\propto h^*(t_2) \\exp\\left\\{ \\frac{\\mu}{\\sigma^2} t_2 \\right\\}\n\\]\nComparison with Known Distribution: We know \\(T_2 = \\sum X_i \\sim N(n\\mu, n\\sigma^2)\\). The density is: \\[\nf(t_2|\\mu) = \\frac{1}{\\sqrt{2\\pi n \\sigma^2}} \\exp\\left\\{ -\\frac{(t_2 - n\\mu)^2}{2n\\sigma^2} \\right\\}\n\\]\nExpanding the square \\(-\\frac{1}{2n\\sigma^2}(t_2^2 - 2n\\mu t_2 + n^2\\mu^2)\\) and regrouping terms:\n\\[\nf(t_2|\\mu) = \\underbrace{ \\exp\\left\\{ -\\frac{n\\mu^2}{2\\sigma^2} \\right\\} }_{C(\\mu)} \\underbrace{ \\frac{1}{\\sqrt{2\\pi n \\sigma^2}} \\exp\\left\\{ -\\frac{t_2^2}{2n\\sigma^2} \\right\\} }_{h(t_2)} \\exp\\left\\{ \\frac{\\mu}{\\sigma^2} t_2 \\right\\}\n\\]\nResult: The Lemma correctly identifies the form, with the natural parameter \\(\\frac{\\mu}{\\sigma^2}\\).\n\nMarginal of \\(T_1\\) (fixing \\(\\mu\\)) Suppose \\(\\mu\\) is known (constant). We investigate whether the marginal distribution of \\(T_1 = \\sum X_i^2\\) remains in the exponential family by checking the behavior of the remaining natural parameter \\(\\pi_2 = \\frac{\\mu}{\\sigma^2}\\) with respect to the free parameter \\(\\sigma^2\\).\nCase A (\\(\\mu = 0\\)):\nIn this case, \\(\\pi_2 = 0\\), which is trivially constant with respect to \\(\\sigma^2\\). The condition of the Marginal Distribution Lemma is satisfied.\n\nImplied Form by MDL: The lemma claims that \\(T_1\\) must follow an exponential family form with natural parameter \\(\\pi_1 = -\\frac{1}{2\\sigma^2}\\): \\[\nf_{T_1}(t_1|\\sigma^2) \\propto h^*(t_1) \\exp\\left\\{ -\\frac{1}{2\\sigma^2} t_1 \\right\\}\n\\]\nVerification: We know that for \\(\\mu=0\\), the scaled statistic \\(T_1 / \\sigma^2\\) follows a central Chi-squared distribution with \\(n\\) degrees of freedom (\\(\\chi^2_n\\)). The density is proportional to: \\[\nf(t_1) \\propto t_1^{n/2 - 1} \\exp\\left\\{ -\\frac{t_1}{2\\sigma^2} \\right\\}\n\\] This matches the form claimed by the MDL perfectly, with natural parameter \\(-1/(2\\sigma^2)\\) and base measure \\(h^*(t_1) = t_1^{n/2-1}\\).\n\nCase B (\\(\\mu \\neq 0\\)):\nIn this case, \\(\\pi_2 = \\frac{\\mu}{\\sigma^2}\\) is a function of \\(\\sigma^2\\). As \\(\\sigma^2\\) changes, \\(\\pi_2\\) changes. The condition of the Lemma “\\(\\pi_i(\\theta)\\) are constant for all \\(i \\notin S\\)” fails. Therefore, the structure claimed by the lemma—a simple exponential family with parameter \\(\\pi_1\\)—is not guaranteed.\n\nVerification: We know that for \\(\\mu \\neq 0\\), the statistic \\(T_1/\\sigma^2\\) follows a Non-central Chi-squared distribution \\(\\chi'^2_n(\\lambda)\\) with non-centrality parameter \\(\\lambda = \\frac{\\sum \\mu^2}{\\sigma^2} = \\frac{n\\mu^2}{\\sigma^2}\\).\nThe density of a non-central Chi-squared involves an infinite mixture of central densities: \\[\nf(t_1) = \\sum_{k=0}^\\infty P(K=k) f_{\\chi^2_{n+2k}}(t_1)\n\\] where the weights \\(P(K=k)\\) depend on \\(\\lambda\\) (and thus \\(\\sigma^2\\)).\nResult: Because the parameter \\(\\sigma^2\\) appears inside the Poisson weights of the infinite sum, the density cannot be factored into the simple form \\(C(\\sigma^2)h(t_1)\\exp(\\pi_1 t_1)\\). This confirms that when the orthogonality condition is violated, the marginal distribution of a sufficient statistic leaves the simple exponential family.\n\n\n\n\nExample 5.14 (Distribution of Sample Variance \\(S^2\\) via Marginal Distribution Lemma) Consider the Normal model \\(X_i \\sim N(\\mu, \\sigma^2)\\). We wish to find the marginal distribution of the sufficient statistic \\(S^2 = \\frac{1}{n-1}\\sum (X_i - \\bar{X})^2\\).\n\nJacobian of the Transformation When transforming the joint density from the \\(n\\) data points \\((X_1, \\dots, X_n)\\) to the sufficient statistics \\((\\bar{X}, S^2)\\), we must account for the change in volume (the Jacobian).\nThe vector of residuals \\((X_1 - \\bar{X}, \\dots, X_n - \\bar{X})\\) lies on an \\((n-1)\\)-dimensional sphere with squared radius proportional to \\(s^2\\). The surface area of this sphere scales as \\((s^2)^{\\frac{n-1}{2}-1}\\). Thus, the volume element transforms as:\n\\[\n\\prod dx_i \\propto (s^2)^{\\frac{n-3}{2}} d s^2 d\\bar{x}\n\\]\nDecomposition of the Joint Density Incorporating this Jacobian into the exponential family form, the joint density of \\((\\bar{X}, S^2)\\) is:\n\\[\n\\begin{aligned}\nf(s^2, \\bar{x}) &\\propto \\underbrace{(s^2)^{\\frac{n-3}{2}}}_{\\text{Jacobian } h(s^2, \\bar{x})} \\cdot \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum x_i^2 + \\frac{\\mu}{\\sigma^2}\\sum x_i \\right\\} \\\\\n&\\propto (s^2)^{\\frac{n-3}{2}} \\cdot \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\left[ (n-1)s^2 + n\\bar{x}^2 \\right] + \\frac{n\\mu}{\\sigma^2}\\bar{x} \\right\\}\n\\end{aligned}\n\\]\nGrouping the terms by variable:\n\\[\nf(s^2, \\bar{x}) \\propto \\underbrace{\\left[ (s^2)^{\\frac{n-3}{2}} \\exp\\left\\{ -\\frac{n-1}{2\\sigma^2} s^2 \\right\\} \\right]}_{\\text{Terms involving } s^2} \\cdot \\underbrace{\\left[ \\exp\\left\\{ -\\frac{n}{2\\sigma^2}\\bar{x}^2 + \\frac{n\\mu}{\\sigma^2}\\bar{x} \\right\\} \\right]}_{\\text{Terms involving } \\bar{x}}\n\\]\nMarginal Integration To find the marginal distribution of \\(S^2\\), we integrate out \\(\\bar{x}\\). Since the density factors completely (orthogonality), the integral over \\(\\bar{x}\\) contributes only a multiplicative constant \\(C(\\mu, \\sigma^2)\\) and does not depend on \\(s^2\\).\nThe marginal density is simply the \\(s^2\\) component retained from the joint density:\n\\[\nf_{S^2}(s^2) \\propto (s^2)^{\\frac{n-3}{2}} \\exp\\left\\{ -\\frac{n-1}{2\\sigma^2} s^2 \\right\\}\n\\]\nIdentification of Degrees of Freedom We match this result against the kernel of a standard Gamma distribution (or Chi-squared type), \\(f(y) \\propto y^{k-1} e^{-\\beta y}\\).\n\nShape Parameter (\\(k\\)): \\[\n  k - 1 = \\frac{n-3}{2} \\implies k = \\frac{n-1}{2}\n  \\] Since the degrees of freedom \\(\\nu\\) is defined as \\(2k\\), we have \\(\\nu = n-1\\).\nRate Parameter (\\(\\beta\\)): \\[\n  \\beta = \\frac{n-1}{2\\sigma^2}\n  \\]\n\nThis confirms that \\(S^2 \\sim \\text{Gamma}(\\frac{n-1}{2}, \\frac{n-1}{2\\sigma^2})\\), or equivalently, \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exponential Families</span>"
    ]
  },
  {
    "objectID": "umvue.html",
    "href": "umvue.html",
    "title": "6  Minimum Variance Estimators",
    "section": "",
    "text": "6.1 Completeness\nSignificance: If \\(T\\) is complete, then there exists at most one unbiased estimator for \\(\\theta\\) that is a function of \\(T\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Variance Estimators</span>"
    ]
  },
  {
    "objectID": "umvue.html#completeness",
    "href": "umvue.html#completeness",
    "title": "6  Minimum Variance Estimators",
    "section": "",
    "text": "Definition 6.1 (Complete Statistic) A statistic \\(T\\) is said to be complete if for any real-valued function \\(g\\), \\[\nE[g(T)|\\theta] = 0 \\quad \\text{for all } \\theta\n\\] implies \\[\nP(g(T) = 0 | \\theta) = 1 \\quad \\text{for all } \\theta\n\\]\n\n\n\nExample 6.1 (Uniform Distribution (Not Complete)) Let \\(X_1, \\dots, X_n \\sim \\text{Unif}(\\theta-1, \\theta+1)\\). The density is:\n\\[\nf(x) = \\prod I(\\theta-1 &lt; x_i &lt; \\theta+1) = I(\\theta \\in (x_{(n)}-1, x_{(1)}+1))\n\\]\nThe statistic \\(T(X) = (X_{(1)}, X_{(n)})\\) is a Minimal Sufficient Statistic. However, it is not complete.\nConsider the range \\(R = X_{(n)} - X_{(1)}\\). The distribution of \\(R\\) does not depend on \\(\\theta\\) (it is an ancillary statistic). Let \\(g(T) = X_{(n)} - X_{(1)} - c\\), where \\(c = E[X_{(n)} - X_{(1)}]\\). Then \\(E[g(T)] = 0\\) for all \\(\\theta\\), but \\(g(T)\\) is not identically zero.\n\n\nLemma 6.1 (Exponential Family Completeness) If \\(T = (T_1, \\dots, T_k)\\) is the natural statistic of an exponential family that contains an open rectangle in the parameter space, then \\(T\\) is complete.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Variance Estimators</span>"
    ]
  },
  {
    "objectID": "umvue.html#umvue",
    "href": "umvue.html#umvue",
    "title": "6  Minimum Variance Estimators",
    "section": "6.2 UMVUE",
    "text": "6.2 UMVUE\n\nDefinition 6.2 (Uniformly Minimum Variance Unbiased Estimator (UMVUE)) A statistic \\(T(x)\\) is a UMVUE for \\(\\theta\\) if:\n\n\\(E(T(x)|\\theta) = \\theta\\) for all \\(\\theta\\) (Unbiased).\n\\(Var(T(x)|\\theta) \\le Var(d(x)|\\theta)\\) for all \\(\\theta\\) and for all other unbiased estimators \\(d(x)\\).\n\n\nThe relationship between statistics types is visualized below:\n\n\n\n\n\n\n\n\nFigure 6.1: Hierarchy of Statistics\n\n\n\n\n\n\nTheorem 6.1 (Lehmann-Scheffe Theorem) If \\(T\\) is a complete and sufficient statistic, and there is an unbiased estimator \\(d(X)\\) such that \\(E[d(X)] = \\theta\\), then \\(\\phi(T) = E[d(X)|T]\\) is the unique UMVUE for \\(\\theta\\).\n\n\nTheorem 6.2 (Rao-Blackwell Theorem) Given that \\(T\\) is a sufficient statistic and \\(d_1(x)\\) is an unbiased estimator (\\(E[d_1(x)] = \\theta\\)). Define \\(g(T) = E[d_1(x) | T]\\). Then:\n\n\\(g(T)\\) is a statistic (free of \\(\\theta\\) because \\(T\\) is sufficient).\n\\(E[g(T)] = \\theta\\) (Unbiased).\n\\(Var(g(T)) \\le Var(d_1(x))\\).\n\n\n\nProof. Proof of Rao-Blackwell:\n\nSince \\(T\\) is sufficient, the conditional distribution \\(X|T\\) is independent of \\(\\theta\\), so \\(g(T)\\) is a valid statistic.\nBy the Law of Iterated Expectations: \\[\n    E[g(T)] = E_T[ E_X(d_1(X)|T) ] = E_X[d_1(X)] = \\theta\n    \\]\nBy the variance decomposition formula: \\[\nVar(d_1(X)) = Var(E[d_1(X)|T]) + E[Var(d_1(X)|T)]\n\\] \\[\nVar(d_1(X)) = Var(g(T)) + E[(d_1(X) - g(T))^2 | T]\n\\] Since \\((d_1(X) - g(T))^2 \\ge 0\\), we have \\(Var(g(T)) \\le Var(d_1(X))\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Variance Estimators</span>"
    ]
  },
  {
    "objectID": "umvue.html#methods-for-finding-umvue",
    "href": "umvue.html#methods-for-finding-umvue",
    "title": "6  Minimum Variance Estimators",
    "section": "6.3 Methods for Finding UMVUE",
    "text": "6.3 Methods for Finding UMVUE\nTo find the UMVUE for a parameter \\(\\theta\\):\n\nFind a Complete Sufficient Statistic: Identify \\(T\\) which is complete and sufficient for \\(\\theta\\) (often using the Exponential Family properties).\nFind an Unbiased Estimator: Find any simple statistic \\(d(X)\\) such that \\(E[d(X)] = \\theta\\).\nRao-Blackwellize: Compute \\(g(T) = E[d(X)|T]\\). The result \\(g(T)\\) is the UMVUE.\n\n\nExample 6.2 (Poisson UMVUE) Let \\(X_1, \\dots, X_n \\sim \\text{Poisson}(\\lambda)\\). Find the UMVUE for \\(\\lambda\\) and \\(\\lambda^2\\).\n\nFor \\(\\lambda\\): \\(T = \\sum X_i\\) is a complete sufficient statistic (Poisson is exponential family). Let \\(d_1(X) = X_1\\). \\(E[X_1] = \\lambda\\). We compute \\(g(T) = E[X_1 | T]\\). Since the conditional distribution of \\(X_1\\) given \\(T=t\\) is Binomial(\\(t, 1/n\\)):\n\n\\[\nE[X_1 | T] = t \\cdot \\frac{1}{n} = \\frac{T}{n} = \\bar{X}\n\\]\nThus, \\(\\bar{X}\\) is the UMVUE for \\(\\lambda\\).\n\nFor \\(\\lambda^2\\): We know \\(Var(X_1) = \\lambda = E(X_1^2) - (E(X_1))^2\\). So \\(E(X_1^2) - \\lambda = \\lambda^2\\), which implies \\(E(X_1^2 - X_1) = \\lambda^2\\). Let \\(d_2(X) = X_1^2 - X_1\\). This is an unbiased estimator for \\(\\lambda^2\\).\n\nWe calculate \\(g(T) = E[X_1^2 - X_1 | T]\\).\n\\[\ng(T) = E[X_1^2 | T] - E[X_1 | T]\n\\]\nUsing the second moment of the Binomial distribution \\(Bin(T, 1/n)\\): \\(E[X_1^2|T] = Var(X_1|T) + (E[X_1|T])^2 = T \\frac{1}{n}(1-\\frac{1}{n}) + (\\frac{T}{n})^2\\).\n\\[\ng(T) = \\left[ \\frac{T}{n} - \\frac{T}{n^2} + \\frac{T^2}{n^2} \\right] - \\frac{T}{n} = \\frac{T^2 - T}{n^2} = \\frac{T(T-1)}{n^2}\n\\]\nThus, \\(\\frac{T(T-1)}{n^2}\\) is the UMVUE for \\(\\lambda^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Minimum Variance Estimators</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "",
    "text": "8.1 Formulation for Hypothesis Testing",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#formulation-for-hypothesis-testing",
    "href": "hypothesis.html#formulation-for-hypothesis-testing",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "",
    "text": "8.1.1 Hypothesis\nWe formulate the problem of hypothesis testing as deciding between two competing claims about a parameter \\(\\theta\\):\n\\[\nH_0: \\theta \\in \\Theta_0 \\quad \\text{(Null Hypothesis)}\n\\]\n\\[\nH_1: \\theta \\in \\Theta_1 \\quad \\text{(Alternative Hypothesis)}\n\\]\n\nDefinition 8.1 (Simple and Composite Hypotheses) A hypothesis is called simple if it specifies a single value for the parameter (e.g., \\(\\Theta_0\\) contains only one point). It is called composite if it specifies more than one value.\n\n\nExample 8.1 (Normal Mean Test) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\).\n\nIf \\(\\sigma^2\\) is known, \\(H_0: \\mu = \\mu_0\\) is a simple hypothesis.\nIf \\(\\sigma^2\\) is unknown, \\(H_0: \\mu = \\mu_0\\) is a composite hypothesis (since \\(\\sigma^2\\) can vary).\n\n\n\n\n8.1.2 Test Functions\nA test is defined by a critical region \\(C_\\alpha\\) such that we reject \\(H_0\\) if the data \\(x \\in C_\\alpha\\). Equivalently, we can define a test function \\(\\phi(x)\\) representing the probability of rejecting \\(H_0\\) given data \\(x\\).\n\nA non-randomized test is given as follows:\n\n\\[\n\\phi(x) = I(x \\in C_\\alpha) = \\begin{cases} 1 & \\text{if } x \\in C_\\alpha \\text{ (Reject } H_0 \\text{)} \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\nA randomized test, \\(\\phi(x)\\) can take values in \\([0, 1]\\), which can be expressed typically as follows:\n\\[\n\\phi(x) = \\begin{cases}\n1 & \\text{if } x \\in C_1 \\\\\n\\gamma & \\text{if } x \\in C_* \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere:\n\n\\(C_1\\) is the region where we strictly reject \\(H_0\\).\n\\(C_*\\) is the boundary region (often where \\(T(x) = k\\)) where we reject \\(H_0\\) with probability \\(\\gamma\\).\n\nMore generally, \\(\\phi(x)\\) is just a function of \\(x\\) with values in \\([0,1]\\), which represents the probability that we will reject \\(H_0\\).\n\n\nExample 8.2 (Randomized Test for Binomial) Let \\(X \\sim \\text{Bin}(n=10, \\theta)\\). Consider testing \\(H_0: \\theta = 1/2\\) vs \\(H_1: \\theta &gt; 1/2\\) with target size \\(\\alpha = 0.05\\).\nSuppose we choose a critical region \\(X \\ge k\\).\n\nIf \\(k=9\\), \\(P(X \\ge 9 | \\theta=0.5) \\approx 0.0107\\).\nIf \\(k=8\\), \\(P(X \\ge 8 | \\theta=0.5) \\approx 0.0547\\).\n\nSince we cannot achieve exactly 0.05 with a non-randomized test (the survival function jumps over 0.05), we must use a randomized test function.\nThe randomized test is defined as:\n\\[\n\\phi(x) = \\begin{cases}\n1 & \\text{if } x \\in C_1 \\text{ (i.e., } x \\ge 9) \\\\\n\\gamma & \\text{if } x \\in C_* \\text{ (i.e., } x = 8) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nFrom the figure, we see that \\(\\alpha = 0.05\\) lies between \\(P(X \\ge 9)\\) and \\(P(X \\ge 8)\\). We always reject the “tail” where probabilities are strictly less than \\(\\alpha\\) (here \\(x \\ge 9\\)). At the boundary \\(x=8\\), we cannot reject with probability 1 (which would give total size 0.0547), nor with probability 0 (which would give total size 0.0107).\nWe choose \\(\\gamma\\) to bridge this gap:\n\\[\n\\begin{aligned}\n\\alpha &= P(X \\ge 9) + \\gamma \\cdot P(X = 8) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (P(X \\ge 8) - P(X \\ge 9)) \\\\\n0.05 &= 0.01074 + \\gamma \\cdot (0.05469 - 0.01074)\n\\end{aligned}\n\\]\nSolving for \\(\\gamma\\):\n\\[\n\\gamma = \\frac{0.05 - 0.01074}{0.04395} \\approx \\frac{39}{44} \\approx 0.89\n\\]\n\n\n\n\n\n\n\n\n\nFigure 8.1: Survival Function P(X &gt;= k) with randomization detail\n\n\n\n\n\n\n\n8.1.3 Size\n\nDefinition 8.2 (Size of a Test) The size of a test \\(\\phi(x)\\) is the maximum probability of rejecting the null hypothesis when it is true:\n\\[\n\\text{Size}(\\phi) = \\sup_{\\theta \\in \\Theta_0} W_\\phi(\\theta) = \\sup_{\\theta \\in \\Theta_0} E_\\theta[\\phi(X)]\n\\]\n\n\n\n8.1.4 Power\nWe distinguish between the power function varying over parameters and the power metric of a specific test.\n\nPower Function (\\(W_\\phi(\\theta)\\)) The probability of rejecting \\(H_0\\) as a function of the parameter \\(\\theta\\):\n\n\\[\nW_\\phi(\\theta) = E_\\theta[\\phi(X)]\n\\]\n\nPower of the Test (\\(\\text{Power}(\\phi)\\)) In the context of a specific alternative hypothesis (e.g., \\(H_1: \\theta = \\theta_1\\)), we define the power as a scalar functional of \\(\\phi\\):\n\n\\[\n\\text{Power}(\\phi) = E_{\\theta_1}[\\phi(X)]\n\\]\nIdeally, we want:\n\n\\(W_\\phi(\\theta) \\le \\text{Size}(\\phi)\\) for all \\(\\theta \\in \\Theta_0\\) (Control Type I error).\n\\(\\text{Power}(\\phi)\\) to be as large as possible (Maximize sensitivity to \\(H_1\\)).\n\n\n\nCode\nlibrary(ggplot2)\n\n# 1. Define Parameters\nmu0 &lt;- 0\nmu1 &lt;- 3\nsigma &lt;- 1\nc_val &lt;- 1.5      # Critical value\ngamma_val &lt;- 0.5  # Randomization constant\n\n# 2. Scaling Constants\nmax_dens &lt;- dnorm(mu0, mean = mu0, sd = sigma)\ny_limit &lt;- max_dens * 1.1\nphi_scale &lt;- 0.1 * y_limit\n\n# 3. Define the Test Function phi(x) (Single Test)\n# Step function: 0 -&gt; 1 at c_val\ndf_phi &lt;- data.frame(\n  x_start = c(-3, c_val),\n  x_end   = c(c_val, 6),\n  y_start = c(0, phi_scale),\n  y_end   = c(0, phi_scale)\n)\n\nggplot() +\n  # --- Layer 1: Densities (Solid Lines, No Fill) ---\n  \n  # H0: Normal(0, 1) - Blue (Cool)\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                color = \"blue\", size = 0.8) +\n  \n  # H1: Normal(3, 1) - Red (Hot)\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                color = \"red\", size = 0.8) +\n\n  # --- Layer 2: Test Function phi(x) (Thick Pink Line) ---\n  \n  # The horizontal segments\n  geom_segment(data = df_phi, \n               aes(x = x_start, xend = x_end, y = y_start, yend = y_end), \n               color = \"deeppink\", size = 2.5, alpha = 0.5) +\n  \n  # The vertical threshold line\n  geom_segment(aes(x = c_val, xend = c_val, y = 0, yend = phi_scale), \n               linetype = \"dotted\", color = \"deeppink\", size = 0.8) +\n  \n  # Points at discontinuity\n  geom_point(aes(x = c_val, y = gamma_val * phi_scale), \n             color = \"deeppink\", size = 3) + \n  geom_point(aes(x = c_val, y = 0), size = 3, shape = 21, fill = \"white\", color = \"deeppink\") +\n  geom_point(aes(x = c_val, y = phi_scale), size = 3, shape = 21, fill = \"white\", color = \"deeppink\") +\n\n  # --- Layer 3: Annotations ---\n  \n  # Gamma label\n  annotate(\"text\", x = c_val + 0.2, y = gamma_val * phi_scale, \n           label = expression(gamma), hjust = 0, fontface = \"bold\", color = \"deeppink\") +\n  \n  # H0 / H1 Labels\n  annotate(\"text\", x = mu0, y = max_dens * 0.9, \n           label = expression(H[0]), color = \"blue\", size = 5) +\n  annotate(\"text\", x = mu1, y = max_dens * 0.9, \n           label = expression(H[1]), color = \"red\", size = 5) +\n  \n  # Critical Value Label\n  annotate(\"text\", x = c_val, y = -0.01, label = \"c\", vjust = 1) +\n\n  # --- Layer 4: Scales ---\n  scale_y_continuous(\n    name = \"Density f(x)\",\n    limits = c(-0.02, y_limit),\n    expand = c(0, 0),\n    \n    # Secondary Axis for phi\n    sec.axis = sec_axis(~ . / phi_scale, \n                        name = expression(phi(x)),\n                        breaks = c(0, 1))\n  ) +\n  scale_x_continuous(name = \"Observation x\", limits = c(-3, 6)) +\n  \n  theme_minimal() +\n  theme(\n    axis.title.y.right = element_text(angle = 90, vjust = 0.5),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 8.2: Illustration of a Test Function \\(\\phi(x)\\) (Pink) relative to Size (\\(H_0\\), Blue) and Power (\\(H_1\\), Red).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#the-neyman-pearson-lemma",
    "href": "hypothesis.html#the-neyman-pearson-lemma",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "8.2 The Neyman-Pearson Lemma",
    "text": "8.2 The Neyman-Pearson Lemma\nConsider testing a simple null against a simple alternative: \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta_1\\).\nWe define the Likelihood Ratio \\(\\Lambda(x)\\) as:\n\\[\n\\Lambda(x) = \\frac{f_1(x)}{f_0(x)} = \\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n\\]\n\nDefinition 8.3 (Likelihood Ratio Test (LRT)) A test \\(\\phi(x)\\) is a Likelihood Ratio Test if it has the form:\n\\[\n\\phi_{\\text{LRT}}(x) = \\begin{cases}\n1 & \\text{if } \\Lambda(x) &gt; k \\\\\n\\gamma(x) & \\text{if } \\Lambda(x) = k \\\\\n0 & \\text{if } \\Lambda(x) &lt; k\n\\end{cases}\n\\]\nwhere \\(k \\ge 0\\) is a constant and \\(0 \\le \\gamma(x) \\le 1\\).\n\n\n8.2.1 Neyman-Pearson Lemma\n\nTheorem 8.1 (Neyman-Pearson Lemma)  \n\nOptimality: For any \\(k\\) and \\(\\gamma(x)\\), the LRT \\(\\phi_0(x)\\) defined above has maximum power among all tests whose size is less than or equal to the size of \\(\\phi_0(x)\\).\nExistence: Given \\(\\alpha \\in (0, 1)\\), there exist constants \\(k\\) and \\(\\gamma_0\\) such that the LRT defined by this \\(k\\) and \\(\\gamma(x) = \\gamma_0\\) has size exactly \\(\\alpha\\).\nUniqueness: If a test \\(\\phi\\) has size \\(\\alpha\\) and is of maximum power among all tests of size \\(\\alpha\\), then \\(\\phi\\) is necessarily an LRT, except possibly on a set of measure zero under \\(H_0\\) and \\(H_1\\).\n\n\n\n\n8.2.2 A Derivation with The Lagrange Multiplier Approach\nTo make the optimality of the Likelihood Ratio Test (LRT) intuitive, we can frame the search for the best test function \\(\\phi(x)\\) as a constrained optimization problem.\nWe want to maximize the power of the test:\n\\[\n\\text{Power}(\\phi) = \\int \\phi(x) f_1(x) dx\n\\]\nsubject to the constraint on the size of the test \\(\\alpha\\):\n\\[\n\\text{Size}(\\phi) = \\int \\phi(x) f_0(x) dx = \\alpha\n\\]\nUsing the method of Lagrange multipliers, we define the objective function \\(L\\) with a multiplier \\(k\\):\n\\[\nL(\\phi, k) = \\int \\phi(x) f_1(x) dx - k \\left( \\int \\phi(x) f_0(x) dx - \\alpha \\right)\n\\]\nRearranging the terms inside the integral, we get:\n\\[\nL(\\phi, k) = \\int \\phi(x) [f_1(x) - k f_0(x)] dx + k\\alpha\n\\]\nTo maximize \\(L\\) with respect to \\(\\phi(x)\\), we look at the integrand. Since \\(0 \\le \\phi(x) \\le 1\\), we should choose \\(\\phi(x)\\) to be as large as possible whenever its coefficient is positive, and as small as possible whenever its coefficient is negative:\n\nIf \\(f_1(x) - k f_0(x) &gt; 0\\), set \\(\\phi(x) = 1\\).\nIf \\(f_1(x) - k f_0(x) &lt; 0\\), set \\(\\phi(x) = 0\\).\nIf \\(f_1(x) - k f_0(x) = 0\\), the value of \\(\\phi(x)\\) does not affect the integral (this is where \\(\\gamma\\) comes in).\n\nThis decision rule is equivalent to:\n\\[\n\\phi(x) =\n\\begin{cases}\n1 & \\text{if } \\frac{f_1(x)}{f_0(x)} &gt; k \\\\\n0 & \\text{if } \\frac{f_1(x)}{f_0(x)} &lt; k\n\\end{cases}\n\\]\nThis is precisely the form of the Likelihood Ratio Test. The “shadow price” or Lagrange multiplier \\(k\\) represents the critical threshold that balances the gain in power against the cost of increasing the Type I error.\n\n\n8.2.3 Proof of NP Lemma\n\nProof. Proof of (a) Optimality: Let \\(\\phi_{\\text{LRT}}\\) be the LRT with size \\(\\alpha\\), and \\(\\phi\\) be any other test with size \\(\\le \\alpha\\). Define the function \\(U(x)\\) as the difference in test functions weighted by the linear combination of densities:\n\\[\nU(x) = (\\phi_{}(x) - \\phi(x))(f_1(x) - k f_0(x))\n\\]\nWe analyze the sign of \\(U(x)\\) by looking at the sign of its two factors in three cases:\n\nIf \\(f_1(x) - k f_0(x) &gt; 0\\) (implies \\(\\Lambda(x) &gt; k\\)). Since \\(\\phi_{\\text{LRT}}(x) = 1\\) and \\(\\phi(x) \\le 1\\), we have:\n\\[\n  \\begin{aligned}\n  \\phi_{\\text{LRT}}(x) - \\phi(x) &\\ge 0 \\\\\n  U(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) &\\ge 0\n  \\end{aligned}\n  \\]\nIf \\(f_1(x) - k f_0(x) &lt; 0\\) (implies \\(\\Lambda(x) &lt; k\\)). Since \\(\\phi_{\\text{LRT}}(x) = 0\\) and \\(\\phi(x) \\ge 0\\), we have:\n\\[\n  \\begin{aligned}\n  \\phi_{\\text{LRT}}(x) - \\phi(x) &\\le 0 \\\\\n  U(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) &\\ge 0\n  \\end{aligned}\n  \\]\nIf \\(f_1(x) - k f_0(x) = 0\\). The product is zero regardless of the test functions.\n\\[\n  U(x) = 0\n  \\]\n\nCombining these cases, we conclude that the product is non-negative for all \\(x\\):\n\\[\nU(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) \\ge 0\n\\]\nTherefore, integrating \\(U(x)\\) over the entire domain:\n\\[\n\\int U(x) dx = \\int (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) dx \\ge 0\n\\]\nExpanding the integral:\n\\[\n\\int \\phi_{\\text{LRT}}(x) f_1(x) \\, dx - \\int \\phi(x) f_1(x) \\, dx - k \\left( \\int \\phi_{\\text{LRT}}(x) f_0(x) \\, dx - \\int \\phi(x) f_0(x) \\, dx \\right) \\ge 0\n\\] Converting to expectations:\n\\[\nE_{\\theta_1}[\\phi_{\\text{LRT}}] - E_{\\theta_1}[\\phi] \\ge k (E_{\\theta_0}[\\phi_{\\text{LRT}}] - E_{\\theta_0}[\\phi])  \n\\]\nSince \\(E_{\\theta_0}[\\phi_{\\text{LRT}}] =\\text{Size}(\\phi_{\\text{LRT}})= \\alpha\\) and we require that \\(E_{\\theta_0}[\\phi] = \\text{Size}(\\phi)\\le \\alpha\\),\n\\[\nE_{\\theta_0}[\\phi_{\\text{LRT}}] - E_{\\theta_0}[\\phi] \\ge 0\n\\]\nThereore, given that \\(k \\ge 0\\):\n\\[\n\\text{Power}(\\phi_{\\text{LRT}}) \\ge \\text{Power}(\\phi)\n\\]\nProof of (b) Existence:\nLet \\(G(k) = P_{\\theta_0}(\\Lambda(X) \\le k)\\). \\(G(k)\\) is the cumulative distribution function of the random variable \\(\\Lambda(X)\\), so it is non-decreasing. We seek \\(k_0\\) such that \\(1 - G(k_0) \\approx \\alpha\\). Because of discrete jumps, we might not hit \\(\\alpha\\) exactly. We choose \\(k_0\\) such that:\n\\[\nP_{\\theta_0}(\\Lambda(X) &gt; k_0) \\le \\alpha \\le P_{\\theta_0}(\\Lambda(X) \\ge k_0)\n\\]\nSet \\(\\gamma_0 = \\frac{\\alpha - P_{\\theta_0}(\\Lambda(X) &gt; k_0)}{P_{\\theta_0}(\\Lambda(X) = k_0)}\\).\nProof of (c) Uniqueness\nLet \\(\\phi_{\\text{LRT}}\\) be the LRT of size \\(\\alpha\\). Suppose there exists another test \\(\\phi\\) that is also Most Powerful (MP) with size \\(\\le \\alpha\\). We wish to show that \\(\\phi(x) = \\phi_{\\text{LRT}}(x)\\) for almost all \\(x\\) where \\(f_1(x) \\ne k f_0(x)\\).\nAs established in the optimality proof, the function: \\[\nU(x) = (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x))\n\\] is non-negative for all \\(x\\). Since both tests are MP, they have the same power: \\(E_{\\theta_1}[\\phi_{\\text{LRT}}] = E_{\\theta_1}[\\phi]\\).\nFrom the integral of \\(U(x)\\), we have: \\[\n0 \\le \\int U(x) dx = (E_{\\theta_1}[\\phi_{\\text{LRT}}] - E_{\\theta_1}[\\phi]) - k(E_{\\theta_0}[\\phi_{\\text{LRT}}] - E_{\\theta_0}[\\phi])\n\\]\nSubstituting the equality of power: \\[\n0 \\le -k(\\alpha - E_{\\theta_0}[\\phi])\n\\]\nSince \\(k &gt; 0\\) and \\(E_{\\theta_0}[\\phi] \\le \\alpha\\), the term \\(-k(\\alpha - E_{\\theta_0}[\\phi])\\) is \\(\\le 0\\). The only way for the integral of a non-negative function \\(U(x)\\) to be \\(\\le 0\\) is if the integral is exactly zero: \\[\n\\int (\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) \\, dx = 0\n\\]\nFor the integral of a non-negative function to be zero, the integrand must be zero almost everywhere: \\[\n(\\phi_{\\text{LRT}}(x) - \\phi(x))(f_1(x) - k f_0(x)) = 0 \\quad \\text{a.e.}\n\\]\nThis implies that for any \\(x\\) where \\(f_1(x) - k f_0(x) \\ne 0\\), we must have: \\[\n\\phi_{\\text{LRT}}(x) - \\phi(x) = 0 \\implies \\phi(x) = \\phi_{\\text{LRT}}(x)\n\\]\nThus, the test is unique except possibly on the boundary set \\(\\{x : f_1(x) = k f_0(x)\\}\\). If \\(P_{\\theta_0}(\\Lambda(X) = k) = 0\\) (as in continuous distributions like the Normal), the MP test is unique almost everywhere.\n\n\n\nCode\nlibrary(ggplot2)\n\n# 1. Define Parameters\nmu0 &lt;- 0\nmu1 &lt;- 3\nsigma &lt;- 1\nc_lrt &lt;- 2\ngamma_val &lt;- 0.5\n\n# Scaling constants\nmax_dens &lt;- dnorm(mu0, mean = mu0, sd = sigma)\ny_limit &lt;- max_dens * 1.1\nphi_scale &lt;- 0.1 * y_limit\n\n# 2. Define Test Functions Data\ndf_lrt &lt;- data.frame(\n  x_start = c(-3, c_lrt),\n  x_end   = c(c_lrt, 6),\n  y_val   = c(0, phi_scale),\n  Test    = \"phi[LRT]\" \n)\n\ndf_other &lt;- data.frame(\n  x_start = c(-3, 0.5, 1, 3),\n  x_end   = c(0.5, 1, 3,6),\n  y_val   = c(0, phi_scale, 0, 0),\n  Test    = \"phi\"\n)\n\nggplot() +\n  # --- Layer 1: Densities (Solid Lines, No Fill) ---\n  stat_function(fun = dnorm, args = list(mean = mu0, sd = sigma), \n                color = \"blue\", size = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = sigma), \n                color = \"red\", size = 0.8) +\n\n  # --- Layer 2: Test Functions (Segments) ---\n  \n  # 2a. LRT Segments (Thick, Transparent Pink)\n  geom_segment(data = df_lrt,\n               aes(x = x_start, xend = x_end, \n                   y = y_val, yend = y_val, \n                   color = Test, linetype = Test, size = Test),\n               alpha = 0.4) + \n  \n  # Vertical connector for LRT\n  geom_segment(aes(x = c_lrt, xend = c_lrt, y = 0, yend = phi_scale),\n               color = \"deeppink\", linetype = \"dotted\", size = 0.5, alpha = 0.6) +\n\n  # 2b. Other Test Segments (Thin Black Opaque)\n  geom_segment(data = df_other,\n               aes(x = x_start, xend = x_end, \n                   y = y_val, yend = y_val, \n                   color = Test, linetype = Test, size = Test)) +\n  \n  # Vertical connectors for Other Test\n  geom_segment(aes(x = 1, xend = 1, y = 0, yend = phi_scale),\n               color = \"black\", linetype = \"dotted\", size = 0.5) +\n  geom_segment(aes(x = 3, xend = 3, y = 0, yend = phi_scale),\n               color = \"black\", linetype = \"dotted\", size = 0.5) +\n\n  # --- Layer 3: Annotations ---\n  # LRT Gamma Point (Pink)\n  geom_point(aes(x = c_lrt, y = gamma_val * phi_scale), \n             color = \"deeppink\", size = 3) +\n  \n  # Density Labels\n  annotate(\"text\", x = mu0, y = max_dens * 0.9, \n           label = expression(H[0]), color = \"blue\", size = 5) +\n  annotate(\"text\", x = mu1, y = max_dens * 0.9, \n           label = expression(H[1]), color = \"red\", size = 5) +\n\n  # --- Layer 4: Scales and Legend ---\n  scale_y_continuous(\n    name = \"Density f(x)\", limits = c(-0.02, y_limit), expand = c(0, 0),\n    sec.axis = sec_axis(~ . / phi_scale, name = expression(phi(x)), breaks = c(0, 1))\n  ) +\n  scale_x_continuous(name = \"Observation x\", limits = c(-3, 6)) +\n  \n  # Manual Legend Controls (FIXED using named vectors for labels)\n  scale_color_manual(name = \"Test Function\",\n                     values = c(\"phi[LRT]\" = \"deeppink\", \"phi\" = \"black\"),\n                     labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  scale_linetype_manual(name = \"Test Function\",\n                        values = c(\"phi[LRT]\" = \"solid\", \"phi\" = \"solid\"),\n                        labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  scale_size_manual(name = \"Test Function\",\n                    values = c(\"phi[LRT]\" = 4,   # Extra Thick\n                               \"phi\" = 1), # Thin\n                    labels = c(\"phi[LRT]\" = expression(phi[LRT]), \"phi\" = expression(phi))) +\n  \n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.title.y.right = element_text(angle = 90, vjust = 0.5),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 8.3: Visualizing the NP Lemma. The thick, transparent pink line is \\(\\phi_{ ext{LRT}}\\). The thin solid black line is \\(\\phi\\). Overlap is visible as a black line inside pink.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#uniformly-most-powerful-ump-tests",
    "href": "hypothesis.html#uniformly-most-powerful-ump-tests",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "8.3 Uniformly Most Powerful (UMP) Tests",
    "text": "8.3 Uniformly Most Powerful (UMP) Tests\nWhen the alternative hypothesis is composite (\\(H_1: \\theta \\in \\Theta_1\\)), we seek a test that is “best” for all \\(\\theta \\in \\Theta_1\\).\n\nDefinition 8.4 (Uniformly Most Powerful Test) A test \\(\\phi_0(x)\\) of size \\(\\alpha\\) is Uniformly Most Powerful (UMP) if:\n\n\\(E_{\\theta}[\\phi_0(X)] \\le \\alpha\\) for all \\(\\theta \\in \\Theta_0\\).\nFor any other test \\(\\phi(x)\\) satisfying (1), \\(E_{\\theta}[\\phi_0(X)] \\ge E_{\\theta}[\\phi(X)]\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Diagram of UMP Tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#monotone-likelihood-ratio-mlr",
    "href": "hypothesis.html#monotone-likelihood-ratio-mlr",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "8.4 Monotone Likelihood Ratio (MLR)",
    "text": "8.4 Monotone Likelihood Ratio (MLR)\n\nDefinition 8.5 (Monotone Likelihood Ratio) A family of densities \\(\\{f(x; \\theta)\\}\\) has a Monotone Likelihood Ratio (MLR) with respect to a statistic \\(T(x)\\) if for any \\(\\theta_1 &gt; \\theta_0\\), the ratio:\n\\[\n\\frac{f(x; \\theta_1)}{f(x; \\theta_0)}\n\\]\nis a non-decreasing function of \\(T(x)\\).\n\nCommon examples include the one-parameter Exponential Family: \\(f(x; \\theta) = h(x) c(\\theta) \\exp\\{w(\\theta) T(x)\\}\\). If \\(w(\\theta)\\) is increasing, the family has MLR w.r.t \\(T(x)\\).\n\nExample 8.3 Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)\\) with pdf \\(f(x) = \\frac{1}{\\theta} e^{-x/\\theta}\\). Test \\[H_0: \\theta = \\theta_0 \\text{ vs } H_1: \\theta &gt; \\theta_0.\\]\nThe Likelihood Ratio for \\(\\theta_1 &gt; \\theta_0\\) is:\n\\[\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n\\]\nSince \\(\\theta_1 &gt; \\theta_0\\), the term \\((\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})\\) is positive. Thus, \\(\\Lambda(x)\\) is an increasing function of the sum \\(T (x) = \\sum x_i\\).\nRejecting for large \\(\\Lambda(x)\\) is equivalent to rejecting for \\(T(x)=\\sum x_i &gt; C\\).\nUnder \\(H_0\\), \\(X_i \\sim \\text{Exp}(\\theta_0)\\), which is equivalent to \\(\\text{Gamma}(1, \\theta_0)\\). By the reproductive property of the Gamma distribution:\n\\[T = \\sum_{i=1}^n X_i \\sim \\text{Gamma}(n, \\theta_0)\\]\nAlternatively, using the relationship with the Chi-square distribution:\n\\[\\frac{2T}{\\theta_0} \\sim \\chi^2_{2n}\\]\nTo find \\(C\\) for a significance level \\(\\alpha\\), we set \\(P(T &gt; C | \\theta_0) = \\alpha\\). Using the \\(\\chi^2\\) transformation:\n\\[P\\left(\\frac{2T}{\\theta_0} &gt; \\frac{2C}{\\theta_0}\\right) = \\alpha \\implies \\frac{2C}{\\theta_0} = \\chi^2_{2n, \\alpha}\\]\nThus, the critical value is:\n\\[C = \\frac{\\theta_0}{2} \\chi^2_{2n, \\alpha}\\]\nwhere \\(\\chi^2_{2n, \\alpha}\\) is the upper-\\(\\alpha\\) quantile of a Chi-square distribution with \\(2n\\) degrees of freedom.\n\n\n\n\n\n\nImportant\n\n\n\nWe note that the value \\(C\\) does not depend on \\(\\theta_1\\).\n\n\n\n\n8.4.1 Karlin-Rubin Theorem\n\nTheorem 8.2 (Chebyshev’s Association Inequality) Let \\(X\\) be a random variable, and let \\(f(x)\\) and \\(g(x)\\) be two functions that are both non-decreasing (or both non-increasing). Then: \\[\nE[f(X)g(X)] \\ge E[f(X)] \\cdot E[g(X)]\n\\] Equivalently, the covariance is non-negative: \\(\\text{Cov}(f(X), g(X)) \\ge 0\\).\n\n\nProof. Let \\(Y\\) be an independent copy of \\(X\\) (i.e., \\(X\\) and \\(Y\\) are i.i.d.). Consider the quantity: \\[\n\\Delta = (f(X) - f(Y))(g(X) - g(Y))\n\\]\nSince \\(f\\) and \\(g\\) are both non-decreasing (or both non-increasing), the terms \\((f(X) - f(Y))\\) and \\((g(X) - g(Y))\\) always share the same sign. Thus, their product is always non-negative: \\[\n\\Delta \\ge 0\n\\]\nTaking the expectation: \\[\nE[(f(X) - f(Y))(g(X) - g(Y))] \\ge 0\n\\]\nExpanding the product and using linearity of expectation: \\[\nE[f(X)g(X)] - E[f(X)g(Y)] - E[f(Y)g(X)] + E[f(Y)g(Y)] \\ge 0\n\\]\nSince \\(X\\) and \\(Y\\) are i.i.d.:\n\n\\(E[f(Y)g(Y)] = E[f(X)g(X)]\\)\n\\(E[f(X)g(Y)] = E[f(X)]E[g(Y)] = E[f(X)]E[g(X)]\\) (Independence)\n\\(E[f(Y)g(X)] = E[f(Y)]E[g(X)] = E[f(X)]E[g(X)]\\) (Independence)\n\nSubstituting these back yields: \\[\n2E[f(X)g(X)] - 2E[f(X)]E[g(X)] \\ge 0\n\\]\nDividing by 2 proves the inequality: \\[\nE[f(X)g(X)] \\ge E[f(X)]E[g(X)]\n\\]\n\n\nTheorem 8.3 (Karlin-Rubin Theorem) Suppose \\(X\\) has a distribution from a family with MLR with respect to \\(T(X)\\), and the distribution of \\(T(X)\\) is continuous. Consider testing \\(H_0: \\theta \\le \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\).\nThe test:\n\\[\n\\phi(x) = \\begin{cases}\n1 & \\text{if } T(x) &gt; t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n\\]\nwhere \\(t_0\\) is determined by \\(P_{\\theta_0}(T(X) &gt; t_0) = \\alpha\\), is the UMP size \\(\\alpha\\) test.\n\n\nProof. The Test: Define the test \\(\\phi_0(x)\\) as:\n\\[\n\\phi_0(x) = \\begin{cases}\n1 & \\text{if } T(x) &gt; t_0 \\\\\n0 & \\text{if } T(x) \\le t_0\n\\end{cases}\n\\]\nwhere \\(t_0\\) is determined such that the power at the boundary is \\(\\alpha\\), i.e., \\(W_{\\phi_0}(\\theta_0) = \\alpha\\).\n\nMonotonicity of the Power Function\nWe first establish that \\(W_{\\phi_0}(\\theta)\\) is non-decreasing. Let \\(\\theta_1 &gt; \\theta_0\\). Define the test indicator function \\(h(t) = \\mathbb{I}(t &gt; t_0)\\) and the likelihood ratio \\(\\Lambda(t) = \\frac{f_{\\theta_1}(t)}{f_{\\theta_0}(t)}\\).\nBecause of the Monotone Likelihood Ratio (MLR) property, \\(\\Lambda(t)\\) is a non-decreasing function of \\(t\\). The indicator function \\(h(t)\\) is clearly non-decreasing.\nThe power at \\(\\theta_1\\) can be written as an integral involving the density under \\(\\theta_0\\): \\[\nW_{\\phi_0}(\\theta_1) = \\int_{-\\infty}^{\\infty} h(t) f_{\\theta_1}(t) \\, dt = \\int_{-\\infty}^{\\infty} h(t) \\frac{f_{\\theta_1}(t)}{f_{\\theta_0}(t)} f_{\\theta_0}(t) \\, dt = \\int_{-\\infty}^{\\infty} h(t) \\Lambda(t) f_{\\theta_0}(t) \\, dt\n\\]\nThis integral is the expectation \\(E_{\\theta_0}[h(T) \\Lambda(T)]\\). By Chebyshev’s Association Inequality (Covariance Inequality), since both \\(h(t)\\) and \\(\\Lambda(t)\\) are non-decreasing, the expectation of their product is at least the product of their expectations: \\[\n\\int_{-\\infty}^{\\infty} h(t) \\Lambda(t) f_{\\theta_0}(t) \\, dt \\ge \\left( \\int_{-\\infty}^{\\infty} h(t) f_{\\theta_0}(t) \\, dt \\right) \\left( \\int_{-\\infty}^{\\infty} \\Lambda(t) f_{\\theta_0}(t) \\, dt \\right)\n\\]\nWe evaluate the two integrals on the right-hand side:\n\nThe first term is the power at \\(\\theta_0\\): \\(\\int h(t) f_{\\theta_0}(t) \\, dt = W_{\\phi_0}(\\theta_0)\\).\nThe second term integrates the likelihood ratio: \\(\\int \\frac{f_{\\theta_1}(t)}{f_{\\theta_0}(t)} f_{\\theta_0}(t) \\, dt = \\int f_{\\theta_1}(t) \\, dt = 1\\).\n\nSubstituting these back, we get: \\[\nW_{\\phi_0}(\\theta_1) \\ge W_{\\phi_0}(\\theta_0) \\cdot 1\n\\] Thus, \\(W_{\\phi_0}(\\theta)\\) is non-decreasing.\nSize Control\nFor the composite null \\(H_0: \\theta \\le \\theta_0\\), we require the size to be at most \\(\\alpha\\). Since \\(W_{\\phi_0}(\\theta)\\) is non-decreasing (from Step 1) and we explicitly set \\(W_{\\phi_0}(\\theta_0) = \\alpha\\):\n\\[\nW_{\\phi_0}(\\theta) \\le W_{\\phi_0}(\\theta_0) = \\alpha \\quad \\text{for all } \\theta \\le \\theta_0\n\\]\nThis confirms \\(\\phi_0\\) is a valid level-\\(\\alpha\\) test.\nUniformly Most Powerful (UMP) via Neyman-Pearson Lemma\nLet \\(\\phi'(x)\\) be any other valid test of size \\(\\alpha\\) for \\(H_0\\). This implies \\(W_{\\phi'}(\\theta_0) \\le \\alpha\\).\nConsider any specific alternative \\(\\theta_1 &gt; \\theta_0\\). Because the family has MLR, the likelihood ratio \\(\\Lambda(x)\\) is increasing in \\(T(x)\\). Therefore, the test \\(\\phi_0\\) (which rejects for large \\(T\\)) is identified by the Neyman-Pearson Lemma as the Most Powerful (MP) test for the simple hypothesis \\(\\theta_0\\) vs \\(\\theta_1\\).\nComparing the power of \\(\\phi_0\\) and \\(\\phi'\\) at this specific \\(\\theta_1\\): \\[\nW_{\\phi_0}(\\theta_1) \\ge W_{\\phi'}(\\theta_1)\n\\] Since \\(\\theta_1\\) was arbitrary, this inequality holds for all \\(\\theta &gt; \\theta_0\\). Thus, \\(\\phi_0\\) is the UMP test.\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: Visualizing Monotonicity of Power Function \\(W(\\theta)\\) for MLR Distributions\n\n\n\n\n\n\nExample 8.4 (UMP Test for Exponential/Gamma) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\text{Exp}(\\theta)\\) with pdf \\(f(x) = \\frac{1}{\\theta} e^{-x/\\theta}\\). Test \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\). The sum \\(T = \\sum X_i\\) is a sufficient statistic, and \\(T \\sim \\text{Gamma}(n, \\theta)\\).\nThe Likelihood Ratio for \\(\\theta_1 &gt; \\theta_0\\) is:\n\\[\n\\frac{L(\\theta_1)}{L(\\theta_0)} = \\frac{\\theta_1^{-n} e^{-\\sum x_i / \\theta_1}}{\\theta_0^{-n} e^{-\\sum x_i / \\theta_0}} = \\left(\\frac{\\theta_0}{\\theta_1}\\right)^n \\exp \\left\\{ \\left( \\frac{1}{\\theta_0} - \\frac{1}{\\theta_1} \\right) \\sum x_i \\right\\}\n\\]\nSince \\(\\theta_1 &gt; \\theta_0\\), the term \\((\\frac{1}{\\theta_0} - \\frac{1}{\\theta_1})\\) is positive. Thus, \\(\\Lambda(x)\\) is an increasing function of \\(\\sum x_i\\).\nRejecting for large \\(\\Lambda(x)\\) is equivalent to rejecting for \\(\\sum x_i &gt; C\\).\n\\(T \\sim \\text{Gamma}(n, \\theta)\\)\nThis test form does not depend on the specific \\(\\theta_1\\), so it is UMP for all \\(\\theta &gt; \\theta_0\\).\n\n\n\n8.4.2 Non-Existence of UMP for Two-Sided Hypotheses\nFor testing a point null hypothesis \\(H_0: \\theta = \\theta_0\\) against a two-sided alternative \\(H_1: \\theta \\neq \\theta_0\\) in a family with a monotone likelihood ratio (e.g., Normal, Exponential), a Uniformly Most Powerful (UMP) test generally does not exist. The non-existence proof relies on the uniqueness of the Most Powerful (MP) test derived from the Neyman-Pearson Lemma:\n\nConflict of Optimal Regions: Consider a specific alternative \\(\\theta_1 &gt; \\theta_0\\). By the Neyman-Pearson Lemma, the MP test \\(\\phi_1\\) rejects \\(H_0\\) for large values of the sufficient statistic \\(T(\\mathbf{X}) &gt; k_1\\). Conversely, consider an alternative \\(\\theta_2 &lt; \\theta_0\\). The MP test \\(\\phi_2\\) rejects \\(H_0\\) for small values of the statistic \\(T(\\mathbf{X}) &lt; k_2\\).\nFailure of Uniformity: A UMP test \\(\\phi^*\\) would need to be the MP test for every \\(\\theta \\in H_1\\).\n\nFor \\(\\phi^*\\) to be most powerful against \\(\\theta_1\\), it must be equivalent to \\(\\phi_1\\) (rejecting in the right tail).\nFor \\(\\phi^*\\) to be most powerful against \\(\\theta_2\\), it must be equivalent to \\(\\phi_2\\) (rejecting in the left tail).\n\nBiased Power Function: The MP test \\(\\phi_1\\) (Right-Sided) has a power function that drops below the size \\(\\alpha\\) for values \\(\\theta &lt; \\theta_0\\). Therefore, it cannot be the most powerful test for \\(\\theta_2\\), as there exists a valid test (e.g., \\(\\phi_2\\)) with power strictly greater than \\(\\alpha\\) at \\(\\theta_2\\).\n\nSince no single critical region can simultaneously maximize power for both \\(\\theta &gt; \\theta_0\\) and \\(\\theta &lt; \\theta_0\\), no UMP test exists. We typically restrict our search to Unbiased tests (UMPU) to resolve this.\n\n\n\n\n\n\n\n\nFigure 8.6: Illustration of Non-existence of UMP Tests for Two-sided Tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#wilks-theorem-for-general-likelihood-ratio-tests",
    "href": "hypothesis.html#wilks-theorem-for-general-likelihood-ratio-tests",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "8.5 Wilks’ Theorem for General Likelihood Ratio Tests",
    "text": "8.5 Wilks’ Theorem for General Likelihood Ratio Tests\nThe Likelihood Ratio Test (LRT) provides a unified approach for constructing hypothesis tests for parametric models. Let \\(X_1, \\dots, X_n\\) be a random sample with joint probability density function \\(L(\\theta; \\mathbf{x})\\), where \\(\\theta \\in \\Theta\\). We consider the general hypothesis testing problem: \\[H_0: \\theta \\in \\Theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\in \\Theta \\setminus \\Theta_0\\]\nThe Likelihood Ratio Statistic is defined as the ratio of the maximum likelihood achievable under the null hypothesis to the maximum likelihood achievable under the full parameter space:\n\\[\\Lambda(\\mathbf{x}) = \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta; \\mathbf{x})}{\\sup_{\\theta \\in \\Theta} L(\\theta; \\mathbf{x})} = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\\]\nwhere \\(\\hat{\\theta}_0\\) is the restricted MLE and \\(\\hat{\\theta}\\) is the unrestricted (global) MLE. By definition, the likelihood ratio is bounded by \\(0 \\le \\Lambda(\\mathbf{x}) \\le 1\\). The Likelihood Ratio Test (LRT) is defined by the decision function:\n\\[\n\\phi(\\mathbf{x}) = \\begin{cases}\n1 & \\text{if } \\Lambda(\\mathbf{x}) \\le c \\quad (\\text{Reject } H_0) \\\\\n0 & \\text{if } \\Lambda(\\mathbf{x}) &gt; c \\quad (\\text{Fail to reject } H_0)\n\\end{cases}\n\\]\nwhere \\(c \\in (0, 1)\\) is a critical value determined by the significance level \\(\\alpha\\). This rejection region \\(\\{\\mathbf{x} : \\Lambda(\\mathbf{x}) \\le c\\}\\) corresponds to cases where the restricted likelihood (under \\(H_0\\)) is significantly smaller than the unrestricted likelihood.\n\n8.5.1 Wilks’ Theorem\nThe asymptotic distribution of the Likelihood Ratio Test statistic is given by the celebrated result from Wilks (1938). This theorem allows us to construct hypothesis tests without deriving the exact distribution of \\(\\Lambda\\) for every specific model.\n\nTheorem 8.4 (Wilks’ Theorem) Let \\(X_1, \\dots, X_n\\) be an i.i.d. sample from a regular family of distributions with parameters \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^{p_1}\\). Consider the test: \\[ H_0: \\theta \\in \\Theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\in \\Theta \\setminus \\Theta_0 \\] where \\(\\Theta_0\\) is a smooth subset of \\(\\Theta\\) with dimension \\(p_0 &lt; p_1\\). Let \\(\\Lambda_n\\) be the likelihood ratio statistic. Under \\(H_0\\), as \\(n \\to \\infty\\): \\[ -2 \\ln \\Lambda_n \\xrightarrow{d} \\chi^2_{k} \\] where the degrees of freedom \\(k = p_1 - p_0\\) is the difference in the number of free parameters between the full model and the null model.\n\n\nProof. Sketch of Proof:\nThe proof relies on the Taylor expansion of the log-likelihood function \\(\\ell(\\theta)\\) around the unrestricted MLE \\(\\hat{\\theta}\\).\n\nQuadratic Approximation: For large \\(n\\), \\(\\hat{\\theta}\\) is close to the true parameter \\(\\theta_0\\). We expand \\(\\ell(\\theta_0)\\) around \\(\\hat{\\theta}\\): \\[ \\ell(\\theta_0) \\approx \\ell(\\hat{\\theta}) + (\\theta_0 - \\hat{\\theta})^T \\nabla \\ell(\\hat{\\theta}) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^T \\nabla^2 \\ell(\\hat{\\theta}) (\\theta_0 - \\hat{\\theta}) \\] Since \\(\\hat{\\theta}\\) is the MLE, the score vector \\(\\nabla \\ell(\\hat{\\theta}) = 0\\). The Hessian \\(\\nabla^2 \\ell(\\hat{\\theta}) \\approx -n I(\\theta_0)\\), where \\(I(\\theta_0)\\) is the Fisher Information matrix. Thus: \\[ 2[\\ell(\\hat{\\theta}) - \\ell(\\theta_0)] \\approx n(\\hat{\\theta} - \\theta_0)^T I(\\theta_0) (\\hat{\\theta} - \\theta_0) \\] From the asymptotic normality of the MLE, \\(\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, I(\\theta_0)^{-1})\\). Therefore, the quadratic form converges to a Chi-square variable with \\(p_1\\) degrees of freedom: \\[ 2[\\ell(\\hat{\\theta}) - \\ell(\\theta_0)] \\xrightarrow{d} \\chi^2_{p_1} \\]\nDecomposition of the Statistic: The LRT statistic is \\(D = -2 \\ln \\Lambda = 2[\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0)]\\). We can rewrite this by adding and subtracting the true log-likelihood \\(\\ell(\\theta_0)\\): \\[ 2[\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_0)] = \\underbrace{2[\\ell(\\hat{\\theta}) - \\ell(\\theta_0)]}_{\\approx \\chi^2_{p_1}} - \\underbrace{2[\\ell(\\hat{\\theta}_0) - \\ell(\\theta_0)]}_{\\approx \\chi^2_{p_0}} \\] The term \\(2[\\ell(\\hat{\\theta}_0) - \\ell(\\theta_0)]\\) represents the deviation of the restricted MLE from the truth. Since \\(\\Theta_0\\) has dimension \\(p_0\\), this term converges to \\(\\chi^2_{p_0}\\).\nResult via Cochran’s Theorem Logic: Geometrically, this is analogous to projecting the data vector onto orthogonal subspaces (like in ANOVA). The difference of two nested Chi-square variables (where one represents a subspace of the other) is itself Chi-square distributed with degrees of freedom equal to the difference in dimensions: \\[ D \\xrightarrow{d} \\chi^2_{p_1} - \\chi^2_{p_0} \\sim \\chi^2_{p_1 - p_0} = \\chi^2_k \\]\n\n\n\nExample 8.5 LRT for Normal Mean with Unknown Variance\nLet \\(X_1, \\dots, X_n \\overset{i.i.d.}{\\sim} N(\\mu, \\sigma^2)\\) where both \\(\\mu\\) and \\(\\sigma^2\\) are unknown. The parameter vector is \\(\\theta = (\\mu, \\sigma^2)\\). We wish to test: \\[H_0: \\mu = \\mu_0 \\quad \\text{vs} \\quad H_1: \\mu \\neq \\mu_0\\]\n\nFind the Maximum Likelihood Estimators\nThe log-likelihood function is \\(\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\).\n\nUnrestricted MLEs (\\(\\hat{\\theta}\\)): Maximizing over \\(\\mathbb{R} \\times (0, \\infty)\\) yields the sample mean and the biased sample variance: \\[\\hat{\\mu} = \\bar{X}, \\quad \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2\\]\nRestricted MLEs (\\(\\hat{\\theta}_0\\)): Under \\(H_0\\), \\(\\mu\\) is fixed at \\(\\mu_0\\). Maximizing with respect to \\(\\sigma^2\\) yields: \\[\\hat{\\mu}_0 = \\mu_0, \\quad \\hat{\\sigma}_0^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu_0)^2\\]\n\nConstruct the Likelihood Ratio \\(\\Lambda(\\mathbf{x})\\)\nEvaluating the likelihood function \\(L(\\theta) \\propto (\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum (x_i - \\mu)^2 \\right)\\) at the estimators: \\[L(\\hat{\\mu}, \\hat{\\sigma}^2) \\propto (\\hat{\\sigma}^2)^{-n/2} e^{-n/2}\\] \\[L(\\hat{\\mu}_0, \\hat{\\sigma}_0^2) \\propto (\\hat{\\sigma}_0^2)^{-n/2} e^{-n/2}\\]\nThe ratio simplifies to a function of the variances: \\[\\Lambda(\\mathbf{x}) = \\frac{(\\hat{\\sigma}_0^2)^{-n/2}}{(\\hat{\\sigma}^2)^{-n/2}} = \\left( \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}_0^2} \\right)^{n/2}\\]\nRelate to the t-statistic\nWe use the decomposition of the sum of squares \\(\\sum (X_i - \\mu_0)^2 = \\sum (X_i - \\bar{X})^2 + n(\\bar{X} - \\mu_0)^2\\). Dividing by \\(n\\) gives \\(\\hat{\\sigma}_0^2 = \\hat{\\sigma}^2 + (\\bar{X} - \\mu_0)^2\\).\nSubstituting this into the ratio: \\[\\Lambda(\\mathbf{x}) = \\left( \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2 + (\\bar{X} - \\mu_0)^2} \\right)^{n/2} = \\left( \\frac{1}{1 + \\frac{(\\bar{X} - \\mu_0)^2}{\\hat{\\sigma}^2}} \\right)^{n/2}\\]\nRecall the t-statistic \\(T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}\\), where \\(S^2 = \\frac{n}{n-1}\\hat{\\sigma}^2\\). The term in the denominator can be rewritten as: \\[\\frac{(\\bar{X} - \\mu_0)^2}{\\hat{\\sigma}^2} = \\frac{(\\bar{X} - \\mu_0)^2}{\\frac{n-1}{n} S^2} = \\frac{T^2}{n-1}\\]\nThus, the LRT statistic is a monotone function of \\(T^2\\): \\[\\Lambda(\\mathbf{x}) = \\left( 1 + \\frac{T^2}{n-1} \\right)^{-n/2}\\]\nRejecting \\(H_0\\) for small \\(\\Lambda\\) is equivalent to rejecting for large values of \\(T^2\\) (or \\(|T|\\)). Therefore, in this case, the LRT is equivalent to the classic Two-Sample t-test.\nAsymptotic Distribution (Wilks’ Theorem)\nAccording to Wilks’ Theorem, \\(-2 \\ln \\Lambda \\xrightarrow{d} \\chi^2_1\\). We can verify this relationship using the Taylor expansion \\(\\ln(1+x) \\approx x\\): \\[-2 \\ln \\Lambda = n \\ln\\left( 1 + \\frac{T^2}{n-1} \\right) \\approx n \\left( \\frac{T^2}{n-1} \\right) \\approx T^2\\] Since \\(T \\xrightarrow{d} Z\\) as \\(n \\to \\infty\\), \\(T^2 \\xrightarrow{d} \\chi^2_1\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#historical-notes-neyman-pearson-wilks-and-karlin-rubin",
    "href": "hypothesis.html#historical-notes-neyman-pearson-wilks-and-karlin-rubin",
    "title": "8  Likelihood-based Hypothesis Testing",
    "section": "8.6 Historical Notes: Neyman-Pearson, Wilks, and Karlin-Rubin",
    "text": "8.6 Historical Notes: Neyman-Pearson, Wilks, and Karlin-Rubin\n\nThe Foundation: Neyman & Pearson (1928, 1933)\nIn 1928, Jerzy Neyman and Egon Pearson published On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference. In this landmark paper, they introduced the likelihood ratio \\(\\lambda\\) as a general principle for constructing tests. However, at this stage, they relied on ad-hoc methods to find the distribution of \\(\\lambda\\) for specific cases (like the t-test or F-test) and merely conjectured that it might follow a Chi-square distribution in the limit.\nIn 1933, they published their most famous work, On the Problem of the Most Efficient Tests of Statistical Hypotheses. This paper established the Neyman-Pearson Lemma, proving that for testing a simple null against a simple alternative (no unknown nuisance parameters), the Likelihood Ratio test is Uniformly Most Powerful (UMP).\nThe Gap: The Problem of Composite Hypotheses\nWhile Neyman and Pearson proved that the LR test was “optimal” for simple cases, two major problems remained for composite hypotheses (cases where parameters are free or tested as inequalities, e.g., \\(H_0: \\theta \\le \\theta_0\\)):\n\nDistributional: How do we find critical values when nuisance parameters are present?\nOptimality: Does a UMP test even exist when the alternative is not a single point?\n\nThe Asymptotic Solution: S.S. Wilks (1938)\nFive years after the NP Lemma, Samuel S. Wilks published The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses (1938).\nWilks, a student of Henryietz Rietz influenced by Neyman and Pearson, provided the general solution for the distributional problem. He proved mathematically that as \\(n \\to \\infty\\), the distribution of \\(-2 \\ln \\Lambda\\) does not depend on the specific form of the underlying density, but purely on the number of free parameters (\\(k = p_1 - p_0\\)). This made the LRT universally applicable for large samples.\nThe Finite-Sample Optimality: Karlin-Rubin (1956)\nWhile Wilks handled the asymptotics, the question of exact optimality for composite hypotheses remained tricky. In 1956, Samuel Karlin and Herman Rubin formalized the condition of Monotone Likelihood Ratio (MLR).\nThe Karlin-Rubin Theorem extended the logic of the Neyman-Pearson Lemma to one-sided composite hypotheses (\\(H_0: \\theta \\le \\theta_0\\) vs \\(H_1: \\theta &gt; \\theta_0\\)). They proved that if the family of distributions possesses the MLR property (e.g., Normal, Exponential, Binomial), then the test based on the sufficient statistic is indeed UMP. This result completed the theoretical foundation, delineating exactly when a “perfect” test exists for finite samples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Likelihood-based Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "",
    "text": "6.1 Definitions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#chapter-8-likelihood-theory",
    "href": "mle.html#chapter-8-likelihood-theory",
    "title": "7  Likelihood Theory and Maximum Likelihood Estimation",
    "section": "",
    "text": "7.1.1 Definitions\n\nLikelihood Function Let \\(f(x|\\theta)\\) be the probability density function (or mass function). The likelihood function is: \\[\nL(\\theta; x) = f(x|\\theta)\n\\]\nLog-likelihood \\[\nl(\\theta; x) = \\log L(\\theta; x) = \\log f(x|\\theta)\n\\]\nScore Function The score function is the derivative of the log-likelihood with respect to the parameter \\(\\theta\\): \\[\nS(\\theta; x) = \\frac{\\partial}{\\partial \\theta} l(\\theta; x) = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta; x)\n\\]\nMaximum Likelihood Estimator (MLE) The MLE is the value that maximizes the likelihood function: \\[\n\\hat{\\theta}_{\\text{MLE}}(x) = \\operatorname{\\text{argmax}}_{\\theta} L(\\theta; x) = \\operatorname{\\text{argmax}}_{\\theta} l(\\theta; x)\n\\] An approach to finding \\(\\hat{\\theta}\\) is to solve the score equation: \\[\n\\forall_{\\theta}, \\quad S(\\theta; x) = 0\n\\]\n\n\nExample 7.1 (Uniform Distribution MLE) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{\\text{Unif}}(0, \\theta)\\).\nThe likelihood function is: \\[\nL(\\theta; x) = \\prod_{i=1}^{n} f(x_i | \\theta) = \\frac{1}{\\theta^n} I(X_{(n)} &lt; \\theta)\n\\] where \\(X_{(n)} = \\max\\{X_1, \\dots, X_n\\}\\).\nTo maximize this function, we observe that \\(L(\\theta)\\) decreases as \\(\\theta\\) increases, but \\(\\theta\\) must be at least \\(X_{(n)}\\). Therefore: \\[\n\\hat{\\theta}_{\\text{MLE}}(x) = X_{(n)}\n\\]\nProperties of this estimator: The CDF of \\(X_{(n)}\\) is: \\[\nP(X_{(n)} \\le x) = [P(X_1 \\le x)]^n = \\left(\\frac{x}{\\theta}\\right)^n \\quad \\text{for } 0 &lt; x &lt; \\theta\n\\] The PDF is \\(f_{X_{(n)}}(x) = n \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta}\\).\nThe expected value is: \\[\nE(X_{(n)}) = \\int_{0}^{\\theta} x \\cdot \\frac{n x^{n-1}}{\\theta^n} dx = \\frac{n}{\\theta^n} \\int_{0}^{\\theta} x^n dx = \\frac{n}{\\theta^n} \\left[ \\frac{x^{n+1}}{n+1} \\right]_0^{\\theta} = \\frac{n}{n+1}\\theta &lt; \\theta\n\\] Thus, it is a biased estimator.\n\n\nExample 7.2 (Normal Distribution MLE) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Let \\(\\theta = (\\mu, \\sigma^2)\\).\nThe likelihood is: \\[\nL(\\theta; x) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2} \\right)\n\\] The log-likelihood is: \\[\nl(\\theta; x) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2}\n\\]\nScore Functions:\n\nWith respect to \\(\\mu\\): \\[\n\\frac{\\partial l}{\\partial \\mu} = \\frac{2\\sum(x_i - \\mu)}{2\\sigma^2} = \\frac{\\sum(x_i - \\mu)}{\\sigma^2} = 0 \\implies \\hat{\\mu}_{\\text{MLE}} = \\bar{x}\n\\]\nWith respect to \\(\\sigma^2\\): \\[\n\\frac{\\partial l}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\sum(x_i - \\mu)^2}{2(\\sigma^2)^2} = 0\n\\] Solving for \\(\\sigma^2\\): \\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\sum(x_i - \\hat{\\mu})^2}{n} = \\frac{\\sum(x_i - \\bar{x})^2}{n}\n\\]\n\n\n\n\n7.1.2 Comparison of Variance Estimators\nConsider the sample variance \\(S^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\). We know that \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\).\n\nBias:\n\n\\(E(S^2) = \\sigma^2\\) (Unbiased)\n\\(E(\\hat{\\sigma}^2_{\\text{MLE}}) = E\\left(\\frac{n-1}{n} S^2\\right) = \\frac{n-1}{n} \\sigma^2 \\ne \\sigma^2\\) (Biased)\n\nMSE Comparison: \\[\n\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2\n\\]\nFor \\(\\hat{\\sigma}^2_{\\text{MLE}}\\): \\[\n\\begin{aligned}\n\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}}) &= Var\\left(\\frac{n-1}{n} S^2\\right) + \\left( \\frac{n-1}{n}\\sigma^2 - \\sigma^2 \\right)^2 \\\\\n&= \\left(\\frac{n-1}{n}\\right)^2 \\frac{2\\sigma^4}{n-1} + \\left( -\\frac{1}{n}\\sigma^2 \\right)^2 \\\\\n&= \\frac{2(n-1)\\sigma^4}{n^2} + \\frac{\\sigma^4}{n^2} \\\\\n&= \\frac{2n - 2 + 1}{n^2}\\sigma^4 = \\frac{2n-1}{n^2}\\sigma^4\n\\end{aligned}\n\\]\nFor \\(S^2\\): \\[\n\\text{MSE}(S^2) = \\text{Var}(S^2) = \\frac{2\\sigma^4}{n-1}\n\\]\nComparing the two: \\[\n\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\left(\\frac{2}{n} - \\frac{1}{n^2}\\right)\\sigma^4 &lt; \\frac{2}{n-1}\\sigma^4 = \\text{MSE}(S^2)\n\\] The MLE has a smaller Mean Squared Error despite being biased.\n\n\n\n7.1.3 Fisher Information\n\nDefinition 7.1 (Fisher Information) Some properties about the Score function \\(S(\\theta; x)\\):\n\nMean: \\(E[S(\\theta; x) | \\theta] = 0\\).\nVariance/Covariance: \\[\n    \\text{Cov}(S_i(\\theta; x), S_j(\\theta; x)) = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n    \\] The Fisher Information matrix \\(I(\\theta)\\) is defined as: \\[\n    I(\\theta) = \\text{Cov}(S(\\theta; x)) = E[S(\\theta; x) S(\\theta; x)^T] = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta^2} \\right]\n    \\] Note: \\(J(\\theta, x) = -\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_k} l(\\theta; x)\\) is the Observed Fisher Information. \\(I(\\theta) = E[J(\\theta, x)]\\).\n\n\n\nTheorem 7.1 (Properties of Score Function) Given the support of \\(X\\) is free of \\(\\theta\\):\n\n\\(E_X[S(\\theta; x)] = 0\\)\n\\(\\text{Cov}(S(\\theta; x)) = I(\\theta)\\)\n\n\n\nProof. Proof of Mean 0: \\[\n\\begin{aligned}\nE[S(\\theta; X)] &= \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx \\quad \\text{(assuming regularity conditions allow interchange)} \\\\\n&= \\frac{\\partial}{\\partial \\theta} (1) = 0\n\\end{aligned}\n\\]\nProof of Variance: Differentiating \\(\\int f(x|\\theta) dx = 1\\) twice with respect to \\(\\theta\\) leads to the identity: \\[\n\\text{Var}(S(\\theta)) = E\\left[ \\left(\\frac{\\partial l}{\\partial \\theta}\\right)^2 \\right] = -E\\left[ \\frac{\\partial^2 l}{\\partial \\theta^2} \\right]\n\\]\n\n\nRemark 7.1. If \\(X_1, \\dots, X_n\\) are i.i.d with \\(f(x|\\theta)\\), then: \\[\nl(\\theta; x) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\] The score function is the sum of individual score functions. Since variance of a sum of independent variables is the sum of variances: \\[\nI_n(\\theta) = n I_1(\\theta)\n\\] where \\(I_1(\\theta) = E\\left[ -\\frac{\\partial^2}{\\partial \\theta^2} \\log f(X_1|\\theta) \\right]\\).\n\n\n\n7.1.4 Cramer-Rao Lower Bound (CRLB)\n\nTheorem 7.2 (Cramer-Rao Lower Bound) Let \\(W(X)\\) be any estimator with \\(m(\\theta) = E[W(X)]\\). Under regularity conditions (support of X independent of \\(\\theta\\)), \\[\n\\text{Var}(W(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n\\] Particular Case: If \\(W(X)\\) is an unbiased estimator of \\(\\theta\\) (i.e., \\(m(\\theta) = \\theta, m'(\\theta)=1\\)), then: \\[\n\\text{Var}(W(X)) \\ge \\frac{1}{I(\\theta)}\n\\]\n\n\nProof. Let \\(Z = S(\\theta; X)\\). We know \\(E[Z] = 0\\) and \\(\\text{Var}(Z) = I(\\theta)\\). Using the Covariance inequality: \\[\n[\\text{Cov}(W, Z)]^2 \\le \\text{Var}(W) \\text{Var}(Z)\n\\] Evaluate the covariance: \\[\n\\begin{aligned}\n\\text{Cov}(W, Z) &= E[W(X) S(\\theta; X)] - E[W]E[S] \\\\\n&= \\int w(x) \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx - 0 \\\\\n&= \\int w(x) \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int w(x) f(x|\\theta) dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} m(\\theta) = m'(\\theta)\n\\end{aligned}\n\\] Therefore: \\[\n[m'(\\theta)]^2 \\le \\text{Var}(W) \\cdot I(\\theta) \\implies \\text{Var}(W) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n\\]\n\n\nExample 7.3 Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Exp}(\\theta)\\), where \\(f(x|\\theta) = \\frac{1}{\\theta} e^{-x/\\theta}\\). Log-likelihood: \\[\nl(\\theta; x) = -n \\log \\theta - \\frac{1}{\\theta} \\sum x_i\n\\] Score function: \\[\nS(\\theta; x) = -\\frac{n}{\\theta} + \\frac{\\sum x_i}{\\theta^2}\n\\] Setting \\(S=0 \\implies \\hat{\\theta}_{\\text{MLE}} = \\bar{x}\\).\nFisher Information: \\[\nI(\\theta) = \\text{Var}(S) = \\frac{1}{\\theta^4} \\text{Var}(\\sum X_i) = \\frac{1}{\\theta^4} n \\theta^2 = \\frac{n}{\\theta^2}\n\\] Alternatively using the second derivative: \\[\nS'(\\theta) = \\frac{n}{\\theta^2} - \\frac{2\\sum x_i}{\\theta^3}\n\\] \\[\n-E[S'] = -\\left( \\frac{n}{\\theta^2} - \\frac{2 n \\theta}{\\theta^3} \\right) = -\\left( \\frac{n}{\\theta^2} - \\frac{2n}{\\theta^2} \\right) = \\frac{n}{\\theta^2}\n\\]\nVariance of MLE: \\[\n\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\theta^2}{n}\n\\] CRLB for unbiased estimator: \\[\n\\text{CRLB} = \\frac{1}{I(\\theta)} = \\frac{1}{n/\\theta^2} = \\frac{\\theta^2}{n}\n\\] Since \\(\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{CRLB}\\), the MLE is efficient.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Likelihood Theory and Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#asymptotic-properties-of-mle",
    "href": "mle.html#asymptotic-properties-of-mle",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.4 Asymptotic Properties of MLE",
    "text": "6.4 Asymptotic Properties of MLE\nThere are three main asymptotic properties:\n\nConsistency: \\(\\hat{\\theta}_n \\xrightarrow{p} \\theta_0\\)\nAsymptotic Normality: \\(\\hat{\\theta}_n \\sim N(\\theta_0, 1/I(\\theta_0))\\) roughly.\nEfficiency: Variance achieves CRLB asymptotically.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#hypothesis-testing-likelihood-ratio-test",
    "href": "mle.html#hypothesis-testing-likelihood-ratio-test",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.8 Hypothesis Testing: Likelihood Ratio Test",
    "text": "6.8 Hypothesis Testing: Likelihood Ratio Test\nConsider testing: \\[\nH_0: \\theta \\in \\Theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\in \\Theta \\setminus \\Theta_0\n\\] where \\(\\dim(\\Theta) = p\\) and \\(\\dim(\\Theta_0) = p-m\\).\nThe Likelihood Ratio Statistic is: \\[\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta; x)}{\\sup_{\\theta \\in \\Theta} L(\\theta; x)} = \\frac{L(\\hat{\\theta}_0)}{L(\\hat{\\theta})}\n\\]\n\nTheorem 6.5 (Wilks’ Theorem) Under regularity conditions, under \\(H_0\\): \\[\n-2 \\log \\Lambda \\xrightarrow{d} \\chi^2_m\n\\] where \\(m\\) is the difference in dimensions (number of restrictions).\n\n\nExample 6.5 Let \\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\). \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\ne \\mu_0\\). Here \\(p=2\\) (\\(\\mu, \\sigma^2\\)) and under \\(H_0\\), free parameters = 1 (\\(\\sigma^2\\)). So \\(m = 2-1 = 1\\).\nThe statistic \\(\\Lambda\\): \\[\n\\Lambda = \\left( \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}_0^2} \\right)^{n/2} = \\left( \\frac{\\sum(x_i - \\bar{x})^2}{\\sum(x_i - \\mu_0)^2} \\right)^{n/2}\n\\] It can be shown that: \\[\n\\Lambda = \\left( 1 + \\frac{(\\bar{x} - \\mu_0)^2}{\\hat{\\sigma}^2} \\right)^{-n/2}\n\\] The rejection region \\(-2 \\log \\Lambda &gt; \\chi^2_{1, \\alpha}\\) is equivalent to the t-test: \\[\n\\left| \\frac{\\bar{x} - \\mu_0}{S/\\sqrt{n}} \\right| &gt; t_{n-1, \\alpha/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#definitions",
    "href": "mle.html#definitions",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "",
    "text": "Likelihood Function Let \\(f(x|\\theta)\\) be the probability density function (or mass function). The likelihood function is: \\[\nL(\\theta; x) = f(x|\\theta)\n\\]\nLog-likelihood \\[\nl(\\theta; x) = \\log L(\\theta; x) = \\log f(x|\\theta)\n\\]\nScore Function The score function is the derivative of the log-likelihood with respect to the parameter \\(\\theta\\): \\[\nS(\\theta; x) = \\frac{\\partial}{\\partial \\theta} l(\\theta; x) = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta; x)\n\\]\nMaximum Likelihood Estimator (MLE) The MLE is the value that maximizes the likelihood function: \\[\n\\hat{\\theta}_{\\text{MLE}}(x) = \\operatorname{\\text{argmax}}_{\\theta} L(\\theta; x) = \\operatorname{\\text{argmax}}_{\\theta} l(\\theta; x)\n\\] An approach to finding \\(\\hat{\\theta}\\) is to solve the score equation: \\[\n\\forall_{\\theta}, \\quad S(\\theta; x) = 0\n\\]\n\n\nExample 6.1 (Uniform Distribution MLE) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{\\text{Unif}}(0, \\theta)\\).\nThe likelihood function is: \\[\nL(\\theta; x) = \\prod_{i=1}^{n} f(x_i | \\theta) = \\frac{1}{\\theta^n} I(X_{(n)} &lt; \\theta)\n\\] where \\(X_{(n)} = \\max\\{X_1, \\dots, X_n\\}\\).\nTo maximize this function, we observe that \\(L(\\theta)\\) decreases as \\(\\theta\\) increases, but \\(\\theta\\) must be at least \\(X_{(n)}\\). Therefore: \\[\n\\hat{\\theta}_{\\text{MLE}}(x) = X_{(n)}\n\\]\nProperties of this estimator: The CDF of \\(X_{(n)}\\) is: \\[\nP(X_{(n)} \\le x) = [P(X_1 \\le x)]^n = \\left(\\frac{x}{\\theta}\\right)^n \\quad \\text{for } 0 &lt; x &lt; \\theta\n\\] The PDF is \\(f_{X_{(n)}}(x) = n \\left(\\frac{x}{\\theta}\\right)^{n-1} \\frac{1}{\\theta}\\).\nThe expected value is: \\[\nE(X_{(n)}) = \\int_{0}^{\\theta} x \\cdot \\frac{n x^{n-1}}{\\theta^n} dx = \\frac{n}{\\theta^n} \\int_{0}^{\\theta} x^n dx = \\frac{n}{\\theta^n} \\left[ \\frac{x^{n+1}}{n+1} \\right]_0^{\\theta} = \\frac{n}{n+1}\\theta &lt; \\theta\n\\] Thus, it is a biased estimator.\n\n\nExample 6.2 (Normal Distribution MLE) Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Let \\(\\theta = (\\mu, \\sigma^2)\\).\nThe likelihood is: \\[\nL(\\theta; x) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2} \\right)\n\\] The log-likelihood is: \\[\nl(\\theta; x) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{\\sum(x_i - \\mu)^2}{2\\sigma^2}\n\\]\nScore Functions:\n\nWith respect to \\(\\mu\\): \\[\n\\frac{\\partial l}{\\partial \\mu} = \\frac{2\\sum(x_i - \\mu)}{2\\sigma^2} = \\frac{\\sum(x_i - \\mu)}{\\sigma^2} = 0 \\implies \\hat{\\mu}_{\\text{MLE}} = \\bar{x}\n\\]\nWith respect to \\(\\sigma^2\\): \\[\n\\frac{\\partial l}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\sum(x_i - \\mu)^2}{2(\\sigma^2)^2} = 0\n\\] Solving for \\(\\sigma^2\\): \\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\sum(x_i - \\hat{\\mu})^2}{n} = \\frac{\\sum(x_i - \\bar{x})^2}{n}\n\\] Bias:\n\n\\(E(S^2) = \\sigma^2\\) (Unbiased)\n\\(E(\\hat{\\sigma}^2_{\\text{MLE}}) = E\\left(\\frac{n-1}{n} S^2\\right) = \\frac{n-1}{n} \\sigma^2 \\ne \\sigma^2\\) (Biased)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#comparison-of-variance-estimators",
    "href": "mle.html#comparison-of-variance-estimators",
    "title": "7  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "7.2 Comparison of Variance Estimators",
    "text": "7.2 Comparison of Variance Estimators\nConsider the sample variance \\(S^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\). We know that \\(\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\).\n\nBias:\n\n\\(E(S^2) = \\sigma^2\\) (Unbiased)\n\\(E(\\hat{\\sigma}^2_{\\text{MLE}}) = E\\left(\\frac{n-1}{n} S^2\\right) = \\frac{n-1}{n} \\sigma^2 \\ne \\sigma^2\\) (Biased)\n\nMSE Comparison: \\[\n\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2\n\\]\nFor \\(\\hat{\\sigma}^2_{\\text{MLE}}\\): \\[\n\\begin{aligned}\n\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}}) &= Var\\left(\\frac{n-1}{n} S^2\\right) + \\left( \\frac{n-1}{n}\\sigma^2 - \\sigma^2 \\right)^2 \\\\\n&= \\left(\\frac{n-1}{n}\\right)^2 \\frac{2\\sigma^4}{n-1} + \\left( -\\frac{1}{n}\\sigma^2 \\right)^2 \\\\\n&= \\frac{2(n-1)\\sigma^4}{n^2} + \\frac{\\sigma^4}{n^2} \\\\\n&= \\frac{2n - 2 + 1}{n^2}\\sigma^4 = \\frac{2n-1}{n^2}\\sigma^4\n\\end{aligned}\n\\]\nFor \\(S^2\\): \\[\n\\text{MSE}(S^2) = \\text{Var}(S^2) = \\frac{2\\sigma^4}{n-1}\n\\]\nComparing the two: \\[\n\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\left(\\frac{2}{n} - \\frac{1}{n^2}\\right)\\sigma^4 &lt; \\frac{2}{n-1}\\sigma^4 = \\text{MSE}(S^2)\n\\] The MLE has a smaller Mean Squared Error despite being biased.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#fisher-information",
    "href": "mle.html#fisher-information",
    "title": "7  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "7.2 Fisher Information",
    "text": "7.2 Fisher Information\n\nDefinition 7.1 (Definition of Fisher Information) Some properties about the Score function \\(S(\\theta; x)\\):\n\nMean: \\(E[S(\\theta; x) | \\theta] = 0\\).\nVariance/Covariance: \\[\n    \\text{Cov}(S_i(\\theta; x), S_j(\\theta; x)) = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n    \\] The Fisher Information matrix \\(I(\\theta)\\) is defined as: \\[\n    I(\\theta) = \\text{Cov}(S(\\theta; x)) = E[S(\\theta; x) S(\\theta; x)^T] = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta^2} \\right]\n    \\] Note: \\(J(\\theta, x) = -\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_k} l(\\theta; x)\\) is the Observed Fisher Information. \\(I(\\theta) = E[J(\\theta, x)]\\).\n\n\n\nTheorem 7.1 (Properties of Score Function) Given the support of \\(X\\) is free of \\(\\theta\\):\n\n\\(E_X[S(\\theta; x)] = 0\\)\n\\(\\text{Cov}(S(\\theta; x)) = I(\\theta)\\)\n\n\n\nProof. Proof of Mean 0: \\[\n\\begin{aligned}\nE[S(\\theta; X)] &= \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx \\quad \\text{(assuming regularity conditions allow interchange)} \\\\\n&= \\frac{\\partial}{\\partial \\theta} (1) = 0\n\\end{aligned}\n\\]\nProof of Variance: Differentiating \\(\\int f(x|\\theta) dx = 1\\) twice with respect to \\(\\theta\\) leads to the identity: \\[\n\\text{Var}(S(\\theta)) = E\\left[ \\left(\\frac{\\partial l}{\\partial \\theta}\\right)^2 \\right] = -E\\left[ \\frac{\\partial^2 l}{\\partial \\theta^2} \\right]\n\\]\n\n\nRemark 7.1. If \\(X_1, \\dots, X_n\\) are i.i.d with \\(f(x|\\theta)\\), then: \\[\nl(\\theta; x) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\] The score function is the sum of individual score functions. Since variance of a sum of independent variables is the sum of variances: \\[\nI_n(\\theta) = n I_1(\\theta)\n\\] where \\(I_1(\\theta) = E\\left[ -\\frac{\\partial^2}{\\partial \\theta^2} \\log f(X_1|\\theta) \\right]\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#cramer-rao-lower-bound-crlb",
    "href": "mle.html#cramer-rao-lower-bound-crlb",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.3 Cramer-Rao Lower Bound (CRLB)",
    "text": "6.3 Cramer-Rao Lower Bound (CRLB)\n\nTheorem 6.2 (Cramer-Rao Lower Bound) Let \\(W(X)\\) be any estimator with \\(m(\\theta) = E[W(X)]\\). Under regularity conditions (support of X independent of \\(\\theta\\)), \\[\n\\text{Var}(W(X)) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n\\] Particular Case: If \\(W(X)\\) is an unbiased estimator of \\(\\theta\\) (i.e., \\(m(\\theta) = \\theta, m'(\\theta)=1\\)), then: \\[\n\\text{Var}(W(X)) \\ge \\frac{1}{I(\\theta)}\n\\]\n\n\nProof. Let \\(Z = S(\\theta; X)\\). We know \\(E[Z] = 0\\) and \\(\\text{Var}(Z) = I(\\theta)\\). Using the Covariance inequality: \\[\n[\\text{Cov}(W, Z)]^2 \\le \\text{Var}(W) \\text{Var}(Z)\n\\] Evaluate the covariance: \\[\n\\begin{aligned}\n\\text{Cov}(W, Z) &= E[W(X) S(\\theta; X)] - E[W]E[S] \\\\\n&= \\int w(x) \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx - 0 \\\\\n&= \\int w(x) \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int w(x) f(x|\\theta) dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} m(\\theta) = m'(\\theta)\n\\end{aligned}\n\\] Therefore: \\[\n[m'(\\theta)]^2 \\le \\text{Var}(W) \\cdot I(\\theta) \\implies \\text{Var}(W) \\ge \\frac{[m'(\\theta)]^2}{I(\\theta)}\n\\]\n\n\nExample 6.3 Let \\(X_1, \\dots, X_n \\overset{iid}{\\sim} \\operatorname{Exp}(\\theta)\\), where \\(f(x|\\theta) = \\frac{1}{\\theta} e^{-x/\\theta}\\). Log-likelihood: \\[\nl(\\theta; x) = -n \\log \\theta - \\frac{1}{\\theta} \\sum x_i\n\\] Score function: \\[\nS(\\theta; x) = -\\frac{n}{\\theta} + \\frac{\\sum x_i}{\\theta^2}\n\\] Setting \\(S=0 \\implies \\hat{\\theta}_{\\text{MLE}} = \\bar{x}\\).\nFisher Information: \\[\nI(\\theta) = \\text{Var}(S) = \\frac{1}{\\theta^4} \\text{Var}(\\sum X_i) = \\frac{1}{\\theta^4} n \\theta^2 = \\frac{n}{\\theta^2}\n\\] Alternatively using the second derivative: \\[\nS'(\\theta) = \\frac{n}{\\theta^2} - \\frac{2\\sum x_i}{\\theta^3}\n\\] \\[\n-E[S'] = -\\left( \\frac{n}{\\theta^2} - \\frac{2 n \\theta}{\\theta^3} \\right) = -\\left( \\frac{n}{\\theta^2} - \\frac{2n}{\\theta^2} \\right) = \\frac{n}{\\theta^2}\n\\]\nVariance of MLE: \\[\n\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\theta^2}{n}\n\\] CRLB for unbiased estimator: \\[\n\\text{CRLB} = \\frac{1}{I(\\theta)} = \\frac{1}{n/\\theta^2} = \\frac{\\theta^2}{n}\n\\] Since \\(\\text{Var}(\\hat{\\theta}_{\\text{MLE}}) = \\text{CRLB}\\), the MLE is efficient.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#review-of-convergence",
    "href": "mle.html#review-of-convergence",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.5 Review of Convergence",
    "text": "6.5 Review of Convergence\n\nLLN (Law of Large Numbers): \\(\\bar{X} \\xrightarrow{p} E(X)\\).\nCLT (Central Limit Theorem): \\(\\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\).\nSlutsky’s Theorem: If \\(X_n \\xrightarrow{d} X\\) and \\(Y_n \\xrightarrow{p} a\\) (constant), then: * \\(X_n + Y_n \\xrightarrow{d} X + a\\) * \\(X_n Y_n \\xrightarrow{d} aX\\) * \\(X_n / Y_n \\xrightarrow{d} X/a\\)\n\n\nExample 6.4 Let \\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\). \\(\\hat{\\sigma}^2 = \\frac{\\sum(X_i - \\bar{X})^2}{n}\\). We can show: \\[\n\\sqrt{n}(\\hat{\\sigma}^2 - \\sigma^2) \\xrightarrow{d} N(0, 2\\sigma^4)\n\\] Using CLT on \\(Y_i = (X_i - \\mu)^2\\) and Slutsky’s theorem.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#consistency",
    "href": "mle.html#consistency",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.6 Consistency",
    "text": "6.6 Consistency\n\nTheorem 6.3 (Consistency) Under regularity conditions, let \\(\\theta_0\\) be the true parameter. Then \\(\\hat{\\theta}_n \\xrightarrow{p} \\theta_0\\).\n\nProof Idea: \\(\\hat{\\theta}_n\\) maximizes \\(\\frac{1}{n} l(\\theta; x)\\). By LLN, \\(\\frac{1}{n} l(\\theta; x) \\xrightarrow{p} E[\\log f(X|\\theta)]\\). We compare the expected log-likelihood at \\(\\theta\\) vs \\(\\theta_0\\): \\[\nE\\left[ \\log \\frac{f(X|\\theta)}{f(X|\\theta_0)} \\right] \\le \\log E\\left[ \\frac{f(X|\\theta)}{f(X|\\theta_0)} \\right] \\quad \\text{(Jensen's Inequality)}\n\\] \\[\n= \\log \\int f(x|\\theta_0) \\frac{f(x|\\theta)}{f(x|\\theta_0)} dx = \\log \\int f(x|\\theta) dx = \\log(1) = 0\n\\] Thus \\(E[\\log f(X|\\theta)]\\) is maximized at \\(\\theta = \\theta_0\\).\n\n\n\n\n\n\n\n\nFigure 6.1: Log-likelihood function maximized at MLE",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#asymptotic-normality",
    "href": "mle.html#asymptotic-normality",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.7 Asymptotic Normality",
    "text": "6.7 Asymptotic Normality\n\nTheorem 6.4 (Asymptotic Normality of MLE) Under regularity conditions:\n\nSupport of \\(f(x|\\theta)\\) does not depend on \\(\\theta\\).\nLikelihood is twice continuously differentiable.\nFisher Information exists and is positive.\n\nThen: \\[\n\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} N\\left(0, \\frac{1}{I_1(\\theta_0)}\\right)\n\\]\n\n\nProof. Taylor Expansion Method: Expand the score function \\(l'(\\theta)\\) around the true parameter \\(\\theta_0\\): \\[\nl'(\\hat{\\theta}_n) = l'(\\theta_0) + (\\hat{\\theta}_n - \\theta_0) l''(\\theta_n^*)\n\\] where \\(\\theta_n^*\\) lies between \\(\\hat{\\theta}_n\\) and \\(\\theta_0\\). Since \\(\\hat{\\theta}_n\\) is the MLE, \\(l'(\\hat{\\theta}_n) = 0\\). \\[\n0 = l'(\\theta_0) + (\\hat{\\theta}_n - \\theta_0) l''(\\theta_n^*)\n\\] Rearranging: \\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) = \\frac{-\\frac{1}{\\sqrt{n}} l'(\\theta_0)}{\\frac{1}{n} l''(\\theta_n^*)}\n\\] We analyze the numerator and denominator:\n\nNumerator: \\(E[l'(\\theta_0)] = 0\\), \\(\\text{Var}(l'(\\theta_0)) = n I_1(\\theta_0)\\). By CLT: \\(\\frac{1}{\\sqrt{n}} l'(\\theta_0) \\xrightarrow{d} N(0, I_1(\\theta_0))\\).\nDenominator: By LLN and Consistency, \\(\\frac{1}{n} l''(\\theta_n^*) \\xrightarrow{p} E[l''(\\theta_0)] = -I_1(\\theta_0)\\).\n\nCombining via Slutsky’s Theorem: \\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} \\frac{N(0, I_1(\\theta_0))}{I_1(\\theta_0)} \\sim N\\left(0, \\frac{1}{I_1(\\theta_0)}\\right)\n\\]\n\n\nRemark 6.2. For a vector parameter \\(\\theta \\in \\mathbb{R}^p\\): \\[\n\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} N_p(0, I(\\theta_0)^{-1})\n\\] where \\(I(\\theta_0)\\) is the Fisher Information Matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#illustration-of-wilks-theorem",
    "href": "mle.html#illustration-of-wilks-theorem",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.9 Illustration of Wilks’ Theorem",
    "text": "6.9 Illustration of Wilks’ Theorem\nWe can approximate the statistic using Taylor expansion. Consider the scalar case. \\[\n-2 \\log \\Lambda = 2 [l(\\hat{\\theta}) - l(\\theta_0)]\n\\]\n\n\n\n\n\n\n\n\nFigure 6.2: Approximation of Likelihood Ratio\n\n\n\n\n\nUsing Taylor expansion around the MLE \\(\\hat{\\theta}\\): \\[\nl(\\theta_0) \\approx l(\\hat{\\theta}) + (\\theta_0 - \\hat{\\theta})l'(\\hat{\\theta}) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta})\n\\] Since \\(l'(\\hat{\\theta}) = 0\\): \\[\n2[l(\\hat{\\theta}) - l(\\theta_0)] \\approx -(\\theta_0 - \\hat{\\theta})^2 l''(\\hat{\\theta}) = (\\hat{\\theta} - \\theta_0)^2 [-\\frac{1}{n} l''(\\hat{\\theta})] \\cdot n\n\\] As \\(n \\to \\infty\\), \\(-\\frac{1}{n}l'' \\to I(\\theta_0)\\) and \\(\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\to N(0, 1/I)\\). Thus, the expression behaves like \\(Z^2 \\sim \\chi^2_1\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#properties-of-score-and-fisher-information",
    "href": "mle.html#properties-of-score-and-fisher-information",
    "title": "6  Maximum Likelihood Estimation and Likelihood Theory",
    "section": "6.2 Properties of Score and Fisher Information",
    "text": "6.2 Properties of Score and Fisher Information\n\nDefinition 6.1 (Definition of Fisher Information) Some properties about the Score function \\(S(\\theta; x)\\):\n\nMean: \\(E[S(\\theta; x) | \\theta] = 0\\).\nVariance/Covariance: \\[\n    \\text{Cov}(S_i(\\theta; x), S_j(\\theta; x)) = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta_i \\partial \\theta_j} \\right]\n    \\] The Fisher Information matrix \\(I(\\theta)\\) is defined as: \\[\n    I(\\theta) = \\text{Cov}(S(\\theta; x)) = E[S(\\theta; x) S(\\theta; x)^T] = -E\\left[ \\frac{\\partial^2 l(\\theta; x)}{\\partial \\theta^2} \\right]\n    \\] Note: \\(J(\\theta, x) = -\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_k} l(\\theta; x)\\) is the Observed Fisher Information. \\(I(\\theta) = E[J(\\theta, x)]\\).\n\n\n\nTheorem 6.1 (Properties of Score Function) Given the support of \\(X\\) is free of \\(\\theta\\):\n\n\\(E_X[S(\\theta; x)] = 0\\)\n\\(\\text{Cov}(S(\\theta; x)) = I(\\theta)\\)\n\n\n\nProof. Proof of Mean 0: \\[\n\\begin{aligned}\nE[S(\\theta; X)] &= \\int \\frac{\\partial \\log f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{1}{f(x|\\theta)} \\frac{\\partial f(x|\\theta)}{\\partial \\theta} f(x|\\theta) dx \\\\\n&= \\int \\frac{\\partial f(x|\\theta)}{\\partial \\theta} dx \\\\\n&= \\frac{\\partial}{\\partial \\theta} \\int f(x|\\theta) dx \\quad \\text{(assuming regularity conditions allow interchange)} \\\\\n&= \\frac{\\partial}{\\partial \\theta} (1) = 0\n\\end{aligned}\n\\]\nProof of Variance: Differentiating \\(\\int f(x|\\theta) dx = 1\\) twice with respect to \\(\\theta\\) leads to the identity: \\[\n\\text{Var}(S(\\theta)) = E\\left[ \\left(\\frac{\\partial l}{\\partial \\theta}\\right)^2 \\right] = -E\\left[ \\frac{\\partial^2 l}{\\partial \\theta^2} \\right]\n\\]\n\n\nRemark 6.1. If \\(X_1, \\dots, X_n\\) are i.i.d with \\(f(x|\\theta)\\), then: \\[\nl(\\theta; x) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\] The score function is the sum of individual score functions. Since variance of a sum of independent variables is the sum of variances: \\[\nI_n(\\theta) = n I_1(\\theta)\n\\] where \\(I_1(\\theta) = E\\left[ -\\frac{\\partial^2}{\\partial \\theta^2} \\log f(X_1|\\theta) \\right]\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Likelihood Theory</span>"
    ]
  }
]