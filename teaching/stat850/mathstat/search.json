[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Preface\nThis is a concise course about statistical inference.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Inference",
    "section": "Key Features",
    "text": "Key Features\n\nUse simulation and graphs to illustrate the concepts in probability theory and statistical inference\nRigourous derivation of the key theorems in statistical inference",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introstatinf.html",
    "href": "introstatinf.html",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "1.1 Population Model (Data Model)\nWe begin with observations (units) \\(X_1, X_2, \\dots, X_n\\). These may be vectors. We regard these observations as a realization of random variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#statistical-inference-setup",
    "href": "introstatinf.html#statistical-inference-setup",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\n1.1.1 Assumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model: Normal) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).\n\n\nExample 1.2 (Parametric Model: Bernoulli) Consider a sequence of binary outcomes (e.g., Success/Failure) where each \\(X_i \\in \\{0, 1\\}\\). We assume \\(X_i \\sim \\text{Bernoulli}(\\theta)\\). The probability mass function is: \\[f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}\\] Here, the parameter space is \\(\\Theta = [0, 1]\\), where \\(\\theta\\) represents the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probability-vs.-statistics",
    "href": "introstatinf.html#probability-vs.-statistics",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probability vs. Statistics",
    "text": "1.2 Probability vs. Statistics\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\). We can visualize this using a “shooting target” analogy:\n\n\\(\\theta\\) (The Center): The true, unknown bullseye location.\n\\(x\\) (The Shots): The observed holes on the target board.\nProbability (Deductive): The center \\(\\theta\\) is known. We predict where the shots \\(x\\) will land.\nStatistics (Inductive): The shots \\(x\\) are observed on the board. The center \\(\\theta\\) is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n\n\n\n\n\n\n\nFigure 1.1: Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#types-of-statistical-inference",
    "href": "introstatinf.html#types-of-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Types of Statistical Inference",
    "text": "1.4 Types of Statistical Inference\nWe can categorize inference into four main types:\n\nDefinition 1.3 (Point Estimation) We use a single number to capture the parameter. \\[\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\]\n\n\nExample 1.4 (Estimating Average Height) We want to estimate the average height (\\(\\mu\\)) of all students in a university. We measure 100 students and calculate the sample mean \\(\\bar{x} = 170\\) cm. Our point estimate is \\(\\hat{\\mu} = 170\\).\n\n\nDefinition 1.4 (Interval Estimation) We construct an interval that likely contains the true parameter. \\[\\theta \\in (L(X_1, \\dots, X_n), U(X_1, \\dots, X_n))\\] The true parameter is within this interval.\n\n\nExample 1.5 (Confidence Interval for Height) Using the same height data, we calculate a 95% Confidence Interval. We state: “We are 95% confident that the true average height is between 168 cm and 172 cm.”\n\n\nDefinition 1.5 (Hypothesis Testing) We test a specific theory about the parameter. \\[H_0: \\theta = \\theta_0 \\quad \\text{vs} \\quad H_1: \\theta \\neq \\theta_0\\] (Or one-sided alternatives like \\(\\theta &gt; \\theta_0\\)).\n\n\nExample 1.6 (Testing Soda Volume) A manufacturer claims their soda bottles contain exactly 500ml (\\(H_0: \\mu = 500\\)). We measure a sample and find an average of 495ml. We perform a test to see if this difference is significant enough to reject the manufacturer’s claim.\n\n\nDefinition 1.6 (Predictive Inference) Given observed data \\((X_1, Y_1), \\dots, (X_n, Y_n)\\), we want to predict a new observation \\(Y_{n+1}\\) given \\(X_{n+1}\\). This is often the primary goal in Machine Learning.\n\n\nExample 1.7 (Predicting House Prices) Based on data about house sizes (\\(X\\)) and prices (\\(Y\\)) from the last year, we want to predict the selling price (\\(Y_{n+1}\\)) of a specific new house that is 2000 sq ft (\\(X_{n+1}\\)).\n\nThere are two primary frameworks for how to perform these inferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#standard-paradigms-for-inference",
    "href": "introstatinf.html#standard-paradigms-for-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 Standard Paradigms for Inference",
    "text": "1.5 Standard Paradigms for Inference\nThere are two primary frameworks for how to perform these inferences.\n\n1.5.1 Frequentist Inference (Fisher)\nDeveloped largely by Fisher (c. 1920).\n\nConcept: The parameter \\(\\theta\\) is a fixed, unknown constant. The data \\(X\\) are random.\nRepeated Sampling Principle: Inference is based on the performance of methods (estimators) under hypothetical repeated sampling of the data.\nSampling Distribution: We analyze how the estimator \\(\\hat{\\theta}\\) behaves over many different datasets generated from the same population.\n\n\n\n\n\n\n\n\n\nFigure 1.3: Frequentist Concept: The Sampling Distribution. The parameter \\(\\theta\\) is fixed (vertical line). The curve represents how the estimator \\(\\hat{\\theta}\\) varies across many hypothetical samples.\n\n\n\n\n\n\n1.5.1.1 Bernoulli Example: Sampling Distribution\nFor our Bernoulli data (\\(x = \\{1, 0, 1\\}\\), \\(n=3\\)), the estimator \\(\\bar{X}\\) is a scaled Binomial variable. If the true parameter were \\(\\theta = 0.5\\), the exact distribution and its Normal approximation would be:\n\n\nCode\ntrue_theta &lt;- 0.5\nn &lt;- 3\nk_vals &lt;- 0:n\nx_bar_vals &lt;- k_vals / n\nprobs &lt;- dbinom(k_vals, size=n, prob=true_theta)\n\ndf_exact &lt;- data.frame(x_bar = x_bar_vals, prob = probs)\n\nx_grid &lt;- seq(-0.2, 1.2, length.out=200)\nsd_approx &lt;- sqrt(true_theta * (1 - true_theta) / n)\nnorm_dens &lt;- dnorm(x_grid, mean=true_theta, sd=sd_approx)\ndf_approx &lt;- data.frame(x = x_grid, density = norm_dens)\n\n# Identify extreme regions for p-value shading (Two-sided: x &gt;= 2/3 and x &lt;= 1/3)\n# Note: 1/3 is symmetric to 2/3 around the mean of 0.5\ndf_shade &lt;- subset(df_approx, x &gt;= 2/3 | x &lt;= 1/3)\n\nggplot() +\n  # 1. Exact Discrete Bars\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob), \n               size=5, color=\"darkgreen\", alpha=0.6) +\n  \n  # 2. Shaded P-value Regions (Normal Approx)\n  geom_area(data=df_shade, aes(x=x, y=density * (1/n)), \n            fill=\"red\", alpha=0.3) +\n  \n  # 3. Normal Approximation Curve\n  geom_line(data=df_approx, aes(x=x, y=density * (1/n)), \n            color=\"red\", size=1.2, linetype=\"dashed\") +\n  \n  # 4. Vertical Line at Observed Mean\n  geom_vline(xintercept = 2/3, color = \"blue\", size = 1, linetype = \"solid\") +\n  annotate(\"text\", x = 2/3, y = -0.02, label = \"Observed\\nbar(x) == 2/3\", \n           parse = TRUE, color = \"blue\", vjust = 1, size = 3.5) +\n  \n  labs(title = expression(paste(\"Sampling Distribution: Transformed Binomial vs. Normal\")),\n       subtitle = expression(paste(\"Testing \", H[0]: theta == 0.5, \" vs \", H[1]: theta != 0.5)),\n       x = expression(bar(x)), y = \"Probability Mass\") +\n  theme_minimal() +\n  # Increase bottom margin for annotation\n  theme(plot.margin = margin(t=10, r=10, b=30, l=10))\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution of \\(\\bar{X}\\) for \\(n=3\\) assuming true \\(\\theta=0.5\\). The vertical line shows the observed \\(\\bar{x}=2/3\\). The shaded red areas represent the p-value (probability of observing a result as extreme as 2/3 under the Null).\n\n\n\n\n\n\n\n1.5.1.2 Key Questions in Statistical Inference\n\nPoint Estimation\n\nConstruction: How do we construct an estimator for an unknown parameter \\(\\theta\\)? (e.g., Method of Moments, Maximum Likelihood Estimation).\nEvaluation: Which estimator is “better”? How do we compare them? (e.g., Unbiasedness, Minimum Variance, Mean Squared Error, Consistency).\nDistribution: What is the sampling distribution of the estimator? Is it exact or asymptotic (approximate)?\n\n\n\nHypothesis Testing\n\nConstruction: How do we construct a test statistic to decide between hypotheses? (e.g., Likelihood Ratio Test, Wald Test, Score Test).\nDecision Rule: How do we determine the critical region or rejection rule?\nErrors: How do we control the probability of making errors? (Type I vs. Type II errors, Power of the test).\n\n\n\nInterval Estimation\n\nConstruction: How do we construct a confidence interval (or credible interval) that covers the true parameter with high probability?\nDuality: How does interval estimation relate to hypothesis testing? (e.g., Inverting a test statistic).\n\n\n\nPrediction\n\nFuture Observations: How do we account for both the uncertainty in the parameter estimate and the random variation of the new data point?\n\n\n\n\n\n1.5.2 Bayesian Inference\nIn the Bayesian framework, we treat the parameter \\(\\theta\\) as a random variable representing our knowledge/uncertainty.\n\nPrior: We assign a prior distribution \\(\\pi(\\theta)\\) reflecting beliefs before seeing data.\nData Model: We have the likelihood \\(f(x_1, \\ldots, x_n|\\theta)\\).\nPosterior: We compute the posterior distribution using Bayes’ theorem: \\[f(\\theta|x_1, \\ldots, x_n) = \\frac{\\pi(\\theta)f(x_1, \\ldots, x_n|\\theta)}{\\int \\pi(\\theta)f(x_1, \\ldots, x_n|\\theta) d\\theta}\\]\n\nIn this framework, inference is based entirely on the Posterior Distribution, which combines the Prior and the Likelihood.\n\n1.5.2.1 Bernoulli Example: Bayesian Update\nUsing \\(x = \\{1, 0, 1\\}\\) and a weakly informative \\(\\text{Beta}(2,2)\\) prior:\n\nPrior: \\(\\theta \\sim \\text{Beta}(2,2)\\)\nLikelihood: \\(L(\\theta) \\propto \\theta^2(1-\\theta)^1\\)\nPosterior: \\(\\theta|x \\sim \\text{Beta}(2+2, 2+1) = \\text{Beta}(4,3)\\)\n\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nprior &lt;- dbeta(theta_grid, 2, 2)\nposterior &lt;- dbeta(theta_grid, 4, 3)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"Posterior\" = \"blue\", \"Prior\" = \"gray\")) +\n  labs(title = \"Bayesian Updating: Prior vs Posterior\",\n       x = expression(theta), y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Bayesian Updating for Bernoulli Data {1, 0, 1}.\n\n\n\n\n\n\n\n\n1.5.3 Bayesian Prediction\nThe predictive density for a new observation \\(x_{n+1}\\) is obtained by integrating over the posterior: \\[f(x_{n+1}|x) = \\int f(x_{n+1}|\\theta) \\pi(\\theta|x) d\\theta\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "2  Decision Theory",
    "section": "",
    "text": "2.1 Formulation of Decision Theory\nIn decision theory, we formalize the process of making decisions under uncertainty using the following components:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#decision-rules-and-risk-functions",
    "href": "decision.html#decision-rules-and-risk-functions",
    "title": "2  Decision Theory",
    "section": "2.2 Decision Rules and Risk Functions",
    "text": "2.2 Decision Rules and Risk Functions\n\n2.2.1 Decision Rule\nA decision rule is a function \\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\). It dictates the action \\(d(x)\\) we take when we observe data \\(x\\).\n\n\n2.2.2 Risk Function\nThe risk function is the expected loss for a given decision rule \\(d\\) as a function of the parameter \\(\\theta\\).\n\\[R(\\theta, d) = E_\\theta[L(\\theta, d(X))]\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#comparing-decision-rules",
    "href": "decision.html#comparing-decision-rules",
    "title": "2  Decision Theory",
    "section": "2.5 Comparing Decision Rules",
    "text": "2.5 Comparing Decision Rules\nWe typically do not have a single rule \\(d\\) that is better than all other rules for all \\(\\theta\\).\n\n\\(d\\) strictly dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nAdmissibility: A decision rule \\(d\\) is admissible if it is not dominated by any other rule. If it is dominated, it is inadmissible.\n\n\n2.5.1 Minimax Principle\nA rule \\(d\\) is Minimax if it minimizes the maximum possible risk.\n\\[\\sup_{\\theta} R(\\theta, d) \\le \\sup_{\\theta} R(\\theta, d') \\quad \\text{for all } d' \\in \\mathcal{D}\\]\nVisualize the risk functions of two rules, \\(d_1\\) and \\(d_2\\). \\(d_2\\) might have a higher risk in some areas but a lower “peak” risk, making it Minimax.\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Minimax: Rule d2 has a lower maximum risk than d1, making it the Minimax rule, even though d1 is better for some values of theta.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#bayes-decision-rules",
    "href": "decision.html#bayes-decision-rules",
    "title": "Decision Theory",
    "section": "2.5 Bayes Decision Rules",
    "text": "2.5 Bayes Decision Rules\nWe specify a prior distribution \\(\\pi(\\theta)\\) on the parameter space \\(\\Theta\\).\n\n2.5.1 Bayes Risk\nThe Bayes risk of a rule \\(d\\) with respect to prior \\(\\pi\\) is the weighted average of the frequentist risk:\n\\[r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n2.5.2 Bayes Rule Definition\nA decision rule \\(d_\\pi\\) is a Bayes rule with respect to \\(\\pi\\) if it minimizes the Bayes risk:\n\\[r(\\pi, d_\\pi) = \\inf_{d' \\in \\mathcal{D}} r(\\pi, d')\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#randomized-decision-rules-risk-sets",
    "href": "decision.html#randomized-decision-rules-risk-sets",
    "title": "Decision Theory",
    "section": "2.6 Randomized Decision Rules & Risk Sets",
    "text": "2.6 Randomized Decision Rules & Risk Sets\n\n2.6.1 Randomized Rules\nA randomized decision rule chooses between deterministic rules \\(d_1, d_2, \\dots\\) with probabilities \\(p_1, p_2, \\dots\\). The risk of a randomized rule \\(d^* = \\sum p_i d_i\\) is the linear combination of their risks: \\[R(\\theta, d^*) = \\sum p_i R(\\theta, d_i)\\]\n\n\n2.6.2 Geometric Interpretation (Finite Parameter Space)\nIf \\(\\Theta = \\{\\theta_1, \\theta_2, \\dots, \\theta_k\\}\\) is finite, we can plot the risk vector \\((R(\\theta_1, d), \\dots, R(\\theta_k, d))\\) in \\(\\mathbb{R}^k\\).\nRisk Set (\\(S\\)): The set of all possible risk vectors. Lemma: The Risk Set \\(S\\) is convex. This is because we can form randomized rules that lie on the line segment connecting any two deterministic rules.\n\n\n\n\n\n\n\n\nFigure 2.3: The Risk Set S. Points d1 and d2 are deterministic rules. The line connecting them represents randomized rules. The Minimax rule lies on the line R1=R2 (if accessible). The Bayes rule is found by bringing a tangent line with slope -pi1/pi2 toward the origin.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#example-the-great-aunts-necklace",
    "href": "decision.html#example-the-great-aunts-necklace",
    "title": "2  Decision Theory",
    "section": "2.6 Example: The “Great Aunt’s Necklace”",
    "text": "2.6 Example: The “Great Aunt’s Necklace”\nScenario: Two boxes. One contains a real necklace (\\(\\theta=1\\)), the other an imitation (\\(\\theta=2\\)). Loss is 1 if we choose the wrong box, 0 otherwise.\n\n\\(\\theta \\in \\{1, 2\\}\\)\nAction \\(a \\in \\{1, 2\\}\\) (Choose Box 1 or Box 2)\n\nData (\\(X\\)): Great Aunt’s judgment. \\(X \\in \\{1, 2\\}\\) (Aunt says Box 1 or Box 2). Probabilities: * If \\(\\theta=1\\) (Real in Box 1): Aunt is senile. \\(P(X=1)=1, P(X=2)=0\\). * If \\(\\theta=2\\) (Real in Box 2): Aunt guesses. \\(P(X=1)=0.5, P(X=2)=0.5\\).\nDecision Rules: 1. \\(d_1\\): Always choose Box 1. 2. \\(d_2\\): Always choose Box 2. 3. \\(d_3(x) = x\\): Follow Aunt’s advice. 4. \\(d_4(x) = 3-x\\): Do opposite of Aunt.\nRisk Calculation:\n\n\n\nRule\n\\(R(\\theta=1)\\)\n\\(R(\\theta=2)\\)\nCoordinates \\((R_1, R_2)\\)\n\n\n\n\n\\(d_1\\)\n0\n1\n(0, 1)\n\n\n\\(d_2\\)\n1\n0\n(1, 0)\n\n\n\\(d_3\\)\n0\n0.5\n(0, 0.5)\n\n\n\\(d_4\\)\n1\n0.5\n(1, 0.5)\n\n\n\nGeometry and Minimax: The risk set is the convex hull of these four points.\n\n\n\n\n\n\n\n\nFigure 2.3: Risk Set for the Necklace Example. The set is the quadrilateral defined by d1, d2, d3, d4. The Minimax rule is the intersection of the line R1=R2 and the lower boundary segment connecting d3 and d2.\n\n\n\n\n\nCalculation of Minimax Rule: The lower boundary connects \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\). Equation of line: \\(R_2 - 0 = \\frac{0.5 - 0}{0 - 1} (R_1 - 1) \\Rightarrow R_2 = -0.5(R_1 - 1)\\). For Minimax, set \\(R_1 = R_2 = R\\). \\(R = -0.5R + 0.5 \\Rightarrow 1.5R = 0.5 \\Rightarrow R = 1/3\\). The minimax rule is a randomized combination: \\(d^* = \\frac{2}{3}d_3 + \\frac{1}{3}d_2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes",
    "href": "decision.html#theorems-relating-minimax-and-bayes",
    "title": "2  Decision Theory",
    "section": "2.7 Theorems relating Minimax and Bayes",
    "text": "2.7 Theorems relating Minimax and Bayes\nTheorem 2.1: If a sequence of Bayes rules \\(\\delta_n\\) has Bayes risk converging to \\(C\\), and \\(R(\\theta, \\delta_0) \\le C\\) for all \\(\\theta\\), then \\(\\delta_0\\) is Minimax.\nTheorem 2.2 (Equalizer Rule): An Extended Bayes rule that is an equalizer rule (constant risk across all \\(\\theta\\)) must be Minimax.\nTheorem 2.3: Assume that the parameter space \\(\\Theta\\) is finite, and that the prior \\(\\pi(\\theta)\\) gives positive probability to each \\(\\theta_i\\). Then, a Bayes rule with respect to \\(\\pi\\) is admissible.\nTheorem 2.4: If a Bayes rule is unique, it is admissible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-decision-theory",
    "href": "decision.html#formulation-of-decision-theory",
    "title": "2  Decision Theory",
    "section": "",
    "text": "Parameter Space (\\(\\Theta\\)): The set of all possible states of nature or values that the parameter can take. \\(\\theta \\in \\Theta\\) (e.g., mean, variance).\nSample Space (\\(\\mathcal{X}\\)): The space where the data \\(X\\) lies. Example: \\(X = (X_1, X_2, \\dots, X_n)\\) where \\(X_i \\in \\mathbb{R}\\). So \\(\\mathcal{X} \\in \\mathbb{R}^n\\).\nFamily of Probability Distributions: \\(\\{P_\\theta(x) : \\theta \\in \\Theta\\}\\). This describes how likely we are to see the data \\(X\\) given a specific parameter \\(\\theta\\).\n\nIf \\(X\\) is continuous: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Density Function).\nIf \\(X\\) is discrete: \\(P_\\theta(x) = f(x, \\theta)\\) (Probability Mass Function).\n\nAction Space (\\(\\mathcal{A}\\)): The set of all actions or decisions available to the experimenter.\nLoss Function: \\(L: \\Theta \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\). \\(L(\\theta, a)\\) specifies the loss incurred if the true parameter is \\(\\theta\\) and we take action \\(a\\). Generally, \\(L(\\theta, a) \\ge 0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "3  Bayesian Methods",
    "section": "",
    "text": "3.1 Fundamental Elements of Bayesian Inference\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#examples",
    "href": "bayesian.html#examples",
    "title": "3  Bayesian Inference",
    "section": "3.2 Examples",
    "text": "3.2 Examples\n\n3.2.1 1. Binomial-Beta\n\n\\(X|\\theta \\sim \\text{Bin}(n, \\theta) \\Rightarrow f(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\\)\nPrior \\(\\theta \\sim \\text{Beta}(a, b) \\Rightarrow \\pi(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\)\n\nPosterior: \\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x} = \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nSo, \\(\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\\).\nMoments:\n\nMean: \\(E(\\theta|x) = \\frac{a+x}{a+b+n} \\approx \\frac{x}{n}\\) (for small \\(n\\))\nVariance: \\(\\text{Var}(\\theta|x) = \\frac{(a+x)(b+n-x)}{(a+b+n)^2(a+b+n+1)}\\)\n\n\n\n3.2.2 2. Normal-Normal (Known Variance)\n\n\\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is known.\nPrior \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\n\nLet \\(\\tau_0 = 1/\\sigma_0^2\\) (prior precision), \\(\\tau = 1/\\sigma^2\\) (data precision). The posterior precision is \\(\\tau_1 = \\tau_0 + n\\tau\\).\nPosterior: \\[\n\\mu|x \\sim N\\left( \\frac{\\tau_0 \\mu_0 + n\\tau \\bar{x}}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nThis shows the posterior mean is a weighted average of the prior mean and the sample mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "href": "bayesian.html#squared-error-loss-ltheta-a-theta---a2",
    "title": "3  Bayesian Inference",
    "section": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)",
    "text": "4.1 1. Squared Error Loss: \\(L(\\theta, a) = (\\theta - a)^2\\)\nMinimizing \\(E_{\\theta|x}[(\\theta - d)^2]\\) leads to: \\[\nd(x) = E(\\theta|x) \\quad \\text{(Posterior Mean)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "href": "bayesian.html#absolute-error-loss-ltheta-a-theta---a",
    "title": "3  Bayesian Inference",
    "section": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)",
    "text": "4.2 2. Absolute Error Loss: \\(L(\\theta, a) = |\\theta - a|\\)\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) leads to: \\[\n\\int_{-\\infty}^d \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = 0.5\n\\] So, \\(d(x) = \\text{Median of } \\pi(\\theta|x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#loss-hypothesis-testing",
    "href": "bayesian.html#loss-hypothesis-testing",
    "title": "3  Bayesian Inference",
    "section": "4.3 3. 0-1 Loss (Hypothesis Testing)",
    "text": "4.3 3. 0-1 Loss (Hypothesis Testing)\n\nLoss is 1 if error, 0 if correct.\nTesting \\(\\Theta_0\\) vs \\(\\Theta_1\\).\nBayes Rule: Choose class with highest posterior probability.\n\nReject \\(H_0\\) if \\(P(\\theta \\in \\Theta_1 | x) &gt; P(\\theta \\in \\Theta_0 | x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#interval-estimation",
    "href": "bayesian.html#interval-estimation",
    "title": "3  Bayesian Inference",
    "section": "4.4 4. Interval Estimation",
    "text": "4.4 4. Interval Estimation\nWe want an interval \\(A = (d-\\delta, d+\\delta)\\) minimizing risk (maximizing coverage probability \\(1-\\alpha\\)).\nHighest Posterior Density (HPD) Interval: The set \\(C = \\{ \\theta : \\pi(\\theta|x) \\ge k \\}\\) where \\(P(\\theta \\in C|x) = 1-\\alpha\\). This is the shortest interval for a given confidence level if the posterior is unimodal.\n\n\nCode\nlibrary(ggplot2)\n\nx &lt;- seq(0, 15, length.out = 1000)\ny &lt;- dgamma(x, shape = 3, rate = 0.5)\ndf &lt;- data.frame(x = x, y = y)\n\n# Approximate HPD cutoff (visual)\nhpd_level &lt;- 0.05\ncutoff &lt;- 0.08 # Chosen for visual representation of the cut\n\nggplot(df, aes(x, y)) +\n  geom_line(size = 1) +\n  geom_area(data = subset(df, y &gt; cutoff), fill = \"skyblue\", alpha = 0.5) +\n  geom_hline(yintercept = cutoff, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = 10, y = cutoff + 0.02, label = \"HPD Cutoff line\", color = \"red\") +\n  labs(title = \"Highest Posterior Density (HPD) Interval\", \n       subtitle = \"Points with density higher than the red line form the HPD set\",\n       x = \"Theta\", y = \"Posterior Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of Highest Posterior Density (HPD) Interval vs Equi-tailed Interval on a skewed posterior.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#baseball-example-efron-morris",
    "href": "bayesian.html#baseball-example-efron-morris",
    "title": "3  Bayesian Methods",
    "section": "3.13 3.8 Baseball Example (Efron & Morris)",
    "text": "3.13 3.8 Baseball Example (Efron & Morris)\nWe illustrate Stein estimation using baseball batting averages. Let \\(y_i\\) be the number of hits for player \\(i\\) in their first \\(n=45\\) at-bats. Let \\(\\hat{p}_i = y_i/n\\) be the observed average.\nTo apply the Normal model, we use a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\] Under this transformation, \\(X_i \\approx N(\\mu_i, 1)\\).\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value \\(\\mu_0\\)). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages \\(\\hat{p}_i\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#empirical-bayes",
    "href": "bayesian.html#empirical-bayes",
    "title": "3  Bayesian Methods",
    "section": "3.6 Empirical Bayes",
    "text": "3.6 Empirical Bayes\nThe James-Stein estimator can be motivated via an Empirical Bayes approach.\nModel:\n\n\\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\)\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\)\n\nThe posterior mean for \\(\\mu_i\\) (if \\(\\tau^2\\) were known) is: \\[E(\\mu_i|x_i) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\]\nThe marginal distribution of \\(X_i\\) is \\(N(0, 1+\\tau^2)\\). Consequently, \\(S = \\sum X_i^2 \\sim (1+\\tau^2) \\chi^2_p\\).\nWe can estimate the unknown shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\) using the data. Since \\(E[ \\frac{p-2}{S} ] = \\frac{1}{1+\\tau^2}\\), we replace the theoretical shrinkage factor with its unbiased estimate: \\[\\hat{B} = \\frac{p-2}{||X||^2}\\]\nThis recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\n\nExample 3.6 (Baseball Example (efron & Morris)) We illustrate Stein estimation using baseball batting averages. Let \\(y_i\\) be the number of hits for player \\(i\\) in their first \\(n=45\\) at-bats. Let \\(\\hat{p}_i = y_i/n\\) be the observed average.\nTo apply the Normal model, we use a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\] Under this transformation, \\(X_i \\approx N(\\mu_i, 1)\\).\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value \\(\\mu_0\\)). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages \\(\\hat{p}_i\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-models",
    "href": "bayesian.html#hierarchical-models",
    "title": "3  Bayesian Inference",
    "section": "7.2 Hierarchical Models",
    "text": "7.2 Hierarchical Models\nWe assume a multistage structure:\n\nData model: \\(X|\\theta \\sim f(x|\\theta)\\)\nParameter model: \\(\\theta|\\lambda \\sim \\pi(\\theta|\\lambda)\\)\nHyperparameter model: \\(\\lambda \\sim h(\\lambda)\\)\n\nComputation: Since analytical solutions are often impossible, we use Markov Chain Monte Carlo (MCMC).\n\n7.2.1 Gibbs Sampling\nTo sample from the joint posterior \\(f(\\theta, \\lambda | x)\\), we sample iteratively from the full conditional distributions:\n\nDraw \\(\\theta^{(k+1)} \\sim f(\\theta | \\lambda^{(k)}, x)\\)\nDraw \\(\\lambda^{(k+1)} \\sim f(\\lambda | \\theta^{(k+1)}, x)\\)\n\n\n\n7.2.2 Metropolis-Hastings\nIf a conditional distribution is hard to sample from directly:\n\nPropose \\(\\theta^*\\) from a proposal density \\(q(\\theta^* | \\theta^{(t)})\\).\nCalculate acceptance ratio \\(\\alpha = \\min \\left( 1, \\frac{f(\\theta^*|x)q(\\theta^{(t)}|\\theta^*)}{f(\\theta^{(t)}|x)q(\\theta^*|\\theta^{(t)})} \\right)\\).\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "decision.html#examples-of-decision-problems",
    "href": "decision.html#examples-of-decision-problems",
    "title": "2  Decision Theory",
    "section": "2.3 Examples of Decision Problems",
    "text": "2.3 Examples of Decision Problems\n\n2.3.1 Example 1: Hypothesis Testing\nWe want to test \\(H_0\\) vs \\(H_1\\).\n\nAction Space: \\(\\mathcal{A} = \\{0, 1\\}\\) (0=“Accept \\(H_0\\)”, 1=“Reject \\(H_0\\)”).\nLoss Function (0-1 Loss): 0 if correct, 1 if wrong.\nRisk Function:\n\nIf \\(\\theta \\in H_0\\): \\(R(\\theta, d) = P(\\text{Type I Error})\\).\nIf \\(\\theta \\in H_1\\): \\(R(\\theta, d) = P(\\text{Type II Error})\\).\n\n\n\n\n2.3.2 Example 2: Point Estimation\nWe want to estimate a parameter \\(\\theta\\).\n\nAction Space: \\(\\mathcal{A} = \\Theta\\).\nLoss Function (Squared Error): \\(L(\\theta, a) = (\\theta - a)^2\\).\nRisk Function (MSE): \\(R(\\theta, d) = \\text{Var}(\\bar{x}) + \\text{Bias}^2\\).\n\n\n\n2.3.3 Example 3: Interval Estimation\nWe want to estimate a range for the parameter.\n\nAction Space: \\(\\mathcal{A} = \\{(l, u) : l \\in \\mathbb{R}, u \\in \\mathbb{R}, l \\le u\\}\\).\n\n\n\n2.3.4 Example 4: The Duchess and the Emerald Necklace\nScenario: You are the Duchess of Omnium. You have two necklaces: a priceless Real one and a valueless Imitation. They are indistinguishable to you. One is in the Left Drawer (Box 1), the other is in the Right Drawer (Box 2).\nThe Data (Great Aunt): You consult your Great Aunt. She inspects the Left Drawer first, then the Right.\n\nIf the Real necklace is in the Left (\\(\\theta=1\\)): She identifies it correctly. (Infallible).\nIf the Real necklace is in the Right (\\(\\theta=2\\)): She sees the fake first, gets confused, and guesses randomly (\\(50/50\\)).\n\n\n2.3.4.1 Formulation\n\nParameter Space: \\(\\Theta = \\{1, 2\\}\\) (1=Real Left, 2=Real Right).\nAction Space: \\(\\mathcal{A} = \\{1, 2\\}\\) (1=Wear Left, 2=Wear Right).\nLoss Function: 0 if correct, 1 if wrong.\n\n\n\n2.3.4.2 Risk Calculation for Deterministic Rules\nWe consider four deterministic rules \\(d(X)\\). We calculate the risk (\\(R_1\\) for \\(\\theta=1\\) and \\(R_2\\) for \\(\\theta=2\\)) for each.\nRule \\(d_1\\) (Always Left)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 1\\)\n\n\n\nRule \\(d_2\\) (Always Right)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0\\)\n\n\n\nRule \\(d_3\\) (Follow Aunt)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)\n\n\n\nRule \\(d_4\\) (Do Opposite)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nLoss \\(L(1, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nLoss \\(L(2, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\\(R_2 = 0.5\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#example-the-duchess-and-the-emerald-necklace",
    "href": "decision.html#example-the-duchess-and-the-emerald-necklace",
    "title": "2  Decision Theory",
    "section": "2.5 Example: The Duchess and the Emerald Necklace",
    "text": "2.5 Example: The Duchess and the Emerald Necklace\nScenario: You are the Duchess of Omnium. You have two necklaces: a priceless Real one and a valueless Imitation. They are indistinguishable to you. One is in the Left Drawer (Box 1), the other is in the Right Drawer (Box 2).\nThe Data (Great Aunt): You consult your Great Aunt. She inspects the Left Drawer first, then the Right.\n\nIf the Real necklace is in the Left (\\(\\theta=1\\)): She identifies it correctly. (Infallible).\nIf the Real necklace is in the Right (\\(\\theta=2\\)): She sees the fake first, gets confused, and guesses randomly (\\(50/50\\)).\n\n\n2.5.1 Formulation\n\nParameter Space: \\(\\Theta = \\{1, 2\\}\\) (1=Real Left, 2=Real Right).\nAction Space: \\(\\mathcal{A} = \\{1, 2\\}\\) (1=Wear Left, 2=Wear Right).\nData: \\(X \\in \\{1, 2\\}\\) (Aunt says Left/Right).\nLoss Function: 0 if correct, 1 if wrong.\n\n\n\n2.5.2 Probability of Data Given Parameter (\\(P_\\theta(X)\\))\n\n\n\n\nState \\(\\theta=1\\)\n\nState \\(\\theta=2\\)\n\n\n\n\n\nOutcome\n\\(X=1\\)\n\\(X=2\\)\n\\(X=1\\)\n\\(X=2\\)\n\n\nProbability\n1\n0\n0.5\n0.5\n\n\n\n\n\n2.5.3 Decision Rules\nWe consider four deterministic rules \\(d(X)\\).\n\n\n\nRule\nDescription\nAction if \\(X=1\\)\nAction if \\(X=2\\)\n\n\n\n\n\\(d_1\\)\nAlways Left\n1\n1\n\n\n\\(d_2\\)\nAlways Right\n2\n2\n\n\n\\(d_3\\)\nFollow Aunt\n1\n2\n\n\n\\(d_4\\)\nDo Opposite\n2\n1\n\n\n\n\n\n2.5.4 Risk Calculation Tables\nFor each rule, we calculate the risk (\\(R_1\\) for \\(\\theta=1\\) and \\(R_2\\) for \\(\\theta=2\\)).\n\n2.5.4.1 Rule \\(d_1\\) (Always Left)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_1(X)\\)\n1\n1\n\n\n\n\nLoss \\(L(1, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n0\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_1(X)\\)\n1\n1\n\n\n\n\nLoss \\(L(2, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0.5\n0.5\n\\(R_2 = 1\\)\n\n\n\n\n\n2.5.4.2 Rule \\(d_2\\) (Always Right)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_2(X)\\)\n2\n2\n\n\n\n\nLoss \\(L(1, d)\\)\n1\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_2(X)\\)\n2\n2\n\n\n\n\nLoss \\(L(2, d)\\)\n0\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0\n0\n\\(R_2 = 0\\)\n\n\n\n\n\n2.5.4.3 Rule \\(d_3\\) (Follow Aunt)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_3(X)\\)\n1\n2\n\n\n\n\nLoss \\(L(1, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n0\n0\n\\(R_1 = 0\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_3(X)\\)\n1\n2\n\n\n\n\nLoss \\(L(2, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0.5\n0\n\\(R_2 = 0.5\\)\n\n\n\n\n\n2.5.4.4 Rule \\(d_4\\) (Do Opposite)\n\n\n\nState\nComponent\n\\(X=1\\)\n\\(X=2\\)\nRisk (Sum)\n\n\n\n\n\\(\\theta=1\\)\nAction \\(d_4(X)\\)\n2\n1\n\n\n\n\nLoss \\(L(1, d)\\)\n1\n0\n\n\n\n\nProb \\(P(X \\mid \\theta=1)\\)\n1\n0\n\n\n\n\nProduct\n1\n0\n\\(R_1 = 1\\)\n\n\n\\(\\theta=2\\)\nAction \\(d_4(X)\\)\n2\n1\n\n\n\n\nLoss \\(L(2, d)\\)\n0\n1\n\n\n\n\nProb \\(P(X \\mid \\theta=2)\\)\n0.5\n0.5\n\n\n\n\nProduct\n0\n0.5\n\\(R_2 = 0.5\\)\n\n\n\n\n\n\n2.5.5 Application of Geometric Analysis to Necklace Problem\nWe plot the risks calculated above:\n\n\\(d_1\\): \\((0, 1)\\)\n\\(d_2\\): \\((1, 0)\\)\n\\(d_3\\): \\((0, 0.5)\\)\n\\(d_4\\): \\((1, 0.5)\\)\n\nFrom the plot below, we can see that the admissible boundary connects \\(d_3\\) and \\(d_2\\). The Minimax rule is a randomized mixture of these two.\n\n\n\n\n\n\n\n\nFigure 2.2: Risk Set for the Necklace Problem. The Minimax rule is found at the intersection of y=x and the lower boundary. The orange and green lines represent Bayes risks for different priors.\n\n\n\n\n\n\n\n2.5.6 Finding the Minimax Weights\nThe minimax rule lies on the boundary connecting \\(d_3\\) and \\(d_2\\). \\[R(\\delta^*) = p R(d_3) + (1-p) R(d_2) = p \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} + (1-p) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-p \\\\ 0.5p \\end{pmatrix}\\] Setting \\(R_1 = R_2\\): \\[1-p = 0.5p \\implies 1 = 1.5p \\implies p = 2/3\\] The Minimax strategy is to choose \\(d_3\\) with probability \\(2/3\\) and \\(d_2\\) with probability \\(1/3\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#formulation-of-the-decision-problem",
    "href": "decision.html#formulation-of-the-decision-problem",
    "title": "2  Decision Theory",
    "section": "2.5 Formulation of the Decision Problem",
    "text": "2.5 Formulation of the Decision Problem\n\n2.5.1 Parameter Space (\\(\\Theta\\))\nThe set of possible states of nature regarding the location of the real necklace:\n\n\\(\\theta_1\\): The Real necklace is in the Left Drawer.\n\\(\\theta_2\\): The Real necklace is in the Right Drawer.\n\n\n\n2.5.2 Action Space (\\(\\mathcal{A}\\))\nThe set of possible actions available to the Duchess:\n\n\\(a_1\\): Wear the Left necklace.\n\\(a_2\\): Wear the Right necklace.\n\n\n\n2.5.3 The Data (\\(X\\))\nThe data consists of the Great Aunt’s judgment after inspecting the necklaces:\n\n\\(X \\in \\{1, 2\\}\\)\n\\(X=1\\): Aunt says “Left is Real”.\n\\(X=2\\): Aunt says “Right is Real”.\n\n\n\n2.5.4 Probability of Data Given Parameter (\\(P_\\theta(X)\\))\nThe probability of the Aunt’s advice changes depending on the true state of nature (\\(\\theta\\)).\n\n\n\nState\n\\(P(X=1 \\mid \\theta)\\)\n\\(P(X=2 \\mid \\theta)\\)\n\n\n\n\n\\(\\theta_1\\) (Real Left)\n1\n0\n\n\n\\(\\theta_2\\) (Real Right)\n0.5\n0.5\n\n\n\n\n\n2.5.5 Loss Function (\\(L\\))\nThe loss is defined as 0 for a correct choice and 1 (representing £1M) for an incorrect choice.\n\n\n\nState  Action\n\\(a_1\\) (Wear Left)\n\\(a_2\\) (Wear Right)\n\n\n\n\n\\(\\theta_1\\) (Real Left)\n0\n1\n\n\n\\(\\theta_2\\) (Real Right)\n1\n0\n\n\n\n\n\n2.5.6 Decision Rules\nThere are four possible deterministic decision rules (\\(d: \\mathcal{X} \\rightarrow \\mathcal{A}\\)).\n\n\n\n\n\n\n\n\n\nRule\nDescription\nAction if \\(X=1\\) (Left)\nAction if \\(X=2\\) (Right)\n\n\n\n\n\\(d_1\\)\nAlways Left\n\\(a_1\\)\n\\(a_1\\)\n\n\n\\(d_2\\)\nAlways Right\n\\(a_2\\)\n\\(a_2\\)\n\n\n\\(d_3\\)\nFollow Aunt\n\\(a_1\\)\n\\(a_2\\)\n\n\n\\(d_4\\)\nDo Opposite\n\\(a_2\\)\n\\(a_1\\)\n\n\n\n\n\n2.5.7 Risk Calculation\nBelow are the risk calculation tables for each decision rule.\n\n2.5.7.1 Rule \\(d_1\\) (Always Left)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n0\n0\n\\(R(\\theta_1) = (0 \\times 1) + (0 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 0\\)\n\n\n\\(\\theta_2\\)\nLoss\n1\n1\n\\(R(\\theta_2) = (1 \\times 0.5) + (1 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 1\\)\n\n\n\n\n\n2.5.7.2 Rule \\(d_2\\) (Always Right)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n1\n1\n\\(R(\\theta_1) = (1 \\times 1) + (1 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 1\\)\n\n\n\\(\\theta_2\\)\nLoss\n0\n0\n\\(R(\\theta_2) = (0 \\times 0.5) + (0 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0\\)\n\n\n\n\n\n2.5.7.3 Rule \\(d_3\\) (Follow Aunt)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n0\n1\n\\(R(\\theta_1) = (0 \\times 1) + (1 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 0\\)\n\n\n\\(\\theta_2\\)\nLoss\n1\n0\n\\(R(\\theta_2) = (1 \\times 0.5) + (0 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0.5\\)\n\n\n\n\n\n2.5.7.4 Rule \\(d_4\\) (Do Opposite)\n\n\n\n\n\n\n\n\n\n\nState\nMetric\n\\(X=1\\)\n\\(X=2\\)\nRisk Calculation\n\n\n\n\n\\(\\theta_1\\)\nLoss\n1\n0\n\\(R(\\theta_1) = (1 \\times 1) + (0 \\times 0)\\)\n\n\n\nProb\n1\n0\n\\(= 1\\)\n\n\n\\(\\theta_2\\)\nLoss\n0\n1\n\\(R(\\theta_2) = (0 \\times 0.5) + (1 \\times 0.5)\\)\n\n\n\nProb\n0.5\n0.5\n\\(= 0.5\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#geometric-interpretation-and-principles",
    "href": "decision.html#geometric-interpretation-and-principles",
    "title": "2  Decision Theory",
    "section": "2.4 Geometric Interpretation and Principles",
    "text": "2.4 Geometric Interpretation and Principles\nWhen the parameter space is finite (e.g., \\(\\Theta = \\{1, 2\\}\\)), the risk function \\(R(\\theta, d)\\) can be represented as a point in \\(\\mathbb{R}^2\\) with coordinates \\((R_1, R_2) = (R(\\theta_1, d), R(\\theta_2, d))\\). This geometric perspective allows us to visualize fundamental concepts like Admissibility, Minimax, and Bayes rules.\n\n2.4.1 The Risk Set (\\(S\\))\nThe Risk Set \\(S\\) represents the collection of risk vectors for all possible decision rules.\n\nDeterministic Rules: These form the vertices (corners) of the set.\nRandomized Rules: If we mix two rules \\(d_1\\) and \\(d_2\\) with probabilities \\(p\\) and \\((1-p)\\), the resulting risk lies on the straight line segment connecting \\(R(d_1)\\) and \\(R(d_2)\\).\nConvexity: Because we can randomize between any rules, the Risk Set \\(S\\) is the convex hull of the deterministic rules. It typically forms a polygon filled with all possible randomized strategies.\n\n\n\n2.4.2 Admissibility (The Efficient Frontier)\nWe always prefer a lower risk.\n\nA rule \\(d\\) dominates rule \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\). Geometrically, this means \\(d\\) is to the “south-west” of \\(d'\\).\nA rule is Admissible if it is not dominated.\nThe set of admissible rules forms the lower-left boundary of the Risk Set \\(S\\). This is often called the “efficient frontier.”\n\n\n\n2.4.3 Finding the Minimax Rule\nThe Minimax principle seeks to minimize the maximum possible risk: \\(\\min_d [\\max(R_1, R_2)]\\).\n\nGeometrically, points of constant maximum risk (\\(\\max(R_1, R_2) = k\\)) form “L-shaped” right angles centered on the line \\(R_1 = R_2\\).\nTo find the Minimax rule, we find the intersection of the Risk Set \\(S\\) with the \\(45^\\circ\\) line \\(y = x\\) (\\(R_1 = R_2\\)).\nThe intersection point on the lower boundary of \\(S\\) is the Minimax rule.\n\n\n\n2.4.4 Finding the Bayes Rule\nA Bayes rule minimizes the weighted sum of risks for a given prior \\(\\pi = (\\pi_1, \\pi_2)\\).\n\nWe minimize Bayes Risk \\(r(\\pi, d) = \\pi_1 R_1 + \\pi_2 R_2\\).\nThe equation \\(\\pi_1 R_1 + \\pi_2 R_2 = c\\) defines a family of parallel lines with slope \\(m = -\\frac{\\pi_1}{\\pi_2}\\).\nTo find the Bayes rule, imagine taking a line with slope \\(-\\frac{\\pi_1}{\\pi_2}\\) and moving it from the origin outward until it just touches the Risk Set \\(S\\).\nThe point(s) of tangency are the Bayes rules for that prior.\n\n\n\n\n\n\n\n\n\nFigure 2.1: Geometric Representation of Decision Principles. The gray polygon is the Risk Set. The blue line segment represents the Admissible Rules. The red point is the Minimax Rule (intersection with R1=R2). The green point is a Bayes Rule, found by the tangent line determined by the prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#principles-for-choosing-a-decision-rule",
    "href": "decision.html#principles-for-choosing-a-decision-rule",
    "title": "2  Decision Theory",
    "section": "2.4 Principles for Choosing a Decision Rule",
    "text": "2.4 Principles for Choosing a Decision Rule\nSince no single rule minimizes risk for all \\(\\theta\\), we rely on several principles to order and select decision rules.\n\n2.4.1 Admissibility\nA decision rule \\(d\\) is admissible if it is not “dominated” by any other rule.\n\nDomination: A rule \\(d\\) dominates \\(d'\\) if \\(R(\\theta, d) \\le R(\\theta, d')\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\).\nInadmissibility: If a rule is dominated, it is inadmissible and can be discarded (we can do better or equal in every possible state).\n\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Domination: Rule A (Red) is inadmissible because Rule B (Blue) has lower risk for all values of theta.\n\n\n\n\n\n\n\n2.4.2 Minimax Principle\nThe Minimax principle is a conservative approach that guards against the worst-case scenario. It selects the rule that minimizes the maximum risk. \\[ \\min_{d} \\left[ \\sup_{\\theta} R(\\theta, d) \\right] \\]\nIn the plot below, while Rule B has lower risk in the center, it has a very high maximum risk. Rule A is “flatter” and has a lower maximum value, making it the Minimax choice.\n\n\n\n\n\n\n\n\nFigure 2.2: Illustration of Minimax: Rule A has a lower peak risk than Rule B, making Rule A the Minimax choice.\n\n\n\n\n\n\n\n2.4.3 Bayes Decision Rules\nThe Bayes principle incorporates prior knowledge. If we assign a probability distribution (prior) \\(\\pi(\\theta)\\) to the parameter, we can calculate the Bayes Risk, which is the weighted average of the risk function. We choose the rule that minimizes this average. \\[ r(\\pi, d) = E_\\pi [R(\\theta, d)] = \\int_\\Theta R(\\theta, d) \\pi(\\theta) d\\theta \\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#geometric-interpretation-risk-sets",
    "href": "decision.html#geometric-interpretation-risk-sets",
    "title": "2  Decision Theory",
    "section": "2.5 Geometric Interpretation (Risk Sets)",
    "text": "2.5 Geometric Interpretation (Risk Sets)\nFor finite parameter spaces (e.g., \\(\\Theta = \\{1, 2\\}\\)), we can visualize the problem in 2D space where the axes are \\(R_1 = R(\\theta_1)\\) and \\(R_2 = R(\\theta_2)\\).\n\n2.5.1 The Risk Set (\\(S\\))\nThe set of all possible risk vectors is called the Risk Set \\(S\\). * Deterministic Rules: These are the vertices of the set. * Randomized Rules: By choosing rule \\(d_i\\) with probability \\(p\\) and \\(d_j\\) with probability \\(1-p\\), we can achieve any risk on the line segment connecting them. * Convexity: The Risk Set is the convex hull of the deterministic rules.\n\n\n2.5.2 Visualizing Admissibility\nThe admissible rules lie on the lower-left boundary of the set. Any point to the “north-east” of another point is dominated (inadmissible).\n\n\n2.5.3 Visualizing Minimax\nThe Minimax rule is found by intersecting the Risk Set with the line \\(y=x\\) (\\(R_1 = R_2\\)). * We look for the point in \\(S\\) that touches the \\(45^\\circ\\) line at the lowest value. * If the set is entirely below the line, we minimize \\(R_2\\). If entirely above, we minimize \\(R_1\\).\n\n\n2.5.4 Visualizing Bayes Rules\nA Bayes rule minimizes \\(\\pi_1 R_1 + \\pi_2 R_2 = k\\). This equation represents a line with slope \\(m = -\\pi_1 / \\pi_2\\). * To find the Bayes rule, we find the tangent line to the Risk Set \\(S\\) with slope \\(-\\pi_1 / \\pi_2\\).\n\n\n\n\n\n\n\n\nFigure 2.3: Geometric Interpretation: The gray polygon is the Risk Set S. The blue boundary represents admissible rules. The red point is the Minimax rule. The green line represents a Bayes rule for a specific prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "href": "decision.html#revisiting-the-necklace-example-geometric-solution",
    "title": "2  Decision Theory",
    "section": "2.6 Revisiting the Necklace Example: Geometric Solution",
    "text": "2.6 Revisiting the Necklace Example: Geometric Solution\nWe now apply the geometric interpretation to the Necklace problem using the risks calculated in Section 2.3.4.\n\n\\(d_1\\): \\((0, 1)\\)\n\\(d_2\\): \\((1, 0)\\)\n\\(d_3\\): \\((0, 0.5)\\)\n\\(d_4\\): \\((1, 0.5)\\)\n\n\n2.6.1 Analysis\n\nAdmissibility:\n\n\\(d_4\\) has risk \\((1, 0.5)\\). \\(d_3\\) has risk \\((0, 0.5)\\). Since \\(0 &lt; 1\\), \\(d_3\\) strictly dominates \\(d_4\\). Thus \\(d_4\\) is inadmissible.\nThe efficient frontier connects \\(d_3\\) and \\(d_2\\).\n\nMinimax Solution: The Minimax rule lies on the segment connecting \\(d_3 (0, 0.5)\\) and \\(d_2 (1, 0)\\).\n\nLet the randomized rule be \\(\\delta^* = p d_3 + (1-p) d_2\\).\n\\(R(\\delta^*) = p \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} + (1-p) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-p \\\\ 0.5p \\end{pmatrix}\\).\nSet \\(R_1 = R_2\\): \\(1-p = 0.5p \\Rightarrow 1 = 1.5p \\Rightarrow p = 2/3\\).\nResult: The Minimax rule is to choose \\(d_3\\) with probability \\(2/3\\) and \\(d_2\\) with probability \\(1/3\\).\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Necklace Problem Solution. The Minimax rule (red diamond) is the specific randomized combination of d3 and d2 that equalizes the risk.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "href": "decision.html#theorems-relating-minimax-and-bayes-rules",
    "title": "2  Decision Theory",
    "section": "2.7 Theorems Relating Minimax and Bayes Rules",
    "text": "2.7 Theorems Relating Minimax and Bayes Rules\nIn practice, finding a Minimax rule directly is mathematically difficult. A standard strategy is to “guess” a Least Favorable Prior \\(\\pi\\)—defined as the prior distribution that maximizes the minimum Bayes risk (i.e., the prior against which it is hardest to defend)—find the corresponding Bayes rule, and then check if it satisfies specific conditions to confirm it is Minimax.\n\n2.7.1 Equalizer Rules\n\nTheorem 2.1 (The Equalizer Rule Strategy) If \\(\\delta^*\\) is a Bayes rule with respect to some prior \\(\\pi\\), and if \\(\\delta^*\\) is an equalizer rule (meaning \\(R(\\theta, \\delta^*) = C\\) for some constant \\(C\\) for all \\(\\theta \\in \\Theta\\)), then \\(\\delta^*\\) is Minimax.\n\n\nProof. \n\nBayes Risk Definition: Since \\(\\delta^*\\) is an equalizer rule with risk \\(C\\), its Bayes risk with respect to \\(\\pi\\) is: \\[r(\\pi, \\delta^*) = \\int_\\Theta R(\\theta, \\delta^*) \\pi(\\theta) d\\theta = \\int_\\Theta C \\pi(\\theta) d\\theta = C \\cdot 1 = C\\]\nMinimax Contradiction: Suppose, for the sake of contradiction, that \\(\\delta^*\\) is not Minimax. This implies there exists another rule \\(\\delta'\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta') &lt; \\sup_{\\theta} R(\\theta, \\delta^*)\\] Since \\(R(\\theta, \\delta^*) = C\\) for all \\(\\theta\\), the supremum is \\(C\\). Thus: \\[\\sup_{\\theta} R(\\theta, \\delta') &lt; C\\]\nInequality: This implies that for all \\(\\theta\\), \\(R(\\theta, \\delta') &lt; C\\).\nBayes Risk Comparison: Now, consider the Bayes risk of this alternative rule \\(\\delta'\\): \\[r(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta\\] Since \\(R(\\theta, \\delta') &lt; C\\) for all \\(\\theta\\), it follows that: \\[r(\\pi, \\delta') &lt; \\int_\\Theta C \\pi(\\theta) d\\theta = C\\]\nConclusion: We have established that \\(r(\\pi, \\delta') &lt; C\\). However, we established in step 1 that \\(r(\\pi, \\delta^*) = C\\). This yields \\(r(\\pi, \\delta') &lt; r(\\pi, \\delta^*)\\). This contradicts the assumption that \\(\\delta^*\\) is a Bayes rule (since a Bayes rule must minimize the Bayes risk). Therefore, no such \\(\\delta'\\) exists, and \\(\\delta^*\\) is Minimax. \\(\\blacksquare\\)\n\n\n\n\n2.7.2 Limits of Bayes Rules\nSometimes the Minimax rule corresponds to an “improper” prior (a prior that does not integrate to 1, like a uniform distribution on the real line). We approach these via a limiting sequence.\n\nTheorem 2.2 (Limits of Bayes Rules) Let \\(\\{\\delta_n\\}\\) be a sequence of Bayes rules with respect to priors \\(\\{\\pi_n\\}\\). Let \\(r(\\pi_n, \\delta_n)\\) be the associated Bayes risks. If there exists a rule \\(\\delta_0\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta_0) \\le \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\] Then \\(\\delta_0\\) is Minimax.\n\n\nProof. \n\nDefine Limit: Let \\(V = \\lim_{n \\to \\infty} r(\\pi_n, \\delta_n)\\). We are given that \\(\\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\).\nContradiction Setup: Suppose \\(\\delta_0\\) is not Minimax. Then there exists a rule \\(\\delta^*\\) such that: \\[\\sup_{\\theta} R(\\theta, \\delta^*) &lt; \\sup_{\\theta} R(\\theta, \\delta_0) \\le V\\] Let \\(\\sup_{\\theta} R(\\theta, \\delta^*) = V - \\epsilon\\) for some \\(\\epsilon &gt; 0\\).\nBayes Risk Bound: For any prior \\(\\pi_n\\), the Bayes risk of \\(\\delta^*\\) cannot exceed its maximum risk: \\[r(\\pi_n, \\delta^*) = \\int R(\\theta, \\delta^*) \\pi_n(\\theta) d\\theta \\le \\int (V - \\epsilon) \\pi_n(\\theta) d\\theta = V - \\epsilon\\]\nOptimality of \\(\\delta_n\\): Since \\(\\delta_n\\) is the Bayes rule for \\(\\pi_n\\), it minimizes Bayes risk. Thus: \\[r(\\pi_n, \\delta_n) \\le r(\\pi_n, \\delta^*)\\]\nCombining Inequalities: Combining steps 3 and 4: \\[r(\\pi_n, \\delta_n) \\le V - \\epsilon\\]\nTaking Limits: Taking the limit as \\(n \\to \\infty\\): \\[\\lim_{n \\to \\infty} r(\\pi_n, \\delta_n) \\le V - \\epsilon\\] \\[V \\le V - \\epsilon\\] This is a contradiction since \\(\\epsilon &gt; 0\\). Therefore, \\(\\delta_0\\) must be Minimax. \\(\\blacksquare\\)\n\n\n\n\n2.7.3 Admissibility of Bayes Rules\nBayes rules are generally good candidates for admissibility. If a rule is Bayes, it is likely efficient, provided the prior doesn’t ignore parts of the parameter space.\n\nTheorem 2.3 (Admissibility of Bayes Rules (Finite Support)) If the parameter space \\(\\Theta\\) is finite (or countable) and the prior \\(\\pi\\) assigns positive probability to every \\(\\theta \\in \\Theta\\) (i.e., \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\)), then any Bayes rule \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) that dominates it. By definition of domination:\n\n\\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\).\n\\(R(\\theta_k, \\delta') &lt; R(\\theta_k, \\delta_\\pi)\\) for at least one \\(\\theta_k\\).\n\nBayes Risk Difference: Consider the difference in Bayes risk: \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') = \\sum_{\\theta \\in \\Theta} \\pi(\\theta) [R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\]\nStrict Positivity:\n\nSince \\(\\delta'\\) dominates \\(\\delta_\\pi\\), each term \\([R(\\theta, \\delta_\\pi) - R(\\theta, \\delta')]\\) is non-negative (\\(\\ge 0\\)).\nAt \\(\\theta_k\\), the term is strictly positive (\\(&gt; 0\\)).\nWe assumed the prior has full support, so \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta\\).\n\nSummation: A sum of non-negative terms where at least one term is strictly positive must be strictly positive. \\[r(\\pi, \\delta_\\pi) - r(\\pi, \\delta') &gt; 0 \\implies r(\\pi, \\delta') &lt; r(\\pi, \\delta_\\pi)\\]\nConclusion: This contradicts the definition that \\(\\delta_\\pi\\) is a Bayes rule (which must minimize Bayes risk). Therefore, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)\n\n\n\n\n2.7.4 Admissibility of Unique Bayes Rules\nIf the Bayes rule is unique, we can drop the requirement that the parameter space be discrete or finite.\n\nTheorem 2.4 (Admissibility of Unique Bayes Rules) Let \\(\\delta_\\pi\\) be a Bayes rule with respect to \\(\\pi\\). If \\(\\delta_\\pi\\) is the unique Bayes rule (up to risk equivalence), then \\(\\delta_\\pi\\) is admissible.\n\n\nProof. \n\nContradiction Setup: Suppose \\(\\delta_\\pi\\) is inadmissible. Then there exists a rule \\(\\delta'\\) such that: \\(R(\\theta, \\delta') \\le R(\\theta, \\delta_\\pi)\\) for all \\(\\theta\\), with strict inequality for some set of \\(\\theta\\).\nBayes Risk Inequality: Taking the expectation with respect to \\(\\pi\\): \\[r(\\pi, \\delta') = \\int R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\int R(\\theta, \\delta_\\pi) \\pi(\\theta) d\\theta = r(\\pi, \\delta_\\pi)\\]\nMinimality: Since \\(\\delta_\\pi\\) is Bayes, it minimizes the risk, so \\(r(\\pi, \\delta_\\pi) \\le r(\\pi, \\delta')\\). Combining these gives \\(r(\\pi, \\delta') = r(\\pi, \\delta_\\pi)\\).\nUniqueness: This implies that \\(\\delta'\\) is also a Bayes rule. However, we assumed that \\(\\delta_\\pi\\) is the unique Bayes rule. Therefore, \\(\\delta'\\) must be equal to \\(\\delta_\\pi\\) (in terms of risk functions).\nConclusion: If \\(\\delta'\\) and \\(\\delta_\\pi\\) have identical risk functions, then \\(\\delta'\\) cannot strictly dominate \\(\\delta_\\pi\\). This contradicts the assumption of inadmissibility. Thus, \\(\\delta_\\pi\\) is admissible. \\(\\blacksquare\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#risk-set-for-finite-parameter-space",
    "href": "decision.html#risk-set-for-finite-parameter-space",
    "title": "2  Decision Theory",
    "section": "2.5 Risk Set for Finite Parameter Space",
    "text": "2.5 Risk Set for Finite Parameter Space\nFor finite parameter spaces (e.g., \\(\\Theta = \\{1, 2\\}\\)), we can visualize the problem in 2D space where the axes are \\(R_1 = R(\\theta_1)\\) and \\(R_2 = R(\\theta_2)\\).\n\n2.5.1 The Risk Set (\\(S\\))\nThe set of all possible risk vectors is called the Risk Set \\(S\\).\n\nDeterministic Rules: These are the vertices of the set.\nRandomized Rules: By choosing rule \\(d_i\\) with probability \\(p\\) and \\(d_j\\) with probability \\(1-p\\), we can achieve any risk on the line segment connecting them.\nConvexity: The Risk Set is the convex hull of the deterministic rules.\n\n\n\n2.5.2 Visualizing Admissibility\nThe admissible rules lie on the lower-left boundary of the set. Any point to the “north-east” of another point is dominated (inadmissible).\n\n\n2.5.3 Visualizing Minimax\nThe Minimax rule is found by intersecting the Risk Set with the line \\(y=x\\) (\\(R_1 = R_2\\)).\n\nWe look for the point in \\(S\\) that touches the \\(45^\\circ\\) line at the lowest value.\nIf the set is entirely below the line, we minimize \\(R_2\\). If entirely above, we minimize \\(R_1\\).\n\n\n\n2.5.4 Visualizing Bayes Rules\nA Bayes rule minimizes \\(\\pi_1 R_1 + \\pi_2 R_2 = k\\). This equation represents a line with slope \\(m = -\\pi_1 / \\pi_2\\).\n\nTo find the Bayes rule, we find the tangent line to the Risk Set \\(S\\) with slope \\(-\\pi_1 / \\pi_2\\).\n\n\n\n\n\n\n\n\n\nFigure 2.3: Geometric Interpretation: The gray polygon is the Risk Set S. The blue boundary represents admissible rules. The red point is the Minimax rule. The green line represents a Bayes rule for a specific prior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#the-likelihood-function",
    "href": "introstatinf.html#the-likelihood-function",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.5 The Likelihood Function",
    "text": "1.5 The Likelihood Function\nThe bridge between probability and statistics is the Likelihood Function.\n\nDefinition 1.2 (Likelihood Function) Let \\(f(x_1, \\dots, x_n; \\theta)\\) be the joint probability density (or mass) function of the data given the parameter \\(\\theta\\). When we view this function as a function of \\(\\theta\\) for fixed observed data \\(x_1, \\dots, x_n\\), we call it the likelihood function, denoted \\(L(\\theta)\\). \\[L(\\theta) = f(x_1, \\dots, x_n; \\theta)\\]\n\n\nExample: Lady Tasting Tea\nFor our Tea Tasting data, the likelihood is proportional to the Binomial probability: \\[L(\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}\\]\n\nn=10 (k=7)n=40 (k=28)\n\n\nHere, \\(L(\\theta) = \\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\).\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{10}{7} \\theta^{7} (1-\\theta)^{3}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n120 \\(\\times 0^{7} \\times 1^{3}\\)\n0.0000\n\n\n0.2\n120 \\(\\times 0.2^{7} \\times 0.8^{3}\\)\n0.0008\n\n\n0.4\n120 \\(\\times 0.4^{7} \\times 0.6^{3}\\)\n0.0425\n\n\n0.6\n120 \\(\\times 0.6^{7} \\times 0.4^{3}\\)\n0.2150\n\n\n0.7\n120 \\(\\times 0.7^{7} \\times 0.3^{3}\\)\n0.2668 (Max)\n\n\n0.8\n120 \\(\\times 0.8^{7} \\times 0.2^{3}\\)\n0.2013\n\n\n1.0\n120 \\(\\times 1^{7} \\times 0^{3}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_small, k_small) * theta^k_small * (1 - theta)^(n_small-k_small) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_small/n_small, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_small/n_small, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_small/n_small), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_small, \", k=\", k_small, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.2: Likelihood Function (n= 10 )\n\n\n\n\n\n\n\nHere, \\(L(\\theta) = \\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\). Notice how the likelihood becomes narrower (more peaked) with more data, even though the peak remains at 0.7.\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nCalculation \\(\\binom{40}{28} \\theta^{28} (1-\\theta)^{12}\\)\n\\(L(\\theta)\\)\n\n\n\n\n0.0\n5.5868535^{9} \\(\\times 0^{28} \\times 1^{12}\\)\n0.0000\n\n\n0.2\n5.5868535^{9} \\(\\times 0.2^{28} \\times 0.8^{12}\\)\n0.0000\n\n\n0.4\n5.5868535^{9} \\(\\times 0.4^{28} \\times 0.6^{12}\\)\n0.0001\n\n\n0.6\n5.5868535^{9} \\(\\times 0.6^{28} \\times 0.4^{12}\\)\n0.0576\n\n\n0.7\n5.5868535^{9} \\(\\times 0.7^{28} \\times 0.3^{12}\\)\n0.1366 (Max)\n\n\n0.8\n5.5868535^{9} \\(\\times 0.8^{28} \\times 0.2^{12}\\)\n0.0443\n\n\n1.0\n5.5868535^{9} \\(\\times 1^{28} \\times 0^{12}\\)\n0.0000\n\n\n\n\n\nCode\nlikelihood_fun &lt;- function(theta) { choose(n_large, k_large) * theta^k_large * (1 - theta)^(n_large-k_large) }\ntheta_vals &lt;- seq(0, 1, length.out = 200)\ndf &lt;- data.frame(theta = theta_vals, Likelihood = likelihood_fun(theta_vals))\n\nggplot(df, aes(x = theta, y = Likelihood)) +\n  geom_line(color = \"darkblue\", size = 1.2) +\n  geom_vline(xintercept = k_large/n_large, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = k_large/n_large, y = max(df$Likelihood)/4, label = paste(\"Max at\", k_large/n_large), color = \"red\", angle = 90, vjust = -0.5) +\n  labs(title = TeX(paste0(\"Likelihood $L(\\\\theta)$ for $n=\", n_large, \", k=\", k_large, \"$\")),\n       x = TeX(r'(Parameter $\\theta$)'),\n       y = TeX(r'(Likelihood $L(\\theta)$)')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.3: Likelihood Function (n= 40 )\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nIs an estimator like \\(\\bar x\\), which is called Maximum Likelihood Estimator (MLE), a good estimator in general?\nWhat do you discover from actually observing the two likelihood unctions of different sample size \\(n\\)?\nIs the likelihood function central to all inference problems?\nWhat are the essential ‘parameters’ of the likelihood function?\n\nThere are two primary frameworks for “How” to perform these inferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#frequentist-inference-fisher",
    "href": "introstatinf.html#frequentist-inference-fisher",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Frequentist Inference (Fisher)",
    "text": "1.6 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#bayesian-inference",
    "href": "introstatinf.html#bayesian-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.7 Bayesian Inference",
    "text": "1.7 Bayesian Inference\n\nConcept: \\(\\theta\\) is regarded as a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\nExample: Bayesian Analysis of the Lady Tasting Tea\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: \\(\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)\\)\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: \\(\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)\\).\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.7.1 Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not or general inference?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?\nPrior: How to choose our prior? What’s its effect on our inference?\nModel Comparison and Assessment: How to assess a Bayesian model?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#motivating-example-the-lady-tasting-tea",
    "href": "introstatinf.html#motivating-example-the-lady-tasting-tea",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 Motivating Example: The Lady Tasting Tea",
    "text": "1.3 Motivating Example: The Lady Tasting Tea\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare \\(n\\) cups of tea.\n\nRandom Variable: Let \\(X_i=1\\) if she identifies the cup correctly, and \\(0\\) otherwise.\nParameter: Let \\(\\theta\\) be the probability that she correctly identifies a cup.\nThe Data: Suppose we observe that she identifies 70% of cups correctly (\\(\\bar{x} = 0.7\\)).\n\n\nSmall Sample (n=10)Large Sample (n=40)\n\n\nWe observe 7 out of 10 correct (\\(k=7\\)). \\[\\bar{x} = 0.7\\]\n\n\nWe observe 28 out of 40 correct (\\(k=28\\)). \\[\\bar{x} = 0.7\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#paradigms-of-inference",
    "href": "introstatinf.html#paradigms-of-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Paradigms of Inference",
    "text": "1.6 Paradigms of Inference\nThere are two primary frameworks for “How” to perform these inferences.\n\n1.6.1 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\n1.6.1.1 Application: Frequentist Test of the Tea Lady\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1.2 Methodologies & Challenges (Frequentist)\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE Construction: How do we find the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for complex models where no closed-form solution exists?\nComparing Estimator Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? If exact derivation is impossible, how do we use Asymptotic Theory to prove \\(\\hat{\\theta} \\overset{d}{\\to} N(\\theta, I^{-1}(\\theta))\\)?\nConfidence Intervals: How do we construct Confidence Intervals for parameters in general multiparameter models?\nHypothesis Testing: How do we derive powerful tests (like the Likelihood Ratio Test) and determine their critical values?\n\n\n\n\n1.6.2 Bayesian Inference\n\nConcept: \\(\\theta\\) is a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\n1.6.2.1 Application: Bayesian Analysis of the Tea Lady\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: $(1+7, 1+3) = (8, 4)`.\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: $(1+28, 1+12) = (29, 13)`. The posterior is taller and narrower, indicating higher certainty.\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.2.2 Methodologies & Challenges (Bayesian)\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#key-questions-in-statistical-inference",
    "href": "introstatinf.html#key-questions-in-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Key Questions in Statistical Inference",
    "text": "1.4 Key Questions in Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "href": "introstatinf.html#a-motivating-example-the-lady-tasting-tea",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.3 A Motivating Example: The Lady Tasting Tea",
    "text": "1.3 A Motivating Example: The Lady Tasting Tea\nTo illustrate the concepts of statistical inference, we consider the famous experiment described by R.A. Fisher.\nA lady claims she can distinguish whether milk was poured into the cup before or after the tea. To test this claim, we prepare \\(n\\) cups of tea.\n\nRandom Variable: Let \\(X_i=1\\) if she identifies the cup correctly, and \\(0\\) otherwise.\nParameter: Let \\(\\theta\\) be the probability that she correctly identifies a cup.\nThe Data: Suppose we observe that she identifies 70% of cups correctly (\\(\\bar{x} = 0.7\\)), which is a summary of the observed vector of \\(x_i\\), for example,\n\n\\[x=(0,1,1,0, 1,1,0,1,1,1)\\]\n\nSmall Sample (n=10)Large Sample (n=40)\n\n\nWe observe 7 out of 10 correct (\\(k=7\\)). \\[\\bar{x} = 0.7\\]\n\n\nWe observe 28 out of 40 correct (\\(k=28\\)). \\[\\bar{x} = 0.7\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#questions-answered-with-statistical-inference",
    "href": "introstatinf.html#questions-answered-with-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Questions Answered with Statistical Inference",
    "text": "1.4 Questions Answered with Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nModel Assessment\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nExample: Can we use a reduced model? What level of complexity of \\(f(x; \\theta)\\) is necessary?\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#two-paradigms-of-inference",
    "href": "introstatinf.html#two-paradigms-of-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Two Paradigms of Inference",
    "text": "1.6 Two Paradigms of Inference\nThere are two primary frameworks for “How” to perform these inferences.\n\n1.6.1 Frequentist Inference (Fisher)\n\nConcept: \\(\\theta\\) is fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?\n\n\n\n\n1.6.2 Bayesian Inference\n\nConcept: \\(\\theta\\) is a random variable.\nPosterior: Posterior \\(\\propto\\) Likelihood \\(\\times\\) Prior.\n\n\nExample: Bayesian Analysis of the Lady Tasting Tea\nPrior: \\(\\text{Beta}(1,1)\\) (Uniform).\n\nn=10 (k=7)n=40 (k=28)\n\n\nPosterior: \\(\\text{Beta}(1+7, 1+3) = \\text{Beta}(8, 4)\\)\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_small, 1+(n_small-k_small))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_small, 1+(n_small-k_small), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_small, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.6: Bayesian Update (n= 10 )\n\n\n\n\n\n\n\nPosterior: \\(\\text{Beta}(1+28, 1+12) = \\text{Beta}(29, 13)\\).\n\n\nCode\ntheta_grid &lt;- seq(0, 1, length.out = 200)\nposterior &lt;- dbeta(theta_grid, 1+k_large, 1+(n_large-k_large))\nprior &lt;- dbeta(theta_grid, 1, 1)\n\nprob_skill &lt;- pbeta(0.5, 1+k_large, 1+(n_large-k_large), lower.tail = FALSE)\n\ndf_bayes &lt;- data.frame(\n  Theta = rep(theta_grid, 2),\n  Density = c(prior, posterior),\n  Type = rep(c(\"Prior\", \"Posterior\"), each = 200)\n)\n\nggplot(df_bayes, aes(x = Theta, y = Density, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"gray\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  annotate(\"label\", x = 0.1, y = 2.5, \n           label = TeX(paste0(\"$P(\\\\theta &gt; 0.5 | x) = \", round(prob_skill, 3), \"$\")), \n           hjust = 0, color=\"blue\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Bayesian Update ($n=\", n_large, \"$)\")),\n       x = TeX(r'($\\theta$)'), y = \"Density\") +\n  theme_minimal() + theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 1.7: Bayesian Update (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.2.1 Questions to Answer\nWe will also tackle the specific technical challenges involved in Bayesian analysis.\n\nPosterior Derivation: How do we derive the posterior distribution \\(f(\\theta|x)\\) for various likelihoods and priors?\nComparing with Other methods: Are Bayesain methods good or not or general inference?\nComputation: When the posterior cannot be derived analytically, how do we use computational techniques like Markov Chain Monte Carlo (MCMC) to sample from it?\nSummarization: How do we construct Credible Intervals (e.g., Highest Posterior Density regions) from posterior samples?\nPrediction: How do we solve the integral required to compute the posterior predictive distribution for future data?\nPrior: How to choose our prior? What’s its effect on our inference?\nModel Comparison and Assessment: How to assess a Bayesian model?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "href": "introstatinf.html#questions-to-answer-in-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.4 Questions to Answer in Statistical Inference",
    "text": "1.4 Questions to Answer in Statistical Inference\nUsing this example, we identify the four main types of statistical inference.\n\nPoint Estimation\nWe want to use a single number to capture the parameter: \\(\\hat{\\theta} = \\theta(X_1, \\dots, X_n)\\).\n\nTea Example: Our best guess for her success rate is \\(\\hat{\\theta} = 0.7\\).\n\n\n\nHypothesis Testing\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nTea Example: Is she just guessing? We test \\(H_0: \\theta = 0.5\\) vs \\(H_1: \\theta &gt; 0.5\\).\n\n\n\nModel Assessment\nWe want to test a theory about the parameter: \\(H_0\\) vs \\(H_1\\).\n\nExample: Can we use a reduced model? What level of complexity of \\(f(x; \\theta)\\) is necessary?\n\n\n\nInterval Estimation\nWe want to construct an interval likely to contain the parameter: \\(\\theta \\in (L, U)\\).\n\nTea Example: We might say her true skill \\(\\theta\\) is likely between \\(0.45\\) and \\(0.95\\).\n\n\n\nPrediction\nWe want to predict a new observation \\(Y_{n+1}\\) given previous data.\n\nTea Example: If we give her an \\((n+1)\\)-th cup, what is the probability she identifies it correctly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#population-model-data-model",
    "href": "introstatinf.html#population-model-data-model",
    "title": "1  Introduction to Statistical Inference",
    "section": "",
    "text": "Definition 1.1 (Population Distribution) We assume that \\(X_1, X_2, \\dots, X_n \\sim f(x)\\). The function \\(f(x)\\) is called the population distribution.\n\n\nAssumptions and Scope\nFor simplicity, we often assume the data are Independent and Identically Distributed (i.i.d.). The assumption of identical distribution can be relaxed to regression settings in which the distributions of \\(x_i\\)’s are independent but dependent on covariate \\(x_i\\).\nIn Parametric Statistics, we assume \\(f(x)\\) is of a known analytic form but involves unknown parameters.\n\nExample 1.1 (Parametric Model: Normal) Consider the Normal distribution: \\[f(x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Here, the parameter space is \\(\\Theta = \\{ (\\mu, \\sigma^2) : \\mu \\in \\mathbb{R}, \\sigma \\in [0, +\\infty) \\}\\). The goal is to learn aspects of the unknown \\(\\theta\\) from observations \\(X_1, \\dots, X_n\\).\n\n\nExample 1.2 (Parametric Model: Bernoulli) Consider a sequence of binary outcomes (e.g., Success/Failure) where each \\(X_i \\in \\{0, 1\\}\\). We assume \\(X_i \\sim \\text{Bernoulli}(\\theta)\\). The probability mass function is: \\[f(x; \\theta) = \\theta^x (1-\\theta)^{1-x}\\] Here, the parameter space is \\(\\Theta = [0, 1]\\), where \\(\\theta\\) represents the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "href": "introstatinf.html#probabilistic-model-vs.-statistical-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.2 Probabilistic Model vs. Statistical Inference",
    "text": "1.2 Probabilistic Model vs. Statistical Inference\nThere is a fundamental distinction between probability and statistics regarding the parameter \\(\\theta\\). We can visualize this using a “shooting target” analogy:\n\n\\(\\theta\\) (The Center): The true, unknown bullseye location.\n\\(x\\) (The Shots): The observed holes on the target board.\nProbability (Deductive): The center \\(\\theta\\) is known. We predict where the shots \\(x\\) will land.\nStatistics (Inductive): The shots \\(x\\) are observed on the board. The center \\(\\theta\\) is unknown. We hypothesize different potential centers to see which one best explains the shots.\n\n\n\n\n\n\n\n\n\nFigure 1.1: Probability vs Statistics. Left: Probability—The model is fixed (Blue center/contours), generating random data. Right: Statistics—Data is fixed (Black points); we test two hypothesized models: H1 (Green) centered at the sample mean (Good Fit) and H2 (Red) shifted by (1.5, 1.5) (Bad Fit).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "introstatinf.html#frequentist-inference",
    "href": "introstatinf.html#frequentist-inference",
    "title": "1  Introduction to Statistical Inference",
    "section": "1.6 Frequentist Inference",
    "text": "1.6 Frequentist Inference\n\nConcept: \\(\\theta\\) is unknown but fixed; Data \\(X\\) is random.\nSampling Distribution: We analyze how \\(\\hat{\\theta}\\) behaves under hypothetical repeated sampling.\n\n\nExample: Frequentist Test of Lady Tasting Tea\nWe test \\(H_0: \\theta=0.5\\) (Guessing) vs \\(H_1: \\theta &gt; 0.5\\) (Skill). We analyze the behavior of \\(\\bar{X}\\) assuming \\(H_0\\) is true. The rejection region (one-sided) is shaded red.\n\nn=10 (k=7)n=40 (k=28)\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 7\\) correct out of 10, assuming \\(\\theta=0.5\\).\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_small\nprobs &lt;- dbinom(k_vals, size=n_small, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_small, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_small/n_small, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\n# Plot\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=5, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_small/n_small, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.25, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_small, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.4: Sampling Distribution (n= 10 )\n\n\n\n\n\n\n\nWe calculate the P-value: Probability of observing \\(\\ge 28\\) correct out of 40. With a larger sample size, the same proportion (0.7) provides stronger evidence against the null.\n\n\nCode\ntrue_theta &lt;- 0.5; \nk_vals &lt;- 0:n_large\nprobs &lt;- dbinom(k_vals, size=n_large, prob=true_theta)\ndf_exact &lt;- data.frame(x_bar = k_vals/n_large, prob = probs)\n\n# One-sided rejection region\ndf_exact$color_group &lt;- ifelse(df_exact$x_bar &gt;= k_large/n_large, \"Extreme\", \"Normal\")\np_val &lt;- sum(df_exact$prob[df_exact$color_group == \"Extreme\"])\n\nggplot() +\n  geom_segment(data=df_exact, aes(x=x_bar, xend=x_bar, y=0, yend=prob, color=color_group), \n               size=4, alpha=0.8) +\n  scale_color_manual(values=c(\"Extreme\"=\"red\", \"Normal\"=\"darkgreen\"), guide=\"none\") +\n  geom_vline(xintercept = k_large/n_large, color = \"blue\", size = 1) +\n  annotate(\"label\", x = 1.05, y = 0.15, \n           label = paste0(\"P-value = \", round(p_val, 3)), \n           hjust = 1, color=\"red\", fontface=\"bold\") +\n  labs(title = TeX(paste0(\"Sampling Distribution ($n=\", n_large, \"$)\")),\n       subtitle = TeX(r'(Testing $H_0: \\theta=0.5$ vs $H_1: \\theta &gt; 0.5$)'),\n       x = TeX(r'(Sample Mean $\\bar{x}$)'), y = \"Probability Mass\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1.5: Sampling Distribution (n= 40 )\n\n\n\n\n\n\n\n\n\n\n1.6.1 Questions to Answer\nIn this course, we will answer several challenging questions related to general parametric models in the Frequentist framework.\n\nMLE: Can we use the Maximum Likelihood Estimator (MLE) \\(\\hat{\\theta}\\) for general models even no closed-form solution exists? Is MLE a good method?\nSampling Distributions: What is the distribution of \\(\\hat{\\theta}_{\\text{MLE}}\\)? What’s its mean and standard deviation?\nConfidence Intervals: How to construct CI with \\(\\hat{\\theta}\\)?\nHypothesis Testing: How do we derive powerful tests from the likelihood function? How to assess goodness-of-fit of parametric models with their likelhiood information?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Inference</span>"
    ]
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Statistical Inference",
    "section": "Audience",
    "text": "Audience\nThis course requires a strong command of multivariate calculus, alongside a rigorous foundation in intermediate probability theory including asymptotic theorey for probability. Students should also possess prior exposure to applied statistical methods and familiar with basic statistical concepts such as p-value and confidence internal.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "bayesian.html#fundamental-elements-of-bayesian-inference",
    "href": "bayesian.html#fundamental-elements-of-bayesian-inference",
    "title": "3  Bayesian Methods",
    "section": "",
    "text": "Definition 3.1 (Posterior Distribution) Suppose we have a parameter \\(\\theta\\) with a prior distribution denoted by \\(\\pi(\\theta)\\). If we observe data \\(x\\) drawn from a distribution with probability density function (pdf) \\(f(x; \\theta)\\), then the posterior density of \\(\\theta\\) given the data \\(x\\) is defined as:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{m(x)}\n\\]\nwhere \\(m(x)\\) is the marginal distribution (or marginal likelihood) of the data, calculated as: \\[\nm(x) = \\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta\n\\]\nIn this context, \\(m(x)\\) acts as a normalizing constant. Since it depends only on the data \\(x\\) and not on the parameter \\(\\theta\\), it ensures that the posterior density integrates to 1 but does not influence the shape of the posterior distribution.\nThus, we often state the proportional relationship:\n\\[\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n\\]\n\n\nExample 3.1 (Binomial-beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\nNumerical Illustration:\nSuppose we are estimating a probability \\(\\theta\\).\n\nPrior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (Mean = 0.5).\nData: 10 trials, 8 successes (\\(n=10, x=8\\)).\nPosterior: \\(\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)\\) (Mean \\(\\approx\\) 0.71).\n\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\nCode\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Prior: Beta(2, 2)\nprior &lt;- dbeta(theta, shape1 = 2, shape2 = 2)\n\n# Posterior: Beta(10, 4)\nposterior &lt;- dbeta(theta, shape1 = 10, shape2 = 4)\n\nplot(theta, posterior, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = \"Beta Prior vs Posterior\", ylim = c(0, max(c(prior, posterior))))\nlines(theta, prior, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,4)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 3.1: Prior vs Posterior for Beta-Binomial Example\n\n\n\n\n\n\n\nExample 3.2 (Normal-normal Conjugacy (known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\nNumerical Illustration:\nSuppose we estimate a mean height \\(\\mu\\).\n\nKnown Variance: \\(\\sigma^2 = 100\\) (\\(\\tau = 0.01\\)).\nPrior: \\(\\mu \\sim N(175, 25)\\) (Precision \\(\\tau_0 = 0.04\\)).\nData: \\(n=10, \\bar{x}=180\\). (Total data precision \\(n\\tau = 0.1\\)).\nPosterior:\n\nPrecision \\(\\tau_1 = 0.04 + 0.1 = 0.14\\).\nVariance \\(\\sigma_1^2 \\approx 7.14\\).\nMean \\(\\mu_1 = \\frac{175(0.04) + 180(0.1)}{0.14} \\approx 178.6\\).\n\n\nThe plot below illustrates the prior (dashed) and posterior (solid) normal densities.\n\n\nCode\nmu_vals &lt;- seq(150, 200, length.out = 200)\n\n# Prior: N(175, 25) -&gt; SD = 5\nprior_norm &lt;- dnorm(mu_vals, mean = 175, sd = 5)\n\n# Posterior: N(178.6, 7.14) -&gt; SD = Sqrt(7.14) Approx 2.67\nposterior_norm &lt;- dnorm(mu_vals, mean = 178.6, sd = sqrt(7.14))\n\nplot(mu_vals, posterior_norm, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Density\",\n     main = \"Normal Prior vs Posterior\",\n     ylim = c(0, max(c(prior_norm, posterior_norm))))\nlines(mu_vals, prior_norm, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior N(175, 25)\", \"Posterior N(178.6, 7.14)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 3.2: Prior vs Posterior for Normal-Normal Example\n\n\n\n\n\n\n\nExample 3.3 (Discrete Posterior Calculation) Consider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). The calculation of the posterior probabilities is summarized in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta=1\\)\n\\(\\theta=2\\)\n\\(\\theta=3\\)\nSum\n\n\n\n\nPrior \\(\\pi(\\theta)\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1/3\\)\n\\(1\\)\n\n\nLikelihood \\(\\pi(x=2|\\theta)\\)\n\\(0\\)\n\\(1/3\\)\n\\(1/4\\)\n-\n\n\nProduct \\(\\pi(\\theta)\\pi(x|\\theta)\\)\n\\(0\\)\n\\(1/9\\)\n\\(1/12\\)\n\\(7/36\\)\n\n\nPosterior \\(\\pi(\\theta|x)\\)\n\\(0\\)\n\\(4/7\\)\n\\(3/7\\)\n\\(1\\)\n\n\n\nThe marginal sum (evidence) is calculated as \\(0 + 1/9 + 1/12 = 4/36 + 3/36 = 7/36\\). The posterior values are obtained by dividing the product row by this sum.\n\n\nExample 3.4 (Normal with Unknown Mean and Variance) Consider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is the product of the conditional and the marginal: \\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nDerivation of the Posterior:\nFirst, we write the likelihood in terms of the sufficient statistics \\(\\bar{x}\\) and \\(S_{xx} = \\sum (x_i - \\bar{x})^2\\): \\[\nL(\\mu, \\tau|x) \\propto \\tau^{n/2} \\exp\\left\\{ -\\frac{\\tau}{2} \\left[ S_{xx} + n(\\bar{x}-\\mu)^2 \\right] \\right\\}\n\\]\nMultiplying the prior by the likelihood gives the joint posterior: \\[\n\\begin{aligned}\n\\pi(\\mu, \\tau | x) &\\propto \\tau^{\\alpha - 1/2} e^{-\\beta\\tau} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2} \\cdot \\tau^{n/2} e^{-\\frac{\\tau}{2}S_{xx}} e^{-\\frac{n\\tau}{2}(\\mu-\\bar{x})^2} \\\\\n&\\propto \\tau^{\\alpha + n/2 - 1/2} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{1}{2}\\left( k(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 \\right) \\right] \\right\\}\n\\end{aligned}\n\\]\nNext, we complete the square for the terms involving \\(\\mu\\) inside the brackets. It can be shown that: \\[\nk(\\mu-\\nu)^2 + n(\\mu-\\bar{x})^2 = (k+n)\\left(\\mu - \\frac{k\\nu+n\\bar{x}}{k+n}\\right)^2 + \\frac{nk}{n+k}(\\bar{x}-\\nu)^2\n\\]\nSubstituting this back into the joint density and grouping terms that do not depend on \\(\\mu\\): \\[\n\\pi(\\mu, \\tau | x) \\propto \\underbrace{\\tau^{\\alpha + n/2 - 1} \\exp\\left\\{ -\\tau \\left[ \\beta + \\frac{S_{xx}}{2} + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2 \\right] \\right\\}}_{\\text{Marginal of } \\tau} \\cdot \\underbrace{\\tau^{1/2} \\exp\\left\\{ -\\frac{(k+n)\\tau}{2} \\left( \\mu - \\frac{k\\nu+n\\bar{x}}{k+n} \\right)^2 \\right\\}}_{\\text{Conditional of } \\mu|\\tau}\n\\]\nResults:\nBy inspecting the factored equation above, we identify the updated parameters:\n\nMarginal Posterior of \\(\\tau\\): The first part corresponds to a Gamma kernel \\(\\tau^{\\alpha' - 1} e^{-\\beta'\\tau}\\). \\[\\tau|x \\sim \\text{Gamma}(\\alpha', \\beta')\\] where \\(\\alpha' = \\alpha + n/2\\) and \\(\\beta' = \\beta + \\frac{1}{2}\\sum(x_i-\\bar{x})^2 + \\frac{nk}{2(n+k)}(\\bar{x}-\\nu)^2\\).\nConditional Posterior of \\(\\mu\\): The second part corresponds to a Normal kernel with precision \\(k'\\tau\\). \\[\\mu|\\tau, x \\sim N(\\nu', 1/(k'\\tau))\\] where \\(k' = k + n\\) and \\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-1-binomial-beta-conjugacy",
    "href": "bayesian.html#example-1-binomial-beta-conjugacy",
    "title": "3  Bayesian Methods",
    "section": "3.2 Example 1: Binomial-Beta Conjugacy",
    "text": "3.2 Example 1: Binomial-Beta Conjugacy\nConsider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-2-normal-normal-conjugacy-known-variance",
    "href": "bayesian.html#example-2-normal-normal-conjugacy-known-variance",
    "title": "3  Bayesian Methods",
    "section": "3.3 Example 2: Normal-Normal Conjugacy (Known Variance)",
    "text": "3.3 Example 2: Normal-Normal Conjugacy (Known Variance)\nLet \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#discrete-example",
    "href": "bayesian.html#discrete-example",
    "title": "3  Bayesian Methods",
    "section": "3.4 3.2 Discrete Example",
    "text": "3.4 3.2 Discrete Example\nConsider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#normal-with-unknown-mean-and-variance",
    "href": "bayesian.html#normal-with-unknown-mean-and-variance",
    "title": "3  Bayesian Methods",
    "section": "3.2 Normal with Unknown Mean and Variance",
    "text": "3.2 Normal with Unknown Mean and Variance\nConsider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is:\n\\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nMultiplying by the likelihood leads to a posterior of the same form (Conjugate), with updated parameters:\n\n\\(\\alpha' = \\alpha + n/2\\)\n\\(k' = k + n\\)\n\\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\)\n\\(\\beta' = \\beta + \\frac{1}{2} \\frac{nk}{n+k}(\\bar{x}-\\nu)^2 + \\frac{1}{2}\\sum (x_i - \\bar{x})^2\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#decision-theory-and-bayes-rules",
    "href": "bayesian.html#decision-theory-and-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.2 Decision Theory and Bayes Rules",
    "text": "3.2 Decision Theory and Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) satisfies: \\[\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n\\]\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\nTherefore, to minimize the Bayes risk, one must choose \\(d(x)\\) to minimize the posterior expected loss for each \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#common-loss-functions-and-estimators",
    "href": "bayesian.html#common-loss-functions-and-estimators",
    "title": "3  Bayesian Methods",
    "section": "3.3 Common Loss Functions and Estimators",
    "text": "3.3 Common Loss Functions and Estimators\n\n\n3.3.1 Squared Error Loss (point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule is the posterior mean.\n\n\n\n\n3.3.2 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) requires solving:\n\\[\\int_{-\\infty}^{d} \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = \\frac{1}{2}\\]\nResult: The Bayes rule is the posterior median.\n\n\n\n\n3.3.3 Hypothesis Testing (0-1 Loss)\nTesting \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\).\n\\[L(\\theta, a) = \\begin{cases} 1 & \\text{if error} \\\\ 0 & \\text{if correct} \\end{cases}\\]\nThe Bayes rule selects the hypothesis with the higher posterior probability.\n\\[d(x) = 1 \\iff P(\\theta \\in \\Theta_1 | x) \\ge P(\\theta \\in \\Theta_0 | x)\\]\n\n\n\nDefinition 3.3 (Highest Posterior Density (HPD) Interval) In interval estimation, we prescribe a set \\(A = (d-\\delta, d+\\delta)\\) and minimize the loss associated with \\(\\theta\\) falling outside this interval.\nThe Bayes rule \\(d(x)\\) is the center of the interval with the highest probability coverage. This leads to the Highest Posterior Density (HPD) interval.\nIn practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the Equal-Tailed Interval, where we cut off \\(\\alpha/2\\) probability from each tail.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#minimax-estimation",
    "href": "bayesian.html#minimax-estimation",
    "title": "3  Bayesian Methods",
    "section": "3.4 Minimax Estimation",
    "text": "3.4 Minimax Estimation\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Theorem) If a Bayes rule \\(d^\\pi\\) has constant risk (i.e., \\(R(\\theta, d^\\pi) = c\\) for all \\(\\theta\\)), then \\(d^\\pi\\) is a minimax estimator.\n\n\n\n3.4.1 Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#example-binomial-minimax-estimator",
    "href": "bayesian.html#example-binomial-minimax-estimator",
    "title": "3  Bayesian Methods",
    "section": "3.9 Example: Binomial Minimax Estimator",
    "text": "3.9 Example: Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#stein-estimation-and-shrinkage",
    "href": "bayesian.html#stein-estimation-and-shrinkage",
    "title": "3  Bayesian Methods",
    "section": "3.5 Stein Estimation and Shrinkage",
    "text": "3.5 Stein Estimation and Shrinkage\nConsider estimating a multivariate mean vector \\(\\mu = (\\mu_1, \\dots, \\mu_p)\\) given independent observations \\(X_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nThe standard estimator is the MLE: \\(d^0(X) = X\\). The loss function is the sum of squared errors: \\(L(\\mu, d) = ||\\mu - d||^2 = \\sum (\\mu_i - d_i)^2\\).\n\nTheorem 3.3 (Stein’s Result) When \\(p \\ge 3\\), the estimator \\(d^0(X)\\) is inadmissible. There exists an estimator that strictly dominates it (has lower risk everywhere).\n\nConsider the class of shrinkage estimators: \\[d^a(X) = \\left( 1 - \\frac{a}{||X||^2} \\right) X\\] where \\(X = (X_1, \\dots, X_p)^T\\).\nWhen \\(a &gt; 0\\), this estimator “shrinks” the data vector toward the origin \\((0, \\dots, 0)\\).\n\nLemma 3.1 (Stein’s Lemma) If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\n\n\nProof. Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#steins-lemma",
    "href": "bayesian.html#steins-lemma",
    "title": "3  Bayesian Methods",
    "section": "3.11 Stein’s Lemma",
    "text": "3.11 Stein’s Lemma\nTo prove dominance, we use Stein’s Lemma. If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\nUsing this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#predictive-distributions",
    "href": "bayesian.html#predictive-distributions",
    "title": "3  Bayesian Methods",
    "section": "3.7 Predictive Distributions",
    "text": "3.7 Predictive Distributions\nA key feature of Bayesian analysis is the predictive distribution for a future observation \\(x^*\\).\n\\[f(x^*|x) = \\int f(x^*|\\theta) \\pi(\\theta|x) d\\theta\\]\n\nExample 3.7 (Normal-normal Predictive Distribution) If \\(x_1, \\dots, x_n \\sim N(\\mu, \\sigma^2)\\) (with \\(\\sigma^2\\) known) and \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\), the predictive distribution for a new observation \\(x^*\\) is:\n\\[x^*|x \\sim N(\\mu_1, \\sigma^2 + \\sigma_1^2)\\]\nwhere \\(\\mu_1\\) and \\(\\sigma_1^2\\) are the posterior mean and variance of \\(\\mu\\). The predictive variance includes both the inherent sampling uncertainty (\\(\\sigma^2\\)) and the uncertainty about the parameter (\\(\\sigma_1^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#hierarchical-modeling-and-mcmc",
    "href": "bayesian.html#hierarchical-modeling-and-mcmc",
    "title": "3  Bayesian Methods",
    "section": "3.8 Hierarchical Modeling and MCMC",
    "text": "3.8 Hierarchical Modeling and MCMC\nWhen analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).\nHierarchical Structure:\n\nData: \\(X_i | \\mu_i \\sim f(x_i|\\mu_i)\\)\nParameters: \\(\\mu_i | \\theta \\sim \\pi(\\mu_i|\\theta)\\)\nHyperparameters: \\(\\theta \\sim \\pi(\\theta)\\)\n\nGibbs Sampling: To estimate the posterior \\(f(\\mu, \\theta | x)\\), we sample iteratively from the full conditional distributions:\n\nSample \\(\\mu_i\\) from \\(f(\\mu_i | x, \\theta)\\).\nSample \\(\\theta\\) from \\(f(\\theta | \\mu, x)\\).\n\n\nExample 3.8 (Baseball Example with Hierarchical Model)  \n\n\\(Y_i \\sim \\text{Bin}(n_i, p_i)\\)\nLogit transform: \\(\\mu_i = \\text{logit}(p_i)\\)\n\\(\\mu_i \\sim N(\\theta, \\tau^2)\\)\nPriors on \\(\\theta\\) and \\(\\tau^2\\).\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use Metropolis-Hastings steps within the Gibbs sampler.\nAlgorithm:\n\nInitialize parameters \\(\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}\\).\nPropose new values based on a candidate distribution.\nAccept or reject based on the acceptance probability ratio (Likelihood \\(\\times\\) Prior ratio).\nRepeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., \\(f(\\mu_j|x)\\)) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#discrete-parameter-space",
    "href": "bayesian.html#discrete-parameter-space",
    "title": "3  Bayesian Methods",
    "section": "3.2 Discrete Parameter Space",
    "text": "3.2 Discrete Parameter Space\n\n\n3.2.1 Discrete Posterior Calculation\nConsider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#fundamental-elements-of-bayesian-inference-1",
    "href": "bayesian.html#fundamental-elements-of-bayesian-inference-1",
    "title": "3  Bayesian Methods",
    "section": "4.1 Fundamental Elements of Bayesian Inference",
    "text": "4.1 Fundamental Elements of Bayesian Inference\nThe foundation of Bayesian inference relies on the relationship between the prior distribution, the likelihood of the data, and the posterior distribution. This relationship is governed by Bayes’ Theorem (or Law).\n\nTheorem 4.1 (Bayes’ Theorem) Suppose we have a parameter \\(\\theta\\) with a prior distribution denoted by \\(\\pi(\\theta)\\). If we observe data \\(x\\) drawn from a distribution with probability density function (pdf) \\(f(x; \\theta)\\), then the posterior density of \\(\\theta\\) given the data \\(x\\) is defined as:\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta) f(x;\\theta)}{\\int_{\\Theta} \\pi(\\theta) f(x;\\theta) d\\theta}\n\\]\nIn this equation:\n\n\\(\\pi(\\theta)\\) is the prior.\n\\(f(x;\\theta)\\) is the likelihood.\nThe denominator is the marginal distribution of \\(x\\), often represented as a normalizing constant \\(c(x)\\) which is free of \\(\\theta\\).\n\nThus, we can state the proportional relationship:\n\\[\n\\pi(\\theta|x) \\propto \\pi(\\theta) f(x;\\theta)\n\\]\n\n\nExample 4.1 (Binomial-Beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\n\n\nExample 4.2 (Normal-Normal Conjugacy (Known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#numerical-illustrations-of-conjugacy",
    "href": "bayesian.html#numerical-illustrations-of-conjugacy",
    "title": "3  Bayesian Methods",
    "section": "4.2 Numerical Illustrations of Conjugacy",
    "text": "4.2 Numerical Illustrations of Conjugacy\nThe following examples visualize the shift in belief from prior to posterior using specific numerical values.\n\nExample 4.7 (Numerical Illustration: Binomial-Beta) Suppose we are estimating a probability \\(\\theta\\). * Prior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (Mean = 0.5). * Data: 10 trials, 8 successes (\\(n=10, x=8\\)). * Posterior: \\(\\theta|x \\sim \\text{Beta}(2+8, 2+2) = \\text{Beta}(10, 4)\\) (Mean \\(\\approx\\) 0.71).\nThe plot below shows the prior (dashed) and posterior (solid) densities.\n\n\nCode\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# Prior: Beta(2, 2)\nprior &lt;- dbeta(theta, shape1 = 2, shape2 = 2)\n\n# Posterior: Beta(10, 4)\nposterior &lt;- dbeta(theta, shape1 = 10, shape2 = 4)\n\nplot(theta, posterior, type = 'l', lwd = 2, col = \"blue\",\n     xlab = expression(theta), ylab = \"Density\",\n     main = \"Beta Prior vs Posterior\", ylim = c(0, max(c(prior, posterior))))\nlines(theta, prior, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,4)\"),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 4.1: Prior vs Posterior for Beta-Binomial Example\n\n\n\n\n\n\nExample 4.3 (Binomial-Beta Conjugacy) Consider an experiment where \\(x|\\theta \\sim \\text{Bin}(n, \\theta)\\). The likelihood function is:\n\\[\nf(x|\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}\n\\]\nSuppose we choose a Beta distribution as the prior for \\(\\theta\\), such that \\(\\theta \\sim \\text{Beta}(a, b)\\). The prior density is:\n\\[\n\\pi(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere \\(B(a,b)\\) is the Beta function defined as \\(\\int_{0}^{1} \\theta^{a-1}(1-\\theta)^{b-1} d\\theta\\).\nTo find the posterior, we multiply the prior and the likelihood:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a-1}(1-\\theta)^{b-1} \\cdot \\theta^x (1-\\theta)^{n-x}\n\\]\nCombining terms with the same base:\n\\[\n\\pi(\\theta|x) \\propto \\theta^{a+x-1} (1-\\theta)^{b+n-x-1}\n\\]\nWe can recognize this kernel as a Beta distribution. Therefore, we conclude that the posterior distribution is:\n\\[\n\\theta|x \\sim \\text{Beta}(a+x, b+n-x)\n\\]\nProperties of the Posterior:\n\nThe posterior mean is: \\[E(\\theta|x) = \\frac{a+x}{a+b+n}\\] As \\(n \\to \\infty\\), this approximates the maximum likelihood estimate \\(\\frac{x}{n}\\).\nThe posterior variance is: \\[\\text{Var}(\\theta|x) = \\frac{(a+x)(n+b-x)}{(a+b+n)^2(a+b+n+1)}\\] For large \\(n\\), this approximates \\(\\frac{x(n-x)}{n^3} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\).\n\n\n\nExample 4.4 (Normal-Normal Conjugacy (Known Variance)) Let \\(X_1, X_2, \\dots, X_n\\) be independent and identically distributed (i.i.d.) variables such that \\(X_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known.\nWe assign a Normal prior to the mean \\(\\mu\\): \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\).\nTo find the posterior \\(\\pi(\\mu|x_1, \\dots, x_n)\\), let \\(x = (x_1, \\dots, x_n)\\). The posterior is proportional to:\n\\[\n\\pi(\\mu|x) \\propto \\pi(\\mu) \\cdot f(x|\\mu)\n\\]\n\\[\n\\propto \\exp\\left\\{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2}\\right\\} \\cdot \\exp\\left\\{-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nExpanding the squares and collecting terms involving \\(\\mu\\) (completing the square), we find that the posterior is also a Normal distribution \\(\\mu|x \\sim N(\\mu_1, \\sigma_1^2)\\).\nPosterior Precision:\nIt is often more convenient to work with precision (the inverse of variance). Let:\n\n\\(\\tau_0 = 1/\\sigma_0^2\\) (Prior precision)\n\\(\\tau = 1/\\sigma^2\\) (Data precision)\n\\(\\tau_1 = 1/\\sigma_1^2\\) (Posterior precision)\n\nThe relationship is additive:\n\\[\n\\tau_1 = \\tau_0 + n\\tau\n\\]\n\\[\n\\text{Posterior Precision} = \\text{Prior Precision} + \\text{Precision of Data}\n\\]\nThe posterior mean \\(\\mu_1\\) is a weighted average of the prior mean and the sample mean:\n\\[\n\\mu_1 = \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}\n\\]\nSo, the posterior distribution is:\n\\[\n\\mu|x_1, \\dots, x_n \\sim N\\left( \\frac{\\mu_0 \\tau_0 + n\\bar{x}\\tau}{\\tau_0 + n\\tau}, \\frac{1}{\\tau_0 + n\\tau} \\right)\n\\]\n\n\nExample 4.5 (Discrete Posterior Calculation) Consider the following table where we calculate the posterior probabilities for a discrete parameter space.\nLet the parameter \\(\\theta\\) take values \\(\\{1, 2, 3\\}\\) with prior probabilities \\(\\pi(\\theta)\\). Let the data \\(x\\) take values \\(\\{0, 1, 2, \\dots\\}\\).\nGiven:\n\nPrior \\(\\pi(\\theta)\\): \\(\\pi(1)=1/3, \\pi(2)=1/3, \\pi(3)=1/3\\).\nLikelihood \\(\\pi(x|\\theta)\\):\n\nIf \\(\\theta=1\\), \\(x \\sim \\text{Uniform on } \\{0, 1\\}\\) (Prob = 1/2).\nIf \\(\\theta=2\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2\\}\\) (Prob = 1/3).\nIf \\(\\theta=3\\), \\(x \\sim \\text{Uniform on } \\{0, 1, 2, 3\\}\\) (Prob = 1/4).\n\n\nSuppose we observe \\(x=2\\). We calculate the posterior \\(\\pi(\\theta|x=2)\\):\n\\[\n\\pi(\\theta|x) = \\frac{\\pi(\\theta)\\pi(x|\\theta)}{\\sum \\pi(\\theta)\\pi(x|\\theta)}\n\\]\n\nFor \\(\\theta=1\\): \\(\\pi(x=2|1) = 0\\). Product = 0.\nFor \\(\\theta=2\\): \\(\\pi(x=2|2) = 1/3\\). Product = \\(1/3 \\times 1/3 = 1/9\\).\nFor \\(\\theta=3\\): \\(\\pi(x=2|3) = 1/4\\). Product = \\(1/3 \\times 1/4 = 1/12\\).\n\nThe marginal sum is \\(0 + 1/9 + 1/12 = 7/36\\). The posterior probabilities are:\n\n\\(\\pi(1|x=2) = 0\\)\n\\(\\pi(2|x=2) = (1/9) / (7/36) = 4/7\\)\n\\(\\pi(3|x=2) = (1/12) / (7/36) = 3/7\\)\n\n\n\nExample 4.6 (Normal with Unknown Mean and Variance) Consider \\(X_1, \\dots, X_n \\sim N(\\mu, 1/\\tau)\\), where both \\(\\mu\\) and the precision \\(\\tau\\) are unknown.\nWe use a Normal-Gamma conjugate prior:\n\n\\(\\tau \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[\\pi(\\tau) \\propto \\tau^{\\alpha-1} e^{-\\beta\\tau}\\]\n\\(\\mu|\\tau \\sim N(\\nu, 1/(k\\tau))\\) \\[\\pi(\\mu|\\tau) \\propto \\tau^{1/2} e^{-\\frac{k\\tau}{2}(\\mu-\\nu)^2}\\]\n\nThe joint prior is:\n\\[\n\\pi(\\mu, \\tau) \\propto \\tau^{\\alpha - 1/2} \\exp\\left\\{ -\\tau \\left( \\beta + \\frac{k}{2}(\\mu - \\nu)^2 \\right) \\right\\}\n\\]\nMultiplying by the likelihood leads to a posterior of the same form (Conjugate), with updated parameters:\n\n\\(\\alpha' = \\alpha + n/2\\)\n\\(k' = k + n\\)\n\\(\\nu' = \\frac{k\\nu + n\\bar{x}}{k+n}\\)\n\\(\\beta' = \\beta + \\frac{1}{2} \\frac{nk}{n+k}(\\bar{x}-\\nu)^2 + \\frac{1}{2}\\sum (x_i - \\bar{x})^2\\)\n\n\n\n4.3 Decision Theory and Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 4.1 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\nMinimizing the Bayes Risk is equivalent to minimizing the expected loss for each \\(x\\). The quantity to minimize is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\n\n4.3.1 Common Loss Functions and Estimators\n\n\n\n4.3.2 Squared Error Loss (Point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule is the posterior mean.\n\n\n\n\n4.3.3 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nMinimizing \\(E_{\\theta|x}[|\\theta - d|]\\) requires solving:\n\\[\\int_{-\\infty}^{d} \\pi(\\theta|x) d\\theta = \\int_{d}^{\\infty} \\pi(\\theta|x) d\\theta = \\frac{1}{2}\\]\nResult: The Bayes rule is the posterior median.\n\n\n\n\n4.3.4 Hypothesis Testing (0-1 Loss)\nTesting \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\).\n\\[L(\\theta, a) = \\begin{cases} 1 & \\text{if error} \\\\ 0 & \\text{if correct} \\end{cases}\\]\nThe Bayes rule selects the hypothesis with the higher posterior probability.\n\\[d(x) = 1 \\iff P(\\theta \\in \\Theta_1 | x) \\ge P(\\theta \\in \\Theta_0 | x)\\]\n\n\n\nDefinition 4.2 (Highest Posterior Density (HPD) Interval) In interval estimation, we prescribe a set \\(A = (d-\\delta, d+\\delta)\\) and minimize the loss associated with \\(\\theta\\) falling outside this interval.\nThe Bayes rule \\(d(x)\\) is the center of the interval with the highest probability coverage. This leads to the Highest Posterior Density (HPD) interval.\nIn practice, if the posterior is unimodal and symmetric (like the Normal distribution), the HPD interval coincides with the Equal-Tailed Interval, where we cut off \\(\\alpha/2\\) probability from each tail.\n\n\n\n4.4 Minimax Estimation\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 4.2 (Constant Risk Theorem) If a Bayes rule \\(d^\\pi\\) has constant risk (i.e., \\(R(\\theta, d^\\pi) = c\\) for all \\(\\theta\\)), then \\(d^\\pi\\) is a minimax estimator.\n\n\n\n4.4.1 Binomial Minimax Estimator\nLet \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).\n\n\n\n\n4.5 Stein Estimation and Shrinkage\nConsider estimating a multivariate mean vector \\(\\mu = (\\mu_1, \\dots, \\mu_p)\\) given independent observations \\(X_i \\sim N(\\mu_i, 1)\\) for \\(i=1, \\dots, p\\).\nThe standard estimator is the MLE: \\(d^0(X) = X\\). The loss function is the sum of squared errors: \\(L(\\mu, d) = ||\\mu - d||^2 = \\sum (\\mu_i - d_i)^2\\).\n\nTheorem 4.3 (Stein’s Result) When \\(p \\ge 3\\), the estimator \\(d^0(X)\\) is inadmissible. There exists an estimator that strictly dominates it (has lower risk everywhere).\n\nConsider the class of shrinkage estimators: \\[d^a(X) = \\left( 1 - \\frac{a}{||X||^2} \\right) X\\] where \\(X = (X_1, \\dots, X_p)^T\\).\nWhen \\(a &gt; 0\\), this estimator “shrinks” the data vector toward the origin \\((0, \\dots, 0)\\).\n\nLemma 4.1 (Stein’s Lemma) If \\(X \\sim N(\\mu, 1)\\), then for a differentiable function \\(h\\): \\[E[(X-\\mu)h(X)] = E[h'(X)]\\]\n\n\nProof. Using this lemma and integration by parts, we can evaluate the risk of the shrinkage estimator \\(d^a\\).\n\\[R(\\mu, d^a) = E || \\mu - d^a(X) ||^2\\]\nAfter expanding and applying Stein’s Lemma, the risk becomes: \\[R(\\mu, d^a) = p - [2a(p-2) - a^2] E \\left( \\frac{1}{||X||^2} \\right)\\]\nFor \\(d^a\\) to possess lower risk than \\(d^0\\) (where risk = \\(p\\)), we need the term in the brackets to be positive: \\[2a(p-2) - a^2 &gt; 0 \\implies 0 &lt; a &lt; 2(p-2)\\]\nThe optimal choice (minimizing risk) is \\(a = p-2\\). This yields the James-Stein Estimator: \\[\\delta^{JS}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\n\n\n\n4.6 Empirical Bayes\nThe James-Stein estimator can be motivated via an Empirical Bayes approach.\nModel:\n\n\\(X_i | \\mu_i \\sim N(\\mu_i, 1)\\)\nPrior: \\(\\mu_i \\sim N(0, \\tau^2)\\)\n\nThe posterior mean for \\(\\mu_i\\) (if \\(\\tau^2\\) were known) is: \\[E(\\mu_i|x_i) = \\frac{\\tau^2}{1+\\tau^2} x_i = \\left( 1 - \\frac{1}{1+\\tau^2} \\right) x_i\\]\nThe marginal distribution of \\(X_i\\) is \\(N(0, 1+\\tau^2)\\). Consequently, \\(S = \\sum X_i^2 \\sim (1+\\tau^2) \\chi^2_p\\).\nWe can estimate the unknown shrinkage factor \\(B = \\frac{1}{1+\\tau^2}\\) using the data. Since \\(E[ \\frac{p-2}{S} ] = \\frac{1}{1+\\tau^2}\\), we replace the theoretical shrinkage factor with its unbiased estimate: \\[\\hat{B} = \\frac{p-2}{||X||^2}\\]\nThis recovers the James-Stein rule: \\[\\delta^{EB}(X) = \\left( 1 - \\frac{p-2}{||X||^2} \\right) X\\]\n\n\n4.6.1 Baseball Example (Efron & Morris)\nWe illustrate Stein estimation using baseball batting averages. Let \\(y_i\\) be the number of hits for player \\(i\\) in their first \\(n=45\\) at-bats. Let \\(\\hat{p}_i = y_i/n\\) be the observed average.\nTo apply the Normal model, we use a variance-stabilizing transformation: \\[X_i = \\sqrt{n} \\arcsin(2\\hat{p}_i - 1)\\] Under this transformation, \\(X_i \\approx N(\\mu_i, 1)\\).\nUsing the James-Stein estimator on the transformed data shrinks the individual averages toward the grand mean (or a specific value \\(\\mu_0\\)). Result: The James-Stein estimator provides a lower total prediction error for the rest of the season compared to the individual averages \\(\\hat{p}_i\\).\n\n\n\n\n4.7 Predictive Distributions\nA key feature of Bayesian analysis is the predictive distribution for a future observation \\(x^*\\).\n\\[f(x^*|x) = \\int f(x^*|\\theta) \\pi(\\theta|x) d\\theta\\]\n\n\n4.7.1 Normal-Normal Predictive Distribution\nIf \\(x_1, \\dots, x_n \\sim N(\\mu, \\sigma^2)\\) (with \\(\\sigma^2\\) known) and \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\), the predictive distribution for a new observation \\(x^*\\) is:\n\\[x^*|x \\sim N(\\mu_1, \\sigma^2 + \\sigma_1^2)\\]\nwhere \\(\\mu_1\\) and \\(\\sigma_1^2\\) are the posterior mean and variance of \\(\\mu\\). The predictive variance includes both the inherent sampling uncertainty (\\(\\sigma^2\\)) and the uncertainty about the parameter (\\(\\sigma_1^2\\)).\n\n\n\n\n4.8 Hierarchical Modeling and MCMC\nWhen analytic solutions are unavailable, we use Hierarchical Models and Markov Chain Monte Carlo (MCMC).\nHierarchical Structure:\n\nData: \\(X_i | \\mu_i \\sim f(x_i|\\mu_i)\\)\nParameters: \\(\\mu_i | \\theta \\sim \\pi(\\mu_i|\\theta)\\)\nHyperparameters: \\(\\theta \\sim \\pi(\\theta)\\)\n\nGibbs Sampling: To estimate the posterior \\(f(\\mu, \\theta | x)\\), we sample iteratively from the full conditional distributions:\n\nSample \\(\\mu_i\\) from \\(f(\\mu_i | x, \\theta)\\).\nSample \\(\\theta\\) from \\(f(\\theta | \\mu, x)\\).\n\n\n\n4.8.1 Baseball Example with Hierarchical Model\n\n\\(Y_i \\sim \\text{Bin}(n_i, p_i)\\)\nLogit transform: \\(\\mu_i = \\text{logit}(p_i)\\)\n\\(\\mu_i \\sim N(\\theta, \\tau^2)\\)\nPriors on \\(\\theta\\) and \\(\\tau^2\\).\n\nSince the full conditionals for the Binomial-Normal hierarchy are not closed-form, we use Metropolis-Hastings steps within the Gibbs sampler.\nAlgorithm:\n\nInitialize parameters \\(\\mu^{(0)}, \\theta^{(0)}, \\tau^{(0)}\\).\nPropose new values based on a candidate distribution.\nAccept or reject based on the acceptance probability ratio (Likelihood \\(\\times\\) Prior ratio).\nRepeat until convergence.\n\nThe marginal posterior density for a specific parameter (e.g., \\(f(\\mu_j|x)\\)) can be estimated using Kernel Density Estimation on the MCMC samples or via Rao-Blackwellization.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#paradigm-to-find-bayes-rules",
    "href": "bayesian.html#paradigm-to-find-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.2 Paradigm to Find Bayes Rules",
    "text": "3.2 Paradigm to Find Bayes Rules\nThe general form of Bayes rule is derived by minimizing risk.\n\nDefinition 3.2 (Risk Function and Bayes Risk)  \n\nRisk Function: \\(R(\\theta, d) = \\int_{X} L(\\theta, d(x)) f(x;\\theta) dx\\)\nBayes Risk: The expected risk with respect to the prior. \\[r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta\\]\n\n\n\nTheorem 3.1 (Minimization of Bayes Risk) Minimizing the Bayes risk \\(r(\\pi, d)\\) is equivalent to minimizing the posterior expected loss for each observed \\(x\\). That is, the Bayes rule \\(d(x)\\) satisfies: \\[\nd(x) = \\underset{a}{\\arg\\min} \\ E_{\\theta|x} [ L(\\theta, a) ]\n\\]\n\n\nProof. We start by writing the Bayes risk essentially as a double integral over the parameters and the data. Substituting the definition of the risk function \\(R(\\theta, d)\\):\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta \\\\\n&= \\int_{\\Theta} \\left[ \\int_{X} L(\\theta, d(x)) f(x|\\theta) dx \\right] \\pi(\\theta) d\\theta\n\\end{aligned}\n\\]\nAssuming the conditions for Fubini’s Theorem are met, we switch the order of integration:\n\\[\nr(\\pi, d) = \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) f(x|\\theta) \\pi(\\theta) d\\theta \\right] dx\n\\]\nRecall that the joint density can be factored as \\(f(x, \\theta) = f(x|\\theta)\\pi(\\theta) = \\pi(\\theta|x)m(x)\\), where \\(m(x)\\) is the marginal density of the data. Substituting this into the inner integral:\n\\[\n\\begin{aligned}\nr(\\pi, d) &= \\int_{X} \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) m(x) d\\theta \\right] dx \\\\\n&= \\int_{X} m(x) \\left[ \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta \\right] dx\n\\end{aligned}\n\\]\nSince the marginal density \\(m(x)\\) is non-negative, minimizing the total integral \\(r(\\pi, d)\\) with respect to the decision rule \\(d(\\cdot)\\) is equivalent to minimizing the term inside the brackets for every \\(x\\) (specifically where \\(m(x) &gt; 0\\)).\nThe term inside the brackets is the Posterior Expected Loss:\n\\[\n\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta|x) d\\theta = E_{\\theta|x} [ L(\\theta, d(x)) ]\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nTherefore, to minimize the Bayes risk, one just need to choose \\(d(x)\\) to minimize the posterior expected loss for each \\(x\\).\nThe following diagram summarizes the general workflow for deriving a Bayes estimator:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#common-loss-functions-and-bayes-estimators",
    "href": "bayesian.html#common-loss-functions-and-bayes-estimators",
    "title": "3  Bayesian Methods",
    "section": "3.3 Common Loss Functions and Bayes Estimators",
    "text": "3.3 Common Loss Functions and Bayes Estimators\n\n3.3.1 Squared Error Loss (Point Estimate)\n\\[L(\\theta, a) = (\\theta - a)^2\\]\nTo find the optimal estimator \\(d(x)\\), we minimize the posterior expected loss \\(E_{\\theta|x}[(\\theta - d(x))^2]\\). Taking the derivative with respect to \\(d\\) and setting it to 0:\n\\[-2 E_{\\theta|x}(\\theta - d) = 0 \\implies d(x) = E(\\theta|x)\\]\nResult: The Bayes rule under squared error loss is the posterior mean.\n\n\n3.3.2 Absolute Error Loss\n\\[L(\\theta, d) = |\\theta - d|\\]\nTo find the Bayes rule, we minimize the posterior expected loss:\n\\[\n\\psi(d) = E_{\\theta|x} [ |\\theta - d| ] = \\int_{-\\infty}^{\\infty} |\\theta - d| \\, dF(\\theta|x)\n\\]\nwhere \\(F(\\theta|x)\\) is the cumulative distribution function (CDF) of the posterior. Splitting the integral at the decision point \\(d\\):\n\\[\n\\psi(d) = \\int_{-\\infty}^{d} (d - \\theta) \\, dF(\\theta|x) + \\int_{d}^{\\infty} (\\theta - d) \\, dF(\\theta|x)\n\\]\nWe find the minimum by analyzing the rate of change of \\(\\psi(d)\\) with respect to \\(d\\). Differentiating (or taking the subgradient for non-differentiable points):\n\\[\n\\frac{d}{dd} \\psi(d) = \\int_{-\\infty}^{d} 1 \\, dF(\\theta|x) - \\int_{d}^{\\infty} 1 \\, dF(\\theta|x) = P(\\theta \\le d|x) - P(\\theta &gt; d|x)\n\\]\nSetting this derivative to zero implies we seek a point where the probability mass to the left equals the probability mass to the right:\n\\[\nP(\\theta \\le d|x) = P(\\theta &gt; d|x)\n\\]\nSince the total probability is 1, this condition simplifies to finding \\(d\\) such that the cumulative probability is \\(1/2\\).\nGeneral Case (Discrete or Mixed Distributions)\nIn cases where the posterior distribution is discrete or has jump discontinuities (e.g., the CDF jumps from 0.4 to 0.6 at a specific value), an exact solution to \\(F(d) = 0.5\\) may not exist. To generalize, the Bayes rule is defined as any median \\(m\\) of the posterior distribution.\nA median is formally defined as any value \\(m\\) that satisfies the following two conditions simultaneously:\n\n\\(P(\\theta \\le m|x) \\ge \\frac{1}{2}\\)\n\\(P(\\theta \\ge m|x) \\ge \\frac{1}{2}\\)\n\nResult: The Bayes rule under absolute error loss is the posterior median.\n\n\n3.3.3 Hypothesis Testing (0-1 Loss)\nConsider the hypothesis test \\(H_0: \\theta \\in \\Theta_0\\) versus \\(H_1: \\theta \\in \\Theta_1\\). We define the decision space as \\(\\mathcal{A} = \\{0, 1\\}\\), where \\(a=0\\) means accepting \\(H_0\\) and \\(a=1\\) means rejecting \\(H_0\\) (accepting \\(H_1\\)).\nCase 1: 0-1 Loss\nThe standard 0-1 loss function assigns a penalty of 1 for an incorrect decision and 0 for a correct one: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\ (\\text{Correct } H_0) \\\\ 1 & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\ (\\text{Correct } H_1) \\end{cases}\\]\nTo find the Bayes rule, we minimize the posterior expected loss for a given \\(x\\), denoted as \\(E_{\\theta|x}[L(\\theta, a)]\\).\n\nExpected Loss for choosing \\(a=0\\) (Accept \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 0)] = 0 \\cdot P(\\theta \\in \\Theta_0|x) + 1 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_1|x)\n  \\]\nExpected Loss for choosing \\(a=1\\) (Reject \\(H_0\\)): \\[\n  E_{\\theta|x}[L(\\theta, 1)] = 1 \\cdot P(\\theta \\in \\Theta_0|x) + 0 \\cdot P(\\theta \\in \\Theta_1|x) = P(\\theta \\in \\Theta_0|x)\n  \\]\n\nThe Bayes rule selects the action with the smaller expected loss. Thus, we choose \\(a=1\\) if: \\[\nP(\\theta \\in \\Theta_0|x) \\le P(\\theta \\in \\Theta_1|x)\n\\] This confirms that under 0-1 loss, the Bayes rule simply selects the hypothesis with the higher posterior probability.\nCase 2: General Loss (Asymmetric Costs)\nIn many practical applications, the cost of errors is not symmetric. For example, a Type I error (false rejection) might be more costly than a Type II error. Let \\(c_1\\) be the cost of a Type I error and \\(c_2\\) be the cost of a Type II error. Usually, we normalize one cost to 1.\nSuppose the loss function is: \\[L(\\theta, a) = \\begin{cases} 0 & \\text{if } \\theta \\in \\Theta_0, a=0 \\\\ c & \\text{if } \\theta \\in \\Theta_0, a=1 \\ (\\text{Cost of Type I Error}) \\\\ 1 & \\text{if } \\theta \\in \\Theta_1, a=0 \\ (\\text{Cost of Type II Error}) \\\\ 0 & \\text{if } \\theta \\in \\Theta_1, a=1 \\end{cases}\\]\nWe again calculate the posterior expected loss:\n\nExpected Loss for \\(a=0\\): \\[E[L(\\theta, 0)|x] = 0 \\cdot P(\\Theta_0|x) + 1 \\cdot P(\\Theta_1|x) = P(\\Theta_1|x)\\]\nExpected Loss for \\(a=1\\): \\[E[L(\\theta, 1)|x] = c \\cdot P(\\Theta_0|x) + 0 \\cdot P(\\Theta_1|x) = c P(\\Theta_0|x)\\]\n\nWe reject \\(H_0\\) (\\(a=1\\)) if the expected loss of doing so is lower: \\[\nc P(\\Theta_0|x) \\le P(\\Theta_1|x)\n\\]\nSince \\(P(\\Theta_1|x) = 1 - P(\\Theta_0|x)\\), we can rewrite this condition as: \\[\nc P(\\Theta_0|x) \\le 1 - P(\\Theta_0|x) \\implies (1+c) P(\\Theta_0|x) \\le 1\n\\] \\[\nP(\\Theta_0|x) \\le \\frac{1}{1+c}\n\\]\nResult: With asymmetric costs, we accept \\(H_1\\) only if the posterior probability of the null hypothesis is sufficiently small (below the threshold \\(\\frac{1}{1+c}\\)). If the cost of false rejection \\(c\\) is high, we require stronger evidence against \\(H_0\\).\n\n\n3.3.4 Classification Prediction (Categorical Parameter)\nIn classification problems, the parameter of interest is a discrete class label \\(\\theta\\) (often denoted as \\(y\\)) taking values in a set of categories \\(\\{1, 2, \\dots, K\\}\\). The goal is to predict the true class label based on observed features \\(x\\).\nWe typically employ the 0-1 loss function, which assigns a penalty of 1 for a misclassification and 0 for a correct prediction:\n\\[L(\\theta, \\hat{\\theta}) = \\begin{cases} 0 & \\text{if } \\hat{\\theta} = \\theta \\ (\\text{Correct Classification}) \\\\ 1 & \\text{if } \\hat{\\theta} \\neq \\theta \\ (\\text{Misclassification}) \\end{cases}\\]\nTo find the optimal classification rule (the Bayes Classifier), we minimize the posterior expected loss, which is equivalent to minimizing the probability of misclassification.\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta} L(\\theta, \\hat{\\theta}) \\pi(\\theta|x)\n\\]\nSince the loss is 1 only when the predicted class \\(\\hat{\\theta}\\) differs from the true class \\(\\theta\\), this sum simplifies to:\n\\[\nE_{\\theta|x}[L(\\theta, \\hat{\\theta})] = \\sum_{\\theta \\neq \\hat{\\theta}} 1 \\cdot \\pi(\\theta|x) = P(\\theta \\neq \\hat{\\theta} | x) = 1 - P(\\theta = \\hat{\\theta} | x)\n\\]\nMinimizing the misclassification rate \\(1 - P(\\theta = \\hat{\\theta} | x)\\) is mathematically equivalent to maximizing the probability of being correct, \\(P(\\theta = \\hat{\\theta} | x)\\).\nResult: The Bayes rule for classification is to predict the class with the highest posterior probability. While this is technically the Maximum A Posteriori (MAP) estimator, in the context of machine learning and pattern recognition, this decision rule is known as the Bayes Optimal Classifier.\n\\[\n\\hat{\\theta}_{Bayes}(x) = \\underset{k \\in \\{1, \\dots, K\\}}{\\arg\\max} \\ P(\\theta = k | x)\n\\]\n\n\n3.3.5 Interval Estimation and Highest Posterior Density (HPD)\nIn interval estimation, our goal is typically to find a set or interval that contains the true parameter \\(\\theta\\) with a high degree of certainty.\nLoss Function (Fixed Width Approach):\nWe can motivate interval estimation using a loss function where we seek the “best” interval of a predetermined length \\(2\\delta\\) centered at \\(d\\). The loss is defined as:\n\\[\nL(\\theta, d) = \\begin{cases} 0 & \\text{if } |\\theta - d| \\le \\delta \\\\ 1 & \\text{if } |\\theta - d| &gt; \\delta \\end{cases}\n\\]\nMinimizing the posterior expected loss is equivalent to minimizing the posterior probability that \\(\\theta\\) falls outside the interval \\((d - \\delta, d + \\delta)\\). Conversely, this means we want to maximize the posterior probability that \\(\\theta\\) is contained within the interval:\n\\[\n\\text{Maximize } P(d - \\delta \\le \\theta \\le d + \\delta | x)\n\\]\nDual Formulation (Fixed Probability):\nWhile the loss function above fixes the length and maximizes probability, in practice we often do the reverse: we specify a coverage probability \\(1-\\alpha\\) (e.g., 0.95) and seek the interval of the shortest length that satisfies this coverage.\nFor a unimodal posterior density (one with a single peak that decreases monotonically on both sides), the solution to both problems—maximizing probability for a fixed width, or minimizing width for a fixed probability—is identical. The resulting interval collects the values of \\(\\theta\\) with the highest probability density.\n\nDefinition 3.3 (Highest Posterior Density (HPD) Interval) The Highest Posterior Density (HPD) interval is defined as the set of values where the posterior density exceeds a certain threshold \\(c\\):\n\\[\nC_{HPD} = \\{ \\theta : \\pi(\\theta|x) \\ge c \\}\n\\]\nwhere \\(c\\) is chosen such that the total posterior probability of the set is \\(1-\\alpha\\).\n\nComputation and Alternatives:\nAlthough the HPD interval is optimal in terms of length, it can be computationally difficult to evaluate for complex posteriors. Common alternatives include:\n\nEqual-Tailed Interval: We construct the interval \\([L, U]\\) such that the probability of \\(\\theta\\) being below \\(L\\) or above \\(U\\) is exactly \\(\\alpha/2\\). \\[P(\\theta &lt; L | x) = \\alpha/2 \\quad \\text{and} \\quad P(\\theta &gt; U | x) = \\alpha/2\\]\nNormal Approximation: If the posterior is approximately normal (which is often true for large samples), we can use the posterior mean \\(\\mu\\) and standard deviation \\(\\sigma\\): \\[(\\mu - z_{\\alpha/2}\\sigma, \\mu + z_{\\alpha/2}\\sigma)\\] where \\(z_{\\alpha/2}\\) is the upper-\\(\\alpha\\) point of the standard normal distribution.\n\nThe plot below illustrates a skewed posterior distribution (Gamma). Notice how the HPD Interval (Blue) is shifted toward the mode to capture the highest density values, resulting in a shorter interval length compared to the Equal-Tailed Interval (Red).\n\n\nCode\n# Define a skewed distribution: Gamma(shape=2, rate=0.5)\nx_vals &lt;- seq(0, 15, length.out = 1000)\ny_vals &lt;- dgamma(x_vals, shape = 2, rate = 0.5)\n\n# Target coverage\nalpha &lt;- 0.10\ntarget_prob &lt;- 1 - alpha\n\n# 1. Equal-Tailed Interval (Quantiles)\neq_lower &lt;- qgamma(alpha/2, shape = 2, rate = 0.5)\neq_upper &lt;- qgamma(1 - alpha/2, shape = 2, rate = 0.5)\n\n# 2. HPD Interval (Density threshold optimization)\n# We look for a density threshold k such that the area above k is 0.90\nfind_hpd &lt;- function(dist_vals, density_vals, probability) {\n  # Sort density values\n  ord &lt;- order(density_vals, decreasing = TRUE)\n  sorted_dens &lt;- density_vals[ord]\n  sorted_dist &lt;- dist_vals[ord]\n  \n  # Accumulate probability (approximation)\n  dx &lt;- diff(dist_vals)[1]\n  cum_prob &lt;- cumsum(sorted_dens * dx)\n  \n  # Find cutoff index\n  cutoff_idx &lt;- which(cum_prob &gt;= probability)[1]\n  \n  # Get the subset of x values\n  hpd_set &lt;- sorted_dist[1:cutoff_idx]\n  return(c(min(hpd_set), max(hpd_set)))\n}\n\nhpd_bounds &lt;- find_hpd(x_vals, y_vals, target_prob)\nhpd_lower &lt;- hpd_bounds[1]\nhpd_upper &lt;- hpd_bounds[2]\n\n# Plotting\nplot(x_vals, y_vals, type = 'l', lwd = 2, col = \"black\",\n     main = \"90% Credible Intervals (Skewed Posterior)\",\n     xlab = expression(theta), ylab = \"Density\",\n     ylim = c(0, max(y_vals) * 1.2))\n\n# Shade HPD\npolygon(c(x_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], hpd_upper, hpd_lower),\n        c(y_vals[x_vals &gt;= hpd_lower & x_vals &lt;= hpd_upper], 0, 0),\n        col = rgb(0, 0, 1, 0.2), border = NA)\n\n# Draw Equal-Tailed lines (Red)\nabline(v = c(eq_lower, eq_upper), col = \"red\", lwd = 2, lty = 2)\n# Draw HPD lines (Blue)\nabline(v = c(hpd_lower, hpd_upper), col = \"blue\", lwd = 2, lty = 1)\n\nlegend(\"topright\", \n       legend = c(\"Posterior Density\", \n                  paste0(\"Equal-Tailed (Len: \", round(eq_upper - eq_lower, 2), \")\"), \n                  paste0(\"HPD (Len: \", round(hpd_upper - hpd_lower, 2), \")\")),\n       col = c(\"black\", \"red\", \"blue\"), \n       lty = c(1, 2, 1), lwd = 2,\n       fill = c(NA, NA, rgb(0, 0, 1, 0.2)), border = NA)\n\n\n\n\n\n\n\n\nFigure 3.3: Comparison of HPD and Equal-Tailed Intervals for a Skewed Distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  },
  {
    "objectID": "bayesian.html#finding-minimax-estimators-with-bayes-rules",
    "href": "bayesian.html#finding-minimax-estimators-with-bayes-rules",
    "title": "3  Bayesian Methods",
    "section": "3.4 Finding Minimax Estimators with Bayes Rules",
    "text": "3.4 Finding Minimax Estimators with Bayes Rules\nA decision rule \\(d(x)\\) is minimax if it minimizes the maximum possible risk: \\(\\sup_\\theta R(\\theta, d)\\).\n\nTheorem 3.2 (Constant Risk Bayes Estimator is Minimax) Let \\(\\delta^\\pi\\) be a Bayes estimator with respect to a prior \\(\\pi\\). If the risk function of \\(\\delta^\\pi\\) is constant on the parameter space \\(\\Theta\\), such that \\(R(\\theta, \\delta^\\pi) = c\\) for all \\(\\theta \\in \\Theta\\), then \\(\\delta^\\pi\\) is a minimax estimator.\n\n\nProof. Let \\(\\delta^\\pi\\) be the Bayes estimator with constant risk \\(c\\). First, we compute its Bayes risk \\(r(\\pi, \\delta^\\pi)\\). Since the risk is constant:\n\\[\nr(\\pi, \\delta^\\pi) = \\int_\\Theta R(\\theta, \\delta^\\pi) \\pi(\\theta) d\\theta = \\int_\\Theta c \\, \\pi(\\theta) d\\theta = c\n\\]\nNow, let \\(\\delta'\\) be any arbitrary estimator. By the definition of a Bayes estimator, \\(\\delta^\\pi\\) minimizes the Bayes risk among all estimators:\n\\[\nr(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta')\n\\]\nNext, we observe that the Bayes risk of \\(\\delta'\\) is the expectation of its risk function with respect to the prior \\(\\pi\\). An average cannot exceed the maximum value of the function being averaged (the supremum):\n\\[\nr(\\pi, \\delta') = \\int_\\Theta R(\\theta, \\delta') \\pi(\\theta) d\\theta \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta') \\cdot \\int_\\Theta \\pi(\\theta) d\\theta = \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nCombining these inequalities, we have:\n\\[\n\\sup_{\\theta \\in \\Theta} R(\\theta, \\delta^\\pi) = c = r(\\pi, \\delta^\\pi) \\le r(\\pi, \\delta') \\le \\sup_{\\theta \\in \\Theta} R(\\theta, \\delta')\n\\]\nSince \\(\\sup_\\theta R(\\theta, \\delta^\\pi) \\le \\sup_\\theta R(\\theta, \\delta')\\) holds for any estimator \\(\\delta'\\), \\(\\delta^\\pi\\) minimizes the maximum risk. Therefore, it is minimax.\n\nThe plot below visualizes the logic of the proof. The red line represents the Constant Risk Bayes Estimator (\\(\\delta^\\pi\\)), which has a constant height \\(c\\). The blue curve represents an Arbitrary Estimator (\\(\\delta'\\)).\nBecause \\(\\delta^\\pi\\) minimizes the weighted average risk (Bayes risk), the average height of the blue curve cannot be lower than the red line (with respect to the prior). Consequently, the blue curve must rise above the red line at some point, making its maximum risk (\\(\\sup R\\)) strictly greater than or equal to \\(c\\). Thus, the constant risk estimator has the lowest possible maximum.\n\n\nCode\n# Define parameter space theta\ntheta &lt;- seq(0, 1, length.out = 200)\n\n# 1. Constant Risk Bayes Estimator (Risk = c)\nc_val &lt;- 0.5\nrisk_bayes &lt;- rep(c_val, length(theta))\n\n# 2. Arbitrary Alternative Estimator\n# This function is chosen such that it dips below c but rises above it elsewhere\nrisk_alt &lt;- c_val + 0.2 * sin(2 * pi * theta) - 0.05\n\n# Plotting\nplot(theta, risk_alt, type = 'l', lwd = 2, col = \"blue\",\n     ylim = c(0, 1), ylab = \"Risk R(theta, d)\", xlab = expression(theta),\n     main = \"Geometry of the Minimax Theorem\")\n\n# Add Constant Risk Line\nlines(theta, risk_bayes, col = \"red\", lwd = 2)\n\n# Mark the Maximum of the Alternative\nmax_alt &lt;- max(risk_alt)\nmax_theta &lt;- theta[which.max(risk_alt)]\npoints(max_theta, max_alt, pch = 19, col = \"blue\")\ntext(max_theta, max_alt, labels = expression(sup~R(theta, delta^\"'\")), pos = 3, col = \"blue\")\n\n# Label the Constant Risk\ntext(0.1, c_val, labels = expression(R(theta, delta^pi) == c), pos = 3, col = \"red\")\n\n# Add Legend\nlegend(\"bottomright\", legend = c(\"Arbitrary Estimator\", \"Constant Risk Bayes Est.\"),\n       col = c(\"blue\", \"red\"), lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\nFigure 3.4: Visual Proof: Any alternative estimator (Blue) with Bayes risk comparable to the Constant Risk estimator (Red) must have a higher maximum risk.\n\n\n\n\n\n\nExample 3.5 (Binomial Minimax Estimator) Let \\(X \\sim \\text{Bin}(n, \\theta)\\) and \\(\\theta \\sim \\text{Beta}(a, b)\\). The squared error loss is \\(L(\\theta, d) = (\\theta - d)^2\\). The Bayes estimator is the posterior mean: \\[d(x) = \\frac{a+x}{a+b+n}\\]\nWe calculate the risk \\(R(\\theta, d)\\):\n\\[\nR(\\theta, d) = E_x \\left[ \\left( \\theta - \\frac{a+x}{a+b+n} \\right)^2 \\right]\n\\]\nLet \\(c = a+b+n\\). \\[R(\\theta, d) = \\frac{1}{c^2} E \\left[ (c\\theta - a - x)^2 \\right]\\]\nUsing the bias-variance decomposition and knowing \\(E(x) = n\\theta\\) and \\(E(x^2) = (n\\theta)^2 + n\\theta(1-\\theta)\\), we expand the risk function. To make the risk constant (independent of \\(\\theta\\)), we set the coefficients of \\(\\theta\\) and \\(\\theta^2\\) to zero.\nSolving the resulting system of equations yields: \\[a = b = \\frac{\\sqrt{n}}{2}\\]\nThus, the minimax estimator is: \\[d(x) = \\frac{x + \\sqrt{n}/2}{n + \\sqrt{n}}\\]\nThis differs from the standard MLE \\(\\hat{p} = x/n\\) and the uniform prior Bayes estimator (\\(a=b=1\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Methods</span>"
    ]
  }
]