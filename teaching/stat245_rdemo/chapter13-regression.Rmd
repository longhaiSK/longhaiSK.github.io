---
title: "Introduction to Statistical Methods"
subtitle: "Simple Linear Regression"
author: "Longhai Li"
date: "September 2019"
output: 
   html_document:
       toc: true
       number_sections: true
       highlight: tango
       fig_width: 10
       fig_height: 8
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE, echo=FALSE}
require("knitr")
opts_knit$set(
    root.dir = "~/OneDrive - University of Saskatchewan/teaching/stat245_1909/rdemo/demo"
    )
knitr::opts_chunk$set(
    comment = "#",
    fig.width=10, 
    fig.height=8, 
    cache = TRUE
)
```

```{r }
library ("latex2exp")
```

# Input data

```{r }
issu <- data.frame (
    driving = c(5, 2, 12, 9, 15, 6, 25, 16),
    premium = c(64, 87, 50, 71, 44, 56, 42, 60)
)

y <- issu$premium
x <- issu$driving
plot(x,y, xlab = "Driving", ylab = "Premium")
```

# Correlation Coefficient


```{r }
xbar <- mean(x)
ybar <- mean (y)
n <- length(y)

data.frame (xi=x,yi=y, xiyi=x*y, x.squared=x^2, y.squared=y^2, 
            x_c=x-xbar, y_c=y-ybar, x_cy_c=(x-xbar)*(y-ybar))

SSxy <- sum(x*y) - n*xbar*ybar; SSxy
sum((x-xbar)*(y-ybar))
SSxx <- sum (x^2) - n*xbar^2;SSxx
sum((x-xbar)^2)
SSyy <- sum(y^2) - n*ybar^2;SSyy
sum((y-ybar)^2)

r <- SSxy/(sqrt(SSxx)*sqrt(SSyy));r
```

The correlation coefficient $r$ can be computed with a built-in function in R

```{r }
cor(x,y)
```

# Estimating regression coefficients
There are such relationships between the correlation $\rho$ and regression coefficients $\beta_0$ and $\beta_1$:

$$\beta_1=\rho \frac{\sigma_y}{\sigma_x}$$
$$\beta_0=\mu_y - \beta_1\mu_x.$$ 
Using the above relationships, we can estimate $\beta_0$ and $\beta_1$ with 
$$\hat{\beta_1}=r\frac{s_y}{s_x}=\frac{SS_{xy}}{SS_{xx}}$$
$$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

```{r }
xbar <- mean(x)
ybar <- mean (y)
n <- length(y)
SSxy <- sum(x*y) - n*xbar*ybar
SSxx <- sum (x^2) - n*xbar^2
SSyy <- sum (y^2) - n*ybar^2

beta1 <- SSxy/SSxx; beta1
beta0 <- ybar - beta1*xbar; beta0

plot(x,y)
abline (a = beta0, b = beta1)
```

Let us verify the relationship between rho and beta1

```{r }
r*sd(y)/sd(x)

r*sqrt(SSyy)/sqrt(SSxx)

SSxy/SSxx
```

# Fitted Values, Residuals and Sum Squares


```{r }
fitted1 <- beta0+beta1*x
fitted0 <- rep(ybar, n)
residual1 <- y - fitted1
residual0 <- y - fitted0

data.frame (y, fitted0, residual0, fitted1, residual1, diff.res=fitted1-fitted0)
```

Visualize the fitted line, fitted values, and residuals

```{r fig.height= 5}
par (mfrow = c(1,3))
plot (premium ~ driving, data = issu, ylim = range(y, fitted1, fitted0))
abline (h=mean(y))
points(x, fitted0, pch = 4,col=4)
for (i in 1: length(y)){
    arrows( x[i],fitted0[i],x[i], y[i], length=0.1, col = 4)
}
title(main= TeX("Residuals of Fitting Model 0: $y_i-\\bar{y}$"))


plot (premium ~ driving, data = issu, ylim = range(y, fitted1, fitted0))
abline (h=mean(y))
abline(a=beta0, b=beta1)
points(x, fitted1, pch = 3,col=3)
for (i in 1: length(y)){
    arrows( x[i],fitted1[i],x[i], y[i], length=0.1, col = 3)
    
}
title(main= TeX("Residuals of Fitting Model 1: $y_i-\\hat{y}_i$"))


plot (premium ~ driving, data = issu, ylim = range(y, fitted1, fitted0))
abline (h=mean(y))
abline(a=beta0, b=beta1)
points(x, fitted0, pch = 4,col=4)
points(x, fitted1, pch = 3,col=3)

for (i in 1: length(y)){
    arrows( x[i],fitted0[i],x[i], fitted1[i], length=0.1, col = 2)
    
}
title(main= TeX("Differences in Residuals: $\\hat{y}_i-\\bar{y}$"))

```

**Definitions of SSR, SSE, SST**:

```{r}
SST <- sum((y-fitted0)^2); SST
SSE <- sum ((y-fitted1)^2);SSE
SSR <- SST-SSE;SSR

```
SSR can be computed directly with
```{r}
sum((fitted1-fitted0)^2)
```


**Coefficient of Determination: $R^2$** 

```{r }
R2 <- SSR/SST; R2
```

**F statististic**

```{r }
f <- (SSR/1)/(SSE/(n-2)); f
```

p-value to assess whether the relationship exists (statistical significance measure)

```{r }
pvf <- pf(f, df1=1, df2=n-2, lower.tail = FALSE); pvf
```

**ANOVA table**

```{r }
Ftable <- data.frame(df = c(1,n-2), SS=c(SSR,SSE), 
                     MSS = c(SSR/1, SSE/(n-2)), Fstat=c(f,NA),
                     p.value = c(pvf, NA),
                     R2 = c(SSR, SSE)/SST)
Ftable
```

# Short-cut formulae for SST, SSR, SSE

An important formular for computing SSR for simple linear regression is:

$$
SSR = \hat\beta_1^2 SS_{xx} = \hat\beta_1 SS_{xy}
$$

With SSR and SST ($=SS_{yy}$), we can compute SSE with this equation:

$$
SST = SSR + SSE
$$
```{r }

# The following 6 summary numbers are the beginning of computing lm:
xbar <- mean(x)
ybar <- mean (y)
n <- length(y)
SSxy <- sum(x*y) - n*xbar*ybar
SSxx <- sum (x^2) - n*xbar^2
SSyy <- sum (y^2) - n*ybar^2

# estimate linear line:
beta1 <- SSxy/SSxx
beta0 <- ybar - beta1*xbar

# compute SSR, SSE, and SST
SST <- SSyy; SST
SSR <- beta1^2*SSxx;SSR
```
Alternatively, SSR can be computed with:
```{r}
SSR_alt <- beta1*SSxy; SSR_alt

SSE <- SST-SSR; SSE
```

# Statistical Inference for $\beta_1$
To make inference about $\beta_1$, we need to calculate the standard error of $\hat{\beta_1}$. The formula is
$$
\hat{SE}(\hat{\beta_1})=\frac{s_e}{\sqrt{SS_{xx}}}
$$
where, $s_e=\sqrt{\frac{SSE}{n-2}}$ is the standard deviation of residuals. 

```{r }
sd_e <- sqrt(SSE/(n-2)); sd_e
se_beta1 <- sd_e/sqrt(SSxx); se_beta1
tstat <- beta1/se_beta1; tstat
```

To test $H_1: \beta_1\not=0$, the p-value is:

```{r }
2*pt(-abs(beta1)/se_beta1, df = n-2)
```

To test $H_1: \beta_1<0$, the p-value is:

```{r }
pt(beta1/se_beta1, df = n-2)
```

To test $H_1: \beta_1>0$, the p-value is:

```{r }
pt(beta1/se_beta1, df = n-2, lower.tail = FALSE)
```

To find C.I. for $\beta_1$:

```{r}
alpha <- 0.05
me_beta1 <- qt(alpha/2, df = n-2, lower.tail = FALSE) * se_beta1
# 95% CI:
beta1 + c(-me_beta1, me_beta1)
```


# R functions for Regression Analysis
All the above calculation can be done with a single function lm:

```{r }

issu <- data.frame (
    driving = c(5, 2, 12, 9, 15, 6, 25, 16),
    premium = c(64, 87, 50, 71, 44, 56, 42, 60)
)

lmfit_issu <- lm (premium ~ driving, data = issu)
# summary of fitting results
summary (lmfit_issu)
# more results are in lmfit_issu
names(lmfit_issu)
# confidence interval
confint(lmfit_issu)
# anova table
anova (lmfit_issu)
```

# Confidence intervals for the mean of $y_i$ at $x_i$ 


```{r }
newdata <- data.frame (driving = seq(0,30,by = 1))
newdata

means.pred <- predict (lmfit_issu, newdata = newdata)
cbind(newdata,predicted.premium = means.pred)
conf.int.means <- predict (lmfit_issu, newdata = newdata, interval = "confidence")
conf.int.means
```

# Predictive interval for $y_i$ at $x_i$ 

```{r }
int_y <- predict (lmfit_issu, newdata = newdata, 
                  interval = "prediction")
int_y
```

We can visualize these prediction with such a plot:

```{r }
plot(x, y, ylim = range(int_y), xlim = c(0,30))
lines(newdata$driving, conf.int.means[,1], lty = 1)
lines(newdata$driving, conf.int.means[,2], lty = 2, col = 2)
lines(newdata$driving, conf.int.means[,3], lty = 2, col = 2 )
lines(newdata$driving, int_y[,2], lty = 3, col = 3)
lines(newdata$driving, int_y[,3], lty = 3, col = 3)
```


---
title: "chapter13-regression-.R"
author: "lol553"
date: "2019-11-26"
---
