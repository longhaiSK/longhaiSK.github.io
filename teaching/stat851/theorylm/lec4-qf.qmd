---
title: "Distribution of Quadratic Forms"
---

```{r}
#| label: setup-images-lec40-p1-10
#| include: false
#| warning: false

library(pdftools)
library(magick)

# Define path to PDF
pdf_path <- "../Lec40-quadratic-form.pdf"
img_dir <- "lec40_images"

# Create image directory
if (!dir.exists(img_dir)) {
  dir.create(img_dir)
}

# Helper function to extract and crop a specific page
extract_page_image <- function(page_num, filename, geometry = "1200x800+0+200", force = FALSE) {
  target_file <- file.path(img_dir, filename)
  
  if (file.exists(pdf_path) && (force || !file.exists(target_file))) {
    tryCatch({
      bitmap <- pdf_render_page(pdf_path, page = page_num, dpi = 150)
      img <- image_read(bitmap)
      img_cropped <- image_crop(img, geometry)
      image_write(img_cropped, target_file)
    }, error = function(e) {
      warning(paste("Failed to extract page", page_num, ":", e$message))
    })
  }
}
```

This chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.

## Quadratic Forms

A quadratic form is a polynomial with terms all of degree two.

::: {#def-quadratic-form name="Quadratic Form"}
Let $y = (y_1, \dots, y_n)'$ be a random vector and $A$ be a symmetric $n \times n$ matrix. The scalar quantity $y'Ay$ is called a **quadratic form** in $y$.

$$
y'Ay = \sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j
$$
:::

**Examples:**

* **Squared Norm:** If $A = I_n$, then $y'I_n y = y'y = \sum y_i^2 = ||y||^2$.
* **Weighted Sum of Squares:** If $A$ is diagonal with elements $\lambda_i$, then $y'Ay = \sum \lambda_i y_i^2$.
* **Projection Sum of Squares:** If $P$ is a projection matrix, $||Py||^2 = (Py)'(Py) = y'P'Py = y'Py$ (since $P$ is symmetric and idempotent).

## Mean of Quadratic Forms

We can find the expected value of a quadratic form without assuming normality.

::: {#lem-simple-qf name="Mean of Simplified Quadratic Form"}
If $y$ is a random vector with mean $E(y) = \mu$ and covariance matrix $\text{Var}(y) = I_n$, then:
$$
E(y'y) = \text{tr}(I_n) + \mu'\mu = n + \mu'\mu
$$
:::

::: {.proof}
Let us decompose $y$ into its mean and a stochastic component: $y = \mu + z$, where $E(z) = 0$ and $\text{Var}(z) = E(zz') = I_n$.
Substituting this into the quadratic form:
$$
\begin{aligned}
y'y &= (\mu + z)'(\mu + z) \\
&= \mu'\mu + \mu'z + z'\mu + z'z \\
&= \mu'\mu + 2\mu'z + z'z
\end{aligned}
$$
Taking the expectation:
$$
\begin{aligned}
E(y'y) &= \mu'\mu + 2\mu'E(z) + E(z'z) \\
&= \mu'\mu + 0 + E\left(\sum_{i=1}^n z_i^2\right)
\end{aligned}
$$
Since $\text{Var}(z_i) = E(z_i^2) - (E(z_i))^2 = 1 - 0 = 1$, we have $E(\sum z_i^2) = \sum 1 = n$.
Thus, $E(y'y) = n + \mu'\mu$.
:::



```{r fig-qf-norm}
#| fig-cap: Illustration of the Mean and Distribution of Quadratic Forms
library(ggplot2)
library(MASS)
library(dplyr)

# --- 1. Setup Data & Parameters ---
set.seed(42)
n <- 100
sigma_val <- 1          
Sigma <- diag(2) * sigma_val^2

mu_orig <- c(5, 6)      # Original Mean
y_orig  <- c(7, 5)      # Updated Point y

# Generate 100 points from N(mu, I)
data_orig <- mvrnorm(n, mu_orig, Sigma)

# Define rotation angles
angles <- c(0, 70, 180)

# --- 2. Process Data for Each Angle ---
points_list <- list()
vectors_list <- list()

for (deg in angles) {
  theta <- deg * pi / 180
  rot_mat <- matrix(c(cos(theta), -sin(theta), 
                      sin(theta),  cos(theta)), nrow = 2, byrow = TRUE)
  
  # A. Rotate Points
  data_rot <- data_orig %*% t(rot_mat)
  df_pts <- data.frame(x = data_rot[,1], y = data_rot[,2])
  df_pts$Angle <- factor(paste0(deg, "째"), levels = paste0(angles, "째"))
  points_list[[length(points_list) + 1]] <- df_pts
  
  # B. Rotate Vectors (mu and y)
  mu_rot <- as.vector(rot_mat %*% mu_orig)
  y_rot  <- as.vector(rot_mat %*% y_orig)
  
  df_vec <- data.frame(
    Angle = factor(paste0(deg, "째"), levels = paste0(angles, "째")),
    mu_x = mu_rot[1], mu_y = mu_rot[2],
    y_x  = y_rot[1],  y_y  = y_rot[2]
  )
  vectors_list[[length(vectors_list) + 1]] <- df_vec
}

all_points  <- do.call(rbind, points_list)
all_vectors <- do.call(rbind, vectors_list)

# --- 3. Create Circle Data ---
# Radius is the length of mu
radius_mu <- sqrt(sum(mu_orig^2))
circle_data <- data.frame(
  x0 = 0, y0 = 0, r = radius_mu
)

# --- 4. Generate the Plot ---
ggplot() +
  # 1. Circle through the mu's (Centered at 0,0)
  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = radius_mu), 
                       color = "gray50", linetype = "dotted", size = 0.5) +
  
  # 2. Points (Data Cloud)
  geom_point(data = all_points, aes(x = x, y = y, color = Angle), 
             size = 0.5, alpha = 0.5) +
  
  # 3. Vector mu (Origin -> mu)
  geom_segment(data = all_vectors, 
               aes(x = 0, y = 0, xend = mu_x, yend = mu_y, color = Angle),
               arrow = arrow(length = unit(0.2, "cm")), size = 0.8) +
  
  # 4. Vector y (Origin -> y)
  geom_segment(data = all_vectors, 
               aes(x = 0, y = 0, xend = y_x, yend = y_y, color = Angle),
               arrow = arrow(length = unit(0.2, "cm")), size = 0.8) +
  
  # 5. Vector y - mu (mu -> y)
  geom_segment(data = all_vectors, 
               aes(x = mu_x, y = mu_y, xend = y_x, yend = y_y, color = Angle),
               arrow = arrow(length = unit(0.15, "cm")), 
               linetype = "dashed", size = 0.6) +
  
  # 6. Labels for mu, y, and y-mu
  geom_text(data = all_vectors, aes(x = mu_x, y = mu_y, label = expression(mu), color = Angle),
            parse = TRUE, vjust = -0.5, size = 4, show.legend = FALSE) +
  
  geom_text(data = all_vectors, aes(x = y_x, y = y_y, label = "y", color = Angle),
            vjust = -0.5, hjust = -0.2, size = 4, fontface = "italic", show.legend = FALSE) +
  
  # Label for y - mu (placed at midpoint)
  geom_text(data = all_vectors, aes(x = (mu_x + y_x)/2, y = (mu_y + y_y)/2, 
                                    label = expression(y - mu), color = Angle),
            parse = TRUE, size = 3, vjust = 1.5, show.legend = FALSE) +

  # 7. Origin Marker
  geom_point(aes(x=0, y=0), color="black", size=2) +
  
  # Formatting
  coord_fixed() +
  theme_minimal() +
  labs(title = "Rotations of Normal Cloud",
       x = "x", y = "y") +
  theme(legend.position = "bottom")
```

::: {#thm-mean-qf name="Mean of Quadratic Form"}
If $y$ is a random vector with mean $E(y) = \mu$ and covariance matrix $\text{Var}(y) = \Sigma$, and $A$ is a symmetric matrix of constants, then:

$$
E(y'Ay) = \text{tr}(A\Sigma) + \mu'A\mu
$$
:::

::: {.proof}
We present three methods to derive the expectation of the quadratic form.

**Method 1: Using the Trace Trick**

Using the fact that a scalar is equal to its own trace ($\text{tr}(c) = c$) and the linearity of expectation:
$$
\begin{aligned}
E(y'Ay) &= E[\text{tr}(y'Ay)] \\
&= E[\text{tr}(Ayy')] \quad \text{(cyclic property of trace)} \\
&= \text{tr}(A E[yy']) \quad \text{(linearity of expectation)}
\end{aligned}
$$
Recall that the covariance matrix is defined as $\Sigma = E[(y-\mu)(y-\mu)'] = E(yy') - \mu\mu'$. Rearranging this gives the second moment: $E(yy') = \Sigma + \mu\mu'$. Substituting this back:
$$
\begin{aligned}
E(y'Ay) &= \text{tr}(A(\Sigma + \mu\mu')) \\
&= \text{tr}(A\Sigma) + \text{tr}(A\mu\mu') \\
&= \text{tr}(A\Sigma) + \text{tr}(\mu'A\mu) \quad \text{(cyclic property on second term)} \\
&= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
$$

**Method 2: Using Scalar Summation**

We can express the quadratic form in scalar notation using the entries of $A=(a_{ij})$, $\Sigma=(\sigma_{ij})$, and $\mu=(\mu_i)$:
$$
\begin{aligned}
E(y'Ay) &= E\left(\sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j\right) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} E(y_i y_j) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} (\sigma_{ij} + \mu_i \mu_j) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} \sigma_{ji} + \sum_{i=1}^n \sum_{j=1}^n \mu_i a_{ij} \mu_j \quad (\text{since } \Sigma \text{ is symmetric, } \sigma_{ij}=\sigma_{ji}) \\
&= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
$$

**Method 3: Using Spectral Decomposition**

Since $A$ is symmetric, we can apply the **spectral decomposition** $A = \sum_{i=1}^n \lambda_i q_i q_i'$, where $\lambda_i$ are eigenvalues and $q_i$ are orthogonal eigenvectors. We rewrite the quadratic form as:
$$
y'Ay = y' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) y = \sum_{i=1}^n \lambda_i (q_i'y)^2
$$
To find $E[(q_i'y)^2]$, we **standardize** $y$. Let $y = \mu + \Sigma^{1/2}z$, where $E(z)=0$ and $\text{Var}(z)=I_n$. Then the scalar term $q_i'y$ becomes:
$$
q_i'y = q_i'(\mu + \Sigma^{1/2}z) = q_i'\mu + q_i'\Sigma^{1/2}z
$$
Squaring this scalar and taking the expectation:
$$
\begin{aligned}
E[(q_i'y)^2] &= (q_i'\mu)^2 + 2(q_i'\mu)E[q_i'\Sigma^{1/2}z] + E[(q_i'\Sigma^{1/2}z)^2] \\
&= (q_i'\mu)^2 + 0 + \text{Var}(q_i'\Sigma^{1/2}z)
\end{aligned}
$$
The variance term simplifies to $\text{Var}(q_i'\Sigma^{1/2}z) = q_i'\Sigma^{1/2} I_n \Sigma^{1/2} q_i = q_i' \Sigma q_i$. Substituting this back into the sum:
$$
\begin{aligned}
E(y'Ay) &= \sum_{i=1}^n \lambda_i \left[ q_i' \Sigma q_i + (q_i'\mu)^2 \right] \\
&= \sum_{i=1}^n \text{tr}( \lambda_i q_i' \Sigma q_i ) + \mu' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) \mu \\
&= \text{tr}\left( \Sigma \sum_{i=1}^n \lambda_i q_i q_i' \right) + \mu' A \mu \\
&= \text{tr}(\Sigma A) + \mu' A \mu
\end{aligned}
$$
:::

:::{#exm-mean-of-s2 name="Mean of Sample Variance"}

Consider a random sample $x = (x_1, \dots, x_n)'$. We want to find the expectation of the sum of squared errors $SSE = \sum (x_i - \bar{x})^2$, which can be written in matrix notation as the quadratic form $x'(I_n - H)x$.

To apply @thm-mean-qf, we identify the following components:

1.  **The Random Vector Parameters**:
    * Mean: $E(x) = \mu j_n$ (where $j_n$ is a vector of ones).
    * Covariance: $\Sigma = \sigma^2 I_n$.

2.  **The Quadratic Form Matrix**:
    * $A = I_n - H$, where $H = \frac{1}{n}j_nj_n'$.

**Calculation:**

Using the formula $E(x'Ax) = \text{tr}(A\Sigma) + E(x)' A E(x)$:

**Term 1: The Trace**
$$
\begin{aligned}
\text{tr}(A\Sigma) &= \text{tr}((I_n - H) \sigma^2 I_n) \\
&= \sigma^2 \text{tr}(I_n - H) \\
&= \sigma^2 (\text{tr}(I_n) - \text{tr}(H)) \\
&= \sigma^2 (n - 1)
\end{aligned}
$$
*(Note: $\text{tr}(H) = \text{tr}(\frac{1}{n}j_n j_n') = 1$)*.

**Term 2: The Mean Component**
$$
\begin{aligned}
E(x)' A E(x) &= (\mu j_n)' (I_n - H) (\mu j_n) \\
&= \mu^2 j_n' \underbrace{(I_n - H) j_n}_{0} \\
&= 0
\end{aligned}
$$
*(Note: $(I_n - H)j_n = j_n - H j_n = j_n - j_n = 0$, because projecting the mean vector onto the mean space leaves it unchanged, so the residual is zero).*

**Conclusion:**
$$
E\left(\sum (x_i - \bar{x})^2\right) = \sigma^2(n-1) + 0 = (n-1)\sigma^2
$$

This implies that $S^2 = \frac{SSE}{n-1}$ is an unbiased estimator for $\sigma^2$.
:::

## Non-central Chi-square Distribution

To understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.

::: {#def-nc-chisq name="Non-central Chi-square Distribution"}
Let $x \sim N_n(\mu, I_n)$. The random variable $Y = x'x = \sum x_i^2$ follows a **non-central chi-square distribution** with $n$ degrees of freedom and non-centrality parameter $\lambda$.

$$
Y \sim \chi^2(n, \lambda) \quad \text{where } \lambda = \frac{1}{2} \mu'\mu = \frac{1}{2} ||\mu||^2
$$
:::

**Note:** Some definitions use $\lambda = \mu'\mu$. In this course, we use $\lambda = \frac{1}{2}\mu'\mu$.

### Visualizing Chi-square Distributions

Here is a plot visualizing the difference between central and non-central Chi-square distributions.


```{r}
#| label: fig-plot-chisq
#| echo: false
#| fig-cap: "Central vs Non-central Chi-square Distribution"

library(ggplot2)

x <- seq(0, 20, length.out = 500)
df <- 5
ncp <- 3 # Non-centrality parameter

y_central <- dchisq(x, df = df)
y_noncentral <- dchisq(x, df = df, ncp = ncp)

data <- data.frame(
  x = rep(x, 2),
  density = c(y_central, y_noncentral),
  type = rep(c("Central (n=5)", "Non-central (n=5, lambda=3)"), each = length(x))
)

ggplot(data, aes(x = x, y = density, color = type)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(title = "Chi-square Distributions",
       y = "Density",
       x = "x") +
  theme(legend.position = "top")
```





The density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter $\lambda$ increases.

### Mean, Variance, and MGF

We summarize the key properties of the non-central chi-square distribution.

::: {#thm-chisq-properties name="Properties of Non-central Chi-square"}
Let $Y \sim \chi^2(n, \lambda)$. Then:

1.  **Mean:** $E(Y) = n + 2\lambda$
2.  **Variance:** $\text{Var}(Y) = 2n + 8\lambda$
3.  **Moment Generating Function (MGF):**
    $$
    m_Y(t) = \frac{\exp[-\lambda \{1 - 1/(1-2t)\}]}{(1-2t)^{n/2}}
    $$
:::

::: {.proof name="Mean"}
We derived $E(Y) = n + 2\lambda$ in the previous section using the decomposition $Y = ||z + \mu||^2$.
:::

### Additivity

::: {#thm-chisq-additivity name="Additivity of Chi-square"}
If $v_1, \dots, v_k$ are independent random variables distributed as $\chi^2(n_i, \lambda_i)$, then their sum follows a chi-square distribution:

$$
\sum_{i=1}^k v_i \sim \chi^2\left(\sum_{i=1}^k n_i, \sum_{i=1}^k \lambda_i\right)
$$
:::

::: {.proof}
**Method 1: Using MGFs**

The moment generating function of $v_i \sim \chi^2(n_i, \lambda_i)$ is:
$$
M_{v_i}(t) = \frac{\exp\left[-\lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}}
$$

Since $v_1, \dots, v_k$ are independent, the MGF of their sum $V = \sum v_i$ is the product of their individual MGFs:

$$
\begin{aligned}
M_V(t) &= \prod_{i=1}^k M_{v_i}(t) \\
&= \prod_{i=1}^k \frac{\exp\left[-\lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}} \\
&= \frac{\exp\left[-\sum \lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{\sum n_i/2}}
\end{aligned}
$$

This is the MGF of a non-central chi-square distribution with degrees of freedom $\sum n_i$ and non-centrality parameter $\sum \lambda_i$.

**Method 2: Geometric Interpretation**

Let $v_i = ||y_i||^2$ where $y_i \sim N_{n_i}(\mu_i, I_{n_i})$. Since the vectors $y_i$ are independent, we can stack them into a larger vector $y = (y_1', \dots, y_k')'$.

$$
y \sim N_{\sum n_i}(\mu, I_{\sum n_i}) \quad \text{where } \mu = (\mu_1', \dots, \mu_k')'
$$

The sum of squares is:
$$
\sum v_i = \sum ||y_i||^2 = ||y||^2
$$

By definition, $||y||^2$ follows a non-central chi-square distribution with degrees of freedom equal to the dimension of $y$ ($\sum n_i$) and non-centrality parameter $\lambda = \frac{1}{2} ||\mu||^2$.

$$
\lambda = \frac{1}{2} \sum_{i=1}^k ||\mu_i||^2 = \sum_{i=1}^k \lambda_i
$$
:::


## Distribution of Quadratic Forms

### MGF of Quadratic Forms

To determine the distribution of general quadratic forms $y'Ay$, we look at their MGF.

::: {#thm-mgf-quad name="MGF of Quadratic Form"}
If $y \sim N_p(\mu, \Sigma)$, then the MGF of $Q = y'Ay$ is:

$$
M_Q(t) = |I - 2tA\Sigma|^{-1/2} \exp\left(-\frac{1}{2} \mu' [I - (I - 2tA\Sigma)^{-1}] \Sigma^{-1} \mu\right)
$$
:::

*This is one of the most important theorems in the course, establishing that a quadratic form follows a chi-square distribution.*

### Non-central $\chi^2$ of Quadratic Forms

::: {#thm-dist-quad name="Distribution of y'Ay"}
Let $y \sim N_p(\mu, \Sigma)$. Let $A$ be a symmetric matrix of rank $r$.
Then $y'Ay \sim \chi^2(r, \lambda)$ with $\lambda = \frac{1}{2} \mu' A \mu$ **if and only if** $A\Sigma$ is idempotent ($A\Sigma A\Sigma = A\Sigma$).

**Special Case ($\Sigma = I$):**
If $\Sigma = I$, the condition simplifies to $A$ being idempotent ($A^2 = A$).
:::


We will prove a simplified version of @thm-dist-quad first.

::: {#thm-proj-matrix name="Distribution of Projected Spherical Normal"}
If $y \sim N_n(\mu, \sigma^2 I_n)$ and $P_V$ is a projection matrix onto a subspace $V$ of dimension $r$, then:

$$
\frac{1}{\sigma^2} y'P_V y = \frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{2\sigma^2}\right)
$$

This holds because $\frac{1}{\sigma^2} P_V (\sigma^2 I) = P_V$, which is idempotent.
:::

::: {.proof}
**When $\sigma^2=1$**


Let $P_V$ be the projection matrix. We know $P_V = QQ'$ where $Q = (q_1, \dots, q_r)$ is an $n \times r$ matrix with orthonormal columns ($Q'Q = I_r$).

The projection of vector $y$ onto the subspace $V$ can be expressed using the orthonormal basis vectors:
$$
P_V y = Q Q' y = (q_1, \dots, q_r) \begin{pmatrix} q_1' y \\ \vdots \\ q_r' y \end{pmatrix} = \sum_{i=1}^r (q_i' y) q_i
$$

The squared norm of the projection is:
$$
y' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2
$$

Since $y \sim N(\mu, I_n)$, the linear transformation $z = Q'y$ follows:
$$
z \sim N(Q'\mu, Q' I_n Q) = N(Q'\mu, I_r)
$$

Thus, $z$ is a vector of $r$ independent normal variables with variance 1. The sum of squares $||z||^2$ is by definition non-central chi-square:

$$
||z||^2 \sim \chi^2(r, \lambda)
$$
where the non-centrality parameter is:
$$
\lambda = \frac{1}{2} ||E(z)||^2 = \frac{1}{2} ||Q'\mu||^2
$$

Note that $||Q'\mu||^2 = \mu' Q Q' \mu = \mu' P_V \mu = ||P_V \mu||^2$.

Thus, $y' P_V y \sim \chi^2(r, \frac{1}{2} ||P_V \mu||^2)$.

**When $\sigma^2\not=1$**

If $y \sim N(\mu, \sigma^2 I_n)$, we standardize by dividing by $\sigma$.

Let $z = y/\sigma$. Then $z \sim N(\mu/\sigma, I_n)$.
Applying the previous result to $z$:

$$
z' P_V z = \frac{y' P_V y}{\sigma^2} \sim \chi^2\left(r, \frac{1}{2} \left|\left| P_V \frac{\mu}{\sigma} \right|\right|^2\right)
$$
which simplifies to:
$$
\frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{2\sigma^2}\right)
$$

:::

```{r}
#| label: plot-projection-3d-refined
#| warning: false
#| message: false
#| eval: !expr knitr::is_html_output()

library(plotly)
library(MASS)

# 1. Generate Data
set.seed(123)
n <- 200
mu <- c(2, 3, 5) 
sigma <- diag(3)
data <- mvrnorm(n, mu, sigma)
df <- as.data.frame(data)
colnames(df) <- c("x", "y", "z")

# 2. Project Points
proj_points <- t(apply(data, 1, function(p) {
  sum(p * c(1,0,0)) * c(1,0,0) + sum(p * c(0,1,0)) * c(0,1,0)
}))
df_proj <- as.data.frame(proj_points)
colnames(df_proj) <- c("px", "py", "pz")

# 3. Setup Axis Styles
ax_style <- list(
  title = "",
  showgrid = TRUE,        # Keep the coordinate plane grids
  gridcolor = "gray",
  gridwidth = 0.5,
  zeroline = FALSE,       # REMOVED: The static "crosshair" lines at 0
  showline = FALSE,       # REMOVED: The front bounding box
  showticklabels = FALSE,
  showbackground = FALSE,
  showspikes = FALSE
)

# 4. Create the Plot
plot_ly() %>%
  # --- Optional: Floor Plane ---
  add_trace(
    x = c(-2, 8, 8, -2), y = c(-2, -2, 8, 8), z = c(0, 0, 0, 0),
    type = "mesh3d", opacity = 0.05, color = 'gray', 
    hoverinfo = "none", showlegend = FALSE
  ) %>%
  # --- Original Data (y) ---
  add_trace(
    data = df, x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'markers',
    marker = list(size = 3, color = 'blue', opacity = 0.6),
    # Legend with Math (Italic y)
    name = '<i>y</i>' 
  ) %>%
  # --- Projected Shadow (P_V y) ---
  add_trace(
    data = df_proj, x = ~px, y = ~py, z = ~pz,
    type = 'scatter3d', mode = 'markers',
    marker = list(size = 3, color = 'red', opacity = 0.8),
    # Legend with Math (P_sub_V y)
    name = "<i>P</i><sub>V</sub><i>y</i>"
  ) %>%
  # --- Residual Lines ---
  add_segments(
    x = df$x, xend = df_proj$px,
    y = df$y, yend = df_proj$py,
    z = df$z, zend = df_proj$pz,
    line = list(color = 'gray', width = 1),
    showlegend = FALSE, hoverinfo = "none"
  ) %>%
  # --- Manual Labels ---
  add_text(
    x = c(8.5, 0, 0), y = c(0, 8.5, 0), z = c(0, 0, 8.5),
    # Used Unicode \u22A5 for perpendicular symbol
    text = c("<i>q</i><sub>1</sub>", "<i>q</i><sub>2</sub>", "<i>V</i><sup>\u22A5</sup>"),
    textfont = list(size = 15, color = "black"),
    showlegend = FALSE
  ) %>%
  layout(
    scene = list(
      xaxis = ax_style,
      yaxis = ax_style,
      zaxis = ax_style,
      aspectmode = "cube",
      camera = list(eye = list(x = 1.6, y = 1.6, z = 1.3))
    ),
    title = "Projection of Trivariate Normal onto 2D Subspace",
    margin = list(l=0, r=0, b=0, t=30)
  )
```


**Important Note:** The term $||P_V y||^2$ itself is **not** chi-square; it is a scaled chi-square variable. Its mean is:
$$
E(||P_V y||^2) = \sigma^2 \left(r + \frac{||P_V \mu||^2}{\sigma^2}\right) = r\sigma^2 + ||P_V \mu||^2
$$
