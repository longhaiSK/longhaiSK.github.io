---
title: "Distribution of Quadratic Forms"
---

```{r}
#| label: setup-images-lec40-p1-10
#| include: false
#| warning: false

library(pdftools)
library(magick)

# Define path to PDF
pdf_path <- "../Lec40-quadratic-form.pdf"
img_dir <- "lec40_images"

# Create image directory
if (!dir.exists(img_dir)) {
  dir.create(img_dir)
}

# Helper function to extract and crop a specific page
extract_page_image <- function(page_num, filename, geometry = "1200x800+0+200", force = FALSE) {
  target_file <- file.path(img_dir, filename)
  
  if (file.exists(pdf_path) && (force || !file.exists(target_file))) {
    tryCatch({
      bitmap <- pdf_render_page(pdf_path, page = page_num, dpi = 150)
      img <- image_read(bitmap)
      img_cropped <- image_crop(img, geometry)
      image_write(img_cropped, target_file)
    }, error = function(e) {
      warning(paste("Failed to extract page", page_num, ":", e$message))
    })
  }
}
```

This chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.

## Quadratic Forms

A quadratic form is a polynomial with terms all of degree two.

::: {#def-quadratic-form name="Quadratic Form"}
Let $y = (y_1, \dots, y_n)'$ be a random vector and $A$ be a symmetric $n \times n$ matrix. The scalar quantity $y'Ay$ is called a **quadratic form** in $y$.

$$
y'Ay = \sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j
$$
:::

**Examples:**

* **Squared Norm:** If $A = I_n$, then $y'I_n y = y'y = \sum y_i^2 = ||y||^2$.
* **Weighted Sum of Squares:** If $A$ is diagonal with elements $\lambda_i$, then $y'Ay = \sum \lambda_i y_i^2$.
* **Projection Sum of Squares:** If $P$ is a projection matrix, $||Py||^2 = (Py)'(Py) = y'P'Py = y'Py$ (since $P$ is symmetric and idempotent).

## Mean of Quadratic Forms

We can find the expected value of a quadratic form without assuming normality.

::: {#lem-simple-qf name="Mean of Simplified Quadratic Form"}
If $y$ is a random vector with mean $E(y) = \mu$ and covariance matrix $\text{Var}(y) = I_n$, then:
$$
E(y'y) = \text{tr}(I_n) + \mu'\mu = n + \mu'\mu
$$
:::

::: {.proof}
Let us decompose $y$ into its mean and a stochastic component: $y = \mu + z$, where $E(z) = 0$ and $\text{Var}(z) = E(zz') = I_n$.
Substituting this into the quadratic form:
$$
\begin{aligned}
y'y &= (\mu + z)'(\mu + z) \\
&= \mu'\mu + \mu'z + z'\mu + z'z \\
&= \mu'\mu + 2\mu'z + z'z
\end{aligned}
$$
Taking the expectation:
$$
\begin{aligned}
E(y'y) &= \mu'\mu + 2\mu'E(z) + E(z'z) \\
&= \mu'\mu + 0 + E\left(\sum_{i=1}^n z_i^2\right)
\end{aligned}
$$
Since $\text{Var}(z_i) = E(z_i^2) - (E(z_i))^2 = 1 - 0 = 1$, we have $E(\sum z_i^2) = \sum 1 = n$.
Thus, $E(y'y) = n + \mu'\mu$.
:::



```{r fig-qf-norm}
#| fig-cap: Illustration of the Mean and Distribution of Quadratic Forms
library(ggplot2)
library(MASS)
library(dplyr)

# --- 1. Setup Data & Parameters ---
set.seed(42)
n <- 100
sigma_val <- 1          
Sigma <- diag(2) * sigma_val^2

mu_orig <- c(5, 6)      # Original Mean
y_orig  <- c(7, 5)      # Updated Point y

# Generate 100 points from N(mu, I)
data_orig <- mvrnorm(n, mu_orig, Sigma)

# Define rotation angles
angles <- c(0, 70, 180)

# --- 2. Process Data for Each Angle ---
points_list <- list()
vectors_list <- list()

for (deg in angles) {
  theta <- deg * pi / 180
  rot_mat <- matrix(c(cos(theta), -sin(theta), 
                      sin(theta),  cos(theta)), nrow = 2, byrow = TRUE)
  
  # A. Rotate Points
  data_rot <- data_orig %*% t(rot_mat)
  df_pts <- data.frame(x = data_rot[,1], y = data_rot[,2])
  df_pts$Angle <- factor(paste0(deg, "째"), levels = paste0(angles, "째"))
  points_list[[length(points_list) + 1]] <- df_pts
  
  # B. Rotate Vectors (mu and y)
  mu_rot <- as.vector(rot_mat %*% mu_orig)
  y_rot  <- as.vector(rot_mat %*% y_orig)
  
  df_vec <- data.frame(
    Angle = factor(paste0(deg, "째"), levels = paste0(angles, "째")),
    mu_x = mu_rot[1], mu_y = mu_rot[2],
    y_x  = y_rot[1],  y_y  = y_rot[2]
  )
  vectors_list[[length(vectors_list) + 1]] <- df_vec
}

all_points  <- do.call(rbind, points_list)
all_vectors <- do.call(rbind, vectors_list)

# --- 3. Create Circle Data ---
# Radius is the length of mu
radius_mu <- sqrt(sum(mu_orig^2))
circle_data <- data.frame(
  x0 = 0, y0 = 0, r = radius_mu
)

# --- 4. Generate the Plot ---
ggplot() +
  # 1. Circle through the mu's (Centered at 0,0)
  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = radius_mu), 
                       color = "gray50", linetype = "dotted", size = 0.5) +
  
  # 2. Points (Data Cloud)
  geom_point(data = all_points, aes(x = x, y = y, color = Angle), 
             size = 0.5, alpha = 0.5) +
  
  # 3. Vector mu (Origin -> mu)
  geom_segment(data = all_vectors, 
               aes(x = 0, y = 0, xend = mu_x, yend = mu_y, color = Angle),
               arrow = arrow(length = unit(0.2, "cm")), size = 0.8) +
  
  # 4. Vector y (Origin -> y)
  geom_segment(data = all_vectors, 
               aes(x = 0, y = 0, xend = y_x, yend = y_y, color = Angle),
               arrow = arrow(length = unit(0.2, "cm")), size = 0.8) +
  
  # 5. Vector y - mu (mu -> y)
  geom_segment(data = all_vectors, 
               aes(x = mu_x, y = mu_y, xend = y_x, yend = y_y, color = Angle),
               arrow = arrow(length = unit(0.15, "cm")), 
               linetype = "dashed", size = 0.6) +
  
  # 6. Labels for mu, y, and y-mu
  geom_text(data = all_vectors, aes(x = mu_x, y = mu_y, label = expression(mu), color = Angle),
            parse = TRUE, vjust = -0.5, size = 4, show.legend = FALSE) +
  
  geom_text(data = all_vectors, aes(x = y_x, y = y_y, label = "y", color = Angle),
            vjust = -0.5, hjust = -0.2, size = 4, fontface = "italic", show.legend = FALSE) +
  
  # Label for y - mu (placed at midpoint)
  geom_text(data = all_vectors, aes(x = (mu_x + y_x)/2, y = (mu_y + y_y)/2, 
                                    label = expression(y - mu), color = Angle),
            parse = TRUE, size = 3, vjust = 1.5, show.legend = FALSE) +

  # 7. Origin Marker
  geom_point(aes(x=0, y=0), color="black", size=2) +
  
  # Formatting
  coord_fixed() +
  theme_minimal() +
  labs(title = "Rotations of Normal Cloud",
       x = "x", y = "y") +
  theme(legend.position = "bottom")
```


::: {#thm-mean-qf name="Mean of Quadratic Form"}
If $y$ is a random vector with mean $E(y) = \mu$ and covariance matrix $\text{Var}(y) = \Sigma$, and $A$ is a symmetric matrix of constants, then:

$$
E(y'Ay) = \text{tr}(A\Sigma) + \mu'A\mu
$$
:::

::: {.proof}
We present three methods to derive the expectation of the quadratic form.

**Method 1: Using the Trace Trick**

Using the fact that a scalar is equal to its own trace ($\text{tr}(c) = c$) and the linearity of expectation:
$$
\begin{aligned}
E(y'Ay) &= E[\text{tr}(y'Ay)] \\
&= E[\text{tr}(Ayy')] \quad \text{(cyclic property of trace)} \\
&= \text{tr}(A E[yy']) \quad \text{(linearity of expectation)}
\end{aligned}
$$
Recall that the covariance matrix is defined as $\Sigma = E[(y-\mu)(y-\mu)'] = E(yy') - \mu\mu'$. Rearranging this gives the second moment: $E(yy') = \Sigma + \mu\mu'$. Substituting this back:
$$
\begin{aligned}
E(y'Ay) &= \text{tr}(A(\Sigma + \mu\mu')) \\
&= \text{tr}(A\Sigma) + \text{tr}(A\mu\mu') \\
&= \text{tr}(A\Sigma) + \text{tr}(\mu'A\mu) \quad \text{(cyclic property on second term)} \\
&= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
$$

**Method 2: Using Scalar Summation**

We can express the quadratic form in scalar notation using the entries of $A=(a_{ij})$, $\Sigma=(\sigma_{ij})$, and $\mu=(\mu_i)$:
$$
\begin{aligned}
E(y'Ay) &= E\left(\sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j\right) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} E(y_i y_j) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} (\sigma_{ij} + \mu_i \mu_j) \\
&= \sum_{i=1}^n \sum_{j=1}^n a_{ij} \sigma_{ji} + \sum_{i=1}^n \sum_{j=1}^n \mu_i a_{ij} \mu_j \quad (\text{since } \Sigma \text{ is symmetric, } \sigma_{ij}=\sigma_{ji}) \\
&= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
$$

**Method 3: Using Spectral Decomposition of A**

Since $A$ is symmetric, we use its spectral decomposition $A = \sum_{i=1}^n \lambda_i q_i q_i'$. Substituting this into the quadratic form:
$$
y'Ay = y' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) y = \sum_{i=1}^n \lambda_i (q_i' y)^2
$$
Let $w_i = q_i' y$. This is a scalar random variable which is a linear transformation of $y$. Its properties are:
1.  **Mean:** $E(w_i) = q_i' E(y) = q_i' \mu$.
2.  **Variance:** $\text{Var}(w_i) = \text{Var}(q_i' y) = q_i' \text{Var}(y) q_i = q_i' \Sigma q_i$.

Using the relation $E(w_i^2) = \text{Var}(w_i) + [E(w_i)]^2$, we have:
$$
E[(q_i' y)^2] = q_i' \Sigma q_i + (q_i' \mu)^2
$$
Summing over all $i$ weighted by $\lambda_i$:
$$
\begin{aligned}
E(y'Ay) &= \sum_{i=1}^n \lambda_i \left[ q_i' \Sigma q_i + (q_i' \mu)^2 \right] \\
&= \sum_{i=1}^n \text{tr}(\lambda_i q_i' \Sigma q_i) + \mu' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) \mu \\
&= \text{tr}\left( \Sigma \sum_{i=1}^n \lambda_i q_i q_i' \right) + \mu' A \mu \\
&= \text{tr}(\Sigma A) + \mu' A \mu
\end{aligned}
$$
:::

::: {.remark name="Geometric Interpretation via Sigma"}
If we further decompose $\Sigma = \sum_{j=1}^n \gamma_j v_j v_j'$ (where $\gamma_j, v_j$ are eigenvalues/vectors of $\Sigma$), the trace term becomes:
$$
\text{tr}(A\Sigma) = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \gamma_j (q_i' v_j)^2
$$
Here, $(q_i' v_j)^2 = \cos^2(\theta_{ij})$ represents the alignment between the axes of the quadratic form ($A$) and the axes of the data covariance ($\Sigma$). The expectation is maximized when the eigenspaces of $A$ and $\Sigma$ align.
:::

::: {#cor-projection-mean name="Expectation with Projection Matrix"}
Consider the special case where:
1.  $P$ is a **projection matrix** (symmetric and idempotent, $P^2=P$).
2.  The covariance is **spherical**: $\Sigma = \sigma^2 I_n$.

Then the expectation simplifies to:
$$
E(y'Py) = \sigma^2 r + ||P\mu||^2
$$
where $r = \text{rank}(P) = \text{tr}(P)$.

**Proof:**
Using @thm-mean-qf with $A=P$ and $\Sigma=\sigma^2 I_n$:

1.  **Trace Term:** $\text{tr}(P\Sigma) = \text{tr}(P(\sigma^2 I_n)) = \sigma^2 \text{tr}(P)$. Since $P$ is idempotent, its eigenvalues are either 0 or 1, so $\text{tr}(P) = \text{rank}(P) = r$.
2.  **Mean Term:** Since $P$ is symmetric and idempotent ($P'P = P^2 = P$), we can rewrite the quadratic form:
    $$
    \mu' P \mu = \mu' P' P \mu = (P\mu)' (P\mu) = ||P\mu||^2
    $$
:::

:::{#exm-mean-ss-decomposition name="Expectation of Sum of Squares Decomposition (i.i.d. Case)"}

Consider a random vector $y = (y_1, \dots, y_n)'$ with mean vector $\mu_y = \mu j_n$ and covariance $\Sigma = \sigma^2 I_n$. We analyze the two components of the total sum of squares by projecting $y$ onto the mean space ($P_{j_n}$) and the residual space ($I-P_{j_n}$).

**1. The Projection Vectors**

First, we write the explicit forms of the projected vectors using $P_{j_n} = \frac{1}{n}j_n j_n'$:

* **Mean Vector ($P_{j_n}y$):** Projecting $y$ onto the column space of $j_n$ replaces every element with the sample mean $\bar{y}$.
    $$
    P_{j_n}y = \bar{y} j_n = \begin{pmatrix} \bar{y} \\ \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
    $$

* **Residual Vector ($(I-P_{j_n})y$):** Subtracting the mean projection from $y$ yields the deviations.
    $$
    (I-P_{j_n})y = y - \bar{y}j_n = \begin{pmatrix} y_1 - \bar{y} \\ y_2 - \bar{y} \\ \vdots \\ y_n - \bar{y} \end{pmatrix}
    $$

**2. Expectations of Squared Norms**

We now find the expectation of the squared length of these vectors using @cor-projection-mean.

**Part A: Sum of Squares for Mean**
The quadratic form is the squared norm of the projected mean vector:
$$
y'P_{j_n}y = ||P_{j_n}y||^2 = \sum_{i=1}^n \bar{y}^2 = n\bar{y}^2
$$
Applying the corollary with $P=P_{j_n}$:

* **Rank:** $\text{tr}(P_{j_n}) = 1$.
* **Mean:** $P_{j_n}\mu_y = P_{j_n}(\mu j_n) = \mu j_n$. The squared norm is $n\mu^2$.

$$
E[||P_{j_n}y||^2] = \sigma^2(1) + n\mu^2
$$

**Part B: Sum of Squared Errors (SSE)**
The quadratic form is the squared norm of the residual vector:
$$
y'(I-P_{j_n})y = ||(I-P_{j_n})y||^2 = \sum_{i=1}^n (y_i - \bar{y})^2
$$
Applying the corollary with $P=I-P_{j_n}$:

* **Rank:** $\text{tr}(I-P_{j_n}) = n - 1$.
* **Mean:** $(I-P_{j_n})\mu_y = \mu_y - P_{j_n}\mu_y = \mu j_n - \mu j_n = 0$. The squared norm is $0$.

$$
E[||(I-P_{j_n})y||^2] = \sigma^2(n-1) + 0
$$

**Conclusion**
These results confirm the standard properties: $E(\bar{y}^2) = \frac{\sigma^2}{n} + \mu^2$ and $E(S^2) = \sigma^2$.
:::

:::{#exm-mean-sst-regression name="Expectation of Total Sum of Squares (Regression Case)"}

Consider now a regression setting where the mean of $y$ depends on covariates (e.g., $\mu_i = \beta_0 + \beta_1 x_i$). The mean vector $\mu_y$ is **not** proportional to $j_n$. We are interested in the expectation of the **Total Sum of Squares (SST)**.

**1. Identification**
The SST measures the variation of $y$ around the *global sample mean* $\bar{y}$, ignoring the covariates:
$$
SST = \sum_{i=1}^n (y_i - \bar{y})^2 = y'(I - P_{j_n})y
$$
This is the same quadratic form as Part B in the previous example, but the underlying mean $\mu_y$ has changed.

**2. Calculation**
We apply @cor-projection-mean with $P = I - P_{j_n}$ and general $\mu_y$:

* **Rank Term:** Same as before, $\text{tr}(I - P_{j_n}) = n - 1$.
* **Mean Term:** The projection of the mean vector is no longer zero.
    $$
    (I - P_{j_n})\mu_y = \mu_y - \bar{\mu}j_n = \begin{pmatrix} \mu_1 - \bar{\mu} \\ \vdots \\ \mu_n - \bar{\mu} \end{pmatrix}
    $$
    where $\bar{\mu} = \frac{1}{n}\sum \mu_i$ is the average of the true means.
    The squared norm is the sum of squared deviations of the true means:
    $$
    ||(I - P_{j_n})\mu_y||^2 = \sum_{i=1}^n (\mu_i - \bar{\mu})^2
    $$

**Conclusion**
$$
E(SST) = (n-1)\sigma^2 + \sum_{i=1}^n (\mu_i - \bar{\mu})^2
$$
This shows that in regression, the SST estimates $(n-1)\sigma^2$ *plus* the variability introduced by the regression signal (the spread of the true means $\mu_i$).
:::
## Non-central $\chi^2$ Distribution

To understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.

::: {#def-nc-chisq name="Non-central $\chi^2$ Distribution"}
Let $y \sim N_n(\mu, I_n)$. The random variable $V = y'y = \sum y_i^2$ follows a **non-central chi-square distribution** with $n$ degrees of freedom and non-centrality parameter $\lambda$.

$$
V \sim \chi^2(n, \lambda) \quad \text{where } \lambda = \frac{1}{2} \mu'\mu = \frac{1}{2} ||\mu||^2
$$
:::

**Note:** Some definitions of non-central $\chi^2$ use $\lambda = \mu'\mu$. In this course, we use $\lambda = \frac{1}{2}\mu'\mu$.

### Visualizing $\chi^2$ Distributions

Here is a plot visualizing the difference between central and non-central Chi-square distributions.


```{r}
#| label: fig-plot-chisq
#| echo: false
#| fig-cap: "Central vs Non-central Chi-square Distribution"

library(ggplot2)

x <- seq(0, 20, length.out = 500)
df <- 5
ncp <- 3 # Non-centrality parameter

y_central <- dchisq(x, df = df)
y_noncentral <- dchisq(x, df = df, ncp = ncp)

data <- data.frame(
  x = rep(x, 2),
  density = c(y_central, y_noncentral),
  type = rep(c("Central (n=5)", "Non-central (n=5, lambda=3)"), each = length(x))
)

ggplot(data, aes(x = x, y = density, color = type)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(title = "Chi-square Distributions",
       y = "Density",
       x = "x") +
  theme(legend.position = "top")
```





The density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter $\lambda$ increases.

### Mean, Variance, and MGF

We summarize the key properties of the non-central chi-square distribution.

::: {#thm-chisq-properties name="Properties of Non-central Chi-square"}
Let $V \sim \chi^2(n, \lambda)$. Then:

1.  **Mean:** $E(V) = n + 2\lambda$
2.  **Variance:** $\text{Var}(V) = 2n + 8\lambda$
3.  **Moment Generating Function (MGF):**
    $$
    m_V(t) = \frac{\exp[-\lambda \{1 - 1/(1-2t)\}]}{(1-2t)^{n/2}} \quad \text{for } t < 1/2
    $$
:::

::: {.proof name="Mean"}
By definition, $V \sim \chi^2(n, \lambda)$ is the distribution of $y'y$ where $y \sim N_n(\mu, I_n)$ and the non-centrality parameter is $\lambda = \frac{1}{2}\mu'\mu$.
Applying @lem-simple-qf to the random vector $y$:
$$
E(V) = E(y'y) = n + \mu'\mu = n + 2\lambda
$$
:::

::: {.proof name="MGF"}
Since the components $y_i$ of the vector $y$ are independent $N(\mu_i, 1)$, and $V = \sum_{i=1}^n y_i^2$, the MGF of $V$ is the product of the MGFs of each $y_i^2$:
$$
m_V(t) = E[e^{t \sum y_i^2}] = \prod_{i=1}^n E[e^{t y_i^2}]
$$
Consider a single component $y_i \sim N(\mu_i, 1)$. Its squared expectation is:
$$
\begin{aligned}
E[e^{t y_i^2}] &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ty^2} e^{-\frac{1}{2}(y-\mu_i)^2} dy \\
&= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left\{ -\frac{1}{2} \left[ (1-2t)y^2 - 2\mu_i y + \mu_i^2 \right] \right\} dy
\end{aligned}
$$
Completing the square in the exponent for $y$ (assuming $t < 1/2$):
$$
(1-2t)y^2 - 2\mu_i y + \mu_i^2 = (1-2t)\left(y - \frac{\mu_i}{1-2t}\right)^2 + \mu_i^2 - \frac{\mu_i^2}{1-2t}
$$
The integral of the Gaussian kernel $\exp\{ -\frac{1}{2}(1-2t)(y - \dots)^2 \}$ yields $\sqrt{\frac{2\pi}{1-2t}}$. The remaining constant term is:
$$
\exp\left\{ -\frac{1}{2} \left( \mu_i^2 - \frac{\mu_i^2}{1-2t} \right) \right\} = \exp\left\{ \frac{\mu_i^2}{2} \left( \frac{1}{1-2t} - 1 \right) \right\} = \exp\left\{ \frac{\mu_i^2 t}{1-2t} \right\}
$$
Thus, for a single component:
$$
m_{y_i^2}(t) = (1-2t)^{-1/2} \exp\left( \frac{\mu_i^2 t}{1-2t} \right)
$$
Multiplying the MGFs for all $n$ components:
$$
\begin{aligned}
m_V(t) &= \prod_{i=1}^n (1-2t)^{-1/2} \exp\left( \frac{\mu_i^2 t}{1-2t} \right) \\
&= (1-2t)^{-n/2} \exp\left( \frac{t \sum \mu_i^2}{1-2t} \right)
\end{aligned}
$$
Substituting $\lambda = \frac{1}{2} \sum \mu_i^2$ (so $\sum \mu_i^2 = 2\lambda$):
$$
m_V(t) = (1-2t)^{-n/2} \exp\left( \frac{2\lambda t}{1-2t} \right)
$$
This form matches the theorem statement, noting that $\frac{2\lambda t}{1-2t} = -\lambda (1 - \frac{1}{1-2t})$.
:::

::: {.proof name="Variance"}
We use the **Cumulant Generating Function**, $K_V(t) = \ln m_V(t)$, as its derivatives yield the mean and variance directly:
$$
K_V(t) = -\frac{n}{2} \ln(1-2t) + \frac{2\lambda t}{1-2t}
$$
First derivative (Mean):
$$
\begin{aligned}
K'_V(t) &= -\frac{n}{2} \left(\frac{-2}{1-2t}\right) + 2\lambda \left[ \frac{1(1-2t) - t(-2)}{(1-2t)^2} \right] \\
&= \frac{n}{1-2t} + 2\lambda \frac{1}{(1-2t)^2}
\end{aligned}
$$
Second derivative (Variance):
$$
\begin{aligned}
K''_V(t) &= n(-1)(1-2t)^{-2}(-2) + 2\lambda(-2)(1-2t)^{-3}(-2) \\
&= \frac{2n}{(1-2t)^2} + \frac{8\lambda}{(1-2t)^3}
\end{aligned}
$$
Evaluating at $t=0$:
$$
\text{Var}(V) = K''_V(0) = 2n + 8\lambda
$$
:::

### Additivity

::: {#thm-chisq-additivity name="Additivity of Chi-square"}
If $v_1, \dots, v_k$ are independent random variables distributed as $\chi^2(n_i, \lambda_i)$, then their sum follows a chi-square distribution:

$$
\sum_{i=1}^k v_i \sim \chi^2\left(\sum_{i=1}^k n_i, \sum_{i=1}^k \lambda_i\right)
$$
:::

::: {.proof}
**Method 1: Using MGFs**

The moment generating function of $v_i \sim \chi^2(n_i, \lambda_i)$ is:
$$
M_{v_i}(t) = \frac{\exp\left[-\lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}}
$$

Since $v_1, \dots, v_k$ are independent, the MGF of their sum $V = \sum v_i$ is the product of their individual MGFs:

$$
\begin{aligned}
M_V(t) &= \prod_{i=1}^k M_{v_i}(t) \\
&= \prod_{i=1}^k \frac{\exp\left[-\lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}} \\
&= \frac{\exp\left[-\sum \lambda_i \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{\sum n_i/2}}
\end{aligned}
$$

This is the MGF of a non-central chi-square distribution with degrees of freedom $\sum n_i$ and non-centrality parameter $\sum \lambda_i$.

**Method 2: Geometric Interpretation**

Let $v_i = ||y_i||^2$ where $y_i \sim N_{n_i}(\mu_i, I_{n_i})$. Since the vectors $y_i$ are independent, we can stack them into a larger vector $y = (y_1', \dots, y_k')'$.

$$
y \sim N_{\sum n_i}(\mu, I_{\sum n_i}) \quad \text{where } \mu = (\mu_1', \dots, \mu_k')'
$$

The sum of squares is:
$$
\sum v_i = \sum ||y_i||^2 = ||y||^2
$$

By definition, $||y||^2$ follows a non-central chi-square distribution with degrees of freedom equal to the dimension of $y$ ($\sum n_i$) and non-centrality parameter $\lambda = \frac{1}{2} ||\mu||^2$.

$$
\lambda = \frac{1}{2} \sum_{i=1}^k ||\mu_i||^2 = \sum_{i=1}^k \lambda_i
$$
:::

### Poisson Mixture Representation

::: {#thm-chisq-poisson-mixture name="Poisson Mixture Representation"}
Let $v \sim \chi^2(n, \lambda)$ be a non-central chi-square random variable. Its probability density function can be represented as a Poisson-weighted sum of central chi-square density functions:

$$
f(v; n, \lambda) = \sum_{j=0}^{\infty} \left( \frac{e^{-\lambda} \lambda^j}{j!} \right) f(v; n+2j, 0)
$$

where $f(v; \nu, 0)$ is the density of a central chi-square distribution with $\nu$ degrees of freedom.
:::

::: {.proof}
We use the Moment Generating Function (MGF) approach. The MGF of a non-central chi-square distribution $v \sim \chi^2(n, \lambda)$ is:
$$
M_v(t) = (1-2t)^{-n/2} \exp\left( \lambda \left[ \frac{1}{1-2t} - 1 \right] \right)
$$

We can expand the exponential term using the power series $e^x = \sum_{j=0}^\infty \frac{x^j}{j!}$:
$$
\begin{aligned}
M_v(t) &= (1-2t)^{-n/2} e^{-\lambda} \exp\left( \frac{\lambda}{1-2t} \right) \\
&= e^{-\lambda} (1-2t)^{-n/2} \sum_{j=0}^{\infty} \frac{1}{j!} \left( \frac{\lambda}{1-2t} \right)^j \\
&= \sum_{j=0}^{\infty} \left( \frac{e^{-\lambda} \lambda^j}{j!} \right) (1-2t)^{-(n+2j)/2}
\end{aligned}
$$

Recognizing the terms:

1.  The term in parentheses, $P(J=j) = \frac{e^{-\lambda} \lambda^j}{j!}$, is the probability mass function of a **Poisson** random variable $J \sim \text{Poisson}(\lambda)$.
2.  The term $(1-2t)^{-(n+2j)/2}$ is the MGF of a **central chi-square** distribution with $n+2j$ degrees of freedom.

Since the MGF of the mixture is the sum of the MGFs of the components weighted by the mixture probabilities, the density must follow the same mixture structure.
:::

::: {.remark}
This theorem implies a hierarchical model for generating a non-central chi-square variable:

1.  Sample $J \sim \text{Poisson}(\lambda)$.
2.  Given $J=j$, sample $V \sim \chi^2(n+2j, 0)$.

This is particularly useful for numerical computation, as it allows the non-central CDF to be approximated by a finite sum of central chi-square CDFs.
:::

### Poisson Mixture Representation

::: {#thm-chisq-poisson-mixture name="Poisson Mixture Representation"}
Let $v \sim \chi^2(n, \lambda)$ where $\lambda = \frac{1}{2}\|\mu\|^2$. Its probability density function is a Poisson mixture of central chi-square densities:

$$
f(v; n, \lambda) = \sum_{j=0}^{\infty} \text{Poisson}(j; \lambda) \cdot f(v; n+2j, 0)
$$

where $\text{Poisson}(j; \lambda) = \frac{e^{-\lambda} \lambda^j}{j!}$ and $f(v; \nu, 0)$ is the central chi-square density with $\nu$ degrees of freedom.
:::

::: {.proof}
The non-central chi-square distribution arises from $v = \sum_{i=1}^n y_i^2$ where $y_i \sim N(\mu_i, 1)$. 
We can decompose $v$ into two independent parts:
1. One component in the direction of the mean vector $\mu$.
2. $n-1$ components orthogonal to $\mu$.

Let $z_1 \sim N(\|\mu\|, 1)$ and $z_2, \dots, z_n \sim N(0, 1)$. Then $v = z_1^2 + \sum_{i=2}^n z_i^2$.
The second term is simply $\chi^2(n-1, 0)$. The first term $z_1^2$ is the interesting part. 

Using the power series expansion of the MGF (as shown in the previous derivation):
$$
M_v(t) = \sum_{j=0}^{\infty} \left( \frac{e^{-\lambda} \lambda^j}{j!} \right) (1-2t)^{-(n+2j)/2}
$$
By using $\lambda = \frac{1}{2}\|\mu\|^2$, we ensure that the weights $\frac{e^{-\lambda} \lambda^j}{j!}$ sum to 1, perfectly matching the Poisson distribution probabilities.
:::

::: {.remark name="Geometric Insight"}
This representation reveals that a non-central chi-square variable behaves like a central chi-square variable whose **degrees of freedom are themselves random**. 

Specifically:
$$
V \mid J \sim \chi^2(n + 2J, 0) \quad \text{where} \quad J \sim \text{Poisson}\left(\frac{1}{2}\|\mu\|^2\right)
$$
As the distance $\|\mu\|$ increases, the Poisson distribution shifts to the right, increasing the expected degrees of freedom and thus shifting the mass of the chi-square distribution to the right.
:::
## Distribution of Quadratic Forms

### MGF of Quadratic Forms

To determine the distribution of general quadratic forms $y'Ay$, we look at their MGF.

::: {#thm-mgf-quad name="MGF of Quadratic Form"}
If $y \sim N_p(\mu, \Sigma)$, then the MGF of $Q = y'Ay$ is:

$$
M_Q(t) = |I - 2tA\Sigma|^{-1/2} \exp\left(-\frac{1}{2} \mu' [I - (I - 2tA\Sigma)^{-1}] \Sigma^{-1} \mu\right)
$$
:::


### Non-central $\chi^2$ of Quadratic Forms



::: {#thm-dist-quad name="Distribution of y'Ay"}
Let $y \sim N_p(\mu, \Sigma)$. Let $A$ be a symmetric matrix of rank $r$.
Then $y'Ay \sim \chi^2(r, \lambda)$ with $\lambda = \frac{1}{2} \mu' A \mu$ **if and only if** $A\Sigma$ is idempotent ($A\Sigma A\Sigma = A\Sigma$).

**Special Case ($\Sigma = I$):**
If $\Sigma = I$, the condition simplifies to $A$ being idempotent ($A^2 = A$).
:::


We will prove a simplified version of @thm-dist-quad first.

::: {#thm-proj-matrix name="Distribution of Projected Spherical Normal"}
If $y \sim N_n(\mu, \sigma^2 I_n)$ and $P_V$ is a projection matrix onto a subspace $V$ of dimension $r$, then:

$$
\frac{1}{\sigma^2} y'P_V y = \frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{2\sigma^2}\right)
$$

This holds because $\frac{1}{\sigma^2} P_V (\sigma^2 I) = P_V$, which is idempotent.
:::

*Note: This is one of the most important theorems in the course, establishing that a quadratic form follows a chi-square distribution.*

::: {.proof}

**When $\sigma^2=1$**


Let $P_V$ be the projection matrix. We know $P_V = QQ'$ where $Q = (q_1, \dots, q_r)$ is an $n \times r$ matrix with orthonormal columns ($Q'Q = I_r$).

The projection of vector $y$ onto the subspace $V$ can be expressed using the orthonormal basis vectors:
$$
P_V y = Q Q' y = (q_1, \dots, q_r) \begin{pmatrix} q_1' y \\ \vdots \\ q_r' y \end{pmatrix} = \sum_{i=1}^r (q_i' y) q_i
$$

The squared norm of the projection is:
$$
y' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2
$$

Since $y \sim N(\mu, I_n)$, the linear transformation $z = Q'y$ follows:
$$
z \sim N(Q'\mu, Q' I_n Q) = N(Q'\mu, I_r)
$$

Thus, $z$ is a vector of $r$ independent normal variables with variance 1. The sum of squares $||z||^2$ is by definition non-central chi-square:

$$
||z||^2 \sim \chi^2(r, \lambda)
$$
where the non-centrality parameter is:
$$
\lambda = \frac{1}{2} ||E(z)||^2 = \frac{1}{2} ||Q'\mu||^2
$$

Note that $||Q'\mu||^2 = \mu' Q Q' \mu = \mu' P_V \mu = ||P_V \mu||^2$.

Thus, $y' P_V y \sim \chi^2(r, \frac{1}{2} ||P_V \mu||^2)$.

**When $\sigma^2\not=1$**

If $y \sim N(\mu, \sigma^2 I_n)$, we standardize by dividing by $\sigma$.

Let $z = y/\sigma$. Then $z \sim N(\mu/\sigma, I_n)$.
Applying the previous result to $z$:

$$
z' P_V z = \frac{y' P_V y}{\sigma^2} \sim \chi^2\left(r, \frac{1}{2} \left|\left| P_V \frac{\mu}{\sigma} \right|\right|^2\right)
$$
which simplifies to:
$$
\frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{2\sigma^2}\right)
$$

:::

::: {.callout-important}
## Scale of the Quadratic Form

The term $\|P_V y\|^2$ itself is **not** a standard chi-square variable; it is a scaled chi-square variable. Its mean is:

$$
E(\|P_V y\|^2) = \sigma^2 \left(r + \frac{\|P_V \mu\|^2}{\sigma^2}\right) = r\sigma^2 + \|P_V \mu\|^2
$$
:::

```{r}
#| label: plot-projection-3d-refined
#| warning: false
#| message: false
#| fig-cap: "Visualization of Projected Trivariate Normal Cloud"
#| screenshot.force: true
#| webshot2: !expr list(vwidth = 900, vheight = 600, zoom = 2)
#| out-width: "100%"

library(plotly)
library(MASS)

# 1. Generate Data
set.seed(123)
n <- 200
mu <- c(2, 3, 5) 
sigma <- diag(3)
data <- mvrnorm(n, mu, sigma)
df <- as.data.frame(data)
colnames(df) <- c("x", "y", "z")

# 2. Project Points
proj_points <- t(apply(data, 1, function(p) {
  sum(p * c(1,0,0)) * c(1,0,0) + sum(p * c(0,1,0)) * c(0,1,0)
}))
df_proj <- as.data.frame(proj_points)
colnames(df_proj) <- c("px", "py", "pz")

# 3. Setup Axis Styles
ax_style <- list(
  title = "",
  showgrid = TRUE,        # Keep the coordinate plane grids
  gridcolor = "gray",
  gridwidth = 0.5,
  zeroline = FALSE,       # REMOVED: The static "crosshair" lines at 0
  showline = FALSE,       # REMOVED: The front bounding box
  showticklabels = FALSE,
  showbackground = FALSE,
  showspikes = FALSE
)

# 4. Create the Plot
plot_ly() %>%
  # --- Optional: Floor Plane ---
  add_trace(
    x = c(-2, 8, 8, -2), y = c(-2, -2, 8, 8), z = c(0, 0, 0, 0),
    type = "mesh3d", opacity = 0.05, color = 'gray', 
    hoverinfo = "none", showlegend = FALSE
  ) %>%
  # --- Original Data (y) ---
  add_trace(
    data = df, x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'markers',
    marker = list(size = 3, color = 'blue', opacity = 0.6),
    # Legend with Math (Italic y)
    name = '<i>y</i>' 
  ) %>%
  # --- Projected Shadow (P_V y) ---
  add_trace(
    data = df_proj, x = ~px, y = ~py, z = ~pz,
    type = 'scatter3d', mode = 'markers',
    marker = list(size = 3, color = 'red', opacity = 0.8),
    # Legend with Math (P_sub_V y)
    name = "<i>P</i><sub>V</sub><i>y</i>"
  ) %>%
  # --- Residual Lines ---
  add_segments(
    x = df$x, xend = df_proj$px,
    y = df$y, yend = df_proj$py,
    z = df$z, zend = df_proj$pz,
    line = list(color = 'gray', width = 1),
    showlegend = FALSE, hoverinfo = "none"
  ) %>%
  # --- Manual Labels ---
  add_text(
    x = c(8.5, 0, 0), y = c(0, 8.5, 0), z = c(0, 0, 8.5),
    # Used Unicode \u22A5 for perpendicular symbol
    text = c("<i>q</i><sub>1</sub>", "<i>q</i><sub>2</sub>", "<i>V</i><sup>\u22A5</sup>"),
    textfont = list(size = 15, color = "black"),
    showlegend = FALSE
  ) %>%
  layout(
    scene = list(
      xaxis = ax_style,
      yaxis = ax_style,
      zaxis = ax_style,
      aspectmode = "cube",
      camera = list(eye = list(x = 1.6, y = 1.6, z = 1.3))
    ),
    title = "Projection of Trivariate Normal onto 2D Subspace",
    margin = list(l=0, r=0, b=0, t=30)
  )
```

