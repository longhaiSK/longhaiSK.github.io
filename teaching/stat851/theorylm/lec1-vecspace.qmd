---
title: " Projection in Vector Space"
---

```{r}
#| label: setup-images-common
#| include: false
#| warning: false

library(pdftools)
library(magick)

# Define path to PDF
pdf_path <- "../Lec10-vector space and projection.pdf"
img_dir <- "lec1_images"

# Create image directory
if (!dir.exists(img_dir)) {
  dir.create(img_dir)
}

# Helper function to extract and crop a specific page
extract_page_image <- function(page_num, filename, geometry = "1200x800+0+200", force = FALSE) {
  target_file <- file.path(img_dir, filename)
  
  # Process if PDF exists AND (Force is TRUE OR Image doesn't exist yet)
  if (file.exists(pdf_path) && (force || !file.exists(target_file))) {
    tryCatch({
      bitmap <- pdf_render_page(pdf_path, page = page_num, dpi = 150)
      img <- image_read(bitmap)
      img_cropped <- image_crop(img, geometry)
      image_write(img_cropped, target_file)
    }, error = function(e) {
      warning(paste("Failed to extract page", page_num, ":", e$message))
    })
  }
}
```

## Vector and Projection onto a Line

**Vectors and Operations**

The concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.

::: {#def-vector}
### Vector
A **vector** $x$ is defined as a point in $n$-dimensional space ($\mathbb{R}^n$). It is typically represented as a column vector containing $n$ real-valued components:

$$
x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
$$
:::

Vectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.

**Vector Arithmetic:**
Vectors can be manipulated geometrically:

::: {#def-vector-addition}
### Vector Addition
The sum of two vectors $x$ and $y$ creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the "parallelogram rule" or the "head-to-tail" method, where you place the tail of $y$ at the head of $x$.

$$
x + y = \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}
$$
:::

::: {#def-vector-subtraction}
### Vector Subtraction
The difference $d = y - x$ is the vector that "closes the triangle" formed by $x$ and $y$. It represents the displacement vector that connects the tip of $x$ to the tip of $y$, such that $x + d = y$.
:::


**Scalar Multiplication and Length**

In addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.

::: {#def-scalar-mult}
### Scalar Multiplication
Multiplying a vector by a scalar $c$ scales its magnitude (length) without changing its line of direction. If $c$ is positive, the direction remains the same; if $c$ is negative, the direction is reversed.

$$
cx = \begin{pmatrix} cx_1 \\ \vdots \\ cx_n \end{pmatrix}
$$
:::

We often need to quantify the "size" of a vector. This is done using the concept of length, or norm.

::: {#def-euclidean-distance}
### Euclidean Distance (Length)
The length (or norm) of a vector $x = (x_1, \dots, x_n)^T$ corresponds to the straight-line distance from the origin to the point defined by $x$. It is defined as the square root of the sum of squared components:

$$
||x||^2 = \sum_{i=1}^n x_i^2
$$

$$
||x|| = \sqrt{\sum_{i=1}^n x_i^2}
$$
:::

**Angle and Inner Product**

To understand the relationship between two vectors $x$ and $y$ beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors $x$, $y$, and their difference $y-x$. By applying the classic **Law of Cosines** to this triangle, we can relate the geometric angle to the vector lengths.

::: {#thm-law-of-cosines}
### Law of Cosines
For a triangle with sides $a, b, c$ and angle $\theta$ opposite to side $c$:

$$
c^2 = a^2 + b^2 - 2ab \cos \theta
$$
:::

Translating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:

$$
||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \cdot ||y|| \cos \theta
$$

This equation provides a critical link between the geometric angle $\theta$ and the algebraic norms of the vectors.

**Derivation of Inner Product**

We can express the squared distance term $||y - x||^2$ purely algebraically by expanding the components:

$$
||y - x||^2 = \sum_{i=1}^n (x_i - y_i)^2
$$

$$
= \sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)
$$

$$
= ||x||^2 + ||y||^2 - 2 \sum_{i=1}^n x_i y_i
$$

By comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the **Inner Product** (or dot product).

::: {#def-inner-product}
### Inner Product
The inner product of two vectors $x$ and $y$ is defined as the sum of the products of their corresponding components:

$$
x'y = \sum_{i=1}^n x_i y_i = \langle x, y \rangle
$$
:::

Thus, equating the geometric and algebraic forms yields the fundamental relationship:

$$
x'y = ||x|| \cdot ||y|| \cos \theta
$$

**Coordinate (Scalar) Projection**

The inner product allows us to calculate projections, which quantify how much of one vector "lies along" another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the "shadow" cast by vector $y$ onto vector $x$.

The length of this projection is given by:

$$
||y|| \cos \theta = \frac{x'y}{||x||}
$$

This expression can be interpreted as the inner product of $y$ with the normalized (unit) vector in the direction of $x$:

$$
\text{Scalar Projection} = \left\langle \frac{x}{||x||}, y \right\rangle
$$

**Vector Projection Formula**

The scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.

::: {#def-vector-projection}
### Vector Projection
The projection of vector $y$ onto vector $x$, denoted $\hat{y}$, is calculated as:

$$
\text{Projection Vector} = (\text{Length}) \cdot (\text{Direction})
$$

$$
\hat{y} = \left( \frac{x'y}{||x||} \right) \cdot \frac{x}{||x||}
$$

This is often written compactly by combining the denominators:

$$
\hat{y} = \frac{x'y}{||x||^2} x
$$
:::

**Perpendicularity (Orthogonality)**

A special case of the angle between vectors arises when $\theta = 90^\circ$. This geometric concept of perpendicularity is central to the theory of projections and least squares.

::: {#def-perpendicularity}
### Perpendicularity
Two vectors are defined as **perpendicular** (or orthogonal) if the angle between them is $90^\circ$ ($\pi/2$).

Since $\cos(90^\circ) = 0$, the condition for orthogonality simplifies to the inner product being zero:

$$
x'y = 0 \iff x \perp y
$$
:::

::: {#exm-orthogonal-vectors}
### Orthogonal Vectors
Consider two vectors in $\mathbb{R}^2$: $x = (1, 1)'$ and $y = (1, -1)'$.

$$
x'y = 1(1) + 1(-1) = 1 - 1 = 0
$$

Since their inner product is zero, these vectors are orthogonal to each other.
:::

**Projection onto a Line (Subspace)**

We can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.

::: {#def-line-space}
### Line Spanned by a Vector
The line space $L(x)$, or the space spanned by a vector $x$, is defined as the set of all scalar multiples of $x$:

$$
L(x) = \{ cx \mid c \in \mathbb{R} \}
$$
:::

The projection of $y$ onto $L(x)$, denoted $\hat{y}$, is defined by the geometric property that it is the closest point on the line to $y$. This implies that the error vector (or residual) must be perpendicular to the line itself.

::: {#def-projection-line}
### Projection onto a Line
A vector $\hat{y}$ is the projection of $y$ onto the line $L(x)$ if:

1. $\hat{y}$ lies on the line $L(x)$ (i.e., $\hat{y} = cx$ for some scalar $c$).

2. The residual vector $(y - \hat{y})$ is perpendicular to the direction vector $x$.
:::

**Derivation:**
To find the value of the scalar $c$, we apply the orthogonality condition:

$$
(y - \hat{y}) \perp x \implies x'(y - cx) = 0
$$

Expanding this inner product gives:

$$
x'y - c(x'x) = 0
$$

Solving for $c$, we obtain:

$$
c = \frac{x'y}{||x||^2}
$$

This confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.






**Alternative Forms of the Projection Formula**

We can express the projection vector $\hat{y}$ in several equivalent ways to highlight different geometric interpretations.

::: {#def-projection-formulae}
### Forms of Projection
The projection of $y$ onto the vector $x$ is given by:

$$
\hat{y} = \frac{x'y}{||x||^2} x = \left\langle y, \frac{x}{||x||} \right\rangle \frac{x}{||x||}
$$

This second form separates the components into:
$$
\text{Projection} = (\text{Scalar Projection}) \times (\text{Unit Direction})
$$
:::

**Projection Matrix ($P_x$)**

In linear models, it is often more convenient to view projection as a linear transformation applied to the vector $y$. This allows us to define a **Projection Matrix**.

We can rewrite the formula for $\hat{y}$ by factoring out $y$:

$$
\hat{y} = \text{proj}(y|x) = x \frac{x'y}{||x||^2} = \frac{xx'}{||x||^2} y
$$

This leads to the definition of the projection matrix $P_x$.

::: {#def-projection-matrix}
### Projection Matrix onto a Single Vector
The matrix $P_x$ that projects any vector $y$ onto the line spanned by $x$ is defined as:

$$
P_x = \frac{xx'}{||x||^2}
$$

Using this matrix, the projection is simply:
$$
\hat{y} = P_x y
$$

If $x \in \mathbb{R}^p$, then $P_x$ is a $p \times p$ symmetric matrix.
:::

**Example: Projection in $\mathbb{R}^2$**

Let's apply these concepts to a concrete example.

::: {#exm-projection-r2}
### Numerical Projection
Let $y = (1, 3)'$ and $x = (1, 1)'$. We want to find the projection of $y$ onto $x$.

**Method 1: Using the Vector Formula**
First, calculate the inner products:
$$
x'y = 1(1) + 1(3) = 4
$$
$$
||x||^2 = 1^2 + 1^2 = 2
$$

Now, apply the formula:
$$
\hat{y} = \frac{4}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 2 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
$$

**Method 2: Using the Projection Matrix**
Construct the matrix $P_x$:
$$
P_x = \frac{1}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix}
$$

Multiply by $y$:
$$
\hat{y} = P_x y = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \end{pmatrix} = \begin{pmatrix} 0.5(1) + 0.5(3) \\ 0.5(1) + 0.5(3) \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
$$
:::

**Example: Projection onto the Mean Vector**

A very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.

::: {#exm-mean-projection}
### Projection onto the "One" Vector
Let $y = (y_1, \dots, y_n)'$ be a data vector.
Let $j_n = (1, 1, \dots, 1)'$ be a vector of all ones.

The projection of $y$ onto $j_n$ is:
$$
\text{proj}(y|j_n) = \frac{j_n' y}{||j_n||^2} j_n
$$

Calculating the components:
$$
j_n' y = \sum_{i=1}^n y_i \quad \text{(Sum of observations)}
$$
$$
||j_n||^2 = \sum_{i=1}^n 1^2 = n
$$

Substituting these back:
$$
\hat{y} = \frac{\sum y_i}{n} j_n = \bar{y} j_n = \begin{pmatrix} \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
$$

Thus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.
:::




**Pythagorean Theorem**

The Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.

::: {#thm-pythagorean}
### Pythagorean Theorem
If two vectors $x$ and $y$ are orthogonal (i.e., $x \perp y$ or $x'y = 0$), then the squared length of their sum is equal to the sum of their squared lengths:

$$
||x + y||^2 = ||x||^2 + ||y||^2
$$
:::

**Proof:**
We expand the squared norm using the inner product:

$$
\begin{aligned}
||x + y||^2 &= (x + y)' (x + y) \\
&= x'x + x'y + y'x + y'y \\
&= ||x||^2 + 2x'y + ||y||^2
\end{aligned}
$$

Since $x \perp y$, the inner product $x'y = 0$. Thus, the term $2x'y$ vanishes, leaving:

$$
||x + y||^2 = ||x||^2 + ||y||^2
$$

```{r}
#| label: extract-p16
#| include: false
extract_page_image(16, "page16_pythagoras.png", "1200x600+0+300")
```

![Pythagorean Theorem in Vector Space](lec1_images/page16_pythagoras.png){width=70%}

**Least Square Property**

One of the most important properties of the orthogonal projection is that it minimizes the distance between the vector $y$ and the subspace (or line) onto which it is projected.

::: {#thm-shortest-distance}
### Least Square Property
Let $\hat{y}$ be the projection of $y$ onto the line $L(x)$. For any other vector $y^*$ on the line $L(x)$, the distance from $y$ to $y^*$ is always greater than or equal to the distance from $y$ to $\hat{y}$.

$$
||y - y^*|| \ge ||y - \hat{y}||
$$
:::

**Proof:**
Since both $\hat{y}$ and $y^*$ lie on the line $L(x)$, their difference $(\hat{y} - y^*)$ also lies on $L(x)$.
From the definition of projection, the residual $(y - \hat{y})$ is orthogonal to the line $L(x)$. Therefore:

$$
(y - \hat{y}) \perp (\hat{y} - y^*)
$$

We can write the vector $(y - y^*)$ as:
$$
y - y^* = (y - \hat{y}) + (\hat{y} - y^*)
$$

Applying the Pythagorean Theorem:
$$
||y - y^*||^2 = ||y - \hat{y}||^2 + ||\hat{y} - y^*||^2
$$

Since $||\hat{y} - y^*||^2 \ge 0$, it follows that:
$$
||y - y^*||^2 \ge ||y - \hat{y}||^2
$$


## General Vector Space

We now generalize our discussion from lines to broader spaces.


::: {#def-vector-space}
### Vector Space
A set $V \subseteq \mathbb{R}^n$ is called a **Vector Space** if it is closed under vector addition and scalar multiplication:

1.  **Closed under Addition:** If $x_1 \in V$ and $x_2 \in V$, then $x_1 + x_2 \in V$.
2.  **Closed under Scalar Multiplication:** If $x \in V$, then $cx \in V$ for any scalar $c \in \mathbb{R}$.
:::

It follows that the zero vector $0$ must belong to any subspace (by choosing $c=0$).

**Spanned Vector Space**

The most common way to construct a vector space in linear models is by spanning it with a set of vectors.

::: {#def-spanned-space}
### Spanned Vector Space
Let $x_1, \dots, x_p$ be a set of vectors in $\mathbb{R}^n$. The space spanned by these vectors, denoted $L(x_1, \dots, x_p)$, is the set of all possible linear combinations of them:

$$
L(x_1, \dots, x_p) = \{ r \mid r = c_1 x_1 + \dots + c_p x_p, \text{ for } c_i \in \mathbb{R} \}
$$
:::


**Column Space and Row Space**

When vectors are arranged into a matrix, we define specific spaces based on their columns and rows.

::: {#def-column-space}
### Column Space
For a matrix $X = (x_1, \dots, x_p)$, the **Column Space**, denoted $Col(X)$, is the vector space spanned by its columns:

$$
Col(X) = L(x_1, \dots, x_p)
$$
:::

::: {#def-row-space}
### Row Space
The **Row Space**, denoted $Row(X)$, is the vector space spanned by the rows of the matrix $X$.
:::




**Linear Independence and Rank**

Not all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.

::: {#def-linear-independence}
### Linear Independence
A set of vectors $x_1, \dots, x_p$ is said to be **Linearly Independent** if the only solution to the linear combination equation equal to zero is the trivial solution:

$$
\sum_{i=1}^p c_i x_i = 0 \implies c_1 = c_2 = \dots = c_p = 0
$$

If there exist non-zero $c_i$'s such that sum is zero, the vectors are **Linearly Dependent**.
:::

## Rank of Matrices and Dim of Vector Space
::: {#def-rank}
#### Rank
The **Rank** of a matrix $X$, denoted $\text{Rank}(X)$, is the maximum number of linearly independent columns in $X$. This is equivalent to the dimension of the column space:

$$
\text{Rank}(X) = \text{Dim}(Col(X))
$$
:::

#### Properties of Rank

There are several fundamental properties regarding the rank of a matrix.

::: {#thm-rank-properties}
#### Properties of Rank
1.  **Row Rank equals Column Rank:** The dimension of the column space is equal to the dimension of the row space.
    $$
    \text{Dim}(Col(X)) = \text{Dim}(Row(X)) \implies \text{Rank}(X) = \text{Rank}(X')
    $$

2.  **Bounds:** For an $n \times p$ matrix $X$:
    $$
    \text{Rank}(X) \le \min(n, p)
    $$
:::


::: {.proof}
  Let $X$ be an $n \times p$ matrix. Let $r$ be the row rank of $X$.
  This means the dimension of the row space is $r$. Let $u_1, \dots, u_r$ be a basis for the row space of $X$ (these are row vectors).
  Since every row of $X$ is in the row space, each row $x_{i.}$ can be written as a linear combination of the basis vectors:
  $$
  x_{i.} = c_{i1}u_1 + c_{i2}u_2 + \dots + c_{ir}u_r \quad \text{for } i=1,\dots,n
  $$
  
  We can write this in matrix notation as:
  $$
  X = C U
  $$
  where $C$ is an $n \times r$ matrix of coefficients $c_{ij}$, and $U$ is an $r \times p$ matrix with rows $u_1, \dots, u_r$.
  
  Now consider the columns of $X$. Since $X = CU$, the columns of $X$ are linear combinations of the columns of $C$.
  Let $c^{(j)}$ be the $j$-th column of $C$. The columns of $X$ lie in the space spanned by $\{c^{(1)}, \dots, c^{(r)}\}$.
  Thus, the column space of $X$, $Col(X)$, is a subspace of the column space of $C$.
  $$
  \text{Dim}(Col(X)) \le \text{Dim}(Col(C)) \le r
  $$
  The dimension of the column space of $C$ is at most $r$ (since $C$ has only $r$ columns).
  Therefore, Column Rank $\le$ Row Rank.
  
  Applying the same logic to $X'$, we get Row Rank $\le$ Column Rank.
  Combining these inequalities gives: **Row Rank = Column Rank**.
:::

::: {.example}
**Example: 2x3 Matrix**

Consider the following $2 \times 3$ matrix:
$$
X = \begin{pmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \end{pmatrix}
$$

**Row Rank:**
The rows are $r_1 = (1, 0, 1)$ and $r_2 = (0, 1, 1)$. Neither is a multiple of the other, so they are linearly independent.
$$
\text{Row Rank} = 2
$$

**Column Rank:**
The columns are $c_1 = \binom{1}{0}$, $c_2 = \binom{0}{1}$, and $c_3 = \binom{1}{1}$.
Notice that $c_3 = c_1 + c_2$. The third column is dependent on the first two.
However, $c_1$ and $c_2$ are independent (standard basis vectors).
$$
\text{Column Rank} = 2
$$

Thus, Rank(Row) = Rank(Col) = 2.
:::


**Orthogonality to a Subspace**

We can extend the concept of orthogonality from single vectors to entire subspaces.

::: {#def-orth-subspace}
### Orthogonality to a Subspace
A vector $y$ is orthogonal to a subspace $V$ (denoted $y \perp V$) if $y$ is orthogonal to **every** vector $x$ in $V$.

$$
y \perp V \iff y'x = 0 \quad \forall x \in V
$$
:::

::: {#def-orthogonal-complement}
### Orthogonal Complement
The set of all vectors that are orthogonal to a subspace $V$ is called the **Orthogonal Complement** of $V$, denoted $V^\perp$.

$$
V^\perp = \{ y \in \mathbb{R}^n \mid y \perp V \}
$$
:::

**Kernel (Null Space) and Image**

For a matrix transformation defined by $X$, we define two key spaces: the Image (Column Space) and the Kernel (Null Space).

::: {#def-image-kernel}
### Image and Kernel
1.  **Image (Column Space):** The set of all possible outputs.
    $$
    \text{Im}(X) = Col(X) = \{ X\beta \mid \beta \in \mathbb{R}^p \}
    $$

2.  **Kernel (Null Space):** The set of all inputs mapped to the zero vector.
    $$
    \text{Ker}(X) = \{ \beta \in \mathbb{R}^p \mid X\beta = 0 \}
    $$
:::

::: {#thm-kernel-rowspace}
### Relationship between Kernel and Row Space
The kernel of $X$ is the orthogonal complement of the row space of $X$:

$$
\text{Ker}(X) = [Row(X)]^\perp
$$
:::

**Nullity Theorem**

There is a fundamental relationship between the dimensions of these spaces.

::: {#thm-nullity}
### Rank-Nullity Theorem
For an $n \times p$ matrix $X$:

$$
\text{Rank}(X) + \text{Nullity}(X) = p
$$

Where $\text{Nullity}(X) = \text{Dim}(\text{Ker}(X))$.
:::

-



**Rank Inequalities**

Understanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.

::: {#thm-rank-product}
### Rank of a Matrix Product
Let $X$ be an $n \times p$ matrix and $Z$ be a $p \times k$ matrix. The rank of their product $XZ$ is bounded by the rank of the individual matrices:

$$
\text{Rank}(XZ) \le \min(\text{Rank}(X), \text{Rank}(Z))
$$
:::

**Proof:**
The columns of $XZ$ are linear combinations of the columns of $X$. Thus, the column space of $XZ$ is a subspace of the column space of $X$:
$$
Col(XZ) \subseteq Col(X) \implies \text{Rank}(XZ) \le \text{Rank}(X)
$$
Similarly, the rows of $XZ$ are linear combinations of the rows of $Z$. Thus, the row space of $XZ$ is a subspace of the row space of $Z$:
$$
Row(XZ) \subseteq Row(Z) \implies \text{Rank}(XZ) \le \text{Rank}(Z)
$$

**Rank and Invertible Matrices**

Multiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.

::: {#thm-rank-invertible}
### Rank with Non-Singular Multiplication
Let $A$ be an $n \times n$ invertible matrix (i.e., $\text{Rank}(A) = n$) and $X$ be an $n \times p$ matrix. Then:

$$
\text{Rank}(AX) = \text{Rank}(X)
$$

Similarly, if $B$ is a $p \times p$ invertible matrix, then:

$$
\text{Rank}(XB) = \text{Rank}(X)
$$
:::

**Proof:**
From the previous theorem, we know $\text{Rank}(AX) \le \text{Rank}(X)$.
Since $A$ is invertible, we can write $X = A^{-1}(AX)$. Applying the theorem again:
$$
\text{Rank}(X) = \text{Rank}(A^{-1}(AX)) \le \text{Rank}(AX)
$$
Thus, $\text{Rank}(AX) = \text{Rank}(X)$.



**Rank of $X'X$ and $XX'$**

The matrix $X'X$ (the Gram matrix) appears in the normal equations for least squares ($X'X\beta = X'y$). Its properties are closely tied to $X$.

::: {#thm-rank-gram}
### Rank of Gram Matrix
For any real matrix $X$, the rank of $X'X$ and $XX'$ is the same as the rank of $X$ itself:

$$
\text{Rank}(X'X) = \text{Rank}(X)
$$
$$
\text{Rank}(XX') = \text{Rank}(X)
$$
:::

**Proof:**
We first show that the null space (kernel) of $X$ is the same as the null space of $X'X$.
If $v \in \text{Ker}(X)$, then $Xv = 0 \implies X'Xv = 0 \implies v \in \text{Ker}(X'X)$.
Conversely, if $v \in \text{Ker}(X'X)$, then $X'Xv = 0$. Multiply by $v'$:
$$
v'X'Xv = 0 \implies (Xv)'(Xv) = 0 \implies ||Xv||^2 = 0 \implies Xv = 0
$$
So $\text{Ker}(X) = \text{Ker}(X'X)$.
By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.


**Column Space of $XX'$**

Beyond just the rank, the column spaces themselves are related.

::: {#thm-colspace-gram}
### Column Space Equivalence
The column space of $XX'$ is identical to the column space of $X$:

$$
Col(XX') = Col(X)
$$
:::

**Implication:**
This property ensures that for any $y$, the projection of $y$ onto $Col(X)$ lies in the same space as the projection onto $Col(XX')$. This is vital for the existence of solutions in generalized least squares.



## Projection via Orthonormal Basis ($Q$)


### Orthonomal Basis

Before discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.

::: {#def-basis}
#### Basis
A set of vectors $\{x_1, \dots, x_k\}$ is a **Basis** for a vector space $V$ if:

1.  The vectors span the space: $V = L(x_1, \dots, x_k)$.
2.  The vectors are linearly independent.
:::

The number of vectors in a basis is unique and is defined as the **Dimension** of $V$.


Calculations become significantly simpler if we choose a basis with special geometric properties.

::: {#def-orthonormal-basis}
#### Orthonormal Basis
A basis $\{q_1, \dots, q_k\}$ is called an **Orthonormal Basis** if:

1.  **Orthogonal:** Each pair of vectors is perpendicular.
    $$
    q_i'q_j = 0 \quad \text{for } i \ne j
    $$
2.  **Normalized:** Each vector has unit length.
    $$
    ||q_i||^2 = q_i'q_i = 1
    $$

Combining these, we write $q_i'q_j = \delta_{ij}$ (Kronecker delta).
:::



We now generalize the projection problem. Instead of projecting $y$ onto a single line, we project it onto a subspace $V$ of dimension $k$.

If we have an orthonormal basis $\{q_1, \dots, q_k\}$ for $V$, the projection $\hat{y}$ is simply the sum of the projections onto the individual basis vectors.

::: {#def-proj-orthonormal}
#### Projection Defined with Orthonormal Basis
The projection of $y$ onto the subspace $V = L(q_1, \dots, q_k)$ is:

$$
\hat{y} = \sum_{i=1}^k \text{proj}(y|q_i) = \sum_{i=1}^k (q_i'y) q_i
$$

Since the basis vectors are normalized, we do not need to divide by $||q_i||^2$.
:::




::: {#def-projection-matrix}
#### Definition of Projection onto a Subspace $V$
Let $V$ be a subspace of $\mathbb{R}^n$. For any vector $y \in \mathbb{R}^n$, there exists a **unique** vector $\hat{y} \in V$ such that the residual is orthogonal to the subspace:

$$
(y - \hat{y}) \perp V
$$

Equivalently:
$$
\langle y - \hat{y}, v \rangle = 0 \quad \forall v \in V
$$
:::

::: {#thm-orthonormal-basis-proj}
#### Projection via Orthonormal Basis
Let $\{q_1, \dots, q_k\}$ be an orthonormal basis for the subspace $V \subseteq \mathbb{R}^n$. The vector defined by the sum of individual projections:
$$
\hat{y} = \sum_{i=1}^k \langle y, q_i \rangle q_i
$$
is indeed the orthogonal projection of $y$ onto $V$. That is, it satisfies $(y - \hat{y}) \perp V$.
:::

::: {.proof}
To prove this, we must check two conditions:

1.  **$\hat{y} \in V$**: This is immediate because $\hat{y}$ is a linear combination of the basis vectors $\{q_1, \dots, q_k\}$.

2.  **$(y - \hat{y}) \perp V$**: It suffices to show that the error vector $e = y - \hat{y}$ is orthogonal to every basis vector $q_j$ (for $j = 1, \dots, k$).

    Let's calculate the inner product $\langle y - \hat{y}, q_j \rangle$:
    $$
    \begin{aligned}
    \langle y - \hat{y}, q_j \rangle &= \langle y, q_j \rangle - \langle \hat{y}, q_j \rangle \\
    &= \langle y, q_j \rangle - \left\langle \sum_{i=1}^k \langle y, q_i \rangle q_i, q_j \right\rangle \\
    &= \langle y, q_j \rangle - \sum_{i=1}^k \langle y, q_i \rangle \underbrace{\langle q_i, q_j \rangle}_{\delta_{ij}}
    \end{aligned}
    $$
    
    Since the basis is orthonormal, $\langle q_i, q_j \rangle$ is 1 if $i=j$ and 0 otherwise. Thus, the summation collapses to a single term where $i=j$:
    $$
    \begin{aligned}
    \langle y - \hat{y}, q_j \rangle &= \langle y, q_j \rangle - \langle y, q_j \rangle \cdot 1 \\
    &= 0
    \end{aligned}
    $$
    
    Since $(y - \hat{y})$ is orthogonal to every basis vector $q_j$, it is orthogonal to the entire subspace $V$. Thus, $\hat{y}$ is the unique orthogonal projection.
:::

### Projection Matrix via Orthonomal Basis ($Q$)

**Matrix Form with Orthonormal Basis**

We can express the summation formula for $\hat{y}$ compactly using matrix notation.

Let $Q$ be an $n \times k$ matrix whose columns are the orthonormal basis vectors $q_1, \dots, q_k$.
$$
Q = \begin{pmatrix} q_1 & q_2 & \dots & q_k \end{pmatrix}
$$

Properties of $Q$:
* $Q'Q = I_k$ (Identity matrix of size $k \times k$).
* $QQ'$ is **not** necessarily $I_n$ (unless $k=n$).




::: {#def-proj-matrix-orthonormal}
#### Projection Matrix in Terms of $Q$
The projection $\hat{y}$ can be written as:

$$
\hat{y} = \begin{pmatrix} q_1 & \dots & q_k \end{pmatrix} \begin{pmatrix} q_1'y \\ \vdots \\ q_k'y \end{pmatrix} = Q (Q'y) = (QQ') y
$$

Thus, the projection matrix $P$ onto the subspace $V$ is:
$$
P = QQ'
$$
:::



**Properties of Projection Matrices**

We have defined the projection matrix as $P = X(X'X)^{-1}X'$ (or $P=QQ'$ for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.

::: {#thm-projection-properties}
#### Symmeticity and Idempotence
A square matrix $P$ represents an orthogonal projection onto some subspace if and only if it satisfies:

1.  **Idempotence:** $P^2 = P$ (Applying the projection twice is the same as applying it once).
2.  **Symmetry:** $P' = P$.
:::

:::{.proof}
If $\hat{y} = Py$ is already in the subspace $Col(X)$, then projecting it again should not change it.
$$
P(Py) = Py \implies P^2 y = Py \quad \forall y
$$
Thus, $P^2 = P$.
:::

**Example: ANOVA (Analysis of Variance)**

One of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.

::: {#exm-anova-projection}
#### Finding Projection for One-way ANOVA
Consider a one-way ANOVA model with $k$ groups:
$$
y_{ij} = \mu_i + \epsilon_{ij}
$$
where $i \in \{1, \dots, k\}$ represents the group and $j \in \{1, \dots, n_i\}$ represents the observation within the group. Let $N = \sum_{i=1}^k n_i$ be the total number of observations.

**1. Matrix Definitions**
We define the data vector $y$ and the design matrix $X$ as follows:

* **Data Vector ($y$):** An $N \times 1$ vector containing all observations stacked by group:
    $$
    y = \begin{pmatrix} y_{11} \\ \vdots \\ y_{1n_1} \\ y_{21} \\ \vdots \\ y_{kn_k} \end{pmatrix}
    $$

* **Design Matrix ($X$):** An $N \times k$ matrix constructed from $k$ column vectors, $X = (x_1, x_2, \dots, x_k)$. Each vector $x_g$ is an **indicator variable** (dummy variable) for group $g$:
    $$
    x_g = \begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix} \quad \leftarrow \text{Entries are 1 if observation belongs to group } g
    $$

**2. Orthogonality**
These column vectors $x_1, \dots, x_k$ are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero:
$$
\langle x_g, x_h \rangle = 0 \quad \text{for } g \neq h
$$
This allows us to find the projection onto the column space of $X$ by simply summing the projections onto each column individually.

**3. Calculating Individual Projections**
For a specific group vector $x_g$, the projection is:
$$
\text{proj}(y|x_g) = \frac{\langle y, x_g \rangle}{\langle x_g, x_g \rangle} x_g
$$

We calculate the two scalar terms:

* **Denominator ($\langle x_g, x_g \rangle$):** The sum of squared elements of $x_g$. Since $x_g$ contains $n_g$ ones and zeros elsewhere:
    $$
    \langle x_g, x_g \rangle = \sum \mathbb{1}_{\{i=g\}}^2 = n_g
    $$

* **Numerator ($\langle y, x_g \rangle$):** The dot product sums only the $y$ values belonging to group $g$:
    $$
    \langle y, x_g \rangle = \sum_{i,j} y_{ij} \cdot \mathbb{1}_{\{i=g\}} = \sum_{j=1}^{n_g} y_{gj} = y_{g.} \quad (\text{Group Total})
    $$

**4. The Resulting Projection**
Substituting these back into the formula gives the coefficient for the vector $x_g$:
$$
\text{proj}(y|x_g) = \frac{y_{g.}}{n_g} x_g = \bar{y}_{g.} x_g
$$

The total projection $\hat{y}$ is the sum over all groups:
$$
\hat{y} = \sum_{g=1}^k \bar{y}_{g.} x_g
$$
This confirms that the fitted value for any specific observation $y_{ij}$ is simply its group mean $\bar{y}_{i.}$.
:::


### Gram-Schmidt Process

To use the simplified formula $P = QQ'$, we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.

::: {.algorithm}
**Gram-Schmidt Process**
Given linearly independent vectors $x_1, \dots, x_p$:

1.  **Step 1:** Normalize the first vector.
    $$
    q_1 = \frac{x_1}{||x_1||}
    $$

2.  **Step 2:** Project $x_2$ onto $q_1$ and subtract it to find the orthogonal component.
    $$
    v_2 = x_2 - (x_2'q_1)q_1
    $$
    Then normalize:
    $$
    q_2 = \frac{v_2}{||v_2||}
    $$

3.  **Step k:** Subtract the projections onto all previous $q$ vectors.
    $$
    v_k = x_k - \sum_{j=1}^{k-1} (x_k'q_j)q_j
    $$
    $$
    q_k = \frac{v_k}{||v_k||}
    $$
:::

```{r}
#| label: setup-python-env
#| include: false
#| output: false

library(reticulate)

# 1. Ensure Python is initialized
try(py_config(), silent = TRUE)

# 2. Define requirements
required_py_modules <- c("matplotlib", "numpy")

# 3. Filter to only missing modules
# We check if they are importable. If not, they are "missing".
missing_modules <- required_py_modules[!sapply(required_py_modules, py_module_available)]

# 4. Install only if there are missing modules
if (length(missing_modules) > 0) {
  message("Installing missing Python modules: ", paste(missing_modules, collapse = ", "))
  # Pass the vector directly to py_install to avoid repetitive environment checks
  py_install(missing_modules)
}
```

```{python}
#| label: fig-gram-schmidt-python
#| echo: false
#| results: hide
#| fig-cap: "Gram-Schmidt Process: Projecting $x_2$ onto $x_1$"

import matplotlib.pyplot as plt
import numpy as np

# 1. Setup the plot
fig, ax = plt.subplots(figsize=(6, 4))
ax.set_aspect('equal')
ax.set_xlim(-0.5, 3.5)
ax.set_ylim(-0.5, 2.5)
ax.axis('off')  # Turn off the box frame and axis numbers

# Optional: Draw a light grid for reference
ax.grid(True, linestyle=':', alpha=0.3)

# 2. Define Coordinates
o = np.array([0, 0])
x1 = np.array([3, 0])
x2 = np.array([2, 2])
proj = np.array([2, 0]) # Projection of x2 on x1

# 3. Helper function to draw vectors
def draw_vector(start, end, color, label=None, label_pos='end'):
    ax.annotate('', xy=end, xytext=start,
                arrowprops=dict(facecolor=color, edgecolor=color, width=1.5, headwidth=8))
    
    if label:
        if label_pos == 'end':
            # Place label at the tip
            ax.text(end[0], end[1], label, fontsize=12, color=color, 
                    ha='left', va='bottom', weight='bold')
        elif label_pos == 'mid':
            # Place label in the middle (good for error vectors)
            mid = (start + end) / 2
            ax.text(mid[0] + 0.1, mid[1], label, fontsize=12, color=color, 
                    weight='bold')
        elif label_pos == 'below':
            ax.text(end[0], end[1] - 0.3, label, fontsize=12, color=color, 
                    ha='center', weight='bold')

# 4. Draw the Main Vectors
# x1 (Blue)
draw_vector(o, x1, 'blue', r'$\mathbf{x}_1$', 'below')

# x2 (Blue)
draw_vector(o, x2, 'blue', r'$\mathbf{x}_2$', 'end')

# Projection Line (Dashed)
ax.plot([o[0], proj[0]], [o[1], proj[1]], linestyle='--', color='blue', alpha=0.5)
ax.text(2, -0.3, r'$\hat{\mathbf{x}}_2$', color='blue', fontsize=12, ha='center')

# Error Vector e2 (Red) - from Projection UP to x2
draw_vector(proj, x2, 'red', r'$\mathbf{e}_2$', 'mid')

# Dashed line completing the triangle (visual aid)
ax.plot([0, 2], [0, 2], linestyle=':', color='gray', alpha=0.3)

# 5. Draw Unit Vectors (q) - Thicker Orange
# q1 (on x-axis)
draw_vector(o, [1, 0], 'orange')
ax.text(1, -0.3, r'$\mathbf{q}_1$', color='orange', fontsize=12, weight='bold', ha='center')

# q2 (parallel to e2, starting from projection for clarity)
draw_vector(proj, [2, 1], 'orange')
ax.text(2.1, 1, r'$\mathbf{q}_2$', color='orange', fontsize=12, weight='bold')

# 6. Add Right Angle Symbol
# Draw a small square at the projection point
size = 0.2
ax.plot([proj[0]-size, proj[0]-size, proj[0]], 
        [proj[1], proj[1]+size, proj[1]+size], 'k-', linewidth=0.8)

# 7. Add Title Text
ax.text(0, 2.2, 'Gram-Schmidt Process', fontsize=14, style='italic')
ax.text(0, 2.0, r'$\mathbb{R}^2$ Projection', fontsize=10, color='gray')

plt.show()
```

This process leads to the **QR Decomposition** of a matrix: $X = QR$, where $Q$ is orthogonal and $R$ is upper triangular.


## Hat Matrix (Projection Matrix via $X$)



### Norm Equations

Let $X = (x_1, \dots, x_p)$ be an $n \times p$ matrix, where each column $x_j$ is a predictor vector.

We want to project the target vector $y$ onto the column space $C(X)$. This is equivalent to finding a coefficient vector $\beta \in \mathbb{R}^p$ such that the error vector (residual) is orthogonal to the entire subspace $C(X)$.

$$
y - X\beta \perp C(X)
$$

Since the columns of $X$ span the subspace, the residual must be orthogonal to **every** column vector $x_j$ individually:

$$
y - X\beta \perp x_j \quad \text{for } j = 1, \dots, p
$$

Writing this geometric condition as an algebraic dot product (where $x_j'$ denotes the transpose):

$$
x_j'(y - X\beta) = 0 \quad \text{for each } j
$$

We can stack these $p$ separate linear equations into a single matrix equation. Since the rows of $X'$ are the columns of $X$, this becomes:

$$
\begin{pmatrix} x_1' \\ \vdots \\ x_p' \end{pmatrix} (y - X\beta) = \mathbf{0}
\implies X'(y - X\beta) = 0
$$

Finally, we distribute the matrix transpose and rearrange terms to solve for $\beta$:

$$
\begin{aligned}
X'y - X'X\beta &= 0 \\
X'X\beta &= X'y
\end{aligned}
$$

This system is known as the **Normal Equations**.

::: {#thm-least-squares-estimator}
### Least Squares Estimator
If $X'X$ is invertible (i.e., $X$ has full column rank), the unique solution for $\beta$ is:

$$
\hat{\beta} = (X'X)^{-1}X'y
$$
:::

### Hat Matrix

Substituting the estimator $\hat{\beta}$ back into the equation for $\hat{y}$ gives us the projection matrix.

::: {#def-projection-matrix-general}
### Hat Matrix
The projection of $y$ onto $Col(X)$ is given by:

$$
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y
$$

Thus, the hat matrix $P$ is defined as:

$$
P = X(X'X)^{-1}X'
$$
:::

### Equivalence of Hat Matrix and $QQ'$

If we use the QR decomposition such that $X = QR$, where the columns of $Q$ form an orthonormal basis for $Col(X)$, the formula simplifies significantly.

Recall that for orthonormal columns, $Q'Q = I$. Substituting $X=QR$ into the general formula:

$$
\begin{aligned}
P &= QR((QR)'(QR))^{-1}(QR)' \\
  &= QR(R'Q'QR)^{-1}R'Q' \\
  &= QR(R' \underbrace{Q'Q}_{I} R)^{-1}R'Q' \\
  &= QR(R'R)^{-1}R'Q' \\
  &= QR R^{-1} (R')^{-1} R' Q' \\
  &= Q \underbrace{R R^{-1}}_{I} \underbrace{(R')^{-1} R'}_{I} Q' \\
  &= Q Q'
\end{aligned}
$$

This confirms that $P = QQ'$ is consistent with the general formula $P = X(X'X)^{-1}X'$.

### Properties of Hat Matrix

We revisit the properties of projection matrices in this general context.

::: {#thm-projection-properties-revisited}
### Properties of P
The matrix $P = X(X'X)^{-1}X'$ satisfies:

1.  **Symmetric:** $P' = P$
2.  **Idempotent:** $P^2 = P$
3.  **Trace:** The trace of a projection matrix equals the dimension of the subspace it projects onto.
    $$
    \text{tr}(P) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_p) = p
    $$
:::

**Projection onto Complement**

As before, the projection onto the orthogonal complement (the residual maker matrix) is $M = I - P$.

::: {#def-m-matrix}
### Residual Maker Matrix M
$$
M = I - X(X'X)^{-1}X'
$$

This matrix projects $y$ onto the null space of $X'$ (the orthogonal complement of the column space of $X$).
:::


## General Projection Matrices onto Nested Subspaces

### Nested Models and Subspaces

In hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.

::: {#def-nested-models}
#### Nested Models
Consider two models:
1.  **Reduced Model ($M_0$):** $y \in Col(X_0)$
2.  **Full Model ($M_1$):** $y \in Col(X_1)$

We say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model:
$$
Col(X_0) \subseteq Col(X_1)
$$
:::

Usually, $X_1$ is constructed by adding columns to $X_0$: $X_1 = [X_0, X_{new}]$.



### Projections onto Nested Subspaces

Let $P_0$ be the projection matrix onto $Col(X_0)$ and $P_1$ be the projection matrix onto $Col(X_1)$. Since $Col(X_0) \subseteq Col(X_1)$, we have important relationships between these matrices.

::: {#thm-nested-projections}
### Composition of Projections
If $Col(P_0) \subseteq Col(P_1)$, then:

1.  $P_1 P_0 = P_0$ (Projecting onto the small space, then the large space, keeps you in the small space).
2.  $P_0 P_1 = P_0$ (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).
:::

**Difference of Projections**

The difference between the two projection matrices, $P_1 - P_0$, is itself a projection matrix.

::: {#thm-diff-projection}
### Difference Projection
The matrix $P_{\Delta} = P_1 - P_0$ is an orthogonal projection matrix onto the subspace $Col(X_1) \cap Col(X_0)^\perp$. This subspace represents the "extra" information in the full model that is orthogonal to the reduced model.

**Properties:**

  1.  **Symmetric:** $(P_1 - P_0)' = P_1 - P_0$.
  2.  **Idempotent:** $(P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0$.
  3.  **Orthogonality:** $(P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0$.
:::

### Decomposition of Projections and their Sum Squares

::: {#thm-nested-decomposition}
#### Orthogonal Decomposition of Nested Models

Let $M_0 \subset M_1$ be two nested linear models with corresponding design matrices $X_0$ and $X_1$ such that $C(X_0) \subset C(X_1)$. Let $P_0$ and $P_1$ be the orthogonal projection matrices onto $C(X_0)$ and $C(X_1)$ respectively.

For any observation vector $y$, we have the decomposition:
$$
y = \underbrace{P_0 y}_{\hat{y}_0} + \underbrace{(P_1 - P_0) y}_{\hat{y}_1 - \hat{y}_0} + \underbrace{(I - P_1) y}_{y - \hat{y}_1}
$$

The three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm:
$$
\|y\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2 + \|y - \hat{y}_1\|^2
$$
:::

::: {.proof}

**1. Definitions**
We define the three components as vectors $v_1, v_2, v_3$:

* $v_1 = \hat{y}_0 = P_0 y$.
* $v_2 = \hat{y}_1 - \hat{y}_0 = (P_1 - P_0)y$.
* $v_3 = y - \hat{y}_1 = (I - P_1)y$.
    * **Note:** Since $P_1$ projects onto $C(X_1)$, the matrix $(I - P_1)$ projects onto the **orthogonal complement** $C(X_1)^\perp$. Thus, $v_3 \in \text{Col}(I - P_1)$.

Note that since $C(X_0) \subset C(X_1)$, we have the property $P_1 P_0 = P_0 P_1 = P_0$. (Projecting onto the smaller subspace $M_0$ is unchanged if we first project onto the enclosing subspace $M_1$).

**2. Orthogonality of $v_1$ and $v_2$**
We check the inner product $\langle v_1, v_2 \rangle = v_1' v_2$:
$$
\begin{aligned}
v_1' v_2 &= (P_0 y)' (P_1 - P_0) y \\
&= y' P_0' (P_1 - P_0) y \\
&= y' (P_0 P_1 - P_0^2) y \quad (\text{Since } P_0 \text{ is symmetric}) \\
&= y' (P_0 - P_0) y \quad (\text{Since } P_0 P_1 = P_0 \text{ and } P_0^2 = P_0) \\
&= 0
\end{aligned}
$$

**3. Orthogonality of $(v_1 + v_2)$ and $v_3$**
Note that $v_1 + v_2 = P_1 y = \hat{y}_1$. We check if the total fit $\hat{y}_1$ is orthogonal to the residual $v_3$:
$$
\begin{aligned}
\hat{y}_1' v_3 &= (P_1 y)' (I - P_1) y \\
&= y' P_1 (I - P_1) y \\
&= y' (P_1 - P_1^2) y \\
&= y' (P_1 - P_1) y \\
&= 0
\end{aligned}
$$
Since $\hat{y}_1$ is orthogonal to $v_3$, and $\hat{y}_0$ is a component of $\hat{y}_1$, it follows that all three pieces are mutually orthogonal.

**4. Sum of Squares**
By the Pythagorean theorem applied twice to these orthogonal vectors, the equality of squared norms follows immediately.
:::


```{python}
#| label: fig-anova-decomposition-v2
#| fig-cap: "Illustration of Projections onto Nested Subspaces"
#| echo: false
#| results: hide

import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# 1. Setup the 3D Plot
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# 2. Define the key vectors
o = np.array([0, 0, 0])
y0 = np.array([4, 0, 0])   # y_hat_0
y1 = np.array([4, 3, 0])   # y_hat_1
# Increased Z height for better visibility (from 2 to 5)
y = np.array([4, 3, 5])    # y

# 3. Draw Subspaces (Visual Context)
# Plane C(P1)
xx, yy = np.meshgrid(np.linspace(-1, 6, 10), np.linspace(-1, 6, 10))
z_plane = np.zeros_like(xx)
ax.plot_surface(xx, yy, z_plane, alpha=0.1, color='blue')
ax.text(6, 6, 0, r'$C(P_1)$', color='blue')

# Line C(P0)
ax.plot([-1, 6], [0, 0], [0, 0], 'k--', alpha=0.3)
ax.text(6, 0, 0, r'$C(P_0)$', color='black')

# 4. Helper to draw heavier arrows
def draw_arrow(start, end, color, label, label_pos='mid', offset=(0,0,0), lw=6, style='solid'):
    vec = end - start
    
    if style == 'dotted':
        # Matplotlib 3D quiver doesn't support linestyle. 
        # Workaround: Draw shaft with plot() and head with quiver()
        ax.plot([start[0], end[0]], [start[1], end[1]], [start[2], end[2]], 
                color=color, linestyle=':', linewidth=lw)
        # Draw just the head using a 0-length quiver at the end? 
        # Easier: Draw a short quiver segment at the end to simulate the head
        # Normalize vector for head direction
        length = np.linalg.norm(vec)
        norm_vec = vec / length
        # Draw a small head at the end
        ax.quiver(end[0]-norm_vec[0]*0.1, end[1]-norm_vec[1]*0.1, end[2]-norm_vec[2]*0.1, 
                  norm_vec[0], norm_vec[1], norm_vec[2], 
                  color=color, arrow_length_ratio=0.5, length=0.5, linewidth=lw)
    else:
        ax.quiver(start[0], start[1], start[2], 
                  vec[0], vec[1], vec[2], 
                  color=color, arrow_length_ratio=0.08, linewidth=lw)
    
    # Label positioning logic
    if label_pos == 'mid':
        pos = (start + end) / 2
    elif label_pos == 'end':
        pos = end
        
    ax.text(pos[0]+offset[0], pos[1]+offset[1], pos[2]+offset[2], 
            label, color=color, fontsize=14, fontweight='bold')

# 5. Draw the Decomposition Chain
# y_hat_0 (Green)
draw_arrow(o, y0, 'green', r'$\hat{y}_0$', offset=(0,-0.8,0))

# y_hat_1 - y_hat_0 (Orange)
draw_arrow(y0, y1, 'orange', r'$\hat{y}_1 - \hat{y}_0$', offset=(0.2,0,0))

# y - y_hat_1 (Red)
draw_arrow(y1, y, 'red', r'$y - \hat{y}_1$', offset=(0.2,0,0))

# 6. Draw the extra residual arrow: y - y_hat_0
draw_arrow(y0, y, 'purple', r'$y - \hat{y}_0$', label_pos='mid', offset=(-1, 0.5, 0.5))

# 7. Draw the total vector y (Heavy Black)
draw_arrow(o, y, 'black', r'$y$', label_pos='end', offset=(0,0,0.3), lw=6)

# 8. NEW: Draw y_hat_1 (Dotted Blue)
# We draw this from origin to y1
draw_arrow(o, y1, 'blue', r'$\hat{y}_1$', label_pos='mid', offset=(-0.5, -0.5, 0), style='dotted')

# 9. Add projection drop-lines (dashed) for visual clarity
ax.plot([y[0], y[0]], [y[1], y[1]], [0, y[2]], 'k:', alpha=0.4) # y down to y1
ax.plot([y1[0], y1[0]], [0, y1[1]], [0, 0], 'k:', alpha=0.4)    # y1 over to y0

# 10. Formatting
ax.set_xlim(-1, 6)
ax.set_ylim(-1, 6)
ax.set_zlim(0, 6) # Increased limit to accommodate higher y
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.view_init(elev=20, azim=-25) # Adjusted view angle to see the height

plt.show()
```


::: {#exm-anova-ss}
#### Geometric Interpretation of One-way ANOVA

We apply the **Nested Model Theorem** ($M_0 \subset M_1$) to the One-way ANOVA setting with $k$ groups and $n_i$ observations per group.

**1. The Data Vector**
We stack all observations into a single $N \times 1$ vector ($N = \sum n_i$):
$$
y = \begin{pmatrix} y_{11} \\ \vdots \\ y_{1 n_1} \\ \hline \vdots \\ \hline y_{k1} \\ \vdots \\ y_{k n_k} \end{pmatrix}
$$

**2. The Projections (as Vectors)**

* **$\hat{y}_0$ (Null Model Projection):**
    Projecting onto the intercept vector $\mathbf{1}$ replaces every observation with the **Grand Mean** $\bar{y}_{..}$.
    $$
    \hat{y}_0 = P_0 y = \begin{pmatrix} \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \\ \hline \vdots \\ \hline \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \end{pmatrix}
    $$

* **$\hat{y}_1$ (Full Model Projection):**
    Projecting onto the group indicators replaces observations in group $i$ with the **Group Mean** $\bar{y}_{i.}$.
    $$
    \hat{y}_1 = P_1 y = \begin{pmatrix} \bar{y}_{1.} \\ \vdots \\ \bar{y}_{1.} \\ \hline \vdots \\ \hline \bar{y}_{k.} \\ \vdots \\ \bar{y}_{k.} \end{pmatrix}
    $$

**3. The Decomposition**
The total deviation vector $y - \hat{y}_0$ splits into two orthogonal vector components:

* **Model Improvement Vector ($\hat{y}_1 - \hat{y}_0$):**
    The difference between group means and the grand mean.
    $$
    \hat{y}_1 - \hat{y}_0 = \begin{pmatrix} \bar{y}_{1.} - \bar{y}_{..} \\ \vdots \\ \bar{y}_{1.} - \bar{y}_{..} \\ \hline \vdots \\ \hline \bar{y}_{k.} - \bar{y}_{..} \\ \vdots \\ \bar{y}_{k.} - \bar{y}_{..} \end{pmatrix}
    $$
    *Squared Norm:* $RSS_0 - RSS_1 = \sum_{i} n_i (\bar{y}_{i.} - \bar{y}_{..})^2$ ($SS_{between}$)

* **Residual Vector ($y - \hat{y}_1$):**
    The difference between individual observations and their specific group mean.
    $$
    y - \hat{y}_1 = \begin{pmatrix} y_{11} - \bar{y}_{1.} \\ \vdots \\ y_{1 n_1} - \bar{y}_{1.} \\ \hline \vdots \\ \hline y_{k1} - \bar{y}_{k.} \\ \vdots \\ y_{k n_k} - \bar{y}_{k.} \end{pmatrix}
    $$
    *Squared Norm:* $RSS_1 = \sum_{i}\sum_{j} (y_{ij} - \bar{y}_{i.})^2$ ($SS_{within}$)

**Conclusion:**
By orthogonality, the squared length of the total deviation equals the sum of the squared lengths of these components:
$$
\underbrace{||\hat{y}_1 - \hat{y}_0||^2}_{SS_{between}} + \underbrace{||y - \hat{y}_1||^2}_{SS_{within}} = \underbrace{||y - \hat{y}_0||^2}_{TSS}
$$
:::

### Projections in Orthogonal subspaces

Finally, we consider the case where the entire space $\mathbb{R}^n$ is decomposed into mutually orthogonal subspaces.

::: {#thm-orth-decomposition}
### Orthogonal Decomposition
If $\mathbb{R}^n$ is the direct sum of orthogonal subspaces $V_1, V_2, \dots, V_k$:

$$
\mathbb{R}^n = V_1 \oplus V_2 \oplus \dots \oplus V_k
$$
where $V_i \perp V_j$ for all $i \ne j$.

Then any vector $y$ can be uniquely written as:
$$
y = x_1 + x_2 + \dots + x_k
$$
where $x_i \in V_i$.

Furthermore, each component $x_i$ is simply the projection of $y$ onto the subspace $V_i$:
$$
x_i = P_i y
$$
:::

This implies that the identity matrix can be decomposed into a sum of projection matrices:
$$
I_n = P_1 + P_2 + \dots + P_k
$$

```{r}
#| label: extract-p91
#| include: false
extract_page_image(91, "page91_orth_spaces.png", "1200x800+0+200")
```

![Orthogonal Space Decomposition](lec1_images/page91_orth_spaces.png){width=70%}

The following diagram summarizes the relationships between the vector spaces and projections discussed in this lecture.

```{r}
#| label: extract-p92
#| include: false
extract_page_image(92, "page92_diagram.png", "1200x1000+0+300", force = TRUE)
```

![Summary Diagram](lec1_images/page92_diagram.png){width=70%}
