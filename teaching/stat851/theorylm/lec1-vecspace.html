<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Projection in Vector Space – Theory of Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lec2-matrix.html" rel="next">
<link href="./introlm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1b3e43c72e8be34557c75123b0b69e0d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d1855ce4d3ca2472244e2456266329f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.9/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<script src="site_libs/jquery-3.5.1/jquery.min.js"></script>
<link href="site_libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/mystyles.css">
<script src="resources/num_eq.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./lec1-vecspace.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Projection in Vector Space</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Theory of Linear Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec1-vecspace.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Projection in Vector Space</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec2-matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec3-mvn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multivariate Normal Distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec4-qf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Distribution of Quadratic Forms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec5-est.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference for A Multiple Linear Regression Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ginv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Generalized Inverses</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#vector-and-projection-onto-a-line" id="toc-vector-and-projection-onto-a-line" class="nav-link active" data-scroll-target="#vector-and-projection-onto-a-line"><span class="header-section-number">2.1</span> Vector and Projection onto a Line</a>
  <ul>
  <li><a href="#vectors-and-operations" id="toc-vectors-and-operations" class="nav-link" data-scroll-target="#vectors-and-operations"><span class="header-section-number">2.1.1</span> Vectors and Operations</a></li>
  <li><a href="#scalar-multiplication-and-distance" id="toc-scalar-multiplication-and-distance" class="nav-link" data-scroll-target="#scalar-multiplication-and-distance"><span class="header-section-number">2.1.2</span> Scalar Multiplication and Distance</a></li>
  <li><a href="#angle-and-inner-product" id="toc-angle-and-inner-product" class="nav-link" data-scroll-target="#angle-and-inner-product"><span class="header-section-number">2.1.3</span> Angle and Inner Product</a></li>
  <li><a href="#coordinate-scalar-projection" id="toc-coordinate-scalar-projection" class="nav-link" data-scroll-target="#coordinate-scalar-projection"><span class="header-section-number">2.1.4</span> Coordinate (Scalar) Projection</a></li>
  <li><a href="#vector-projection-formula" id="toc-vector-projection-formula" class="nav-link" data-scroll-target="#vector-projection-formula"><span class="header-section-number">2.1.5</span> Vector Projection Formula</a></li>
  <li><a href="#perpendicularity-orthogonality" id="toc-perpendicularity-orthogonality" class="nav-link" data-scroll-target="#perpendicularity-orthogonality"><span class="header-section-number">2.1.6</span> Perpendicularity (Orthogonality)</a></li>
  <li><a href="#projection-onto-a-line-subspace" id="toc-projection-onto-a-line-subspace" class="nav-link" data-scroll-target="#projection-onto-a-line-subspace"><span class="header-section-number">2.1.7</span> Projection onto a Line (Subspace)</a></li>
  <li><a href="#projection-matrix-p_x" id="toc-projection-matrix-p_x" class="nav-link" data-scroll-target="#projection-matrix-p_x"><span class="header-section-number">2.1.8</span> Projection Matrix (<span class="math inline">\(P_x\)</span>)</a></li>
  <li><a href="#pythagorean-theorem" id="toc-pythagorean-theorem" class="nav-link" data-scroll-target="#pythagorean-theorem"><span class="header-section-number">2.1.9</span> Pythagorean Theorem</a></li>
  <li><a href="#least-square-property" id="toc-least-square-property" class="nav-link" data-scroll-target="#least-square-property"><span class="header-section-number">2.1.10</span> Least Square Property</a></li>
  </ul></li>
  <li><a href="#vector-space" id="toc-vector-space" class="nav-link" data-scroll-target="#vector-space"><span class="header-section-number">2.2</span> Vector Space</a>
  <ul>
  <li><a href="#spanned-vector-space" id="toc-spanned-vector-space" class="nav-link" data-scroll-target="#spanned-vector-space"><span class="header-section-number">2.2.1</span> Spanned Vector Space</a></li>
  <li><a href="#column-space-and-row-space" id="toc-column-space-and-row-space" class="nav-link" data-scroll-target="#column-space-and-row-space"><span class="header-section-number">2.2.2</span> Column Space and Row Space</a></li>
  <li><a href="#linear-independence-and-rank" id="toc-linear-independence-and-rank" class="nav-link" data-scroll-target="#linear-independence-and-rank"><span class="header-section-number">2.2.3</span> Linear Independence and Rank</a></li>
  </ul></li>
  <li><a href="#rank-of-matrices-and-dim-of-vector-space" id="toc-rank-of-matrices-and-dim-of-vector-space" class="nav-link" data-scroll-target="#rank-of-matrices-and-dim-of-vector-space"><span class="header-section-number">2.3</span> Rank of Matrices and Dim of Vector Space</a>
  <ul>
  <li><a href="#orthogonality-to-a-subspace" id="toc-orthogonality-to-a-subspace" class="nav-link" data-scroll-target="#orthogonality-to-a-subspace"><span class="header-section-number">2.3.1</span> Orthogonality to a Subspace</a></li>
  <li><a href="#kernel-null-space-and-image" id="toc-kernel-null-space-and-image" class="nav-link" data-scroll-target="#kernel-null-space-and-image"><span class="header-section-number">2.3.2</span> Kernel (Null Space) and Image</a></li>
  <li><a href="#nullity-theorem" id="toc-nullity-theorem" class="nav-link" data-scroll-target="#nullity-theorem"><span class="header-section-number">2.3.3</span> Nullity Theorem</a></li>
  <li><a href="#rank-inequalities" id="toc-rank-inequalities" class="nav-link" data-scroll-target="#rank-inequalities"><span class="header-section-number">2.3.4</span> Rank Inequalities</a></li>
  <li><a href="#rank-of-xx-and-xx" id="toc-rank-of-xx-and-xx" class="nav-link" data-scroll-target="#rank-of-xx-and-xx"><span class="header-section-number">2.3.5</span> Rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span></a></li>
  </ul></li>
  <li><a href="#orthogonal-projection-onto-a-subspace" id="toc-orthogonal-projection-onto-a-subspace" class="nav-link" data-scroll-target="#orthogonal-projection-onto-a-subspace"><span class="header-section-number">2.4</span> Orthogonal Projection onto a Subspace</a>
  <ul>
  <li><a href="#equivalence-to-least-squares" id="toc-equivalence-to-least-squares" class="nav-link" data-scroll-target="#equivalence-to-least-squares"><span class="header-section-number">2.4.1</span> Equivalence to Least Squares</a></li>
  <li><a href="#uniqueness-of-projection" id="toc-uniqueness-of-projection" class="nav-link" data-scroll-target="#uniqueness-of-projection"><span class="header-section-number">2.4.2</span> Uniqueness of Projection</a></li>
  </ul></li>
  <li><a href="#projection-via-orthonormal-basis-q" id="toc-projection-via-orthonormal-basis-q" class="nav-link" data-scroll-target="#projection-via-orthonormal-basis-q"><span class="header-section-number">2.5</span> Projection via Orthonormal Basis (<span class="math inline">\(Q\)</span>)</a>
  <ul>
  <li><a href="#orthonomal-basis" id="toc-orthonomal-basis" class="nav-link" data-scroll-target="#orthonomal-basis"><span class="header-section-number">2.5.1</span> Orthonomal Basis</a></li>
  <li><a href="#projection-matrix-via-orthonomal-basis-q" id="toc-projection-matrix-via-orthonomal-basis-q" class="nav-link" data-scroll-target="#projection-matrix-via-orthonomal-basis-q"><span class="header-section-number">2.5.2</span> Projection Matrix via Orthonomal Basis (<span class="math inline">\(Q\)</span>)</a></li>
  <li><a href="#gram-schmidt-process" id="toc-gram-schmidt-process" class="nav-link" data-scroll-target="#gram-schmidt-process"><span class="header-section-number">2.5.3</span> Gram-Schmidt Process</a></li>
  </ul></li>
  <li><a href="#hat-matrix-projection-matrix-via-x" id="toc-hat-matrix-projection-matrix-via-x" class="nav-link" data-scroll-target="#hat-matrix-projection-matrix-via-x"><span class="header-section-number">2.6</span> Hat Matrix (Projection Matrix via <span class="math inline">\(X\)</span>)</a>
  <ul>
  <li><a href="#norm-equations" id="toc-norm-equations" class="nav-link" data-scroll-target="#norm-equations"><span class="header-section-number">2.6.1</span> Norm Equations</a></li>
  <li><a href="#hat-matrix" id="toc-hat-matrix" class="nav-link" data-scroll-target="#hat-matrix"><span class="header-section-number">2.6.2</span> Hat Matrix</a></li>
  <li><a href="#equivalence-of-hat-matrix-and-qq" id="toc-equivalence-of-hat-matrix-and-qq" class="nav-link" data-scroll-target="#equivalence-of-hat-matrix-and-qq"><span class="header-section-number">2.6.3</span> Equivalence of Hat Matrix and <span class="math inline">\(QQ'\)</span></a></li>
  <li><a href="#properties-of-hat-matrix" id="toc-properties-of-hat-matrix" class="nav-link" data-scroll-target="#properties-of-hat-matrix"><span class="header-section-number">2.6.4</span> Properties of Hat Matrix</a></li>
  </ul></li>
  <li><a href="#projection-defined-with-orthogonal-projection-matrix" id="toc-projection-defined-with-orthogonal-projection-matrix" class="nav-link" data-scroll-target="#projection-defined-with-orthogonal-projection-matrix"><span class="header-section-number">2.7</span> Projection Defined with Orthogonal Projection Matrix</a>
  <ul>
  <li><a href="#orthogonal-projection-matrix" id="toc-orthogonal-projection-matrix" class="nav-link" data-scroll-target="#orthogonal-projection-matrix"><span class="header-section-number">2.7.1</span> Orthogonal Projection Matrix</a></li>
  <li><a href="#projection-onto-complement-space" id="toc-projection-onto-complement-space" class="nav-link" data-scroll-target="#projection-onto-complement-space"><span class="header-section-number">2.7.2</span> Projection onto Complement Space</a></li>
  </ul></li>
  <li><a href="#projection-onto-nested-subspaces" id="toc-projection-onto-nested-subspaces" class="nav-link" data-scroll-target="#projection-onto-nested-subspaces"><span class="header-section-number">2.8</span> Projection onto Nested Subspaces</a>
  <ul>
  <li><a href="#nested-models-and-subspaces" id="toc-nested-models-and-subspaces" class="nav-link" data-scroll-target="#nested-models-and-subspaces"><span class="header-section-number">2.8.1</span> Nested Models and Subspaces</a></li>
  <li><a href="#projections-onto-nested-subspaces" id="toc-projections-onto-nested-subspaces" class="nav-link" data-scroll-target="#projections-onto-nested-subspaces"><span class="header-section-number">2.8.2</span> Projections onto Nested Subspaces</a></li>
  <li><a href="#decomposition-of-projections-and-their-sum-squares" id="toc-decomposition-of-projections-and-their-sum-squares" class="nav-link" data-scroll-target="#decomposition-of-projections-and-their-sum-squares"><span class="header-section-number">2.8.3</span> Decomposition of Projections and their Sum Squares</a></li>
  </ul></li>
  <li><a href="#projections-onto-orthogonal-subspaces" id="toc-projections-onto-orthogonal-subspaces" class="nav-link" data-scroll-target="#projections-onto-orthogonal-subspaces"><span class="header-section-number">2.9</span> Projections onto Orthogonal Subspaces</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Projection in Vector Space</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="vector-and-projection-onto-a-line" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="vector-and-projection-onto-a-line"><span class="header-section-number">2.1</span> Vector and Projection onto a Line</h2>
<section id="vectors-and-operations" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="vectors-and-operations"><span class="header-section-number">2.1.1</span> Vectors and Operations</h3>
<p>The concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.</p>
<div id="def-vector" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Vector)</strong></span> A <strong>vector</strong> <span class="math inline">\(x\)</span> is defined as a point in <span class="math inline">\(n\)</span>-dimensional space (<span class="math inline">\(\mathbb{R}^n\)</span>). It is typically represented as a column vector containing <span class="math inline">\(n\)</span> real-valued components:</p>
<p><span class="math display">\[
x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
\]</span></p>
</div>
<p>Vectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.</p>
<p><strong>Vector Arithmetic:</strong> Vectors can be manipulated geometrically:</p>
<div id="def-vector-addition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Vector Addition)</strong></span> The sum of two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of <span class="math inline">\(y\)</span> at the head of <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
x + y = \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}
\]</span></p>
</div>
<div id="def-vector-subtraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Vector Subtraction)</strong></span> The difference <span class="math inline">\(d = y - x\)</span> is the vector that “closes the triangle” formed by <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. It represents the displacement vector that connects the tip of <span class="math inline">\(x\)</span> to the tip of <span class="math inline">\(y\)</span>, such that <span class="math inline">\(x + d = y\)</span>.</p>
</div>
</section>
<section id="scalar-multiplication-and-distance" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="scalar-multiplication-and-distance"><span class="header-section-number">2.1.2</span> Scalar Multiplication and Distance</h3>
<p>In addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.</p>
<div id="def-scalar-mult" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Scalar Multiplication)</strong></span> Multiplying a vector by a scalar <span class="math inline">\(c\)</span> scales its magnitude (length) without changing its line of direction. If <span class="math inline">\(c\)</span> is positive, the direction remains the same; if <span class="math inline">\(c\)</span> is negative, the direction is reversed.</p>
<p><span class="math display">\[
c x = \begin{pmatrix} c x_1 \\ \vdots \\ c x_n \end{pmatrix}
\]</span></p>
</div>
<p>We often need to quantify the “size” of a vector. This is done using the concept of length, or norm.</p>
<div id="def-euclidean-distance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Euclidean Distance (Length))</strong></span> The length (or norm) of a vector <span class="math inline">\(x = (x_1, \dots, x_n)^T\)</span> corresponds to the straight-line distance from the origin to the point defined by <span class="math inline">\(x\)</span>. It is defined as the square root of the sum of squared components:</p>
<p><span class="math display">\[
||x||^2 = \sum_{i=1}^n x_i^2
\]</span></p>
<p><span class="math display">\[
||x|| = \sqrt{\sum_{i=1}^n x_i^2}
\]</span></p>
</div>
</section>
<section id="angle-and-inner-product" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="angle-and-inner-product"><span class="header-section-number">2.1.3</span> Angle and Inner Product</h3>
<p>To understand the relationship between two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and their difference <span class="math inline">\(y-x\)</span>. By applying the classic <strong>Law of Cosines</strong> to this triangle, we can relate the geometric angle to the vector lengths.</p>
<div id="thm-law-of-cosines" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Law of Cosines)</strong></span> For a triangle with sides <span class="math inline">\(a, b, c\)</span> and angle <span class="math inline">\(\theta\)</span> opposite to side <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[
c^2 = a^2 + b^2 - 2ab \cos \theta
\]</span></p>
</div>
<p>Translating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:</p>
<p><span class="math display">\[
||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \cdot ||y|| \cos \theta
\]</span></p>
<p>This equation provides a critical link between the geometric angle <span class="math inline">\(\theta\)</span> and the algebraic norms of the vectors.</p>
<p><strong>Derivation of Inner Product</strong></p>
<p>We can express the squared distance term <span class="math inline">\(||y - x||^2\)</span> purely algebraically by expanding the components:</p>
<p><span class="math display">\[
||y - x||^2 = \sum_{i=1}^n (x_i - y_i)^2
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)
\]</span></p>
<p><span class="math display">\[
= ||x||^2 + ||y||^2 - 2 \sum_{i=1}^n x_i y_i
\]</span></p>
<p>By comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the <strong>Inner Product</strong> (or dot product).</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Inner Product)</strong></span> The inner product of two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is defined as the sum of the products of their corresponding components:</p>
<p><span class="math display">\[
x'y = \sum_{i=1}^n x_i y_i = \langle x, y \rangle
\]</span></p>
</div>
<p>Thus, equating the geometric and algebraic forms yields the fundamental relationship:</p>
<p><span class="math display">\[
x'y = ||x|| \cdot ||y|| \cos \theta
\]</span></p>
</section>
<section id="coordinate-scalar-projection" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="coordinate-scalar-projection"><span class="header-section-number">2.1.4</span> Coordinate (Scalar) Projection</h3>
<p>The inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector <span class="math inline">\(y\)</span> onto vector <span class="math inline">\(x\)</span>.</p>
<p>The length of this projection is given by:</p>
<p><span class="math display">\[
||y|| \cos \theta = \frac{x'y}{||x||}
\]</span></p>
<p>This expression can be interpreted as the inner product of <span class="math inline">\(y\)</span> with the normalized (unit) vector in the direction of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\text{Scalar Projection} = \left\langle \frac{x}{||x||}, y \right\rangle
\]</span></p>
</section>
<section id="vector-projection-formula" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="vector-projection-formula"><span class="header-section-number">2.1.5</span> Vector Projection Formula</h3>
<p>The scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.</p>
<div id="def-vector-projection" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 (Vector Projection)</strong></span> The projection of vector <span class="math inline">\(y\)</span> onto vector <span class="math inline">\(x\)</span>, denoted <span class="math inline">\(\hat{y}\)</span>, is calculated as:</p>
<p><span class="math display">\[
\text{Projection Vector} = (\text{Length}) \cdot (\text{Direction})
\]</span></p>
<p><span class="math display">\[
\hat{y} = \left( \frac{x'y}{||x||} \right) \cdot \frac{x}{||x||}
\]</span></p>
<p>This is often written compactly by combining the denominators:</p>
<p><span class="math display">\[
\hat{y} = \frac{x'y}{||x||^2} x
\]</span></p>
</div>
</section>
<section id="perpendicularity-orthogonality" class="level3" data-number="2.1.6">
<h3 data-number="2.1.6" class="anchored" data-anchor-id="perpendicularity-orthogonality"><span class="header-section-number">2.1.6</span> Perpendicularity (Orthogonality)</h3>
<p>A special case of the angle between vectors arises when <span class="math inline">\(\theta = 90^\circ\)</span>. This geometric concept of perpendicularity is central to the theory of projections and least squares.</p>
<div id="def-perpendicularity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 (Perpendicularity)</strong></span> Two vectors are defined as <strong>perpendicular</strong> (or orthogonal) if the angle between them is <span class="math inline">\(90^\circ\)</span> (<span class="math inline">\(\pi/2\)</span>).</p>
<p>Since <span class="math inline">\(\cos(90^\circ) = 0\)</span>, the condition for orthogonality simplifies to the inner product being zero:</p>
<p><span class="math display">\[
x'y = 0 \iff x \perp y
\]</span></p>
</div>
<div id="exm-orthogonal-vectors" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Orthogonal Vectors)</strong></span> Consider two vectors in <span class="math inline">\(\mathbb{R}^2\)</span>: <span class="math inline">\(x = (1, 1)'\)</span> and <span class="math inline">\(y = (1, -1)'\)</span>.</p>
<p><span class="math display">\[
x'y = 1(1) + 1(-1) = 1 - 1 = 0
\]</span></p>
<p>Since their inner product is zero, these vectors are orthogonal to each other.</p>
</div>
</section>
<section id="projection-onto-a-line-subspace" class="level3" data-number="2.1.7">
<h3 data-number="2.1.7" class="anchored" data-anchor-id="projection-onto-a-line-subspace"><span class="header-section-number">2.1.7</span> Projection onto a Line (Subspace)</h3>
<p>We can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.</p>
<div id="def-line-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9 (Line Spanned by a Vector)</strong></span> The line space <span class="math inline">\(L(x)\)</span>, or the space spanned by a vector <span class="math inline">\(x\)</span>, is defined as the set of all scalar multiples of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
L(x) = \{ cx \mid c \in \mathbb{R} \}
\]</span></p>
</div>
<p>The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(L(x)\)</span>, denoted <span class="math inline">\(\hat{y}\)</span>, is defined by the geometric property that it is the closest point on the line to <span class="math inline">\(y\)</span>. This implies that the error vector (or residual) must be perpendicular to the line itself.</p>
<div id="def-projection-line" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.10 (Projection onto a Line)</strong></span> A vector <span class="math inline">\(\hat{y}\)</span> is the projection of <span class="math inline">\(y\)</span> onto the line <span class="math inline">\(L(x)\)</span> if:</p>
<ol type="1">
<li><p><span class="math inline">\(\hat{y}\)</span> lies on the line <span class="math inline">\(L(x)\)</span> (i.e., <span class="math inline">\(\hat{y} = cx\)</span> for some scalar <span class="math inline">\(c\)</span>).</p></li>
<li><p>The residual vector <span class="math inline">\((y - \hat{y})\)</span> is perpendicular to the direction vector <span class="math inline">\(x\)</span>.</p></li>
</ol>
</div>
<p><strong>Derivation:</strong> To find the value of the scalar <span class="math inline">\(c\)</span>, we apply the orthogonality condition:</p>
<p><span class="math display">\[
(y - \hat{y}) \perp x \implies x'(y - cx) = 0
\]</span></p>
<p>Expanding this inner product gives:</p>
<p><span class="math display">\[
x'y - c(x'x) = 0
\]</span></p>
<p>Solving for <span class="math inline">\(c\)</span>, we obtain:</p>
<p><span class="math display">\[
c = \frac{x'y}{||x||^2}
\]</span></p>
<p>This confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.</p>
<p><strong>Alternative Forms of the Projection Formula</strong></p>
<p>We can express the projection vector <span class="math inline">\(\hat{y}\)</span> in several equivalent ways to highlight different geometric interpretations.</p>
<div id="def-projection-formulae" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.11 (Forms of Projection)</strong></span> The projection of <span class="math inline">\(y\)</span> onto the vector <span class="math inline">\(x\)</span> is given by:</p>
<p><span class="math display">\[
\hat{y} = \frac{x'y}{||x||^2} x = \left\langle y, \frac{x}{||x||} \right\rangle \frac{x}{||x||}
\]</span></p>
<p>This second form separates the components into: <span class="math display">\[
\text{Projection} = (\text{Scalar Projection}) \times (\text{Unit Direction})
\]</span></p>
</div>
</section>
<section id="projection-matrix-p_x" class="level3" data-number="2.1.8">
<h3 data-number="2.1.8" class="anchored" data-anchor-id="projection-matrix-p_x"><span class="header-section-number">2.1.8</span> Projection Matrix (<span class="math inline">\(P_x\)</span>)</h3>
<p>In linear models, it is often more convenient to view projection as a linear transformation applied to the vector <span class="math inline">\(y\)</span>. This allows us to define a <strong>Projection Matrix</strong>.</p>
<p>We can rewrite the formula for <span class="math inline">\(\hat{y}\)</span> by factoring out <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\hat{y} = \text{proj}(y|x) = x \frac{x'y}{||x||^2} = \frac{xx'}{||x||^2} y
\]</span></p>
<p>This leads to the definition of the projection matrix <span class="math inline">\(P_x\)</span>.</p>
<div id="def-projection-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.12 (Projection Matrix onto a Single Vector)</strong></span> The matrix <span class="math inline">\(P_x\)</span> that projects any vector <span class="math inline">\(y\)</span> onto the line spanned by <span class="math inline">\(x\)</span> is defined as:</p>
<p><span class="math display">\[
P_x = \frac{xx'}{||x||^2}
\]</span></p>
<p>Using this matrix, the projection is simply: <span class="math display">\[
\hat{y} = P_x y
\]</span></p>
<p>If <span class="math inline">\(x \in \mathbb{R}^p\)</span>, then <span class="math inline">\(P_x\)</span> is a <span class="math inline">\(p \times p\)</span> symmetric matrix.</p>
</div>
<p>Let’s apply these concepts to a concrete example.</p>
<div id="exm-projection-r2" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Numerical Projection)</strong></span> Let <span class="math inline">\(y = (1, 3)'\)</span> and <span class="math inline">\(x = (1, 1)'\)</span>. We want to find the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span>.</p>
<p><strong>Method 1: Using the Vector Formula</strong> First, calculate the inner products: <span class="math display">\[
x'y = 1(1) + 1(3) = 4
\]</span> <span class="math display">\[
||x||^2 = 1^2 + 1^2 = 2
\]</span></p>
<p>Now, apply the formula: <span class="math display">\[
\hat{y} = \frac{4}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 2 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
\]</span></p>
<p><strong>Method 2: Using the Projection Matrix</strong> Construct the matrix <span class="math inline">\(P_x\)</span>: <span class="math display">\[
P_x = \frac{1}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} = \begin{pmatrix} 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 \end{pmatrix}
\]</span></p>
<p>Multiply by <span class="math inline">\(y\)</span>: <span class="math display">\[
\hat{y} = P_x y = \begin{pmatrix} 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \end{pmatrix} = \begin{pmatrix} 0.5(1) + 0.5(3) \\ 0.5(1) + 0.5(3) \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
\]</span></p>
</div>
<p><strong>Example: Projection onto the Ones Vector (</strong><span class="math inline">\(j_n\)</span>)</p>
<p>A very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.</p>
<div id="exm-mean-projection" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Projection onto the Ones Vector)</strong></span> Let <span class="math inline">\(y = (y_1, \dots, y_n)'\)</span> be a data vector. Let <span class="math inline">\(j_n = (1, 1, \dots, 1)'\)</span> be a vector of all ones.</p>
<p>The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(j_n\)</span> is: <span class="math display">\[
\text{proj}(y|j_n) = \frac{j_n' y}{||j_n||^2} j_n
\]</span></p>
<p>Calculating the components: <span class="math display">\[
j_n' y = \sum_{i=1}^n y_i \quad \text{(Sum of observations)}
\]</span> <span class="math display">\[
||j_n||^2 = \sum_{i=1}^n 1^2 = n
\]</span></p>
<p>Substituting these back: <span class="math display">\[
\hat{y} = \frac{\sum y_i}{n} j_n = \bar{y} j_n = \begin{pmatrix} \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
\]</span></p>
<p>Thus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.</p>
</div>
</section>
<section id="pythagorean-theorem" class="level3" data-number="2.1.9">
<h3 data-number="2.1.9" class="anchored" data-anchor-id="pythagorean-theorem"><span class="header-section-number">2.1.9</span> Pythagorean Theorem</h3>
<p>The Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.</p>
<div id="thm-pythagorean" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2 (Pythagorean Theorem)</strong></span> If two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are orthogonal (i.e., <span class="math inline">\(x \perp y\)</span> or <span class="math inline">\(x'y = 0\)</span>), then the squared length of their sum is equal to the sum of their squared lengths:</p>
<p><span class="math display">\[
||x + y||^2 = ||x||^2 + ||y||^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We expand the squared norm using the inner product:</p>
<p><span class="math display">\[
\begin{aligned}
||x + y||^2 &amp;= (x + y)' (x + y) \\
&amp;= x'x + x'y + y'x + y'y \\
&amp;= ||x||^2 + 2x'y + ||y||^2
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(x \perp y\)</span>, the inner product <span class="math inline">\(x'y = 0\)</span>. Thus, the term <span class="math inline">\(2x'y\)</span> vanishes, leaving:</p>
<p><span class="math display">\[
||x + y||^2 = ||x||^2 + ||y||^2
\]</span></p>
</div>
<p>The proof after defining inner product to represent <span class="math inline">\(\cos(\theta)\)</span> is trivival. <a href="#fig-pythagoras-proof" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows a geometric proof of the fundamental Pythagorean Theorem (aka <strong>勾股定理</strong>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pythagoras-proof" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" style="width: 80% !important;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pythagoras-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-pythagoras-proof-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pythagoras-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Proof of Pythagorean Theorem using Area Scaling
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="least-square-property" class="level3" data-number="2.1.10">
<h3 data-number="2.1.10" class="anchored" data-anchor-id="least-square-property"><span class="header-section-number">2.1.10</span> Least Square Property</h3>
<p>One of the most important properties of the orthogonal projection is that it minimizes the distance between the vector <span class="math inline">\(y\)</span> and the subspace (or line) onto which it is projected.</p>
<div id="thm-shortest-distance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3 (Least Square Property)</strong></span> Let <span class="math inline">\(\hat{y}\)</span> be the projection of <span class="math inline">\(y\)</span> onto the line <span class="math inline">\(L(x)\)</span>. For any other vector <span class="math inline">\(y^*\)</span> on the line <span class="math inline">\(L(x)\)</span>, the distance from <span class="math inline">\(y\)</span> to <span class="math inline">\(y^*\)</span> is always greater than or equal to the distance from <span class="math inline">\(y\)</span> to <span class="math inline">\(\hat{y}\)</span>.</p>
<p><span class="math display">\[
||y - y^*|| \ge ||y - \hat{y}||
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since both <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y^*\)</span> lie on the line <span class="math inline">\(L(x)\)</span>, their difference <span class="math inline">\((\hat{y} - y^*)\)</span> also lies on <span class="math inline">\(L(x)\)</span>. From the definition of projection, the residual <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to the line <span class="math inline">\(L(x)\)</span>. Therefore:</p>
<p><span class="math display">\[
(y - \hat{y}) \perp (\hat{y} - y^*)
\]</span></p>
<p>We can write the vector <span class="math inline">\((y - y^*)\)</span> as: <span class="math display">\[
y - y^* = (y - \hat{y}) + (\hat{y} - y^*)
\]</span></p>
<p>Applying the Pythagorean Theorem: <span class="math display">\[
||y - y^*||^2 = ||y - \hat{y}||^2 + ||\hat{y} - y^*||^2
\]</span></p>
<p>Since <span class="math inline">\(||\hat{y} - y^*||^2 \ge 0\)</span>, it follows that: <span class="math display">\[
||y - y^*||^2 \ge ||y - \hat{y}||^2
\]</span></p>
</div>
</section>
</section>
<section id="vector-space" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="vector-space"><span class="header-section-number">2.2</span> Vector Space</h2>
<p>We now generalize our discussion from lines to broader spaces.</p>
<div id="def-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.13 (Vector Space)</strong></span> A set <span class="math inline">\(V \subseteq \mathbb{R}^n\)</span> is called a <strong>Vector Space</strong> if it is closed under vector addition and scalar multiplication:</p>
<ol type="1">
<li><strong>Closed under Addition:</strong> If <span class="math inline">\(x_1 \in V\)</span> and <span class="math inline">\(x_2 \in V\)</span>, then <span class="math inline">\(x_1 + x_2 \in V\)</span>.</li>
<li><strong>Closed under Scalar Multiplication:</strong> If <span class="math inline">\(x \in V\)</span>, then <span class="math inline">\(cx \in V\)</span> for any scalar <span class="math inline">\(c \in \mathbb{R}\)</span>.</li>
</ol>
</div>
<p>It follows that the zero vector <span class="math inline">\(0\)</span> must belong to any subspace (by choosing <span class="math inline">\(c=0\)</span>).</p>
<section id="spanned-vector-space" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="spanned-vector-space"><span class="header-section-number">2.2.1</span> Spanned Vector Space</h3>
<p>The most common way to construct a vector space in linear models is by spanning it with a set of vectors.</p>
<div id="def-spanned-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.14 (Spanned Vector Space)</strong></span> Let <span class="math inline">\(x_1, \dots, x_p\)</span> be a set of vectors in <span class="math inline">\(\mathbb{R}^n\)</span>. The space spanned by these vectors, denoted <span class="math inline">\(L(x_1, \dots, x_p)\)</span>, is the set of all possible linear combinations of them:</p>
<p><span class="math display">\[
L(x_1, \dots, x_p) = \{ r \mid r = c_1 x_1 + \dots + c_p x_p, \text{ for } c_i \in \mathbb{R} \}
\]</span></p>
</div>
</section>
<section id="column-space-and-row-space" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="column-space-and-row-space"><span class="header-section-number">2.2.2</span> Column Space and Row Space</h3>
<p>When vectors are arranged into a matrix, we define specific spaces based on their columns and rows.</p>
<div id="def-column-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.15 (Column Space)</strong></span> For a matrix <span class="math inline">\(X = (x_1, \dots, x_p)\)</span>, the <strong>Column Space</strong>, denoted <span class="math inline">\(\text{Col}(X)\)</span>, is the vector space spanned by its columns:</p>
<p><span class="math display">\[
\text{Col}(X) = L(x_1, \dots, x_p)
\]</span></p>
</div>
<div id="def-row-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.16 (Row Space)</strong></span> The <strong>Row Space</strong>, denoted <span class="math inline">\(\text{Row}(X)\)</span>, is the vector space spanned by the rows of the matrix <span class="math inline">\(X\)</span>.</p>
</div>
</section>
<section id="linear-independence-and-rank" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="linear-independence-and-rank"><span class="header-section-number">2.2.3</span> Linear Independence and Rank</h3>
<p>Not all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.</p>
<div id="def-linear-independence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.17 (Linear Independence)</strong></span> A set of vectors <span class="math inline">\(x_1, \dots, x_p\)</span> is said to be <strong>Linearly Independent</strong> if the only solution to the linear combination equation equal to zero is the trivial solution:</p>
<p><span class="math display">\[
\sum_{i=1}^p c_i x_i = 0 \implies c_1 = c_2 = \dots = c_p = 0
\]</span></p>
<p>If there exist non-zero <span class="math inline">\(c_i\)</span>’s such that sum is zero, the vectors are <strong>Linearly Dependent</strong>.</p>
</div>
</section>
</section>
<section id="rank-of-matrices-and-dim-of-vector-space" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="rank-of-matrices-and-dim-of-vector-space"><span class="header-section-number">2.3</span> Rank of Matrices and Dim of Vector Space</h2>
<div id="def-rank" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.18 (Rank)</strong></span> The <strong>Rank</strong> of a matrix <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\text{Rank}(X)\)</span>, is the maximum number of linearly independent columns in <span class="math inline">\(X\)</span>. This is equivalent to the dimension of the column space:</p>
<p><span class="math display">\[
\text{Rank}(X) = \text{Dim}(\text{Col}(X))
\]</span></p>
</div>
<p>There are several fundamental properties regarding the rank of a matrix.</p>
<div id="exm-row-rank-equal-col-rank" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Example of the Equality of Row and Col Rank)</strong></span> Consider the following <span class="math inline">\(3 \times 4\)</span> matrix (<span class="math inline">\(n=3, p=4\)</span>): <span class="math display">\[
X = \begin{pmatrix}
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{pmatrix}
\]</span> Notice that the third row is the sum of the first two (<span class="math inline">\(r_3 = r_1 + r_2\)</span>).</p>
<p><strong>1. Row Rank and Basis</strong> <span class="math inline">\(U\)</span> The first two rows are linearly independent. We set the row rank <span class="math inline">\(r=2\)</span> and use these rows as our basis matrix <span class="math inline">\(U\)</span> (<span class="math inline">\(2 \times 4\)</span>): <span class="math display">\[
U = \begin{pmatrix}
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<p><strong>2. Coefficient Matrix</strong> <span class="math inline">\(C\)</span> We express every row of <span class="math inline">\(X\)</span> as a linear combination of the rows of <span class="math inline">\(U\)</span>:</p>
<ul>
<li>Row 1: <span class="math inline">\(1 \cdot u_1 + 0 \cdot u_2\)</span></li>
<li>Row 2: <span class="math inline">\(0 \cdot u_1 + 1 \cdot u_2\)</span></li>
<li>Row 3: <span class="math inline">\(1 \cdot u_1 + 1 \cdot u_2\)</span></li>
</ul>
<p>These coefficients form the matrix <span class="math inline">\(C\)</span> (<span class="math inline">\(3 \times 2\)</span>): <span class="math display">\[
C = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
1 &amp; 1
\end{pmatrix}
\]</span></p>
<p><strong>3. The Decomposition (</strong><span class="math inline">\(X = CU\)</span>) We verify that <span class="math inline">\(X\)</span> is the product of <span class="math inline">\(C\)</span> and <span class="math inline">\(U\)</span>: <span class="math display">\[
\underbrace{\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \end{pmatrix}}_{X \ (3 \times 4)}
=
\underbrace{\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}}_{C \ (3 \times 2)}
\underbrace{\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 1 \end{pmatrix}}_{U \ (2 \times 4)}
\]</span></p>
<p><strong>4. Conclusion on Column Rank</strong> The columns of <span class="math inline">\(X\)</span> are linear combinations of the columns of <span class="math inline">\(C\)</span>. <span class="math display">\[
\text{Col}(X) \subseteq \text{Col}(C)
\]</span> Since <span class="math inline">\(C\)</span> has only 2 columns, the dimension of its column space (and thus <span class="math inline">\(X\)</span>’s column space) cannot exceed 2. <span class="math display">\[
\text{Dim}(\text{Col}(X)) \le 2
\]</span> This confirms that Row Rank (2) <span class="math inline">\(\ge\)</span> Column Rank. (By symmetry, they are equal).</p>
</div>
<div id="thm-rank-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.4 (Row Rank equals Column Rank)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Row Rank equals Column Rank:</strong> The dimension of the column space is equal to the dimension of the row space. <span class="math display">\[
\text{Dim}(\text{Col}(X)) = \text{Dim}(\text{Row}(X)) \implies \text{Rank}(X) = \text{Rank}(X')
\]</span></p></li>
<li><p><strong>Bounds:</strong> For an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Rank}(X) \le \min(n, p)
\]</span></p></li>
</ol>
</div>
<section id="orthogonality-to-a-subspace" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="orthogonality-to-a-subspace"><span class="header-section-number">2.3.1</span> Orthogonality to a Subspace</h3>
<p>We can extend the concept of orthogonality from single vectors to entire subspaces.</p>
<div id="def-orth-subspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.19 (Orthogonality to a Subspace)</strong></span> A vector <span class="math inline">\(y\)</span> is orthogonal to a subspace <span class="math inline">\(V\)</span> (denoted <span class="math inline">\(y \perp V\)</span>) if <span class="math inline">\(y\)</span> is orthogonal to <strong>every</strong> vector <span class="math inline">\(x\)</span> in <span class="math inline">\(V\)</span>.</p>
<p><span class="math display">\[
y \perp V \iff y'x = 0 \quad \forall x \in V
\]</span></p>
</div>
<div id="def-orthogonal-complement" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.20 (Orthogonal Complement)</strong></span> The set of all vectors that are orthogonal to a subspace <span class="math inline">\(V\)</span> is called the <strong>Orthogonal Complement</strong> of <span class="math inline">\(V\)</span>, denoted <span class="math inline">\(V^\perp\)</span>.</p>
<p><span class="math display">\[
V^\perp = \{ y \in \mathbb{R}^n \mid y \perp V \}
\]</span></p>
</div>
</section>
<section id="kernel-null-space-and-image" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kernel-null-space-and-image"><span class="header-section-number">2.3.2</span> Kernel (Null Space) and Image</h3>
<p>For a matrix transformation defined by <span class="math inline">\(X\)</span>, we define two key spaces: the Image (Column Space) and the Kernel (Null Space).</p>
<div id="def-image-kernel" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.21 (Image and Kernel)</strong></span> &nbsp;</p>
<ol type="1">
<li><p><strong>Image (Column Space):</strong> The set of all possible outputs. <span class="math display">\[
\text{Im}(X) = \text{Col}(X) = \{ X\beta \mid \beta \in \mathbb{R}^p \}
\]</span></p></li>
<li><p><strong>Kernel (Null Space):</strong> The set of all inputs mapped to the zero vector. <span class="math display">\[
\text{Ker}(X) = \{ \beta \in \mathbb{R}^p \mid X\beta = 0 \}
\]</span></p></li>
</ol>
</div>
<div id="thm-kernel-rowspace" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.5 (Relationship between Kernel and Row Space)</strong></span> The kernel of <span class="math inline">\(X\)</span> is the orthogonal complement of the row space of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\text{Ker}(X) = [\text{Row}(X)]^\perp
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(x \in \mathbb{R}^p\)</span>. <span class="math inline">\(x \in \text{Ker}(X)\)</span> if and only if <span class="math inline">\(Xx = 0\)</span>. If we denote the rows of <span class="math inline">\(X\)</span> as <span class="math inline">\(r_1', \dots, r_n'\)</span>, then the equation <span class="math inline">\(Xx = 0\)</span> is equivalent to the system of equations: <span class="math display">\[
\begin{pmatrix} r_1' \\ \vdots \\ r_n' \end{pmatrix} x = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix} \iff r_i' x = 0 \text{ for all } i = 1, \dots, n
\]</span> This means <span class="math inline">\(x\)</span> is orthogonal to every row of <span class="math inline">\(X\)</span>. Since the rows span the row space <span class="math inline">\(\text{Row}(X)\)</span>, being orthogonal to every generator <span class="math inline">\(r_i\)</span> implies <span class="math inline">\(x\)</span> is orthogonal to the entire space <span class="math inline">\(\text{Row}(X)\)</span>. Thus, <span class="math inline">\(\text{Ker}(X) = \{ x \mid x \perp \text{Row}(X) \} = [\text{Row}(X)]^\perp\)</span>.</p>
</div>
</section>
<section id="nullity-theorem" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="nullity-theorem"><span class="header-section-number">2.3.3</span> Nullity Theorem</h3>
<p>There is a fundamental relationship between the dimensions of these spaces.</p>
<div id="thm-nullity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.6 (Rank-Nullity Theorem)</strong></span> For an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\text{Rank}(X) + \text{Nullity}(X) = p
\]</span> where <span class="math inline">\(\text{Nullity}(X) = \text{Dim}(\text{Ker}(X))\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the previous theorem, we established that the kernel is the orthogonal complement of the row space: <span class="math display">\[
\text{Ker}(X) = [\text{Row}(X)]^\perp
\]</span></p>
<p>Since the row space is a subspace of <span class="math inline">\(\mathbb{R}^p\)</span>, the entire space can be decomposed into the direct sum of the row space and its orthogonal complement: <span class="math display">\[
\mathbb{R}^p = \text{Row}(X) \oplus [\text{Row}(X)]^\perp = \text{Row}(X) \oplus \text{Ker}(X)
\]</span></p>
<p>Taking the dimensions of these spaces: <span class="math display">\[
\text{Dim}(\mathbb{R}^p) = \text{Dim}(\text{Row}(X)) + \text{Dim}(\text{Ker}(X))
\]</span></p>
<p>Substituting the definitions of Rank (dimension of row/column space) and Nullity: <span class="math display">\[
p = \text{Rank}(X) + \text{Nullity}(X)
\]</span></p>
</div>
<p><strong>Comparing Ranks via Kernel Containment</strong></p>
<p>The Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (with the same number of columns) by inspecting their null spaces.</p>
<div id="thm-rank-kernel" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.7 (Kernel Containment and Rank Inequality)</strong></span> Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two matrices with <span class="math inline">\(p\)</span> columns. If the kernel of <span class="math inline">\(A\)</span> is contained within the kernel of <span class="math inline">\(B\)</span>, then the rank of <span class="math inline">\(A\)</span> is greater than or equal to the rank of <span class="math inline">\(B\)</span>.</p>
<p><span class="math display">\[
\text{Ker}(A) \subseteq \text{Ker}(B) \implies \text{Rank}(A) \ge \text{Rank}(B)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the subspace inclusion <span class="math inline">\(\text{Ker}(A) \subseteq \text{Ker}(B)\)</span>, it follows that the dimension of the smaller space cannot exceed the dimension of the larger space: <span class="math display">\[
\text{Nullity}(A) \le \text{Nullity}(B)
\]</span> Using the Rank-Nullity Theorem (<span class="math inline">\(\text{Rank} = p - \text{Nullity}\)</span>), we reverse the inequality: <span class="math display">\[
p - \text{Nullity}(A) \ge p - \text{Nullity}(B)
\]</span> <span class="math display">\[
\text{Rank}(A) \ge \text{Rank}(B)
\]</span></p>
</div>
</section>
<section id="rank-inequalities" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="rank-inequalities"><span class="header-section-number">2.3.4</span> Rank Inequalities</h3>
<p>Understanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.</p>
<div id="thm-rank-product" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.8 (Rank of a Matrix Product)</strong></span> Let <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix and <span class="math inline">\(Z\)</span> be a <span class="math inline">\(p \times k\)</span> matrix. The rank of their product <span class="math inline">\(XZ\)</span> is bounded by the rank of the individual matrices:</p>
<p><span class="math display">\[
\text{Rank}(XZ) \le \min(\text{Rank}(X), \text{Rank}(Z))
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The columns of <span class="math inline">\(XZ\)</span> are linear combinations of the columns of <span class="math inline">\(X\)</span>. Thus, the column space of <span class="math inline">\(XZ\)</span> is a subspace of the column space of <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Col}(XZ) \subseteq \text{Col}(X) \implies \text{Rank}(XZ) \le \text{Rank}(X)
\]</span> Similarly, the rows of <span class="math inline">\(XZ\)</span> are linear combinations of the rows of <span class="math inline">\(Z\)</span>. Thus, the row space of <span class="math inline">\(XZ\)</span> is a subspace of the row space of <span class="math inline">\(Z\)</span>: <span class="math display">\[
\text{Row}(XZ) \subseteq \text{Row}(Z) \implies \text{Rank}(XZ) \le \text{Rank}(Z)
\]</span></p>
</div>
<p><strong>Rank and Invertible Matrices</strong></p>
<p>Multiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.</p>
<div id="thm-rank-invertible" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.9 (Rank with Non-Singular Multiplication)</strong></span> Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> invertible matrix (i.e., <span class="math inline">\(\text{Rank}(A) = n\)</span>) and <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix. Then:</p>
<p><span class="math display">\[
\text{Rank}(AX) = \text{Rank}(X)
\]</span></p>
<p>Similarly, if <span class="math inline">\(B\)</span> is a <span class="math inline">\(p \times p\)</span> invertible matrix, then:</p>
<p><span class="math display">\[
\text{Rank}(XB) = \text{Rank}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the previous theorem, we know <span class="math inline">\(\text{Rank}(AX) \le \text{Rank}(X)\)</span>. Since <span class="math inline">\(A\)</span> is invertible, we can write <span class="math inline">\(X = A^{-1}(AX)\)</span>. Applying the theorem again: <span class="math display">\[
\text{Rank}(X) = \text{Rank}(A^{-1}(AX)) \le \text{Rank}(AX)
\]</span> Thus, <span class="math inline">\(\text{Rank}(AX) = \text{Rank}(X)\)</span>.</p>
</div>
</section>
<section id="rank-of-xx-and-xx" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="rank-of-xx-and-xx"><span class="header-section-number">2.3.5</span> Rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span></h3>
<p>The matrix <span class="math inline">\(X'X\)</span> (the Gram matrix) appears in the normal equations for least squares (<span class="math inline">\(X'X\beta = X'y\)</span>). Its properties are closely tied to <span class="math inline">\(X\)</span>.</p>
<div id="thm-rank-gram" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.10 (Rank of Gram Matrix)</strong></span> For any real matrix <span class="math inline">\(X\)</span>, the rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span> is the same as the rank of <span class="math inline">\(X\)</span> itself:</p>
<p><span class="math display">\[
\text{Rank}(X'X) = \text{Rank}(X)
\]</span> <span class="math display">\[
\text{Rank}(XX') = \text{Rank}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first show that the null space (kernel) of <span class="math inline">\(X\)</span> is the same as the null space of <span class="math inline">\(X'X\)</span>. If <span class="math inline">\(v \in \text{Ker}(X)\)</span>, then <span class="math inline">\(Xv = 0 \implies X'Xv = 0 \implies v \in \text{Ker}(X'X)\)</span>. Conversely, if <span class="math inline">\(v \in \text{Ker}(X'X)\)</span>, then <span class="math inline">\(X'Xv = 0\)</span>. Multiply by <span class="math inline">\(v'\)</span>: <span class="math display">\[
v'X'Xv = 0 \implies (Xv)'(Xv) = 0 \implies ||Xv||^2 = 0 \implies Xv = 0
\]</span> So <span class="math inline">\(\text{Ker}(X) = \text{Ker}(X'X)\)</span>. By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.</p>
</div>
<p><strong>Column Space of</strong> <span class="math inline">\(XX'\)</span></p>
<p>Beyond just the rank, the column spaces themselves are related.</p>
<div id="thm-colspace-gram" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.11 (Column Space Equivalence)</strong></span> The column space of <span class="math inline">\(XX'\)</span> is identical to the column space of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\text{Col}(XX') = \text{Col}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p><strong>Forward (</strong><span class="math inline">\(\subseteq\)</span>): Let <span class="math inline">\(z \in \text{Col}(XX')\)</span>. Then <span class="math inline">\(z = XX'w\)</span> for some vector <span class="math inline">\(w\)</span>. We can rewrite this as <span class="math inline">\(z = X(X'w)\)</span>. Since <span class="math inline">\(z\)</span> is a linear combination of columns of <span class="math inline">\(X\)</span> (with coefficients <span class="math inline">\(X'w\)</span>), <span class="math inline">\(z \in \text{Col}(X)\)</span>. Thus, <span class="math inline">\(\text{Col}(XX') \subseteq \text{Col}(X)\)</span>.</p></li>
<li><p><strong>Equality via Rank:</strong> From the previous theorem, we know that <span class="math inline">\(\text{Rank}(XX') = \text{Rank}(X)\)</span>. Since <span class="math inline">\(\text{Col}(XX')\)</span> is a subspace of <span class="math inline">\(\text{Col}(X)\)</span> and they have the same finite dimension (Rank), the subspaces must be identical.</p></li>
</ol>
</div>
<p><strong>Implication:</strong> This property ensures that for any <span class="math inline">\(y\)</span>, the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span> lies in the same space as the projection onto <span class="math inline">\(\text{Col}(XX')\)</span>. This is vital for the existence of solutions in generalized least squares.</p>
</section>
</section>
<section id="orthogonal-projection-onto-a-subspace" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="orthogonal-projection-onto-a-subspace"><span class="header-section-number">2.4</span> Orthogonal Projection onto a Subspace</h2>
<div name="Definition of Projection onto a Subspace $V$">
<p>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. For any vector <span class="math inline">\(y \in \mathbb{R}^n\)</span>, there exists a <strong>unique</strong> vector <span class="math inline">\(\hat{y} \in V\)</span> such that the residual is orthogonal to the subspace:</p>
<p><span class="math display">\[
(y - \hat{y}) \perp V
\]</span></p>
<p>Equivalently: <span class="math display">\[
\langle y - \hat{y}, v \rangle = 0 \quad \forall v \in V
\]</span></p>
</div>
<section id="equivalence-to-least-squares" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="equivalence-to-least-squares"><span class="header-section-number">2.4.1</span> Equivalence to Least Squares</h3>
<p>The geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).</p>
<div id="thm-best-approximation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.12 (Best Approximation Theorem (Least Squares Property))</strong></span> Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> and <span class="math inline">\(y \in \mathbb{R}^n\)</span>. Let <span class="math inline">\(\hat{y}\)</span> be the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V\)</span>. Then <span class="math inline">\(\hat{y}\)</span> is the closest point in <span class="math inline">\(V\)</span> to <span class="math inline">\(y\)</span>. That is, for any vector <span class="math inline">\(v \in V\)</span> such that <span class="math inline">\(v \ne \hat{y}\)</span>:</p>
<p><span class="math display">\[
\|y - \hat{y}\|^2 &lt; \|y - v\|^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(v\)</span> be any vector in <span class="math inline">\(V\)</span>. We can rewrite the difference vector <span class="math inline">\(y - v\)</span> by adding and subtracting the projection <span class="math inline">\(\hat{y}\)</span>: <span class="math display">\[
y - v = (y - \hat{y}) + (\hat{y} - v)
\]</span></p>
<p>Observe the properties of the two terms on the right-hand side:</p>
<ol type="1">
<li><strong>Residual:</strong> <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to <span class="math inline">\(V\)</span> by definition.</li>
<li><strong>Difference in Subspace:</strong> Since both <span class="math inline">\(\hat{y} \in V\)</span> and <span class="math inline">\(v \in V\)</span>, their difference <span class="math inline">\((\hat{y} - v)\)</span> is also in <span class="math inline">\(V\)</span>.</li>
</ol>
<p>Therefore, the two terms are orthogonal to each other: <span class="math display">\[
(y - \hat{y}) \perp (\hat{y} - v)
\]</span></p>
<p>Applying the Pythagorean Theorem: <span class="math display">\[
\|y - v\|^2 = \|y - \hat{y}\|^2 + \|\hat{y} - v\|^2
\]</span></p>
<p>Since squared norms are non-negative, and <span class="math inline">\(\|\hat{y} - v\|^2 &gt; 0\)</span> (because <span class="math inline">\(v \ne \hat{y}\)</span>): <span class="math display">\[
\|y - v\|^2 &gt; \|y - \hat{y}\|^2
\]</span> The projection <span class="math inline">\(\hat{y}\)</span> minimizes the squared error distance (and error distance itself).</p>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-3d-proof" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3d-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-3d-proof-1.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3d-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Visualization of the Best Approximation Theorem
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="uniqueness-of-projection" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="uniqueness-of-projection"><span class="header-section-number">2.4.2</span> Uniqueness of Projection</h3>
<p>While the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.</p>
<div id="thm-projection-uniqueness" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.13 (Uniqueness of Orthogonal Projection)</strong></span> For a given vector <span class="math inline">\(y\)</span> and subspace <span class="math inline">\(V\)</span>, the projection vector <span class="math inline">\(\hat{y}\)</span> satisfying <span class="math inline">\((y - \hat{y}) \perp V\)</span> is unique.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Assume there are two vectors <span class="math inline">\(\hat{y}_1 \in V\)</span> and <span class="math inline">\(\hat{y}_2 \in V\)</span> that both satisfy the orthogonality condition. <span class="math display">\[
(y - \hat{y}_1) \perp V \quad \text{and} \quad (y - \hat{y}_2) \perp V
\]</span> This means that for any <span class="math inline">\(v \in V\)</span>, both inner products are zero: <span class="math display">\[
\langle y - \hat{y}_1, v \rangle = 0
\]</span> <span class="math display">\[
\langle y - \hat{y}_2, v \rangle = 0
\]</span></p>
<p>Subtracting the second equation from the first: <span class="math display">\[
\langle y - \hat{y}_1, v \rangle - \langle y - \hat{y}_2, v \rangle = 0
\]</span> Using the linearity of the inner product: <span class="math display">\[
\langle (y - \hat{y}_1) - (y - \hat{y}_2), v \rangle = 0
\]</span> <span class="math display">\[
\langle \hat{y}_2 - \hat{y}_1, v \rangle = 0
\]</span></p>
<p>This equation holds for <strong>all</strong> <span class="math inline">\(v \in V\)</span>. Since <span class="math inline">\(\hat{y}_1\)</span> and <span class="math inline">\(\hat{y}_2\)</span> are both in <span class="math inline">\(V\)</span>, their difference <span class="math inline">\(d = \hat{y}_2 - \hat{y}_1\)</span> must also be in <span class="math inline">\(V\)</span>. We can therefore choose <span class="math inline">\(v = d = \hat{y}_2 - \hat{y}_1\)</span>. <span class="math display">\[
\langle \hat{y}_2 - \hat{y}_1, \hat{y}_2 - \hat{y}_1 \rangle = 0 \implies \|\hat{y}_2 - \hat{y}_1\|^2 = 0
\]</span> The only vector with a norm of zero is the zero vector itself. <span class="math display">\[
\hat{y}_2 - \hat{y}_1 = 0 \implies \hat{y}_1 = \hat{y}_2
\]</span> Thus, the projection is unique.</p>
</div>
</section>
</section>
<section id="projection-via-orthonormal-basis-q" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="projection-via-orthonormal-basis-q"><span class="header-section-number">2.5</span> Projection via Orthonormal Basis (<span class="math inline">\(Q\)</span>)</h2>
<section id="orthonomal-basis" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="orthonomal-basis"><span class="header-section-number">2.5.1</span> Orthonomal Basis</h3>
<p>Before discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.</p>
<div id="def-basis" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.22 (Basis)</strong></span> A set of vectors <span class="math inline">\(\{x_1, \dots, x_k\}\)</span> is a <strong>Basis</strong> for a vector space <span class="math inline">\(V\)</span> if:</p>
<ol type="1">
<li>The vectors span the space: <span class="math inline">\(V = L(x_1, \dots, x_k)\)</span>.</li>
<li>The vectors are linearly independent.</li>
</ol>
</div>
<p>The number of vectors in a basis is unique and is defined as the <strong>Dimension</strong> of <span class="math inline">\(V\)</span>.</p>
<p>Calculations become significantly simpler if we choose a basis with special geometric properties.</p>
<div id="def-orthonormal-basis" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.23 (Orthonormal Basis)</strong></span> A basis <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> is called an <strong>Orthonormal Basis</strong> if:</p>
<ol type="1">
<li><p><strong>Orthogonal:</strong> Each pair of vectors is perpendicular. <span class="math display">\[
q_i'q_j = 0 \quad \text{for } i \ne j
\]</span></p></li>
<li><p><strong>Normalized:</strong> Each vector has unit length. <span class="math display">\[
||q_i||^2 = q_i'q_i = 1
\]</span></p></li>
</ol>
<p>Combining these, we write <span class="math inline">\(q_i'q_j = \delta_{ij}\)</span> (Kronecker delta).</p>
</div>
<p>We now generalize the projection problem. Instead of projecting <span class="math inline">\(y\)</span> onto a single line, we project it onto a subspace <span class="math inline">\(V\)</span> of dimension <span class="math inline">\(k\)</span>.</p>
<p>If we have an orthonormal basis <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> for <span class="math inline">\(V\)</span>, the projection <span class="math inline">\(\hat{y}\)</span> is simply the sum of the projections onto the individual basis vectors.</p>
<div id="def-proj-orthonormal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.24 (Projection Defined with Orthonormal Basis)</strong></span> The projection of <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(V = L(q_1, \dots, q_k)\)</span> is:</p>
<p><span class="math display">\[
\hat{y} = \sum_{i=1}^k \text{proj}(y|q_i) = \sum_{i=1}^k (q_i'y) q_i
\]</span></p>
<p>Since the basis vectors are normalized, we do not need to divide by <span class="math inline">\(||q_i||^2\)</span>.</p>
</div>
<div id="thm-orthonormal-basis-proj" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.14 (Projection via Orthonormal Basis)</strong></span> Let <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> be an orthonormal basis for the subspace <span class="math inline">\(V \subseteq \mathbb{R}^n\)</span>. The vector defined by the sum of individual projections: <span class="math display">\[
\hat{y} = \sum_{i=1}^k \langle y, q_i \rangle q_i
\]</span> is indeed the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V\)</span>. That is, it satisfies <span class="math inline">\((y - \hat{y}) \perp V\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove this, we must check two conditions:</p>
<ol type="1">
<li><p><span class="math inline">\(\hat{y} \in V\)</span>: This is immediate because <span class="math inline">\(\hat{y}\)</span> is a linear combination of the basis vectors <span class="math inline">\(\{q_1, \dots, q_k\}\)</span>.</p></li>
<li><p><span class="math inline">\((y - \hat{y}) \perp V\)</span>: It suffices to show that the error vector <span class="math inline">\(e = y - \hat{y}\)</span> is orthogonal to every basis vector <span class="math inline">\(q_j\)</span> (for <span class="math inline">\(j = 1, \dots, k\)</span>).</p>
<p>Let’s calculate the inner product <span class="math inline">\(\langle y - \hat{y}, q_j \rangle\)</span>: <span class="math display">\[
\begin{aligned}
\langle y - \hat{y}, q_j \rangle &amp;= \langle y, q_j \rangle - \langle \hat{y}, q_j \rangle \\
&amp;= \langle y, q_j \rangle - \left\langle \sum_{i=1}^k \langle y, q_i \rangle q_i, q_j \right\rangle \\
&amp;= \langle y, q_j \rangle - \sum_{i=1}^k \langle y, q_i \rangle \underbrace{\langle q_i, q_j \rangle}_{\delta_{ij}}
\end{aligned}
\]</span></p>
<p>Since the basis is orthonormal, <span class="math inline">\(\langle q_i, q_j \rangle\)</span> is 1 if <span class="math inline">\(i=j\)</span> and 0 otherwise. Thus, the summation collapses to a single term where <span class="math inline">\(i=j\)</span>: <span class="math display">\[
\begin{aligned}
\langle y - \hat{y}, q_j \rangle &amp;= \langle y, q_j \rangle - \langle y, q_j \rangle \cdot 1 \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to every basis vector <span class="math inline">\(q_j\)</span>, it is orthogonal to the entire subspace <span class="math inline">\(V\)</span>. Thus, <span class="math inline">\(\hat{y}\)</span> is the unique orthogonal projection.</p></li>
</ol>
</div>
</section>
<section id="projection-matrix-via-orthonomal-basis-q" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="projection-matrix-via-orthonomal-basis-q"><span class="header-section-number">2.5.2</span> Projection Matrix via Orthonomal Basis (<span class="math inline">\(Q\)</span>)</h3>
<p><strong>Matrix Form with Orthonormal Basis</strong></p>
<p>We can express the summation formula for <span class="math inline">\(\hat{y}\)</span> compactly using matrix notation.</p>
<p>Let <span class="math inline">\(Q\)</span> be an <span class="math inline">\(n \times k\)</span> matrix whose columns are the orthonormal basis vectors <span class="math inline">\(q_1, \dots, q_k\)</span>. <span class="math display">\[
Q = \begin{pmatrix} q_1 &amp; q_2 &amp; \dots &amp; q_k \end{pmatrix}
\]</span></p>
<p>Properties of <span class="math inline">\(Q\)</span>:</p>
<ul>
<li><span class="math inline">\(Q'Q = I_k\)</span> (Identity matrix of size <span class="math inline">\(k \times k\)</span>).</li>
<li><span class="math inline">\(QQ'\)</span> is <strong>not</strong> necessarily <span class="math inline">\(I_n\)</span> (unless <span class="math inline">\(k=n\)</span>).</li>
</ul>
<div id="def-proj-matrix-orthonormal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.25 (Projection Matrix in Terms of <span class="math inline">\(Q\)</span>)</strong></span> The projection <span class="math inline">\(\hat{y}\)</span> can be written as:</p>
<p><span class="math display">\[
\hat{y} = \begin{pmatrix} q_1 &amp; \dots &amp; q_k \end{pmatrix} \begin{pmatrix} q_1'y \\ \vdots \\ q_k'y \end{pmatrix} = Q (Q'y) = (QQ') y
\]</span></p>
<p>Thus, the projection matrix <span class="math inline">\(P\)</span> onto the subspace <span class="math inline">\(V\)</span> is: <span class="math display">\[
P = QQ'
\]</span></p>
</div>
<p><strong>Properties of Projection Matrices</strong></p>
<p>We have defined the projection matrix as <span class="math inline">\(P = X(X'X)^{-1}X'\)</span> (or <span class="math inline">\(P=QQ'\)</span> for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.</p>
<div id="thm-projection-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.15 (Symmeticity and Idempotence)</strong></span> A square matrix <span class="math inline">\(P\)</span> represents an orthogonal projection onto some subspace if and only if it satisfies:</p>
<ol type="1">
<li><strong>Idempotence:</strong> <span class="math inline">\(P^2 = P\)</span> (Applying the projection twice is the same as applying it once).</li>
<li><strong>Symmetry:</strong> <span class="math inline">\(P' = P\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(\hat{y} = Py\)</span> is already in the subspace <span class="math inline">\(\text{Col}(X)\)</span>, then projecting it again should not change it. <span class="math display">\[
P(Py) = Py \implies P^2 y = Py \quad \forall y
\]</span> Thus, <span class="math inline">\(P^2 = P\)</span>.</p>
</div>
<p><strong>Example: ANOVA (Analysis of Variance)</strong></p>
<p>One of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.</p>
<div id="exm-anova-projection" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Finding Projection for One-way ANOVA)</strong></span> Consider a one-way ANOVA model with <span class="math inline">\(k\)</span> groups: <span class="math display">\[
y_{ij} = \mu_i + \epsilon_{ij}
\]</span> where <span class="math inline">\(i \in \{1, \dots, k\}\)</span> represents the group and <span class="math inline">\(j \in \{1, \dots, n_i\}\)</span> represents the observation within the group. Let <span class="math inline">\(N = \sum_{i=1}^k n_i\)</span> be the total number of observations.</p>
<p><strong>1. Matrix Definitions</strong> We define the data vector <span class="math inline">\(y\)</span> and the design matrix <span class="math inline">\(X\)</span> as follows:</p>
<ul>
<li><p><strong>Data Vector (</strong><span class="math inline">\(y\)</span>): An <span class="math inline">\(N \times 1\)</span> vector containing all observations stacked by group: <span class="math display">\[
  y = \begin{pmatrix} y_{11} \\ \vdots \\ y_{1n_1} \\ y_{21} \\ \vdots \\ y_{kn_k} \end{pmatrix}
  \]</span></p></li>
<li><p><strong>Design Matrix (</strong><span class="math inline">\(X\)</span>): An <span class="math inline">\(N \times k\)</span> matrix constructed from <span class="math inline">\(k\)</span> column vectors, <span class="math inline">\(X = (x_1, x_2, \dots, x_k)\)</span>. Each vector <span class="math inline">\(x_g\)</span> is an <strong>indicator variable</strong> (dummy variable) for group <span class="math inline">\(g\)</span>: <span class="math display">\[
  x_g = \begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix} \quad \leftarrow \text{Entries are 1 if observation belongs to group } g
  \]</span></p></li>
</ul>
<p><strong>2. Orthogonality</strong> These column vectors <span class="math inline">\(x_1, \dots, x_k\)</span> are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero: <span class="math display">\[
\langle x_g, x_h \rangle = 0 \quad \text{for } g \neq h
\]</span> This allows us to find the projection onto the column space of <span class="math inline">\(X\)</span> by simply summing the projections onto each column individually.</p>
<p><strong>3. Calculating Individual Projections</strong> For a specific group vector <span class="math inline">\(x_g\)</span>, the projection is: <span class="math display">\[
\text{proj}(y|x_g) = \frac{\langle y, x_g \rangle}{\langle x_g, x_g \rangle} x_g
\]</span></p>
<p>We calculate the two scalar terms:</p>
<ul>
<li><p><strong>Denominator (</strong><span class="math inline">\(\langle x_g, x_g \rangle\)</span>): The sum of squared elements of <span class="math inline">\(x_g\)</span>. Since <span class="math inline">\(x_g\)</span> contains <span class="math inline">\(n_g\)</span> ones and zeros elsewhere: <span class="math display">\[
  \langle x_g, x_g \rangle = \sum \mathbb{1}_{\{i=g\}}^2 = n_g
  \]</span></p></li>
<li><p><strong>Numerator (</strong><span class="math inline">\(\langle y, x_g \rangle\)</span>): The dot product sums only the <span class="math inline">\(y\)</span> values belonging to group <span class="math inline">\(g\)</span>: <span class="math display">\[
  \langle y, x_g \rangle = \sum_{i,j} y_{ij} \cdot \mathbb{1}_{\{i=g\}} = \sum_{j=1}^{n_g} y_{gj} = y_{g.} \quad (\text{Group Total})
  \]</span></p></li>
</ul>
<p><strong>4. The Resulting Projection</strong> Substituting these back into the formula gives the coefficient for the vector <span class="math inline">\(x_g\)</span>: <span class="math display">\[
\text{proj}(y|x_g) = \frac{y_{g.}}{n_g} x_g = \bar{y}_{g.} x_g
\]</span></p>
<p>The total projection <span class="math inline">\(\hat{y}\)</span> is the sum over all groups: <span class="math display">\[
\hat{y} = \sum_{g=1}^k \bar{y}_{g.} x_g
\]</span> This confirms that the fitted value for any specific observation <span class="math inline">\(y_{ij}\)</span> is simply its group mean <span class="math inline">\(\bar{y}_{i.}\)</span>.</p>
</div>
</section>
<section id="gram-schmidt-process" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="gram-schmidt-process"><span class="header-section-number">2.5.3</span> Gram-Schmidt Process</h3>
<p>To use the simplified formula <span class="math inline">\(P = QQ'\)</span>, we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.</p>
<div class="algorithm">
<p><strong>Gram-Schmidt Process</strong> Given linearly independent vectors <span class="math inline">\(x_1, \dots, x_p\)</span>:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Normalize the first vector. <span class="math display">\[
q_1 = \frac{x_1}{||x_1||}
\]</span></p></li>
<li><p><strong>Step 2:</strong> Project <span class="math inline">\(x_2\)</span> onto <span class="math inline">\(q_1\)</span> and subtract it to find the orthogonal component. <span class="math display">\[
v_2 = x_2 - (x_2'q_1)q_1
\]</span> Then normalize: <span class="math display">\[
q_2 = \frac{v_2}{||v_2||}
\]</span></p></li>
<li><p><strong>Step k:</strong> Subtract the projections onto all previous <span class="math inline">\(q\)</span> vectors. <span class="math display">\[
v_k = x_k - \sum_{j=1}^{k-1} (x_k'q_j)q_j
\]</span> <span class="math display">\[
q_k = \frac{v_k}{||v_k||}
\]</span></p></li>
</ol>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gram-schmidt-python" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gram-schmidt-python-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-gram-schmidt-python-3.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gram-schmidt-python-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Gram-Schmidt Process: Projecting <span class="math inline">\(x_2\)</span> onto <span class="math inline">\(x_1\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>This process leads to the <strong>QR Decomposition</strong> of a matrix: <span class="math inline">\(X = QR\)</span>, where <span class="math inline">\(Q\)</span> is orthogonal and <span class="math inline">\(R\)</span> is upper triangular.</p>
</section>
</section>
<section id="hat-matrix-projection-matrix-via-x" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="hat-matrix-projection-matrix-via-x"><span class="header-section-number">2.6</span> Hat Matrix (Projection Matrix via <span class="math inline">\(X\)</span>)</h2>
<section id="norm-equations" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="norm-equations"><span class="header-section-number">2.6.1</span> Norm Equations</h3>
<p>Let <span class="math inline">\(X = (x_1, \dots, x_p)\)</span> be an <span class="math inline">\(n \times p\)</span> matrix, where each column <span class="math inline">\(x_j\)</span> is a predictor vector.</p>
<p>We want to project the target vector <span class="math inline">\(y\)</span> onto the column space <span class="math inline">\(\text{Col}(X)\)</span>. This is equivalent to finding a coefficient vector <span class="math inline">\(\beta \in \mathbb{R}^p\)</span> such that the error vector (residual) is orthogonal to the entire subspace <span class="math inline">\(\text{Col}(X)\)</span>.</p>
<p><span class="math display">\[
y - X\beta \perp \text{Col}(X)
\]</span></p>
<p>Since the columns of <span class="math inline">\(X\)</span> span the subspace, the residual must be orthogonal to <strong>every</strong> column vector <span class="math inline">\(x_j\)</span> individually:</p>
<p><span class="math display">\[
y - X\beta \perp x_j \quad \text{for } j = 1, \dots, p
\]</span></p>
<p>Writing this geometric condition as an algebraic dot product (where <span class="math inline">\(x_j'\)</span> denotes the transpose):</p>
<p><span class="math display">\[
x_j'(y - X\beta) = 0 \quad \text{for each } j
\]</span></p>
<p>We can stack these <span class="math inline">\(p\)</span> separate linear equations into a single matrix equation. Since the rows of <span class="math inline">\(X'\)</span> are the columns of <span class="math inline">\(X\)</span>, this becomes:</p>
<p><span class="math display">\[
\begin{pmatrix} x_1' \\ \vdots \\ x_p' \end{pmatrix} (y - X\beta) = \mathbf{0}
\implies X'(y - X\beta) = 0
\]</span></p>
<p>Finally, we distribute the matrix transpose and rearrange terms to solve for <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
X'y - X'X\beta &amp;= 0 \\
X'X\beta &amp;= X'y
\end{aligned}
\]</span></p>
<p>This system is known as the <strong>Normal Equations</strong>.</p>
<div id="thm-least-squares-estimator" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.16 (Least Squares Estimator)</strong></span> If <span class="math inline">\(X'X\)</span> is invertible (i.e., <span class="math inline">\(X\)</span> has full column rank), the unique solution for <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'y
\]</span></p>
</div>
</section>
<section id="hat-matrix" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="hat-matrix"><span class="header-section-number">2.6.2</span> Hat Matrix</h3>
<p>Substituting the estimator <span class="math inline">\(\hat{\beta}\)</span> back into the equation for <span class="math inline">\(\hat{y}\)</span> gives us the projection matrix.</p>
<div id="def-projection-matrix-general" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.26 (Hat Matrix)</strong></span> The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span> is given by:</p>
<p><span class="math display">\[
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y
\]</span></p>
<p>Thus, the hat matrix <span class="math inline">\(H\)</span> is defined as:</p>
<p><span class="math display">\[
H = X(X'X)^{-1}X'
\]</span></p>
</div>
</section>
<section id="equivalence-of-hat-matrix-and-qq" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="equivalence-of-hat-matrix-and-qq"><span class="header-section-number">2.6.3</span> Equivalence of Hat Matrix and <span class="math inline">\(QQ'\)</span></h3>
<p>If we use the QR decomposition such that <span class="math inline">\(X = QR\)</span>, where the columns of <span class="math inline">\(Q\)</span> form an orthonormal basis for <span class="math inline">\(\text{Col}(X)\)</span>, the formula simplifies significantly.</p>
<p>Recall that for orthonormal columns, <span class="math inline">\(Q'Q = I\)</span>. Substituting <span class="math inline">\(X=QR\)</span> into the general formula:</p>
<p><span class="math display">\[
\begin{aligned}
H &amp;= QR((QR)'(QR))^{-1}(QR)' \\
  &amp;= QR(R'Q'QR)^{-1}R'Q' \\
  &amp;= QR(R' \underbrace{Q'Q}_{I} R)^{-1}R'Q' \\
  &amp;= QR(R'R)^{-1}R'Q' \\
  &amp;= QR R^{-1} (R')^{-1} R' Q' \\
  &amp;= Q \underbrace{R R^{-1}}_{I} \underbrace{(R')^{-1} R'}_{I} Q' \\
  &amp;= Q Q'
\end{aligned}
\]</span></p>
<p>This confirms that <span class="math inline">\(H = QQ'\)</span> is consistent with the general formula <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>.</p>
</section>
<section id="properties-of-hat-matrix" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="properties-of-hat-matrix"><span class="header-section-number">2.6.4</span> Properties of Hat Matrix</h3>
<p>We revisit the properties of projection matrices in this general context.</p>
<div id="thm-projection-properties-revisited" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.17 (Properties of Hat Matrix)</strong></span> The matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span> satisfies:</p>
<ol type="1">
<li><strong>Symmetric:</strong> <span class="math inline">\(H' = H\)</span></li>
<li><strong>Idempotent:</strong> <span class="math inline">\(H^2 = H\)</span></li>
<li><strong>Trace:</strong> The trace of a projection matrix equals the dimension of the subspace it projects onto. <span class="math display">\[
\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_p) = p
\]</span></li>
</ol>
</div>
</section>
</section>
<section id="projection-defined-with-orthogonal-projection-matrix" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="projection-defined-with-orthogonal-projection-matrix"><span class="header-section-number">2.7</span> Projection Defined with Orthogonal Projection Matrix</h2>
<p>Projection don’t have to be defined with a subspace or a matrix <span class="math inline">\(X\)</span> as we discussed before. Projection matrix is a self-contained definition of the subspace it projects onto.</p>
<section id="orthogonal-projection-matrix" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="orthogonal-projection-matrix"><span class="header-section-number">2.7.1</span> Orthogonal Projection Matrix</h3>
<div id="def-proj-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.27 (Orthogonal Projection Matrix)</strong></span> A square matrix <span class="math inline">\(P\)</span> is called an <strong>orthogonal projection matrix</strong> if it satisfies two conditions:</p>
<ol type="1">
<li><strong>Symmetry:</strong> <span class="math inline">\(P^\top = P\)</span></li>
<li><strong>Idempotency:</strong> <span class="math inline">\(P^2 = P\)</span></li>
</ol>
</div>
<div id="thm-proj-col" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.18 (Projection onto Column Space)</strong></span> If a matrix <span class="math inline">\(P\)</span> is symmetric and idempotent, then <span class="math inline">\(P\)</span> represents the orthogonal projection onto its column space, <span class="math inline">\(\text{Col}(P)\)</span>.</p>
<p>Specifically, for any vector <span class="math inline">\(y\)</span>, the vector <span class="math inline">\(\hat{y} = Py\)</span> is the unique vector in <span class="math inline">\(\text{Col}(P)\)</span> such that the residual <span class="math inline">\(e = y - \hat{y}\)</span> is orthogonal to <span class="math inline">\(\text{Col}(P)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(y \in \mathbb{R}^n\)</span>. We decompose <span class="math inline">\(y\)</span> as <span class="math inline">\(y = Py + (I - P)y\)</span>. We must show that the residual term <span class="math inline">\((I-P)y\)</span> is orthogonal to any vector <span class="math inline">\(z \in \text{Col}(P)\)</span>.</p>
<p>Since <span class="math inline">\(z \in \text{Col}(P)\)</span>, there exists a vector <span class="math inline">\(x\)</span> such that <span class="math inline">\(z = Px\)</span>. The inner product between <span class="math inline">\(z\)</span> and the residual is: <span id="eq-inner-prod"><span class="math display">\[
\langle z, (I - P)y \rangle = z^\top (I - P)y = (Px)^\top (I - P)y
\tag{2.1}\]</span></span></p>
<p>Using the matrix transpose property <span class="math inline">\((AB)^\top = B^\top A^\top\)</span>, we rewrite <a href="#eq-inner-prod" class="quarto-xref">Equation&nbsp;<span>2.1</span></a> as: <span id="eq-transpose-step"><span class="math display">\[
\langle z, (I - P)y \rangle = x^\top P^\top (I - P)y
\tag{2.2}\]</span></span></p>
<p>Since <span class="math inline">\(P\)</span> is symmetric (<span class="math inline">\(P^\top = P\)</span>), we can substitute <span class="math inline">\(P\)</span> for <span class="math inline">\(P^\top\)</span> in <a href="#eq-transpose-step" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>: <span id="eq-sym-step"><span class="math display">\[
\langle z, (I - P)y \rangle = x^\top P (I - P)y = x^\top (P - P^2)y
\tag{2.3}\]</span></span></p>
<p>Finally, utilizing the idempotency of <span class="math inline">\(P\)</span> (where <span class="math inline">\(P^2 = P\)</span>), the expression in <a href="#eq-sym-step" class="quarto-xref">Equation&nbsp;<span>2.3</span></a> simplifies to 0: <span id="eq-final-step"><span class="math display">\[
x^\top (P - P)y = x^\top (0)y = 0
\tag{2.4}\]</span></span></p>
<p>Since the inner product is 0, the residual is orthogonal to every vector in <span class="math inline">\(\text{Col}(P)\)</span>. Thus, <span class="math inline">\(P\)</span> is the orthogonal projector.</p>
</div>
</section>
<section id="projection-onto-complement-space" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="projection-onto-complement-space"><span class="header-section-number">2.7.2</span> Projection onto Complement Space</h3>
<div id="thm-complement-proj" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.19 (Projection onto Orthogonal Complement)</strong></span> Let <span class="math inline">\(P\)</span> be an orthogonal projection matrix. The matrix <span class="math inline">\(M\)</span> defined as: <span class="math display">\[
M = I - P
\]</span> is the orthogonal projection matrix onto the orthogonal complement of the column space of <span class="math inline">\(P\)</span>, denoted <span class="math inline">\(\text{Col}(P)^\perp\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Symmetry and Idempotency</strong> Since <span class="math inline">\(P\)</span> is a projection matrix, <span class="math inline">\(P^\top = P\)</span> and <span class="math inline">\(P^2 = P\)</span>. We verify these properties for <span class="math inline">\(M\)</span>: <span id="eq-m-sym"><span class="math display">\[
M^\top = (I - P)^\top = I - P^\top = I - P = M
\tag{2.5}\]</span></span> <span id="eq-m-idemp"><span class="math display">\[
M^2 = (I - P)(I - P) = I - 2P + P^2 = I - 2P + P = I - P = M
\tag{2.6}\]</span></span> By <a href="#eq-m-sym" class="quarto-xref">Equation&nbsp;<span>2.5</span></a> and <a href="#eq-m-idemp" class="quarto-xref">Equation&nbsp;<span>2.6</span></a>, <span class="math inline">\(M\)</span> is symmetric and idempotent, so it is an orthogonal projection matrix.</p>
<p><strong>2. Identifying the Subspace</strong> By <a href="#thm-proj-col" class="quarto-xref">Theorem&nbsp;<span>2.18</span></a>, <span class="math inline">\(M\)</span> projects onto its own column space, <span class="math inline">\(\text{Col}(M)\)</span>. A vector <span class="math inline">\(v\)</span> is in <span class="math inline">\(\text{Col}(M)\)</span> if and only if it is fixed by the projection (<span class="math inline">\(Mv = v\)</span>). <span id="eq-fixed-point"><span class="math display">\[
Mv = v
\tag{2.7}\]</span></span></p>
<p>Substituting <span class="math inline">\(M = I - P\)</span> into <a href="#eq-fixed-point" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> gives: <span id="eq-sub-m"><span class="math display">\[
(I - P)v = v
\tag{2.8}\]</span></span></p>
<p>Rearranging <a href="#eq-sub-m" class="quarto-xref">Equation&nbsp;<span>2.8</span></a>, we find the condition for <span class="math inline">\(v\)</span>: <span id="eq-nullspace"><span class="math display">\[
v - Pv = v \implies Pv = 0
\tag{2.9}\]</span></span></p>
<p>The condition <span class="math inline">\(Pv = 0\)</span> in <a href="#eq-nullspace" class="quarto-xref">Equation&nbsp;<span>2.9</span></a> implies that <span class="math inline">\(v\)</span> belongs to the null space of <span class="math inline">\(P\)</span>, denoted <span class="math inline">\(\text{Null}(P)\)</span>. By the Fundamental Theorem of Linear Algebra for symmetric matrices, the null space is the orthogonal complement of the column space: <span class="math display">\[
\text{Null}(P) = \text{Col}(P^\top)^\perp = \text{Col}(P)^\perp
\]</span> Thus, the image of <span class="math inline">\(M\)</span> is exactly <span class="math inline">\(\text{Col}(P)^\perp\)</span>.</p>
</div>
<div id="exr-col-spaces" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1 (Column Space of the Hat Matrix)</strong></span> Let <span class="math inline">\(H = X(X^\top X)^{-1}X^\top\)</span> be the hat matrix.</p>
<ol type="1">
<li>Prove that the column space of <span class="math inline">\(H\)</span> is identical to the column space of <span class="math inline">\(X\)</span>: <span class="math display">\[ \text{Col}(H) = \text{Col}(X) \]</span></li>
<li>Using the result above, show that the column space of the residual maker matrix <span class="math inline">\(M = I - H\)</span> is the orthogonal complement of <span class="math inline">\(\text{Col}(X)\)</span>: <span class="math display">\[ \text{Col}(M) = \text{Col}(X)^\perp \]</span></li>
</ol>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="Solutions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solutions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>1. Equivalence of Column Spaces</strong> To prove <span class="math inline">\(\text{Col}(H) = \text{Col}(X)\)</span>, we show inclusion in both directions.</p>
<ul>
<li><p><strong>Forward (</strong><span class="math inline">\(\text{Col}(H) \subseteq \text{Col}(X)\)</span>): By definition, <span class="math inline">\(H = X [(X^\top X)^{-1}X^\top]\)</span>. Any column of <span class="math inline">\(H\)</span> is a linear combination of the columns of <span class="math inline">\(X\)</span> (weighted by the matrix in brackets). Therefore, any vector in the image of <span class="math inline">\(H\)</span> must lie in <span class="math inline">\(\text{Col}(X)\)</span>.</p></li>
<li><p><strong>Reverse (</strong><span class="math inline">\(\text{Col}(X) \subseteq \text{Col}(H)\)</span>): Take any vector <span class="math inline">\(v \in \text{Col}(X)\)</span>. By definition, <span class="math inline">\(v = Xb\)</span> for some vector <span class="math inline">\(b\)</span>. Apply <span class="math inline">\(H\)</span> to <span class="math inline">\(v\)</span>: <span class="math display">\[
  Hv = X(X^\top X)^{-1}X^\top (Xb) = X(X^\top X)^{-1}(X^\top X)b = X(I)b = Xb = v
  \]</span> Since <span class="math inline">\(Hv = v\)</span>, the vector <span class="math inline">\(v\)</span> lies in the column space of <span class="math inline">\(H\)</span> (specifically, it is an eigenvector with eigenvalue 1).</p></li>
</ul>
<p>Since both inclusions hold, <span class="math inline">\(\text{Col}(H) = \text{Col}(X)\)</span>.</p>
<p><strong>2. Orthogonal Complements</strong> From part 1, we know the subspaces are identical. Therefore, their orthogonal complements must also be identical: <span class="math display">\[
\text{Col}(H)^\perp = \text{Col}(X)^\perp
\]</span> We previously established in <a href="#thm-complement-proj" class="quarto-xref">Theorem&nbsp;<span>2.19</span></a> that for any projection matrix <span class="math inline">\(P\)</span>, the complement projection <span class="math inline">\(M = I - P\)</span> projects onto <span class="math inline">\(\text{Col}(P)^\perp\)</span>. Substituting <span class="math inline">\(H\)</span> for <span class="math inline">\(P\)</span>: <span class="math display">\[
\text{Col}(M) = \text{Col}(H)^\perp
\]</span> Combining these results gives the required equality: <span class="math display">\[
\text{Col}(M) = \text{Col}(X)^\perp
\]</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="projection-onto-nested-subspaces" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="projection-onto-nested-subspaces"><span class="header-section-number">2.8</span> Projection onto Nested Subspaces</h2>
<section id="nested-models-and-subspaces" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="nested-models-and-subspaces"><span class="header-section-number">2.8.1</span> Nested Models and Subspaces</h3>
<p>In hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.</p>
<div id="def-nested-models" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.28 (Nested Models)</strong></span> Consider two models:</p>
<ol type="1">
<li><strong>Reduced Model (</strong><span class="math inline">\(M_0\)</span>): <span class="math inline">\(y \in \text{Col}(X_0)\)</span></li>
<li><strong>Full Model (</strong><span class="math inline">\(M_1\)</span>): <span class="math inline">\(y \in \text{Col}(X_1)\)</span></li>
</ol>
<p>We say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: <span class="math display">\[
\text{Col}(X_0) \subseteq \text{Col}(X_1)
\]</span></p>
</div>
<p>Usually, <span class="math inline">\(X_1\)</span> is constructed by adding columns to <span class="math inline">\(X_0\)</span>: <span class="math inline">\(X_1 = [X_0, X_{\text{new}}]\)</span>.</p>
</section>
<section id="projections-onto-nested-subspaces" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="projections-onto-nested-subspaces"><span class="header-section-number">2.8.2</span> Projections onto Nested Subspaces</h3>
<p>Let <span class="math inline">\(P_0\)</span> be the projection matrix onto <span class="math inline">\(\text{Col}(X_0)\)</span> and <span class="math inline">\(P_1\)</span> be the projection matrix onto <span class="math inline">\(\text{Col}(X_1)\)</span>. Since <span class="math inline">\(\text{Col}(X_0) \subseteq \text{Col}(X_1)\)</span>, we have important relationships between these matrices.</p>
<div id="thm-nested-projections" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.20 (Composition of Projections)</strong></span> If <span class="math inline">\(\text{Col}(P_0) \subseteq \text{Col}(P_1)\)</span>, then:</p>
<ol type="1">
<li><span class="math inline">\(P_1 P_0 = P_0\)</span> (Projecting onto the small space, then the large space, keeps you in the small space).</li>
<li><span class="math inline">\(P_0 P_1 = P_0\)</span> (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Proof of</strong> <span class="math inline">\(P_1 P_0 = P_0\)</span>: For any vector <span class="math inline">\(y \in \mathbb{R}^n\)</span>, the vector <span class="math inline">\(v = P_0 y\)</span> lies in <span class="math inline">\(\text{Col}(X_0)\)</span>. Since <span class="math inline">\(\text{Col}(X_0) \subseteq \text{Col}(X_1)\)</span>, the vector <span class="math inline">\(v\)</span> also lies in <span class="math inline">\(\text{Col}(X_1)\)</span>. A projection matrix <span class="math inline">\(P_1\)</span> acts as the identity operator for any vector already in its column space. Therefore, <span class="math inline">\(P_1 v = v\)</span>. Substituting <span class="math inline">\(v = P_0 y\)</span>, we get <span class="math inline">\(P_1 P_0 y = P_0 y\)</span> for all <span class="math inline">\(y\)</span>. Thus, <span class="math inline">\(P_1 P_0 = P_0\)</span>.</p>
<p><strong>2. Proof of</strong> <span class="math inline">\(P_0 P_1 = P_0\)</span>: Take the transpose of the previous result (<span class="math inline">\(P_1 P_0 = P_0\)</span>). <span class="math display">\[
(P_1 P_0)' = P_0'
\]</span> Using the property that projection matrices are symmetric (<span class="math inline">\(P' = P\)</span>): <span class="math display">\[
P_0' P_1' = P_0' \implies P_0 P_1 = P_0
\]</span></p>
</div>
<p><strong>Difference of Projections</strong></p>
<p>The difference between the two projection matrices, <span class="math inline">\(P_1 - P_0\)</span>, is itself a projection matrix.</p>
<div id="thm-diff-projection" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.21 (Difference Projection)</strong></span> The matrix <span class="math inline">\(P_{\Delta} = P_1 - P_0\)</span> is an orthogonal projection matrix onto the subspace <span class="math inline">\(\text{Col}(X_1) \cap \text{Col}(X_0)^\perp\)</span>. This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.</p>
<p><strong>Properties:</strong></p>
<ol type="1">
<li><strong>Symmetric:</strong> <span class="math inline">\((P_1 - P_0)' = P_1 - P_0\)</span>.</li>
<li><strong>Idempotent:</strong> <span class="math inline">\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\)</span>.</li>
<li><strong>Orthogonality:</strong> <span class="math inline">\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Symmetry:</strong> Since <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_0\)</span> are symmetric: <span class="math inline">\((P_1 - P_0)' = P_1' - P_0' = P_1 - P_0\)</span>.</p>
<p><strong>2. Idempotency:</strong> <span class="math display">\[
\begin{aligned}
(P_1 - P_0)^2 &amp;= (P_1 - P_0)(P_1 - P_0) \\
&amp;= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2
\end{aligned}
\]</span> Using the projection properties (<span class="math inline">\(P^2=P\)</span>) and the nested property (<span class="math inline">\(P_1 P_0 = P_0\)</span> and <span class="math inline">\(P_0 P_1 = P_0\)</span>): <span class="math display">\[
= P_1 - P_0 - P_0 + P_0 = P_1 - P_0
\]</span></p>
<p><strong>3. Orthogonality to</strong> <span class="math inline">\(P_0\)</span>: <span class="math display">\[
(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0
\]</span> Since <span class="math inline">\((P_1 - P_0)\)</span> is symmetric and idempotent, it is an orthogonal projection matrix. Since it is orthogonal to <span class="math inline">\(P_0\)</span> (the space of <span class="math inline">\(M_0\)</span>) but is derived from <span class="math inline">\(P_1\)</span>, it projects onto the subspace of <span class="math inline">\(M_1\)</span> that is orthogonal to <span class="math inline">\(M_0\)</span>.</p>
</div>
</section>
<section id="decomposition-of-projections-and-their-sum-squares" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="decomposition-of-projections-and-their-sum-squares"><span class="header-section-number">2.8.3</span> Decomposition of Projections and their Sum Squares</h3>
<div id="thm-nested-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.22 (Orthogonal Decomposition)</strong></span> Let <span class="math inline">\(M_0 \subset M_1\)</span> be two nested linear models with corresponding design matrices <span class="math inline">\(X_0\)</span> and <span class="math inline">\(X_1\)</span> such that <span class="math inline">\(\text{Col}(X_0) \subset \text{Col}(X_1)\)</span>. Let <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span> be the orthogonal projection matrices onto <span class="math inline">\(\text{Col}(X_0)\)</span> and <span class="math inline">\(\text{Col}(X_1)\)</span> respectively.</p>
<p>For any observation vector <span class="math inline">\(y\)</span>, we have the decomposition: <span class="math display">\[
y = \underbrace{P_0 y}_{\hat{y}_0} + \underbrace{(P_1 - P_0) y}_{\hat{y}_1 - \hat{y}_0} + \underbrace{(I - P_1) y}_{y - \hat{y}_1}
\]</span></p>
<p><strong>Geometric Interpretation:</strong></p>
<ol type="1">
<li><span class="math inline">\(\hat{y}_0 \in \text{Col}(X_0)\)</span>: The fit of the reduced model.</li>
<li><span class="math inline">\((\hat{y}_1 - \hat{y}_0) \in \text{Col}(X_0)^\perp \cap \text{Col}(X_1)\)</span>: The additional fit provided by <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_0\)</span>.</li>
<li><span class="math inline">\((y - \hat{y}_1) \in \text{Col}(X_1)^\perp\)</span>: The projection of <span class="math inline">\(y\)</span> onto the <strong>orthogonal complement</strong> of <span class="math inline">\(\text{Col}(X_1)\)</span>.</li>
</ol>
<p>The three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm: <span class="math display">\[
\|y\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2 + \|y - \hat{y}_1\|^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Definitions</strong> We define the three components as vectors <span class="math inline">\(v_1, v_2, v_3\)</span>:</p>
<ul>
<li><span class="math inline">\(v_1 = \hat{y}_0 = P_0 y\)</span>.</li>
<li><span class="math inline">\(v_2 = \hat{y}_1 - \hat{y}_0 = (P_1 - P_0)y\)</span>.</li>
<li><span class="math inline">\(v_3 = y - \hat{y}_1 = (I - P_1)y\)</span>.
<ul>
<li><strong>Note:</strong> Since <span class="math inline">\(P_1\)</span> projects onto <span class="math inline">\(\text{Col}(X_1)\)</span>, the matrix <span class="math inline">\((I - P_1)\)</span> projects onto the <strong>orthogonal complement</strong> <span class="math inline">\(\text{Col}(X_1)^\perp\)</span>. Thus, <span class="math inline">\(v_3 \in \text{Col}(I - P_1)\)</span>.</li>
</ul></li>
</ul>
<p>Note that since <span class="math inline">\(\text{Col}(X_0) \subset \text{Col}(X_1)\)</span>, we have the property <span class="math inline">\(P_1 P_0 = P_0 P_1 = P_0\)</span>. (Projecting onto the smaller subspace <span class="math inline">\(M_0\)</span> is unchanged if we first project onto the enclosing subspace <span class="math inline">\(M_1\)</span>).</p>
<p><strong>2. Orthogonality of</strong> <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> We check the inner product <span class="math inline">\(\langle v_1, v_2 \rangle = v_1' v_2\)</span>: <span class="math display">\[
\begin{aligned}
v_1' v_2 &amp;= (P_0 y)' (P_1 - P_0) y \\
&amp;= y' P_0' (P_1 - P_0) y \\
&amp;= y' (P_0 P_1 - P_0^2) y \quad (\text{Since } P_0 \text{ is symmetric}) \\
&amp;= y' (P_0 - P_0) y \quad (\text{Since } P_0 P_1 = P_0 \text{ and } P_0^2 = P_0) \\
&amp;= 0
\end{aligned}
\]</span></p>
<p><strong>3. Orthogonality of</strong> <span class="math inline">\((v_1 + v_2)\)</span> and <span class="math inline">\(v_3\)</span> Note that <span class="math inline">\(v_1 + v_2 = P_1 y = \hat{y}_1\)</span>. We check if the total fit <span class="math inline">\(\hat{y}_1\)</span> is orthogonal to the residual <span class="math inline">\(v_3\)</span>: <span class="math display">\[
\begin{aligned}
\hat{y}_1' v_3 &amp;= (P_1 y)' (I - P_1) y \\
&amp;= y' P_1 (I - P_1) y \\
&amp;= y' (P_1 - P_1^2) y \\
&amp;= y' (P_1 - P_1) y \\
&amp;= 0
\end{aligned}
\]</span> Since <span class="math inline">\(\hat{y}_1\)</span> is orthogonal to <span class="math inline">\(v_3\)</span>, and <span class="math inline">\(\hat{y}_0\)</span> is a component of <span class="math inline">\(\hat{y}_1\)</span>, it follows that all three pieces are mutually orthogonal.</p>
<p><strong>4. Sum of Squares</strong> By the Pythagorean theorem applied twice to these orthogonal vectors, the equality of squared norms follows immediately.</p>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-anova-decomposition-v2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-anova-decomposition-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-anova-decomposition-v2-5.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-anova-decomposition-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Illustration of Projections onto Nested Subspaces
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-anova-ss" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (ANOVA Sum Squares)</strong></span> We apply the <strong>Nested Model Theorem</strong> (<span class="math inline">\(M_0 \subset M_1\)</span>) to the One-way ANOVA setting.</p>
<p><strong>1. Notation and Definitions</strong></p>
<p>Consider a dataset with <span class="math inline">\(k\)</span> groups. Let <span class="math inline">\(i = 1, \dots, k\)</span> index the groups, and <span class="math inline">\(j = 1, \dots, n_i\)</span> index the observations within group <span class="math inline">\(i\)</span>.</p>
<ul>
<li><p><span class="math inline">\(N\)</span>: Total number of observations, <span class="math inline">\(N = \sum_{i=1}^k n_i\)</span>.</p></li>
<li><p><span class="math inline">\(y_{ij}\)</span>: The <span class="math inline">\(j\)</span>-th observation in the <span class="math inline">\(i\)</span>-th group.</p></li>
<li><p><span class="math inline">\(\bar{y}_{i.}\)</span>: The sample mean of group <span class="math inline">\(i\)</span>. <span class="math display">\[ \bar{y}_{i.} = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \]</span></p></li>
<li><p><span class="math inline">\(\bar{y}_{..}\)</span>: The grand mean of all observations. <span class="math display">\[ \bar{y}_{..} = \frac{1}{N} \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij} \]</span></p></li>
</ul>
<p><strong>2. The Data and Projection Vectors</strong></p>
<div id="tbl-anova-vectors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-anova-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: ANOVA Vectors: Data, Null Model, and Full Model
</figcaption>
<div aria-describedby="tbl-anova-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Observation (<span class="math inline">\(y\)</span>)</th>
<th style="text-align: center;">Null Projection (<span class="math inline">\(\hat{y}_0\)</span>)</th>
<th style="text-align: center;">Full Projection (<span class="math inline">\(\hat{y}_1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} y_{11} \\ \vdots \\ y_{1 n_1} \\ \hline \vdots \\ \hline y_{k1} \\ \vdots \\ y_{k n_k} \end{pmatrix}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \\ \hline \vdots \\ \hline \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \end{pmatrix}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} \bar{y}_{1.} \\ \vdots \\ \bar{y}_{1.} \\ \hline \vdots \\ \hline \bar{y}_{k.} \\ \vdots \\ \bar{y}_{k.} \end{pmatrix}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>3. Decomposition and Sum of Squares</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: left;">Vector Elements</th>
<th style="text-align: left;">Squared Norm (Sum of Squares)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Null Proj.</strong></td>
<td style="text-align: center;"><span class="math inline">\(\hat{y}_0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(P_0 y\)</span></td>
<td style="text-align: left;">Grand Mean (<span class="math inline">\(\bar{y}_{..}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\|\hat{y}_0\|^2 = N \bar{y}_{..}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Full Proj.</strong></td>
<td style="text-align: center;"><span class="math inline">\(\hat{y}_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(P_1 y\)</span></td>
<td style="text-align: left;">Group Means (<span class="math inline">\(\bar{y}_{i.}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\|\hat{y}_1\|^2 = \sum_{i=1}^k n_i \bar{y}_{i.}^2\)</span></td>
</tr>
</tbody>
</table>
<p><strong>4. Geometric Justification of Shortcut Formulas</strong></p>
<p><strong>A. Total Sum of Squares (SST)</strong> Since <span class="math inline">\(\hat{y}_0 \perp (y - \hat{y}_0)\)</span>, we have <span class="math inline">\(\|y\|^2 = \|\hat{y}_0\|^2 + \|y - \hat{y}_0\|^2\)</span>: <span class="math display">\[ \text{SST} = \|y - \hat{y}_0\|^2 = \|y\|^2 - \|\hat{y}_0\|^2 \]</span> <span class="math display">\[ \text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij}^2 - N\bar{y}_{..}^2 \]</span></p>
<p><strong>B. Between Group Sum of Squares (SSB)</strong> Since <span class="math inline">\(\hat{y}_0 \perp (\hat{y}_1 - \hat{y}_0)\)</span>, we have <span class="math inline">\(\|\hat{y}_1\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2\)</span>: <span class="math display">\[ \text{SSB} = \|\hat{y}_1 - \hat{y}_0\|^2 = \|\hat{y}_1\|^2 - \|\hat{y}_0\|^2 \]</span> <span class="math display">\[ \text{SSB} = \sum_{i=1}^k n_i\bar{y}_{i.}^2 - N\bar{y}_{..}^2 \]</span></p>
<p><strong>C. Within Group Sum of Squares (SSW)</strong> Since <span class="math inline">\(\hat{y}_1 \perp (y - \hat{y}_1)\)</span>, we have <span class="math inline">\(\|y\|^2 = \|\hat{y}_1\|^2 + \|y - \hat{y}_1\|^2\)</span>: <span class="math display">\[ \text{SSW} = \|y - \hat{y}_1\|^2 = \|y\|^2 - \|\hat{y}_1\|^2 \]</span> <span class="math display">\[ \text{SSW} = \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij}^2 - \sum_{i=1}^k n_i\bar{y}_{i.}^2 \]</span></p>
<p><strong>Conclusion:</strong> <span class="math display">\[ \underbrace{\|y\|^2 - N\bar{y}_{..}^2}_{\text{SST}} = \underbrace{(\sum n_i\bar{y}_{i.}^2 - N\bar{y}_{..}^2)}_{\text{SSB}} + \underbrace{(\sum \sum y_{ij}^2 - \sum n_i\bar{y}_{i.}^2)}_{\text{SSW}} \]</span> <strong>5. Visualizing ANOVA Components in Data Space</strong></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate Data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>group_names <span class="op">=</span> [<span class="st">'A'</span>, <span class="st">'B'</span>, <span class="st">'C'</span>, <span class="st">'D'</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>n_i <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">8</span>, <span class="dv">15</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">12</span>, <span class="dv">18</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>std_dev <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define colors and markers for each group</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#1f77b4'</span>, <span class="st">'#ff7f0e'</span>, <span class="st">'#2ca02c'</span>, <span class="st">'#d62728'</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">'o'</span>, <span class="st">'s'</span>, <span class="st">'^'</span>, <span class="st">'D'</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>data_x <span class="op">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>data_y <span class="op">=</span> []</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>group_boundaries <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>group_indices <span class="op">=</span> [] <span class="co"># To store indices for each group</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>current_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(n_i):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    group_data <span class="op">=</span> np.random.normal(means[i], std_dev, n)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.arange(current_idx, current_idx <span class="op">+</span> n)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    data_x.extend(indices)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    data_y.extend(group_data)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    group_indices.append(indices) <span class="co"># Store indices for plotting later</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    current_idx <span class="op">+=</span> n</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    group_boundaries.append(current_idx)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>data_x <span class="op">=</span> np.array(data_x)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>data_y <span class="op">=</span> np.array(data_y)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Stats</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>grand_mean <span class="op">=</span> np.mean(data_y)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>group_means <span class="op">=</span> [np.mean(data_y[group_boundaries[i]:group_boundaries[i<span class="op">+</span><span class="dv">1</span>]]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(n_i))]</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Plotting</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw Grand Mean (Full span)</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>grand_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Grand Mean ($</span><span class="ch">\\</span><span class="ss">bar</span><span class="ch">{{</span><span class="ss">y</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="ss">..</span><span class="ch">}}</span><span class="ss">$ = </span><span class="sc">{</span>grand_mean<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through each group to plot points and means with matching colors</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(n_i)):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    start, end <span class="op">=</span> group_boundaries[i], group_boundaries[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> group_indices[i]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Scatter plot for the group with unique color and marker</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    plt.scatter(data_x[idx], data_y[idx], color<span class="op">=</span>colors[i], marker<span class="op">=</span>markers[i], </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span>, s<span class="op">=</span><span class="dv">60</span>, label<span class="op">=</span><span class="ss">f'Group </span><span class="sc">{</span>group_names[i]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Horizontal line for group mean with the SAME color</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    plt.hlines(y<span class="op">=</span>group_means[i], xmin<span class="op">=</span>start, xmax<span class="op">=</span>end<span class="op">-</span><span class="dv">1</span>, color<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Visualizing the "Within" residuals (faint lines)</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> idx:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        plt.vlines(x<span class="op">=</span>j, ymin<span class="op">=</span><span class="bu">min</span>(data_y[j], group_means[i]), </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                   ymax<span class="op">=</span><span class="bu">max</span>(data_y[j], group_means[i]), </span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>                   color<span class="op">=</span>colors[i], alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">':'</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Formatting</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"One-Way ANOVA: Data, Group Means, and Grand Mean"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Observation Index ($j$ grouped by $i$)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Value ($y_</span><span class="sc">{ij}</span><span class="st">$)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Set x-ticks at the center of each group</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.array(group_boundaries[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">+</span> np.array(n_i)<span class="op">/</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span>, </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>           [<span class="ss">f"Group </span><span class="sc">{</span>g<span class="sc">}</span><span class="ch">\n</span><span class="ss">($n_</span><span class="ch">{{</span><span class="sc">{</span>g<span class="sc">.</span>lower()<span class="sc">}</span><span class="ch">}}</span><span class="ss">=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">$)"</span> <span class="cf">for</span> g, n <span class="kw">in</span> <span class="bu">zip</span>(group_names, n_i)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>plt.grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust legend to show group markers and the grand mean line</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>handles, labels <span class="op">=</span> plt.gca().get_legend_handles_labels()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Reorder legend: Groups first, then Grand Mean</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.legend([handles[idx] <span class="cf">for</span> idx <span class="kw">in</span> order], [labels[idx] <span class="cf">for</span> idx <span class="kw">in</span> order], </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>           bbox_to_anchor<span class="op">=</span>(<span class="fl">1.02</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>, borderaxespad<span class="op">=</span><span class="fl">0.</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-anova-data-space-colored" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-anova-data-space-colored-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-anova-data-space-colored-7.png" class="img-fluid figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-anova-data-space-colored-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Visualization of Group Means vs.&nbsp;Grand Mean
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="projections-onto-orthogonal-subspaces" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="projections-onto-orthogonal-subspaces"><span class="header-section-number">2.9</span> Projections onto Orthogonal Subspaces</h2>
<p>Finally, we consider the case where the entire space <span class="math inline">\(\mathbb{R}^n\)</span> is decomposed into mutually orthogonal subspaces.</p>
<div id="thm-orth-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.23 (General Orthogonal Projections)</strong></span> If <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of orthogonal subspaces <span class="math inline">\(V_1, V_2, \dots, V_k\)</span>:</p>
<p><span class="math display">\[
\mathbb{R}^n = V_1 \oplus V_2 \oplus \dots \oplus V_k
\]</span> where <span class="math inline">\(V_i \perp V_j\)</span> for all <span class="math inline">\(i \ne j\)</span>.</p>
<p>Then any vector <span class="math inline">\(y\)</span> can be uniquely written as: <span class="math display">\[
y = \hat{y}_1 + \hat{y}_2 + \dots + \hat{y}_k
\]</span> where <span class="math inline">\(\hat{y}_i \in V_i\)</span>.</p>
<p>Furthermore, each component <span class="math inline">\(\hat{y}_i\)</span> is simply the projection of <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(V_i\)</span>: <span class="math display">\[
\hat{y}_i = P_i y
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Existence:</strong> Since <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of <span class="math inline">\(V_1, \dots, V_k\)</span>, by definition, any vector <span class="math inline">\(y \in \mathbb{R}^n\)</span> can be written as a sum <span class="math inline">\(y = v_1 + \dots + v_k\)</span> where <span class="math inline">\(v_i \in V_i\)</span>.</p>
<p><strong>2. Uniqueness:</strong> Suppose there are two such representations: <span class="math inline">\(y = \sum v_i = \sum w_i\)</span>, with <span class="math inline">\(v_i, w_i \in V_i\)</span>. Then <span class="math inline">\(\sum (v_i - w_i) = 0\)</span>. Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, <span class="math inline">\(v_i - w_i = 0 \implies v_i = w_i\)</span>. The representation is unique. Let <span class="math inline">\(\hat{y}_i = v_i\)</span>.</p>
<p><strong>3. Projection Property:</strong> We claim that the <span class="math inline">\(i\)</span>-th component <span class="math inline">\(\hat{y}_i\)</span> is the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V_i\)</span>. We must show that the residual <span class="math inline">\((y - \hat{y}_i)\)</span> is orthogonal to <span class="math inline">\(V_i\)</span>. <span class="math display">\[
y - \hat{y}_i = \sum_{j \ne i} \hat{y}_j
\]</span> Let <span class="math inline">\(z\)</span> be any vector in <span class="math inline">\(V_i\)</span>. We calculate the inner product: <span class="math display">\[
\langle y - \hat{y}_i, z \rangle = \left\langle \sum_{j \ne i} \hat{y}_j, z \right\rangle = \sum_{j \ne i} \langle \hat{y}_j, z \rangle
\]</span> Since <span class="math inline">\(\hat{y}_j \in V_j\)</span> and <span class="math inline">\(z \in V_i\)</span>, and the subspaces are mutually orthogonal (<span class="math inline">\(V_j \perp V_i\)</span> for <span class="math inline">\(j \ne i\)</span>), every term in the sum is zero. Therefore, <span class="math inline">\((y - \hat{y}_i) \perp V_i\)</span>. By the definition of orthogonal projection, <span class="math inline">\(\hat{y}_i = P_i y\)</span>.</p>
</div>
<p>This implies that the identity matrix can be decomposed into a sum of projection matrices: <span class="math display">\[
I_n = P_1 + P_2 + \dots + P_k
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-orthogonal-decomp-rotated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-orthogonal-decomp-rotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-orthogonal-decomp-rotated-9.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-orthogonal-decomp-rotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Orthogonal decomposition of vector y into subspaces
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Define Vectors ---</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>origin <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Projections (P_i y)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">0</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial Sums (P_i y + P_j y)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>sum_12 <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>sum_13 <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p3</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>sum_23 <span class="ot">&lt;-</span> p2 <span class="sc">+</span> p3</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Helper Functions ---</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to add a vector with an arrowhead (Cone)</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>add_vec_arrow <span class="ot">&lt;-</span> <span class="cf">function</span>(p, start, end, color, name) {</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  p <span class="sc">%&gt;%</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"scatter3d"</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>      <span class="at">mode =</span> <span class="st">"lines"</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">c</span>(start[<span class="dv">1</span>], end[<span class="dv">1</span>]),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">c</span>(start[<span class="dv">2</span>], end[<span class="dv">2</span>]),</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">z =</span> <span class="fu">c</span>(start[<span class="dv">3</span>], end[<span class="dv">3</span>]),</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fu">list</span>(<span class="at">color =</span> color, <span class="at">width =</span> <span class="dv">6</span>),</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>      <span class="at">showlegend =</span> <span class="cn">TRUE</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"cone"</span>,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> end[<span class="dv">1</span>], <span class="at">y =</span> end[<span class="dv">2</span>], <span class="at">z =</span> end[<span class="dv">3</span>],</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>      <span class="at">u =</span> end[<span class="dv">1</span>]<span class="sc">-</span>start[<span class="dv">1</span>], <span class="at">v =</span> end[<span class="dv">2</span>]<span class="sc">-</span>start[<span class="dv">2</span>], <span class="at">w =</span> end[<span class="dv">3</span>]<span class="sc">-</span>start[<span class="dv">3</span>],</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>      <span class="at">sizemode =</span> <span class="st">"absolute"</span>,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>      <span class="at">sizeref =</span> <span class="fl">0.5</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>      <span class="at">anchor =</span> <span class="st">"tip"</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>      <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(color, color)),</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>      <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>      <span class="at">showlegend =</span> <span class="cn">FALSE</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to add dashed "error" lines</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>add_dashed_line <span class="ot">&lt;-</span> <span class="cf">function</span>(p, start, end, color, name) {</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>  p <span class="sc">%&gt;%</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"scatter3d"</span>,</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>      <span class="at">mode =</span> <span class="st">"lines"</span>,</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">c</span>(start[<span class="dv">1</span>], end[<span class="dv">1</span>]),</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">c</span>(start[<span class="dv">2</span>], end[<span class="dv">2</span>]),</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>      <span class="at">z =</span> <span class="fu">c</span>(start[<span class="dv">3</span>], end[<span class="dv">3</span>]),</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fu">list</span>(<span class="at">color =</span> color, <span class="at">width =</span> <span class="dv">3</span>, <span class="at">dash =</span> <span class="st">"dash"</span>),</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>      <span class="at">hoverinfo =</span> <span class="st">"text"</span>,</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>      <span class="at">text =</span> name</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Build Plot ---</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> <span class="fu">plot_ly</span>()</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Main Vectors (Solid + Cones)</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p1, <span class="st">"red"</span>, <span class="st">"P1 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p2, <span class="st">"green"</span>, <span class="st">"P2 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p3, <span class="st">"blue"</span>, <span class="st">"P3 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, y_vec, <span class="st">"black"</span>, <span class="st">"y"</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Dashed Lines from y to Single Projections</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p1, <span class="st">"rgba(255, 0, 0, 0.5)"</span>, <span class="st">"y -&gt; P1"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p2, <span class="st">"rgba(0, 255, 0, 0.5)"</span>, <span class="st">"y -&gt; P2"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p3, <span class="st">"rgba(0, 0, 255, 0.5)"</span>, <span class="st">"y -&gt; P3"</span>)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Dashed Lines from y to Partial Sums</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_12, <span class="st">"purple"</span>, <span class="st">"y -&gt; (P1+P2)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_13, <span class="st">"orange"</span>, <span class="st">"y -&gt; (P1+P3)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_23, <span class="st">"cyan"</span>,   <span class="st">"y -&gt; (P2+P3)"</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Axes (Subspaces)</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>limit <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>axis_style <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">"gray"</span>, <span class="at">dash =</span> <span class="st">"dot"</span>, <span class="at">width =</span> <span class="dv">2</span>)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), </span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V1 (x)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), </span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V2 (y)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), </span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V3 (z)"</span>)</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Layout ---</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span> <span class="fu">layout</span>(</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Orthogonal Decomposition Geometry"</span>,</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>  <span class="at">width =</span> <span class="dv">900</span>,</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>  <span class="at">height =</span> <span class="dv">700</span>,</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>  <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V1"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V2"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V3"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    <span class="at">aspectmode =</span> <span class="st">"cube"</span>,</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    <span class="at">camera =</span> <span class="fu">list</span>(<span class="at">eye =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">1.5</span>, <span class="at">y =</span> <span class="fl">1.5</span>, <span class="at">z =</span> <span class="fl">1.2</span>))</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>  <span class="at">margin =</span> <span class="fu">list</span>(<span class="at">l =</span> <span class="dv">0</span>, <span class="at">r =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">t =</span> <span class="dv">50</span>),</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">0.75</span>, <span class="at">y =</span> <span class="fl">0.9</span>)</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-orthogonal-decomp-r" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-orthogonal-decomp-r-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-5919cc0ddd953badc152" style="width:100%;height:520px;"></div>
<script type="application/json" data-for="htmlwidget-5919cc0ddd953badc152">{"x":{"visdat":{"ce61751bc5":["function () ","plotlyVisDat"]},"cur_data":"ce61751bc5","attrs":{"ce61751bc5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"red","width":6},"name":"P1 y","showlegend":true,"inherit":true},"ce61751bc5.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":3,"y":0,"z":0,"u":3,"v":0,"w":0,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["red","red"]],"showscale":false,"name":"P1 y","showlegend":false,"inherit":true},"ce61751bc5.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,4],"z":[0,0],"line":{"color":"green","width":6},"name":"P2 y","showlegend":true,"inherit":true},"ce61751bc5.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":0,"y":4,"z":0,"u":0,"v":4,"w":0,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["green","green"]],"showscale":false,"name":"P2 y","showlegend":false,"inherit":true},"ce61751bc5.4":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,5],"line":{"color":"blue","width":6},"name":"P3 y","showlegend":true,"inherit":true},"ce61751bc5.5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":0,"y":0,"z":5,"u":0,"v":0,"w":5,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["blue","blue"]],"showscale":false,"name":"P3 y","showlegend":false,"inherit":true},"ce61751bc5.6":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,5],"line":{"color":"black","width":6},"name":"y","showlegend":true,"inherit":true},"ce61751bc5.7":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":3,"y":4,"z":5,"u":3,"v":4,"w":5,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["black","black"]],"showscale":false,"name":"y","showlegend":false,"inherit":true},"ce61751bc5.8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,0],"line":{"color":"rgba(255, 0, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P1","hoverinfo":"text","text":"y -> P1","inherit":true},"ce61751bc5.9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,0],"line":{"color":"rgba(0, 255, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P2","hoverinfo":"text","text":"y -> P2","inherit":true},"ce61751bc5.10":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,0],"z":[5,5],"line":{"color":"rgba(0, 0, 255, 0.5)","width":3,"dash":"dash"},"name":"y -> P3","hoverinfo":"text","text":"y -> P3","inherit":true},"ce61751bc5.11":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,4],"z":[5,0],"line":{"color":"purple","width":3,"dash":"dash"},"name":"y -> (P1+P2)","hoverinfo":"text","text":"y -> (P1+P2)","inherit":true},"ce61751bc5.12":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,5],"line":{"color":"orange","width":3,"dash":"dash"},"name":"y -> (P1+P3)","hoverinfo":"text","text":"y -> (P1+P3)","inherit":true},"ce61751bc5.13":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,5],"line":{"color":"cyan","width":3,"dash":"dash"},"name":"y -> (P2+P3)","hoverinfo":"text","text":"y -> (P2+P3)","inherit":true},"ce61751bc5.14":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,6],"y":[0,0],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V1 (x)","inherit":true},"ce61751bc5.15":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,6],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V2 (y)","inherit":true},"ce61751bc5.16":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,6],"line":{"color":"gray","dash":"dot","width":2},"name":"V3 (z)","inherit":true}},"layout":{"width":900,"height":700,"margin":{"b":0,"l":0,"t":50,"r":0},"title":"Orthogonal Decomposition Geometry","scene":{"xaxis":{"title":"V1","range":[0,6]},"yaxis":{"title":"V2","range":[0,6]},"zaxis":{"title":"V3","range":[0,6]},"aspectmode":"cube","camera":{"eye":{"x":1.5,"y":1.5,"z":1.2}}},"legend":{"x":0.75,"y":0.90000000000000002},"xaxis":{"title":[]},"yaxis":{"title":[]},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"red","width":6},"name":"P1 y","showlegend":true,"marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"red"]],"showscale":false,"type":"cone","x":[3],"y":[0],"z":[0],"u":[3],"v":[0],"w":[0],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P1 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,4],"z":[0,0],"line":{"color":"green","width":6},"name":"P2 y","showlegend":true,"marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"green"],[1,"green"]],"showscale":false,"type":"cone","x":[0],"y":[4],"z":[0],"u":[0],"v":[4],"w":[0],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P2 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,5],"line":{"color":"blue","width":6},"name":"P3 y","showlegend":true,"marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"blue"],[1,"blue"]],"showscale":false,"type":"cone","x":[0],"y":[0],"z":[5],"u":[0],"v":[0],"w":[5],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P3 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,5],"line":{"color":"black","width":6},"name":"y","showlegend":true,"marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"black"],[1,"black"]],"showscale":false,"type":"cone","x":[3],"y":[4],"z":[5],"u":[3],"v":[4],"w":[5],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,0],"line":{"color":"rgba(255, 0, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P1","hoverinfo":["text","text"],"text":["y -> P1","y -> P1"],"marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,0],"line":{"color":"rgba(0, 255, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P2","hoverinfo":["text","text"],"text":["y -> P2","y -> P2"],"marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,0],"z":[5,5],"line":{"color":"rgba(0, 0, 255, 0.5)","width":3,"dash":"dash"},"name":"y -> P3","hoverinfo":["text","text"],"text":["y -> P3","y -> P3"],"marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,4],"z":[5,0],"line":{"color":"purple","width":3,"dash":"dash"},"name":"y -> (P1+P2)","hoverinfo":["text","text"],"text":["y -> (P1+P2)","y -> (P1+P2)"],"marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,5],"line":{"color":"orange","width":3,"dash":"dash"},"name":"y -> (P1+P3)","hoverinfo":["text","text"],"text":["y -> (P1+P3)","y -> (P1+P3)"],"marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,5],"line":{"color":"cyan","width":3,"dash":"dash"},"name":"y -> (P2+P3)","hoverinfo":["text","text"],"text":["y -> (P2+P3)","y -> (P2+P3)"],"marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,6],"y":[0,0],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V1 (x)","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,6],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V2 (y)","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,6],"line":{"color":"gray","dash":"dot","width":2},"name":"V3 (z)","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-orthogonal-decomp-r-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Orthogonal decomposition geometry (R Plotly)
</figcaption>
</figure>
</div>
</div>
<div id="thm-complete-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.24 (Complete Orthogonal Decomposition of <span class="math inline">\(\mathbb{R}^n\)</span>)</strong></span> Let <span class="math inline">\(P_0, P_1, \dots, P_k\)</span> be a sequence of orthogonal projection matrices with nested column spaces: <span class="math display">\[
\text{Col}(P_0) \subseteq \text{Col}(P_1) \subseteq \dots \subseteq \text{Col}(P_k)
\]</span></p>
<p>Define the sequence of difference matrices <span class="math inline">\(\Delta P_i\)</span> and their column spaces <span class="math inline">\(V_i\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
\Delta P_0 &amp;= P_0, &amp; V_0 &amp;= \text{Col}(\Delta P_0) \\
\Delta P_i &amp;= P_i - P_{i-1} \quad (1 \le i \le k), &amp; V_i &amp;= \text{Col}(\Delta P_i) \\
\Delta P_{k+1} &amp;= I - P_k, &amp; V_{k+1} &amp;= \text{Col}(\Delta P_{k+1})
\end{align*}\]</span></p>
<p><strong>Conclusion:</strong></p>
<ol type="1">
<li><p><strong>Projection Property:</strong> Each <span class="math inline">\(\Delta P_i\)</span> is the orthogonal projection matrix onto <span class="math inline">\(V_i\)</span> for <span class="math inline">\(i = 0, \dots, k+1\)</span>.</p></li>
<li><p><strong>Mutual Orthogonality:</strong> The collection <span class="math inline">\(\{\Delta P_i\}\)</span> are mutually orthogonal operators: <span class="math display">\[ \Delta P_i \Delta P_j = 0 \quad \text{for all } i \ne j \]</span></p></li>
<li><p><strong>Direct Sum Decomposition:</strong> The vector space <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of these orthogonal subspaces: <span class="math display">\[ \mathbb{R}^n = V_0 \oplus V_1 \oplus \dots \oplus V_{k+1} \]</span></p></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>1. Proof that</strong> <span class="math inline">\(\Delta P_i\)</span> is the Projection onto <span class="math inline">\(V_i\)</span> We must show each <span class="math inline">\(\Delta P_i\)</span> is symmetric and idempotent.</p>
<ul>
<li>For <span class="math inline">\(\Delta P_0 = P_0\)</span>: True by definition.</li>
<li>For <span class="math inline">\(\Delta P_i\)</span> (<span class="math inline">\(1 \le i \le k\)</span>):
<ul>
<li><strong>Symmetry:</strong> Difference of symmetric matrices (<span class="math inline">\(P_i, P_{i-1}\)</span>) is symmetric.</li>
<li><strong>Idempotency:</strong> <span class="math inline">\((\Delta P_i)^2 = (P_i - P_{i-1})^2 = P_i^2 - P_i P_{i-1} - P_{i-1} P_i + P_{i-1}^2\)</span>. Using nested properties (<span class="math inline">\(P_i P_{i-1} = P_{i-1}\)</span>), this simplifies to <span class="math inline">\(P_i - P_{i-1} = \Delta P_i\)</span>.</li>
</ul></li>
<li>For <span class="math inline">\(\Delta P_{k+1} = I - P_k\)</span>:
<ul>
<li><strong>Symmetry:</strong> <span class="math inline">\((I - P_k)' = I - P_k\)</span>.</li>
<li><strong>Idempotency:</strong> <span class="math inline">\((I - P_k)^2 = I - 2P_k + P_k^2 = I - P_k\)</span>.</li>
</ul></li>
</ul>
<p><strong>2. Proof of Mutual Orthogonality</strong> We show <span class="math inline">\(\Delta P_j \Delta P_i = 0\)</span> for <span class="math inline">\(i &lt; j\)</span>.</p>
<ul>
<li><p><strong>Case 1: Both indices</strong> <span class="math inline">\(\le k\)</span> (i.e., <span class="math inline">\(1 \le i &lt; j \le k\)</span>): <span class="math display">\[ (P_j - P_{j-1})(P_i - P_{i-1}) = P_j P_i - P_j P_{i-1} - P_{j-1} P_i + P_{j-1} P_{i-1} \]</span> Since <span class="math inline">\(\text{Col}(P_i) \subseteq \text{Col}(P_{j-1})\)</span>, all terms reduce to <span class="math inline">\(P_i - P_{i-1} - P_i + P_{i-1} = 0\)</span>.</p></li>
<li><p><strong>Case 2: One index is the residual</strong> (<span class="math inline">\(j = k+1\)</span>): We check <span class="math inline">\(\Delta P_{k+1} \Delta P_i = (I - P_k)\Delta P_i\)</span> for any <span class="math inline">\(i \le k\)</span>. Since <span class="math inline">\(V_i \subseteq \text{Col}(P_k)\)</span>, we have <span class="math inline">\(P_k \Delta P_i = \Delta P_i\)</span>. <span class="math display">\[ (I - P_k)\Delta P_i = \Delta P_i - P_k \Delta P_i = \Delta P_i - \Delta P_i = 0 \]</span></p></li>
</ul>
<p><strong>3. Proof of Direct Sum</strong> The sum of the difference matrices forms a telescoping series: <span class="math display">\[ \sum_{j=0}^{k+1} \Delta P_j = P_0 + \sum_{i=1}^k (P_i - P_{i-1}) + (I - P_k) \]</span> <span class="math display">\[ = P_k + (I - P_k) = I \]</span> Since the identity operator <span class="math inline">\(I\)</span> (which maps <span class="math inline">\(\mathbb{R}^n\)</span> to itself) is the sum of mutually orthogonal projection operators, the space <span class="math inline">\(\mathbb{R}^n\)</span> decomposes into the direct sum of their respective image subspaces <span class="math inline">\(V_i\)</span>.</p>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-venn-nested-projection" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" style="width: 80% !important;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-venn-nested-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec1-vecspace_files/figure-html/fig-venn-nested-projection-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-venn-nested-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Venn Diagram of Nested Projections with Colored Increments
</figcaption>
</figure>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./introlm.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lec2-matrix.html" class="pagination-link" aria-label="Matrix Algebra">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>