---
title: "Vector Space and Projection"
author: "Longhai Li"
subtitle: "Lecture 10"
format:
  html:
    toc: true
    number-sections: true
  pdf:
    toc: true
    number-sections: true
---

```{r}
#| label: setup-images
#| include: false
#| warning: false

library(pdftools)
library(magick)

# Define path to PDF
pdf_path <- "../Lec10-vector space and projection.pdf"
img_dir <- "lec1_images"

# Create image directory
if (!dir.exists(img_dir)) {
  dir.create(img_dir)
}

# Helper function to extract and crop a specific page
extract_page_image <- function(page_num, filename, geometry = "1200x800+0+200") {
  if (file.exists(pdf_path) && !file.exists(file.path(img_dir, filename))) {
    tryCatch({
      bitmap <- pdf_render_page(pdf_path, page = page_num, dpi = 150)
      img <- image_read(bitmap)
      img_cropped <- image_crop(img, geometry)
      image_write(img_cropped, file.path(img_dir, filename))
    }, error = function(e) {
      warning(paste("Failed to extract page", page_num, ":", e$message))
    })
  }
}

# Extract images for specific pages with visual content
extract_page_image(3, "page03_vectors.png", "1200x600+0+300")
extract_page_image(5, "page05_angle.png", "1200x700+0+200")
extract_page_image(10, "page10_projection.png", "1200x800+0+300")
extract_page_image(16, "page16_pythagoras.png", "1200x700+0+200")
extract_page_image(58, "page58_anova_proj.png")
extract_page_image(65, "page65_gram_schmidt.png")
extract_page_image(86, "page86_nested_figure.png", "1200x800+0+200")
```

# Vector and Projection

## Page 2: Outline
* Vector and Geometry
* Inner Product and Perpendicular
* Projection to a Single Vector
* Pythagorean theory
* Shortest distance property of projection

## Page 3: Vectors
A vector $x$ is a point in $\mathbb{R}^n$.

$$
x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
$$

**Operations:**
* **Addition:** $x + y = \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}$
* **Subtraction:** $d = y - x \implies x + d = y$

![Vector Addition and Subtraction](lec1_images/page03_vectors.png){width=70%}

## Page 4: Scalar Multiplication and Length
**Multiplication by scalar $c$:**
$cx$ scales the vector. Note: $cX = X[c]$ (matrix notation).

**Length (Euclidean Distance):**
For $x = (x_1, \dots, x_n)'$:
$$
||x||^2 = \sum_{i=1}^n x_i^2
$$
$$
||x|| = \sqrt{\sum_{i=1}^n x_i^2}
$$

## Page 5: Angle and Inner Product
Using the Law of Cosines on the triangle formed by $x$, $y$, and $y-x$:
$$
c^2 = a^2 + b^2 - 2ab \cos \theta
$$
$$
||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \cdot ||y|| \cos \theta
$$

![Geometry of Inner Product](lec1_images/page05_angle.png){width=70%}

## Page 6: Deriving Inner Product
Expanding the distance squared:
$$
||y - x||^2 = \sum (x_i - y_i)^2 = \sum (x_i^2 + y_i^2 - 2x_i y_i)
$$
$$
= ||x||^2 + ||y||^2 - 2 \sum x_i y_i
$$

Comparing this with the Law of Cosines:
$$
x'y = \sum_{i=1}^n x_i y_i = \langle x, y \rangle = ||x|| \cdot ||y|| \cos \theta
$$
This defines the **Inner Product**.

## Page 7: Coordinate Projection
Rearranging the cosine formula:
$$
||y|| \cos \theta = \frac{x'y}{||x||} = \left\langle \frac{x}{||x||}, y \right\rangle
$$
This represents the length of the projection of $y$ onto $x$.

## Page 8: Projection Vector
The projection vector itself is the length multiplied by the unit direction vector:
$$
\text{Projection} = (||y|| \cos \theta) \cdot \frac{x}{||x||}
$$
$$
= \frac{x'y}{||x||} \cdot \frac{x}{||x||}
$$

## Page 9: Perpendicularity
Two vectors are perpendicular (orthogonal) if $\theta = 90^\circ$ ($\pi/2$).
This implies $\cos \theta = 0$, so:
$$
x'y = 0 \iff x \perp y
$$

## Page 10: Projection Definition
Let $L(x) = \{ cx \mid c \in \mathbb{R} \}$.
$\hat{y}$ is the projection of $y$ onto $L(x)$ if:
1.  $\hat{y} \in L(x)$ (i.e., $\hat{y} = cx$)
2.  $(y - \hat{y}) \perp x$

Solving for $c$:
$$
x'(y - cx) = 0 \implies x'y - c(x'x) = 0 \implies c = \frac{x'y}{||x||^2}
$$

![Projection Definition](lec1_images/page10_projection.png){width=70%}

## Page 11: Projection Formula
$$
\hat{y} = \frac{x'y}{||x||^2} x = \left\langle y, \frac{x}{||x||} \right\rangle \frac{x}{||x||}
$$
This can be seen as (Scale) $\times$ (Direction).

## Page 12: Projection Matrix $P_x$
$$
\hat{y} = \text{proj}(y|x) = \frac{x'y}{||x||^2} x = x \frac{x'y}{||x||^2}
$$
$$
= \frac{xx'}{||x||^2} y = P_x y
$$
Here, $P_x = \frac{xx'}{||x||^2}$ is the projection matrix (dimensions $p \times p$ if $x \in \mathbb{R}^p$).

## Page 13-14: Examples
**Example 1:**
$y = (1, 3)'$, $x = (1, 1)'$.
$$
\hat{y} = \frac{1(1) + 1(3)}{1^2 + 1^2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{4}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
$$

**Using Matrix:**
$$
P_x = \frac{1}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}
$$
$$
\hat{y} = P_x y = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
$$

## Page 15: Example with Mean Vector
Let $y = (y_1, \dots, y_n)'$ and $J_n = (1, \dots, 1)'$.
$$
\text{proj}(y|J_n) = \frac{J_n' y}{||J_n||^2} J_n = \frac{\sum y_i}{n} J_n = \bar{y} J_n = \begin{pmatrix} \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
$$

## Page 16-17: Pythagorean Theorem
**Geometry:** $c^2 = a^2 + b^2$.

**Vector Space:**
If $x \perp y$ (i.e., $x'y = 0$), then:
$$
||x + y||^2 = ||x||^2 + ||y||^2
$$
**Proof:**
$$
||x + y||^2 = (x+y)'(x+y) = x'x + 2x'y + y'y = ||x||^2 + 0 + ||y||^2
$$

![Pythagorean Theorem](lec1_images/page16_pythagoras.png){width=70%}

## Page 18-19: Shortest Distance Property
The projection $\hat{y} = P(y|x)$ is the closest vector in $L(x)$ to $y$.
For any other vector $y^* \in L(x)$:
$$
||y - y^*||^2 = ||(y - \hat{y}) + (\hat{y} - y^*)||^2
$$
Since $(y - \hat{y}) \perp L(x)$ and $(\hat{y} - y^*) \in L(x)$, they are orthogonal.
$$
= ||y - \hat{y}||^2 + ||\hat{y} - y^*||^2 \ge ||y - \hat{y}||^2
$$

# Basics of Vector Space

## Page 21: Definition
A subset $V \subseteq \mathbb{R}^n$ is a vector space if:
1.  Closed under addition: $x_1, x_2 \in V \implies x_1 + x_2 \in V$.
2.  Closed under scalar multiplication: $x \in V \implies cx \in V$ (including $c=0$).

## Page 23: Spanned Vector Space
$$
L(x_1, \dots, x_p) = \{ r \mid r = c_1 x_1 + \dots + c_p x_p \}
$$
If vectors are dependent (e.g., $x_1 = c x_2$), the space spanned is the same as the reduced set.

## Page 25: Column and Row Space
For matrix $X$:
* **Column Space:** $C(X) = L(\text{columns of } X)$
* **Row Space:** $Row(X) = L(\text{rows of } X)$

## Page 26: Linear Independence
Vectors $x_1, \dots, x_p$ are Linearly Independent (LIN) if:
$$
\sum c_i x_i = 0 \implies c_i = 0 \quad \forall i
$$

## Page 27: Rank
$\text{Rank}(X)$ is the number of linearly independent columns (or rows).
$$
\text{Rank}(X) = \text{Dim}(C(X))
$$

## Page 28-29: Rank Properties
1.  $\text{Rank}(X) = \text{Rank}(X')$ (Row rank = Column rank).
2.  $\text{Rank}(X) \le \min(n, p)$.

Proof of Row Rank = Column Rank involves expressing columns as combinations of a basis and showing the transpose preserves the dimension relation.

## Page 32: Orthogonality to Subspace
$y \perp V$ if $y \perp x$ for all $x \in V$.
**Orthogonal Complement ($V^\perp$):** The set of all vectors orthogonal to $V$.

## Page 33: Kernel and Image
* **Image:** $\text{Im}(X) = C(X) = \{ X\beta \mid \beta \in \mathbb{R}^p \}$.
* **Kernel (Null Space):** $\text{Ker}(X) = \{ \beta \in \mathbb{R}^p \mid X\beta = 0 \}$.
* Relation: $\text{Ker}(X) = [Row(X)]^\perp$.

## Page 34: Nullity Theorem
$$
\text{Nullity}(X) + \text{Rank}(X) = p
$$
where $\text{Nullity}(X) = \text{Dim}(\text{Ker}(X))$.

## Page 38-40: Rank Inequalities
* $\text{Rank}(XZ) \le \min(\text{Rank}(X), \text{Rank}(Z))$.
* If $A$ is invertible ($n \times n$), $\text{Rank}(AX) = \text{Rank}(X)$.

## Page 44: Rank of $X'X$
$$
\text{Rank}(XX') = \text{Rank}(X'X) = \text{Rank}(X)
$$
Furthermore, $C(XX') = C(X)$.

# Projection onto Vector Space via Orthonormal Basis

## Page 48: Projection Concept
We want $\hat{y} \in L(X)$ such that $(y - \hat{y}) \perp L(X)$.
If we have an orthonormal basis $q_1, \dots, q_k$:
$$
\hat{y} = \sum \langle q_i, y \rangle q_i
$$

## Page 51: Projection Theorem
If $U = L(x_1, \dots, x_p)$, $\hat{y} = \text{proj}(y|U)$ is unique such that $(y - \hat{y}) \perp x_i$ for all $i$.

## Page 55: Matrix Form with Orthonormal Basis
If $Q = (q_1, \dots, q_k)$ is an orthonormal basis for $V$:
$$
\text{proj}(y|V) = \sum (q_i q_i') y = (Q Q') y
$$
Here $Q'Q = I_k$.

## Page 58: Example (ANOVA)
$y_{ij} = \mu_i + \epsilon_{ij}$.
Projection onto groups involves indicators $x_1, x_2, x_3$.
Since group indicators are orthogonal:
$$
\hat{y} = \text{proj}(y|x_1) + \text{proj}(y|x_2) + \dots
$$
Result: Group means.

![ANOVA Projection](lec1_images/page58_anova_proj.png){width=70%}

## Page 65: Gram-Schmidt / QR Factorization
Any matrix $X$ can be decomposed as $X = QR$, where $Q$ has orthonormal columns and $R$ is upper triangular.
This allows constructing an orthonormal basis from arbitrary columns.

![Gram Schmidt](lec1_images/page65_gram_schmidt.png){width=70%}

# Projection Matrix Definition

## Page 68: Normal Equations
To find $\hat{y} = X\beta$ such that $y - X\beta \perp C(X)$:
$$
X'(y - X\beta) = 0 \implies X'X\beta = X'y
$$
$$
\hat{\beta} = (X'X)^{-1}X'y
$$

## Page 69: The Matrix $P$
$$
\hat{y} = X(X'X)^{-1}X'y = Py
$$
Relationship with QR: If $X=QR$, then $P = QQ'$.

## Page 72-74: Properties of $P$
$P$ is a projection matrix onto $C(P)$ if and only if:
1.  **Symmetric:** $P' = P$
2.  **Idempotent:** $P^2 = P$

**Proof of Idempotence:** If $\hat{y}$ is already in the space, projecting it again shouldn't change it. $P(Py) = Py \implies P^2 = P$.

## Page 76: Projection onto Complement
If $P$ projects onto $C(P)$, then $I - P$ projects onto $C(P)^\perp$.
Properties of $M = I - P$:
1.  Symmetric.
2.  Idempotent ($M^2 = M$).
3.  $PM = P(I-P) = P - P^2 = 0$ (Orthogonal).

# Nested Subspaces

## Page 80: Nested Models
$H_0: y \in C(X_1)$ vs $H_1: y \in C([X_1, X_2])$.
$$
C(X_1) \subseteq C([X_1, X_2])
$$

## Page 81: Projection Composition
If $C(P_0) \subseteq C(P_1)$:
$$
P_1 P_0 = P_0 P_1 = P_0
$$
Projecting onto the smaller space is the same regardless of whether you project onto the larger space first.

## Page 82: Difference of Projections
$P_1 - P_0$ is a projection matrix onto $C(P_1) \cap C(P_0)^\perp$.
This represents the "extra" information in the full model orthogonal to the reduced model.

## Page 86: Decomposition of Sum of Squares
$$
y = \hat{y}_0 + (\hat{y}_1 - \hat{y}_0) + (y - \hat{y}_1)
$$
Squaring norms:
$$
||y||^2 = ||\hat{y}_0||^2 + ||\hat{y}_1 - \hat{y}_0||^2 + ||y - \hat{y}_1||^2
$$
This is the basis for ANOVA Sum of Squares.

![Nested Subspaces Figure](lec1_images/page86_nested_figure.png){width=70%}

## Page 90: ANOVA Sum of Squares
* $RSS_0 = ||y - \hat{y}_0||^2$
* $RSS_1 = ||y - \hat{y}_1||^2$ (SS within groups)
* $RSS_0 - RSS_1 = ||\hat{y}_1 - \hat{y}_0||^2$ (SS between groups)
