<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Point Estimation in Multiple Linear Regression – Statistical Theory for Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lec4-qf.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1b3e43c72e8be34557c75123b0b69e0d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d1855ce4d3ca2472244e2456266329f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="resources/mystyles.css">
<script src="resources/num_eq.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./lec5-est.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Point Estimation in Multiple Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Theory for Linear Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec1-vecspace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Projection in Vector Space</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec2-matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Spectral Theory and Generalized Inverse</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec3-mvn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multivariate Normal Distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec4-qf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Distribution of Quadratic Forms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec5-est.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Point Estimation in Multiple Linear Regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#linear-models-and-least-square-estimator" id="toc-linear-models-and-least-square-estimator" class="nav-link active" data-scroll-target="#linear-models-and-least-square-estimator"><span class="header-section-number">5.1</span> Linear Models and Least Square Estimator</a>
  <ul>
  <li><a href="#assumptions-in-linear-models" id="toc-assumptions-in-linear-models" class="nav-link" data-scroll-target="#assumptions-in-linear-models"><span class="header-section-number">5.1.1</span> Assumptions in Linear Models</a></li>
  <li><a href="#matrix-formulation" id="toc-matrix-formulation" class="nav-link" data-scroll-target="#matrix-formulation"><span class="header-section-number">5.1.2</span> Matrix Formulation</a></li>
  <li><a href="#least-squares-estimator" id="toc-least-squares-estimator" class="nav-link" data-scroll-target="#least-squares-estimator"><span class="header-section-number">5.1.3</span> Least Squares Estimator</a></li>
  <li><a href="#estimation-example" id="toc-estimation-example" class="nav-link" data-scroll-target="#estimation-example"><span class="header-section-number">5.1.4</span> Estimation Example</a></li>
  <li><a href="#properties-of-the-estimator" id="toc-properties-of-the-estimator" class="nav-link" data-scroll-target="#properties-of-the-estimator"><span class="header-section-number">5.1.5</span> Properties of the Estimator</a></li>
  </ul></li>
  <li><a href="#best-linear-unbiased-estimator-blue" id="toc-best-linear-unbiased-estimator-blue" class="nav-link" data-scroll-target="#best-linear-unbiased-estimator-blue"><span class="header-section-number">5.2</span> Best Linear Unbiased Estimator (BLUE)</a>
  <ul>
  <li><a href="#notes-on-gauss-markov" id="toc-notes-on-gauss-markov" class="nav-link" data-scroll-target="#notes-on-gauss-markov"><span class="header-section-number">5.2.1</span> Notes on Gauss-Markov</a>
  <ul class="collapse">
  <li><a href="#limitations-restriction-to-unbiased-estimators" id="toc-limitations-restriction-to-unbiased-estimators" class="nav-link" data-scroll-target="#limitations-restriction-to-unbiased-estimators"><span class="header-section-number">5.2.1.1</span> Limitations: Restriction to Unbiased Estimators</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#estimator-of-error-variance" id="toc-estimator-of-error-variance" class="nav-link" data-scroll-target="#estimator-of-error-variance"><span class="header-section-number">5.3</span> Estimator of Error Variance</a>
  <ul>
  <li><a href="#unbiasedness-of-s2" id="toc-unbiasedness-of-s2" class="nav-link" data-scroll-target="#unbiasedness-of-s2"><span class="header-section-number">5.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></a></li>
  </ul></li>
  <li><a href="#distributions-under-normality" id="toc-distributions-under-normality" class="nav-link" data-scroll-target="#distributions-under-normality"><span class="header-section-number">5.4</span> Distributions under Normality</a></li>
  <li><a href="#maximum-likelihood-estimator-mle" id="toc-maximum-likelihood-estimator-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimator-mle"><span class="header-section-number">5.5</span> Maximum Likelihood Estimator (MLE)</a></li>
  <li><a href="#linear-models-in-centered-form" id="toc-linear-models-in-centered-form" class="nav-link" data-scroll-target="#linear-models-in-centered-form"><span class="header-section-number">5.6</span> Linear Models in Centered Form</a>
  <ul>
  <li><a href="#matrix-formulation-1" id="toc-matrix-formulation-1" class="nav-link" data-scroll-target="#matrix-formulation-1"><span class="header-section-number">5.6.1</span> Matrix Formulation</a></li>
  <li><a href="#estimation-in-centered-form" id="toc-estimation-in-centered-form" class="nav-link" data-scroll-target="#estimation-in-centered-form"><span class="header-section-number">5.6.2</span> Estimation in Centered Form</a></li>
  </ul></li>
  <li><a href="#sum-of-squares-decomposition" id="toc-sum-of-squares-decomposition" class="nav-link" data-scroll-target="#sum-of-squares-decomposition"><span class="header-section-number">5.7</span> Sum of Squares Decomposition</a>
  <ul>
  <li><a href="#geometry-of-the-decomposition" id="toc-geometry-of-the-decomposition" class="nav-link" data-scroll-target="#geometry-of-the-decomposition"><span class="header-section-number">5.7.1</span> Geometry of the Decomposition</a></li>
  </ul></li>
  <li><a href="#coefficient-of-determination-r2" id="toc-coefficient-of-determination-r2" class="nav-link" data-scroll-target="#coefficient-of-determination-r2"><span class="header-section-number">5.8</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</a>
  <ul>
  <li><a href="#adjusted-r2-r2_a" id="toc-adjusted-r2-r2_a" class="nav-link" data-scroll-target="#adjusted-r2-r2_a"><span class="header-section-number">5.8.1</span> Adjusted <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2_a\)</span>)</a></li>
  <li><a href="#expected-value-of-r2" id="toc-expected-value-of-r2" class="nav-link" data-scroll-target="#expected-value-of-r2"><span class="header-section-number">5.8.2</span> Expected Value of <span class="math inline">\(R^2\)</span></a></li>
  </ul></li>
  <li><a href="#underfitting-and-overfitting" id="toc-underfitting-and-overfitting" class="nav-link" data-scroll-target="#underfitting-and-overfitting"><span class="header-section-number">5.9</span> Underfitting and Overfitting</a>
  <ul>
  <li><a href="#underfitting" id="toc-underfitting" class="nav-link" data-scroll-target="#underfitting"><span class="header-section-number">5.9.1</span> Underfitting</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting"><span class="header-section-number">5.9.2</span> Overfitting</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">5.9.3</span> Summary</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Point Estimation in Multiple Linear Regression</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="linear-models-and-least-square-estimator" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="linear-models-and-least-square-estimator"><span class="header-section-number">5.1</span> Linear Models and Least Square Estimator</h2>
<section id="assumptions-in-linear-models" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="assumptions-in-linear-models"><span class="header-section-number">5.1.1</span> Assumptions in Linear Models</h3>
<p>Suppose that on a random sample of <span class="math inline">\(n\)</span> units (patients, animals, trees, etc.) we observe a response variable <span class="math inline">\(Y\)</span> and explanatory variables <span class="math inline">\(X_{1},...,X_{k}\)</span>. Our data are then <span class="math inline">\((y_{i},x_{i1},...,x_{ik})\)</span>, <span class="math inline">\(i=1,...,n\)</span>, or in vector/matrix form <span class="math inline">\(y, x_{1},...,x_{k}\)</span> where <span class="math inline">\(y=(y_{1},...,y_{n})\)</span> and <span class="math inline">\(x_{j}=(x_{1j},...,x_{nj})^{T}\)</span> or <span class="math inline">\(y, X\)</span> where <span class="math inline">\(X=(x_{1},...,x_{k})\)</span>.</p>
<p>Either by design or by conditioning on their observed values, <span class="math inline">\(x_{1},...,x_{k}\)</span> are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:</p>
<div id="def-assumptions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Assumptions A1-A5)</strong></span> <strong>A1. (Additive Error)</strong> <span class="math inline">\(y=\mu+e\)</span> where <span class="math inline">\(e=(e_{1},...,e_{n})^{T}\)</span> is an unobserved random vector with <span class="math inline">\(E(e)=0\)</span>. This implies that <span class="math inline">\(\mu=E(y)\)</span> is the unknown mean of <span class="math inline">\(y\)</span>.</p>
<p><strong>A2. (Linearity)</strong> <span class="math inline">\(\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta\)</span> where <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span> are unknown parameters. This assumption says that <span class="math inline">\(E(y)=\mu\in\text{Col}(X)\)</span> (lies in the column space of <span class="math inline">\(X\)</span>); i.e., it is a linear combination of explanatory vectors <span class="math inline">\(x_{1},...,x_{k}\)</span> with coefficients the unknown parameters in <span class="math inline">\(\beta=(\beta_{1},...,\beta_{k})^{T}\)</span>. Note that it is linear in <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span>, not necessarily in the <span class="math inline">\(x\)</span>’s.</p>
<p><strong>A3. (Independence)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> are independent random variables (and therefore so are <span class="math inline">\(y_{1},...,y_{n})\)</span>.</p>
<p><strong>A4. (Homoscedasticity)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> all have the same variance <span class="math inline">\(\sigma^{2}\)</span>; that is, <span class="math inline">\(\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}\)</span> which implies <span class="math inline">\(\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}\)</span>.</p>
<p><strong>A5. (Normality)</strong> <span class="math inline">\(e\sim N_{n}(0,\sigma^{2}I_{n})\)</span>.</p>
</div>
</section>
<section id="matrix-formulation" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="matrix-formulation"><span class="header-section-number">5.1.2</span> Matrix Formulation</h3>
<p>The model can be written algebraically as: <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n\]</span></p>
<p>Or in matrix notation: <span class="math display">\[
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdot\cdot\cdot &amp; x_{1k}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdot\cdot\cdot &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdot\cdot\cdot &amp; x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
\]</span></p>
<p>This is expressed compactly as: <span class="math display">\[y=X\beta+e\]</span> where <span class="math inline">\(X\)</span> is the design matrix, and <span class="math inline">\(e \sim N_n(0, \sigma^2 I)\)</span>. Alternatively: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e\]</span></p>
<p>Taken together, all five assumptions can be stated more succinctly as: <span class="math display">\[y\sim N_{n}(X\beta,\sigma^{2}I)\]</span> with the mean vector <span class="math inline">\(\mu_{y}=X\beta\in \text{Col}(X)\)</span>.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Note on Coefficients
</div>
</div>
<div class="callout-body-container callout-body">
<p>The effect of a parameter depends upon what other explanatory variables are present in the model. For example, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> in the model: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e\]</span> will typically be different than <span class="math inline">\(\beta_{0}^{*}\)</span> and <span class="math inline">\(\beta_{1}^{*}\)</span> in the model: <span class="math display">\[y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^{*}\]</span> In this context, <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> are the population-projected coefficients of the full model, that is, <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> are the parameters that can best approximate the full model.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will first consider the case that <span class="math inline">\(\text{rank}(X)=k+1\)</span>.</p>
</div>
</div>
</section>
<section id="least-squares-estimator" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="least-squares-estimator"><span class="header-section-number">5.1.3</span> Least Squares Estimator</h3>
<div id="def-least-squares" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 (Least Squares Estimator)</strong></span> The <strong>Least Squares Estimator (LSE)</strong> of <span class="math inline">\(\beta\)</span>, denoted as <span class="math inline">\(\hat{\beta}\)</span>, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses <span class="math inline">\(y\)</span> and the fitted values <span class="math inline">\(X\hat{\beta}\)</span>. <span class="math display">\[
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
\]</span> Solving the normal equations <span class="math inline">\((X'X)\hat{\beta} = X'y\)</span> yields the closed-form solution (assuming <span class="math inline">\(X\)</span> has full rank): <span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'y
\]</span></p>
</div>
</section>
<section id="estimation-example" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4" class="anchored" data-anchor-id="estimation-example"><span class="header-section-number">5.1.4</span> Estimation Example</h3>
<div id="exm-7-3-1a" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 (Computation of <span class="math inline">\(\hat \beta\)</span>)</strong></span> We use the data in <span class="quarto-unresolved-ref">?tbl-7-1</span> to illustrate the computation of <span class="math inline">\(\hat{\beta}\)</span> using the normal equations.</p>
<p>Given the data matrices, we compute the cross-product matrices:</p>
<p><span class="math display">\[
X^{\prime}X = \begin{pmatrix}
12 &amp; 52 &amp; 102\\
52 &amp; 395 &amp; 536\\
102 &amp; 536 &amp; 1004
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
X^{\prime}y = \begin{pmatrix}
90\\
482\\
872
\end{pmatrix}
\]</span></p>
<p>The inverse of the cross-product matrix is: <span class="math display">\[
(X^{\prime}X)^{-1} = \begin{pmatrix}
.97476 &amp; .24290 &amp; -.22871\\
.24290 &amp; .16207 &amp; -.11120\\
-.22871 &amp; -.11120 &amp; .08360
\end{pmatrix}
\]</span></p>
<p>Solving for the estimator: <span class="math display">\[
\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y = \begin{pmatrix}
5.3754\\
3.0118\\
-1.2855
\end{pmatrix}
\]</span></p>
</div>
</section>
<section id="properties-of-the-estimator" class="level3" data-number="5.1.5">
<h3 data-number="5.1.5" class="anchored" data-anchor-id="properties-of-the-estimator"><span class="header-section-number">5.1.5</span> Properties of the Estimator</h3>
<div id="thm-unbiased" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (Unbiasedness of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span>, then <span class="math inline">\(\hat{\beta}\)</span> is an unbiased estimator for <span class="math inline">\(\beta\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
E(\hat{\beta}) &amp;= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&amp;= \beta
\end{aligned}
\]</span></p>
</div>
<div id="thm-covariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2 (Variance of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the covariance matrix for <span class="math inline">\(\hat{\beta}\)</span> is given by <span class="math inline">\(\sigma^{2}(X^{\prime}X)^{-1}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\beta}) &amp;= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
\]</span></p>
</div>
<p><strong>Note:</strong> These theorems require no assumption of normality.</p>
</section>
</section>
<section id="best-linear-unbiased-estimator-blue" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="best-linear-unbiased-estimator-blue"><span class="header-section-number">5.2</span> Best Linear Unbiased Estimator (BLUE)</h2>
<div id="thm-gauss-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3 (Gauss-Markov Theorem)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the least-squares estimators <span class="math inline">\(\hat{\beta}_{j}, j=0,1,...,k\)</span> have minimum variance among all linear unbiased estimators.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We consider a linear estimator <span class="math inline">\(Ay\)</span> of <span class="math inline">\(\beta\)</span> and seek the matrix <span class="math inline">\(A\)</span> for which <span class="math inline">\(Ay\)</span> is a minimum variance unbiased estimator.</p>
<p><strong>1. Unbiasedness Condition:</strong> In order for <span class="math inline">\(Ay\)</span> to be an unbiased estimator of <span class="math inline">\(\beta\)</span>, we must have <span class="math inline">\(E(Ay)=\beta\)</span>. Using the assumption <span class="math inline">\(E(y)=X\beta\)</span>, this is expressed as: <span class="math display">\[E(Ay) = A E(y) = AX\beta = \beta\]</span> which implies the condition <span class="math inline">\(AX=I_{k+1}\)</span> since the relationship must hold for any <span class="math inline">\(\beta\)</span>.</p>
<p><strong>2. Minimizing Variance:</strong> The covariance matrix for the estimator <span class="math inline">\(Ay\)</span> is: <span class="math display">\[\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'\]</span> We need to choose <span class="math inline">\(A\)</span> (subject to <span class="math inline">\(AX=I\)</span>) so that the diagonal elements of <span class="math inline">\(AA'\)</span> are minimized.</p>
<p>To relate <span class="math inline">\(Ay\)</span> to <span class="math inline">\(\hat{\beta}=(X'X)^{-1}X'y\)</span>, we define <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span> and write <span class="math inline">\(A = (A - \hat{A}) + \hat{A}\)</span>. Then: <span class="math display">\[AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'\]</span> Expanding this, the cross terms vanish because <span class="math inline">\((A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'\)</span>. Note that <span class="math inline">\(\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}\)</span>. Also, <span class="math inline">\(A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}\)</span> (since <span class="math inline">\(AX=I\)</span>). Thus, <span class="math inline">\((A - \hat{A})\hat{A}' = 0\)</span>.</p>
<p>The expansion simplifies to: <span class="math display">\[AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'\]</span> The matrix <span class="math inline">\((A - \hat{A})(A - \hat{A})'\)</span> is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of <span class="math inline">\(AA'\)</span>, we must set <span class="math inline">\(A - \hat{A} = 0\)</span>, which implies <span class="math inline">\(A = \hat{A}\)</span>.</p>
<p>Thus, the minimum variance estimator is: <span class="math display">\[Ay = (X'X)^{-1}X'y = \hat{\beta}\]</span></p>
</div>
<section id="notes-on-gauss-markov" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="notes-on-gauss-markov"><span class="header-section-number">5.2.1</span> Notes on Gauss-Markov</h3>
<ol type="1">
<li><p><strong>Distributional Generality:</strong> The remarkable feature of the Gauss-Markov theorem is that it holds for <em>any</em> distribution of <span class="math inline">\(y\)</span>; normality is not required. The only assumptions used are linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>).</p></li>
<li><p><strong>Extension to All Linear Combinations:</strong> The theorem extends beyond just the parameter vector <span class="math inline">\(\beta\)</span> to any linear combination of the parameters.</p></li>
</ol>
<div id="cor-linear-combo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5.1 (BLUE for All Linear Combinations)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the best linear unbiased estimator of the scalar <span class="math inline">\(a'\beta\)</span> is <span class="math inline">\(a'\hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> is the least-squares estimator.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\tilde{\beta} = Ay\)</span> be any other linear unbiased estimator of <span class="math inline">\(\beta\)</span>. The variance of the linear combination <span class="math inline">\(a'\tilde{\beta}\)</span> is: <span class="math display">\[
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
\]</span> From the proof of the Gauss-Markov theorem, we established that <span class="math inline">\(AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}\)</span> where <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span>. Substituting this into the variance equation: <span class="math display">\[
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
\]</span> The term <span class="math inline">\(a'(A-\hat{A})(A-\hat{A})'a\)</span> is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore: <span class="math display">\[
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
\]</span> The variance is minimized when <span class="math inline">\(A=\hat{A}\)</span> (specifically when the first term is zero), proving that <span class="math inline">\(a'\hat{\beta}\)</span> has the minimum variance among all linear unbiased estimators.</p>
</div>
<ol start="3" type="1">
<li><strong>Scaling Invariance:</strong> The predictions made by the model are invariant to the scaling of the explanatory variables.</li>
</ol>
<div id="thm-scaling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.4 (Scaling Explanatory Variables)</strong></span> If <span class="math inline">\(x=(1,x_{1},...,x_{k})'\)</span> and <span class="math inline">\(z=(1,c_{1}x_{1},...,c_{k}x_{k})'\)</span>, then the fitted values are identical: <span class="math inline">\(\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(D = \text{diag}(1, c_1, ..., c_k)\)</span> such that the design matrix is transformed to <span class="math inline">\(Z = XD\)</span>. The LSE for the transformed data is: <span class="math display">\[
\begin{aligned}
\hat{\beta}_z &amp;= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&amp;= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&amp;= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
\]</span> . Then, the prediction is: <span class="math display">\[
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
\]</span> .</p>
</div>
<section id="limitations-restriction-to-unbiased-estimators" class="level4" data-number="5.2.1.1">
<h4 data-number="5.2.1.1" class="anchored" data-anchor-id="limitations-restriction-to-unbiased-estimators"><span class="header-section-number">5.2.1.1</span> Limitations: Restriction to Unbiased Estimators</h4>
<p>It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of <strong>linear</strong> and <strong>unbiased</strong> estimators.</p>
<ul>
<li><strong>Assumption Sensitivity:</strong> If the assumptions of linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>) do not hold, <span class="math inline">\(\hat{\beta}\)</span> may be biased or may have a larger variance than other estimators.</li>
<li><strong>Unbiasedness Constraint:</strong> The theorem does not compare <span class="math inline">\(\hat{\beta}\)</span> to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only “best” (minimum variance) among those estimators that satisfy the unbiasedness constraint.</li>
</ul>
</section>
</section>
</section>
<section id="estimator-of-error-variance" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="estimator-of-error-variance"><span class="header-section-number">5.3</span> Estimator of Error Variance</h2>
<p>We estimate <span class="math inline">\(\sigma^{2}\)</span> by the residual mean square:</p>
<div id="def-s2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (Residual Variance Estimator)</strong></span> <span class="math display">\[s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}\]</span> where <span class="math inline">\(\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})\)</span>.</p>
</div>
<p>Alternatively, SSE can be written as: <span class="math display">\[\text{SSE} = y'y - \hat{\beta}'X'y\]</span> This is often useful for computation (<span class="math inline">\(y'y\)</span> is the total sum of squares of the raw data).</p>
<section id="unbiasedness-of-s2" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="unbiasedness-of-s2"><span class="header-section-number">5.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></h3>
<div id="thm-unbiased-s2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.5 (Unbiasedness of s-squared)</strong></span> If <span class="math inline">\(s^{2}\)</span> is defined as above, and if <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, then <span class="math inline">\(E(s^{2})=\sigma^{2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We use the Hat Matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>, which projects <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>. Thus, <span class="math inline">\(\hat{y} = Hy\)</span>. The residuals are <span class="math inline">\(y - \hat{y} = (I - H)y\)</span>. The Sum of Squared Errors is: <span class="math display">\[\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y\]</span> Since <span class="math inline">\(H\)</span> is symmetric and idempotent, <span class="math inline">\((I-H)\)</span> is also symmetric and idempotent. Thus: <span class="math display">\[\text{SSE} = y'(I-H)y\]</span></p>
<p>To find the expectation, we use the trace trick for quadratic forms: <span class="math inline">\(E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]\)</span>. <span class="math display">\[
\begin{aligned}
E(\text{SSE}) &amp;= E[y'(I-H)y] \\
&amp;= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&amp;= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
\]</span> <strong>Trace Term:</strong> <span class="math inline">\(\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)\)</span>, since <span class="math inline">\(\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1\)</span>.</p>
<p><strong>Non-centrality Term:</strong> Since <span class="math inline">\(HX = X\)</span>, we have <span class="math inline">\((I-H)X = 0\)</span>. Therefore, the second term vanishes: <span class="math inline">\(\beta'X'(I-H)X\beta = 0\)</span>.</p>
<p>Combining these: <span class="math display">\[E(\text{SSE}) = \sigma^2(n - k - 1)\]</span> Dividing by the degrees of freedom <span class="math inline">\((n-k-1)\)</span>, we get <span class="math inline">\(E(s^2) = \sigma^2\)</span>.</p>
</div>
</section>
</section>
<section id="distributions-under-normality" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="distributions-under-normality"><span class="header-section-number">5.4</span> Distributions under Normality</h2>
<p>If we add Assumption A5 (<span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>), we can derive the exact sampling distributions.</p>
<div id="cor-cov-beta" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5.2 (Estimated Covariance of Beta)</strong></span> An unbiased estimator of <span class="math inline">\(\text{Cov}(\hat{\beta})\)</span> is given by: <span class="math display">\[\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}\]</span></p>
</div>
<div id="thm-sampling-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.6 (Sampling Distributions)</strong></span> Under assumptions A1-A5:</p>
<ol type="1">
<li><span class="math inline">\(\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})\)</span>.</li>
<li><span class="math inline">\((n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(s^{2}\)</span> are independent.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part (i):</strong> Since <span class="math inline">\(\hat{\beta} = (X'X)^{-1}X'y\)</span> is a linear transformation of the normal vector <span class="math inline">\(y\)</span>, it is also normally distributed. We already established its mean and variance in <a href="#thm-unbiased" class="quarto-xref">Theorem&nbsp;<span>5.1</span></a> and <a href="#thm-covariance" class="quarto-xref">Theorem&nbsp;<span>5.2</span></a>.</p>
<p><strong>Part (ii):</strong> We showed <span class="math inline">\(\text{SSE} = y'(I-H)y\)</span>. Since <span class="math inline">\((I-H)\)</span> is idempotent with rank <span class="math inline">\(n-k-1\)</span>, and <span class="math inline">\((I-H)X\beta = 0\)</span>, by the theory of quadratic forms in normal variables, <span class="math inline">\(\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)\)</span>.</p>
<p><strong>Part (iii):</strong> <span class="math inline">\(\hat{\beta}\)</span> depends on <span class="math inline">\(Hy\)</span> (or <span class="math inline">\(X'y\)</span>), while <span class="math inline">\(s^2\)</span> depends on <span class="math inline">\((I-H)y\)</span>. Since <span class="math inline">\(H(I-H) = H - H^2 = 0\)</span>, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.</p>
</div>
</section>
<section id="maximum-likelihood-estimator-mle" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="maximum-likelihood-estimator-mle"><span class="header-section-number">5.5</span> Maximum Likelihood Estimator (MLE)</h2>
<div id="thm-mle" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.7 (MLE for Linear Regression)</strong></span> If <span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>, the Maximum Likelihood Estimators are: <span class="math display">\[
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
\]</span> <span class="math display">\[
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The log-likelihood function is: <span class="math display">\[ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) \]</span> Maximizing this with respect to <span class="math inline">\(\beta\)</span> is equivalent to minimizing the quadratic term <span class="math inline">\((y - X\beta)'(y - X\beta)\)</span>, which yields the Least Squares Estimator. Differentiating with respect to <span class="math inline">\(\sigma^2\)</span> and setting to zero yields <span class="math inline">\(\hat{\sigma}^2 = \text{SSE}/n\)</span>.</p>
</div>
<p><strong>Note:</strong> The MLE for <span class="math inline">\(\sigma^2\)</span> is biased (denominator <span class="math inline">\(n\)</span>), whereas <span class="math inline">\(s^2\)</span> is unbiased (denominator <span class="math inline">\(n-k-1\)</span>).</p>
</section>
<section id="linear-models-in-centered-form" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="linear-models-in-centered-form"><span class="header-section-number">5.6</span> Linear Models in Centered Form</h2>
<p>The regression model can be written in a centered form by subtracting the means of the explanatory variables: <span class="math display">\[y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}\]</span> for <span class="math inline">\(i=1,...,n\)</span>, where the intercept term is adjusted: <span class="math display">\[\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}\]</span> and <span class="math inline">\(\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}\)</span>.</p>
<section id="matrix-formulation-1" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="matrix-formulation-1"><span class="header-section-number">5.6.1</span> Matrix Formulation</h3>
<p>In matrix form, the equivalence between the original model and the centered model is: <span class="math display">\[y = X\beta + e = (j_n, X_c)\begin{pmatrix} \alpha \\ \beta_{1} \end{pmatrix} + e\]</span> where <span class="math inline">\(\beta_{1}=(\beta_{1},...,\beta_{k})^{T}\)</span> represents the slope coefficients, and <span class="math inline">\(X_c\)</span> is the centered design matrix: <span class="math display">\[X_c = (I - P_{j_n})X_1\]</span> Here, <span class="math inline">\(X_1\)</span> consists of the original columns of <span class="math inline">\(X\)</span> excluding the intercept column.</p>
</section>
<section id="estimation-in-centered-form" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="estimation-in-centered-form"><span class="header-section-number">5.6.2</span> Estimation in Centered Form</h3>
<p>Because the column space of the intercept <span class="math inline">\(j_n\)</span> is orthogonal to the columns of <span class="math inline">\(X_c\)</span> (since columns of <span class="math inline">\(X_c\)</span> sum to zero), the cross-product matrix becomes block diagonal: <span class="math display">\[
\begin{pmatrix} j_n' \\ X_c' \end{pmatrix} (j_n, X_c) = \begin{pmatrix} j_n'j_n &amp; j_n'X_c \\ X_c'j_n &amp; X_c'X_c \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}
\]</span></p>
<div id="thm-centered-estimators" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.8 (Centered Estimators)</strong></span> The least squares estimators for the centered parameters are: <span class="math display">\[
\begin{pmatrix} \hat{\alpha} \\ \hat{\beta}_{1} \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}^{-1} \begin{pmatrix} j_n'y \\ X_c'y \end{pmatrix} = \begin{pmatrix} \bar{y} \\ (X_c'X_c)^{-1}X_c'y \end{pmatrix}
\]</span> Thus: 1. <span class="math inline">\(\hat{\alpha} = \bar{y}\)</span> (The sample mean of <span class="math inline">\(y\)</span>). 2. <span class="math inline">\(\hat{\beta}_{1} = S_{xx}^{-1}S_{xy}\)</span>, using the sample covariance notations.</p>
</div>
<p>Recovering the original intercept: <span class="math display">\[ \hat{\beta}_0 = \hat{\alpha} - \hat{\beta}_1 \bar{x}_1 - \dots - \hat{\beta}_k \bar{x}_k = \bar{y} - \hat{\beta}_{1}'\bar{x} \]</span></p>
</section>
</section>
<section id="sum-of-squares-decomposition" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="sum-of-squares-decomposition"><span class="header-section-number">5.7</span> Sum of Squares Decomposition</h2>
<p>We partition the total variation based on the orthogonal subspaces.</p>
<div id="def-ss-components" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 (Sum of Squares Components)</strong></span> The total variation is decomposed as <span class="math inline">\(SST = SSR + SSE\)</span>.</p>
<ol type="1">
<li><p><strong>Total Sum of Squares (SST):</strong> The squared length of the centered response vector. <span class="math display">\[SST = \|y - \bar{y}j_n\|^2 = \|(I - P_{j_n})y\|^2\]</span></p></li>
<li><p><strong>Regression Sum of Squares (SSR):</strong> The variation explained by the regressors <span class="math inline">\(X_c\)</span>. <span class="math display">\[SSR = \|\hat{y} - \bar{y}j_n\|^2 = \|P_{X_c}y\|^2 = \hat{\beta}_1' X_c' X_c \hat{\beta}_1\]</span></p></li>
<li><p><strong>Sum of Squared Errors (SSE):</strong> The residual variation. <span class="math display">\[SSE = \|y - \hat{y}\|^2 = \|(I - P_{X})y\|^2\]</span></p></li>
</ol>
</div>
<section id="geometry-of-the-decomposition" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="geometry-of-the-decomposition"><span class="header-section-number">5.7.1</span> Geometry of the Decomposition</h3>
<p>The decomposition relies on the orthogonality of the subspaces: <span class="math display">\[ \text{Col}(X) = \text{Col}(j_n) \oplus \text{Col}(X_c) \]</span> The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span> is the sum of the projection onto the intercept (giving <span class="math inline">\(\bar{y}j_n\)</span>) and the projection onto the centered variables (giving <span class="math inline">\(X_c\hat{\beta}_1\)</span>).</p>
<p><span class="math display">\[ \|y - \bar{y}j_n\|^2 = \|\hat{y} - \bar{y}j_n\|^2 + \|y - \hat{y}\|^2 \]</span></p>
</section>
</section>
<section id="coefficient-of-determination-r2" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="coefficient-of-determination-r2"><span class="header-section-number">5.8</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h2>
<p>The <span class="math inline">\(R^2\)</span> statistic measures the proportion of total variation explained by the regression model.</p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 (R-Squared)</strong></span> <span class="math display">\[R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}\]</span> Since <span class="math inline">\(0 \le SSE \le SST\)</span>, it follows that <span class="math inline">\(0 \le R^2 \le 1\)</span>.</p>
</div>
<p>In terms of the centered coefficients: <span class="math display">\[R^2 = \frac{\hat{\beta}_1' X_c' X_c \hat{\beta}_1}{\sum (y_i - \bar{y})^2}\]</span></p>
<section id="adjusted-r2-r2_a" class="level3" data-number="5.8.1">
<h3 data-number="5.8.1" class="anchored" data-anchor-id="adjusted-r2-r2_a"><span class="header-section-number">5.8.1</span> Adjusted <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2_a\)</span>)</h3>
<p>Standard <span class="math inline">\(R^2\)</span> always increases (or stays the same) as predictors are added, regardless of their value. The Adjusted <span class="math inline">\(R^2\)</span> penalizes for model complexity.</p>
<div id="def-adj-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6 (Adjusted R-Squared)</strong></span> <span class="math display">\[R^2_a = 1 - \frac{SSE / (n - k - 1)}{SST / (n - 1)} = 1 - \frac{\text{MSE}}{s_y^2}\]</span> Relationship to <span class="math inline">\(R^2\)</span>: <span class="math display">\[R^2_a = 1 - (1 - R^2) \frac{n-1}{n-k-1}\]</span></p>
</div>
</section>
<section id="expected-value-of-r2" class="level3" data-number="5.8.2">
<h3 data-number="5.8.2" class="anchored" data-anchor-id="expected-value-of-r2"><span class="header-section-number">5.8.2</span> Expected Value of <span class="math inline">\(R^2\)</span></h3>
<p>If the true slope coefficients are zero (<span class="math inline">\(\beta_1 = \dots = \beta_k = 0\)</span>), the expected value of <span class="math inline">\(R^2\)</span> is not zero due to mathematical constraints.</p>
<ol type="1">
<li><span class="math inline">\(E[SSE] = (n - k - 1)\sigma^2\)</span>.</li>
<li><span class="math inline">\(E[SST] \approx (n-1)\sigma^2\)</span> (under the null hypothesis).</li>
</ol>
<p>Thus, under the null hypothesis: <span class="math display">\[E[R^2] \approx \frac{k}{n-1}\]</span></p>
<p>However, for the Adjusted <span class="math inline">\(R^2\)</span>, under the null hypothesis: <span class="math display">\[E[R^2_a] \approx 1 - \frac{\sigma^2}{\sigma^2} = 0\]</span></p>
<p>This property makes <span class="math inline">\(R^2_a\)</span> a more appropriate metric for comparing models with different numbers of predictors, as it accounts for “overfitting” by penalizing the degrees of freedom consumed by the model.</p>
</section>
</section>
<section id="underfitting-and-overfitting" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="underfitting-and-overfitting"><span class="header-section-number">5.9</span> Underfitting and Overfitting</h2>
<p>We consider the effects of omitting explanatory variables that should be included (underfitting) and including variables that should be excluded (overfitting).</p>
<p>Suppose the true model is: <span class="math display">\[y = X\beta + e = \begin{pmatrix} X_1 &amp; X_2 \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} + e = X_1\beta_1 + X_2\beta_2 + e \quad (\dagger)\]</span> where <span class="math inline">\(\text{Var}(e) = \sigma^2 I\)</span>.</p>
<ul>
<li><strong>Underfitting:</strong> Leaving out <span class="math inline">\(X_2\beta_2\)</span> when <span class="math inline">\(\beta_2 \neq 0\)</span>.</li>
<li><strong>Overfitting:</strong> Including <span class="math inline">\(X_2\beta_2\)</span> when <span class="math inline">\(\beta_2 = 0\)</span>.</li>
</ul>
<section id="underfitting" class="level3" data-number="5.9.1">
<h3 data-number="5.9.1" class="anchored" data-anchor-id="underfitting"><span class="header-section-number">5.9.1</span> Underfitting</h3>
<p>Suppose model <span class="math inline">\((\dagger)\)</span> holds, but we fit the reduced model: <span class="math display">\[y = X_1\beta_1^* + e^*\]</span> The OLS estimator for this reduced model is <span class="math inline">\(\hat{\beta}_1^* = (X_1^T X_1)^{-1}X_1^T y\)</span>.</p>
<div id="thm-underfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.9 (Bias and Variance under Underfitting)</strong></span> If we fit the reduced model when the full model <span class="math inline">\((\dagger)\)</span> is true:</p>
<ol type="1">
<li><strong>Bias:</strong> <span class="math inline">\(E(\hat{\beta}_1^*) = \beta_1 + A\beta_2\)</span>, where <span class="math inline">\(A = (X_1^T X_1)^{-1}X_1^T X_2\)</span> is the alias matrix.</li>
<li><strong>Variance:</strong> <span class="math inline">\(\text{Var}(\hat{\beta}_1^*) = \sigma^2(X_1^T X_1)^{-1}\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part (i):</strong> <span class="math display">\[
\begin{aligned}
E(\hat{\beta}_1^*) &amp;= E[(X_1^T X_1)^{-1}X_1^T y] \\
&amp;= (X_1^T X_1)^{-1}X_1^T E(y) \\
&amp;= (X_1^T X_1)^{-1}X_1^T (X_1\beta_1 + X_2\beta_2) \\
&amp;= (X_1^T X_1)^{-1}X_1^T X_1\beta_1 + (X_1^T X_1)^{-1}X_1^T X_2\beta_2 \\
&amp;= \beta_1 + A\beta_2
\end{aligned}
\]</span> Thus, <span class="math inline">\(\hat{\beta}_1^*\)</span> is biased unless <span class="math inline">\(\beta_2 = 0\)</span> or <span class="math inline">\(X_1^T X_2 = 0\)</span> (orthogonal design).</p>
<p><strong>Part (ii):</strong> <span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\beta}_1^*) &amp;= \text{Var}[(X_1^T X_1)^{-1}X_1^T y] \\
&amp;= (X_1^T X_1)^{-1}X_1^T [\sigma^2 I] X_1 (X_1^T X_1)^{-1} \\
&amp;= \sigma^2 (X_1^T X_1)^{-1}
\end{aligned}
\]</span> .</p>
</div>
</section>
<section id="overfitting" class="level3" data-number="5.9.2">
<h3 data-number="5.9.2" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">5.9.2</span> Overfitting</h3>
<p>Suppose the reduced model <span class="math inline">\(y = X_1\beta_1^* + e\)</span> is true (i.e., <span class="math inline">\(\beta_2 = 0\)</span>), but we fit the full model <span class="math inline">\((\dagger)\)</span>. Since the full model includes the true model as a special case, the estimator <span class="math inline">\(\hat{\beta}\)</span> from the full model remains unbiased.</p>
<p>However, fitting the extraneous variables affects the variance.</p>
<div id="thm-overfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.10 (Variance Comparison)</strong></span> Let <span class="math inline">\(\hat{\beta}_1\)</span> be the estimator from the full model and <span class="math inline">\(\hat{\beta}_1^*\)</span> be the estimator from the reduced model. Then: <span class="math display">\[ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T \]</span> where <span class="math inline">\(A = (X_1^T X_1)^{-1}X_1^T X_2\)</span> and <span class="math inline">\(B = X_2^T X_2 - X_2^T X_1 A\)</span>. Since <span class="math inline">\(A B^{-1} A^T\)</span> is positive semidefinite, <span class="math inline">\(\text{Var}(\hat{\beta}_1) \ge \text{Var}(\hat{\beta}_1^*)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using the inverse of a partitioned matrix, the top-left block of <span class="math inline">\((X^T X)^{-1}\)</span> corresponding to <span class="math inline">\(\beta_1\)</span> is: <span class="math display">\[ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} \]</span> Since <span class="math inline">\(\text{Var}(\hat{\beta}_1) = \sigma^2 H^{11}\)</span> and <span class="math inline">\(\text{Var}(\hat{\beta}_1^*) = \sigma^2 (X_1^T X_1)^{-1}\)</span>, the difference is the second term: <span class="math display">\[ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T \]</span> .</p>
</div>
</section>
<section id="summary" class="level3" data-number="5.9.3">
<h3 data-number="5.9.3" class="anchored" data-anchor-id="summary"><span class="header-section-number">5.9.3</span> Summary</h3>
<ol type="1">
<li><strong>Underfitting:</strong> Reduces variance but introduces bias (unless variables are orthogonal).</li>
<li><strong>Overfitting:</strong> Keeps estimators unbiased but increases variance.</li>
</ol>
<p>The goal of model selection is to balance these two errors, adhering to the principle of <strong>Occam’s Razor</strong>: “entities should not be multiplied beyond necessity”.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./lec4-qf.html" class="pagination-link" aria-label="Distribution of Quadratic Forms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Distribution of Quadratic Forms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>