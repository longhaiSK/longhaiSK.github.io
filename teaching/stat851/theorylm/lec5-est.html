<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Estimation in Multiple Linear Regression – Theory of Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ginv.html" rel="next">
<link href="./lec4-qf.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1b3e43c72e8be34557c75123b0b69e0d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d1855ce4d3ca2472244e2456266329f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.9/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<script src="site_libs/jquery-3.5.1/jquery.min.js"></script>
<link href="site_libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/mystyles.css">
<script src="resources/num_eq.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./lec5-est.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Estimation in Multiple Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Theory of Linear Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec1-vecspace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Projection in Vector Space</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec2-matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec3-mvn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multivariate Normal Distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec4-qf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Distribution of Quadratic Forms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lec5-est.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Estimation in Multiple Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ginv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Generalized Inverse</span></span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#linear-models-and-least-square-estimator" id="toc-linear-models-and-least-square-estimator" class="nav-link active" data-scroll-target="#linear-models-and-least-square-estimator"><span class="header-section-number">6.1</span> Linear Models and Least Square Estimator</a>
  <ul>
  <li><a href="#assumptions-in-linear-models" id="toc-assumptions-in-linear-models" class="nav-link" data-scroll-target="#assumptions-in-linear-models"><span class="header-section-number">6.1.1</span> Assumptions in Linear Models</a></li>
  <li><a href="#matrix-formulation" id="toc-matrix-formulation" class="nav-link" data-scroll-target="#matrix-formulation"><span class="header-section-number">6.1.2</span> Matrix Formulation</a></li>
  <li><a href="#least-squares-estimator-of-beta-and-fitted-value-hat-y" id="toc-least-squares-estimator-of-beta-and-fitted-value-hat-y" class="nav-link" data-scroll-target="#least-squares-estimator-of-beta-and-fitted-value-hat-y"><span class="header-section-number">6.1.3</span> Least Squares Estimator of <span class="math inline">\(\beta\)</span> and Fitted Value <span class="math inline">\(\hat Y\)</span></a>
  <ul class="collapse">
  <li><a href="#obtaining-hat-y" id="toc-obtaining-hat-y" class="nav-link" data-scroll-target="#obtaining-hat-y">1. Obtaining <span class="math inline">\(\hat Y\)</span></a></li>
  <li><a href="#obtaining-hatbeta-by-solving-xbeta-haty" id="toc-obtaining-hatbeta-by-solving-xbeta-haty" class="nav-link" data-scroll-target="#obtaining-hatbeta-by-solving-xbeta-haty">2. Obtaining <span class="math inline">\(\hat{\beta}\)</span> by Solving <span class="math inline">\(x\beta = \hat{y}\)</span></a></li>
  </ul></li>
  <li><a href="#properties-of-the-estimator-hat-beta" id="toc-properties-of-the-estimator-hat-beta" class="nav-link" data-scroll-target="#properties-of-the-estimator-hat-beta"><span class="header-section-number">6.1.4</span> Properties of the Estimator <span class="math inline">\(\hat \beta\)</span></a></li>
  </ul></li>
  <li><a href="#best-linear-unbiased-estimator-blue" id="toc-best-linear-unbiased-estimator-blue" class="nav-link" data-scroll-target="#best-linear-unbiased-estimator-blue"><span class="header-section-number">6.2</span> Best Linear Unbiased Estimator (BLUE)</a>
  <ul>
  <li><a href="#notes-on-gauss-markov" id="toc-notes-on-gauss-markov" class="nav-link" data-scroll-target="#notes-on-gauss-markov"><span class="header-section-number">6.2.1</span> Notes on Gauss-markov</a>
  <ul class="collapse">
  <li><a href="#limitations-restriction-to-unbiased-estimators" id="toc-limitations-restriction-to-unbiased-estimators" class="nav-link" data-scroll-target="#limitations-restriction-to-unbiased-estimators"><span class="header-section-number">6.2.1.1</span> Limitations: Restriction to Unbiased Estimators</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#estimator-of-error-variance" id="toc-estimator-of-error-variance" class="nav-link" data-scroll-target="#estimator-of-error-variance"><span class="header-section-number">6.3</span> Estimator of Error Variance</a>
  <ul>
  <li><a href="#unbiasedness-of-s2" id="toc-unbiasedness-of-s2" class="nav-link" data-scroll-target="#unbiasedness-of-s2"><span class="header-section-number">6.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></a></li>
  </ul></li>
  <li><a href="#distributions-under-normality" id="toc-distributions-under-normality" class="nav-link" data-scroll-target="#distributions-under-normality"><span class="header-section-number">6.4</span> Distributions Under Normality</a></li>
  <li><a href="#maximum-likelihood-estimator-mle" id="toc-maximum-likelihood-estimator-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimator-mle"><span class="header-section-number">6.5</span> Maximum Likelihood Estimator (MLE)</a></li>
  <li><a href="#linear-models-in-centered-form" id="toc-linear-models-in-centered-form" class="nav-link" data-scroll-target="#linear-models-in-centered-form"><span class="header-section-number">6.6</span> Linear Models in Centered Form</a>
  <ul>
  <li><a href="#matrix-formulation-1" id="toc-matrix-formulation-1" class="nav-link" data-scroll-target="#matrix-formulation-1"><span class="header-section-number">6.6.1</span> Matrix Formulation</a></li>
  <li><a href="#estimation-in-centered-form" id="toc-estimation-in-centered-form" class="nav-link" data-scroll-target="#estimation-in-centered-form"><span class="header-section-number">6.6.2</span> Estimation in Centered Form</a></li>
  </ul></li>
  <li><a href="#sum-of-squares-decomposition" id="toc-sum-of-squares-decomposition" class="nav-link" data-scroll-target="#sum-of-squares-decomposition"><span class="header-section-number">6.7</span> Sum of Squares Decomposition</a>
  <ul>
  <li><a href="#d-visualization-of-decomposition-of-y" id="toc-d-visualization-of-decomposition-of-y" class="nav-link" data-scroll-target="#d-visualization-of-decomposition-of-y"><span class="header-section-number">6.7.1</span> 3D Visualization of Decomposition of <span class="math inline">\(y\)</span></a></li>
  <li><a href="#a-diagram-to-show-decomposition-of-sum-of-squares" id="toc-a-diagram-to-show-decomposition-of-sum-of-squares" class="nav-link" data-scroll-target="#a-diagram-to-show-decomposition-of-sum-of-squares"><span class="header-section-number">6.7.2</span> A Diagram to Show Decomposition of Sum of Squares</a></li>
  <li><a href="#distribution-of-sum-of-squares" id="toc-distribution-of-sum-of-squares" class="nav-link" data-scroll-target="#distribution-of-sum-of-squares"><span class="header-section-number">6.7.3</span> Distribution of Sum of Squares</a></li>
  </ul></li>
  <li><a href="#f-test-for-testing-overall-regression-effect" id="toc-f-test-for-testing-overall-regression-effect" class="nav-link" data-scroll-target="#f-test-for-testing-overall-regression-effect"><span class="header-section-number">6.8</span> F-test for Testing Overall Regression Effect</a>
  <ul>
  <li><a href="#the-f-statistic" id="toc-the-f-statistic" class="nav-link" data-scroll-target="#the-f-statistic">The F-statistic</a></li>
  <li><a href="#understanding-f-via-expectations" id="toc-understanding-f-via-expectations" class="nav-link" data-scroll-target="#understanding-f-via-expectations">Understanding <span class="math inline">\(F\)</span> via Expectations</a></li>
  <li><a href="#distributional-theory" id="toc-distributional-theory" class="nav-link" data-scroll-target="#distributional-theory"><span class="header-section-number">6.8.1</span> Distributional Theory</a></li>
  <li><a href="#visualization-of-the-rejection-region" id="toc-visualization-of-the-rejection-region" class="nav-link" data-scroll-target="#visualization-of-the-rejection-region"><span class="header-section-number">6.8.2</span> Visualization of the Rejection Region</a></li>
  </ul></li>
  <li><a href="#coefficient-of-determination-r2" id="toc-coefficient-of-determination-r2" class="nav-link" data-scroll-target="#coefficient-of-determination-r2"><span class="header-section-number">6.9</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</a>
  <ul>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">6.9.1</span> Definition</a></li>
  <li><a href="#expectation-and-bias" id="toc-expectation-and-bias" class="nav-link" data-scroll-target="#expectation-and-bias"><span class="header-section-number">6.9.2</span> Expectation and Bias</a></li>
  <li><a href="#exact-distribution" id="toc-exact-distribution" class="nav-link" data-scroll-target="#exact-distribution"><span class="header-section-number">6.9.3</span> Exact Distribution</a></li>
  <li><a href="#adjusted-r-squared-r2_a" id="toc-adjusted-r-squared-r2_a" class="nav-link" data-scroll-target="#adjusted-r-squared-r2_a"><span class="header-section-number">6.9.4</span> Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</a></li>
  <li><a href="#relationship-with-rao-blackwell-decomposition-of-variances" id="toc-relationship-with-rao-blackwell-decomposition-of-variances" class="nav-link" data-scroll-target="#relationship-with-rao-blackwell-decomposition-of-variances"><span class="header-section-number">6.9.5</span> Relationship with Rao-blackwell Decomposition of Variances</a></li>
  <li><a href="#relationship-with-f-test" id="toc-relationship-with-f-test" class="nav-link" data-scroll-target="#relationship-with-f-test"><span class="header-section-number">6.9.6</span> Relationship with <span class="math inline">\(F\)</span> Test</a></li>
  <li><a href="#an-animation-for-illustrating-r2_a-under-h_0-and-h_1" id="toc-an-animation-for-illustrating-r2_a-under-h_0-and-h_1" class="nav-link" data-scroll-target="#an-animation-for-illustrating-r2_a-under-h_0-and-h_1"><span class="header-section-number">6.9.7</span> An Animation for Illustrating <span class="math inline">\(r^2_a\)</span> Under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></a></li>
  </ul></li>
  <li><a href="#underfitting-and-overfitting" id="toc-underfitting-and-overfitting" class="nav-link" data-scroll-target="#underfitting-and-overfitting"><span class="header-section-number">6.10</span> Underfitting and Overfitting</a>
  <ul>
  <li><a href="#underfitting" id="toc-underfitting" class="nav-link" data-scroll-target="#underfitting"><span class="header-section-number">6.10.1</span> Underfitting</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting"><span class="header-section-number">6.10.2</span> Overfitting</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6.10.3</span> Summary</a></li>
  </ul></li>
  <li><a href="#a-data-example-with-house-price-valuation" id="toc-a-data-example-with-house-price-valuation" class="nav-link" data-scroll-target="#a-data-example-with-house-price-valuation"><span class="header-section-number">6.11</span> A Data Example with House Price Valuation</a>
  <ul>
  <li><a href="#visualize-the-data" id="toc-visualize-the-data" class="nav-link" data-scroll-target="#visualize-the-data"><span class="header-section-number">6.11.1</span> Visualize the Data</a></li>
  <li><a href="#fit-the-model" id="toc-fit-the-model" class="nav-link" data-scroll-target="#fit-the-model"><span class="header-section-number">6.11.2</span> Fit the Model</a>
  <ul class="collapse">
  <li><a href="#method-1-naive-matrix-formula" id="toc-method-1-naive-matrix-formula" class="nav-link" data-scroll-target="#method-1-naive-matrix-formula"><span class="header-section-number">6.11.2.1</span> Method 1: Naive Matrix Formula</a></li>
  <li><a href="#method-2-centralized-formula" id="toc-method-2-centralized-formula" class="nav-link" data-scroll-target="#method-2-centralized-formula"><span class="header-section-number">6.11.2.2</span> Method 2: Centralized Formula</a></li>
  <li><a href="#method-3-using-rs-lm-function" id="toc-method-3-using-rs-lm-function" class="nav-link" data-scroll-target="#method-3-using-rs-lm-function"><span class="header-section-number">6.11.2.3</span> Method 3: Using R’s <code>lm</code> Function</a></li>
  <li><a href="#visualization-of-fitted-values-vs-mean" id="toc-visualization-of-fitted-values-vs-mean" class="nav-link" data-scroll-target="#visualization-of-fitted-values-vs-mean"><span class="header-section-number">6.11.2.4</span> Visualization of Fitted Values vs Mean</a></li>
  </ul></li>
  <li><a href="#computing-sums-of-squares-sse-sst-ssr" id="toc-computing-sums-of-squares-sse-sst-ssr" class="nav-link" data-scroll-target="#computing-sums-of-squares-sse-sst-ssr"><span class="header-section-number">6.11.3</span> Computing Sums of Squares (SSE, SST, SSR)</a>
  <ul class="collapse">
  <li><a href="#naive-sum-of-squared-errors" id="toc-naive-sum-of-squared-errors" class="nav-link" data-scroll-target="#naive-sum-of-squared-errors"><span class="header-section-number">6.11.3.1</span> 1. Naive Sum of Squared Errors</a></li>
  <li><a href="#pythagorean-shortcut-vector-lengths" id="toc-pythagorean-shortcut-vector-lengths" class="nav-link" data-scroll-target="#pythagorean-shortcut-vector-lengths"><span class="header-section-number">6.11.3.2</span> 2. Pythagorean Shortcut (vector Lengths)</a></li>
  <li><a href="#matrix-algebra-shortcuts" id="toc-matrix-algebra-shortcuts" class="nav-link" data-scroll-target="#matrix-algebra-shortcuts"><span class="header-section-number">6.11.3.3</span> Matrix Algebra Shortcuts</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Estimation in Multiple Linear Regression</span></h1>
</div>



<div class="quarto-title-meta column-page-right">

    
  
    
  </div>
  


</header>


<section id="linear-models-and-least-square-estimator" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="linear-models-and-least-square-estimator"><span class="header-section-number">6.1</span> Linear Models and Least Square Estimator</h2>
<section id="assumptions-in-linear-models" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="assumptions-in-linear-models"><span class="header-section-number">6.1.1</span> Assumptions in Linear Models</h3>
<p>Suppose that on a random sample of <span class="math inline">\(n\)</span> units (patients, animals, trees, etc.) we observe a response variable <span class="math inline">\(Y\)</span> and explanatory variables <span class="math inline">\(X_{1},...,X_{k}\)</span>. Our data are then <span class="math inline">\((y_{i},x_{i1},...,x_{ik})\)</span>, <span class="math inline">\(i=1,...,n\)</span>, or in vector/matrix form <span class="math inline">\(y, x_{1},...,x_{k}\)</span> where <span class="math inline">\(y=(y_{1},...,y_{n})\)</span> and <span class="math inline">\(x_{j}=(x_{1j},...,x_{nj})^{T}\)</span> or <span class="math inline">\(y, X\)</span> where <span class="math inline">\(X=(x_{1},...,x_{k})\)</span>.</p>
<p>Either by design or by conditioning on their observed values, <span class="math inline">\(x_{1},...,x_{k}\)</span> are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:</p>
<p><strong>Assumptions on Linear Models</strong></p>
<ul>
<li><p><strong>A1. (Additive Error)</strong> <span class="math inline">\(y=\mu+e\)</span> where <span class="math inline">\(e=(e_{1},...,e_{n})^{T}\)</span> is an unobserved random vector with <span class="math inline">\(E(e)=0\)</span>. This implies that <span class="math inline">\(\mu=E(y)\)</span> is the unknown mean of <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>A2. (Linearity)</strong> <span class="math inline">\(\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta\)</span> where <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span> are unknown parameters. This assumption says that <span class="math inline">\(E(y)=\mu\in\text{Col}(X)\)</span> (lies in the column space of <span class="math inline">\(X\)</span>); i.e., it is a linear combination of explanatory vectors <span class="math inline">\(x_{1},...,x_{k}\)</span> with coefficients the unknown parameters in <span class="math inline">\(\beta=(\beta_{1},...,\beta_{k})^{T}\)</span>. Note that it is linear in <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span>, not necessarily in the <span class="math inline">\(x\)</span>’s.</p></li>
<li><p><strong>A3. (Independence)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> are independent random variables (and therefore so are <span class="math inline">\(y_{1},...,y_{n})\)</span>.</p></li>
<li><p><strong>A4. (Homoscedasticity)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> all have the same variance <span class="math inline">\(\sigma^{2}\)</span>; that is, <span class="math inline">\(\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}\)</span> which implies <span class="math inline">\(\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}\)</span>.</p></li>
<li><p><strong>A5. (Normality)</strong> <span class="math inline">\(e\sim N_{n}(0,\sigma^{2}I_{n})\)</span>.</p></li>
</ul>
</section>
<section id="matrix-formulation" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="matrix-formulation"><span class="header-section-number">6.1.2</span> Matrix Formulation</h3>
<p>The model can be written algebraically as: <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n\]</span></p>
<p>Or in matrix notation: <span class="math display">\[
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdot\cdot\cdot &amp; x_{1k}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdot\cdot\cdot &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdot\cdot\cdot &amp; x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
\]</span></p>
<p>This is expressed compactly as: <span class="math display">\[y=X\beta+e\]</span> where <span class="math inline">\(X\)</span> is the design matrix, and <span class="math inline">\(e \sim N_n(0, \sigma^2 I)\)</span>. Alternatively: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e\]</span></p>
<p>Taken together, all five assumptions can be stated more succinctly as: <span class="math display">\[y\sim N_{n}(X\beta,\sigma^{2}I)\]</span> with the mean vector <span class="math inline">\(\mu_{y}=X\beta\in \text{Col}(X)\)</span>.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Note on Coefficients
</div>
</div>
<div class="callout-body-container callout-body">
<p>The effect of a parameter depends upon what other explanatory variables are present in the model. For example, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> in the model: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e\]</span> will typically be different than <span class="math inline">\(\beta_{0}^{*}\)</span> and <span class="math inline">\(\beta_{1}^{*}\)</span> in the model: <span class="math display">\[y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^{*}\]</span> In this context, <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> are the population-projected coefficients of the full model, that is, <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> are the parameters that can best approximate the full model.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will first consider the case that <span class="math inline">\(\text{rank}(X)=k+1\)</span>.</p>
</div>
</div>
</section>
<section id="least-squares-estimator-of-beta-and-fitted-value-hat-y" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="least-squares-estimator-of-beta-and-fitted-value-hat-y"><span class="header-section-number">6.1.3</span> Least Squares Estimator of <span class="math inline">\(\beta\)</span> and Fitted Value <span class="math inline">\(\hat Y\)</span></h3>
<div id="def-least-squares" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 (Least Squares Estimator)</strong></span> The <strong>Least Squares Estimator (LSE)</strong> of <span class="math inline">\(\beta\)</span>, denoted as <span class="math inline">\(\hat{\beta}\)</span>, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses <span class="math inline">\(y\)</span> and the fitted values <span class="math inline">\(X\hat{\beta}\)</span>. <span class="math display">\[
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
\]</span></p>
</div>
<p>We can derive the closed-form solution for <span class="math inline">\(\hat{\beta}\)</span> using the geometry of projections discussed in previous chapters.</p>
<section id="obtaining-hat-y" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="obtaining-hat-y">1. Obtaining <span class="math inline">\(\hat Y\)</span></h4>
<p>In the linear model <span class="math inline">\(y = X\beta + e\)</span>, the systematic component (the mean <span class="math inline">\(E[y]\)</span>) is constrained to lie in the column space of <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(\text{Col}(X)\)</span>. We seek the vector in <span class="math inline">\(\text{Col}(X)\)</span> that is “closest” to the observed data vector <span class="math inline">\(y\)</span>. As established in the theory of projections, this closest vector is the <strong>orthogonal projection</strong> of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>. Let <span class="math inline">\(\hat{y}\)</span> denote this fitted value vector. Using the explicit formula for the projection matrix <span class="math display">\[H = X(X'X)^{-1}X',\]</span> we have: <span class="math display">\[ \hat{y} = Hy = X(X'X)^{-1}X' y.\]</span></p>
</section>
<section id="obtaining-hatbeta-by-solving-xbeta-haty" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="obtaining-hatbeta-by-solving-xbeta-haty">2. Obtaining <span class="math inline">\(\hat{\beta}\)</span> by Solving <span class="math inline">\(x\beta = \hat{y}\)</span></h4>
<p>Since the fitted vector <span class="math inline">\(\hat{y}\)</span> is a projection onto <span class="math inline">\(\text{Col}(X)\)</span>, it must lie entirely within that column space. This guarantees that the linear system for the coefficients <span class="math inline">\(\hat{\beta}\)</span> is consistent (has an exact solution): <span class="math display">\[ X\hat{\beta} = \hat{y} \]</span></p>
<p>To isolate <span class="math inline">\(\hat{\beta}\)</span>, we pre-multiply both sides by the left pseudo-inverse of <span class="math inline">\(X\)</span>, which is <span class="math inline">\((X'X)^{-1}X'\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
(X'X)^{-1}X' (X\hat{\beta}) &amp;= (X'X)^{-1}X' \hat{y} \\
\underbrace{(X'X)^{-1}(X'X)}_{I} \hat{\beta} &amp;= (X'X)^{-1}X' \hat{y}
\end{aligned}
\]</span></p>
<p>This gives us the estimator expressed in terms of the fitted values:</p>
<p><span class="math display">\[
\boxed{\hat{\beta} = (X'X)^{-1}X' \hat{y}}
\]</span></p>
<p>However, we typically calculate the estimator from the observed data <span class="math inline">\(y\)</span>. Recall that because <span class="math inline">\(\hat{y}\)</span> is an orthogonal projection, the difference <span class="math inline">\(y - \hat{y}\)</span> is orthogonal to <span class="math inline">\(X\)</span>. This implies <span class="math inline">\(X'\hat{y} = X'y\)</span>. Substituting this into the equation above yields the standard closed-form solution:</p>
<p><span class="math display">\[
\boxed{\hat{\beta} = (X'X)^{-1}X'y}
\]</span></p>
</section>
</section>
<section id="properties-of-the-estimator-hat-beta" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="properties-of-the-estimator-hat-beta"><span class="header-section-number">6.1.4</span> Properties of the Estimator <span class="math inline">\(\hat \beta\)</span></h3>
<div id="thm-unbiased" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 (Unbiasedness of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span>, then <span class="math inline">\(\hat{\beta}\)</span> is an unbiased estimator for <span class="math inline">\(\beta\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
E(\hat{\beta}) &amp;= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&amp;= \beta
\end{aligned}
\]</span></p>
</div>
<div id="thm-covariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2 (Variance of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the covariance matrix for <span class="math inline">\(\hat{\beta}\)</span> is given by <span class="math inline">\(\sigma^{2}(X^{\prime}X)^{-1}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\beta}) &amp;= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
\]</span></p>
</div>
<p><strong>Note:</strong> These theorems require no assumption of normality.</p>
</section>
</section>
<section id="best-linear-unbiased-estimator-blue" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="best-linear-unbiased-estimator-blue"><span class="header-section-number">6.2</span> Best Linear Unbiased Estimator (BLUE)</h2>
<div id="thm-gauss-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.3 (Gauss-Markov Theorem)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the least-squares estimators <span class="math inline">\(\hat{\beta}_{j}, j=0,1,...,k\)</span> have minimum variance among all linear unbiased estimators.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We consider a linear estimator <span class="math inline">\(Ay\)</span> of <span class="math inline">\(\beta\)</span> and seek the matrix <span class="math inline">\(A\)</span> for which <span class="math inline">\(Ay\)</span> is a minimum variance unbiased estimator.</p>
<p><strong>1. Unbiasedness Condition:</strong> In order for <span class="math inline">\(Ay\)</span> to be an unbiased estimator of <span class="math inline">\(\beta\)</span>, we must have <span class="math inline">\(E(Ay)=\beta\)</span>. Using the assumption <span class="math inline">\(E(y)=X\beta\)</span>, this is expressed as: <span class="math display">\[E(Ay) = A E(y) = AX\beta = \beta\]</span> which implies the condition <span class="math inline">\(AX=I_{k+1}\)</span> since the relationship must hold for any <span class="math inline">\(\beta\)</span>.</p>
<p><strong>2. Minimizing Variance:</strong> The covariance matrix for the estimator <span class="math inline">\(Ay\)</span> is: <span class="math display">\[\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'\]</span> We need to choose <span class="math inline">\(A\)</span> (subject to <span class="math inline">\(AX=I\)</span>) so that the diagonal elements of <span class="math inline">\(AA'\)</span> are minimized.</p>
<p>To relate <span class="math inline">\(Ay\)</span> to <span class="math inline">\(\hat{\beta}=(X'X)^{-1}X'y\)</span>, we define <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span> and write <span class="math inline">\(A = (A - \hat{A}) + \hat{A}\)</span>. Then: <span class="math display">\[AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'\]</span> Expanding this, the cross terms vanish because <span class="math inline">\((A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'\)</span>. Note that <span class="math inline">\(\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}\)</span>. Also, <span class="math inline">\(A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}\)</span> (since <span class="math inline">\(AX=I\)</span>). Thus, <span class="math inline">\((A - \hat{A})\hat{A}' = 0\)</span>.</p>
<p>The expansion simplifies to: <span class="math display">\[AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'\]</span> The matrix <span class="math inline">\((A - \hat{A})(A - \hat{A})'\)</span> is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of <span class="math inline">\(AA'\)</span>, we must set <span class="math inline">\(A - \hat{A} = 0\)</span>, which implies <span class="math inline">\(A = \hat{A}\)</span>.</p>
<p>Thus, the minimum variance estimator is: <span class="math display">\[Ay = (X'X)^{-1}X'y = \hat{\beta}\]</span></p>
</div>
<section id="notes-on-gauss-markov" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="notes-on-gauss-markov"><span class="header-section-number">6.2.1</span> Notes on Gauss-markov</h3>
<ol type="1">
<li><p><strong>Distributional Generality:</strong> The remarkable feature of the Gauss-Markov theorem is that it holds for <em>any</em> distribution of <span class="math inline">\(y\)</span>; normality is not required. The only assumptions used are linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>).</p></li>
<li><p><strong>Extension to All Linear Combinations:</strong> The theorem extends beyond just the parameter vector <span class="math inline">\(\beta\)</span> to any linear combination of the parameters.</p></li>
</ol>
<div id="cor-linear-combo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 6.1 (BLUE for All Linear Combinations)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the best linear unbiased estimator of the scalar <span class="math inline">\(a'\beta\)</span> is <span class="math inline">\(a'\hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> is the least-squares estimator.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\tilde{\beta} = Ay\)</span> be any other linear unbiased estimator of <span class="math inline">\(\beta\)</span>. The variance of the linear combination <span class="math inline">\(a'\tilde{\beta}\)</span> is: <span class="math display">\[
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
\]</span> From the proof of the Gauss-Markov theorem, we established that <span class="math inline">\(AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}\)</span> where <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span>. Substituting this into the variance equation: <span class="math display">\[
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
\]</span> The term <span class="math inline">\(a'(A-\hat{A})(A-\hat{A})'a\)</span> is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore: <span class="math display">\[
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
\]</span> The variance is minimized when <span class="math inline">\(A=\hat{A}\)</span> (specifically when the first term is zero), proving that <span class="math inline">\(a'\hat{\beta}\)</span> has the minimum variance among all linear unbiased estimators.</p>
</div>
<ol start="3" type="1">
<li><strong>Scaling Invariance:</strong> The predictions made by the model are invariant to the scaling of the explanatory variables.</li>
</ol>
<div id="thm-scaling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.4 (Scaling Explanatory Variables)</strong></span> If <span class="math inline">\(x=(1,x_{1},...,x_{k})'\)</span> and <span class="math inline">\(z=(1,c_{1}x_{1},...,c_{k}x_{k})'\)</span>, then the fitted values are identical: <span class="math inline">\(\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(D = \text{diag}(1, c_1, ..., c_k)\)</span> such that the design matrix is transformed to <span class="math inline">\(Z = XD\)</span>. The LSE for the transformed data is: <span class="math display">\[
\begin{aligned}
\hat{\beta}_z &amp;= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&amp;= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&amp;= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
\]</span> . Then, the prediction is: <span class="math display">\[
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
\]</span> .</p>
</div>
<section id="limitations-restriction-to-unbiased-estimators" class="level4" data-number="6.2.1.1">
<h4 data-number="6.2.1.1" class="anchored" data-anchor-id="limitations-restriction-to-unbiased-estimators"><span class="header-section-number">6.2.1.1</span> Limitations: Restriction to Unbiased Estimators</h4>
<p>It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of <strong>linear</strong> and <strong>unbiased</strong> estimators.</p>
<ul>
<li><strong>Assumption Sensitivity:</strong> If the assumptions of linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>) do not hold, <span class="math inline">\(\hat{\beta}\)</span> may be biased or may have a larger variance than other estimators.</li>
<li><strong>Unbiasedness Constraint:</strong> The theorem does not compare <span class="math inline">\(\hat{\beta}\)</span> to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only “best” (minimum variance) among those estimators that satisfy the unbiasedness constraint.</li>
</ul>
</section>
</section>
</section>
<section id="estimator-of-error-variance" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="estimator-of-error-variance"><span class="header-section-number">6.3</span> Estimator of Error Variance</h2>
<p>We estimate <span class="math inline">\(\sigma^{2}\)</span> by the residual mean square:</p>
<div id="def-s2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 (Residual Variance Estimator)</strong></span> <span class="math display">\[s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}\]</span> where <span class="math inline">\(\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})\)</span>.</p>
</div>
<p>Alternatively, SSE can be written as: <span class="math display">\[\text{SSE} = y'y - \hat{\beta}'X'y\]</span> This is often useful for computation (<span class="math inline">\(y'y\)</span> is the total sum of squares of the raw data).</p>
<section id="unbiasedness-of-s2" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="unbiasedness-of-s2"><span class="header-section-number">6.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></h3>
<div id="thm-unbiased-s2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.5 (Unbiasedness of s-squared)</strong></span> If <span class="math inline">\(s^{2}\)</span> is defined as above, and if <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, then <span class="math inline">\(E(s^{2})=\sigma^{2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We use the Hat Matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>, which projects <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>. Thus, <span class="math inline">\(\hat{y} = Hy\)</span>. The residuals are <span class="math inline">\(y - \hat{y} = (I - H)y\)</span>. The Sum of Squared Errors is: <span class="math display">\[\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y\]</span> Since <span class="math inline">\(H\)</span> is symmetric and idempotent, <span class="math inline">\((I-H)\)</span> is also symmetric and idempotent. Thus: <span class="math display">\[\text{SSE} = y'(I-H)y\]</span></p>
<p>To find the expectation, we use the trace trick for quadratic forms: <span class="math inline">\(E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]\)</span>. <span class="math display">\[
\begin{aligned}
E(\text{SSE}) &amp;= E[y'(I-H)y] \\
&amp;= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&amp;= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
\]</span> <strong>Trace Term:</strong> <span class="math inline">\(\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)\)</span>, since <span class="math inline">\(\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1\)</span>.</p>
<p><strong>Non-centrality Term:</strong> Since <span class="math inline">\(HX = X\)</span>, we have <span class="math inline">\((I-H)X = 0\)</span>. Therefore, the second term vanishes: <span class="math inline">\(\beta'X'(I-H)X\beta = 0\)</span>.</p>
<p>Combining these: <span class="math display">\[E(\text{SSE}) = \sigma^2(n - k - 1)\]</span> Dividing by the degrees of freedom <span class="math inline">\((n-k-1)\)</span>, we get <span class="math inline">\(E(s^2) = \sigma^2\)</span>.</p>
</div>
</section>
</section>
<section id="distributions-under-normality" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="distributions-under-normality"><span class="header-section-number">6.4</span> Distributions Under Normality</h2>
<p>If we add Assumption A5 (<span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>), we can derive the exact sampling distributions.</p>
<div id="cor-cov-beta" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 6.2 (Estimated Covariance of Beta)</strong></span> An unbiased estimator of <span class="math inline">\(\text{Cov}(\hat{\beta})\)</span> is given by: <span class="math display">\[\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}\]</span></p>
</div>
<div id="thm-sampling-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.6 (Sampling Distributions)</strong></span> Under assumptions A1-A5:</p>
<ol type="1">
<li><span class="math inline">\(\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})\)</span>.</li>
<li><span class="math inline">\((n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(s^{2}\)</span> are independent.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part (i):</strong> Since <span class="math inline">\(\hat{\beta} = (X'X)^{-1}X'y\)</span> is a linear transformation of the normal vector <span class="math inline">\(y\)</span>, it is also normally distributed. We already established its mean and variance in <a href="#thm-unbiased" class="quarto-xref">Theorem&nbsp;<span>6.1</span></a> and <a href="#thm-covariance" class="quarto-xref">Theorem&nbsp;<span>6.2</span></a>.</p>
<p><strong>Part (ii):</strong> We showed <span class="math inline">\(\text{SSE} = y'(I-H)y\)</span>. Since <span class="math inline">\((I-H)\)</span> is idempotent with rank <span class="math inline">\(n-k-1\)</span>, and <span class="math inline">\((I-H)X\beta = 0\)</span>, by the theory of quadratic forms in normal variables, <span class="math inline">\(\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)\)</span>.</p>
<p><strong>Part (iii):</strong> <span class="math inline">\(\hat{\beta}\)</span> depends on <span class="math inline">\(Hy\)</span> (or <span class="math inline">\(X'y\)</span>), while <span class="math inline">\(s^2\)</span> depends on <span class="math inline">\((I-H)y\)</span>. Since <span class="math inline">\(H(I-H) = H - H^2 = 0\)</span>, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.</p>
</div>
</section>
<section id="maximum-likelihood-estimator-mle" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="maximum-likelihood-estimator-mle"><span class="header-section-number">6.5</span> Maximum Likelihood Estimator (MLE)</h2>
<div id="thm-mle" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.7 (MLE for Linear Regression)</strong></span> If <span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>, the Maximum Likelihood Estimators are: <span class="math display">\[
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
\]</span> <span class="math display">\[
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The log-likelihood function is: <span class="math display">\[ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) \]</span> Maximizing this with respect to <span class="math inline">\(\beta\)</span> is equivalent to minimizing the quadratic term <span class="math inline">\((y - X\beta)'(y - X\beta)\)</span>, which yields the Least Squares Estimator. Differentiating with respect to <span class="math inline">\(\sigma^2\)</span> and setting to zero yields <span class="math inline">\(\hat{\sigma}^2 = \text{SSE}/n\)</span>.</p>
</div>
<p><strong>Note:</strong> The MLE for <span class="math inline">\(\sigma^2\)</span> is biased (denominator <span class="math inline">\(n\)</span>), whereas <span class="math inline">\(s^2\)</span> is unbiased (denominator <span class="math inline">\(n-k-1\)</span>).</p>
</section>
<section id="linear-models-in-centered-form" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="linear-models-in-centered-form"><span class="header-section-number">6.6</span> Linear Models in Centered Form</h2>
<p>The regression model can be written in a centered form by subtracting the means of the explanatory variables: <span class="math display">\[y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}\]</span> for <span class="math inline">\(i=1,...,n\)</span>, where the intercept term is adjusted: <span class="math display">\[\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}\]</span> and <span class="math inline">\(\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}\)</span>.</p>
<section id="matrix-formulation-1" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="matrix-formulation-1"><span class="header-section-number">6.6.1</span> Matrix Formulation</h3>
<p>In matrix form, the equivalence between the original model and the centered model is: <span class="math display">\[y = X\beta + e = (j_n, X_c)\begin{pmatrix} \alpha \\ \beta_{1} \end{pmatrix} + e\]</span> where <span class="math inline">\(\beta_{1}=(\beta_{1},...,\beta_{k})^{T}\)</span> represents the slope coefficients, and <span class="math inline">\(X_c\)</span> is the centered design matrix: <span class="math display">\[X_c = (I - P_{j_n})X_1\]</span> Here, <span class="math inline">\(X_1\)</span> consists of the original columns of <span class="math inline">\(X\)</span> excluding the intercept column.</p>
<p>To see the structure of <span class="math inline">\(X_c\)</span>, we first calculate the projection of the data onto the intercept space, <span class="math inline">\(P_{j_n}X_1\)</span>: <span class="math display">\[
\begin{aligned}
P_{j_n}X_1 &amp;= \frac{1}{n}j_n j_n' X_1 \\
&amp;= \begin{pmatrix} 1/n &amp; 1/n &amp; \cdots &amp; 1/n \\ 1/n &amp; 1/n &amp; \cdots &amp; 1/n \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1/n &amp; 1/n &amp; \cdots &amp; 1/n \end{pmatrix} \begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk} \end{pmatrix} \\
&amp;= \begin{pmatrix} \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \end{pmatrix}
\end{aligned}
\]</span> This results in a matrix where every row is the vector of column means. Subtracting this from <span class="math inline">\(X_1\)</span> gives <span class="math inline">\(X_c\)</span>: <span class="math display">\[
\begin{aligned}
X_c &amp;= X_1 - P_{j_n}X_1 \\
&amp;= \begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk} \end{pmatrix} - \begin{pmatrix} \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \end{pmatrix} \\
&amp;= \begin{pmatrix} x_{11} - \bar{x}_1 &amp; x_{12} - \bar{x}_2 &amp; \cdots &amp; x_{1k} - \bar{x}_k \\ x_{21} - \bar{x}_1 &amp; x_{22} - \bar{x}_2 &amp; \cdots &amp; x_{2k} - \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} - \bar{x}_1 &amp; x_{n2} - \bar{x}_2 &amp; \cdots &amp; x_{nk} - \bar{x}_k \end{pmatrix}
\end{aligned}
\]</span></p>
</section>
<section id="estimation-in-centered-form" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="estimation-in-centered-form"><span class="header-section-number">6.6.2</span> Estimation in Centered Form</h3>
<p>Because the column space of the intercept <span class="math inline">\(j_n\)</span> is orthogonal to the columns of <span class="math inline">\(X_c\)</span> (since columns of <span class="math inline">\(X_c\)</span> sum to zero), the cross-product matrix becomes block diagonal: <span class="math display">\[
\begin{pmatrix} j_n' \\ X_c' \end{pmatrix} (j_n, X_c) = \begin{pmatrix} j_n'j_n &amp; j_n'X_c \\ X_c'j_n &amp; X_c'X_c \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}
\]</span></p>
<div id="thm-centered-estimators" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.8 (Centered Estimators)</strong></span> The least squares estimators for the centered parameters are: <span class="math display">\[
\begin{pmatrix} \hat{\alpha} \\ \hat{\beta}_{1} \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}^{-1} \begin{pmatrix} j_n'y \\ X_c'y \end{pmatrix} = \begin{pmatrix} \bar{y} \\ (X_c'X_c)^{-1}X_c'y \end{pmatrix}
\]</span> Thus:</p>
<ol type="1">
<li><span class="math inline">\(\hat{\alpha} = \bar{y}\)</span> (The sample mean of <span class="math inline">\(y\)</span>).</li>
<li><span class="math inline">\(\hat{\beta}_{1} = S_{xx}^{-1}S_{xy}\)</span>, using the sample covariance notations.</li>
</ol>
</div>
<p>Recovering the original intercept: <span class="math display">\[ \hat{\beta}_0 = \hat{\alpha} - \hat{\beta}_1 \bar{x}_1 - \dots - \hat{\beta}_k \bar{x}_k = \bar{y} - \hat{\beta}_{1}'\bar{x} \]</span></p>
</section>
</section>
<section id="sum-of-squares-decomposition" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sum-of-squares-decomposition"><span class="header-section-number">6.7</span> Sum of Squares Decomposition</h2>
<p>We partition the total variation based on the orthogonal subspaces.</p>
<div id="def-ss-components" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 (Sum of Squares Components)</strong></span> The total variation is decomposed as <span class="math inline">\(\text{SST} = \text{SSR} + \text{SSE}\)</span>.</p>
<ol type="1">
<li><p><strong>Total Sum of Squares (SST):</strong> The squared length of the centered response vector. <span class="math display">\[\text{SST} = \|y - \bar{y}j_n\|^2 = \|(I - P_{j_n})y\|^2\]</span></p></li>
<li><p><strong>Regression Sum of Squares (SSR):</strong> The variation explained by the regressors <span class="math inline">\(X_c\)</span>. <span class="math display">\[\text{SSR} = \|\hat{y} - \bar{y}j_n\|^2 = \|P_{X_c}y\|^2 = \hat{\beta}_1' X_c' X_c \hat{\beta}_1\]</span></p></li>
<li><p><strong>Sum of Squared Errors (SSE):</strong> The residual variation. <span class="math display">\[\text{SSE} = \|y - \hat{y}\|^2 = \|(I - H)y\|^2\]</span></p></li>
</ol>
</div>
<section id="d-visualization-of-decomposition-of-y" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="d-visualization-of-decomposition-of-y"><span class="header-section-number">6.7.1</span> 3D Visualization of Decomposition of <span class="math inline">\(y\)</span></h3>
<p>We partition the total variation in <span class="math inline">\(y\)</span> based on the orthogonal subspaces.</p>
<ol type="1">
<li><strong>Space of the Mean:</strong> <span class="math inline">\(L(j_n)\)</span>, spanned by the intercept vector <span class="math inline">\(j_n\)</span>.</li>
<li><strong>Space of the Regressors:</strong> <span class="math inline">\(L(X_c)\)</span>, spanned by the centered predictors <span class="math inline">\(X_c\)</span>.</li>
<li><strong>Error Space:</strong> <span class="math inline">\(\text{Col}(X)^\perp\)</span>, orthogonal to the model space.</li>
</ol>
<p>The vector <span class="math inline">\(y\)</span> can be decomposed into three orthogonal components: <span class="math display">\[y = \bar{y}j_n + P_{X_c}y + (y - \hat{y})\]</span> Visually, this corresponds to projecting the vector <span class="math inline">\(y\)</span> onto three orthogonal axes.</p>
<p><strong>Interactive Visualization:</strong></p>
<p>We generate a cloud of 100 observations of <span class="math inline">\(y\)</span> from <span class="math inline">\(N(\mu, \sigma=1)\)</span> where <span class="math inline">\(\mu = (5,5,0)\)</span>. The projections onto the Model Plane (<span class="math inline">\(z=0\)</span>) are highlighted in <strong>red</strong>, and the projections onto the error axis (<span class="math inline">\(z\)</span>) are in <strong>yellow</strong>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Effect Exists (signal)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">No Effect (noise)</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-d96970b9501c191ad406" style="width:100%;height:576px;"></div>
<script type="application/json" data-for="htmlwidget-d96970b9501c191ad406">{"x":{"visdat":{"54a2e11acd8":["function () ","plotlyVisDat"],"54aa6ab089":["function () ","data"],"54a626bf7b6":["function () ","data"],"54a1e540035":["function () ","data"],"54a1cd9a721":["function () ","data"],"54a156b7653":["function () ","data"]},"cur_data":"54a156b7653","attrs":{"54a2e11acd8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"colorscale":[[0,1],["steelblue","steelblue"]],"showscale":false,"name":"Model Space","inherit":true},"54aa6ab089":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"red","symbol":"diamond","opacity":0.80000000000000004},"name":"Proj on Floor","inherit":true},"54a626bf7b6":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":{},"marker":{"size":4,"color":"black","symbol":"circle","opacity":0.59999999999999998},"name":"Data Cloud","inherit":true},"54a1e540035":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"blue","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(jn)","inherit":true},"54a1cd9a721":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"green","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(Xc)","inherit":true},"54a156b7653":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":{},"marker":{"size":4,"color":"gold","symbol":"circle-open","opacity":0.80000000000000004},"name":"Error","inherit":true},"54a156b7653.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","inherit":true},"54a156b7653.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","inherit":true},"54a156b7653.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","inherit":true}},"layout":{"margin":{"b":0,"l":0,"t":30,"r":0},"title":"Scenario A: Effect Exists","scene":{"xaxis":{"title":"L(j<sub>n<\/sub>)","range":[0,8]},"yaxis":{"title":"L(X<sub>c<\/sub>)","range":[-4,8]},"zaxis":{"title":"Col(X)<sup>&perp;<\/sup>","range":[-4,4]},"aspectmode":"cube","camera":{"eye":{"x":1.6000000000000001,"y":1.6000000000000001,"z":0.59999999999999998}}},"showlegend":false,"hovermode":"closest"},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z<br />z","ticklen":2},"colorscale":[[0,"steelblue"],[1,"steelblue"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"name":"Model Space","frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"red","size":4,"symbol":"diamond","opacity":0.80000000000000004,"line":{"color":"rgba(255,127,14,1)"}},"name":"Proj on Floor","error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"black","size":4,"symbol":"circle","opacity":0.59999999999999998,"line":{"color":"rgba(44,160,44,1)"}},"name":"Data Cloud","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"blue","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(214,39,40,1)"}},"name":"Proj L(jn)","error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"green","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(148,103,189,1)"}},"name":"Proj L(Xc)","error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"gold","size":4,"symbol":"circle-open","opacity":0.80000000000000004,"line":{"color":"rgba(140,86,75,1)"}},"name":"Error","error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Scenario 1: Significant regression effect (<span class="math inline">\(\beta_1
ot= 0\)</span>). The mean vector projects significantly onto the predictor space.</p>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-bbf49928d385f1dd9e5c" style="width:100%;height:576px;"></div>
<script type="application/json" data-for="htmlwidget-bbf49928d385f1dd9e5c">{"x":{"visdat":{"54a211b65a9":["function () ","plotlyVisDat"],"54adaf4b3a":["function () ","data"],"54a6f73d3da":["function () ","data"],"54a1d59c660":["function () ","data"],"54a70eed7ad":["function () ","data"],"54a5091d8c7":["function () ","data"]},"cur_data":"54a5091d8c7","attrs":{"54a211b65a9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"colorscale":[[0,1],["steelblue","steelblue"]],"showscale":false,"name":"Model Space","inherit":true},"54adaf4b3a":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"red","symbol":"diamond","opacity":0.80000000000000004},"name":"Proj on Floor","inherit":true},"54a6f73d3da":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":{},"marker":{"size":4,"color":"black","symbol":"circle","opacity":0.59999999999999998},"name":"Data Cloud","inherit":true},"54a1d59c660":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"blue","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(jn)","inherit":true},"54a70eed7ad":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"green","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(Xc)","inherit":true},"54a5091d8c7":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":{},"marker":{"size":4,"color":"gold","symbol":"circle-open","opacity":0.80000000000000004},"name":"Error","inherit":true},"54a5091d8c7.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","inherit":true},"54a5091d8c7.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[0,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","inherit":true},"54a5091d8c7.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[0,0],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","inherit":true}},"layout":{"margin":{"b":0,"l":0,"t":30,"r":0},"title":"Scenario B: No Effect","scene":{"xaxis":{"title":"L(j<sub>n<\/sub>)","range":[0,8]},"yaxis":{"title":"L(X<sub>c<\/sub>)","range":[-4,8]},"zaxis":{"title":"Col(X)<sup>&perp;<\/sup>","range":[-4,4]},"aspectmode":"cube","camera":{"eye":{"x":1.6000000000000001,"y":1.6000000000000001,"z":0.59999999999999998}}},"showlegend":false,"hovermode":"closest"},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z<br />z","ticklen":2},"colorscale":[[0,"steelblue"],[1,"steelblue"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"name":"Model Space","frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"red","size":4,"symbol":"diamond","opacity":0.80000000000000004,"line":{"color":"rgba(255,127,14,1)"}},"name":"Proj on Floor","error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"black","size":4,"symbol":"circle","opacity":0.59999999999999998,"line":{"color":"rgba(44,160,44,1)"}},"name":"Data Cloud","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"blue","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(214,39,40,1)"}},"name":"Proj L(jn)","error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"green","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(148,103,189,1)"}},"name":"Proj L(Xc)","error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"gold","size":4,"symbol":"circle-open","opacity":0.80000000000000004,"line":{"color":"rgba(140,86,75,1)"}},"name":"Error","error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[0,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[0,0],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Scenario 2: No regression effect (<span class="math inline">\(\beta_1 = 0\)</span>). The mean vector lies purely on the intercept axis.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-diagram-to-show-decomposition-of-sum-of-squares" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="a-diagram-to-show-decomposition-of-sum-of-squares"><span class="header-section-number">6.7.2</span> A Diagram to Show Decomposition of Sum of Squares</h3>
<p>The decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ss-decomposition-legend-v2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ss-decomposition-legend-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec5-est_files/figure-html/fig-ss-decomposition-legend-v2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ss-decomposition-legend-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Geometric Decomposition: SST = SSR + SSE
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="distribution-of-sum-of-squares" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="distribution-of-sum-of-squares"><span class="header-section-number">6.7.3</span> Distribution of Sum of Squares</h3>
<p>We apply the general theory of projections to the specific components defined in <a href="#def-ss-components" class="quarto-xref">Definition&nbsp;<span>6.3</span></a>.</p>
<div id="thm-distribution-ss-v2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.9 (Distribution of Sum of Squares)</strong></span> Let <span class="math inline">\(y \sim N(\mu, \sigma^2 I_n)\)</span>, where <span class="math inline">\(\mu \in \text{Col}(X)\)</span>. Consider the decomposition defined by the projection matrices <span class="math inline">\(P_{X_c}\)</span> and <span class="math inline">\(M = I - H\)</span>.</p>
<ul>
<li><p><strong>Independence:</strong> The quadratic forms <span class="math inline">\(\text{SSR}\)</span> and <span class="math inline">\(\text{SSE}\)</span> are statistically independent because the subspaces <span class="math inline">\(L(X_c)\)</span> and <span class="math inline">\(\text{Col}(X)^\perp\)</span> are orthogonal.</p></li>
<li><p><strong>Distribution of SSE:</strong> The scaled sum of squared errors follows a central Chi-squared distribution: <span class="math display">\[ \frac{\text{SSE}}{\sigma^2} = \frac{\|(I - H)y\|^2}{\sigma^2} \sim \chi^2(n-k-1) \]</span> <strong>Mean:</strong> <span class="math display">\[ E[\text{SSE}] = \sigma^2(n-k-1) \]</span></p></li>
<li><p><strong>Distribution of SSR:</strong> The scaled regression sum of squares follows a <strong>non-central</strong> Chi-squared distribution: <span class="math display">\[ \frac{\text{SSR}}{\sigma^2} = \frac{\|P_{X_c}y\|^2}{\sigma^2} \sim \chi^2(k, \lambda) \]</span> <strong>Mean:</strong> <span class="math display">\[ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 \]</span></p></li>
</ul>
<p><strong>Non-centrality Parameter (<span class="math inline">\(\lambda\)</span>):</strong> <span class="math display">\[ \lambda = \frac{1}{2\sigma^2} \|P_{X_c} \mu\|^2 \]</span> where <span class="math display">\[\|P_{X_c} \mu\|^2 = \|X_c \beta_1\|^2 = (X_c \beta_1)' (X_c \beta_1) = \beta_1' X_c' X_c \beta_1\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We apply <a href="lec4-qf.html#thm-proj-dist" class="quarto-xref">Theorem&nbsp;<span>5.8</span></a> to the specific projection matrices identified in the definitions.</p>
<ul>
<li><p><strong>For SSE (Error Space):</strong> <span class="math inline">\(\text{SSE}\)</span> is defined by the projection matrix <span class="math inline">\(P_V = I - H\)</span>.</p>
<ul>
<li><strong>Dimension:</strong> The rank of <span class="math inline">\((I - H)\)</span> is <span class="math inline">\(n - \text{rank}(X) = n - (k+1) = n - k - 1\)</span>.</li>
<li><strong>Non-centrality:</strong> Since <span class="math inline">\(\mu \in \text{Col}(X)\)</span>, the projection onto the orthogonal complement is zero: <span class="math inline">\(\|(I - H)\mu\|^2 = 0\)</span>. Thus, <span class="math inline">\(\lambda = 0\)</span>.</li>
<li><strong>Expectation:</strong> Using Part 2 of <a href="lec4-qf.html#thm-proj-dist" class="quarto-xref">Theorem&nbsp;<span>5.8</span></a> (<span class="math inline">\(E(\|P_V y\|^2) = \sigma^2 \text{rank}(P_V) + \|P_V \mu\|^2\)</span>): <span class="math display">\[ E[\text{SSE}] = \sigma^2(n-k-1) + 0 = \sigma^2(n-k-1) \]</span></li>
</ul></li>
<li><p><strong>For SSR (Regression Space):</strong> <span class="math inline">\(\text{SSR}\)</span> is defined by the projection matrix <span class="math inline">\(P_V = P_{X_c}\)</span>.</p>
<ul>
<li><p><strong>Dimension:</strong> The rank of <span class="math inline">\(P_{X_c}\)</span> is <span class="math inline">\((k+1) - 1 = k\)</span>.</p></li>
<li><p><strong>Non-centrality:</strong> The projection of <span class="math inline">\(\mu\)</span> onto <span class="math inline">\(L(X_c)\)</span> is <span class="math inline">\(P_{X_c}\mu\)</span>. <span class="math display">\[ \lambda = \frac{1}{2\sigma^2} \|P_{X_c} \mu\|^2 \]</span></p></li>
<li><p><strong>Expectation:</strong> Using Part 2 of <a href="lec4-qf.html#thm-proj-dist" class="quarto-xref">Theorem&nbsp;<span>5.8</span></a>: <span class="math display">\[ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 \]</span></p></li>
</ul>
<p>This shows that while <span class="math inline">\(E[\text{SSE}]\)</span> depends only on the noise variance and sample size, <span class="math inline">\(E[\text{SSR}]\)</span> is inflated by the magnitude of the true regression signal <span class="math inline">\(\|P_{X_c}\mu\|^2\)</span>.</p></li>
</ul>
</div>
</section>
</section>
<section id="f-test-for-testing-overall-regression-effect" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="f-test-for-testing-overall-regression-effect"><span class="header-section-number">6.8</span> F-test for Testing Overall Regression Effect</h2>
<p>We wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.</p>
<p><strong>Hypotheses:</strong></p>
<ul>
<li><p><strong>Null Hypothesis (<span class="math inline">\(H_0\)</span>):</strong> <span class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_k = 0\)</span> (No regression effect). This implies <span class="math inline">\(\mu \in \text{span}(j_n)\)</span> and the true signal variance <span class="math inline">\(\|X_c\beta_1\|^2 = 0\)</span>.</p></li>
<li><p><strong>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>):</strong> At least one <span class="math inline">\(\beta_j \neq 0\)</span>.</p></li>
</ul>
<section id="the-f-statistic" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-f-statistic">The F-statistic</h3>
<p>We construct the test statistic using the ratio of the Mean Squares defined previously:</p>
<p><span class="math display">\[F = \frac{\text{MSR}}{\text{MSE}} = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}\]</span></p>
</section>
<section id="understanding-f-via-expectations" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="understanding-f-via-expectations">Understanding <span class="math inline">\(F\)</span> via Expectations</h3>
<p>The logic of the F-test is transparent when we examine the expected values of the numerator and denominator:</p>
<p><span class="math display">\[
\begin{aligned}
E[\text{MSE}] &amp;= \sigma^2 \\
E[\text{MSR}] &amp;= \sigma^2 + \frac{\|X_c \beta_1\|^2}{k}
\end{aligned}
\]</span></p>
<ul>
<li><strong>If <span class="math inline">\(H_0\)</span> is true:</strong> The signal term is zero. Both Mean Squares estimate <span class="math inline">\(\sigma^2\)</span> unbiasedly. We expect <span class="math inline">\(F \approx 1\)</span>.</li>
<li><strong>If <span class="math inline">\(H_1\)</span> is true:</strong> The numerator includes the positive term <span class="math inline">\(\frac{\|X_c \beta_1\|^2}{k}\)</span>. We expect <span class="math inline">\(F &gt; 1\)</span>.</li>
</ul>
<p>Therefore, we reject <span class="math inline">\(H_0\)</span> for sufficiently large values of <span class="math inline">\(F\)</span>. Specifically, we reject at level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(F_{obs} &gt; F_{\alpha}(k, n-k-1)\)</span>.</p>
</section>
<section id="distributional-theory" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="distributional-theory"><span class="header-section-number">6.8.1</span> Distributional Theory</h3>
<p>To derive the exact sampling distribution, we rely on the independence of the sums of squares (from <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;<span>6.9</span></a>) and the definition of the non-central F-distribution given in <strong><a href="lec4-qf.html#def-noncentral-f" class="quarto-xref">Definition&nbsp;<span>5.3</span></a></strong>.</p>
<div id="thm-regression-f-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.10 (Distribution of Regression F-Statistic)</strong></span> Under the assumption of normality, the regression F-statistic follows a <strong>non-central F-distribution</strong>:</p>
<p><span class="math display">\[ F \sim F(k, n-k-1, \lambda) \]</span></p>
<p>The non-centrality parameter <span class="math inline">\(\lambda\)</span> is determined by the ratio of the signal sum of squares to the error variance: <span class="math display">\[ \lambda = \frac{\|X_c \beta_1\|^2}{2\sigma^2} \]</span></p>
<p><strong>Special Cases:</strong></p>
<ol type="1">
<li><strong>Under <span class="math inline">\(H_1\)</span> (Signal exists):</strong> <span class="math inline">\(\lambda &gt; 0\)</span>, so <span class="math inline">\(F\)</span> follows the non-central distribution.</li>
<li><strong>Under <span class="math inline">\(H_0\)</span> (No signal):</strong> <span class="math inline">\(\beta_1 = 0 \implies \lambda = 0\)</span>. The distribution collapses to the <strong>central F-distribution</strong>: <span class="math display">\[ F \sim F(k, n-k-1) \]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We identify the components from <a href="lec4-qf.html#def-noncentral-f" class="quarto-xref">Definition&nbsp;<span>5.3</span></a>:</p>
<ol type="1">
<li><strong>Numerator (<span class="math inline">\(X_1\)</span>):</strong> Let <span class="math inline">\(X_1 = \text{SSR}/\sigma^2\)</span>. From <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;<span>6.9</span></a>, <span class="math inline">\(X_1 \sim \chi^2(k, 2\lambda)\)</span>.</li>
<li><strong>Denominator (<span class="math inline">\(X_2\)</span>):</strong> Let <span class="math inline">\(X_2 = \text{SSE}/\sigma^2\)</span>. From <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;<span>6.9</span></a>, <span class="math inline">\(X_2 \sim \chi^2(n-k-1)\)</span>.</li>
<li><strong>Independence:</strong> <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</li>
</ol>
<p>Substituting these into the F-statistic: <span class="math display">\[
F = \frac{\text{MSR}}{\text{MSE}} = \frac{(\text{SSR}/\sigma^2)/k}{(\text{SSE}/\sigma^2)/(n-k-1)} = \frac{X_1/k}{X_2/(n-k-1)}
\]</span> By definition <a href="lec4-qf.html#def-noncentral-f" class="quarto-xref">Definition&nbsp;<span>5.3</span></a>, this ratio follows <span class="math inline">\(F(k, n-k-1, \lambda)\)</span>.</p>
</div>
</section>
<section id="visualization-of-the-rejection-region" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="visualization-of-the-rejection-region"><span class="header-section-number">6.8.2</span> Visualization of the Rejection Region</h3>
<p>The following plot illustrates the central F-distribution (valid under <span class="math inline">\(H_0\)</span>) for <span class="math inline">\(k=3\)</span> predictors and <span class="math inline">\(n=20\)</span> observations (<span class="math inline">\(df_1 = 3, df_2 = 16\)</span>). An observed statistic of <span class="math inline">\(F=2\)</span> is marked, with the p-value represented by the shaded tail area.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-f-dist-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-f-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="lec5-est_files/figure-html/fig-f-dist-example-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-f-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="coefficient-of-determination-r2" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="coefficient-of-determination-r2"><span class="header-section-number">6.9</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h2>
<section id="definition" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">6.9.1</span> Definition</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic measures the proportion of total variation explained by the regression model.</p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4 (R-Squared)</strong></span> <span class="math display">\[R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}\]</span> Since <span class="math inline">\(0 \le \text{SSE} \le \text{SST}\)</span>, it follows that <span class="math inline">\(0 \le R^2 \le 1\)</span>.</p>
</div>
</section>
<section id="expectation-and-bias" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="expectation-and-bias"><span class="header-section-number">6.9.2</span> Expectation and Bias</h3>
<p>To understand the bias in <span class="math inline">\(R^2\)</span>, it is more illuminating to analyze the expectation of the <strong>unexplained variance</strong> (<span class="math inline">\(1 - R^2\)</span>). This term represents the ratio of error sum of squares to the total sum of squares:</p>
<p><span class="math display">\[ E[1 - R^2] = E\left[ \frac{\text{SSE}}{\text{SST}} \right] \]</span></p>
<p>Using the first-order approximation <span class="math inline">\(E[X/Y] \approx E[X]/E[Y]\)</span>, we examine the numerator and denominator separately:</p>
<p><span class="math display">\[
\begin{aligned}
E[\text{SSE}] &amp;= \sigma^2(n-k-1) \\
E[\text{SST}] &amp;= \sigma^2(n-1) + 2\sigma^2\lambda = \sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)
\end{aligned}
\]</span></p>
<p>Substituting these back, we approximate the expected unexplained fraction:</p>
<p><span class="math display">\[ E[1 - R^2] \approx \frac{\sigma^2(n-k-1)}{\sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)} = \frac{n-k-1}{(n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2}} \]</span></p>
<p><strong>Behavior under Null Hypothesis (<span class="math inline">\(H_0\)</span>):</strong> When there is no true signal (<span class="math inline">\(\beta_1 = 0\)</span>), the term <span class="math inline">\(\frac{\|X_c \beta_1\|^2}{\sigma^2}\)</span> vanishes. The expected proportion of unexplained variance becomes:</p>
<p><span class="math display">\[ E[1 - R^2 | H_0] \approx \frac{n-k-1}{n-1} \]</span></p>
<p>This result reveals the source of the bias: 1. Ideally, if predictors are noise, the model should explain nothing, and <span class="math inline">\(E[1-R^2]\)</span> should be <span class="math inline">\(1\)</span>. 2. Instead, the expected error ratio is <strong>less than 1</strong>, specifically scaled by <span class="math inline">\(\frac{n-k-1}{n-1}\)</span>. 3. This scaling factor is exactly what the <strong>Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</strong> attempts to correct by multiplying the observed ratio by the inverse <span class="math inline">\(\frac{n-1}{n-k-1}\)</span>.</p>
</section>
<section id="exact-distribution" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="exact-distribution"><span class="header-section-number">6.9.3</span> Exact Distribution</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.</p>
<div id="thm-r2-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.11 (Distribution of R-Squared)</strong></span> <span class="math display">\[ R^2 \sim \text{Beta}_1\left( \frac{k}{2}, \frac{n-k-1}{2}, \lambda \right) \]</span> where <span class="math inline">\(\text{df}_1 = k\)</span> and <span class="math inline">\(\text{df}_2 = n-k-1\)</span>.</p>
</div>
</section>
<section id="adjusted-r-squared-r2_a" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="adjusted-r-squared-r2_a"><span class="header-section-number">6.9.4</span> Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</h3>
<p>To correct for the inflation of <span class="math inline">\(R^2\)</span> due to model complexity (<span class="math inline">\(k\)</span>), we introduce the Adjusted <span class="math inline">\(R^2\)</span>.</p>
<p><span class="math display">\[ R^2_a = 1 - \frac{\text{MSE}}{\text{MST}} = 1 - (1 - R^2) \frac{n-1}{n-k-1} \]</span></p>
<p><strong>Expectation:</strong> Under <span class="math inline">\(H_0\)</span>, since <span class="math inline">\(E[\text{MSE}] = E[\text{MST}] = \sigma^2\)</span>, the estimator is asymptotically unbiased: <span class="math display">\[ E[R^2_a | H_0] \approx 0 \]</span></p>
<p><strong>Variance and Stability:</strong> While <span class="math inline">\(R^2_a\)</span> corrects the bias, it introduces instability. The variance of <span class="math inline">\(R^2_a\)</span> under <span class="math inline">\(H_0\)</span> can be derived from the variance of the Beta distribution:</p>
<p><span class="math display">\[ \text{Var}(R^2_a | H_0) = \left( \frac{n-1}{n-k-1} \right)^2 \text{Var}(R^2 | H_0) \]</span></p>
<p>Substituting <span class="math inline">\(\text{Var}(R^2 | H_0) = \frac{2k(n-k-1)}{(n-1)^2(n+1)}\)</span>, we obtain:</p>
<p><span class="math display">\[ \text{Var}(R^2_a | H_0) = \frac{2k}{(n-k-1)(n+1)} \]</span></p>
<p><strong>Key Insight:</strong> As the model complexity <span class="math inline">\(k\)</span> increases relative to <span class="math inline">\(n\)</span>:</p>
<ol type="1">
<li>The denominator <span class="math inline">\((n-k-1)\)</span> shrinks.</li>
<li>The variance <span class="math inline">\(\text{Var}(R^2_a)\)</span> explodes.</li>
</ol>
<p>This implies that for high-dimensional models (large <span class="math inline">\(k/n\)</span>), <span class="math inline">\(R^2_a\)</span> becomes an extremely noisy estimator, often yielding large negative values even for null models.</p>
</section>
<section id="relationship-with-rao-blackwell-decomposition-of-variances" class="level3" data-number="6.9.5">
<h3 data-number="6.9.5" class="anchored" data-anchor-id="relationship-with-rao-blackwell-decomposition-of-variances"><span class="header-section-number">6.9.5</span> Relationship with Rao-blackwell Decomposition of Variances</h3>
<p>The formula for the expected Adjusted <span class="math inline">\(R^2\)</span> reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation <span class="math inline">\(Y_i\)</span> into the expected conditional variance (noise) and the variance of the conditional expectation (signal): <span class="math display">\[ \text{Var}(Y_i) = E[\text{Var}(Y_i|x_{(i)})] + \text{Var}(E[Y_i|x_{(i)}]) \]</span> <span class="math display">\[ \sigma^2_Y = \sigma^2 + \text{Var}_{\text{signal}} \]</span></p>
<p>In our derived expectation for <span class="math inline">\(R^2_a\)</span>: <span class="math display">\[ E[R^2_a] \approx \frac{\frac{\|X_c\beta_1\|^2}{n-1}}{\sigma^2 + \frac{\|X_c\beta_1\|^2}{n-1}} \]</span></p>
<p>The term in the numerator, <span class="math inline">\(\frac{\|X_c\beta_1\|^2}{n-1}\)</span>, is precisely the <strong>sample variance of the true means</strong> <span class="math inline">\(\mu_i\)</span>. Let <span class="math inline">\(\mu = X\beta\)</span>. We can expand the centered signal vector <span class="math inline">\(X_c\beta_1\)</span> to see this explicitly. Since <span class="math inline">\(\mu \in \text{Col}(X)\)</span>, we know <span class="math inline">\(H\mu = \mu\)</span>:</p>
<p><span class="math display">\[
X_c\beta_1 = P_{X_c} \mu = (H - P_{j_n})\mu = H\mu - P_{j_n}\mu = \mu - \bar{\mu}j_n =
\begin{pmatrix}
\mu_1 - \bar{\mu} \\
\mu_2 - \bar{\mu} \\
\vdots \\
\mu_n - \bar{\mu}
\end{pmatrix}
\]</span></p>
<p>This vector represents the deviation of each observation’s true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is: <span class="math display">\[ \frac{\|X_c\beta_1\|^2}{n-1} = \frac{\sum_{i=1}^n (\mu_i - \bar{\mu})^2}{n-1} = \widehat{\text{Var}}(\mu) \]</span></p>
<p>If we view the rows of the design matrix <span class="math inline">\(X\)</span> as random draws <span class="math inline">\(x_{(1)}, \dots, x_{(n)}\)</span> from a population of covariate vectors, this term estimates <span class="math inline">\(\text{Var}(x_{(i)}'\beta)\)</span>, which is the variance of the signal component <span class="math inline">\(\text{Var}(E[Y_i|x_{(i)}])\)</span>.</p>
<p>Thus, <span class="math inline">\(R^2_a\)</span> can be interpreted as a method-of-moments estimator for the <strong>proportion of variance explained by the signal</strong> in the population: <span class="math display">\[ E[R^2_a] \approx \frac{\text{Var}_{\text{signal}}}{\text{Var}_{\text{noise}} + \text{Var}_{\text{signal}}} = \frac{\text{Var}(E[Y_i|x_{(i)}])}{\text{Var}(Y_i)} \]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MSR Is Not a Variance Estimator
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Observing that <span class="math inline">\(E[\text{MST}] \approx \sigma^2 + \text{Var}_{\text{signal}}\)</span> and <span class="math inline">\(E[\text{MSE}] = \sigma^2\)</span>, we can see that the difference <span class="math inline">\(\text{MST} - \text{MSE}\)</span> provides a direct method-of-moments estimator for the variance of the signal itself.</p></li>
<li><p>It is important to recognize that the commonly used <strong>Mean Square Regression (MSR)</strong>, defined as <span class="math inline">\(\text{SSR}/k\)</span>, is <strong>not</strong> an estimator of the signal variance. Because <span class="math inline">\(E[\text{MSR}] = \sigma^2 + \frac{\|X_c\beta_1\|^2}{k}\)</span>, it scales with the sample size <span class="math inline">\(n\)</span> (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting <em>existence</em> of signal), not for estimating the <em>magnitude</em> of the signal variance.</p></li>
</ul>
</div>
</div>
</section>
<section id="relationship-with-f-test" class="level3" data-number="6.9.6">
<h3 data-number="6.9.6" class="anchored" data-anchor-id="relationship-with-f-test"><span class="header-section-number">6.9.6</span> Relationship with <span class="math inline">\(F\)</span> Test</h3>
<p>The proportion of <em>unexplained</em> variance, <span class="math inline">\(1 - R^2\)</span>, follows the Type II Non-central Beta distribution.</p>
<div id="thm-r2-complement" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.12 (Distribution of Unexplained Variance)</strong></span> <span class="math display">\[ 1 - R^2 = \frac{\text{SSE}}{\text{SST}} \sim \text{Beta}_2\left( \frac{n-k-1}{2}, \frac{k}{2}, \lambda \right) \]</span></p>
</div>
<p>Both the standard and adjusted coefficients of determination are monotonic functions of the <span class="math inline">\(F\)</span>-statistic (<span class="math inline">\(F = \text{MSR}/\text{MSE}\)</span>):</p>
<ol type="1">
<li><p><strong>Standard <span class="math inline">\(R^2\)</span>:</strong> <span class="math display">\[ R^2 = \frac{\frac{k}{n-k-1} F}{1 + \frac{k}{n-k-1} F} \]</span></p></li>
<li><p><strong>Adjusted <span class="math inline">\(R^2_a\)</span>:</strong> <span class="math display">\[ R^2_a = 1 - \left( 1 - R^2 \right) \frac{n-1}{n-k-1} = 1 - \frac{n-1}{(n-k-1) + kF} \]</span></p></li>
</ol>
</section>
<section id="an-animation-for-illustrating-r2_a-under-h_0-and-h_1" class="level3" data-number="6.9.7">
<h3 data-number="6.9.7" class="anchored" data-anchor-id="an-animation-for-illustrating-r2_a-under-h_0-and-h_1"><span class="header-section-number">6.9.7</span> An Animation for Illustrating <span class="math inline">\(r^2_a\)</span> Under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></h3>
<p>We simulate a dataset with <span class="math inline">\(n=30\)</span> observations and consider a sequence of nested models adding groups of predictors.</p>
<p><strong>Predictor Groups:</strong></p>
<ol type="1">
<li><strong>Step 1 (<span class="math inline">\(k=1\)</span>):</strong> Add <span class="math inline">\(x_1\)</span>. (Signal under <span class="math inline">\(H_1\)</span>).</li>
<li><strong>Step 2 (<span class="math inline">\(k=6\)</span>):</strong> Add <span class="math inline">\(x_2, \dots, x_6\)</span> (Noise).</li>
<li><strong>Step 3 (<span class="math inline">\(k=11\)</span>):</strong> Add <span class="math inline">\(x_7, \dots, x_{11}\)</span> (Noise).</li>
<li><strong>Step 4 (<span class="math inline">\(k=20\)</span>):</strong> Add <span class="math inline">\(x_{12}, \dots, x_{20}\)</span> (Noise).</li>
</ol>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Null Hypothesis (<span class="math inline">\(H_0\)</span>)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Alternative Hypothesis (<span class="math inline">\(H_1\)</span>)</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Under <span class="math inline">\(H_0\)</span>, the true coefficient for <span class="math inline">\(x_1\)</span> is <span class="math inline">\(\beta_1 = 0\)</span>. All predictors are noise.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<video controls="controls" width="100%">
<source src="figs/rss-h0-v6.mp4" type="video/mp4">
</video>
<p>Simulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable.</p>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>Under <span class="math inline">\(H_1\)</span>, <span class="math inline">\(x_1\)</span> is a true predictor (<span class="math inline">\(\beta_1 = 2\)</span>). The subsequent groups (<span class="math inline">\(x_2 \dots x_{20}\)</span>) remain noise.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<video controls="controls" width="100%">
<source src="figs/rss-h1-v6.mp4" type="video/mp4">
</video>
<p>Simulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors.</p>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="underfitting-and-overfitting" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="underfitting-and-overfitting"><span class="header-section-number">6.10</span> Underfitting and Overfitting</h2>
<p>We consider the effects of omitting explanatory variables that should be included (underfitting) and including variables that should be excluded (overfitting).</p>
<p>Suppose the true model is: <span class="math display">\[y = X\beta + e = \begin{pmatrix} X_1 &amp; X_2 \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} + e = X_1\beta_1 + X_2\beta_2 + e \quad (\dagger)\]</span> where <span class="math inline">\(\text{Var}(e) = \sigma^2 I\)</span>.</p>
<ul>
<li><strong>Underfitting:</strong> Leaving out <span class="math inline">\(X_2\beta_2\)</span> when <span class="math inline">\(\beta_2 \neq 0\)</span>.</li>
<li><strong>Overfitting:</strong> Including <span class="math inline">\(X_2\beta_2\)</span> when <span class="math inline">\(\beta_2 = 0\)</span>.</li>
</ul>
<section id="underfitting" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="underfitting"><span class="header-section-number">6.10.1</span> Underfitting</h3>
<p>Suppose model <span class="math inline">\((\dagger)\)</span> holds, but we fit the reduced model: <span class="math display">\[y = X_1\beta_1^* + e^*\]</span> The OLS estimator for this reduced model is <span class="math inline">\(\hat{\beta}_1^* = (X_1^T X_1)^{-1}X_1^T y\)</span>.</p>
<div id="thm-underfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.13 (Bias and Variance under Underfitting)</strong></span> If we fit the reduced model when the full model <span class="math inline">\((\dagger)\)</span> is true:</p>
<ol type="1">
<li><strong>Bias:</strong> <span class="math inline">\(E(\hat{\beta}_1^*) = \beta_1 + A\beta_2\)</span>, where <span class="math inline">\(A = (X_1^T X_1)^{-1}X_1^T X_2\)</span> is the alias matrix.</li>
<li><strong>Variance:</strong> <span class="math inline">\(\text{Var}(\hat{\beta}_1^*) = \sigma^2(X_1^T X_1)^{-1}\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part (i):</strong> <span class="math display">\[
\begin{aligned}
E(\hat{\beta}_1^*) &amp;= E[(X_1^T X_1)^{-1}X_1^T y] \\
&amp;= (X_1^T X_1)^{-1}X_1^T E(y) \\
&amp;= (X_1^T X_1)^{-1}X_1^T (X_1\beta_1 + X_2\beta_2) \\
&amp;= (X_1^T X_1)^{-1}X_1^T X_1\beta_1 + (X_1^T X_1)^{-1}X_1^T X_2\beta_2 \\
&amp;= \beta_1 + A\beta_2
\end{aligned}
\]</span> Thus, <span class="math inline">\(\hat{\beta}_1^*\)</span> is biased unless <span class="math inline">\(\beta_2 = 0\)</span> or <span class="math inline">\(X_1^T X_2 = 0\)</span> (orthogonal design).</p>
<p><strong>Part (ii):</strong> <span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\beta}_1^*) &amp;= \text{Var}[(X_1^T X_1)^{-1}X_1^T y] \\
&amp;= (X_1^T X_1)^{-1}X_1^T [\sigma^2 I] X_1 (X_1^T X_1)^{-1} \\
&amp;= \sigma^2 (X_1^T X_1)^{-1}
\end{aligned}
\]</span> .</p>
</div>
</section>
<section id="overfitting" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">6.10.2</span> Overfitting</h3>
<p>Suppose the reduced model <span class="math inline">\(y = X_1\beta_1^* + e\)</span> is true (i.e., <span class="math inline">\(\beta_2 = 0\)</span>), but we fit the full model <span class="math inline">\((\dagger)\)</span>. Since the full model includes the true model as a special case, the estimator <span class="math inline">\(\hat{\beta}\)</span> from the full model remains unbiased.</p>
<p>However, fitting the extraneous variables affects the variance.</p>
<div id="thm-overfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.14 (Variance Comparison)</strong></span> Let <span class="math inline">\(\hat{\beta}_1\)</span> be the estimator from the full model and <span class="math inline">\(\hat{\beta}_1^*\)</span> be the estimator from the reduced model. Then: <span class="math display">\[ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T \]</span> where <span class="math inline">\(A = (X_1^T X_1)^{-1}X_1^T X_2\)</span> and <span class="math inline">\(B = X_2^T X_2 - X_2^T X_1 A\)</span>. Since <span class="math inline">\(A B^{-1} A^T\)</span> is positive semidefinite, <span class="math inline">\(\text{Var}(\hat{\beta}_1) \ge \text{Var}(\hat{\beta}_1^*)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using the inverse of a partitioned matrix, the top-left block of <span class="math inline">\((X^T X)^{-1}\)</span> corresponding to <span class="math inline">\(\beta_1\)</span> is: <span class="math display">\[ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} \]</span> Since <span class="math inline">\(\text{Var}(\hat{\beta}_1) = \sigma^2 H^{11}\)</span> and <span class="math inline">\(\text{Var}(\hat{\beta}_1^*) = \sigma^2 (X_1^T X_1)^{-1}\)</span>, the difference is the second term: <span class="math display">\[ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T \]</span> .</p>
</div>
</section>
<section id="summary" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.10.3</span> Summary</h3>
<ol type="1">
<li><strong>Underfitting:</strong> Reduces variance but introduces bias (unless variables are orthogonal).</li>
<li><strong>Overfitting:</strong> Keeps estimators unbiased but increases variance.</li>
</ol>
</section>
</section>
<section id="a-data-example-with-house-price-valuation" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="a-data-example-with-house-price-valuation"><span class="header-section-number">6.11</span> A Data Example with House Price Valuation</h2>
<p>A real estate agency wants to refine their pricing model. They regress the selling price of houses (<span class="math inline">\(y\)</span>) on five predictors (<span class="math inline">\(X\)</span>): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.</p>
<p>We assume the data has been collected and saved to <code>house_prices_5pred.csv</code>.</p>
<section id="visualize-the-data" class="level3" data-number="6.11.1">
<h3 data-number="6.11.1" class="anchored" data-anchor-id="visualize-the-data"><span class="header-section-number">6.11.1</span> Visualize the Data</h3>
<p>First, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"house_prices_5pred.csv"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional Display</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (knitr<span class="sc">::</span><span class="fu">is_html_output</span>()) {</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  rmarkdown<span class="sc">::</span><span class="fu">paged_table</span>(df)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(df, <span class="dv">10</span>), <span class="at">caption =</span> <span class="st">"First 10 rows of House Prices"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["Price"],"name":[1],"type":["int"],"align":["right"]},{"label":["Size"],"name":[2],"type":["int"],"align":["right"]},{"label":["Age"],"name":[3],"type":["int"],"align":["right"]},{"label":["Beds"],"name":[4],"type":["int"],"align":["right"]},{"label":["Garage"],"name":[5],"type":["int"],"align":["right"]},{"label":["Lawn"],"name":[6],"type":["int"],"align":["right"]}],"data":[{"1":"425767","2":"3092","3":"18","4":"5","5":"1","6":"325"},{"1":"336991","2":"1802","3":"37","4":"2","5":"1","6":"687"},{"1":"528842","2":"2701","3":"49","4":"2","5":"0","6":"261"},{"1":"399797","2":"2745","3":"0","4":"5","5":"2","6":"554"},{"1":"427580","2":"2143","3":"1","4":"5","5":"3","6":"296"},{"1":"478082","2":"2754","3":"26","4":"4","5":"0","6":"833"},{"1":"295549","2":"2039","3":"17","4":"2","5":"3","6":"194"},{"1":"335058","2":"1758","3":"11","4":"3","5":"1","6":"111"},{"1":"461110","2":"3191","3":"58","4":"2","5":"2","6":"286"},{"1":"204405","2":"1298","3":"41","4":"2","5":"0","6":"813"},{"1":"609878","2":"3255","3":"41","4":"2","5":"0","6":"122"},{"1":"534170","2":"3383","3":"3","4":"3","5":"2","6":"639"},{"1":"419461","2":"2759","3":"58","4":"3","5":"2","6":"824"},{"1":"382463","2":"2289","3":"1","4":"2","5":"1","6":"134"},{"1":"310660","2":"2124","3":"39","4":"4","5":"0","6":"435"},{"1":"445339","2":"3106","3":"52","4":"2","5":"0","6":"908"},{"1":"83827","2":"1276","3":"55","4":"2","5":"0","6":"849"},{"1":"459278","2":"3158","3":"37","4":"3","5":"2","6":"210"},{"1":"308239","2":"1328","3":"24","4":"5","5":"0","6":"513"},{"1":"244552","2":"1739","3":"51","4":"2","5":"3","6":"298"},{"1":"286738","2":"1118","3":"27","4":"4","5":"2","6":"947"},{"1":"264733","2":"1280","3":"25","4":"2","5":"3","6":"110"},{"1":"455328","2":"2701","3":"17","4":"5","5":"0","6":"815"},{"1":"534467","2":"3125","3":"39","4":"4","5":"1","6":"165"},{"1":"522514","2":"2993","3":"39","4":"2","5":"3","6":"720"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</section>
<section id="fit-the-model" class="level3" data-number="6.11.2">
<h3 data-number="6.11.2" class="anchored" data-anchor-id="fit-the-model"><span class="header-section-number">6.11.2</span> Fit the Model</h3>
<p>We will solve for the coefficients <span class="math inline">\(\hat{\beta}\)</span> and fitted values <span class="math inline">\(\hat{y}\)</span> using three distinct methods.</p>
<section id="method-1-naive-matrix-formula" class="level4" data-number="6.11.2.1">
<h4 data-number="6.11.2.1" class="anchored" data-anchor-id="method-1-naive-matrix-formula"><span class="header-section-number">6.11.2.1</span> Method 1: Naive Matrix Formula</h4>
<p>This method solves the normal equations directly on the raw data: <span class="math inline">\(\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define Y and X (add Column of 1s for Intercept)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(df<span class="sc">$</span>Price)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: "lawn" Is Included Here, Even Though It Is Irrelevant</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>X_naive <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="at">Intercept =</span> <span class="dv">1</span>, </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                           df[, <span class="fu">c</span>(<span class="st">"Size"</span>, <span class="st">"Age"</span>, <span class="st">"Beds"</span>, <span class="st">"Garage"</span>, <span class="st">"Lawn"</span>)]))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Solve Beta</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> X_naive</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Xty <span class="ot">&lt;-</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> y</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>beta_naive <span class="ot">&lt;-</span> <span class="fu">solve</span>(XtX) <span class="sc">%*%</span> Xty</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Calculate Y_hat</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>y_hat_naive <span class="ot">&lt;-</span> X_naive <span class="sc">%*%</span> beta_naive</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">t</span>(beta_naive))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Intercept     Size       Age     Beds    Garage      Lawn
[1,]  85085.08 142.1976 -624.3362 3446.122 -5014.307 -34.10539</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(y_hat_naive, <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]
 [1,] 514654.1
 [2,] 296672.3
 [3,] 436559.1
 [4,] 463725.2
 [5,] 381282.7
 [6,] 445839.3
 [7,] 349645.2
 [8,] 329739.2
 [9,] 489735.7
[10,] 223224.4</code></pre>
</div>
</div>
</section>
<section id="method-2-centralized-formula" class="level4" data-number="6.11.2.2">
<h4 data-number="6.11.2.2" class="anchored" data-anchor-id="method-2-centralized-formula"><span class="header-section-number">6.11.2.2</span> Method 2: Centralized Formula</h4>
<p>This method reduces multicollinearity issues and computational errors by centering the data first. Formula: <span class="math inline">\(\hat{\beta}_{\text{slope}} = (X_c^{\prime}X_c)^{-1}X_c^{\prime}y_c\)</span>. The intercept is calculated separately: <span class="math inline">\(\beta_0 = \bar{y} - \bar{x}^{\prime}\hat{\beta}_{\text{slope}}\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Center the Data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>y_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(y)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>X_raw <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(df[, <span class="fu">c</span>(<span class="st">"Size"</span>, <span class="st">"Age"</span>, <span class="st">"Beds"</span>, <span class="st">"Garage"</span>, <span class="st">"Lawn"</span>)])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(X_raw)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>y_c <span class="ot">&lt;-</span> y <span class="sc">-</span> y_bar</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>X_c <span class="ot">&lt;-</span> <span class="fu">sweep</span>(X_raw, <span class="dv">2</span>, X_means) <span class="co"># Subtract column means from X</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Solve for Slope Coefficients (no Intercept Column in X_c)</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>beta_slope <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X_c) <span class="sc">%*%</span> X_c) <span class="sc">%*%</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y_c</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Recover Intercept</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> y_bar <span class="sc">-</span> <span class="fu">sum</span>(X_means <span class="sc">*</span> beta_slope)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine for Full Beta Vector</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>beta_central <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="at">Intercept =</span> beta_0, beta_slope)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Calculate Y_hat Using the Original X and Reconstructed Betas</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>X_full <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_raw)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>y_hat_central <span class="ot">&lt;-</span> X_full <span class="sc">%*%</span> beta_central</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">t</span>(beta_central))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Intercept     Size       Age     Beds    Garage      Lawn
[1,]  85085.08 142.1976 -624.3362 3446.122 -5014.307 -34.10539</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(y_hat_central, <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]
 [1,] 514654.1
 [2,] 296672.3
 [3,] 436559.1
 [4,] 463725.2
 [5,] 381282.7
 [6,] 445839.3
 [7,] 349645.2
 [8,] 329739.2
 [9,] 489735.7
[10,] 223224.4</code></pre>
</div>
</div>
</section>
<section id="method-3-using-rs-lm-function" class="level4" data-number="6.11.2.3">
<h4 data-number="6.11.2.3" class="anchored" data-anchor-id="method-3-using-rs-lm-function"><span class="header-section-number">6.11.2.3</span> Method 3: Using R’s <code>lm</code> Function</h4>
<p>This is the standard approach for practitioners.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Model</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>model_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Price <span class="sc">~</span> ., <span class="at">data =</span> df)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract Coefficients and Fitted Values</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>beta_lm <span class="ot">&lt;-</span> <span class="fu">coef</span>(model_lm)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>y_hat_lm <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model_lm)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(beta_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)        Size         Age        Beds      Garage        Lawn 
85085.08052   142.19762  -624.33616  3446.12224 -5014.30654   -34.10539 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(y_hat_lm, <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       1        2        3        4        5        6        7        8 
514654.1 296672.3 436559.1 463725.2 381282.7 445839.3 349645.2 329739.2 
       9       10 
489735.7 223224.4 </code></pre>
</div>
</div>
</section>
<section id="visualization-of-fitted-values-vs-mean" class="level4" data-number="6.11.2.4">
<h4 data-number="6.11.2.4" class="anchored" data-anchor-id="visualization-of-fitted-values-vs-mean"><span class="header-section-number">6.11.2.4</span> Visualization of Fitted Values vs Mean</h4>
<p>We define <span class="math inline">\(\hat{y}_0\)</span> as the vector of the mean of <span class="math inline">\(y\)</span> (<span class="math inline">\(\bar{y}\)</span>). We plot the actual <span class="math inline">\(y\)</span> against our fitted model <span class="math inline">\(\hat{y}\)</span>, using a green line to represent the “Null Model” (<span class="math inline">\(\hat{y}_0\)</span>).</p>
<p><em>Note: Axes have been set so that X = Predicted Value and Y = Actual Value.</em></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define Y_hat_0 (the Null Model) - for Conceptual Clarity</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>y_hat_0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(y), <span class="fu">length</span>(y))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot (axes Reversed: X=fitted, Y=actual)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y_hat_lm, y,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Actual vs Fitted Prices"</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Fitted Price (y_hat)"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Actual Price (y)"</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add 1:1 Line (perfect Fit Area, Remains Y=x)</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"gray"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Mean Line Representing the Null Model</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Since Y-axis Is 'actual Y', a Horizontal Line at Mean(y) Represents Y_bar</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">mean</span>(y), <span class="at">col =</span> <span class="st">"green"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Data"</span>, <span class="st">"Mean (y_bar)"</span>),</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"green"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="lec5-est_files/figure-html/plot-y-vs-yhat-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="computing-sums-of-squares-sse-sst-ssr" class="level3" data-number="6.11.3">
<h3 data-number="6.11.3" class="anchored" data-anchor-id="computing-sums-of-squares-sse-sst-ssr"><span class="header-section-number">6.11.3</span> Computing Sums of Squares (SSE, SST, SSR)</h3>
<p>We compare different methods to calculate the sources of variation.</p>
<section id="naive-sum-of-squared-errors" class="level4" data-number="6.11.3.1">
<h4 data-number="6.11.3.1" class="anchored" data-anchor-id="naive-sum-of-squared-errors"><span class="header-section-number">6.11.3.1</span> 1. Naive Sum of Squared Errors</h4>
<p>This uses the standard summation definitions: <span class="math inline">\(\sum (\text{Difference})^2\)</span>.</p>
<ul>
<li><strong>SST (Total):</strong> Variation of <span class="math inline">\(y\)</span> around <span class="math inline">\(\hat{y}_0\)</span> (Mean).</li>
<li><strong>SSR (Regression):</strong> Variation of <span class="math inline">\(\hat{y}\)</span> around <span class="math inline">\(\hat{y}_0\)</span> (Mean).</li>
<li><strong>SSE (Error):</strong> Variation of <span class="math inline">\(y\)</span> around <span class="math inline">\(\hat{y}\)</span> (Model).</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectors</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>y_vec <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y_hat_lm)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>y_bar_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(y), <span class="fu">length</span>(y))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculations</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>SST_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>SSR_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>SSE_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Naive Calculation:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Naive Calculation:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SST:"</span>, SST_naive, <span class="st">" SSR:"</span>, SSR_naive, <span class="st">" SSE:"</span>, SSE_naive, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SST: 358921762209  SSR: 282617820807  SSE: 76303941402 </code></pre>
</div>
</div>
</section>
<section id="pythagorean-shortcut-vector-lengths" class="level4" data-number="6.11.3.2">
<h4 data-number="6.11.3.2" class="anchored" data-anchor-id="pythagorean-shortcut-vector-lengths"><span class="header-section-number">6.11.3.2</span> 2. Pythagorean Shortcut (vector Lengths)</h4>
<p>Based on the geometry of least squares, we can treat the variables as vectors. Because the vectors are orthogonal, we can use squared lengths (dot products with themselves).</p>
<p>Formula: <span class="math inline">\(\text{SSR} = ||\hat{y}||^2 - ||\hat{y}_0||^2\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for Squared Euclidean Norm (length Squared)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>len_sq <span class="ot">&lt;-</span> <span class="cf">function</span>(v) <span class="fu">sum</span>(v<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># SST = ||y||^2 - ||y_0||^2</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>SST_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_vec) <span class="sc">-</span> <span class="fu">len_sq</span>(y_bar_vec)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># SSR = ||y_hat||^2 - ||y_0||^2</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>SSR_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_hat) <span class="sc">-</span> <span class="fu">len_sq</span>(y_bar_vec)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># SSE = ||y||^2 - ||y_hat||^2</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>SSE_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_vec) <span class="sc">-</span> <span class="fu">len_sq</span>(y_hat)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Pythagorean Calculation:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pythagorean Calculation:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SST:"</span>, SST_pyth, <span class="st">" SSR:"</span>, SSR_pyth, <span class="st">" SSE:"</span>, SSE_pyth, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SST: 358921762209  SSR: 282617820807  SSE: 76303941402 </code></pre>
</div>
</div>
</section>
<section id="matrix-algebra-shortcuts" class="level4" data-number="6.11.3.3">
<h4 data-number="6.11.3.3" class="anchored" data-anchor-id="matrix-algebra-shortcuts"><span class="header-section-number">6.11.3.3</span> Matrix Algebra Shortcuts</h4>
<p>These formulas use the <span class="math inline">\(\beta\)</span> and <span class="math inline">\(X\)</span> matrices directly. This is computationally efficient for large datasets.</p>
<ul>
<li>Formula A (Centered with <span class="math inline">\(y_c\)</span>): <span class="math inline">\(\text{SSR} = \hat{\beta}_c^{\prime} X_c^{\prime} y_c\)</span></li>
<li>Formula B (Alternative with <span class="math inline">\(y\)</span>): <span class="math inline">\(\text{SSR} = \hat{\beta}_c^{\prime} X_c^{\prime} y\)</span></li>
<li>Formula C (Uncentered): <span class="math inline">\(\text{SSR} = \hat{\beta}^{\prime} X^{\prime} y - n\bar{y}^2\)</span></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>term_correction <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">mean</span>(y)<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># --- SSR Calculations ---</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. SSR Formula a (centered, Using Y_c)</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>SSR_centered_yc <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_slope) <span class="sc">%*%</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y_c</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. SSR Formula a (alternative, Using Raw Y)</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Since X_c Is Centered, X_c' * 1 = 0, so X_c'y_c Is Equivalent to X_c'y</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>SSR_centered_y <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_slope) <span class="sc">%*%</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. SSR Formula B (uncentered Matrix)</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Beta_naive Includes Intercept, X_naive Includes Column of 1s</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>term_beta_X_y <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_naive) <span class="sc">%*%</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> y</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>SSR_uncentered <span class="ot">&lt;-</span> term_beta_X_y <span class="sc">-</span> term_correction</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Equivalence Check Table ---</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>results_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">Metric =</span> <span class="fu">c</span>(<span class="st">"SSR (Centered $X_c,y_c$)"</span>, </span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>             <span class="st">"SSR (Centered $X_c$)"</span>, </span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>             <span class="st">"SSR (Uncentered)"</span>),</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">Formula =</span> <span class="fu">c</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}_c' X_c' y_c$"</span>, </span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>              <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}_c' X_c' y$"</span>, </span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>              <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}' X' y - n</span><span class="sc">\\</span><span class="st">bar{y}^2$"</span>),</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">Value =</span> <span class="fu">c</span>(<span class="fu">as.numeric</span>(SSR_centered_yc), </span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.numeric</span>(SSR_centered_y), </span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.numeric</span>(SSR_uncentered))</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Render the Table</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(results_table, </span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>             <span class="at">digits =</span> <span class="dv">4</span>, </span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Demonstration of SSR Formula Equivalence"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Demonstration of SSR Formula Equivalence</caption>
<colgroup>
<col style="width: 35%">
<col style="width: 46%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SSR (Centered <span class="math inline">\(X_c,y_c\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}_c' X_c' y_c\)</span></td>
<td style="text-align: right;">282617820807</td>
</tr>
<tr class="even">
<td style="text-align: left;">SSR (Centered <span class="math inline">\(X_c\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}_c' X_c' y\)</span></td>
<td style="text-align: right;">282617820807</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SSR (Uncentered)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}' X' y - n\bar{y}^2\)</span></td>
<td style="text-align: right;">282617820807</td>
</tr>
</tbody>
</table>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="./lec4-qf.html" class="pagination-link" aria-label="Distribution of Quadratic Forms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Distribution of Quadratic Forms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ginv.html" class="pagination-link" aria-label="Generalized Inverse">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Generalized Inverse</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>