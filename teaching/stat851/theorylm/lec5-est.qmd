---
title: "Point Estimation in Multiple Linear Regression"
format: html
---

## Linear Models and Least Square Estimator

### Assumptions in Linear Models

Suppose that on a random sample of $n$ units (patients, animals, trees, etc.) we observe a response variable $Y$ and explanatory variables $X_{1},...,X_{k}$. Our data are then $(y_{i},x_{i1},...,x_{ik})$, $i=1,...,n$, or in vector/matrix form $y, x_{1},...,x_{k}$ where $y=(y_{1},...,y_{n})$ and $x_{j}=(x_{1j},...,x_{nj})^{T}$ or $y, X$ where $X=(x_{1},...,x_{k})$.

Either by design or by conditioning on their observed values, $x_{1},...,x_{k}$ are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:

::: {#def-assumptions name="Assumptions A1-A5"}
**A1. (Additive Error)**
$y=\mu+e$ where $e=(e_{1},...,e_{n})^{T}$ is an unobserved random vector with $E(e)=0$. This implies that $\mu=E(y)$ is the unknown mean of $y$.

**A2. (Linearity)**
$\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta$ where $\beta_{1},...,\beta_{k}$ are unknown parameters. This assumption says that $E(y)=\mu\in\text{Col}(X)$ (lies in the column space of $X$); i.e., it is a linear combination of explanatory vectors $x_{1},...,x_{k}$ with coefficients the unknown parameters in $\beta=(\beta_{1},...,\beta_{k})^{T}$. Note that it is linear in $\beta_{1},...,\beta_{k}$, not necessarily in the $x$'s.

**A3. (Independence)**
$e_{1},...,e_{n}$ are independent random variables (and therefore so are $y_{1},...,y_{n})$.

**A4. (Homoscedasticity)**
$e_{1},...,e_{n}$ all have the same variance $\sigma^{2}$; that is, $\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}$ which implies $\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}$.

**A5. (Normality)**
$e\sim N_{n}(0,\sigma^{2}I_{n})$.
:::

### Matrix Formulation

The model can be written algebraically as:
$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n$$

Or in matrix notation:
$$
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 & x_{11} & x_{12} & \cdot\cdot\cdot & x_{1k}\\
1 & x_{21} & x_{22} & \cdot\cdot\cdot & x_{2k}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
1 & x_{n1} & x_{n2} & \cdot\cdot\cdot & x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
$$

This is expressed compactly as:
$$y=X\beta+e$$
where $X$ is the design matrix, and $e \sim N_n(0, \sigma^2 I)$. Alternatively:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e$$

Taken together, all five assumptions can be stated more succinctly as:
$$y\sim N_{n}(X\beta,\sigma^{2}I)$$
with the mean vector $\mu_{y}=X\beta\in \text{Col}(X)$.

:::{.callout }
### A Note on Coefficients
The effect of a parameter depends upon what other explanatory variables are present in the model. For example, $\beta_{0}$ and $\beta_{1}$ in the model:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e$$
will typically be different than $\beta_{0}^{*}$ and $\beta_{1}^{*}$ in the model:
$$y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^{*}$$
In this context, $\beta_0^*$ and $\beta_1^*$ are the population-projected coefficients of the full model, that is,  $\beta_0^*$ and $\beta_1^*$ are the parameters that can best approximate the full model. 
:::

::: {.callout-important}
We will first consider the case that $\text{rank}(X)=k+1$.
:::

### Least Squares Estimator

::: {#def-least-squares name="Least Squares Estimator"}
The **Least Squares Estimator (LSE)** of $\beta$, denoted as $\hat{\beta}$, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses $y$ and the fitted values $X\hat{\beta}$.
$$
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
$$
Solving the normal equations $(X'X)\hat{\beta} = X'y$ yields the closed-form solution (assuming $X$ has full rank):
$$
\hat{\beta} = (X'X)^{-1}X'y
$$

:::

### Estimation Example

::: {#exm-7-3-1a name="Computation of $\hat \beta$"}
We use the data in @tbl-7-1 to illustrate the computation of $\hat{\beta}$ using the normal equations.

Given the data matrices, we compute the cross-product matrices:

$$
X^{\prime}X = \begin{pmatrix}
12 & 52 & 102\\
52 & 395 & 536\\
102 & 536 & 1004
\end{pmatrix}
$$

$$
X^{\prime}y = \begin{pmatrix}
90\\
482\\
872
\end{pmatrix}
$$

The inverse of the cross-product matrix is:
$$
(X^{\prime}X)^{-1} = \begin{pmatrix}
.97476 & .24290 & -.22871\\
.24290 & .16207 & -.11120\\
-.22871 & -.11120 & .08360
\end{pmatrix}
$$

Solving for the estimator:
$$
\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y = \begin{pmatrix}
5.3754\\
3.0118\\
-1.2855
\end{pmatrix}
$$
:::

### Properties of the Estimator

::: {#thm-unbiased name="Unbiasedness of $\hat \beta$"}
If $E(y)=X\beta$, then $\hat{\beta}$ is an unbiased estimator for $\beta$.
:::

::: {.proof}
$$
\begin{aligned}
E(\hat{\beta}) &= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&= \beta
\end{aligned}
$$
:::

::: {#thm-covariance name="Variance of $\hat \beta$"}
If $\text{Var}(y)=\sigma^{2}I$, the covariance matrix for $\hat{\beta}$ is given by $\sigma^{2}(X^{\prime}X)^{-1}$.
:::

::: {.proof}
$$
\begin{aligned}
\text{Var}(\hat{\beta}) &= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
$$
:::

**Note:** These theorems require no assumption of normality.


## Best Linear Unbiased Estimator (BLUE)

::: {#thm-gauss-markov name="Gauss-Markov Theorem"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the least-squares estimators $\hat{\beta}_{j}, j=0,1,...,k$ have minimum variance among all linear unbiased estimators.
:::

::: {.proof}
We consider a linear estimator $Ay$ of $\beta$ and seek the matrix $A$ for which $Ay$ is a minimum variance unbiased estimator.

**1. Unbiasedness Condition:**
In order for $Ay$ to be an unbiased estimator of $\beta$, we must have $E(Ay)=\beta$. Using the assumption $E(y)=X\beta$, this is expressed as:
$$E(Ay) = A E(y) = AX\beta = \beta$$
which implies the condition $AX=I_{k+1}$ since the relationship must hold for any $\beta$.

**2. Minimizing Variance:**
The covariance matrix for the estimator $Ay$ is:
$$\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'$$
We need to choose $A$ (subject to $AX=I$) so that the diagonal elements of $AA'$ are minimized.

To relate $Ay$ to $\hat{\beta}=(X'X)^{-1}X'y$, we define $\hat{A} = (X'X)^{-1}X'$ and write $A = (A - \hat{A}) + \hat{A}$. Then:
$$AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'$$
Expanding this, the cross terms vanish because $(A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'$.
Note that $\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}$.
Also, $A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}$ (since $AX=I$).
Thus, $(A - \hat{A})\hat{A}' = 0$.

The expansion simplifies to:
$$AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'$$
The matrix $(A - \hat{A})(A - \hat{A})'$ is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of $AA'$, we must set $A - \hat{A} = 0$, which implies $A = \hat{A}$.

Thus, the minimum variance estimator is:
$$Ay = (X'X)^{-1}X'y = \hat{\beta}$$
:::

### Notes on Gauss-Markov

1.  **Distributional Generality:** The remarkable feature of the Gauss-Markov theorem is that it holds for *any* distribution of $y$; normality is not required. The only assumptions used are linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$).

2.  **Extension to All Linear Combinations:** The theorem extends beyond just the parameter vector $\beta$ to any linear combination of the parameters.

::: {#cor-linear-combo name="BLUE for All Linear Combinations"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the best linear unbiased estimator of the scalar $a'\beta$ is $a'\hat{\beta}$, where $\hat{\beta}$ is the least-squares estimator.
:::

::: {.proof}
Let $\tilde{\beta} = Ay$ be any other linear unbiased estimator of $\beta$. The variance of the linear combination $a'\tilde{\beta}$ is:
$$
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
$$
From the proof of the Gauss-Markov theorem, we established that $AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}$ where $\hat{A} = (X'X)^{-1}X'$. Substituting this into the variance equation:
$$
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
$$
The term $a'(A-\hat{A})(A-\hat{A})'a$ is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore:
$$
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
$$
The variance is minimized when $A=\hat{A}$ (specifically when the first term is zero), proving that $a'\hat{\beta}$ has the minimum variance among all linear unbiased estimators.
:::

3.  **Scaling Invariance:** The predictions made by the model are invariant to the scaling of the explanatory variables.

::: {#thm-scaling name="Scaling Explanatory Variables"}
If $x=(1,x_{1},...,x_{k})'$ and $z=(1,c_{1}x_{1},...,c_{k}x_{k})'$, then the fitted values are identical: $\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z$.
:::

::: {.proof}
Let $D = \text{diag}(1, c_1, ..., c_k)$ such that the design matrix is transformed to $Z = XD$. The LSE for the transformed data is:
$$
\begin{aligned}
\hat{\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
$$
. Then, the prediction is:
$$
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
$$
.
:::

#### Limitations: Restriction to Unbiased Estimators

It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of **linear** and **unbiased** estimators.

* **Assumption Sensitivity:** If the assumptions of linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$) do not hold, $\hat{\beta}$ may be biased or may have a larger variance than other estimators.
* **Unbiasedness Constraint:** The theorem does not compare $\hat{\beta}$ to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only "best" (minimum variance) among those estimators that satisfy the unbiasedness constraint.


## Estimator of Error Variance

We estimate $\sigma^{2}$ by the residual mean square:

::: {#def-s2 name="Residual Variance Estimator"}
$$s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}$$
where $\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})$.
:::

Alternatively, SSE can be written as:
$$\text{SSE} = y'y - \hat{\beta}'X'y$$
This is often useful for computation ($y'y$ is the total sum of squares of the raw data).

### Unbiasedness of $s^2$

::: {#thm-unbiased-s2 name="Unbiasedness of s-squared"}
If $s^{2}$ is defined as above, and if $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, then $E(s^{2})=\sigma^{2}$.
:::

::: {.proof}
We use the Hat Matrix $H = X(X'X)^{-1}X'$, which projects $y$ onto $\text{Col}(X)$. Thus, $\hat{y} = Hy$.
The residuals are $y - \hat{y} = (I - H)y$. The Sum of Squared Errors is:
$$\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y$$
Since $H$ is symmetric and idempotent, $(I-H)$ is also symmetric and idempotent. Thus:
$$\text{SSE} = y'(I-H)y$$

To find the expectation, we use the trace trick for quadratic forms: $E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]$.
$$
\begin{aligned}
E(\text{SSE}) &= E[y'(I-H)y] \\
&= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
$$
**Trace Term:** $\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)$, since $\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1$.

**Non-centrality Term:** Since $HX = X$, we have $(I-H)X = 0$. Therefore, the second term vanishes: $\beta'X'(I-H)X\beta = 0$.

Combining these:
$$E(\text{SSE}) = \sigma^2(n - k - 1)$$
Dividing by the degrees of freedom $(n-k-1)$, we get $E(s^2) = \sigma^2$.
:::

## Distributions under Normality

If we add Assumption A5 ($y \sim N_n(X\beta, \sigma^2 I)$), we can derive the exact sampling distributions.

::: {#cor-cov-beta name="Estimated Covariance of Beta"}
An unbiased estimator of $\text{Cov}(\hat{\beta})$ is given by:
$$\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}$$
:::

::: {#thm-sampling-dist name="Sampling Distributions"}
Under assumptions A1-A5:

1.  $\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})$.
2.  $(n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)$.
3.  $\hat{\beta}$ and $s^{2}$ are independent.
:::

::: {.proof}
**Part (i):** Since $\hat{\beta} = (X'X)^{-1}X'y$ is a linear transformation of the normal vector $y$, it is also normally distributed. We already established its mean and variance in @thm-unbiased and @thm-covariance.

**Part (ii):** We showed $\text{SSE} = y'(I-H)y$. Since $(I-H)$ is idempotent with rank $n-k-1$, and $(I-H)X\beta = 0$, by the theory of quadratic forms in normal variables, $\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)$.

**Part (iii):** $\hat{\beta}$ depends on $Hy$ (or $X'y$), while $s^2$ depends on $(I-H)y$. Since $H(I-H) = H - H^2 = 0$, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.
:::

## Maximum Likelihood Estimator (MLE)

::: {#thm-mle name="MLE for Linear Regression"}
If $y \sim N_n(X\beta, \sigma^2 I)$, the Maximum Likelihood Estimators are:
$$
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
$$
$$
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
$$
:::

::: {.proof}
The log-likelihood function is:
$$ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) $$
Maximizing this with respect to $\beta$ is equivalent to minimizing the quadratic term $(y - X\beta)'(y - X\beta)$, which yields the Least Squares Estimator.
Differentiating with respect to $\sigma^2$ and setting to zero yields $\hat{\sigma}^2 = \text{SSE}/n$.
:::

**Note:** The MLE for $\sigma^2$ is biased (denominator $n$), whereas $s^2$ is unbiased (denominator $n-k-1$).

## Linear Models in Centered Form

The regression model can be written in a centered form by subtracting the means of the explanatory variables:
$$y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}$$
for $i=1,...,n$, where the intercept term is adjusted:
$$\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}$$
and $\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}$.

## Linear Models in Centered Form

The regression model can be written in a centered form by subtracting the means of the explanatory variables:
$$y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}$$
for $i=1,...,n$, where the intercept term is adjusted:
$$\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}$$
and $\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}$.

### Matrix Formulation

In matrix form, the equivalence between the original model and the centered model is:
$$y = X\beta + e = (j_n, X_c)\begin{pmatrix} \alpha \\ \beta_{1} \end{pmatrix} + e$$
where $\beta_{1}=(\beta_{1},...,\beta_{k})^{T}$ represents the slope coefficients, and $X_c$ is the centered design matrix:
$$X_c = (I - P_{j_n})X_1$$
Here, $X_1$ consists of the original columns of $X$ excluding the intercept column.



To see the structure of $X_c$, we first calculate the projection of the data onto the intercept space, $P_{j_n}X_1$:
$$
\begin{aligned}
P_{j_n}X_1 &= \frac{1}{n}j_n j_n' X_1 \\
&= \begin{pmatrix} 1/n & 1/n & \cdots & 1/n \\ 1/n & 1/n & \cdots & 1/n \\ \vdots & \vdots & \ddots & \vdots \\ 1/n & 1/n & \cdots & 1/n \end{pmatrix} \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1k} \\ x_{21} & x_{22} & \cdots & x_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nk} \end{pmatrix} \\
&= \begin{pmatrix} \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \end{pmatrix}
\end{aligned}
$$
This results in a matrix where every row is the vector of column means. Subtracting this from $X_1$ gives $X_c$:
$$
\begin{aligned}
X_c &= X_1 - P_{j_n}X_1 \\
&= \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1k} \\ x_{21} & x_{22} & \cdots & x_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nk} \end{pmatrix} - \begin{pmatrix} \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \end{pmatrix} \\
&= \begin{pmatrix} x_{11} - \bar{x}_1 & x_{12} - \bar{x}_2 & \cdots & x_{1k} - \bar{x}_k \\ x_{21} - \bar{x}_1 & x_{22} - \bar{x}_2 & \cdots & x_{2k} - \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} - \bar{x}_1 & x_{n2} - \bar{x}_2 & \cdots & x_{nk} - \bar{x}_k \end{pmatrix}
\end{aligned}
$$


### Estimation in Centered Form

Because the column space of the intercept $j_n$ is orthogonal to the columns of $X_c$ (since columns of $X_c$ sum to zero), the cross-product matrix becomes block diagonal:
$$
\begin{pmatrix} j_n' \\ X_c' \end{pmatrix} (j_n, X_c) = \begin{pmatrix} j_n'j_n & j_n'X_c \\ X_c'j_n & X_c'X_c \end{pmatrix} = \begin{pmatrix} n & 0 \\ 0 & X_c'X_c \end{pmatrix}
$$

::: {#thm-centered-estimators name="Centered Estimators"}
The least squares estimators for the centered parameters are:
$$
\begin{pmatrix} \hat{\alpha} \\ \hat{\beta}_{1} \end{pmatrix} = \begin{pmatrix} n & 0 \\ 0 & X_c'X_c \end{pmatrix}^{-1} \begin{pmatrix} j_n'y \\ X_c'y \end{pmatrix} = \begin{pmatrix} \bar{y} \\ (X_c'X_c)^{-1}X_c'y \end{pmatrix}
$$
Thus:

1.  $\hat{\alpha} = \bar{y}$ (The sample mean of $y$).
2.  $\hat{\beta}_{1} = S_{xx}^{-1}S_{xy}$, using the sample covariance notations.
:::

Recovering the original intercept:
$$ \hat{\beta}_0 = \hat{\alpha} - \hat{\beta}_1 \bar{x}_1 - \dots - \hat{\beta}_k \bar{x}_k = \bar{y} - \hat{\beta}_{1}'\bar{x} $$

## Sum of Squares Decomposition

We partition the total variation based on the orthogonal subspaces.

::: {#def-ss-components name="Sum of Squares Components"}
The total variation is decomposed as $\text{SST} = \text{SSR} + \text{SSE}$.

1.  **Total Sum of Squares (SST):** The squared length of the centered response vector.
    $$\text{SST} = \|y - \bar{y}j_n\|^2 = \|(I - P_{j_n})y\|^2$$

2.  **Regression Sum of Squares (SSR):** The variation explained by the regressors $X_c$.
    $$\text{SSR} = \|\hat{y} - \bar{y}j_n\|^2 = \|P_{X_c}y\|^2 = \hat{\beta}_1' X_c' X_c \hat{\beta}_1$$

3.  **Sum of Squared Errors (SSE):** The residual variation.
    $$\text{SSE} = \|y - \hat{y}\|^2 = \|(I - P_{X})y\|^2$$
:::

### Geometric Decomposition of $y$

We partition the total variation in $y$ based on the orthogonal subspaces.

1.  **Space of the Mean:** $L(j_n)$, spanned by the intercept vector $j_n$.
2.  **Space of the Regressors:** $L(X_c)$, spanned by the centered predictors $X_c$.
3.  **Error Space:** $\text{Col}(X)^\perp$, orthogonal to the model space.

The vector $y$ can be decomposed into three orthogonal components:
$$y = \bar{y}j_n + P_{X_c}y + (y - \hat{y})$$
Visually, this corresponds to projecting the vector $y$ onto three orthogonal axes.

**Interactive Visualization:**

We generate a cloud of 100 observations of $y$ from $N(\mu, \sigma=1)$ where $\mu = (5,5,0)$. The projections onto the Model Plane ($z=0$) are highlighted in **red**, and the projections onto the error axis ($z$) are in **yellow**.

```{r}
#| label: fig-geometry-3d-projections
#| fig-cap: "Geometric Decomposition: Hollow Circles for Axis Projections"
#| warning: false
#| message: false

library(plotly)
library(MASS)

# --- 1. Generate Data for Both Scenarios ---
set.seed(123)
n <- 50
sigma <- 0.5
Sigma <- diag(3) * sigma^2

# Scenario A: Regression Effect (beta1 != 0)
mu_A <- c(3, 4, 0)
data_A <- mvrnorm(n, mu = mu_A, Sigma = Sigma)
df_A <- as.data.frame(data_A)
colnames(df_A) <- c("x", "y", "z")

# Scenario B: No Regression Effect (beta1 = 0)
mu_B <- c(3, 0, 0)
data_B <- mvrnorm(n, mu = mu_B, Sigma = Sigma)
df_B <- as.data.frame(data_B)
colnames(df_B) <- c("x", "y", "z")

# --- 2. Helper to Add Traces for a Scenario ---
# We use a 'group' argument to keep track of which scenario the traces belong to
# visible_status: TRUE for the first scenario, FALSE for the others (hidden initially)
add_scenario_traces <- function(fig, df, mu, visible_status) {
  
  # 1. Floor Projections (Red)
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = ~y, z = rep(0, n),
    marker = list(size = 4, color = 'red', symbol = 'diamond', opacity = 0.8),
    visible = visible_status,
    name = 'Proj on Floor'
  )
  
  # 2. Data Cloud (Black)
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = ~y, z = ~z,
    marker = list(size = 4, color = 'black', symbol = 'circle', opacity = 0.6),
    visible = visible_status,
    name = 'Data Cloud'
  )
  
  # 3. Axis Projections (Hollow)
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = rep(0, n), z = rep(0, n),
    marker = list(size = 4, color = 'blue', symbol = 'circle-open', opacity = 0.6),
    visible = visible_status,
    name = 'Proj L(jn)'
  )
  
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = rep(0, n), y = ~y, z = rep(0, n),
    marker = list(size = 4, color = 'green', symbol = 'circle-open', opacity = 0.6),
    visible = visible_status,
    name = 'Proj L(Xc)'
  )
  
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = rep(0, n), y = rep(0, n), z = ~z,
    marker = list(size = 4, color = 'gold', symbol = 'circle-open', opacity = 0.8),
    visible = visible_status,
    name = 'Error'
  )
  
  # 4. Mean Vector
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(0, mu[1]), y = c(0, mu[2]), z = c(0, 0),
    line = list(color = 'black', width = 6),
    visible = visible_status,
    name = 'Mean Vector'
  )
  
  # 5. Guide Lines
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(mu[1], mu[1]), y = c(mu[2], 0), z = c(0, 0),
    line = list(color = 'blue', width = 4, dash = 'dash'),
    visible = visible_status,
    name = 'Link to X'
  )
  
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(mu[1], 0), y = c(mu[2], mu[2]), z = c(0, 0),
    line = list(color = 'green', width = 4, dash = 'dash'),
    visible = visible_status,
    name = 'Link to Y'
  )
  
  return(fig)
}

# --- 3. Build the Combined Figure ---
fig <- plot_ly()

# Add Static Floor (Always Visible)
x_grid_range <- seq(0, 7, length.out = 20)
y_grid_range <- seq(-4, 8, length.out = 20)

z_plane <- matrix(0, nrow = 20, ncol = 20)
fig <- fig %>% add_surface(
  x = x_grid_range, y = y_grid_range, z = z_plane,
  opacity = 0.3, colorscale = list(c(0, 1), c("steelblue", "steelblue")),
  showscale = FALSE, name = 'Model Space'
)

# Add Scenario A Traces (Indices 2-9) -> Visible initially
fig <- add_scenario_traces(fig, df_A, mu_A, visible_status = TRUE)

# Add Scenario B Traces (Indices 10-17) -> Hidden initially
fig <- add_scenario_traces(fig, df_B, mu_B, visible_status = FALSE)


# --- 4. Define Buttons (Update Menus) ---
# Each trace added creates an index in the plot object.
# Index 1: Surface (Always visible)
# Indices 2-9: Scenario A (8 traces)
# Indices 10-17: Scenario B (8 traces)

# Vector for Scenario A: [TRUE (Surface), TRUE, TRUE..., FALSE, FALSE...]
vis_A <- c(TRUE, rep(TRUE, 8), rep(FALSE, 8))

# Vector for Scenario B: [TRUE (Surface), FALSE, FALSE..., TRUE, TRUE...]
vis_B <- c(TRUE, rep(FALSE, 8), rep(TRUE, 8))

fig <- fig %>% layout(
  title = "Comparing Regression Effects",
  scene = list(
    xaxis = list(title = "L(j<sub>n</sub>)", range = c(0, 8)),
    yaxis = list(title = "L(X<sub>c</sub>)", range = c(-4, 8)),
    zaxis = list(title = "Col(X)<sup>&perp;</sup>", range = c(-4, 4)),
    aspectmode = "cube",
    camera = list(eye = list(x = 1.6, y = 1.6, z = 0.6))
  ),
  updatemenus = list(
    list(
      type = "buttons",
      direction = "left",
      x = 0.05, y = 1,
      buttons = list(
        list(
          method = "update",
          args = list(list(visible = vis_A), 
                      list(title = "Scenario 1: Regression Effect (beta1 != 0)")),
          label = "Effect Exists"
        ),
        list(
          method = "update",
          args = list(list(visible = vis_B), 
                      list(title = "Scenario 2: No Effect (beta1 = 0)")),
          label = "No Effect"
        )
      )
    )
  ),
  showlegend = FALSE
)

fig
```

## Coefficient of Determination ($R^2$)

The $R^2$ statistic measures the proportion of total variation explained by the regression model.

::: {#def-r2 name="R-Squared"}
$$R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}$$
Since $0 \le \text{SSE} \le \text{SST}$, it follows that $0 \le R^2 \le 1$.
:::

In terms of the centered coefficients:
$$R^2 = \frac{\hat{\beta}_1' X_c' X_c \hat{\beta}_1}{\sum (y_i - \bar{y})^2}$$

### Adjusted $R^2$ ($R^2_a$)

Standard $R^2$ always increases (or stays the same) as predictors are added, regardless of their value. The Adjusted $R^2$ penalizes for model complexity.

::: {#def-adj-r2 name="Adjusted R-Squared"}
$$R^2_a = 1 - \frac{\text{SSE} / (n - k - 1)}{\text{SST} / (n - 1)} = 1 - \frac{\text{MSE}}{s_y^2}$$
Relationship to $R^2$:
$$R^2_a = 1 - (1 - R^2) \frac{n-1}{n-k-1}$$
:::

### Expected Value of $R^2$

If the true slope coefficients are zero ($\beta_1 = \dots = \beta_k = 0$), the expected value of $R^2$ is not zero due to mathematical constraints.

::: {#thm-r2-beta name="Exact Distribution of R-Squared"}
Under the null hypothesis where all slope coefficients are zero, $R^2$ follows a Beta distribution:
$$R^2 \sim \text{Beta}\left(\frac{k}{2}, \frac{n-k-1}{2}\right)$$
:::

::: {.proof}
Recall that $R^2 = \frac{\text{SSR}}{\text{SSR} + \text{SSE}}$. Under the null hypothesis:

1.  $\text{SSR}/\sigma^2 \sim \chi^2(k)$.
2.  $\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)$.
3.  $\text{SSR}$ and $\text{SSE}$ are independent.

By the definition of the Beta distribution, if $X \sim \chi^2(u)$ and $Y \sim \chi^2(v)$ are independent, then the ratio $\frac{X}{X+Y}$ follows a $\text{Beta}(\frac{u}{2}, \frac{v}{2})$ distribution.
:::

**Derivation of the Expected Value:**

Using the properties of the Beta distribution, the mean is given by $E[R^2] = \frac{\alpha}{\alpha + \beta}$.
Substituting the degrees of freedom $\alpha = k/2$ and $\beta = (n-k-1)/2$:

$$
E[R^2] = \frac{k/2}{k/2 + (n-k-1)/2} = \frac{k}{n-1}
$$

Thus, even when there is no relationship between $y$ and $X$, the expected $R^2$ is approximately $k/(n-1)$ rather than 0.

However, for the Adjusted $R^2$, under the null hypothesis:
$$E[R^2_a] \approx 1 - \frac{\sigma^2}{\sigma^2} = 0$$

This property makes $R^2_a$ a more appropriate metric for comparing models with different numbers of predictors, as it accounts for "overfitting" by penalizing the degrees of freedom consumed by the model.

## Underfitting and Overfitting

We consider the effects of omitting explanatory variables that should be included (underfitting) and including variables that should be excluded (overfitting).

Suppose the true model is:
$$y = X\beta + e = \begin{pmatrix} X_1 & X_2 \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} + e = X_1\beta_1 + X_2\beta_2 + e \quad (\dagger)$$
where $\text{Var}(e) = \sigma^2 I$.

* **Underfitting:** Leaving out $X_2\beta_2$ when $\beta_2 \neq 0$.
* **Overfitting:** Including $X_2\beta_2$ when $\beta_2 = 0$.

### Underfitting

Suppose model $(\dagger)$ holds, but we fit the reduced model:
$$y = X_1\beta_1^* + e^*$$
The OLS estimator for this reduced model is $\hat{\beta}_1^* = (X_1^T X_1)^{-1}X_1^T y$.

::: {#thm-underfitting name="Bias and Variance under Underfitting"}
If we fit the reduced model when the full model $(\dagger)$ is true:

1.  **Bias:** $E(\hat{\beta}_1^*) = \beta_1 + A\beta_2$, where $A = (X_1^T X_1)^{-1}X_1^T X_2$ is the alias matrix.
2.  **Variance:** $\text{Var}(\hat{\beta}_1^*) = \sigma^2(X_1^T X_1)^{-1}$.
:::

::: {.proof}
**Part (i):**
$$
\begin{aligned}
E(\hat{\beta}_1^*) &= E[(X_1^T X_1)^{-1}X_1^T y] \\
&= (X_1^T X_1)^{-1}X_1^T E(y) \\
&= (X_1^T X_1)^{-1}X_1^T (X_1\beta_1 + X_2\beta_2) \\
&= (X_1^T X_1)^{-1}X_1^T X_1\beta_1 + (X_1^T X_1)^{-1}X_1^T X_2\beta_2 \\
&= \beta_1 + A\beta_2
\end{aligned}
$$
Thus, $\hat{\beta}_1^*$ is biased unless $\beta_2 = 0$ or $X_1^T X_2 = 0$ (orthogonal design).

**Part (ii):**
$$
\begin{aligned}
\text{Var}(\hat{\beta}_1^*) &= \text{Var}[(X_1^T X_1)^{-1}X_1^T y] \\
&= (X_1^T X_1)^{-1}X_1^T [\sigma^2 I] X_1 (X_1^T X_1)^{-1} \\
&= \sigma^2 (X_1^T X_1)^{-1}
\end{aligned}
$$
.
:::

### Overfitting

Suppose the reduced model $y = X_1\beta_1^* + e$ is true (i.e., $\beta_2 = 0$), but we fit the full model $(\dagger)$.
Since the full model includes the true model as a special case, the estimator $\hat{\beta}$ from the full model remains unbiased.

However, fitting the extraneous variables affects the variance.

::: {#thm-overfitting name="Variance Comparison"}
Let $\hat{\beta}_1$ be the estimator from the full model and $\hat{\beta}_1^*$ be the estimator from the reduced model. Then:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
where $A = (X_1^T X_1)^{-1}X_1^T X_2$ and $B = X_2^T X_2 - X_2^T X_1 A$.
Since $A B^{-1} A^T$ is positive semidefinite, $\text{Var}(\hat{\beta}_1) \ge \text{Var}(\hat{\beta}_1^*)$.
:::

::: {.proof}
Using the inverse of a partitioned matrix, the top-left block of $(X^T X)^{-1}$ corresponding to $\beta_1$ is:
$$ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} $$
Since $\text{Var}(\hat{\beta}_1) = \sigma^2 H^{11}$ and $\text{Var}(\hat{\beta}_1^*) = \sigma^2 (X_1^T X_1)^{-1}$, the difference is the second term:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
.
:::



### Summary

1.  **Underfitting:** Reduces variance but introduces bias (unless variables are orthogonal).
2.  **Overfitting:** Keeps estimators unbiased but increases variance.

The goal of model selection is to balance these two errors, adhering to the principle of **Occam's Razor**: "entities should not be multiplied beyond necessity".
