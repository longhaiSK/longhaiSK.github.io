---
format: 
  pdf: default
  html:
    code-fold: false
---
# Estimation in Multiple Linear Regression

## Linear Models and Least Square Estimator

### Assumptions in Linear Models

Suppose that on a random sample of $n$ units (patients, animals, trees, etc.) we observe a response variable $Y$ and explanatory variables $X_{1},...,X_{k}$. Our data are then $(y_{i},x_{i1},...,x_{ik})$, $i=1,...,n$, or in vector/matrix form $y, x_{1},...,x_{k}$ where $y=(y_{1},...,y_{n})$ and $x_{j}=(x_{1j},...,x_{nj})^{T}$ or $y, X$ where $X=(x_{1},...,x_{k})$.

Either by design or by conditioning on their observed values, $x_{1},...,x_{k}$ are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:

**Assumptions on Linear Models**

* **A1. (Additive Error)**
$y=\mu+e$ where $e=(e_{1},...,e_{n})^{T}$ is an unobserved random vector with $E(e)=0$. This implies that $\mu=E(y)$ is the unknown mean of $y$.

* **A2. (Linearity)**
$\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta$ where $\beta_{1},...,\beta_{k}$ are unknown parameters. This assumption says that $E(y)=\mu\in\text{Col}(X)$ (lies in the column space of $X$); i.e., it is a linear combination of explanatory vectors $x_{1},...,x_{k}$ with coefficients the unknown parameters in $\beta=(\beta_{1},...,\beta_{k})^{T}$. Note that it is linear in $\beta_{1},...,\beta_{k}$, not necessarily in the $x$'s.

* **A3. (Independence)**
$e_{1},...,e_{n}$ are independent random variables (and therefore so are $y_{1},...,y_{n})$.

* **A4. (Homoscedasticity)**
$e_{1},...,e_{n}$ all have the same variance $\sigma^{2}$; that is, $\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}$ which implies $\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}$.

* **A5. (Normality)**
$e\sim N_{n}(0,\sigma^{2}I_{n})$.


### Matrix Formulation

The model can be written algebraically as:
$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n$$

Or in matrix notation:
$$
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 & x_{11} & x_{12} & \cdot\cdot\cdot & x_{1k}\\
1 & x_{21} & x_{22} & \cdot\cdot\cdot & x_{2k}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
1 & x_{n1} & x_{n2} & \cdot\cdot\cdot & x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
$$

This is expressed compactly as:
$$y=X\beta+e$$
where $X$ is the design matrix, and $e \sim N_n(0, \sigma^2 I)$. Alternatively:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e$$

Taken together, all five assumptions can be stated more succinctly as:
$$y\sim N_{n}(X\beta,\sigma^{2}I)$$
with the mean vector $\mu_{y}=X\beta\in \text{Col}(X)$.

:::{.callout }
### A Note on Coefficients
The effect of a parameter depends upon what other explanatory variables are present in the model. For example, $\beta_{0}$ and $\beta_{1}$ in the model:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e$$
will typically be different than $\beta_{0}^{*}$ and $\beta_{1}^{*}$ in the model:
$$y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^{*}$$
In this context, $\beta_0^*$ and $\beta_1^*$ are the population-projected coefficients of the full model, that is,  $\beta_0^*$ and $\beta_1^*$ are the parameters that can best approximate the full model. 
:::

::: {.callout-important}
We will first consider the case that $\text{rank}(X)=k+1$.
:::

### Least Squares Estimator of $\beta$ and Fitted Value $\hat Y$

::: {#def-least-squares name="Least Squares Estimator"}
The **Least Squares Estimator (LSE)** of $\beta$, denoted as $\hat{\beta}$, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses $y$ and the fitted values $X\hat{\beta}$.
$$
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
$$
:::

We can derive the closed-form solution for $\hat{\beta}$ using the geometry of projections discussed in previous chapters.

#### 1. Obtaining $\hat Y$ {.unnumbered}

In the linear model $y = X\beta + e$, the systematic component (the mean $E[y]$) is constrained to lie in the column space of $X$, denoted as $\text{Col}(X)$. We seek the vector in $\text{Col}(X)$ that is "closest" to the observed data vector $y$. As established in the theory of projections, this closest vector is the **orthogonal projection** of $y$ onto $\text{Col}(X)$. Let $\hat{y}$ denote this fitted value vector. Using the explicit formula for the projection matrix $$H = X(X'X)^{-1}X',$$ we have:
$$ \hat{y} = Hy = X(X'X)^{-1}X' y.$$

#### 2. Obtaining $\hat{\beta}$ by Solving $x\beta = \hat{y}$ {.unnumbered}

Since the fitted vector $\hat{y}$ is a projection onto $\text{Col}(X)$, it must lie entirely within that column space. This guarantees that the linear system for the coefficients $\hat{\beta}$ is consistent (has an exact solution):
$$ X\hat{\beta} = \hat{y} $$

To isolate $\hat{\beta}$, we pre-multiply both sides by the left pseudo-inverse of $X$, which is $(X'X)^{-1}X'$:

$$
\begin{aligned}
(X'X)^{-1}X' (X\hat{\beta}) &= (X'X)^{-1}X' \hat{y} \\
\underbrace{(X'X)^{-1}(X'X)}_{I} \hat{\beta} &= (X'X)^{-1}X' \hat{y}
\end{aligned}
$$

This gives us the estimator expressed in terms of the fitted values:

$$
\boxed{\hat{\beta} = (X'X)^{-1}X' \hat{y}}
$$

However, we typically calculate the estimator from the observed data $y$. Recall that because $\hat{y}$ is an orthogonal projection, the difference $y - \hat{y}$ is orthogonal to $X$. This implies $X'\hat{y} = X'y$. Substituting this into the equation above yields the standard closed-form solution:

$$
\boxed{\hat{\beta} = (X'X)^{-1}X'y}
$$


### Properties of the Estimator $\hat \beta$

::: {#thm-unbiased name="Unbiasedness of $\hat \beta$"}
If $E(y)=X\beta$, then $\hat{\beta}$ is an unbiased estimator for $\beta$.
:::

::: {.proof}
$$
\begin{aligned}
E(\hat{\beta}) &= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&= \beta
\end{aligned}
$$
:::

::: {#thm-covariance name="Variance of $\hat \beta$"}
If $\text{Var}(y)=\sigma^{2}I$, the covariance matrix for $\hat{\beta}$ is given by $\sigma^{2}(X^{\prime}X)^{-1}$.
:::

::: {.proof}
$$
\begin{aligned}
\text{Var}(\hat{\beta}) &= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
$$
:::

**Note:** These theorems require no assumption of normality.


## Best Linear Unbiased Estimator (BLUE)

::: {#thm-gauss-markov name="Gauss-Markov Theorem"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the least-squares estimators $\hat{\beta}_{j}, j=0,1,...,k$ have minimum variance among all linear unbiased estimators.
:::

::: {.proof}
We consider a linear estimator $Ay$ of $\beta$ and seek the matrix $A$ for which $Ay$ is a minimum variance unbiased estimator.

**1. Unbiasedness Condition:**
In order for $Ay$ to be an unbiased estimator of $\beta$, we must have $E(Ay)=\beta$. Using the assumption $E(y)=X\beta$, this is expressed as:
$$E(Ay) = A E(y) = AX\beta = \beta$$
which implies the condition $AX=I_{k+1}$ since the relationship must hold for any $\beta$.

**2. Minimizing Variance:**
The covariance matrix for the estimator $Ay$ is:
$$\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'$$
We need to choose $A$ (subject to $AX=I$) so that the diagonal elements of $AA'$ are minimized.

To relate $Ay$ to $\hat{\beta}=(X'X)^{-1}X'y$, we define $\hat{A} = (X'X)^{-1}X'$ and write $A = (A - \hat{A}) + \hat{A}$. Then:
$$AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'$$
Expanding this, the cross terms vanish because $(A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'$.
Note that $\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}$.
Also, $A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}$ (since $AX=I$).
Thus, $(A - \hat{A})\hat{A}' = 0$.

The expansion simplifies to:
$$AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'$$
The matrix $(A - \hat{A})(A - \hat{A})'$ is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of $AA'$, we must set $A - \hat{A} = 0$, which implies $A = \hat{A}$.

Thus, the minimum variance estimator is:
$$Ay = (X'X)^{-1}X'y = \hat{\beta}$$
:::

### Notes on Gauss-markov

1.  **Distributional Generality:** The remarkable feature of the Gauss-Markov theorem is that it holds for *any* distribution of $y$; normality is not required. The only assumptions used are linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$).

2.  **Extension to All Linear Combinations:** The theorem extends beyond just the parameter vector $\beta$ to any linear combination of the parameters.

::: {#cor-linear-combo name="BLUE for All Linear Combinations"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the best linear unbiased estimator of the scalar $a'\beta$ is $a'\hat{\beta}$, where $\hat{\beta}$ is the least-squares estimator.
:::

::: {.proof}
Let $\tilde{\beta} = Ay$ be any other linear unbiased estimator of $\beta$. The variance of the linear combination $a'\tilde{\beta}$ is:
$$
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
$$
From the proof of the Gauss-Markov theorem, we established that $AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}$ where $\hat{A} = (X'X)^{-1}X'$. Substituting this into the variance equation:
$$
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
$$
The term $a'(A-\hat{A})(A-\hat{A})'a$ is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore:
$$
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
$$
The variance is minimized when $A=\hat{A}$ (specifically when the first term is zero), proving that $a'\hat{\beta}$ has the minimum variance among all linear unbiased estimators.
:::

3.  **Scaling Invariance:** The predictions made by the model are invariant to the scaling of the explanatory variables.

::: {#thm-scaling name="Scaling Explanatory Variables"}
If $x=(1,x_{1},...,x_{k})'$ and $z=(1,c_{1}x_{1},...,c_{k}x_{k})'$, then the fitted values are identical: $\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z$.
:::

::: {.proof}
Let $D = \text{diag}(1, c_1, ..., c_k)$ such that the design matrix is transformed to $Z = XD$. The LSE for the transformed data is:
$$
\begin{aligned}
\hat{\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
$$
. Then, the prediction is:
$$
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
$$
.
:::

#### Limitations: Restriction to Unbiased Estimators

It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of **linear** and **unbiased** estimators.

* **Assumption Sensitivity:** If the assumptions of linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$) do not hold, $\hat{\beta}$ may be biased or may have a larger variance than other estimators.
* **Unbiasedness Constraint:** The theorem does not compare $\hat{\beta}$ to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only "best" (minimum variance) among those estimators that satisfy the unbiasedness constraint.


## Estimator of Error Variance

We estimate $\sigma^{2}$ by the residual mean square:

::: {#def-s2 name="Residual Variance Estimator"}
$$s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}$$
where $\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})$.
:::

Alternatively, SSE can be written as:
$$\text{SSE} = y'y - \hat{\beta}'X'y$$
This is often useful for computation ($y'y$ is the total sum of squares of the raw data).

### Unbiasedness of $s^2$

::: {#thm-unbiased-s2 name="Unbiasedness of s-squared"}
If $s^{2}$ is defined as above, and if $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, then $E(s^{2})=\sigma^{2}$.
:::

::: {.proof}
We use the Hat Matrix $H = X(X'X)^{-1}X'$, which projects $y$ onto $\text{Col}(X)$. Thus, $\hat{y} = Hy$.
The residuals are $y - \hat{y} = (I - H)y$. The Sum of Squared Errors is:
$$\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y$$
Since $H$ is symmetric and idempotent, $(I-H)$ is also symmetric and idempotent. Thus:
$$\text{SSE} = y'(I-H)y$$

To find the expectation, we use the trace trick for quadratic forms: $E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]$.
$$
\begin{aligned}
E(\text{SSE}) &= E[y'(I-H)y] \\
&= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
$$
**Trace Term:** $\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)$, since $\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1$.

**Non-centrality Term:** Since $HX = X$, we have $(I-H)X = 0$. Therefore, the second term vanishes: $\beta'X'(I-H)X\beta = 0$.

Combining these:
$$E(\text{SSE}) = \sigma^2(n - k - 1)$$
Dividing by the degrees of freedom $(n-k-1)$, we get $E(s^2) = \sigma^2$.
:::

## Distributions Under Normality

If we add Assumption A5 ($y \sim N_n(X\beta, \sigma^2 I)$), we can derive the exact sampling distributions.

::: {#cor-cov-beta name="Estimated Covariance of Beta"}
An unbiased estimator of $\text{Cov}(\hat{\beta})$ is given by:
$$\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}$$
:::

::: {#thm-sampling-dist name="Sampling Distributions"}
Under assumptions A1-A5:

1.  $\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})$.
2.  $(n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)$.
3.  $\hat{\beta}$ and $s^{2}$ are independent.
:::

::: {.proof}
**Part (i):** Since $\hat{\beta} = (X'X)^{-1}X'y$ is a linear transformation of the normal vector $y$, it is also normally distributed. We already established its mean and variance in @thm-unbiased and @thm-covariance.

**Part (ii):** We showed $\text{SSE} = y'(I-H)y$. Since $(I-H)$ is idempotent with rank $n-k-1$, and $(I-H)X\beta = 0$, by the theory of quadratic forms in normal variables, $\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)$.

**Part (iii):** $\hat{\beta}$ depends on $Hy$ (or $X'y$), while $s^2$ depends on $(I-H)y$. Since $H(I-H) = H - H^2 = 0$, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.
:::

## Maximum Likelihood Estimator (MLE)

::: {#thm-mle name="MLE for Linear Regression"}
If $y \sim N_n(X\beta, \sigma^2 I)$, the Maximum Likelihood Estimators are:
$$
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
$$
$$
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
$$
:::

::: {.proof}
The log-likelihood function is:
$$ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) $$
Maximizing this with respect to $\beta$ is equivalent to minimizing the quadratic term $(y - X\beta)'(y - X\beta)$, which yields the Least Squares Estimator.
Differentiating with respect to $\sigma^2$ and setting to zero yields $\hat{\sigma}^2 = \text{SSE}/n$.
:::

**Note:** The MLE for $\sigma^2$ is biased (denominator $n$), whereas $s^2$ is unbiased (denominator $n-k-1$).



## Linear Models in Centered Form

The regression model can be written in a centered form by subtracting the means of the explanatory variables:
$$y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}$$
for $i=1,...,n$, where the intercept term is adjusted:
$$\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}$$
and $\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}$.

### Matrix Formulation

In matrix form, the equivalence between the original model and the centered model is:
$$y = X\beta + e = (j_n, X_c)\begin{pmatrix} \alpha \\ \beta_{1} \end{pmatrix} + e$$
where $\beta_{1}=(\beta_{1},...,\beta_{k})^{T}$ represents the slope coefficients, and $X_c$ is the centered design matrix:
$$X_c = (I - P_{j_n})X_1$$
Here, $X_1$ consists of the original columns of $X$ excluding the intercept column.



To see the structure of $X_c$, we first calculate the projection of the data onto the intercept space, $P_{j_n}X_1$:
$$
\begin{aligned}
P_{j_n}X_1 &= \frac{1}{n}j_n j_n' X_1 \\
&= \begin{pmatrix} 1/n & 1/n & \cdots & 1/n \\ 1/n & 1/n & \cdots & 1/n \\ \vdots & \vdots & \ddots & \vdots \\ 1/n & 1/n & \cdots & 1/n \end{pmatrix} \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1k} \\ x_{21} & x_{22} & \cdots & x_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nk} \end{pmatrix} \\
&= \begin{pmatrix} \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \end{pmatrix}
\end{aligned}
$$
This results in a matrix where every row is the vector of column means. Subtracting this from $X_1$ gives $X_c$:
$$
\begin{aligned}
X_c &= X_1 - P_{j_n}X_1 \\
&= \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1k} \\ x_{21} & x_{22} & \cdots & x_{2k} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nk} \end{pmatrix} - \begin{pmatrix} \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ \bar{x}_1 & \bar{x}_2 & \cdots & \bar{x}_k \end{pmatrix} \\
&= \begin{pmatrix} x_{11} - \bar{x}_1 & x_{12} - \bar{x}_2 & \cdots & x_{1k} - \bar{x}_k \\ x_{21} - \bar{x}_1 & x_{22} - \bar{x}_2 & \cdots & x_{2k} - \bar{x}_k \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} - \bar{x}_1 & x_{n2} - \bar{x}_2 & \cdots & x_{nk} - \bar{x}_k \end{pmatrix}
\end{aligned}
$$


### Estimation in Centered Form

Because the column space of the intercept $j_n$ is orthogonal to the columns of $X_c$ (since columns of $X_c$ sum to zero), the cross-product matrix becomes block diagonal:
$$
\begin{pmatrix} j_n' \\ X_c' \end{pmatrix} (j_n, X_c) = \begin{pmatrix} j_n'j_n & j_n'X_c \\ X_c'j_n & X_c'X_c \end{pmatrix} = \begin{pmatrix} n & 0 \\ 0 & X_c'X_c \end{pmatrix}
$$

::: {#thm-centered-estimators name="Centered Estimators"}
The least squares estimators for the centered parameters are:
$$
\begin{pmatrix} \hat{\alpha} \\ \hat{\beta}_{1} \end{pmatrix} = \begin{pmatrix} n & 0 \\ 0 & X_c'X_c \end{pmatrix}^{-1} \begin{pmatrix} j_n'y \\ X_c'y \end{pmatrix} = \begin{pmatrix} \bar{y} \\ (X_c'X_c)^{-1}X_c'y \end{pmatrix}
$$
Thus:

1.  $\hat{\alpha} = \bar{y}$ (The sample mean of $y$).
2.  $\hat{\beta}_{1} = S_{xx}^{-1}S_{xy}$, using the sample covariance notations.
:::

Recovering the original intercept:
$$ \hat{\beta}_0 = \hat{\alpha} - \hat{\beta}_1 \bar{x}_1 - \dots - \hat{\beta}_k \bar{x}_k = \bar{y} - \hat{\beta}_{1}'\bar{x} $$

## Sum of Squares Decomposition

We partition the total variation based on the orthogonal subspaces.

::: {#def-ss-components name="Sum of Squares Components"}
The total variation is decomposed as $\text{SST} = \text{SSR} + \text{SSE}$.

1.  **Total Sum of Squares (SST):** The squared length of the centered response vector.
    $$\text{SST} = \|y - \bar{y}j_n\|^2 = \|(I - P_{j_n})y\|^2$$

2.  **Regression Sum of Squares (SSR):** The variation explained by the regressors $X_c$.
    $$\text{SSR} = \|\hat{y} - \bar{y}j_n\|^2 = \|P_{X_c}y\|^2 = \hat{\beta}_1' X_c' X_c \hat{\beta}_1$$

3.  **Sum of Squared Errors (SSE):** The residual variation.
    $$\text{SSE} = \|y - \hat{y}\|^2 = \|(I - H)y\|^2$$
:::


### 3D Visualization of Decomposition of $y$

We partition the total variation in $y$ based on the orthogonal subspaces.

1.  **Space of the Mean:** $L(j_n)$, spanned by the intercept vector $j_n$.
2.  **Space of the Regressors:** $L(X_c)$, spanned by the centered predictors $X_c$.
3.  **Error Space:** $\text{Col}(X)^\perp$, orthogonal to the model space.

The vector $y$ can be decomposed into three orthogonal components:
$$y = \bar{y}j_n + P_{X_c}y + (y - \hat{y})$$
Visually, this corresponds to projecting the vector $y$ onto three orthogonal axes.

**Interactive Visualization:**

We generate a cloud of 100 observations of $y$ from $N(\mu, \sigma=1)$ where $\mu = (5,5,0)$. The projections onto the Model Plane ($z=0$) are highlighted in **red**, and the projections onto the error axis ($z$) are in **yellow**.

```{r}
#| label: setup-3d-geometry
#| include: false

library(plotly)
library(MASS)

# --- Check Dependencies for PDF ---
if (!knitr::is_html_output()) {
  if (!requireNamespace("webshot2", quietly = TRUE)) {
    stop("Package 'webshot2' is required for PDF output. Run: install.packages('webshot2')")
  }
}

# --- 1. Data Generation Helper ---
get_data <- function(beta_effect = TRUE) {
  set.seed(123)
  n <- 50
  sigma <- 0.5
  Sigma <- diag(3) * sigma^2
  
  mu <- if(beta_effect) c(3, 4, 0) else c(3, 0, 0)
  
  data <- mvrnorm(n, mu = mu, Sigma = Sigma)
  df <- as.data.frame(data)
  colnames(df) <- c("x", "y", "z")
  
  list(df = df, mu = mu)
}

# --- 2. Plotting Helper Function ---
create_3d_plot <- function(data_list, title) {
  df <- data_list$df
  mu <- data_list$mu
  n <- nrow(df)
  
  fig <- plot_ly()
  
  # Static Floor
  x_grid <- seq(0, 7, length.out = 20)
  y_grid <- seq(-4, 8, length.out = 20)
  z_plane <- matrix(0, nrow = 20, ncol = 20)
  
  fig <- fig %>% add_surface(
    x = x_grid, y = y_grid, z = z_plane,
    opacity = 0.3, colorscale = list(c(0, 1), c("steelblue", "steelblue")),
    showscale = FALSE, name = 'Model Space'
  )
  
  # Floor Projections
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = ~y, z = rep(0, n),
    marker = list(size = 4, color = 'red', symbol = 'diamond', opacity = 0.8),
    name = 'Proj on Floor'
  )
  
  # Data Cloud
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = ~y, z = ~z,
    marker = list(size = 4, color = 'black', symbol = 'circle', opacity = 0.6),
    name = 'Data Cloud'
  )
  
  # Axis Projections
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = ~x, y = rep(0, n), z = rep(0, n),
    marker = list(size = 4, color = 'blue', symbol = 'circle-open', opacity = 0.6),
    name = 'Proj L(jn)'
  )
  
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = rep(0, n), y = ~y, z = rep(0, n),
    marker = list(size = 4, color = 'green', symbol = 'circle-open', opacity = 0.6),
    name = 'Proj L(Xc)'
  )
  
  fig <- fig %>% add_trace(
    data = df, type = 'scatter3d', mode = 'markers',
    x = rep(0, n), y = rep(0, n), z = ~z,
    marker = list(size = 4, color = 'gold', symbol = 'circle-open', opacity = 0.8),
    name = 'Error'
  )
  
  # Vectors
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(0, mu[1]), y = c(0, mu[2]), z = c(0, 0),
    line = list(color = 'black', width = 6),
    name = 'Mean Vector'
  )
  
  # Guide Lines
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(mu[1], mu[1]), y = c(mu[2], 0), z = c(0, 0),
    line = list(color = 'blue', width = 4, dash = 'dash'),
    name = 'Link to X'
  )
  
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'lines',
    x = c(mu[1], 0), y = c(mu[2], mu[2]), z = c(0, 0),
    line = list(color = 'green', width = 4, dash = 'dash'),
    name = 'Link to Y'
  )
  
  # Layout
  fig <- fig %>% layout(
    title = title,
    scene = list(
      xaxis = list(title = "L(j<sub>n</sub>)", range = c(0, 8)),
      yaxis = list(title = "L(X<sub>c</sub>)", range = c(-4, 8)),
      zaxis = list(title = "Col(X)<sup>&perp;</sup>", range = c(-4, 4)),
      aspectmode = "cube",
      camera = list(eye = list(x = 1.6, y = 1.6, z = 0.6))
    ),
    margin = list(l = 0, r = 0, b = 0, t = 30), 
    showlegend = FALSE
  )
  
  return(fig)
}

# --- 3. Output Logic Helper (HTML vs PDF) ---
render_3d <- function(fig, filename) {
  if (knitr::is_html_output()) {
    return(fig)
  } else {
    dir.create("figs", showWarnings = FALSE)
    img_path <- file.path("figs", filename)
    tmp_html <- tempfile(fileext = ".html")
    
    htmlwidgets::saveWidget(as_widget(fig), tmp_html)
    
    # Capture with webshot2
    # Increased vwidth/vheight for higher resolution
    webshot2::webshot(
      url = tmp_html, 
      file = img_path, 
      delay = 2,
      vwidth = 1500, 
      vheight = 1000, 
      cliprect = "viewport"
    )
    
    if (file.exists(tmp_html)) unlink(tmp_html)
    
    if (file.exists(img_path)) {
      knitr::include_graphics(img_path)
    } else {
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, "Error: Screenshot failed.")
    }
  }
}
```

::: {.panel-tabset}

#### Effect Exists (signal){.unnumbered}

```{r}
#| echo: false
#| message: false
#| warning: false
#| out-width: "100%"
#| fig-cap: "Scenario 1: Significant regression effect ($\\beta_1 \not= 0$). The mean vector projects significantly onto the predictor space."

data_A <- get_data(beta_effect = TRUE)
fig_A  <- create_3d_plot(data_A, "Scenario A: Effect Exists")
render_3d(fig_A, "geometry-3d-signal.png")
```

#### No Effect (noise){.unnumbered}

```{r}
#| echo: false
#| message: false
#| warning: false
#| out-width: "100%"
#| fig-cap: "Scenario 2: No regression effect ($\\beta_1 = 0$). The mean vector lies purely on the intercept axis."

data_B <- get_data(beta_effect = FALSE)
fig_B  <- create_3d_plot(data_B, "Scenario B: No Effect")
render_3d(fig_B, "geometry-3d-noise.png")
```

:::

### A Diagram to Show Decomposition of Sum of Squares

The decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).

```{tikz}
%| label: fig-ss-decomposition-legend-v2
%| fig-cap: "Geometric Decomposition: SST = SSR + SSE"
%| fig-align: "center"
%| echo: false

\begin{tikzpicture}[scale=1.5, >=latex, font=\small]

  % --- Coordinates ---
  % Shifted Mean from x=2 to x=3 (Longer by 1/2)
  % Shifted Fitted/Observed by same amount (+1) to preserve triangle shape
  \coordinate (Origin) at (0,0);
  \coordinate (Mean) at (3,0);      % \bar{y}j_n (Length 3)
  \coordinate (Fitted) at (7,1.5);    % \hat{y}
  \coordinate (Observed) at (7,4.5);  % y

  % --- 1. Subspaces ---
  
  % L(j_n) Axis (Horizontal)
  \draw[->, thick, gray] (-0.5,0) -- (8.5,0) node[right, black] {$L(j_n)$};
  
  % L(X_c) Axis (Regressors)
  % Slope = 1.5 / (7-3) = 0.375. Extend to x=8 => y=1.875
  \draw[->, thick, gray] (Mean) -- (8, 1.875) node[right, black] {$L(X_c)$};

  % --- 2. Vectors ---
  
  % Mean Vector (Blue)
  \draw[thick, blue, ->] (Origin) -- (Mean) node[midway, below] {$\bar{y}j_n$};
  
  % Fitted Vector (Blue Dashed)
  \draw[dashed, blue] (Origin) -- (Fitted);
  \node[below right, blue] at (Fitted) {$\hat{y}$};

  % Observed Vector (Black)
  \draw[thick, black, ->] (Origin) -- (Observed) node[above left] {$y$};

  % --- 3. Decomposition Components ---
  
  % SSR (Green)
  \draw[ultra thick, green!60!black, ->] (Mean) -- (Fitted);
    
  % SSE (Red)
  \draw[ultra thick, red, ->] (Fitted) -- (Observed);

  % SST (Orange)
  \draw[dashed, ultra thick, orange] (Mean) -- (Observed);

  % --- 4. Perpendicular Signs ---
  
  % A. At Fitted (Orthogonality of Residuals)
  % Rotated to align with the sloped line L(Xc)
  % The angle of L(Xc) is arctan(1.5/4) approx 20.5 degrees.
  \begin{scope}[shift={(Fitted)}, rotate=20.5]
     \draw[black] (-0.25,0) -- (-0.25, 0.25) -- (0, 0.25);
  \end{scope}

  % B. At Mean (Orthogonality of Centered Regression vs Mean)
  % Angle between horizontal and L(Xc)
  \draw[black] (Mean) ++(-0.25,0) -- ++(0, 0.25) -- ++(0.2, 0); 
  % (Note: Visual approximation for the concept)

  % --- 5. Legend ---
  \node[draw, fill=white, align=left, anchor=north west] at (0.2, 4.8) {
    \textbf{Decomposition:}\\
    % Using tikz inline to draw heavy lines
    \tikz\draw[orange, line width=2.5pt] (0,0) -- (0.6,0); \textbf{ SST}: Total \\ 
    \quad $\small \|y - \bar{y}j_n\|^2$ \\[0.5em]
    
    \tikz\draw[green!60!black, line width=2.5pt] (0,0) -- (0.6,0); \textbf{ SSR}: Regression \\ 
    \quad $\small \|\hat{y} - \bar{y}j_n\|^2$ \\[0.5em]
    
    \tikz\draw[red, line width=2.5pt] (0,0) -- (0.6,0); \textbf{ SSE}: Error \\ 
    \quad $\small \|y - \hat{y}\|^2$
  };

\end{tikzpicture}
```


### Distribution of Sum of Squares

We apply the general theory of projections to the specific components defined in @def-ss-components.

::: {#thm-distribution-ss-v2 name="Distribution of Sum of Squares"}
Let $y \sim N(\mu, \sigma^2 I_n)$, where $\mu \in \text{Col}(X)$.
Consider the decomposition defined by the projection matrices $P_{X_c}$ and $M = I - H$.

* **Independence:** The quadratic forms $\text{SSR}$ and $\text{SSE}$ are statistically independent because the subspaces $L(X_c)$ and $\text{Col}(X)^\perp$ are orthogonal.

* **Distribution of SSE:** The scaled sum of squared errors follows a central Chi-squared distribution:
  $$ \frac{\text{SSE}}{\sigma^2} = \frac{\|(I - H)y\|^2}{\sigma^2} \sim \chi^2(n-k-1) $$
  **Mean:**
  $$ E[\text{SSE}] = \sigma^2(n-k-1) $$

* **Distribution of SSR:** The scaled regression sum of squares follows a **non-central** Chi-squared distribution:
  $$ \frac{\text{SSR}}{\sigma^2} = \frac{\|P_{X_c}y\|^2}{\sigma^2} \sim \chi^2(k, \lambda) $$
  **Mean:**
  $$ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 $$

**Non-centrality Parameter ($\lambda$):**
$$ \lambda = \frac{1}{2\sigma^2} \|P_{X_c} \mu\|^2 $$
where $$\|P_{X_c} \mu\|^2 = \|X_c \beta_1\|^2 = (X_c \beta_1)' (X_c \beta_1) = \beta_1' X_c' X_c \beta_1$$
:::

::: proof
We apply @thm-proj-dist to the specific projection matrices identified in the definitions.

* **For SSE (Error Space):**
  $\text{SSE}$ is defined by the projection matrix $P_V = I - H$.

  - **Dimension:** The rank of $(I - H)$ is $n - \text{rank}(X) = n - (k+1) = n - k - 1$.
  - **Non-centrality:** Since $\mu \in \text{Col}(X)$, the projection onto the orthogonal complement is zero: $\|(I - H)\mu\|^2 = 0$. Thus, $\lambda = 0$.
  - **Expectation:** Using Part 2 of @thm-proj-dist ($E(\|P_V y\|^2) = \sigma^2 \text{rank}(P_V) + \|P_V \mu\|^2$):
    $$ E[\text{SSE}] = \sigma^2(n-k-1) + 0 = \sigma^2(n-k-1) $$

* **For SSR (Regression Space):**
  $\text{SSR}$ is defined by the projection matrix $P_V = P_{X_c}$.

  - **Dimension:** The rank of $P_{X_c}$ is $(k+1) - 1 = k$.
  - **Non-centrality:** The projection of $\mu$ onto $L(X_c)$ is $P_{X_c}\mu$.
    $$ \lambda = \frac{1}{2\sigma^2} \|P_{X_c} \mu\|^2 $$

  - **Expectation:** Using Part 2 of @thm-proj-dist:
    $$ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 $$

  This shows that while $E[\text{SSE}]$ depends only on the noise variance and sample size, $E[\text{SSR}]$ is inflated by the magnitude of the true regression signal $\|P_{X_c}\mu\|^2$.
:::


## F-test for Testing Overall Regression Effect

We wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.

**Hypotheses:**

* **Null Hypothesis ($H_0$):** $\beta_1 = \beta_2 = \dots = \beta_k = 0$ (No regression effect).
    This implies $\mu \in \text{span}(j_n)$ and the true signal variance $\|X_c\beta_1\|^2 = 0$.

* **Alternative Hypothesis ($H_1$):** At least one $\beta_j \neq 0$.

### The F-statistic {.unnumbered}

We construct the test statistic using the ratio of the Mean Squares defined previously:

$$F = \frac{\text{MSR}}{\text{MSE}} = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}$$

### Understanding $F$ via Expectations {.unnumbered}

The logic of the F-test is transparent when we examine the expected values of the numerator and denominator:

$$
\begin{aligned}
E[\text{MSE}] &= \sigma^2 \\
E[\text{MSR}] &= \sigma^2 + \frac{\|X_c \beta_1\|^2}{k}
\end{aligned}
$$

* **If $H_0$ is true:** The signal term is zero. Both Mean Squares estimate $\sigma^2$ unbiasedly. We expect $F \approx 1$.
* **If $H_1$ is true:** The numerator includes the positive term $\frac{\|X_c \beta_1\|^2}{k}$. We expect $F > 1$.

Therefore, we reject $H_0$ for sufficiently large values of $F$. Specifically, we reject at level $\alpha$ if $F_{obs} > F_{\alpha}(k, n-k-1)$.

### Distributional Theory

To derive the exact sampling distribution, we rely on the independence of the sums of squares (from @thm-distribution-ss-v2) and the definition of the non-central F-distribution given in **@def-noncentral-f**.

::: {#thm-regression-f-dist name="Distribution of Regression F-Statistic"}
Under the assumption of normality, the regression F-statistic follows a **non-central F-distribution**:

$$ F \sim F(k, n-k-1, \lambda) $$

The non-centrality parameter $\lambda$ is determined by the ratio of the signal sum of squares to the error variance:
$$ \lambda = \frac{\|X_c \beta_1\|^2}{2\sigma^2} $$

**Special Cases:**

1.  **Under $H_1$ (Signal exists):** $\lambda > 0$, so $F$ follows the non-central distribution.
2.  **Under $H_0$ (No signal):** $\beta_1 = 0 \implies \lambda = 0$. The distribution collapses to the **central F-distribution**:
    $$ F \sim F(k, n-k-1) $$
:::

::: proof
We identify the components from @def-noncentral-f:

1.  **Numerator ($X_1$):** Let $X_1 = \text{SSR}/\sigma^2$. From @thm-distribution-ss-v2, $X_1 \sim \chi^2(k, 2\lambda)$.
2.  **Denominator ($X_2$):** Let $X_2 = \text{SSE}/\sigma^2$. From @thm-distribution-ss-v2, $X_2 \sim \chi^2(n-k-1)$.
3.  **Independence:** $X_1$ and $X_2$ are independent.

Substituting these into the F-statistic:
$$
F = \frac{\text{MSR}}{\text{MSE}} = \frac{(\text{SSR}/\sigma^2)/k}{(\text{SSE}/\sigma^2)/(n-k-1)} = \frac{X_1/k}{X_2/(n-k-1)}
$$
By definition @def-noncentral-f, this ratio follows $F(k, n-k-1, \lambda)$.
:::

### Visualization of the Rejection Region

The following plot illustrates the central F-distribution (valid under $H_0$) for $k=3$ predictors and $n=20$ observations ($df_1 = 3, df_2 = 16$). An observed statistic of $F=2$ is marked, with the p-value represented by the shaded tail area.

```{r}
#| label: fig-f-dist-example
#| fig-cap: "Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value."
#| echo: false
#| warning: false

library(ggplot2)
library(latex2exp)

# Parameters
n <- 20
k <- 3
df1 <- k
df2 <- n - k - 1
f_obs <- 2

# Create Data for Plotting
x_vals <- seq(0, 6, length.out = 500)
df <- data.frame(
  x = x_vals,
  y = df(x_vals, df1, df2)
)

# Create Plot
ggplot(df, aes(x, y)) +
  # Draw the main density line
  geom_line(size = 1) +
  
  # Shade the p-value area (x > f_obs)
  geom_area(data = subset(df, x >= f_obs), aes(x = x, y = y), 
            fill = "firebrick", alpha = 0.6) +
  
  # Add vertical line at F_obs
  geom_vline(xintercept = f_obs, linetype = "dashed", color = "black") +
  
  # Annotations with LaTeX
  annotate("text", x = f_obs + 0.2, y = max(df$y)*0.6, 
           label = TeX(paste0("$F_{obs} = ", f_obs, "$")), 
           hjust = 0, size = 5) +
  
  annotate("text", x = f_obs + 1, y = 0.05, 
           label = TeX(paste0("$P(F > ", f_obs, ")$")), 
           color = "firebrick", size = 5, fontface="bold") +
  
  # Labels and Theme
  labs(x = "F statistic", y = "Density", 
       title = TeX(paste0("Central F-Distribution ($H_0$) with $df_1 = ", df1, "$ and $df_2 = ", df2, "$"))) +
  theme_minimal(base_size = 14) +
  scale_x_continuous(breaks = seq(0, 6, 1))
```


## Coefficient of Determination ($R^2$)

### Definition

The $R^2$ statistic measures the proportion of total variation explained by the regression model.

::: {#def-r2 name="R-Squared"}
$$R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}$$
Since $0 \le \text{SSE} \le \text{SST}$, it follows that $0 \le R^2 \le 1$.
:::


### Expectation and Bias

To understand the bias in $R^2$, it is more illuminating to analyze the expectation of the **unexplained variance** ($1 - R^2$). This term represents the ratio of error sum of squares to the total sum of squares:

$$ E[1 - R^2] = E\left[ \frac{\text{SSE}}{\text{SST}} \right] $$

Using the first-order approximation $E[X/Y] \approx E[X]/E[Y]$, we examine the numerator and denominator separately:

$$
\begin{aligned}
E[\text{SSE}] &= \sigma^2(n-k-1) \\
E[\text{SST}] &= \sigma^2(n-1) + 2\sigma^2\lambda = \sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)
\end{aligned}
$$

Substituting these back, we approximate the expected unexplained fraction:

$$ E[1 - R^2] \approx \frac{\sigma^2(n-k-1)}{\sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)} = \frac{n-k-1}{(n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2}} $$

**Behavior under Null Hypothesis ($H_0$):**
When there is no true signal ($\beta_1 = 0$), the term $\frac{\|X_c \beta_1\|^2}{\sigma^2}$ vanishes. The expected proportion of unexplained variance becomes:

$$ E[1 - R^2 | H_0] \approx \frac{n-k-1}{n-1} $$

This result reveals the source of the bias:

1.  Ideally, if predictors are noise, the model should explain nothing, and $E[1-R^2]$ should be $1$.
2.  Instead, the expected error ratio is **less than 1**, specifically scaled by $\frac{n-k-1}{n-1}$.
3.  This scaling factor is exactly what the **Adjusted R-squared ($R^2_a$)** attempts to correct by multiplying the observed ratio by the inverse $\frac{n-1}{n-k-1}$.

### Exact Distribution

The $R^2$ statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.

::: {#thm-r2-dist name="Distribution of R-Squared"}
$$ R^2 \sim \text{Beta}_1\left( \frac{k}{2}, \frac{n-k-1}{2}, \lambda \right) $$
where $\text{df}_1 = k$ and $\text{df}_2 = n-k-1$.
:::


### Adjusted R-squared ($R^2_a$)

To correct for the inflation of $R^2$ due to model complexity ($k$), we introduce the Adjusted $R^2$. This statistic penalizes the sum of squares by their degrees of freedom:

$$ R^2_a = 1 - \frac{\text{SSE}/(n-k-1)}{\text{SST}/(n-1)} = 1 - \frac{\text{MSE}}{\text{MST}} = 1 - (1 - R^2) \frac{n-1}{n-k-1} $$

**Expectation:**

Under $H_0$, since $E[\text{MSE}] = E[\text{MST}] = \sigma^2$, the estimator is asymptotically unbiased:

$$ E[R^2_a | H_0] \approx 0 $$

**Variance and Stability:**

While $R^2_a$ corrects the bias, it introduces instability. The variance of $R^2_a$ under $H_0$ can be derived from the variance of the Beta distribution:

$$ \text{Var}(R^2_a | H_0) = \left( \frac{n-1}{n-k-1} \right)^2 \text{Var}(R^2 | H_0) $$

Substituting $\text{Var}(R^2 | H_0) = \frac{2k(n-k-1)}{(n-1)^2(n+1)}$, we obtain:

$$ \text{Var}(R^2_a | H_0) = \frac{2k}{(n-k-1)(n+1)} $$

**Key Insight:**

As the model complexity $k$ increases relative to $n$:

1.  The denominator $(n-k-1)$ shrinks.
2.  The variance $\text{Var}(R^2_a)$ explodes.

This implies that for high-dimensional models (large $k/n$), $R^2_a$ becomes an extremely noisy estimator, often yielding large negative values even for null models.

### Relationship with Rao-Blackwell Decomposition of Variances

The formula for the expected Adjusted $R^2$ reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation $Y_i$ into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let $\sigma^2_\mu$ denote the signal variance and $\sigma^2$ denote the noise variance:

$$ \text{Var}(Y_i) = E[\text{Var}(Y_i|x_{(i)})] + \text{Var}(E[Y_i|x_{(i)}]) $$
$$ \sigma^2_Y = \sigma^2 + \sigma^2_\mu $$

In our derived expectation for $R^2_a$:
$$ E[R^2_a] \approx \frac{\frac{\|X_c\beta_1\|^2}{n-1}}{\sigma^2 + \frac{\|X_c\beta_1\|^2}{n-1}} $$

The term in the numerator, $\frac{\|X_c\beta_1\|^2}{n-1}$, is precisely the **sample variance of the true means** $\mu_i$.
Let $\mu = X\beta$. We can expand the centered signal vector $X_c\beta_1$ to see this explicitly. Since $\mu \in \text{Col}(X)$, we know $H\mu = \mu$:

$$
X_c\beta_1 = P_{X_c} \mu = (H - P_{j_n})\mu = H\mu - P_{j_n}\mu = \mu - \bar{\mu}j_n = 
\begin{pmatrix} 
\mu_1 - \bar{\mu} \\ 
\mu_2 - \bar{\mu} \\ 
\vdots \\ 
\mu_n - \bar{\mu} 
\end{pmatrix}
$$

This vector represents the deviation of each observation's true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is:
$$ \frac{\|X_c\beta_1\|^2}{n-1} = \frac{\sum_{i=1}^n (\mu_i - \bar{\mu})^2}{n-1} = \widehat{\sigma}^2_\mu $$

If we view the rows of the design matrix $X$ as random draws $x_{(1)}, \dots, x_{(n)}$ from a population of covariate vectors, this term estimates $\text{Var}(x_{(i)}'\beta)$, which is the variance of the signal component $\sigma^2_\mu$.

Thus, $R^2_a$ can be interpreted as a method-of-moments estimator for the **proportion of variance explained by the signal** in the population:
$$ E[R^2_a] \approx \frac{\sigma^2_\mu}{\sigma^2 + \sigma^2_\mu} = \frac{\text{Var}(E[Y_i|x_{(i)}])}{\text{Var}(Y_i)} $$

::: {.callout-important title="MSR Is Not a Variance Estimator"}

* Observing that $E[\text{MST}] \approx \sigma^2 + \sigma^2_\mu$ and $E[\text{MSE}] = \sigma^2$, we can see that the difference $\text{MST} - \text{MSE}$ provides a direct method-of-moments estimator for the variance of the signal itself ($\sigma^2_\mu$). 

* It is important to recognize that the commonly used **Mean Square Regression (MSR)**, defined as $\text{SSR}/k$, is **not** an estimator of the signal variance. Because $E[\text{MSR}] = \sigma^2 + \frac{\|X_c\beta_1\|^2}{k}$, it scales with the sample size $n$ (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting *existence* of signal), not for estimating the *magnitude* of the signal variance.
:::

## Relationship between $R^2$ and $F$ Test

The proportion of *unexplained* variance, $1 - R^2$, follows the Type II Non-central Beta distribution.

::: {#thm-r2-complement name="Distribution of Unexplained Variance"}
$$1 - R^2 = \frac{\text{SSE}}{\text{SST}} \sim \text{Beta}_2\left( \frac{n-k-1}{2}, \frac{k}{2}, \lambda \right)$$
:::

Both the standard and adjusted coefficients of determination are monotonic functions of the $F$-statistic ($F = \text{MSR}/\text{MSE}$). We can derive these relationships directly from the definition of $F$:

$$F = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)} \implies \frac{\text{SSR}}{\text{SSE}} = \frac{k}{n-k-1} F$$

1.  **Standard $R^2$:**

    Recall that $R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{\text{SSR}}{\text{SSR} + \text{SSE}}$. Dividing the numerator and denominator by $\text{SSE}$:

    $$R^2 = \frac{\text{SSR}/\text{SSE}}{(\text{SSR}/\text{SSE}) + 1} = \frac{\frac{k}{n-k-1} F}{1 + \frac{k}{n-k-1} F}$$

2.  **Adjusted $R^2_a$:**

    Start with the definition involving the unexplained variance scaling:

    $$R^2_a = 1 - (1 - R^2) \frac{n-1}{n-k-1}$$

    From the derivation above, note that the unexplained variance is:
    $$1 - R^2 = \frac{1}{1 + \frac{k}{n-k-1}F} = \frac{n-k-1}{(n-k-1) + kF}$$

    Substituting this into the adjusted formula simplifies the expression:

    $$R^2_a = 1 - \left( \frac{n-k-1}{(n-k-1) + kF} \right) \frac{n-1}{n-k-1} = 1 - \frac{n-1}{(n-k-1) + kF}$$


### Confidence Interval of Population $R^2$

Since we have identified that $R^2_a$ estimates the population proportion of variance, let us define this target parameter formally as $\rho^2$:

$$ \rho^2 = 1 - \frac{\sigma^2}{\sigma^2_Y} = \frac{\sigma^2_\mu}{\sigma^2_Y} $$

While $R^2_a$ provides a point estimate, we can construct an exact confidence interval for $\rho^2$ by exploiting the distribution of the $F$-statistic.

**1. The Link to Non-centrality:**

Recall that the $F$-statistic follows a non-central distribution $F(k, n-k-1, \lambda)$. The non-centrality parameter $\lambda$ is directly related to the population $\rho^2$. Using the variance decomposition derived above:

$$ \lambda = \frac{\|X_c \beta_1\|^2}{2\sigma^2} = \frac{n-1}{2} \left( \frac{\sigma^2_\mu}{\sigma^2} \right) $$

Substituting the signal-to-noise ratio $\frac{\sigma^2_\mu}{\sigma^2} = \frac{\rho^2}{1-\rho^2}$, we obtain a one-to-one mapping between $\lambda$ and $\rho^2$:

$$ \lambda(\rho^2) = \frac{n-1}{2} \left( \frac{\rho^2}{1-\rho^2} \right) $$

**2. Inverting the Test Statistic:**

We find a confidence interval $[\lambda_L, \lambda_U]$ for $\lambda$ by "inverting" the observed $F$-statistic ($F_{obs}$). We search for two specific non-central F-distributions: one where $F_{obs}$ cuts off the upper $\alpha/2$ tail, and one where it cuts off the lower $\alpha/2$ tail.

* **Lower Bound ($\lambda_L$):** The non-centrality parameter such that $F_{obs}$ is the $1-\alpha/2$ quantile.
* **Upper Bound ($\lambda_U$):** The non-centrality parameter such that $F_{obs}$ is the $\alpha/2$ quantile.

This concept is illustrated in the figure below.

```{r}
#| label: fig-ci-inversion
#| fig-cap: "Illustration of constructing a confidence interval for the non-centrality parameter $\\lambda$ by inverting the F-test. The observed $F_{obs}$ (dashed line) is the $97.5^{th}$ percentile of the distribution defined by the lower bound $\\lambda_L$ (blue), and the $2.5^{th}$ percentile of the distribution defined by the upper bound $\\lambda_U$ (red). The shaded areas each represent $\\alpha/2$."
#| echo: false
#| warning: false
#| message: false

library(ggplot2)
library(latex2exp)

# 1. Setup Parameters for illustration
n <- 30; k <- 3
df1 <- k; df2 <- n - k - 1
F_obs <- 5 
alpha <- 0.05

# 2. Use values that approximately correspond to the parameters for illustration
lambda_L_approx <- 2.5 
lambda_U_approx <- 28.1

# 3. Create Data for Plotting
x_seq <- seq(0, 18, length.out = 1000)

# Density curves
dens_L <- df(x_seq, df1, df2, ncp = lambda_L_approx)
dens_U <- df(x_seq, df1, df2, ncp = lambda_U_approx)

# Shading data
shade_L_data <- data.frame(x = x_seq[x_seq >= F_obs], y = dens_L[x_seq >= F_obs])
shade_L_data <- rbind(c(F_obs, 0), shade_L_data, c(max(x_seq), 0))

shade_U_data <- data.frame(x = x_seq[x_seq <= F_obs], y = dens_U[x_seq <= F_obs])
shade_U_data <- rbind(c(min(x_seq), 0), shade_U_data, c(F_obs, 0))

# Combine densities for ggplot
plot_data <- data.frame(
  x = c(x_seq, x_seq),
  y = c(dens_L, dens_U),
  Distribution = rep(c("Lower Bound Dist", "Upper Bound Dist"), each = length(x_seq))
)

# 4. Plot
ggplot() +
  # Densities
  geom_line(data = plot_data, aes(x = x, y = y, color = Distribution), size = 1) +
  
  # Shaded Areas representing alpha/2
  geom_polygon(data = shade_L_data, aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_polygon(data = shade_U_data, aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Vertical line for F_obs
  geom_vline(xintercept = F_obs, linetype = "dashed", size = 1) +
  
  # Annotations
  annotate("text", x = F_obs, y = max(plot_data$y)*1.05, label = TeX("$F_{obs}$"), vjust = 0, size = 5) +
  annotate("text", x = F_obs + 2, y = 0.02, label = TeX("$\\alpha/2$"), color = "blue") +
  annotate("text", x = F_obs - 2, y = 0.02, label = TeX("$\\alpha/2$"), color = "red") +
  
  # Styling
  scale_color_manual(values = c("blue", "red"), 
                     labels = c(TeX("Lower Bound Dist (using $\\lambda_L$)"), 
                                TeX("Upper Bound Dist (using $\\lambda_U$)"))) +
  labs(x = TeX("$F$-statistic value"), y = TeX("Density"), 
       title = TeX("Constructing Confidence Intervals by Inverting the $F$-test")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0, max(plot_data$y)*1.1), xlim=c(0, 16))
```

**3. The Interval for $\rho^2$:**

Once $[\lambda_L, \lambda_U]$ are found numerically, we map them back to the population $R^2$ scale using the inverse relationship:

$$ \rho^2 = \frac{2\lambda}{2\lambda + (n-1)} $$

This produces an exact confidence interval $[\rho^2_L, \rho^2_U]$ for the proportion of variance explained by the model in the population.


## An Animation for Illustrating $R^2_a$ Under $H_0$ and $H_1$

We simulate a dataset with $n=30$ observations and consider a sequence of nested models adding groups of predictors.

**Predictor Groups:**

1.  **Step 1 ($k=1$):** Add $x_1$. (Signal under $H_1$).
2.  **Step 2 ($k=6$):** Add $x_2, \dots, x_6$ (Noise).
3.  **Step 3 ($k=11$):** Add $x_7, \dots, x_{11}$ (Noise).
4.  **Step 4 ($k=20$):** Add $x_{12}, \dots, x_{20}$ (Noise).

```{r}
#| label: generate-all-animations
#| include: false
#| cache: true

# --- Setup ---
library(ggplot2)
library(patchwork)
require("latex2exp")
set.seed(123)

# Knobs
n_frames <- 30
fps      <- 2
n        <- 30
sigma    <- 1
beta1    <- 2 # Explicitly set to 2 as per text description

# [FLEXIBLE] Define your Steps Here.
model_steps <- c(0, 1, 6, 11, 20) 

# Automatically Determine the Maximum Number of Predictors Needed
max_k <- max(model_steps) 

# --- Generator Function ---
generate_animation <- function(scenario = "H0", filename) {
  
  # Ensure directory exists
  dir.create(dirname(filename), showWarnings = FALSE)
  
  # Temp directory for frames
  frame_dir <- file.path("frames_temp", scenario)
  dir.create(frame_dir, recursive = TRUE, showWarnings = FALSE)
  
  # --- 1. Theoretical Limits Setup ---
  
  # A. Y-axis Lower Limit for R^2_adj
  max_p <- max_k + 1
  if (max_p >= n) {
    y_min_limit <- -1 
  } else {
    y_min_limit <- -0.2
  }

  # B. Theoretical Expectations
  var_x <- 1 
  
  # Calculate Total Variance (Var(Y))
  # H0: beta=0 -> Total Var = sigma^2
  # H1: beta=2 -> Total Var = sigma^2 + beta^2*var(x)
  total_var <- if(scenario == "H1") (sigma^2 + beta1^2 * var_x) else sigma^2
  
  # Calculate Expected Residual Variance (Sigma^2_k)
  # k=0: Model has no predictors, so Residual Var = Total Var
  # k>=1: Model includes x1. 
  #       In H1, x1 explains signal, so Residual Var drops to sigma^2.
  #       In H0, x1 is noise, so Residual Var is still sigma^2 (which is also Total Var).
  get_expected_residual_var <- function(k) {
    if (k == 0) return(total_var)
    return(sigma^2)
  }
  
  theory_df <- data.frame(k = model_steps)
  theory_df$expected_mse <- sapply(theory_df$k, get_expected_residual_var)
  
  # Calculate Theoretical R^2 = 1 - (Residual Var / Total Var)
  theory_df$expected_r2 <- 1 - (theory_df$expected_mse / total_var)

  # --- Frame Loop ---
  for (j in 1:n_frames) {
    
    # 2. Generate Data
    X <- as.data.frame(replicate(max_k, rnorm(n)))
    names(X) <- paste0("x", 1:max_k)
    
    beta_vec <- rep(0, max_k)
    if (scenario == "H1") beta_vec[1] <- beta1 
    
    y <- as.numeric(as.matrix(X) %*% beta_vec + rnorm(n, sd = sigma))
    
    # 3. Fit Sequence of Models
    stats_list <- lapply(model_steps, function(k) {
      if (k == 0) {
        fit <- lm(y ~ 1)
      } else {
        k_safe <- min(k, ncol(X))
        form <- as.formula(paste("y ~", paste(names(X)[1:k_safe], collapse = " + ")))
        fit <- lm(form, data = X)
      }
      
      rss <- sum(residuals(fit)^2)
      p   <- k + 1 
      
      denom_mse <- if(n - p > 0) (n - p) else NA
      
      data.frame(
        k = k,
        p = p,
        MSE = if(!is.na(denom_mse)) rss / denom_mse else NA,
        MLE = rss / n,
        R2 = summary(fit)$r.squared,
        R2_adj = summary(fit)$adj.r.squared
      )
    })
    
    df <- do.call(rbind, stats_list)
    
    # 4. Plotting
    
    # Plot A: Variance Estimators
    g1 <- ggplot(df, aes(x = k)) +
      geom_step(data = theory_df, aes(y = expected_mse, linetype = "Theoretical"), 
                color = "gray40", size = 0.8, direction = "vh") +
      
      geom_line(aes(y = MLE, color = "Naive (MLE)"), size = 1, linetype = "solid") +
      geom_point(aes(y = MLE, color = "Naive (MLE)"), size = 2) +
      
      geom_line(aes(y = MSE, color = "Corrected (MSE)"), size = 1, linetype = "dashed") +
      geom_point(aes(y = MSE, color = "Corrected (MSE)"), size = 2, shape = 17) +
      
      scale_color_manual(values = c("Naive (MLE)" = "firebrick", "Corrected (MSE)" = "blue")) +
      scale_linetype_manual(values = c("Theoretical" = "dotted"), name=NULL) +
      
      scale_x_continuous(breaks = model_steps) +
      coord_cartesian(ylim = c(0, max(theory_df$expected_mse) + 1)) +
      labs(title = paste0("Scenario ", scenario, ": Estimators of Error Variance"),
           y = "Variance Estimate", x = NULL, color = NULL) +
      theme_minimal() + theme(legend.position = "top")

    # Plot B: Model Fit
    g2 <- ggplot(df, aes(x = k)) +
      geom_hline(yintercept = 0, color = "black", size = 0.2) +
      
      # Theoretical R2 Line
      geom_step(data = theory_df, aes(y = expected_r2, linetype = "Theoretical"), 
                color = "gray40", size = 0.8, direction = "vh") +
      
      geom_line(aes(y = R2, color = "Naive (R^2)"), size = 1, linetype = "solid") +
      geom_point(aes(y = R2, color = "Naive (R^2)"), size = 2) +
      
      geom_line(aes(y = R2_adj, color = "Corrected (Adj R^2)"), size = 1, linetype = "dashed") +
      geom_point(aes(y = R2_adj, color = "Corrected (Adj R^2)"), size = 2, shape = 17) +
      
      scale_color_manual(values = c("Naive (R^2)" = "firebrick", "Corrected (Adj R^2)" = "blue")) +
      # Re-use linetype scale to match top plot style
      scale_linetype_manual(values = c("Theoretical" = "dotted"), name=NULL) +
      
      scale_x_continuous(breaks = model_steps) +
      scale_y_continuous(limits = c(y_min_limit, 1), labels = scales::percent) +
      labs(title = "Estimate of R-squared",
           y = "Value", x = "Number of Predictors (k)", color = NULL) +
      theme_minimal() + 
      theme(
        legend.position = "top",
        plot.margin = margin(t = 5, r = 5, b = 40, l = 5, unit = "pt")
      )

    final_plot <- g1 / g2
    
    ggsave(file.path(frame_dir, sprintf("f%03d.png", j)), final_plot, width = 7, height = 7, dpi = 100)
    
    if (j == 1) {
      static_name <- sub("\\.mp4$", ".png", filename)
      ggsave(static_name, final_plot, width = 7, height = 7, dpi = 100)
    }
  }
  
  if (requireNamespace("av", quietly = TRUE)) {
    pngs <- list.files(frame_dir, "\\.png$", full.names = TRUE)
    av::av_encode_video(pngs, filename, framerate = fps, verbose = FALSE)
  }
}

# --- Execution ---
generate_animation("H0", "figs/rss-h0-v6.mp4")
generate_animation("H1", "figs/rss-h1-v6.mp4")
```

::: {.panel-tabset}

#### Null Hypothesis ($H_0$)

Under $H_0$, the true coefficient for $x_1$ is $\beta_1 = 0$. All predictors are noise.

```{r}
#| echo: false
#| fig.align: center
#| fig.cap: "Simulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable."

if (knitr::is_html_output()) {
  htmltools::tags$video(
    controls = "controls", width = "100%", 
    htmltools::tags$source(src = "figs/rss-h0-v6.mp4", type = "video/mp4")
  )
} else {
  knitr::include_graphics("figs/rss-h0-v6.png")
}
```

#### Alternative Hypothesis ($H_1$)

Under $H_1$, $x_1$ is a true predictor ($\beta_1 = 2$). The subsequent groups ($x_2 \dots x_{20}$) remain noise.

```{r}
#| echo: false
#| fig.align: center
#| fig.cap: "Simulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors."

if (knitr::is_html_output()) {
  htmltools::tags$video(
    controls = "controls", width = "100%", 
    htmltools::tags$source(src = "figs/rss-h1-v6.mp4", type = "video/mp4")
  )
} else {
  knitr::include_graphics("figs/rss-h1-v6.png")
}
```

:::



## A Data Example with House Price Valuation

A real estate agency wants to refine their pricing model. They regress the selling price of houses ($y$) on five predictors ($X$): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.

We assume the data has been collected and saved to `house_prices_5pred.csv`.

```{r}
#| label: generate-housing-data-5p
#| echo: false

# Hidden Data Generation
set.seed(2024)
n <- 25
size <- round(runif(n, 1000, 3500), 0)      
age <- round(runif(n, 0, 60), 0)           
beds <- sample(2:5, n, replace = TRUE)     
garage <- sample(0:3, n, replace = TRUE)   
lawn <- round(runif(n, 50, 1000), 0) # Irrelevant predictor

# True Model (lawn Is NOT Included Here)
# Sigma Increased from 10,000 to 50,000
price <- round(30000 + 140 * size - 800 * age + 
               15000 * beds + 10000 * garage + rnorm(n, 0, 50000), 0)

housing_data <- data.frame(
  Price = price,
  Size = size,
  Age = age,
  Beds = beds,
  Garage = garage,
  Lawn = lawn
)

write.csv(housing_data, "house_prices_5pred.csv", row.names = FALSE)
```

### Visualize the Data

First, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.

```{r}
#| label: load-and-visualize
#| message: false

# Load Data
df <- read.csv("house_prices_5pred.csv")

# Conditional Display
if (knitr::is_html_output()) {
  rmarkdown::paged_table(df)
} else {
  knitr::kable(head(df, 10), caption = "First 10 rows of House Prices")
}
```

### Fit the Model

We will solve for the coefficients $\hat{\beta}$ using three distinct methods.

#### Method 1: Naive Matrix Formula

This method solves the normal equations directly on the raw data: $\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y$.

```{r}
#| label: method-1-naive

# 1. Define Y and X (add Column of 1s for Intercept)
y <- as.matrix(df$Price)
# Note: "lawn" Is Included Here, Even Though It Is Irrelevant
X_naive <- as.matrix(cbind(Intercept = 1, 
                           df[, c("Size", "Age", "Beds", "Garage", "Lawn")]))

# 2. Compute Intermediate Matrices
XtX <- t(X_naive) %*% X_naive
Xty <- t(X_naive) %*% y

# Display Intermediate Steps
cat("Matrix X'X (Cross-products of predictors):\n")
print(round(XtX, 0))

cat("\nMatrix X'y (Cross-products with response):\n")
print(round(Xty, 0))

# 3. Solve Beta
beta_naive <- solve(XtX) %*% Xty

# Display Result
cat("\nSolved Coefficients (Beta):\n")
print(t(beta_naive))
```

#### Method 2: Centralized Formula

This method reduces multicollinearity issues. Formula: $\hat{\beta}_{\text{slope}} = (X_c^{\prime}X_c)^{-1}X_c^{\prime}y_c$.

```{r}
#| label: method-2-centralized

# 1. Center the Data
y_bar <- mean(y)
X_raw <- as.matrix(df[, c("Size", "Age", "Beds", "Garage", "Lawn")])
X_means <- colMeans(X_raw)

y_c <- y - y_bar
X_c <- sweep(X_raw, 2, X_means) 

# 2. Compute Intermediate Matrices
XctXc <- t(X_c) %*% X_c
Xctyc <- t(X_c) %*% y_c

# Display Intermediate Steps
cat("Matrix X_c'X_c (Centered Sum of Squares):\n")
print(round(XctXc, 0))

cat("\nMatrix X_c'y_c (Centered Cross-products):\n")
print(round(Xctyc, 0))

# 3. Solve for Slope Coefficients
beta_slope <- solve(XctXc) %*% Xctyc

# 4. Recover Intercept
beta_0 <- y_bar - sum(X_means * beta_slope)
beta_central <- rbind(Intercept = beta_0, beta_slope)

# Display Result
cat("\nSolved Coefficients (Beta):\n")
print(t(beta_central))
```

#### Method 3: Using R's `lm` Function

This is the standard approach for practitioners.

```{r}
#| label: method-3-lm

# Fit Model
model_lm <- lm(Price ~ ., data = df)
y_hat_lm <- fitted(model_lm)

# Extract Coefficients
print(coef(model_lm))
```

### Analysis of Variance (ANOVA)

We now evaluate the sources of variation to test the overall model significance.

#### 1. Computing Sums of Squares

```{r}
#| label: ss-calc
#| code-fold: true

# Vectors
y_vec <- as.vector(y)
y_hat <- as.vector(y_hat_lm)
y_bar_vec <- rep(mean(y), length(y))

# Calculations
SST_naive <- sum((y_vec - y_bar_vec)^2)
SSR_naive <- sum((y_hat - y_bar_vec)^2)
SSE_naive <- sum((y_vec - y_hat)^2)

cat("SST:", SST_naive, " SSR:", SSR_naive, " SSE:", SSE_naive, "\n")
```

#### 2. Manual ANOVA Construction

We build the table manually using the sums of squares and degrees of freedom.

```{r}
#| label: anova-manual

# Parameters
k <- 5             # Predictors
df_e <- n - k - 1  # Error DF
df_t <- n - 1      # Total DF

# Mean Squares
MSR <- SSR_naive / k
MSE <- SSE_naive / df_e
MST <- SST_naive / df_t # Mean Square Total (Variance of Y)

# F-statistic
F_stat <- MSR / MSE

# P-value
p_val <- pf(F_stat, k, df_e, lower.tail = FALSE)

# Assemble Table
anova_manual <- data.frame(
  Source = c("Regression (Model)", "Error (Residual)", "Total"),
  DF = c(k, df_e, df_t),
  SS = c(SSR_naive, SSE_naive, SST_naive),
  MS = c(MSR, MSE, MST), # Included MST here
  F_Statistic = c(F_stat, NA, NA),
  P_Value = c(p_val, NA, NA)
)

knitr::kable(anova_manual, digits = 4, caption = "Manual ANOVA Table")
```

#### 3. Standard R Output (`summary` and `anova`)

We display the standard `summary()` which provides the coefficients, t-tests, and the overall F-statistic found at the bottom. We also show `anova()` which gives the sequential sum of squares.

```{r}
#| label: r-summary-output

# Summary shows Coefficients and Overall F-test
summary(model_lm)

# Anova shows sequential breakdown
print(anova(model_lm))
```

### Coefficient of Determination and Variance Decomposition

We calculate $R^2$ and Adjusted $R^2$, and then present them in a **Variance Decomposition Table**.

#### 1. Calculation

```{r}
#| label: calc-r2

# Standard R-squared
R2 <- 1 - (SSE_naive / SST_naive)

# Adjusted R-squared
# Formula: 1 - (MSE / MST)
R2_adj <- 1 - (MSE / MST)

cat("Standard R^2:  ", round(R2, 4), "\n")
cat("Adjusted R^2:  ", round(R2_adj, 4), "\n")
```

#### 2. Variance Decomposition Table

This table extends standard ANOVA. While ANOVA focuses on **Mean Squares (MS)** for hypothesis testing (is $MSR > MSE$?), this table focuses on **Variance Components ($\hat{\sigma}^2$)** for estimation (how much variance is Signal vs. Noise?).

* **Signal Variance ($\hat{\sigma}^2_\mu$):** Estimated by $MST - MSE$. (Note: $MSR$ is biased and overestimates signal).
* **Noise Variance ($\hat{\sigma}^2$):** Estimated by $MSE$.
* **Total Variance ($\hat{\sigma}^2_Y$):** Estimated by $MST$.

```{r}
#| label: variance-decomp-table

# Variance Component Estimators (Method of Moments)
sigma2_noise_est  <- MSE
sigma2_total_est  <- MST
sigma2_signal_est <- MST - MSE

# Proportions
prop_signal <- sigma2_signal_est / sigma2_total_est # Equals R^2_adj
prop_noise  <- sigma2_noise_est / sigma2_total_est  # Equals 1 - R^2_adj

# Assemble Table
decomp_table <- data.frame(
  Component = c("Signal (Model)", "Noise (Error)", "Total (Y)"),
  DF = c(k, df_e, df_t),
  SS = c(SSR_naive, SSE_naive, SST_naive),
  MS = c(MSR, MSE, MST),
  Estimator_Sigma2 = c(sigma2_signal_est, sigma2_noise_est, sigma2_total_est),
  Proportion = c(prop_signal, prop_noise, 1.0)
)

# Display
knitr::kable(decomp_table, 
             digits = 4, 
             col.names = c("Component", "DF", "SS", "MS", "Value ($\\hat{\\sigma}^2$)", "Proportion"),
             caption = "Variance Decomposition Table: Estimating Signal vs. Noise")
```

### Confidence Interval for Population $R^2$ ($\rho^2$)

We construct a 95% confidence interval for the population proportion of variance explained ($\rho^2$) by inverting the non-central F-distribution.

#### Manual Inversion Method

We solve for the non-centrality parameters $\lambda_L$ and $\lambda_U$ such that our observed $F_{obs}$ corresponds to the appropriate quantiles.

```{r}
#| label: ci-rho2-manual

# 1. Define Helper Function to Find Lambda
get_lambda <- function(target_prob, F_val, df1, df2) {
  f_root <- function(lam) {
    pf(F_val, df1, df2, ncp = lam) - target_prob
  }
  tryCatch({
    res <- uniroot(f_root, interval = c(0, 1000))$root
    return(res)
  }, error = function(e) return(NA))
}

# 2. Calculate Lambda Bounds (95% CI -> alpha = 0.05)
alpha <- 0.05
lambda_Lower <- get_lambda(1 - alpha/2, F_stat, k, df_e)
lambda_Upper <- get_lambda(alpha/2, F_stat, k, df_e)

if (is.na(lambda_Lower)) lambda_Lower <- 0

# 3. Convert Lambda to Rho^2
# Formula: rho^2 = 2*lambda / (2*lambda + n - 1)
rho2_Lower <- (2 * lambda_Lower) / (2 * lambda_Lower + n - 1)
rho2_Upper <- (2 * lambda_Upper) / (2 * lambda_Upper + n - 1)

cat("Observed R^2_adj:            ", round(R2_adj, 4), "\n")
cat("95% CI for Population Rho^2: [", round(rho2_Lower, 4), ", ", round(rho2_Upper, 4), "]\n")
```

#### Using R Package `MBESS`

The `MBESS` package automates this exact procedure. We use `Random.Predictors = FALSE` to assume fixed predictors, matching the non-central F approach derived above.

```{r}
#| label: ci-rho2-mbess
#| warning: false
#| message: false

# Check if MBESS is installed, otherwise skip

  
  ci_res1<- MBESS::ci.R2(F.value = F_stat, 
                         df.1 = k, 
                         df.2 = df_e, 
                         conf.level = 0.95,
                         Random.Predictors = FALSE)
 
  ci_res2 <- MBESS::ci.R2(F.value = F_stat, 
                         df.1 = k, 
                         df.2 = df_e, 
                         conf.level = 0.95,
                         Random.Predictors = TRUE)
 
  ci_res1
  ci_res2

```

## Overfitting

Suppose the reduced model $y = X_1\beta_1^* + e$ is true (i.e., $\beta_2 = 0$), but we fit the full model $(\dagger)$.
Since the full model includes the true model as a special case, the estimator $\hat{\beta}$ from the full model remains unbiased.

However, fitting the extraneous variables affects the variance.

::: {#thm-overfitting name="Variance Comparison"}
Let $\hat{\beta}_1$ be the estimator from the full model and $\hat{\beta}_1^*$ be the estimator from the reduced model. Then:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
where $A = (X_1^T X_1)^{-1}X_1^T X_2$ and $B = X_2^T X_2 - X_2^T X_1 A$.
Since $A B^{-1} A^T$ is positive semidefinite, $\text{Var}(\hat{\beta}_1) \ge \text{Var}(\hat{\beta}_1^*)$.
:::

::: {.proof}
Using the inverse of a partitioned matrix, the top-left block of $(X^T X)^{-1}$ corresponding to $\beta_1$ is:
$$ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} $$
Since $\text{Var}(\hat{\beta}_1) = \sigma^2 H^{11}$ and $\text{Var}(\hat{\beta}_1^*) = \sigma^2 (X_1^T X_1)^{-1}$, the difference is the second term:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
.
:::



### Summary

1.  **Underfitting:** Reduces variance but introduces bias (unless variables are orthogonal).
2.  **Overfitting:** Keeps estimators unbiased but increases variance.

