---
title: "Point Estimation in Multiple Linear Regression"
format: html
---

## Linear Models and Least Square Estimator

### Assumptions in Linear Models

Suppose that on a random sample of $n$ units (patients, animals, trees, etc.) we observe a response variable $Y$ and explanatory variables $X_{1},...,X_{k}$. Our data are then $(y_{i},x_{i1},...,x_{ik})$, $i=1,...,n$, or in vector/matrix form $y, x_{1},...,x_{k}$ where $y=(y_{1},...,y_{n})$ and $x_{j}=(x_{1j},...,x_{nj})^{T}$ or $y, X$ where $X=(x_{1},...,x_{k})$.

Either by design or by conditioning on their observed values, $x_{1},...,x_{k}$ are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:

::: {#def-assumptions name="Assumptions A1-A5"}
**A1. (Additive Error)**
$y=\mu+e$ where $e=(e_{1},...,e_{n})^{T}$ is an unobserved random vector with $E(e)=0$. This implies that $\mu=E(y)$ is the unknown mean of $y$.

**A2. (Linearity)**
$\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta$ where $\beta_{1},...,\beta_{k}$ are unknown parameters. This assumption says that $E(y)=\mu\in\text{Col}(X)$ (lies in the column space of $X$); i.e., it is a linear combination of explanatory vectors $x_{1},...,x_{k}$ with coefficients the unknown parameters in $\beta=(\beta_{1},...,\beta_{k})^{T}$. Note that it is linear in $\beta_{1},...,\beta_{k}$, not necessarily in the $x$'s.

**A3. (Independence)**
$e_{1},...,e_{n}$ are independent random variables (and therefore so are $y_{1},...,y_{n})$.

**A4. (Homoscedasticity)**
$e_{1},...,e_{n}$ all have the same variance $\sigma^{2}$; that is, $\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}$ which implies $\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}$.

**A5. (Normality)**
$e\sim N_{n}(0,\sigma^{2}I_{n})$.
:::

### Matrix Formulation

The model can be written algebraically as:
$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n$$

Or in matrix notation:
$$
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 & x_{11} & x_{12} & \cdot\cdot\cdot & x_{1k}\\
1 & x_{21} & x_{22} & \cdot\cdot\cdot & x_{2k}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
1 & x_{n1} & x_{n2} & \cdot\cdot\cdot & x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
$$

This is expressed compactly as:
$$y=X\beta+e$$
where $X$ is the design matrix, and $e \sim N_n(0, \sigma^2 I)$. Alternatively:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e$$

Taken together, all five assumptions can be stated more succinctly as:
$$y\sim N_{n}(X\beta,\sigma^{2}I)$$
with the mean vector $\mu_{y}=X\beta\in \text{Col}(X)$.

:::{.callout }
### A Note on Coefficients
The effect of a parameter depends upon what other explanatory variables are present in the model. For example, $\beta_{0}$ and $\beta_{1}$ in the model:
$$y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e$$
will typically be different than $\beta_{0}^{*}$ and $\beta_{1}^{*}$ in the model:
$$y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^{*}$$
In this context, $\beta_0^*$ and $\beta_1^*$ are the population-projected coefficients of the full model, that is,  $\beta_0^*$ and $\beta_1^*$ are the parameters that can best approximate the full model. 
:::

::: {.callout-important}
We will first consider the case that $\text{rank}(X)=k+1$.
:::

### Least Squares Estimator

::: {#def-least-squares name="Least Squares Estimator"}
The **Least Squares Estimator (LSE)** of $\beta$, denoted as $\hat{\beta}$, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses $y$ and the fitted values $X\hat{\beta}$.
$$
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
$$
Solving the normal equations $(X'X)\hat{\beta} = X'y$ yields the closed-form solution (assuming $X$ has full rank):
$$
\hat{\beta} = (X'X)^{-1}X'y
$$

:::

### Estimation Example

::: {#exm-7-3-1a name="Computation of $\hat \beta$"}
We use the data in @tbl-7-1 to illustrate the computation of $\hat{\beta}$ using the normal equations.

Given the data matrices, we compute the cross-product matrices:

$$
X^{\prime}X = \begin{pmatrix}
12 & 52 & 102\\
52 & 395 & 536\\
102 & 536 & 1004
\end{pmatrix}
$$

$$
X^{\prime}y = \begin{pmatrix}
90\\
482\\
872
\end{pmatrix}
$$

The inverse of the cross-product matrix is:
$$
(X^{\prime}X)^{-1} = \begin{pmatrix}
.97476 & .24290 & -.22871\\
.24290 & .16207 & -.11120\\
-.22871 & -.11120 & .08360
\end{pmatrix}
$$

Solving for the estimator:
$$
\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y = \begin{pmatrix}
5.3754\\
3.0118\\
-1.2855
\end{pmatrix}
$$
:::

### Properties of the Estimator

::: {#thm-unbiased name="Unbiasedness of $\hat \beta$"}
If $E(y)=X\beta$, then $\hat{\beta}$ is an unbiased estimator for $\beta$.
:::

::: {.proof}
$$
\begin{aligned}
E(\hat{\beta}) &= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&= \beta
\end{aligned}
$$
:::

::: {#thm-covariance name="Variance of $\hat \beta$"}
If $\text{Var}(y)=\sigma^{2}I$, the covariance matrix for $\hat{\beta}$ is given by $\sigma^{2}(X^{\prime}X)^{-1}$.
:::

::: {.proof}
$$
\begin{aligned}
\text{Var}(\hat{\beta}) &= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
$$
:::

**Note:** These theorems require no assumption of normality.


## Best Linear Unbiased Estimator (BLUE)

::: {#thm-gauss-markov name="Gauss-Markov Theorem"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the least-squares estimators $\hat{\beta}_{j}, j=0,1,...,k$ have minimum variance among all linear unbiased estimators.
:::

::: {.proof}
We consider a linear estimator $Ay$ of $\beta$ and seek the matrix $A$ for which $Ay$ is a minimum variance unbiased estimator.

**1. Unbiasedness Condition:**
In order for $Ay$ to be an unbiased estimator of $\beta$, we must have $E(Ay)=\beta$. Using the assumption $E(y)=X\beta$, this is expressed as:
$$E(Ay) = A E(y) = AX\beta = \beta$$
which implies the condition $AX=I_{k+1}$ since the relationship must hold for any $\beta$.

**2. Minimizing Variance:**
The covariance matrix for the estimator $Ay$ is:
$$\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'$$
We need to choose $A$ (subject to $AX=I$) so that the diagonal elements of $AA'$ are minimized.

To relate $Ay$ to $\hat{\beta}=(X'X)^{-1}X'y$, we define $\hat{A} = (X'X)^{-1}X'$ and write $A = (A - \hat{A}) + \hat{A}$. Then:
$$AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'$$
Expanding this, the cross terms vanish because $(A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'$.
Note that $\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}$.
Also, $A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}$ (since $AX=I$).
Thus, $(A - \hat{A})\hat{A}' = 0$.

The expansion simplifies to:
$$AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'$$
The matrix $(A - \hat{A})(A - \hat{A})'$ is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of $AA'$, we must set $A - \hat{A} = 0$, which implies $A = \hat{A}$.

Thus, the minimum variance estimator is:
$$Ay = (X'X)^{-1}X'y = \hat{\beta}$$
:::

### Notes on Gauss-Markov

1.  **Distributional Generality:** The remarkable feature of the Gauss-Markov theorem is that it holds for *any* distribution of $y$; normality is not required. The only assumptions used are linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$).

2.  **Extension to All Linear Combinations:** The theorem extends beyond just the parameter vector $\beta$ to any linear combination of the parameters.

::: {#cor-linear-combo name="BLUE for All Linear Combinations"}
If $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, the best linear unbiased estimator of the scalar $a'\beta$ is $a'\hat{\beta}$, where $\hat{\beta}$ is the least-squares estimator.
:::

::: {.proof}
Let $\tilde{\beta} = Ay$ be any other linear unbiased estimator of $\beta$. The variance of the linear combination $a'\tilde{\beta}$ is:
$$
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
$$
From the proof of the Gauss-Markov theorem, we established that $AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}$ where $\hat{A} = (X'X)^{-1}X'$. Substituting this into the variance equation:
$$
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
$$
The term $a'(A-\hat{A})(A-\hat{A})'a$ is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore:
$$
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
$$
The variance is minimized when $A=\hat{A}$ (specifically when the first term is zero), proving that $a'\hat{\beta}$ has the minimum variance among all linear unbiased estimators.
:::

3.  **Scaling Invariance:** The predictions made by the model are invariant to the scaling of the explanatory variables.

::: {#thm-scaling name="Scaling Explanatory Variables"}
If $x=(1,x_{1},...,x_{k})'$ and $z=(1,c_{1}x_{1},...,c_{k}x_{k})'$, then the fitted values are identical: $\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z$.
:::

::: {.proof}
Let $D = \text{diag}(1, c_1, ..., c_k)$ such that the design matrix is transformed to $Z = XD$. The LSE for the transformed data is:
$$
\begin{aligned}
\hat{\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
$$
. Then, the prediction is:
$$
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
$$
.
:::

#### Limitations: Restriction to Unbiased Estimators

It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of **linear** and **unbiased** estimators.

* **Assumption Sensitivity:** If the assumptions of linearity ($E(y)=X\beta$) and homoscedasticity ($\text{Var}(y)=\sigma^2 I$) do not hold, $\hat{\beta}$ may be biased or may have a larger variance than other estimators.
* **Unbiasedness Constraint:** The theorem does not compare $\hat{\beta}$ to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only "best" (minimum variance) among those estimators that satisfy the unbiasedness constraint.


## Estimator of Error Variance

We estimate $\sigma^{2}$ by the residual mean square:

::: {#def-s2 name="Residual Variance Estimator"}
$$s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}$$
where $\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})$.
:::

Alternatively, SSE can be written as:
$$\text{SSE} = y'y - \hat{\beta}'X'y$$
This is often useful for computation ($y'y$ is the total sum of squares of the raw data).

### Unbiasedness of $s^2$

::: {#thm-unbiased-s2 name="Unbiasedness of s-squared"}
If $s^{2}$ is defined as above, and if $E(y)=X\beta$ and $\text{Var}(y)=\sigma^{2}I$, then $E(s^{2})=\sigma^{2}$.
:::

::: {.proof}
We use the Hat Matrix $H = X(X'X)^{-1}X'$, which projects $y$ onto $\text{Col}(X)$. Thus, $\hat{y} = Hy$.
The residuals are $y - \hat{y} = (I - H)y$. The Sum of Squared Errors is:
$$\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y$$
Since $H$ is symmetric and idempotent, $(I-H)$ is also symmetric and idempotent. Thus:
$$\text{SSE} = y'(I-H)y$$

To find the expectation, we use the trace trick for quadratic forms: $E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]$.
$$
\begin{aligned}
E(\text{SSE}) &= E[y'(I-H)y] \\
&= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
$$
**Trace Term:** $\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)$, since $\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1$.

**Non-centrality Term:** Since $HX = X$, we have $(I-H)X = 0$. Therefore, the second term vanishes: $\beta'X'(I-H)X\beta = 0$.

Combining these:
$$E(\text{SSE}) = \sigma^2(n - k - 1)$$
Dividing by the degrees of freedom $(n-k-1)$, we get $E(s^2) = \sigma^2$.
:::

## Distributions under Normality

If we add Assumption A5 ($y \sim N_n(X\beta, \sigma^2 I)$), we can derive the exact sampling distributions.

::: {#cor-cov-beta name="Estimated Covariance of Beta"}
An unbiased estimator of $\text{Cov}(\hat{\beta})$ is given by:
$$\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}$$
:::

::: {#thm-sampling-dist name="Sampling Distributions"}
Under assumptions A1-A5:

1.  $\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})$.
2.  $(n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)$.
3.  $\hat{\beta}$ and $s^{2}$ are independent.
:::

::: {.proof}
**Part (i):** Since $\hat{\beta} = (X'X)^{-1}X'y$ is a linear transformation of the normal vector $y$, it is also normally distributed. We already established its mean and variance in @thm-unbiased and @thm-covariance.

**Part (ii):** We showed $\text{SSE} = y'(I-H)y$. Since $(I-H)$ is idempotent with rank $n-k-1$, and $(I-H)X\beta = 0$, by the theory of quadratic forms in normal variables, $\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)$.

**Part (iii):** $\hat{\beta}$ depends on $Hy$ (or $X'y$), while $s^2$ depends on $(I-H)y$. Since $H(I-H) = H - H^2 = 0$, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.
:::

## Maximum Likelihood Estimator (MLE)

::: {#thm-mle name="MLE of Beta and Sigma Squared"}
If $y \sim N_n(X\beta, \sigma^2 I)$, the Maximum Likelihood Estimators are:
$$
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
$$
$$
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
$$
:::

::: {.proof}
The log-likelihood function is:
$$ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) $$
Maximizing this with respect to $\beta$ is equivalent to minimizing the quadratic term $(y - X\beta)'(y - X\beta)$, which yields the Least Squares Estimator.
Differentiating with respect to $\sigma^2$ and setting to zero yields $\hat{\sigma}^2 = \text{SSE}/n$.
:::

**Note:** The MLE for $\sigma^2$ is biased (denominator $n$), whereas $s^2$ is unbiased (denominator $n-k-1$).

## Underfitting and Overfitting

We consider the effects of omitting explanatory variables that should be included (underfitting) and including variables that should be excluded (overfitting).

Suppose the true model is:
$$y = X\beta + e = \begin{pmatrix} X_1 & X_2 \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} + e = X_1\beta_1 + X_2\beta_2 + e \quad (\dagger)$$
where $\text{Var}(e) = \sigma^2 I$.

* **Underfitting:** Leaving out $X_2\beta_2$ when $\beta_2 \neq 0$.
* **Overfitting:** Including $X_2\beta_2$ when $\beta_2 = 0$.

### Underfitting

Suppose model $(\dagger)$ holds, but we fit the reduced model:
$$y = X_1\beta_1^* + e^*$$
The OLS estimator for this reduced model is $\hat{\beta}_1^* = (X_1^T X_1)^{-1}X_1^T y$.

::: {#thm-underfitting name="Bias and Variance under Underfitting"}
If we fit the reduced model when the full model $(\dagger)$ is true:

1.  **Bias:** $E(\hat{\beta}_1^*) = \beta_1 + A\beta_2$, where $A = (X_1^T X_1)^{-1}X_1^T X_2$ is the alias matrix.
2.  **Variance:** $\text{Var}(\hat{\beta}_1^*) = \sigma^2(X_1^T X_1)^{-1}$.
:::

::: {.proof}
**Part (i):**
$$
\begin{aligned}
E(\hat{\beta}_1^*) &= E[(X_1^T X_1)^{-1}X_1^T y] \\
&= (X_1^T X_1)^{-1}X_1^T E(y) \\
&= (X_1^T X_1)^{-1}X_1^T (X_1\beta_1 + X_2\beta_2) \\
&= (X_1^T X_1)^{-1}X_1^T X_1\beta_1 + (X_1^T X_1)^{-1}X_1^T X_2\beta_2 \\
&= \beta_1 + A\beta_2
\end{aligned}
$$
Thus, $\hat{\beta}_1^*$ is biased unless $\beta_2 = 0$ or $X_1^T X_2 = 0$ (orthogonal design).

**Part (ii):**
$$
\begin{aligned}
\text{Var}(\hat{\beta}_1^*) &= \text{Var}[(X_1^T X_1)^{-1}X_1^T y] \\
&= (X_1^T X_1)^{-1}X_1^T [\sigma^2 I] X_1 (X_1^T X_1)^{-1} \\
&= \sigma^2 (X_1^T X_1)^{-1}
\end{aligned}
$$
.
:::

### Overfitting

Suppose the reduced model $y = X_1\beta_1^* + e$ is true (i.e., $\beta_2 = 0$), but we fit the full model $(\dagger)$.
Since the full model includes the true model as a special case, the estimator $\hat{\beta}$ from the full model remains unbiased.

However, fitting the extraneous variables affects the variance.

::: {#thm-overfitting name="Variance Comparison"}
Let $\hat{\beta}_1$ be the estimator from the full model and $\hat{\beta}_1^*$ be the estimator from the reduced model. Then:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
where $A = (X_1^T X_1)^{-1}X_1^T X_2$ and $B = X_2^T X_2 - X_2^T X_1 A$.
Since $A B^{-1} A^T$ is positive semidefinite, $\text{Var}(\hat{\beta}_1) \ge \text{Var}(\hat{\beta}_1^*)$.
:::

::: {.proof}
Using the inverse of a partitioned matrix, the top-left block of $(X^T X)^{-1}$ corresponding to $\beta_1$ is:
$$ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} $$
Since $\text{Var}(\hat{\beta}_1) = \sigma^2 H^{11}$ and $\text{Var}(\hat{\beta}_1^*) = \sigma^2 (X_1^T X_1)^{-1}$, the difference is the second term:
$$ \text{Var}(\hat{\beta}_1) - \text{Var}(\hat{\beta}_1^*) = \sigma^2 A B^{-1} A^T $$
.
:::



### Summary

1.  **Underfitting:** Reduces variance but introduces bias (unless variables are orthogonal).
2.  **Overfitting:** Keeps estimators unbiased but increases variance.

The goal of model selection is to balance these two errors, adhering to the principle of **Occam's Razor**: "entities should not be multiplied beyond necessity".
