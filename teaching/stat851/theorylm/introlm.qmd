---
format: 
  html: default
  pdf: default
---

# Introduction {.numbered}

## Multiple Linear Regression 

Suppose we have observations on $Y$ and $X_j$. The data can be represented in matrix form.

$$
\underset{n \times 1}{y} = \underset{n \times p}{X} \beta + \underset{n \times 1}{\epsilon}
$$

where the error terms are distributed as: $$
\epsilon \sim N_n(0, \sigma^2 I_n),
$$

in which $I_n$ is the identity matrix: $$
I_n = \begin{pmatrix} 
1 & 0 & \dots & 0 \\ 
0 & 1 & \dots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \dots & 1 
\end{pmatrix}
$$ The scalar equation for a single observation is: $$
Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \epsilon_i
$$

## Examples 

### Polynomial Regression 

Polynomial regression fits a curved line to the data points but remains linear in the parameters ($\beta$).

The model equation is: $$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_{p-1} x_i^{p-1}
$$

### Design Matrix Construction 

The design matrix $X$ is constructed by taking powers of the input variable.

$$
y = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} = 
\begin{pmatrix} 
1 & x_1 & x_1^2 & \dots & x_1^{p-1} \\ 
1 & x_2 & x_2^2 & \dots & x_2^{p-1} \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 
1 & x_n & x_n^2 & \dots & x_n^{p-1} 
\end{pmatrix} 
\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p-1} \end{pmatrix} + 
\begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}
$$

### One-Way ANOVA 

ANOVA can be expressed as a linear model using categorical predictors (dummy variables).

Suppose we have 3 groups ($G_1, G_2, G_3$) with observations: $$
Y_{ij} = \mu_i + \epsilon_{ij}, \quad \epsilon_{ij} \sim N(0, \sigma^2)
$$

$$
\overset{G_1}{
  \boxed{
    \begin{matrix} Y_{11} \\ Y_{12} \end{matrix}
  }
}
\quad
\overset{G_2}{
  \boxed{
    \begin{matrix} Y_{21} \\ Y_{22} \end{matrix}
  }
}
\quad
\overset{G_3}{
  \boxed{
    \begin{matrix} Y_{31} \\ Y_{32} \end{matrix}
  }
}
$$

We construct the matrix $X$ to select the group mean ($\mu$) corresponding to the observation:

$$
\underset{6 \times 1}{y} = \underset{6 \times 3}{X} \begin{pmatrix} \mu_1 \\ \mu_2 \\ \mu_3 \end{pmatrix} + \epsilon
$$

$$
\begin{bmatrix}
Y_{11} \\ Y_{12} \\ Y_{21} \\ Y_{22} \\ Y_{31} \\ Y_{32}
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\mu_1 \\ \mu_2 \\ \mu_3
\end{bmatrix} + \epsilon
$$

### Analysis of Covariance (ANCOVA) 

ANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.

$$
\begin{bmatrix}
Y_1 \\ \vdots \\ Y_n
\end{bmatrix} =
\begin{bmatrix}
X_{1,\text{cont}} & 1 & 0 \\
X_{2,\text{cont}} & 1 & 0 \\
\vdots & 0 & 1 \\
X_{n,\text{cont}} & 0 & 1
\end{bmatrix} \beta + \epsilon
$$

## Least Squares Estimation 

For the general linear model $y = X\beta + \epsilon$, the Least Squares estimator is:

$$
\hat{\beta} = (X'X)^{-1}X'y
$$

The predicted values ($\hat{y}$) are obtained via the Projection Matrix (Hat Matrix) $P_X$:

$$
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y = P_X y
$$

The residuals and Sum of Squared Errors are:

$$
\hat{e} = y - \hat{y}
$$ $$
\text{SSE} = ||\hat{e}||^2
$$

The coefficient of determination is: $$
R^2 = \frac{\text{SST} - \text{SSE}}{\text{SST}}
$$ where $\text{SST} = \sum (y_i - \bar{y})^2$.

## Geometric Perspective of Least Square Estimation 

We align the coordinate system to the models for clarity:

1.  **Reduced Model (**$M_0$): Represented by the **X-axis** (labeled $j_3$).
    -   $\hat{y}_0$ is the projection of $y$ onto this axis.
2.  **Full Model (**$M_1$): Represented by the **XY-plane** (the floor).
    -   $\hat{y}_1$ is the projection of $y$ onto this plane ($z=0$).
3.  **Observed Data (**$y$): A point in 3D space.

The "improvement" due to adding predictors is the distance between $\hat{y}_0$ and $\hat{y}_1$.

```{r}
#| label: setup
#| include: false
#| echo: false

library(reticulate)

# 1. Define the list of required Python modules
required_modules <- c("plotly", "numpy")

# 2. Iterate through the list: Check if available, otherwise install
for (mod in required_modules) {
  if (!reticulate::py_module_available(mod)) {
    message(paste("Installing missing Python module:", mod))
    reticulate::py_install(mod)
  }
}
```

```{r fig-geometry-simple}
#| fig-cap: "Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)"
#| message: false
#| warning: false
#| echo: false
#| out-width: "90%"
#| fig-align: "center"
if (knitr::is_html_output()) {
  library(plotly)
  
  # --- 1. Define Data aligned to axes ---
  origin <- c(0, 0, 0)
  # We choose a point y = (x, y, z)
  # x component = magnitude of projection onto j3 (intercept)
  # y component = magnitude of projection onto x1 (predictor)
  # z component = residual error e1
  y <- c(4, 5, 3) 
  
  # --- 2. Define Projections based on alignment ---
  
  # Reduced Model (M0): Projection onto X-axis only
  y_hat_0 <- c(y[1], 0, 0)
  
  # Full Model (M1): Projection onto XY-plane
  y_hat_1 <- c(y[1], y[2], 0)
  
  # --- 3. Helper Function ---
  make_seg <- function(start, end) {
    rbind(start, end) |> as.data.frame() |> setNames(c("x", "y", "z"))
  }
  
  fig <- plot_ly()
  
  # --- 4. Draw Subspaces ---
  
  # M0 Line (The X-axis / j3)
  fig <- fig %>% add_trace(
    data = make_seg(c(0,0,0), c(8,0,0)), 
    x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines',
    line = list(color = 'gray', width = 5),
    name = 'M₀ Subspace (j₃)', hoverinfo = "none"
  )
  
  # M1 Plane (The Floor)
  fig <- fig %>% add_surface(
    x = c(0, 8), y = c(0, 8), z = matrix(c(0,0,0,0), nrow=2),
    opacity = 0.1, showscale = FALSE, 
    colorscale = list(c(0, 1), c("lightgrey", "lightgrey")),
    name = 'M₁ Subspace'
  )
  
  # --- 5. Draw Vectors ---
  
  # y (Observed)
  fig <- fig %>% add_trace(
    data = make_seg(origin, y), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines+markers',
    line = list(color = 'blue', width = 10),
    marker = list(size = 5, color = 'blue'),
    name = 'y (Observed)'
  )
  
  # y_hat_0 (Reduced)
  fig <- fig %>% add_trace(
    data = make_seg(origin, y_hat_0), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines+markers',
    line = list(color = 'red', width = 10),
    marker = list(size = 5, color = 'red'),
    name = 'ŷ₀'
  )
  
  # y_hat_1 (Full)
  fig <- fig %>% add_trace(
    data = make_seg(origin, y_hat_1), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines+markers',
    line = list(color = 'darkgreen', width = 10),
    marker = list(size = 5, color = 'darkgreen'),
    name = 'ŷ₁'
  )
  
  # --- 6. Draw Residuals & Difference ---
  
  # e0 (RSS0) - Vertical + Horizontal distance
  fig <- fig %>% add_trace(
    data = make_seg(y_hat_0, y), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines',
    line = list(color = 'red', width = 5, dash = 'solid'),
    name = 'e₀ (RSS₀)'
  )
  
  # e1 (RSS1) - Vertical distance only
  fig <- fig %>% add_trace(
    data = make_seg(y_hat_1, y), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines',
    line = list(color = 'darkgreen', width = 5, dash = 'solid'),
    name = 'e₁ (RSS₁)'
  )
  
  # Difference (Model Improvement)
  fig <- fig %>% add_trace(
    data = make_seg(y_hat_0, y_hat_1), x = ~x, y = ~y, z = ~z,
    type = 'scatter3d', mode = 'lines',
    line = list(color = 'orange', width = 6, dash = 'dot'),
    name = 'Diff'
  )
  
  # --- 7. Labels ---
  
  # Axis Labels
  fig <- fig %>% layout(
    scene = list(
      xaxis = list(title = "j3 (Intercept)", titlefont = list(size=20)),
      yaxis = list(title = "x1 (Predictor)", titlefont = list(size=15)),
      zaxis = list(title = "Error", titlefont = list(size=15)),
      aspectmode = "data",
      camera = list(eye = list(x = 1.5, y = 1.5, z = 0.5))
    ),
    title = "Geometric Interpretation: Aligned View",
    margin = list(t = 50)
  )
  
  # Add text annotations for points
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'text',
    x = c(y[1], y_hat_0[1], y_hat_1[1]),
    y = c(y[2], y_hat_0[2], y_hat_1[2]),
    z = c(y[3], y_hat_0[3], y_hat_1[3]),
    text = c("y", "ŷ₀", "ŷ₁"),
    textfont = list(size = 18, color = "black", family = "Arial Black"),
    textposition = c("top center", "top center", "bottom center"),
    showlegend = FALSE
  )
  
  # Add text annotations for Error lines
  fig <- fig %>% add_trace(
    type = 'scatter3d', mode = 'text',
    x = c( (y[1]+y_hat_0[1])/2, (y[1]+y_hat_1[1])/2 ),
    y = c( (y[2]+y_hat_0[2])/2, (y[2]+y_hat_1[2])/2 ),
    z = c( (y[3]+y_hat_0[3])/2, (y[3]+y_hat_1[3])/2 ),
    text = c("e₀", "e₁"),
    textfont = list(size = 14, color = "darkblue"),
    textposition = "middle right",
    showlegend = FALSE
  )
  
  fig
} else {
  knitr::include_graphics("figs/3d-lm.png")
}
```

The geometric perspective is not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:

-   **Statistical Clarity:** Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.
-   **Computational Stability:** A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic "Normal Equations" ($(X'X)^{-1}X'y$) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.
-   **Generalizability:** The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.
