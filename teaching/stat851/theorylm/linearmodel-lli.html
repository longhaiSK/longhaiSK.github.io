<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.9.16">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Longhai Li">
<meta name="dcterms.date" content="2026-01-25">

<title>Theory of Linear Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="linearmodel-lli_files/libs/clipboard/clipboard.min.js"></script>
<script src="linearmodel-lli_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="linearmodel-lli_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="linearmodel-lli_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="linearmodel-lli_files/libs/quarto-html/popper.min.js"></script>
<script src="linearmodel-lli_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="linearmodel-lli_files/libs/quarto-html/anchor.min.js"></script>
<link href="linearmodel-lli_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="linearmodel-lli_files/libs/quarto-html/quarto-syntax-highlighting-792aea850e7e5573310f85939a37799b.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="linearmodel-lli_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="linearmodel-lli_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="linearmodel-lli_files/libs/bootstrap/bootstrap-5e31810c9b7c92c56af6e08cf83ac579.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="linearmodel-lli_files/libs/htmltools-fill-0.5.9/fill.css" rel="stylesheet">
<script src="linearmodel-lli_files/libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="linearmodel-lli_files/libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="linearmodel-lli_files/libs/typedarray-0.1/typedarray.min.js"></script>
<script src="linearmodel-lli_files/libs/jquery-3.5.1/jquery.min.js"></script>
<link href="linearmodel-lli_files/libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet">
<script src="linearmodel-lli_files/libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<link href="linearmodel-lli_files/libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="linearmodel-lli_files/libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="linearmodel-lli_files/libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="linearmodel-lli_files/libs/pagedtable-1.1/js/pagedtable.js"></script>
<link rel="stylesheet" href="resources/bookstyles.css">
<script src="resources/num_eq.js"></script>
<script src="resources/merge-toc.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface">Preface</a>
  <ul>
  <li><a href="#key-features" id="toc-key-features" class="nav-link" data-scroll-target="#key-features">Key Features</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><strong>Overview</strong></a></li>
  <li><a href="#audience" id="toc-audience" class="nav-link" data-scroll-target="#audience"><strong>Audience</strong></a></li>
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link" data-scroll-target="#prerequisites"><strong>Prerequisites</strong></a></li>
  </ul></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">1.1</span> Multiple Linear Regression</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">1.2</span> Examples</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="header-section-number">1.2.1</span> Polynomial Regression</a></li>
  <li><a href="#design-matrix-construction" id="toc-design-matrix-construction" class="nav-link" data-scroll-target="#design-matrix-construction"><span class="header-section-number">1.2.2</span> Design Matrix Construction</a></li>
  <li><a href="#one-way-anova" id="toc-one-way-anova" class="nav-link" data-scroll-target="#one-way-anova"><span class="header-section-number">1.2.3</span> One-Way ANOVA</a></li>
  <li><a href="#analysis-of-covariance-ancova" id="toc-analysis-of-covariance-ancova" class="nav-link" data-scroll-target="#analysis-of-covariance-ancova"><span class="header-section-number">1.2.4</span> Analysis of Covariance (ANCOVA)</a></li>
  </ul></li>
  <li><a href="#least-squares-estimation" id="toc-least-squares-estimation" class="nav-link" data-scroll-target="#least-squares-estimation"><span class="header-section-number">1.3</span> Least Squares Estimation</a></li>
  <li><a href="#geometric-perspective-of-least-square-estimation" id="toc-geometric-perspective-of-least-square-estimation" class="nav-link" data-scroll-target="#geometric-perspective-of-least-square-estimation"><span class="header-section-number">1.4</span> Geometric Perspective of Least Square Estimation</a></li>
  </ul></li>
  <li><a href="#projection-in-vector-space" id="toc-projection-in-vector-space" class="nav-link" data-scroll-target="#projection-in-vector-space"><span class="header-section-number">2</span> Projection in Vector Space</a>
  <ul>
  <li><a href="#vector-and-projection-onto-a-line" id="toc-vector-and-projection-onto-a-line" class="nav-link" data-scroll-target="#vector-and-projection-onto-a-line"><span class="header-section-number">2.1</span> Vector and Projection onto a Line</a>
  <ul class="collapse">
  <li><a href="#vectors-and-operations" id="toc-vectors-and-operations" class="nav-link" data-scroll-target="#vectors-and-operations"><span class="header-section-number">2.1.1</span> Vectors and Operations</a></li>
  <li><a href="#scalar-multiplication-and-distance" id="toc-scalar-multiplication-and-distance" class="nav-link" data-scroll-target="#scalar-multiplication-and-distance"><span class="header-section-number">2.1.2</span> Scalar Multiplication and Distance</a></li>
  <li><a href="#angle-and-inner-product" id="toc-angle-and-inner-product" class="nav-link" data-scroll-target="#angle-and-inner-product"><span class="header-section-number">2.1.3</span> Angle and Inner Product</a></li>
  <li><a href="#coordinate-scalar-projection" id="toc-coordinate-scalar-projection" class="nav-link" data-scroll-target="#coordinate-scalar-projection"><span class="header-section-number">2.1.4</span> Coordinate (Scalar) Projection</a></li>
  <li><a href="#vector-projection-formula" id="toc-vector-projection-formula" class="nav-link" data-scroll-target="#vector-projection-formula"><span class="header-section-number">2.1.5</span> Vector Projection Formula</a></li>
  <li><a href="#perpendicularity-orthogonality" id="toc-perpendicularity-orthogonality" class="nav-link" data-scroll-target="#perpendicularity-orthogonality"><span class="header-section-number">2.1.6</span> Perpendicularity (Orthogonality)</a></li>
  <li><a href="#projection-onto-a-line-subspace" id="toc-projection-onto-a-line-subspace" class="nav-link" data-scroll-target="#projection-onto-a-line-subspace"><span class="header-section-number">2.1.7</span> Projection onto a Line (Subspace)</a></li>
  <li><a href="#projection-matrix-p_x" id="toc-projection-matrix-p_x" class="nav-link" data-scroll-target="#projection-matrix-p_x"><span class="header-section-number">2.1.8</span> Projection Matrix (<span class="math inline">\(P_x\)</span>)</a></li>
  <li><a href="#pythagorean-theorem" id="toc-pythagorean-theorem" class="nav-link" data-scroll-target="#pythagorean-theorem"><span class="header-section-number">2.1.9</span> Pythagorean Theorem</a></li>
  <li><a href="#least-square-property" id="toc-least-square-property" class="nav-link" data-scroll-target="#least-square-property"><span class="header-section-number">2.1.10</span> Least Square Property</a></li>
  </ul></li>
  <li><a href="#vector-space" id="toc-vector-space" class="nav-link" data-scroll-target="#vector-space"><span class="header-section-number">2.2</span> Vector Space</a>
  <ul class="collapse">
  <li><a href="#spanned-vector-space" id="toc-spanned-vector-space" class="nav-link" data-scroll-target="#spanned-vector-space"><span class="header-section-number">2.2.1</span> Spanned Vector Space</a></li>
  <li><a href="#column-space-and-row-space" id="toc-column-space-and-row-space" class="nav-link" data-scroll-target="#column-space-and-row-space"><span class="header-section-number">2.2.2</span> Column Space and Row Space</a></li>
  <li><a href="#linear-independence-and-rank" id="toc-linear-independence-and-rank" class="nav-link" data-scroll-target="#linear-independence-and-rank"><span class="header-section-number">2.2.3</span> Linear Independence and Rank</a></li>
  </ul></li>
  <li><a href="#rank-of-matrices-and-dim-of-vector-space" id="toc-rank-of-matrices-and-dim-of-vector-space" class="nav-link" data-scroll-target="#rank-of-matrices-and-dim-of-vector-space"><span class="header-section-number">2.3</span> Rank of Matrices and Dim of Vector Space</a>
  <ul class="collapse">
  <li><a href="#orthogonality-to-a-subspace" id="toc-orthogonality-to-a-subspace" class="nav-link" data-scroll-target="#orthogonality-to-a-subspace"><span class="header-section-number">2.3.1</span> Orthogonality to a Subspace</a></li>
  <li><a href="#kernel-null-space-and-image" id="toc-kernel-null-space-and-image" class="nav-link" data-scroll-target="#kernel-null-space-and-image"><span class="header-section-number">2.3.2</span> Kernel (Null Space) and Image</a></li>
  <li><a href="#nullity-theorem" id="toc-nullity-theorem" class="nav-link" data-scroll-target="#nullity-theorem"><span class="header-section-number">2.3.3</span> Nullity Theorem</a></li>
  <li><a href="#rank-inequalities" id="toc-rank-inequalities" class="nav-link" data-scroll-target="#rank-inequalities"><span class="header-section-number">2.3.4</span> Rank Inequalities</a></li>
  <li><a href="#rank-of-xx-and-xx" id="toc-rank-of-xx-and-xx" class="nav-link" data-scroll-target="#rank-of-xx-and-xx"><span class="header-section-number">2.3.5</span> Rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span></a></li>
  </ul></li>
  <li><a href="#orthogonal-projection-onto-a-subspace" id="toc-orthogonal-projection-onto-a-subspace" class="nav-link" data-scroll-target="#orthogonal-projection-onto-a-subspace"><span class="header-section-number">2.4</span> Orthogonal Projection onto a Subspace</a>
  <ul class="collapse">
  <li><a href="#equivalence-to-least-squares" id="toc-equivalence-to-least-squares" class="nav-link" data-scroll-target="#equivalence-to-least-squares"><span class="header-section-number">2.4.1</span> Equivalence to Least Squares</a></li>
  <li><a href="#uniqueness-of-projection" id="toc-uniqueness-of-projection" class="nav-link" data-scroll-target="#uniqueness-of-projection"><span class="header-section-number">2.4.2</span> Uniqueness of Projection</a></li>
  </ul></li>
  <li><a href="#projection-via-orthonormal-basis-q" id="toc-projection-via-orthonormal-basis-q" class="nav-link" data-scroll-target="#projection-via-orthonormal-basis-q"><span class="header-section-number">2.5</span> Projection via Orthonormal Basis (<span class="math inline">\(Q\)</span>)</a>
  <ul class="collapse">
  <li><a href="#orthonomal-basis" id="toc-orthonomal-basis" class="nav-link" data-scroll-target="#orthonomal-basis"><span class="header-section-number">2.5.1</span> Orthonomal Basis</a></li>
  <li><a href="#projection-matrix-via-orthonomal-basis-q" id="toc-projection-matrix-via-orthonomal-basis-q" class="nav-link" data-scroll-target="#projection-matrix-via-orthonomal-basis-q"><span class="header-section-number">2.5.2</span> Projection Matrix via Orthonomal Basis (<span class="math inline">\(Q\)</span>)</a></li>
  <li><a href="#gram-schmidt-process" id="toc-gram-schmidt-process" class="nav-link" data-scroll-target="#gram-schmidt-process"><span class="header-section-number">2.5.3</span> Gram-Schmidt Process</a></li>
  </ul></li>
  <li><a href="#hat-matrix-projection-matrix-via-x" id="toc-hat-matrix-projection-matrix-via-x" class="nav-link" data-scroll-target="#hat-matrix-projection-matrix-via-x"><span class="header-section-number">2.6</span> Hat Matrix (Projection Matrix via <span class="math inline">\(X\)</span>)</a>
  <ul class="collapse">
  <li><a href="#norm-equations" id="toc-norm-equations" class="nav-link" data-scroll-target="#norm-equations"><span class="header-section-number">2.6.1</span> Norm Equations</a></li>
  <li><a href="#hat-matrix" id="toc-hat-matrix" class="nav-link" data-scroll-target="#hat-matrix"><span class="header-section-number">2.6.2</span> Hat Matrix</a></li>
  <li><a href="#equivalence-of-hat-matrix-and-qq" id="toc-equivalence-of-hat-matrix-and-qq" class="nav-link" data-scroll-target="#equivalence-of-hat-matrix-and-qq"><span class="header-section-number">2.6.3</span> Equivalence of Hat Matrix and <span class="math inline">\(QQ'\)</span></a></li>
  <li><a href="#properties-of-hat-matrix" id="toc-properties-of-hat-matrix" class="nav-link" data-scroll-target="#properties-of-hat-matrix"><span class="header-section-number">2.6.4</span> Properties of Hat Matrix</a></li>
  </ul></li>
  <li><a href="#projection-defined-with-orthogonal-projection-matrix" id="toc-projection-defined-with-orthogonal-projection-matrix" class="nav-link" data-scroll-target="#projection-defined-with-orthogonal-projection-matrix"><span class="header-section-number">2.7</span> Projection Defined with Orthogonal Projection Matrix</a>
  <ul class="collapse">
  <li><a href="#orthogonal-projection-matrix" id="toc-orthogonal-projection-matrix" class="nav-link" data-scroll-target="#orthogonal-projection-matrix"><span class="header-section-number">2.7.1</span> Orthogonal Projection Matrix</a></li>
  <li><a href="#projection-onto-complement-space" id="toc-projection-onto-complement-space" class="nav-link" data-scroll-target="#projection-onto-complement-space"><span class="header-section-number">2.7.2</span> Projection onto Complement Space</a></li>
  <li><a href="#projections-onto-nested-subspaces" id="toc-projections-onto-nested-subspaces" class="nav-link" data-scroll-target="#projections-onto-nested-subspaces"><span class="header-section-number">2.7.3</span> Projections onto Nested Subspaces</a>
  <ul class="collapse">
  <li><a href="#iterative-projections" id="toc-iterative-projections" class="nav-link" data-scroll-target="#iterative-projections"><span class="header-section-number">2.7.3.1</span> Iterative Projections</a></li>
  <li><a href="#difference-of-projections" id="toc-difference-of-projections" class="nav-link" data-scroll-target="#difference-of-projections"><span class="header-section-number">2.7.3.2</span> Difference of Projections</a></li>
  </ul></li>
  <li><a href="#projection-onto-three-multually-orthogonal-subspaces" id="toc-projection-onto-three-multually-orthogonal-subspaces" class="nav-link" data-scroll-target="#projection-onto-three-multually-orthogonal-subspaces"><span class="header-section-number">2.7.4</span> Projection onto Three Multually Orthogonal Subspaces</a></li>
  </ul></li>
  <li><a href="#projections-onto-more-than-three-orthogonal-subspaces" id="toc-projections-onto-more-than-three-orthogonal-subspaces" class="nav-link" data-scroll-target="#projections-onto-more-than-three-orthogonal-subspaces"><span class="header-section-number">2.8</span> Projections onto More than Three Orthogonal Subspaces</a></li>
  </ul></li>
  <li><a href="#matrix-algebra" id="toc-matrix-algebra" class="nav-link" data-scroll-target="#matrix-algebra"><span class="header-section-number">3</span> Matrix Algebra</a>
  <ul>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors"><span class="header-section-number">3.1</span> Eigenvalues and Eigenvectors</a></li>
  <li><a href="#spectral-theory-for-symmetric-matrices" id="toc-spectral-theory-for-symmetric-matrices" class="nav-link" data-scroll-target="#spectral-theory-for-symmetric-matrices"><span class="header-section-number">3.2</span> Spectral Theory for Symmetric Matrices</a>
  <ul class="collapse">
  <li><a href="#spectral-decomposition" id="toc-spectral-decomposition" class="nav-link" data-scroll-target="#spectral-decomposition"><span class="header-section-number">3.2.1</span> Spectral Decomposition</a></li>
  <li><a href="#quadratic-form" id="toc-quadratic-form" class="nav-link" data-scroll-target="#quadratic-form"><span class="header-section-number">3.2.2</span> Quadratic Form</a></li>
  <li><a href="#positive-and-non-negative-definite-matrices" id="toc-positive-and-non-negative-definite-matrices" class="nav-link" data-scroll-target="#positive-and-non-negative-definite-matrices"><span class="header-section-number">3.2.3</span> Positive and Non-Negative Definite Matrices</a></li>
  <li><a href="#properties-of-symmetric-matrices" id="toc-properties-of-symmetric-matrices" class="nav-link" data-scroll-target="#properties-of-symmetric-matrices"><span class="header-section-number">3.2.4</span> Properties of Symmetric Matrices</a></li>
  <li><a href="#spectral-representation-of-projection-matrices" id="toc-spectral-representation-of-projection-matrices" class="nav-link" data-scroll-target="#spectral-representation-of-projection-matrices"><span class="header-section-number">3.2.5</span> Spectral Representation of Projection Matrices</a></li>
  </ul></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd"><span class="header-section-number">3.3</span> Singular Value Decomposition (SVD)</a>
  <ul class="collapse">
  <li><a href="#connection-to-gram-matrices" id="toc-connection-to-gram-matrices" class="nav-link" data-scroll-target="#connection-to-gram-matrices"><span class="header-section-number">3.3.0.1</span> Connection to Gram Matrices</a></li>
  </ul></li>
  <li><a href="#cholesky-decomposition" id="toc-cholesky-decomposition" class="nav-link" data-scroll-target="#cholesky-decomposition"><span class="header-section-number">3.4</span> Cholesky Decomposition</a>
  <ul class="collapse">
  <li><a href="#matrix-representation-of-the-algorithm" id="toc-matrix-representation-of-the-algorithm" class="nav-link" data-scroll-target="#matrix-representation-of-the-algorithm"><span class="header-section-number">3.4.1</span> Matrix Representation of the Algorithm</a></li>
  <li><a href="#applications-in-statistics" id="toc-applications-in-statistics" class="nav-link" data-scroll-target="#applications-in-statistics"><span class="header-section-number">3.4.2</span> Applications in Statistics</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution"><span class="header-section-number">4</span> Multivariate Normal Distribution</a>
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">4.1</span> Motivation</a></li>
  <li><a href="#random-vectors-and-matrices" id="toc-random-vectors-and-matrices" class="nav-link" data-scroll-target="#random-vectors-and-matrices"><span class="header-section-number">4.2</span> Random Vectors and Matrices</a>
  <ul class="collapse">
  <li><a href="#derivation-of-covariance-matrix-structure" id="toc-derivation-of-covariance-matrix-structure" class="nav-link" data-scroll-target="#derivation-of-covariance-matrix-structure"><span class="header-section-number">4.2.1</span> Derivation of Covariance Matrix Structure</a></li>
  </ul></li>
  <li><a href="#properties-of-mean-and-variance" id="toc-properties-of-mean-and-variance" class="nav-link" data-scroll-target="#properties-of-mean-and-variance"><span class="header-section-number">4.3</span> Properties of Mean and Variance</a></li>
  <li><a href="#the-multivariate-normal-distribution" id="toc-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-multivariate-normal-distribution"><span class="header-section-number">4.4</span> The Multivariate Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#definition-and-density" id="toc-definition-and-density" class="nav-link" data-scroll-target="#definition-and-density"><span class="header-section-number">4.4.1</span> Definition and Density</a></li>
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation"><span class="header-section-number">4.4.2</span> Geometric Interpretation</a></li>
  <li><a href="#probability-density-function" id="toc-probability-density-function" class="nav-link" data-scroll-target="#probability-density-function"><span class="header-section-number">4.4.3</span> Probability Density Function</a></li>
  <li><a href="#moment-generating-function" id="toc-moment-generating-function" class="nav-link" data-scroll-target="#moment-generating-function"><span class="header-section-number">4.4.4</span> Moment Generating Function</a></li>
  </ul></li>
  <li><a href="#construction-and-linear-transformations" id="toc-construction-and-linear-transformations" class="nav-link" data-scroll-target="#construction-and-linear-transformations"><span class="header-section-number">4.5</span> Construction and Linear Transformations</a>
  <ul class="collapse">
  <li><a href="#important-corollaries-of-thm-linear-transform" id="toc-important-corollaries-of-thm-linear-transform" class="nav-link" data-scroll-target="#important-corollaries-of-thm-linear-transform"><span class="header-section-number">4.5.1</span> Important Corollaries of Theorem&nbsp;33</a></li>
  </ul></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence"><span class="header-section-number">4.6</span> Independence</a></li>
  <li><a href="#signal-noise-decomposition-for-multivariate-normal-distribution" id="toc-signal-noise-decomposition-for-multivariate-normal-distribution" class="nav-link" data-scroll-target="#signal-noise-decomposition-for-multivariate-normal-distribution"><span class="header-section-number">4.7</span> Signal-Noise Decomposition for Multivariate Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#connections-with-other-formulas" id="toc-connections-with-other-formulas" class="nav-link" data-scroll-target="#connections-with-other-formulas"><span class="header-section-number">4.7.1</span> Connections with Other Formulas</a>
  <ul class="collapse">
  <li><a href="#rao-blackwell-decomposition-of-variance" id="toc-rao-blackwell-decomposition-of-variance" class="nav-link" data-scroll-target="#rao-blackwell-decomposition-of-variance"><span class="header-section-number">4.7.1.1</span> Rao-Blackwell Decomposition of Variance</a></li>
  <li><a href="#variance-of-noise" id="toc-variance-of-noise" class="nav-link" data-scroll-target="#variance-of-noise">Variance of Noise</a></li>
  <li><a href="#variance-of-signal" id="toc-variance-of-signal" class="nav-link" data-scroll-target="#variance-of-signal">Variance of Signal</a></li>
  <li><a href="#total-variance" id="toc-total-variance" class="nav-link" data-scroll-target="#total-variance">Total Variance</a></li>
  <li><a href="#connection-to-ols-regression-estimators" id="toc-connection-to-ols-regression-estimators" class="nav-link" data-scroll-target="#connection-to-ols-regression-estimators"><span class="header-section-number">4.7.1.2</span> Connection to OLS Regression Estimators</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#partial-and-multiple-correlation" id="toc-partial-and-multiple-correlation" class="nav-link" data-scroll-target="#partial-and-multiple-correlation"><span class="header-section-number">4.8</span> Partial and Multiple Correlation</a></li>
  <li><a href="#examples-1" id="toc-examples-1" class="nav-link" data-scroll-target="#examples-1"><span class="header-section-number">4.9</span> Examples</a></li>
  </ul></li>
  <li><a href="#distribution-of-quadratic-forms" id="toc-distribution-of-quadratic-forms" class="nav-link" data-scroll-target="#distribution-of-quadratic-forms"><span class="header-section-number">5</span> Distribution of Quadratic Forms</a>
  <ul>
  <li><a href="#quadratic-forms" id="toc-quadratic-forms" class="nav-link" data-scroll-target="#quadratic-forms"><span class="header-section-number">5.1</span> Quadratic Forms</a></li>
  <li><a href="#mean-of-quadratic-forms" id="toc-mean-of-quadratic-forms" class="nav-link" data-scroll-target="#mean-of-quadratic-forms"><span class="header-section-number">5.2</span> Mean of Quadratic Forms</a></li>
  <li><a href="#non-central-chi2-distribution" id="toc-non-central-chi2-distribution" class="nav-link" data-scroll-target="#non-central-chi2-distribution"><span class="header-section-number">5.3</span> Non-central <span class="math inline">\(\chi^2\)</span> Distribution</a>
  <ul class="collapse">
  <li><a href="#visualizing-chi2-distributions" id="toc-visualizing-chi2-distributions" class="nav-link" data-scroll-target="#visualizing-chi2-distributions"><span class="header-section-number">5.3.1</span> Visualizing <span class="math inline">\(\chi^2\)</span> Distributions</a></li>
  <li><a href="#mean-variance-and-mgf" id="toc-mean-variance-and-mgf" class="nav-link" data-scroll-target="#mean-variance-and-mgf"><span class="header-section-number">5.3.2</span> Mean, Variance, and MGF</a></li>
  <li><a href="#additivity" id="toc-additivity" class="nav-link" data-scroll-target="#additivity"><span class="header-section-number">5.3.3</span> Additivity</a></li>
  <li><a href="#poisson-mixture-representation" id="toc-poisson-mixture-representation" class="nav-link" data-scroll-target="#poisson-mixture-representation"><span class="header-section-number">5.3.4</span> Poisson Mixture Representation</a></li>
  </ul></li>
  <li><a href="#distribution-of-quadratic-forms-1" id="toc-distribution-of-quadratic-forms-1" class="nav-link" data-scroll-target="#distribution-of-quadratic-forms-1"><span class="header-section-number">5.4</span> Distribution of Quadratic Forms</a>
  <ul class="collapse">
  <li><a href="#mgf-of-quadratic-forms" id="toc-mgf-of-quadratic-forms" class="nav-link" data-scroll-target="#mgf-of-quadratic-forms"><span class="header-section-number">5.4.1</span> MGF of Quadratic Forms</a></li>
  <li><a href="#distribution-of-the-sum-squares-of-projected-spherical-normal" id="toc-distribution-of-the-sum-squares-of-projected-spherical-normal" class="nav-link" data-scroll-target="#distribution-of-the-sum-squares-of-projected-spherical-normal"><span class="header-section-number">5.4.2</span> Distribution of the Sum Squares of Projected Spherical Normal</a></li>
  <li><a href="#distribution-of-general-quadratic-forms" id="toc-distribution-of-general-quadratic-forms" class="nav-link" data-scroll-target="#distribution-of-general-quadratic-forms"><span class="header-section-number">5.4.3</span> Distribution of General Quadratic Forms</a></li>
  <li><a href="#standardized-distance-distribution" id="toc-standardized-distance-distribution" class="nav-link" data-scroll-target="#standardized-distance-distribution"><span class="header-section-number">5.4.4</span> Standardized Distance Distribution</a></li>
  </ul></li>
  <li><a href="#distributions-of-projections-of-spherical-normal" id="toc-distributions-of-projections-of-spherical-normal" class="nav-link" data-scroll-target="#distributions-of-projections-of-spherical-normal"><span class="header-section-number">5.5</span> Distributions of Projections of Spherical Normal</a>
  <ul class="collapse">
  <li><a href="#independence-of-forms" id="toc-independence-of-forms" class="nav-link" data-scroll-target="#independence-of-forms"><span class="header-section-number">5.5.1</span> Independence of Forms</a></li>
  <li><a href="#cochrans-theorem" id="toc-cochrans-theorem" class="nav-link" data-scroll-target="#cochrans-theorem"><span class="header-section-number">5.5.2</span> Cochran’s Theorem</a></li>
  </ul></li>
  <li><a href="#non-central-distributions-derived-from-non-central-chi2" id="toc-non-central-distributions-derived-from-non-central-chi2" class="nav-link" data-scroll-target="#non-central-distributions-derived-from-non-central-chi2"><span class="header-section-number">5.6</span> Non-central Distributions Derived from Non-central <span class="math inline">\(\chi^2\)</span></a>
  <ul class="collapse">
  <li><a href="#the-non-central-f-distribution-ftextdf_1-textdf_2-lambda" id="toc-the-non-central-f-distribution-ftextdf_1-textdf_2-lambda" class="nav-link" data-scroll-target="#the-non-central-f-distribution-ftextdf_1-textdf_2-lambda"><span class="header-section-number">5.6.1</span> The Non-central F-distribution <span class="math inline">\(F(\text{df}_1, \text{df}_2, \lambda)\)</span></a></li>
  <li><a href="#type-i-non-central-beta-textbeta_1textdf_12-textdf_22-lambda" id="toc-type-i-non-central-beta-textbeta_1textdf_12-textdf_22-lambda" class="nav-link" data-scroll-target="#type-i-non-central-beta-textbeta_1textdf_12-textdf_22-lambda"><span class="header-section-number">5.6.2</span> Type I Non-central Beta <span class="math inline">\(\text{Beta}_1(\text{df}_1/2, \text{df}_2/2, \lambda)\)</span></a></li>
  <li><a href="#type-ii-non-central-beta-textbeta_2textdf_22-textdf_12-lambda" id="toc-type-ii-non-central-beta-textbeta_2textdf_22-textdf_12-lambda" class="nav-link" data-scroll-target="#type-ii-non-central-beta-textbeta_2textdf_22-textdf_12-lambda"><span class="header-section-number">5.6.3</span> Type II Non-central Beta <span class="math inline">\(\text{Beta}_2(\text{df}_2/2, \text{df}_1/2, \lambda)\)</span></a></li>
  <li><a href="#scaled-type-ii-beta-textscaled-beta_2textdf_22-textdf_12-lambda" id="toc-scaled-type-ii-beta-textscaled-beta_2textdf_22-textdf_12-lambda" class="nav-link" data-scroll-target="#scaled-type-ii-beta-textscaled-beta_2textdf_22-textdf_12-lambda"><span class="header-section-number">5.6.4</span> Scaled Type II Beta <span class="math inline">\(\text{Scaled-Beta}_2(\text{df}_2/2, \text{df}_1/2, \lambda)\)</span></a></li>
  <li><a href="#the-non-central-t-distribution-ttextdf_2-delta" id="toc-the-non-central-t-distribution-ttextdf_2-delta" class="nav-link" data-scroll-target="#the-non-central-t-distribution-ttextdf_2-delta"><span class="header-section-number">5.6.5</span> The Non-central t-distribution <span class="math inline">\(t(\text{df}_2, \delta)\)</span></a></li>
  </ul></li>
  <li><a href="#example-inference-of-the-mean-of-normal-sample" id="toc-example-inference-of-the-mean-of-normal-sample" class="nav-link" data-scroll-target="#example-inference-of-the-mean-of-normal-sample"><span class="header-section-number">5.7</span> Example: Inference of the Mean of Normal Sample</a>
  <ul class="collapse">
  <li><a href="#sum-of-squares-and-their-distributions" id="toc-sum-of-squares-and-their-distributions" class="nav-link" data-scroll-target="#sum-of-squares-and-their-distributions"><span class="header-section-number">5.7.1</span> Sum of Squares and Their Distributions</a></li>
  <li><a href="#distributions-of-equivalent-statistics" id="toc-distributions-of-equivalent-statistics" class="nav-link" data-scroll-target="#distributions-of-equivalent-statistics"><span class="header-section-number">5.7.2</span> Distributions of Equivalent Statistics</a></li>
  <li><a href="#expectations-under-m_1-and-m_0" id="toc-expectations-under-m_1-and-m_0" class="nav-link" data-scroll-target="#expectations-under-m_1-and-m_0"><span class="header-section-number">5.7.3</span> Expectations Under <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_0\)</span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#inference-for-a-multiple-linear-regression-model" id="toc-inference-for-a-multiple-linear-regression-model" class="nav-link" data-scroll-target="#inference-for-a-multiple-linear-regression-model"><span class="header-section-number">6</span> Inference for A Multiple Linear Regression Model</a>
  <ul>
  <li><a href="#linear-models-and-least-square-estimator" id="toc-linear-models-and-least-square-estimator" class="nav-link" data-scroll-target="#linear-models-and-least-square-estimator"><span class="header-section-number">6.1</span> Linear Models and Least Square Estimator</a>
  <ul class="collapse">
  <li><a href="#assumptions-in-linear-models" id="toc-assumptions-in-linear-models" class="nav-link" data-scroll-target="#assumptions-in-linear-models"><span class="header-section-number">6.1.1</span> Assumptions in Linear Models</a></li>
  <li><a href="#matrix-formulation" id="toc-matrix-formulation" class="nav-link" data-scroll-target="#matrix-formulation"><span class="header-section-number">6.1.2</span> Matrix Formulation</a></li>
  <li><a href="#least-squares-estimator-of-beta-and-fitted-value-hat-y" id="toc-least-squares-estimator-of-beta-and-fitted-value-hat-y" class="nav-link" data-scroll-target="#least-squares-estimator-of-beta-and-fitted-value-hat-y"><span class="header-section-number">6.1.3</span> Least Squares Estimator of <span class="math inline">\(\beta\)</span> and Fitted Value <span class="math inline">\(\hat Y\)</span></a></li>
  <li><a href="#properties-of-the-estimator-hat-beta" id="toc-properties-of-the-estimator-hat-beta" class="nav-link" data-scroll-target="#properties-of-the-estimator-hat-beta"><span class="header-section-number">6.1.4</span> Properties of the Estimator <span class="math inline">\(\hat \beta\)</span></a></li>
  </ul></li>
  <li><a href="#best-linear-unbiased-estimator-blue" id="toc-best-linear-unbiased-estimator-blue" class="nav-link" data-scroll-target="#best-linear-unbiased-estimator-blue"><span class="header-section-number">6.2</span> Best Linear Unbiased Estimator (BLUE)</a>
  <ul class="collapse">
  <li><a href="#notes-on-gauss-markov" id="toc-notes-on-gauss-markov" class="nav-link" data-scroll-target="#notes-on-gauss-markov"><span class="header-section-number">6.2.1</span> Notes on Gauss-markov</a></li>
  <li><a href="#limitations-restriction-to-unbiased-estimators" id="toc-limitations-restriction-to-unbiased-estimators" class="nav-link" data-scroll-target="#limitations-restriction-to-unbiased-estimators"><span class="header-section-number">6.2.2</span> Limitations: Restriction to Unbiased Estimators</a></li>
  </ul></li>
  <li><a href="#estimator-of-error-variance" id="toc-estimator-of-error-variance" class="nav-link" data-scroll-target="#estimator-of-error-variance"><span class="header-section-number">6.3</span> Estimator of Error Variance</a>
  <ul class="collapse">
  <li><a href="#unbiasedness-of-s2" id="toc-unbiasedness-of-s2" class="nav-link" data-scroll-target="#unbiasedness-of-s2"><span class="header-section-number">6.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></a></li>
  </ul></li>
  <li><a href="#distributions-under-normality" id="toc-distributions-under-normality" class="nav-link" data-scroll-target="#distributions-under-normality"><span class="header-section-number">6.4</span> Distributions Under Normality</a></li>
  <li><a href="#maximum-likelihood-estimator-mle" id="toc-maximum-likelihood-estimator-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimator-mle"><span class="header-section-number">6.5</span> Maximum Likelihood Estimator (MLE)</a></li>
  <li><a href="#linear-models-in-centered-form" id="toc-linear-models-in-centered-form" class="nav-link" data-scroll-target="#linear-models-in-centered-form"><span class="header-section-number">6.6</span> Linear Models in Centered Form</a>
  <ul class="collapse">
  <li><a href="#matrix-formulation-1" id="toc-matrix-formulation-1" class="nav-link" data-scroll-target="#matrix-formulation-1"><span class="header-section-number">6.6.1</span> Matrix Formulation</a></li>
  <li><a href="#estimation-in-centered-form" id="toc-estimation-in-centered-form" class="nav-link" data-scroll-target="#estimation-in-centered-form"><span class="header-section-number">6.6.2</span> Estimation in Centered Form</a></li>
  </ul></li>
  <li><a href="#decomposition-of-sum-of-squares" id="toc-decomposition-of-sum-of-squares" class="nav-link" data-scroll-target="#decomposition-of-sum-of-squares"><span class="header-section-number">6.7</span> Decomposition of Sum of Squares</a>
  <ul class="collapse">
  <li><a href="#d-visualization-of-decomposition-of-y" id="toc-d-visualization-of-decomposition-of-y" class="nav-link" data-scroll-target="#d-visualization-of-decomposition-of-y"><span class="header-section-number">6.7.1</span> 3D Visualization of Decomposition of <span class="math inline">\(y\)</span></a></li>
  <li><a href="#a-diagram-to-show-decomposition-of-sum-of-squares" id="toc-a-diagram-to-show-decomposition-of-sum-of-squares" class="nav-link" data-scroll-target="#a-diagram-to-show-decomposition-of-sum-of-squares"><span class="header-section-number">6.7.2</span> A Diagram to Show Decomposition of Sum of Squares</a></li>
  <li><a href="#distribution-of-sum-of-squares" id="toc-distribution-of-sum-of-squares" class="nav-link" data-scroll-target="#distribution-of-sum-of-squares"><span class="header-section-number">6.7.3</span> Distribution of Sum of Squares</a></li>
  </ul></li>
  <li><a href="#f-test-for-testing-overall-regression-effect" id="toc-f-test-for-testing-overall-regression-effect" class="nav-link" data-scroll-target="#f-test-for-testing-overall-regression-effect"><span class="header-section-number">6.8</span> F-test for Testing Overall Regression Effect</a>
  <ul class="collapse">
  <li><a href="#the-f-statistic" id="toc-the-f-statistic" class="nav-link" data-scroll-target="#the-f-statistic">The F-statistic</a></li>
  <li><a href="#understanding-f-via-expectations" id="toc-understanding-f-via-expectations" class="nav-link" data-scroll-target="#understanding-f-via-expectations">Understanding <span class="math inline">\(F\)</span> via Expectations</a></li>
  <li><a href="#distributional-theory" id="toc-distributional-theory" class="nav-link" data-scroll-target="#distributional-theory"><span class="header-section-number">6.8.1</span> Distributional Theory</a></li>
  <li><a href="#visualization-of-the-rejection-region" id="toc-visualization-of-the-rejection-region" class="nav-link" data-scroll-target="#visualization-of-the-rejection-region"><span class="header-section-number">6.8.2</span> Visualization of the Rejection Region</a></li>
  </ul></li>
  <li><a href="#raw-coefficient-of-determination-r2" id="toc-raw-coefficient-of-determination-r2" class="nav-link" data-scroll-target="#raw-coefficient-of-determination-r2"><span class="header-section-number">6.9</span> Raw Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">6.9.1</span> Definition</a></li>
  <li><a href="#expectation-and-bias" id="toc-expectation-and-bias" class="nav-link" data-scroll-target="#expectation-and-bias"><span class="header-section-number">6.9.2</span> Expectation and Bias</a></li>
  <li><a href="#exact-distribution" id="toc-exact-distribution" class="nav-link" data-scroll-target="#exact-distribution"><span class="header-section-number">6.9.3</span> Exact Distribution</a></li>
  </ul></li>
  <li><a href="#adjusted-r-squared-r2_a" id="toc-adjusted-r-squared-r2_a" class="nav-link" data-scroll-target="#adjusted-r-squared-r2_a"><span class="header-section-number">6.10</span> Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</a></li>
  <li><a href="#population-proportion-of-signals-rho2" id="toc-population-proportion-of-signals-rho2" class="nav-link" data-scroll-target="#population-proportion-of-signals-rho2"><span class="header-section-number">6.11</span> Population Proportion of Signals (<span class="math inline">\(\rho^2\)</span>)</a></li>
  <li><a href="#relationship-between-r2-and-f-test" id="toc-relationship-between-r2-and-f-test" class="nav-link" data-scroll-target="#relationship-between-r2-and-f-test"><span class="header-section-number">6.12</span> Relationship between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(F\)</span> Test</a></li>
  <li><a href="#confidence-interval-of-population-rho2" id="toc-confidence-interval-of-population-rho2" class="nav-link" data-scroll-target="#confidence-interval-of-population-rho2"><span class="header-section-number">6.13</span> Confidence Interval of Population <span class="math inline">\(\rho^2\)</span></a></li>
  <li><a href="#an-animation-for-illustrating-r2_a-under-h_0-and-h_1" id="toc-an-animation-for-illustrating-r2_a-under-h_0-and-h_1" class="nav-link" data-scroll-target="#an-animation-for-illustrating-r2_a-under-h_0-and-h_1"><span class="header-section-number">6.14</span> An Animation for Illustrating <span class="math inline">\(R^2_a\)</span> Under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></a></li>
  <li><a href="#a-data-example-with-house-price-valuation" id="toc-a-data-example-with-house-price-valuation" class="nav-link" data-scroll-target="#a-data-example-with-house-price-valuation"><span class="header-section-number">6.15</span> A Data Example with House Price Valuation</a>
  <ul class="collapse">
  <li><a href="#visualize-the-data" id="toc-visualize-the-data" class="nav-link" data-scroll-target="#visualize-the-data"><span class="header-section-number">6.15.1</span> Visualize the Data</a></li>
  <li><a href="#fit-the-model" id="toc-fit-the-model" class="nav-link" data-scroll-target="#fit-the-model"><span class="header-section-number">6.15.2</span> Fit the Model</a>
  <ul class="collapse">
  <li><a href="#method-1-naive-matrix-formula" id="toc-method-1-naive-matrix-formula" class="nav-link" data-scroll-target="#method-1-naive-matrix-formula">Method 1: Naive Matrix Formula</a></li>
  <li><a href="#method-2-centralized-formula" id="toc-method-2-centralized-formula" class="nav-link" data-scroll-target="#method-2-centralized-formula">Method 2: Centralized Formula</a></li>
  <li><a href="#method-3-using-rs-lm-function" id="toc-method-3-using-rs-lm-function" class="nav-link" data-scroll-target="#method-3-using-rs-lm-function">Method 3: Using R’s <code>lm</code> Function</a></li>
  </ul></li>
  <li><a href="#visualization-of-fitted-values-vs-mean" id="toc-visualization-of-fitted-values-vs-mean" class="nav-link" data-scroll-target="#visualization-of-fitted-values-vs-mean"><span class="header-section-number">6.15.3</span> Visualization of Fitted Values vs Mean</a></li>
  <li><a href="#computing-sums-of-squares-sse-sst-ssr" id="toc-computing-sums-of-squares-sse-sst-ssr" class="nav-link" data-scroll-target="#computing-sums-of-squares-sse-sst-ssr"><span class="header-section-number">6.15.4</span> Computing Sums of Squares (SSE, SST, SSR)</a>
  <ul class="collapse">
  <li><a href="#naive-sum-of-squared-errors" id="toc-naive-sum-of-squared-errors" class="nav-link" data-scroll-target="#naive-sum-of-squared-errors"><span class="header-section-number">6.15.4.1</span> 1. Naive Sum of Squared Errors</a></li>
  <li><a href="#pythagorean-shortcut-vector-lengths" id="toc-pythagorean-shortcut-vector-lengths" class="nav-link" data-scroll-target="#pythagorean-shortcut-vector-lengths"><span class="header-section-number">6.15.4.2</span> 2. Pythagorean Shortcut (Vector Lengths)</a></li>
  <li><a href="#matrix-algebra-shortcuts" id="toc-matrix-algebra-shortcuts" class="nav-link" data-scroll-target="#matrix-algebra-shortcuts"><span class="header-section-number">6.15.4.3</span> Matrix Algebra Shortcuts</a></li>
  </ul></li>
  <li><a href="#analysis-of-variance-anova" id="toc-analysis-of-variance-anova" class="nav-link" data-scroll-target="#analysis-of-variance-anova"><span class="header-section-number">6.15.5</span> Analysis of Variance (ANOVA)</a>
  <ul class="collapse">
  <li><a href="#computing-sums-of-squares" id="toc-computing-sums-of-squares" class="nav-link" data-scroll-target="#computing-sums-of-squares">1. Computing Sums of Squares</a></li>
  <li><a href="#manual-anova-construction" id="toc-manual-anova-construction" class="nav-link" data-scroll-target="#manual-anova-construction">2. Manual ANOVA Construction</a></li>
  <li><a href="#standard-r-output-anova" id="toc-standard-r-output-anova" class="nav-link" data-scroll-target="#standard-r-output-anova">3. Standard R Output (<code>anova</code>)</a></li>
  </ul></li>
  <li><a href="#coefficient-of-determination-and-variance-decomposition" id="toc-coefficient-of-determination-and-variance-decomposition" class="nav-link" data-scroll-target="#coefficient-of-determination-and-variance-decomposition"><span class="header-section-number">6.15.6</span> Coefficient of Determination and Variance Decomposition</a>
  <ul class="collapse">
  <li><a href="#calculation" id="toc-calculation" class="nav-link" data-scroll-target="#calculation">1. Calculation</a></li>
  <li><a href="#variance-decomposition-table" id="toc-variance-decomposition-table" class="nav-link" data-scroll-target="#variance-decomposition-table">2. Variance Decomposition Table</a></li>
  </ul></li>
  <li><a href="#confidence-interval-for-population-r2-rho2" id="toc-confidence-interval-for-population-r2-rho2" class="nav-link" data-scroll-target="#confidence-interval-for-population-r2-rho2"><span class="header-section-number">6.15.7</span> Confidence Interval for Population <span class="math inline">\(R^2\)</span> (<span class="math inline">\(\rho^2\)</span>)</a>
  <ul class="collapse">
  <li><a href="#manual-inversion-method" id="toc-manual-inversion-method" class="nav-link" data-scroll-target="#manual-inversion-method">1. Manual Inversion Method</a></li>
  <li><a href="#using-r-package-mbess" id="toc-using-r-package-mbess" class="nav-link" data-scroll-target="#using-r-package-mbess">2. Using R Package <code>MBESS</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#underfitting-and-overfitting" id="toc-underfitting-and-overfitting" class="nav-link" data-scroll-target="#underfitting-and-overfitting"><span class="header-section-number">6.16</span> Underfitting and Overfitting</a>
  <ul class="collapse">
  <li><a href="#notation-and-setup" id="toc-notation-and-setup" class="nav-link" data-scroll-target="#notation-and-setup"><span class="header-section-number">6.16.1</span> Notation and Setup</a></li>
  <li><a href="#case-1-underfitting" id="toc-case-1-underfitting" class="nav-link" data-scroll-target="#case-1-underfitting"><span class="header-section-number">6.16.2</span> Case 1: Underfitting</a></li>
  <li><a href="#case-2-overfitting" id="toc-case-2-overfitting" class="nav-link" data-scroll-target="#case-2-overfitting"><span class="header-section-number">6.16.3</span> Case 2: Overfitting</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#generalized-inverses" id="toc-generalized-inverses" class="nav-link" data-scroll-target="#generalized-inverses"><span class="header-section-number">7</span> Generalized Inverses</a>
  <ul>
  <li><a href="#motivation-1" id="toc-motivation-1" class="nav-link" data-scroll-target="#motivation-1"><span class="header-section-number">7.1</span> Motivation</a></li>
  <li><a href="#definition-of-generalized-inverse" id="toc-definition-of-generalized-inverse" class="nav-link" data-scroll-target="#definition-of-generalized-inverse"><span class="header-section-number">7.2</span> Definition of Generalized Inverse</a></li>
  <li><a href="#a-procedure-to-find-a-generalized-inverse" id="toc-a-procedure-to-find-a-generalized-inverse" class="nav-link" data-scroll-target="#a-procedure-to-find-a-generalized-inverse"><span class="header-section-number">7.3</span> A Procedure to Find a Generalized Inverse</a></li>
  <li><a href="#moore-penrose-inverse" id="toc-moore-penrose-inverse" class="nav-link" data-scroll-target="#moore-penrose-inverse"><span class="header-section-number">7.4</span> Moore-Penrose Inverse</a></li>
  <li><a href="#solving-linear-systems-with-generalized-inverse" id="toc-solving-linear-systems-with-generalized-inverse" class="nav-link" data-scroll-target="#solving-linear-systems-with-generalized-inverse"><span class="header-section-number">7.5</span> Solving Linear Systems with Generalized Inverse</a></li>
  <li><a href="#least-squares-for-non-full-rank-x-with-generalized-inverse" id="toc-least-squares-for-non-full-rank-x-with-generalized-inverse" class="nav-link" data-scroll-target="#least-squares-for-non-full-rank-x-with-generalized-inverse"><span class="header-section-number">7.6</span> Least Squares for Non-full-rank <span class="math inline">\(X\)</span> with Generalized Inverse</a>
  <ul class="collapse">
  <li><a href="#projection-matrix-with-generalized-inverse-of-xx" id="toc-projection-matrix-with-generalized-inverse-of-xx" class="nav-link" data-scroll-target="#projection-matrix-with-generalized-inverse-of-xx"><span class="header-section-number">7.6.1</span> Projection Matrix with Generalized Inverse of <span class="math inline">\(X'X\)</span></a></li>
  <li><a href="#invariance-and-uniqueness-of-the-projection-matrix" id="toc-invariance-and-uniqueness-of-the-projection-matrix" class="nav-link" data-scroll-target="#invariance-and-uniqueness-of-the-projection-matrix"><span class="header-section-number">7.6.2</span> Invariance and Uniqueness of “the” Projection Matrix</a></li>
  </ul></li>
  <li><a href="#the-left-inverse-view-recovering-hatbeta-from-haty" id="toc-the-left-inverse-view-recovering-hatbeta-from-haty" class="nav-link" data-scroll-target="#the-left-inverse-view-recovering-hatbeta-from-haty"><span class="header-section-number">7.7</span> The Left Inverse View: Recovering <span class="math inline">\(\hat{\beta}\)</span> from <span class="math inline">\(\hat{y}\)</span></a>
  <ul class="collapse">
  <li><a href="#the-generalized-left-inverse" id="toc-the-generalized-left-inverse" class="nav-link" data-scroll-target="#the-generalized-left-inverse"><span class="header-section-number">7.7.1</span> The Generalized Left Inverse</a></li>
  <li><a href="#verification-of-the-inverse-property" id="toc-verification-of-the-inverse-property" class="nav-link" data-scroll-target="#verification-of-the-inverse-property"><span class="header-section-number">7.7.2</span> Verification of the Inverse Property</a></li>
  <li><a href="#recovering-the-estimator" id="toc-recovering-the-estimator" class="nav-link" data-scroll-target="#recovering-the-estimator"><span class="header-section-number">7.7.3</span> Recovering the Estimator</a></li>
  </ul></li>
  <li><a href="#non-full-rank-least-squares-with-qr-decomposition" id="toc-non-full-rank-least-squares-with-qr-decomposition" class="nav-link" data-scroll-target="#non-full-rank-least-squares-with-qr-decomposition"><span class="header-section-number">7.8</span> Non-full-rank Least Squares with QR Decomposition</a>
  <ul class="collapse">
  <li><a href="#constructing-a-solution-by-solving-normal-equations" id="toc-constructing-a-solution-by-solving-normal-equations" class="nav-link" data-scroll-target="#constructing-a-solution-by-solving-normal-equations"><span class="header-section-number">7.8.1</span> Constructing a Solution by Solving Normal Equations</a></li>
  <li><a href="#constructing-a-solution-by-solving-reparametrized-beta" id="toc-constructing-a-solution-by-solving-reparametrized-beta" class="nav-link" data-scroll-target="#constructing-a-solution-by-solving-reparametrized-beta"><span class="header-section-number">7.8.2</span> Constructing a Solution by Solving Reparametrized <span class="math inline">\(\beta\)</span></a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="linearmodel-lli.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Theory of Linear Models</h1>
</div>



<div class="quarto-title-meta column-page-right">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Longhai Li </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 25, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preface" class="level1 unnumbered">
<h1 class="unnumbered">Preface</h1>
<section id="key-features" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="key-features">Key Features</h2>
<p>This text adopts a geometric approach to the statistical theory of linear models, aiming to provide a deeper understanding than standard algebraic treatments. Key features include:</p>
<ul>
<li><p><strong>Projection Perspective:</strong> We prioritize the geometric interpretation of least squares, viewing estimation as a projection of the response vector onto a model subspace. This visual framework unifies diverse topics—from simple regression to complex ANOVA designs—under a single theoretical umbrella.</p></li>
<li><p><strong>Interactive Visualizations:</strong> Abstract concepts are brought to life through interactive 3D plots. Readers can rotate and inspect vector spaces, residual planes, and projection geometries to build a tangible intuition for high-dimensional operations.</p></li>
<li><p><strong>Computational Integration:</strong> Theory is seamlessly integrated with practice. The text provides implementation examples using R (and Python), demonstrating how theoretical matrix equations translate directly into computational code.</p></li>
<li><p><strong>Rigorous Foundations:</strong> While visually driven, the text maintains mathematical rigor, covering essential topics such as spectral theory, the generalized inverseand the multivariate normal distribution to ensure a solid theoretical grounding.</p></li>
</ul>
</section>
<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview"><strong>Overview</strong></h2>
<p>This course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.</p>
</section>
<section id="audience" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="audience"><strong>Audience</strong></h2>
<p>This book is designed for graduate students and advanced undergraduate students in statistics, data science, and related quantitative fields. It serves as a bridge between applied regression analysis and the theoretical foundations of linear models. Researchers and practitioners seeking a deeper geometric and algebraic understanding of the statistical methods they use daily will also find this text valuable.</p>
</section>
<section id="prerequisites" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="prerequisites"><strong>Prerequisites</strong></h2>
<p>To get the most out of this book, readers should have a comfortable grasp of the following topics:</p>
<p><strong>Linear Algebra</strong>: An elementary understanding of matrix operations is essential. You should be familiar with matrix multiplication, determinants, inversion, and the basic concepts of vector spaces (such as linear independence, basis vectors, and subspaces). While we review key spectral theory concepts (like eigenvalues and the singular value decomposition) in the early chapters, prior exposure to these ideas is helpful.</p>
<p><strong>Probability and Statistics</strong>: A standard introductory course in probability and mathematical statistics is required. Readers should be familiar with random variables, expectation, variance, covariance, common probability distributions (especially the Normal distribution), and fundamental concepts of hypothesis testing and estimation.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="introduction" class="level1 numbered" data-number="1">
<h1 class="numbered" data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<section id="multiple-linear-regression" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">1.1</span> Multiple Linear Regression</h2>
<p>Suppose we have observations on <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span>. The data can be represented in matrix form.</p>
<p><span class="math display">\[
\underset{n \times 1}{y} = \underset{n \times p}{X} \beta + \underset{n \times 1}{\epsilon}
\]</span></p>
<p>where the error terms are distributed as: <span class="math display">\[
\epsilon \sim N_n(0, \sigma^2 I_n),
\]</span></p>
<p>in which <span class="math inline">\(I_n\)</span> is the identity matrix: <span class="math display">\[
I_n = \begin{pmatrix}
1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; 1
\end{pmatrix}
\]</span> The scalar equation for a single observation is: <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \epsilon_i
\]</span></p>
</section>
<section id="examples" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="examples"><span class="header-section-number">1.2</span> Examples</h2>
<section id="polynomial-regression" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">1.2.1</span> Polynomial Regression</h3>
<p>Polynomial regression fits a curved line to the data points but remains linear in the parameters (<span class="math inline">\(\beta\)</span>).</p>
<p>The model equation is: <span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_{p-1} x_i^{p-1}
\]</span></p>
</section>
<section id="design-matrix-construction" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="design-matrix-construction"><span class="header-section-number">1.2.2</span> Design Matrix Construction</h3>
<p>The design matrix <span class="math inline">\(X\)</span> is constructed by taking powers of the input variable.</p>
<p><span class="math display">\[
y = \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} =
\begin{pmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^{p-1} \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^{p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^{p-1}
\end{pmatrix}
\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p-1} \end{pmatrix} +
\begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}
\]</span></p>
</section>
<section id="one-way-anova" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="one-way-anova"><span class="header-section-number">1.2.3</span> One-Way ANOVA</h3>
<p>ANOVA can be expressed as a linear model using categorical predictors (dummy variables).</p>
<p>Suppose we have 3 groups (<span class="math inline">\(G_1, G_2, G_3\)</span>) with observations: <span class="math display">\[
Y_{ij} = \mu_i + \epsilon_{ij}, \quad \epsilon_{ij} \sim N(0, \sigma^2)
\]</span></p>
<p><span class="math display">\[
\overset{G_1}{
  \boxed{
    \begin{matrix} Y_{11} \\ Y_{12} \end{matrix}
  }
}
\quad
\overset{G_2}{
  \boxed{
    \begin{matrix} Y_{21} \\ Y_{22} \end{matrix}
  }
}
\quad
\overset{G_3}{
  \boxed{
    \begin{matrix} Y_{31} \\ Y_{32} \end{matrix}
  }
}
\]</span></p>
<p>We construct the matrix <span class="math inline">\(X\)</span> to select the group mean (<span class="math inline">\(\mu\)</span>) corresponding to the observation:</p>
<p><span class="math display">\[
\underset{6 \times 1}{y} = \underset{6 \times 3}{X} \begin{pmatrix} \mu_1 \\ \mu_2 \\ \mu_3 \end{pmatrix} + \epsilon
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
Y_{11} \\ Y_{12} \\ Y_{21} \\ Y_{22} \\ Y_{31} \\ Y_{32}
\end{bmatrix} =
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
\mu_1 \\ \mu_2 \\ \mu_3
\end{bmatrix} + \epsilon
\]</span></p>
</section>
<section id="analysis-of-covariance-ancova" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="analysis-of-covariance-ancova"><span class="header-section-number">1.2.4</span> Analysis of Covariance (ANCOVA)</h3>
<p>ANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.</p>
<p><span class="math display">\[
\begin{bmatrix}
Y_1 \\ \vdots \\ Y_n
\end{bmatrix} =
\begin{bmatrix}
X_{1,\text{cont}} &amp; 1 &amp; 0 \\
X_{2,\text{cont}} &amp; 1 &amp; 0 \\
\vdots &amp; 0 &amp; 1 \\
X_{n,\text{cont}} &amp; 0 &amp; 1
\end{bmatrix} \beta + \epsilon
\]</span></p>
</section>
</section>
<section id="least-squares-estimation" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="least-squares-estimation"><span class="header-section-number">1.3</span> Least Squares Estimation</h2>
<p>For the general linear model <span class="math inline">\(y = X\beta + \epsilon\)</span>, the Least Squares estimator is:</p>
<p><span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'y
\]</span></p>
<p>The predicted values (<span class="math inline">\(\hat{y}\)</span>) are obtained via the Projection Matrix (Hat Matrix) <span class="math inline">\(P_X\)</span>:</p>
<p><span class="math display">\[
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y = P_X y
\]</span></p>
<p>The residuals and Sum of Squared Errors are:</p>
<p><span class="math display">\[
\hat{e} = y - \hat{y}
\]</span> <span class="math display">\[
\text{SSE} = ||\hat{e}||^2
\]</span></p>
<p>The coefficient of determination is: <span class="math display">\[
R^2 = \frac{\text{SST} - \text{SSE}}{\text{SST}}
\]</span> where <span class="math inline">\(\text{SST} = \sum (y_i - \bar{y})^2\)</span>.</p>
</section>
<section id="geometric-perspective-of-least-square-estimation" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="geometric-perspective-of-least-square-estimation"><span class="header-section-number">1.4</span> Geometric Perspective of Least Square Estimation</h2>
<p>We align the coordinate system to the models for clarity:</p>
<ol type="1">
<li><strong>Reduced Model (</strong><span class="math inline">\(M_0\)</span>): Represented by the <strong>X-axis</strong> (labeled <span class="math inline">\(j_3\)</span>).
<ul>
<li><span class="math inline">\(\hat{y}_0\)</span> is the projection of <span class="math inline">\(y\)</span> onto this axis.</li>
</ul></li>
<li><strong>Full Model (</strong><span class="math inline">\(M_1\)</span>): Represented by the <strong>XY-plane</strong> (the floor).
<ul>
<li><span class="math inline">\(\hat{y}_1\)</span> is the projection of <span class="math inline">\(y\)</span> onto this plane (<span class="math inline">\(z=0\)</span>).</li>
</ul></li>
<li><strong>Observed Data (</strong><span class="math inline">\(y\)</span>): A point in 3D space.</li>
</ol>
<p>The “improvement” due to adding predictors is the distance between <span class="math inline">\(\hat{y}_0\)</span> and <span class="math inline">\(\hat{y}_1\)</span>.</p>
<div class="cell" data-layout-align="center">
<div id="fig-geometry-simple" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-geometry-simple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-41ef425d3314c3448a41" style="width:90%;height:576px;"></div>
<script type="application/json" data-for="htmlwidget-41ef425d3314c3448a41">{"x":{"visdat":{"15621056daa5":["function () ","plotlyVisDat"],"1562362c8f04":["function () ","data"],"156227616b65":["function () ","data"],"15626cd1c815":["function () ","data"],"156240a71283":["function () ","data"],"156218b0779e":["function () ","data"],"1562697d32bb":["function () ","data"],"15621a8dc918":["function () ","data"]},"cur_data":"15621a8dc918","attrs":{"1562362c8f04":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","line":{"color":"gray","width":5},"name":"M₀ Subspace (j₃)","hoverinfo":"none","inherit":true},"1562362c8f04.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0,0],[0,0]],"type":"surface","x":[0,8],"y":[0,8],"opacity":0.10000000000000001,"showscale":false,"colorscale":[[0,1],["lightgrey","lightgrey"]],"name":"M₁ Subspace","inherit":true},"156227616b65":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines+markers","line":{"color":"blue","width":10},"marker":{"size":5,"color":"blue"},"name":"y (Observed)","inherit":true},"15626cd1c815":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines+markers","line":{"color":"red","width":10},"marker":{"size":5,"color":"red"},"name":"ŷ₀","inherit":true},"156240a71283":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines+markers","line":{"color":"darkgreen","width":10},"marker":{"size":5,"color":"darkgreen"},"name":"ŷ₁","inherit":true},"156218b0779e":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","line":{"color":"red","width":5,"dash":"solid"},"name":"e₀ (RSS₀)","inherit":true},"1562697d32bb":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","line":{"color":"darkgreen","width":5,"dash":"solid"},"name":"e₁ (RSS₁)","inherit":true},"15621a8dc918":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","line":{"color":"orange","width":6,"dash":"dot"},"name":"Diff","inherit":true},"15621a8dc918.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"text","x":[4,4,4],"y":[5,0,5],"z":[3,0,0],"text":["y","ŷ₀","ŷ₁"],"textfont":{"size":18,"color":"black","family":"Arial Black"},"textposition":["top center","top center","bottom center"],"showlegend":false,"inherit":true},"15621a8dc918.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"text","x":[4,4],"y":[2.5,5],"z":[1.5,1.5],"text":["e₀","e₁"],"textfont":{"size":14,"color":"darkblue"},"textposition":"middle right","showlegend":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":50,"r":10},"scene":{"xaxis":{"title":"j3 (Intercept)","titlefont":{"size":20}},"yaxis":{"title":"x1 (Predictor)","titlefont":{"size":15}},"zaxis":{"title":"Error","titlefont":{"size":15}},"aspectmode":"data","camera":{"eye":{"x":1.5,"y":1.5,"z":0.5}}},"title":"Geometric Interpretation: Aligned View","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[0,8],"y":[0,0],"z":[0,0],"type":"scatter3d","mode":"lines","line":{"color":"gray","width":5},"name":"M₀ Subspace (j₃)","hoverinfo":["none","none"],"marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"z<br />z<br />z<br />z<br />z<br />z<br />z","ticklen":2},"colorscale":[[0,"lightgrey"],[1,"lightgrey"]],"showscale":false,"z":[[0,0],[0,0]],"type":"surface","x":[0,8],"y":[0,8],"opacity":0.10000000000000001,"name":"M₁ Subspace","frame":null},{"x":[0,4],"y":[0,5],"z":[0,3],"type":"scatter3d","mode":"lines+markers","line":{"color":"blue","width":10},"marker":{"color":"blue","size":5,"line":{"color":"rgba(44,160,44,1)"}},"name":"y (Observed)","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"frame":null},{"x":[0,4],"y":[0,0],"z":[0,0],"type":"scatter3d","mode":"lines+markers","line":{"color":"red","width":10},"marker":{"color":"red","size":5,"line":{"color":"rgba(214,39,40,1)"}},"name":"ŷ₀","error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"frame":null},{"x":[0,4],"y":[0,5],"z":[0,0],"type":"scatter3d","mode":"lines+markers","line":{"color":"darkgreen","width":10},"marker":{"color":"darkgreen","size":5,"line":{"color":"rgba(148,103,189,1)"}},"name":"ŷ₁","error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"frame":null},{"x":[4,4],"y":[0,5],"z":[0,3],"type":"scatter3d","mode":"lines","line":{"color":"red","width":5,"dash":"solid"},"name":"e₀ (RSS₀)","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"frame":null},{"x":[4,4],"y":[5,5],"z":[0,3],"type":"scatter3d","mode":"lines","line":{"color":"darkgreen","width":5,"dash":"solid"},"name":"e₁ (RSS₁)","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"x":[4,4],"y":[0,5],"z":[0,0],"type":"scatter3d","mode":"lines","line":{"color":"orange","width":6,"dash":"dot"},"name":"Diff","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"frame":null},{"type":"scatter3d","mode":"text","x":[4,4,4],"y":[5,0,5],"z":[3,0,0],"text":["y","ŷ₀","ŷ₁"],"textfont":{"size":18,"color":"black","family":"Arial Black"},"textposition":["top center","top center","bottom center"],"showlegend":false,"marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"line":{"color":"rgba(188,189,34,1)"},"frame":null},{"type":"scatter3d","mode":"text","x":[4,4],"y":[2.5,5],"z":[1.5,1.5],"text":["e₀","e₁"],"textfont":{"size":14,"color":"darkblue"},"textposition":["middle right","middle right"],"showlegend":false,"marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"line":{"color":"rgba(23,190,207,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-geometry-simple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)
</figcaption>
</figure>
</div>
</div>
<p>The geometric perspective is not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:</p>
<ul>
<li><strong>Statistical Clarity:</strong> Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.</li>
<li><strong>Computational Stability:</strong> A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic “Normal Equations” (<span class="math inline">\((X'X)^{-1}X'y\)</span>) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.</li>
<li><strong>Generalizability:</strong> The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.</li>
</ul>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="projection-in-vector-space" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Projection in Vector Space</h1>
<section id="vector-and-projection-onto-a-line" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="vector-and-projection-onto-a-line"><span class="header-section-number">2.1</span> Vector and Projection onto a Line</h2>
<section id="vectors-and-operations" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="vectors-and-operations"><span class="header-section-number">2.1.1</span> Vectors and Operations</h3>
<p>The concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.</p>
<div id="def-vector" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Vector)</strong></span> A <strong>vector</strong> <span class="math inline">\(x\)</span> is defined as a point in <span class="math inline">\(n\)</span>-dimensional space (<span class="math inline">\(\mathbb{R}^n\)</span>). It is typically represented as a column vector containing <span class="math inline">\(n\)</span> real-valued components: <span class="math display">\[
x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
\]</span></p>
</div>
<p>Vectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.</p>
<p><strong>Vector Arithmetic:</strong> Vectors can be manipulated geometrically:</p>
<div id="def-vector-addition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Vector Addition)</strong></span> The sum of two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of <span class="math inline">\(y\)</span> at the head of <span class="math inline">\(x\)</span>. <span class="math display">\[
x + y = \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}
\]</span></p>
</div>
<div id="def-vector-subtraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Vector Subtraction)</strong></span> The difference <span class="math inline">\(d = y - x\)</span> is the vector that “closes the triangle” formed by <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. It represents the displacement vector that connects the tip of <span class="math inline">\(x\)</span> to the tip of <span class="math inline">\(y\)</span>, such that <span class="math inline">\(x + d = y\)</span>.</p>
</div>
</section>
<section id="scalar-multiplication-and-distance" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="scalar-multiplication-and-distance"><span class="header-section-number">2.1.2</span> Scalar Multiplication and Distance</h3>
<p>In addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.</p>
<div id="def-scalar-mult" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Scalar Multiplication)</strong></span> Multiplying a vector by a scalar <span class="math inline">\(c\)</span> scales its magnitude (length) without changing its line of direction. If <span class="math inline">\(c\)</span> is positive, the direction remains the same; if <span class="math inline">\(c\)</span> is negative, the direction is reversed. <span class="math display">\[
c x = \begin{pmatrix} c x_1 \\ \vdots \\ c x_n \end{pmatrix}
\]</span></p>
</div>
<p>We often need to quantify the “size” of a vector. This is done using the concept of length, or norm.</p>
<div id="def-euclidean-distance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Euclidean Distance (Length))</strong></span> The length (or norm) of a vector <span class="math inline">\(x = (x_1, \dots, x_n)^T\)</span> corresponds to the straight-line distance from the origin to the point defined by <span class="math inline">\(x\)</span>. It is defined as the square root of the sum of squared components: <span class="math display">\[
||x||^2 = \sum_{i=1}^n x_i^2
\]</span></p>
<p><span class="math display">\[
||x|| = \sqrt{\sum_{i=1}^n x_i^2}
\]</span></p>
</div>
</section>
<section id="angle-and-inner-product" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="angle-and-inner-product"><span class="header-section-number">2.1.3</span> Angle and Inner Product</h3>
<p>To understand the relationship between two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and their difference <span class="math inline">\(y-x\)</span>. By applying the classic <strong>Law of Cosines</strong> to this triangle, we can relate the geometric angle to the vector lengths.</p>
<div id="thm-law-of-cosines" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Law of Cosines)</strong></span> For a triangle with sides <span class="math inline">\(a, b, c\)</span> and angle <span class="math inline">\(\theta\)</span> opposite to side <span class="math inline">\(c\)</span>: <span class="math display">\[
c^2 = a^2 + b^2 - 2ab \cos \theta
\]</span></p>
</div>
<p>Translating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get: <span class="math display">\[
||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \cdot ||y|| \cos \theta
\]</span></p>
<p>This equation provides a critical link between the geometric angle <span class="math inline">\(\theta\)</span> and the algebraic norms of the vectors.</p>
<p><strong>Derivation of Inner Product</strong></p>
<p>We can express the squared distance term <span class="math inline">\(||y - x||^2\)</span> purely algebraically by expanding the components:</p>
<p><span class="math display">\[
||y - x||^2 = \sum_{i=1}^n (x_i - y_i)^2
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)
\]</span></p>
<p><span class="math display">\[
= ||x||^2 + ||y||^2 - 2 \sum_{i=1}^n x_i y_i
\]</span></p>
<p>By comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the <strong>Inner Product</strong> (or dot product).</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (Inner Product)</strong></span> The inner product of two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is defined as the sum of the products of their corresponding components: <span class="math display">\[
x'y = \sum_{i=1}^n x_i y_i = \langle x, y \rangle
\]</span></p>
</div>
<p>Thus, equating the geometric and algebraic forms yields the fundamental relationship: <span class="math display">\[
x'y = ||x|| \cdot ||y|| \cos \theta
\]</span></p>
</section>
<section id="coordinate-scalar-projection" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="coordinate-scalar-projection"><span class="header-section-number">2.1.4</span> Coordinate (Scalar) Projection</h3>
<p>The inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector <span class="math inline">\(y\)</span> onto vector <span class="math inline">\(x\)</span>.</p>
<p>The length of this projection is given by:</p>
<p><span class="math display">\[
||y|| \cos \theta = \frac{x'y}{||x||}
\]</span></p>
<p>This expression can be interpreted as the inner product of <span class="math inline">\(y\)</span> with the normalized (unit) vector in the direction of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\text{Scalar Projection} = \left\langle \frac{x}{||x||}, y \right\rangle
\]</span></p>
</section>
<section id="vector-projection-formula" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="vector-projection-formula"><span class="header-section-number">2.1.5</span> Vector Projection Formula</h3>
<p>The scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.</p>
<div id="def-vector-projection" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Vector Projection)</strong></span> The projection of vector <span class="math inline">\(y\)</span> onto vector <span class="math inline">\(x\)</span>, denoted <span class="math inline">\(\hat{y}\)</span>, is calculated as: <span class="math display">\[
\text{Projection Vector} = (\text{Length}) \cdot (\text{Direction})
\]</span></p>
<p><span class="math display">\[
\hat{y} = \left( \frac{x'y}{||x||} \right) \cdot \frac{x}{||x||}
\]</span></p>
<p>This is often written compactly by combining the denominators:</p>
<p><span class="math display">\[
\hat{y} = \frac{x'y}{||x||^2} x
\]</span></p>
</div>
</section>
<section id="perpendicularity-orthogonality" class="level3" data-number="2.1.6">
<h3 data-number="2.1.6" class="anchored" data-anchor-id="perpendicularity-orthogonality"><span class="header-section-number">2.1.6</span> Perpendicularity (Orthogonality)</h3>
<p>A special case of the angle between vectors arises when <span class="math inline">\(\theta = 90^\circ\)</span>. This geometric concept of perpendicularity is central to the theory of projections and least squares.</p>
<div id="def-perpendicularity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 (Perpendicularity)</strong></span> Two vectors are defined as <strong>perpendicular</strong> (or orthogonal) if the angle between them is <span class="math inline">\(90^\circ\)</span> (<span class="math inline">\(\pi/2\)</span>).</p>
<p>Since <span class="math inline">\(\cos(90^\circ) = 0\)</span>, the condition for orthogonality simplifies to the inner product being zero:</p>
<p><span class="math display">\[
x'y = 0 \iff x \perp y
\]</span></p>
</div>
<div id="exm-orthogonal-vectors" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Orthogonal Vectors)</strong></span> Consider two vectors in <span class="math inline">\(\mathbb{R}^2\)</span>: <span class="math inline">\(x = (1, 1)'\)</span> and <span class="math inline">\(y = (1, -1)'\)</span>. <span class="math display">\[
x'y = 1(1) + 1(-1) = 1 - 1 = 0
\]</span></p>
<p>Since their inner product is zero, these vectors are orthogonal to each other.</p>
</div>
</section>
<section id="projection-onto-a-line-subspace" class="level3" data-number="2.1.7">
<h3 data-number="2.1.7" class="anchored" data-anchor-id="projection-onto-a-line-subspace"><span class="header-section-number">2.1.7</span> Projection onto a Line (Subspace)</h3>
<p>We can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.</p>
<div id="def-line-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9 (Line Spanned by a Vector)</strong></span> The line space <span class="math inline">\(L(x)\)</span>, or the space spanned by a vector <span class="math inline">\(x\)</span>, is defined as the set of all scalar multiples of <span class="math inline">\(x\)</span>: <span class="math display">\[
L(x) = \{ cx \mid c \in \mathbb{R} \}
\]</span></p>
</div>
<p>The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(L(x)\)</span>, denoted <span class="math inline">\(\hat{y}\)</span>, is defined by the geometric property that it is the closest point on the line to <span class="math inline">\(y\)</span>. This implies that the error vector (or residual) must be perpendicular to the line itself.</p>
<div id="def-projection-line" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10 (Projection onto a Line)</strong></span> A vector <span class="math inline">\(\hat{y}\)</span> is the projection of <span class="math inline">\(y\)</span> onto the line <span class="math inline">\(L(x)\)</span> if:</p>
<ol type="1">
<li><p><span class="math inline">\(\hat{y}\)</span> lies on the line <span class="math inline">\(L(x)\)</span> (i.e., <span class="math inline">\(\hat{y} = cx\)</span> for some scalar <span class="math inline">\(c\)</span>).</p></li>
<li><p>The residual vector <span class="math inline">\((y - \hat{y})\)</span> is perpendicular to the direction vector <span class="math inline">\(x\)</span>.</p></li>
</ol>
</div>
<p><strong>Derivation:</strong> To find the value of the scalar <span class="math inline">\(c\)</span>, we apply the orthogonality condition: <span class="math display">\[
(y - \hat{y}) \perp x \implies x'(y - cx) = 0
\]</span></p>
<p>Expanding this inner product gives:</p>
<p><span class="math display">\[
x'y - c(x'x) = 0
\]</span></p>
<p>Solving for <span class="math inline">\(c\)</span>, we obtain:</p>
<p><span class="math display">\[
c = \frac{x'y}{||x||^2}
\]</span></p>
<p>This confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.</p>
<p><strong>Alternative Forms of the Projection Formula</strong></p>
<p>We can express the projection vector <span class="math inline">\(\hat{y}\)</span> in several equivalent ways to highlight different geometric interpretations.</p>
<div id="def-projection-formulae" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 11 (Forms of Projection)</strong></span> The projection of <span class="math inline">\(y\)</span> onto the vector <span class="math inline">\(x\)</span> is given by: <span class="math display">\[
\hat{y} = \frac{x'y}{||x||^2} x = \left\langle y, \frac{x}{||x||} \right\rangle \frac{x}{||x||}
\]</span></p>
<p>This second form separates the components into:</p>
<p><span class="math display">\[
\text{Projection} = (\text{Scalar Projection}) \times (\text{Unit Direction})
\]</span></p>
</div>
</section>
<section id="projection-matrix-p_x" class="level3" data-number="2.1.8">
<h3 data-number="2.1.8" class="anchored" data-anchor-id="projection-matrix-p_x"><span class="header-section-number">2.1.8</span> Projection Matrix (<span class="math inline">\(P_x\)</span>)</h3>
<p>In linear models, it is often more convenient to view projection as a linear transformation applied to the vector <span class="math inline">\(y\)</span>. This allows us to define a <strong>Projection Matrix</strong>.</p>
<p>We can rewrite the formula for <span class="math inline">\(\hat{y}\)</span> by factoring out <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\hat{y} = \text{proj}(y|x) = x \frac{x'y}{||x||^2} = \frac{xx'}{||x||^2} y
\]</span></p>
<p>This leads to the definition of the projection matrix <span class="math inline">\(P_x\)</span>.</p>
<div id="def-projection-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 12 (Projection Matrix onto a Single Vector)</strong></span> The matrix <span class="math inline">\(P_x\)</span> that projects any vector <span class="math inline">\(y\)</span> onto the line spanned by <span class="math inline">\(x\)</span> is defined as: <span class="math display">\[
P_x = \frac{xx'}{||x||^2}
\]</span></p>
<p>Using this matrix, the projection is simply:</p>
<p><span class="math display">\[
\hat{y} = P_x y
\]</span></p>
<p>If <span class="math inline">\(x \in \mathbb{R}^n\)</span>, then <span class="math inline">\(P_x\)</span> is a <span class="math inline">\(n \times n\)</span> symmetric matrix.</p>
</div>
<p>Let’s apply these concepts to a concrete example.</p>
<div id="exm-projection-r2" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Numerical Projection)</strong></span> Let <span class="math inline">\(y = (1, 3)'\)</span> and <span class="math inline">\(x = (1, 1)'\)</span>. We want to find the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span>.</p>
<p><strong>Method 1: Using the Vector Formula</strong> First, calculate the inner products:</p>
<p><span class="math display">\[
x'y = 1(1) + 1(3) = 4
\]</span> <span class="math display">\[
||x||^2 = 1^2 + 1^2 = 2
\]</span></p>
<p>Now, apply the formula:</p>
<p><span class="math display">\[
\hat{y} = \frac{4}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 2 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
\]</span></p>
<p><strong>Method 2: Using the Projection Matrix</strong> Construct the matrix <span class="math inline">\(P_x\)</span>:</p>
<p><span class="math display">\[
P_x = \frac{1}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} = \begin{pmatrix} 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 \end{pmatrix}
\]</span></p>
<p>Multiply by <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\hat{y} = P_x y = \begin{pmatrix} 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \end{pmatrix} = \begin{pmatrix} 0.5(1) + 0.5(3) \\ 0.5(1) + 0.5(3) \end{pmatrix} = \begin{pmatrix} 2 \\ 2 \end{pmatrix}
\]</span></p>
</div>
<p><strong>Example: Projection onto the Ones Vector (</strong><span class="math inline">\(j_n\)</span>)</p>
<p>A very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.</p>
<div id="exm-mean-projection" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (Projection onto the Ones Vector)</strong></span> Let <span class="math inline">\(y = (y_1, \dots, y_n)'\)</span> be a data vector. Let <span class="math inline">\(j_n = (1, 1, \dots, 1)'\)</span> be a vector of all ones.</p>
<p>The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(j_n\)</span> is:</p>
<p><span class="math display">\[
\text{proj}(y|j_n) = \frac{j_n' y}{||j_n||^2} j_n
\]</span></p>
<p>Calculating the components:</p>
<p><span class="math display">\[
j_n' y = \sum_{i=1}^n y_i \quad \text{(Sum of observations)}
\]</span> <span class="math display">\[
||j_n||^2 = \sum_{i=1}^n 1^2 = n
\]</span></p>
<p>Substituting these back:</p>
<p><span class="math display">\[
\hat{y} = \frac{\sum y_i}{n} j_n = \bar{y} j_n = \begin{pmatrix} \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
\]</span></p>
<p>Thus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.</p>
</div>
</section>
<section id="pythagorean-theorem" class="level3" data-number="2.1.9">
<h3 data-number="2.1.9" class="anchored" data-anchor-id="pythagorean-theorem"><span class="header-section-number">2.1.9</span> Pythagorean Theorem</h3>
<p>The Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.</p>
<div id="thm-pythagorean" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Pythagorean Theorem)</strong></span> If two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are orthogonal (i.e., <span class="math inline">\(x \perp y\)</span> or <span class="math inline">\(x'y = 0\)</span>), then the squared length of their sum is equal to the sum of their squared lengths: <span class="math display">\[
||x + y||^2 = ||x||^2 + ||y||^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We expand the squared norm using the inner product: <span class="math display">\[
\begin{aligned}
||x + y||^2 &amp;= (x + y)' (x + y) \\
&amp;= x'x + x'y + y'x + y'y \\
&amp;= ||x||^2 + 2x'y + ||y||^2
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(x \perp y\)</span>, the inner product <span class="math inline">\(x'y = 0\)</span>. Thus, the term <span class="math inline">\(2x'y\)</span> vanishes, leaving:</p>
<p><span class="math display">\[
||x + y||^2 = ||x||^2 + ||y||^2
\]</span></p>
</div>
<p>The proof after defining inner product to represent <span class="math inline">\(\cos(\theta)\)</span> is trivival. <a href="#fig-pythagoras-proof" class="quarto-xref">Figure&nbsp;2</a> shows a geometric proof of the fundamental Pythagorean Theorem.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pythagoras-proof" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 80% !important;" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pythagoras-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-pythagoras-proof-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pythagoras-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Proof of Pythagorean Theorem using Area Scaling
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="least-square-property" class="level3" data-number="2.1.10">
<h3 data-number="2.1.10" class="anchored" data-anchor-id="least-square-property"><span class="header-section-number">2.1.10</span> Least Square Property</h3>
<p>One of the most important properties of the orthogonal projection is that it minimizes the distance between the vector <span class="math inline">\(y\)</span> and the subspace (or line) onto which it is projected.</p>
<div id="thm-shortest-distance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Least Square Property)</strong></span> Let <span class="math inline">\(\hat{y}\)</span> be the projection of <span class="math inline">\(y\)</span> onto the line <span class="math inline">\(L(x)\)</span>. For any other vector <span class="math inline">\(y^*\)</span> on the line <span class="math inline">\(L(x)\)</span>, the distance from <span class="math inline">\(y\)</span> to <span class="math inline">\(y^*\)</span> is always greater than or equal to the distance from <span class="math inline">\(y\)</span> to <span class="math inline">\(\hat{y}\)</span>. <span class="math display">\[
||y - y^*|| \ge ||y - \hat{y}||
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since both <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y^*\)</span> lie on the line <span class="math inline">\(L(x)\)</span>, their difference <span class="math inline">\((\hat{y} - y^*)\)</span> also lies on <span class="math inline">\(L(x)\)</span>. From the definition of projection, the residual <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to the line <span class="math inline">\(L(x)\)</span>. Therefore: <span class="math display">\[
(y - \hat{y}) \perp (\hat{y} - y^*)
\]</span></p>
<p>We can write the vector <span class="math inline">\((y - y^*)\)</span> as:</p>
<p><span class="math display">\[
y - y^* = (y - \hat{y}) + (\hat{y} - y^*)
\]</span></p>
<p>Applying the Pythagorean Theorem:</p>
<p><span class="math display">\[
||y - y^*||^2 = ||y - \hat{y}||^2 + ||\hat{y} - y^*||^2
\]</span></p>
<p>Since <span class="math inline">\(||\hat{y} - y^*||^2 \ge 0\)</span>, it follows that:</p>
<p><span class="math display">\[
||y - y^*||^2 \ge ||y - \hat{y}||^2
\]</span></p>
</div>
</section>
</section>
<section id="vector-space" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="vector-space"><span class="header-section-number">2.2</span> Vector Space</h2>
<p>We now generalize our discussion from lines to broader spaces.</p>
<div id="def-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 13 (Vector Space)</strong></span> A set <span class="math inline">\(V \subseteq \mathbb{R}^n\)</span> is called a <strong>Vector Space</strong> if it is closed under vector addition and scalar multiplication:</p>
<ol type="1">
<li><strong>Closed under Addition:</strong> If <span class="math inline">\(x_1 \in V\)</span> and <span class="math inline">\(x_2 \in V\)</span>, then <span class="math inline">\(x_1 + x_2 \in V\)</span>.</li>
<li><strong>Closed under Scalar Multiplication:</strong> If <span class="math inline">\(x \in V\)</span>, then <span class="math inline">\(cx \in V\)</span> for any scalar <span class="math inline">\(c \in \mathbb{R}\)</span>.</li>
</ol>
</div>
<p>It follows that the zero vector <span class="math inline">\(0\)</span> must belong to any subspace (by choosing <span class="math inline">\(c=0\)</span>).</p>
<section id="spanned-vector-space" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="spanned-vector-space"><span class="header-section-number">2.2.1</span> Spanned Vector Space</h3>
<p>The most common way to construct a vector space in linear models is by spanning it with a set of vectors.</p>
<div id="def-spanned-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 14 (Spanned Vector Space)</strong></span> Let <span class="math inline">\(x_1, \dots, x_p\)</span> be a set of vectors in <span class="math inline">\(\mathbb{R}^n\)</span>. The space spanned by these vectors, denoted <span class="math inline">\(L(x_1, \dots, x_p)\)</span>, is the set of all possible linear combinations of them: <span class="math display">\[
L(x_1, \dots, x_p) = \{ r \mid r = c_1 x_1 + \dots + c_p x_p, \text{ for } c_i \in \mathbb{R} \}
\]</span></p>
</div>
</section>
<section id="column-space-and-row-space" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="column-space-and-row-space"><span class="header-section-number">2.2.2</span> Column Space and Row Space</h3>
<p>When vectors are arranged into a matrix, we define specific spaces based on their columns and rows.</p>
<div id="def-column-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 15 (Column Space)</strong></span> For a matrix <span class="math inline">\(X = (x_1, \dots, x_p)\)</span>, the <strong>Column Space</strong>, denoted <span class="math inline">\(\text{Col}(X)\)</span>, is the vector space spanned by its columns: <span class="math display">\[
\text{Col}(X) = L(x_1, \dots, x_p)
\]</span></p>
</div>
<div id="def-row-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16 (Row Space)</strong></span> The <strong>Row Space</strong>, denoted <span class="math inline">\(\text{Row}(X)\)</span>, is the vector space spanned by the rows of the matrix <span class="math inline">\(X\)</span>.</p>
</div>
</section>
<section id="linear-independence-and-rank" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="linear-independence-and-rank"><span class="header-section-number">2.2.3</span> Linear Independence and Rank</h3>
<p>Not all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.</p>
<div id="def-linear-independence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 17 (Linear Independence)</strong></span> A set of vectors <span class="math inline">\(x_1, \dots, x_p\)</span> is said to be <strong>Linearly Independent</strong> if the only solution to the linear combination equation equal to zero is the trivial solution: <span class="math display">\[
\sum_{i=1}^p c_i x_i = 0 \implies c_1 = c_2 = \dots = c_p = 0
\]</span></p>
<p>If there exist non-zero <span class="math inline">\(c_i\)</span>’s such that sum is zero, the vectors are <strong>Linearly Dependent</strong>.</p>
</div>
</section>
</section>
<section id="rank-of-matrices-and-dim-of-vector-space" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="rank-of-matrices-and-dim-of-vector-space"><span class="header-section-number">2.3</span> Rank of Matrices and Dim of Vector Space</h2>
<div id="def-rank" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18 (Rank)</strong></span> The <strong>Rank</strong> of a matrix <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\text{Rank}(X)\)</span>, is the maximum number of linearly independent columns in <span class="math inline">\(X\)</span>. This is equivalent to the dimension of the column space: <span class="math display">\[
\text{Rank}(X) = \text{Dim}(\text{Col}(X))
\]</span></p>
</div>
<p>There are several fundamental properties regarding the rank of a matrix.</p>
<div id="exm-row-rank-equal-col-rank" class="theorem example">
<p><span class="theorem-title"><strong>Example 4 (Example of the Equality of Row and Col Rank)</strong></span> Consider the following <span class="math inline">\(3 \times 4\)</span> matrix (<span class="math inline">\(n=3, p=4\)</span>): <span class="math display">\[
X = \begin{pmatrix}
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{pmatrix}
\]</span> Notice that the third row is the sum of the first two (<span class="math inline">\(r_3 = r_1 + r_2\)</span>).</p>
<ol type="1">
<li>Row Rank and Basis <span class="math inline">\(U\)</span> The first two rows are linearly independent. We set the row rank <span class="math inline">\(r=2\)</span> and use these rows as our basis matrix <span class="math inline">\(U\)</span> (<span class="math inline">\(2 \times 4\)</span>):</li>
</ol>
<p><span class="math display">\[
U = \begin{pmatrix}
1 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 1
\end{pmatrix}
\]</span></p>
<ol start="2" type="1">
<li><p>Coefficient Matrix <span class="math inline">\(C\)</span> We express every row of <span class="math inline">\(X\)</span> as a linear combination of the rows of <span class="math inline">\(U\)</span>:</p>
<ul>
<li>Row 1: <span class="math inline">\(1 \cdot u_1 + 0 \cdot u_2\)</span></li>
<li>Row 2: <span class="math inline">\(0 \cdot u_1 + 1 \cdot u_2\)</span></li>
<li>Row 3: <span class="math inline">\(1 \cdot u_1 + 1 \cdot u_2\)</span></li>
</ul></li>
</ol>
<p>These coefficients form the matrix <span class="math inline">\(C\)</span> (<span class="math inline">\(3 \times 2\)</span>):</p>
<p><span class="math display">\[
C = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
1 &amp; 1
\end{pmatrix}
\]</span></p>
<ol type="1">
<li><p>The Decomposition <span class="math inline">\(X = CU\)</span>) We verify that <span class="math inline">\(X\)</span> is the product of <span class="math inline">\(C\)</span> and <span class="math inline">\(U\)</span>: <span class="math display">\[
\underbrace{\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \end{pmatrix}}_{X \ (3 \times 4)}
=
\underbrace{\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}}_{C \ (3 \times 2)}
\underbrace{\begin{pmatrix} 1 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 1 \end{pmatrix}}_{U \ (2 \times 4)}
\]</span></p></li>
<li><p>Conclusion on Column Rank The columns of <span class="math inline">\(X\)</span> are linear combinations of the columns of <span class="math inline">\(C\)</span>. <span class="math display">\[
\text{Col}(X) \subseteq \text{Col}(C)
\]</span> Since <span class="math inline">\(C\)</span> has only 2 columns, the dimension of its column space (and thus <span class="math inline">\(X\)</span>’s column space) cannot exceed 2. <span class="math display">\[
\text{Dim}(\text{Col}(X)) \le 2
\]</span> This confirms that Row Rank (2) <span class="math inline">\(\ge\)</span> Column Rank. (By symmetry, they are equal).</p></li>
</ol>
</div>
<div id="thm-rank-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Row Rank equals Column Rank)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Row Rank equals Column Rank:</strong> The dimension of the column space is equal to the dimension of the row space.</li>
</ol>
<p><span class="math display">\[
    \text{Dim}(\text{Col}(X)) = \text{Dim}(\text{Row}(X)) \implies \text{Rank}(X) = \text{Rank}(X')
    \]</span></p>
<ol start="2" type="1">
<li><strong>Bounds:</strong> For an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
    \text{Rank}(X) \le \min(n, p)
    \]</span></p>
</div>
<section id="orthogonality-to-a-subspace" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="orthogonality-to-a-subspace"><span class="header-section-number">2.3.1</span> Orthogonality to a Subspace</h3>
<p>We can extend the concept of orthogonality from single vectors to entire subspaces.</p>
<div id="def-orth-subspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19 (Orthogonality to a Subspace)</strong></span> A vector <span class="math inline">\(y\)</span> is orthogonal to a subspace <span class="math inline">\(V\)</span> (denoted <span class="math inline">\(y \perp V\)</span>) if <span class="math inline">\(y\)</span> is orthogonal to <strong>every</strong> vector <span class="math inline">\(x\)</span> in <span class="math inline">\(V\)</span>. <span class="math display">\[
y \perp V \iff y'x = 0 \quad \forall x \in V
\]</span></p>
</div>
<div id="def-orthogonal-complement" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 20 (Orthogonal Complement)</strong></span> The set of all vectors that are orthogonal to a subspace <span class="math inline">\(V\)</span> is called the <strong>Orthogonal Complement</strong> of <span class="math inline">\(V\)</span>, denoted <span class="math inline">\(V^\perp\)</span>. <span class="math display">\[
V^\perp = \{ y \in \mathbb{R}^n \mid y \perp V \}
\]</span></p>
</div>
</section>
<section id="kernel-null-space-and-image" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kernel-null-space-and-image"><span class="header-section-number">2.3.2</span> Kernel (Null Space) and Image</h3>
<p>For a matrix transformation defined by <span class="math inline">\(X\)</span>, we define two key spaces: the Image (Column Space) and the Kernel (Null Space).</p>
<div id="def-image-kernel" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 21 (Image and Kernel)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Image (Column Space):</strong> The set of all possible outputs.</li>
</ol>
<p><span class="math display">\[
    \text{Im}(X) = \text{Col}(X) = \{ X\beta \mid \beta \in \mathbb{R}^p \}
    \]</span></p>
<ol start="2" type="1">
<li><strong>Kernel (Null Space):</strong> The set of all inputs mapped to the zero vector.</li>
</ol>
<p><span class="math display">\[
    \text{Ker}(X) = \{ \beta \in \mathbb{R}^p \mid X\beta = 0 \}
    \]</span></p>
</div>
<div id="thm-kernel-rowspace" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Relationship between Kernel and Row Space)</strong></span> The kernel of <span class="math inline">\(X\)</span> is the orthogonal complement of the row space of <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Ker}(X) = [\text{Row}(X)]^\perp
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(x \in \mathbb{R}^p\)</span>. <span class="math inline">\(x \in \text{Ker}(X)\)</span> if and only if <span class="math inline">\(Xx = 0\)</span>. If we denote the rows of <span class="math inline">\(X\)</span> as <span class="math inline">\(r_1', \dots, r_n'\)</span>, then the equation <span class="math inline">\(Xx = 0\)</span> is equivalent to the system of equations: <span class="math display">\[
\begin{pmatrix} r_1' \\ \vdots \\ r_n' \end{pmatrix} x = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix} \iff r_i' x = 0 \text{ for all } i = 1, \dots, n
\]</span> This means <span class="math inline">\(x\)</span> is orthogonal to every row of <span class="math inline">\(X\)</span>. Since the rows span the row space <span class="math inline">\(\text{Row}(X)\)</span>, being orthogonal to every generator <span class="math inline">\(r_i\)</span> implies <span class="math inline">\(x\)</span> is orthogonal to the entire space <span class="math inline">\(\text{Row}(X)\)</span>. Thus, <span class="math inline">\(\text{Ker}(X) = \{ x \mid x \perp \text{Row}(X) \} = [\text{Row}(X)]^\perp\)</span>.</p>
</div>
</section>
<section id="nullity-theorem" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="nullity-theorem"><span class="header-section-number">2.3.3</span> Nullity Theorem</h3>
<p>There is a fundamental relationship between the dimensions of these spaces.</p>
<div id="thm-nullity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Rank-Nullity Theorem)</strong></span> For an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Rank}(X) + \text{Nullity}(X) = p
\]</span> where <span class="math inline">\(\text{Nullity}(X) = \text{Dim}(\text{Ker}(X))\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the previous theorem, we established that the kernel is the orthogonal complement of the row space: <span class="math display">\[
\text{Ker}(X) = [\text{Row}(X)]^\perp
\]</span></p>
<p>Since the row space is a subspace of <span class="math inline">\(\mathbb{R}^p\)</span>, the entire space can be decomposed into the direct sum of the row space and its orthogonal complement:</p>
<p><span class="math display">\[
\mathbb{R}^p = \text{Row}(X) \oplus [\text{Row}(X)]^\perp = \text{Row}(X) \oplus \text{Ker}(X)
\]</span></p>
<p>Taking the dimensions of these spaces:</p>
<p><span class="math display">\[
\text{Dim}(\mathbb{R}^p) = \text{Dim}(\text{Row}(X)) + \text{Dim}(\text{Ker}(X))
\]</span></p>
<p>Substituting the definitions of Rank (dimension of row/column space) and Nullity:</p>
<p><span class="math display">\[
p = \text{Rank}(X) + \text{Nullity}(X)
\]</span></p>
</div>
<p><strong>Comparing Ranks via Kernel Containment</strong></p>
<p>The Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (with the same number of columns) by inspecting their null spaces.</p>
<div id="thm-rank-kernel" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (Kernel Containment and Rank Inequality)</strong></span> Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two matrices with <span class="math inline">\(p\)</span> columns. If the kernel of <span class="math inline">\(A\)</span> is contained within the kernel of <span class="math inline">\(B\)</span>, then the rank of <span class="math inline">\(A\)</span> is greater than or equal to the rank of <span class="math inline">\(B\)</span>. <span class="math display">\[
\text{Ker}(A) \subseteq \text{Ker}(B) \implies \text{Rank}(A) \ge \text{Rank}(B)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the subspace inclusion <span class="math inline">\(\text{Ker}(A) \subseteq \text{Ker}(B)\)</span>, it follows that the dimension of the smaller space cannot exceed the dimension of the larger space: <span class="math display">\[
\text{Nullity}(A) \le \text{Nullity}(B)
\]</span> Using the Rank-Nullity Theorem (<span class="math inline">\(\text{Rank} = p - \text{Nullity}\)</span>), we reverse the inequality: <span class="math display">\[
p - \text{Nullity}(A) \ge p - \text{Nullity}(B)
\]</span> <span class="math display">\[
\text{Rank}(A) \ge \text{Rank}(B)
\]</span></p>
</div>
</section>
<section id="rank-inequalities" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="rank-inequalities"><span class="header-section-number">2.3.4</span> Rank Inequalities</h3>
<p>Understanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.</p>
<div id="thm-rank-product" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (Rank of a Matrix Product)</strong></span> Let <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix and <span class="math inline">\(Z\)</span> be a <span class="math inline">\(p \times k\)</span> matrix. The rank of their product <span class="math inline">\(XZ\)</span> is bounded by the rank of the individual matrices: <span class="math display">\[
\text{Rank}(XZ) \le \min(\text{Rank}(X), \text{Rank}(Z))
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The columns of <span class="math inline">\(XZ\)</span> are linear combinations of the columns of <span class="math inline">\(X\)</span>. Thus, the column space of <span class="math inline">\(XZ\)</span> is a subspace of the column space of <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Col}(XZ) \subseteq \text{Col}(X) \implies \text{Rank}(XZ) \le \text{Rank}(X)
\]</span> Similarly, the rows of <span class="math inline">\(XZ\)</span> are linear combinations of the rows of <span class="math inline">\(Z\)</span>. Thus, the row space of <span class="math inline">\(XZ\)</span> is a subspace of the row space of <span class="math inline">\(Z\)</span>: <span class="math display">\[
\text{Row}(XZ) \subseteq \text{Row}(Z) \implies \text{Rank}(XZ) \le \text{Rank}(Z)
\]</span></p>
</div>
<p><strong>Rank and Invertible Matrices</strong></p>
<p>Multiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.</p>
<div id="thm-rank-invertible" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Rank with Non-Singular Multiplication)</strong></span> Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> invertible matrix (i.e., <span class="math inline">\(\text{Rank}(A) = n\)</span>) and <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix. Then: <span class="math display">\[
\text{Rank}(AX) = \text{Rank}(X)
\]</span></p>
<p>Similarly, if <span class="math inline">\(B\)</span> is a <span class="math inline">\(p \times p\)</span> invertible matrix, then:</p>
<p><span class="math display">\[
\text{Rank}(XB) = \text{Rank}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From the previous theorem, we know <span class="math inline">\(\text{Rank}(AX) \le \text{Rank}(X)\)</span>. Since <span class="math inline">\(A\)</span> is invertible, we can write <span class="math inline">\(X = A^{-1}(AX)\)</span>. Applying the theorem again: <span class="math display">\[
\text{Rank}(X) = \text{Rank}(A^{-1}(AX)) \le \text{Rank}(AX)
\]</span> Thus, <span class="math inline">\(\text{Rank}(AX) = \text{Rank}(X)\)</span>.</p>
</div>
</section>
<section id="rank-of-xx-and-xx" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="rank-of-xx-and-xx"><span class="header-section-number">2.3.5</span> Rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span></h3>
<p>The matrix <span class="math inline">\(X'X\)</span> (the Gram matrix) appears in the normal equations for least squares (<span class="math inline">\(X'X\beta = X'y\)</span>). Its properties are closely tied to <span class="math inline">\(X\)</span>.</p>
<div id="thm-rank-gram" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 (Rank of Gram Matrix)</strong></span> For any real matrix <span class="math inline">\(X\)</span>, the rank of <span class="math inline">\(X'X\)</span> and <span class="math inline">\(XX'\)</span> is the same as the rank of <span class="math inline">\(X\)</span> itself: <span class="math display">\[
\text{Rank}(X'X) = \text{Rank}(X)
\]</span> <span class="math display">\[
\text{Rank}(XX') = \text{Rank}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first show that the null space (kernel) of <span class="math inline">\(X\)</span> is the same as the null space of <span class="math inline">\(X'X\)</span>. If <span class="math inline">\(v \in \text{Ker}(X)\)</span>, then <span class="math inline">\(Xv = 0 \implies X'Xv = 0 \implies v \in \text{Ker}(X'X)\)</span>. Conversely, if <span class="math inline">\(v \in \text{Ker}(X'X)\)</span>, then <span class="math inline">\(X'Xv = 0\)</span>. Multiply by <span class="math inline">\(v'\)</span>: <span class="math display">\[
v'X'Xv = 0 \implies (Xv)'(Xv) = 0 \implies ||Xv||^2 = 0 \implies Xv = 0
\]</span> So <span class="math inline">\(\text{Ker}(X) = \text{Ker}(X'X)\)</span>. By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.</p>
</div>
<p><strong>Column Space of</strong> <span class="math inline">\(XX'\)</span></p>
<p>Beyond just the rank, the column spaces themselves are related.</p>
<div id="thm-colspace-gram" class="theorem">
<p><span class="theorem-title"><strong>Theorem 11 (Column Space Equivalence)</strong></span> The column space of <span class="math inline">\(XX'\)</span> is identical to the column space of <span class="math inline">\(X\)</span>: <span class="math display">\[
\text{Col}(XX') = \text{Col}(X)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p><strong>Forward (</strong><span class="math inline">\(\subseteq\)</span>): Let <span class="math inline">\(z \in \text{Col}(XX')\)</span>. Then <span class="math inline">\(z = XX'w\)</span> for some vector <span class="math inline">\(w\)</span>. We can rewrite this as <span class="math inline">\(z = X(X'w)\)</span>. Since <span class="math inline">\(z\)</span> is a linear combination of columns of <span class="math inline">\(X\)</span> (with coefficients <span class="math inline">\(X'w\)</span>), <span class="math inline">\(z \in \text{Col}(X)\)</span>. Thus, <span class="math inline">\(\text{Col}(XX') \subseteq \text{Col}(X)\)</span>.</p></li>
<li><p><strong>Equality via Rank:</strong> From the previous theorem, we know that <span class="math inline">\(\text{Rank}(XX') = \text{Rank}(X)\)</span>. Since <span class="math inline">\(\text{Col}(XX')\)</span> is a subspace of <span class="math inline">\(\text{Col}(X)\)</span> and they have the same finite dimension (Rank), the subspaces must be identical.</p></li>
</ol>
</div>
<p><strong>Implication:</strong> This property ensures that for any <span class="math inline">\(y\)</span>, the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span> lies in the same space as the projection onto <span class="math inline">\(\text{Col}(XX')\)</span>. This is vital for the existence of solutions in generalized least squares.</p>
</section>
</section>
<section id="orthogonal-projection-onto-a-subspace" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="orthogonal-projection-onto-a-subspace"><span class="header-section-number">2.4</span> Orthogonal Projection onto a Subspace</h2>
<div name="Definition of Projection onto a Subspace $V$">
<p>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. For any vector <span class="math inline">\(y \in \mathbb{R}^n\)</span>, there exists a <strong>unique</strong> vector <span class="math inline">\(\hat{y} \in V\)</span> such that the residual is orthogonal to the subspace: <span class="math display">\[
(y - \hat{y}) \perp V
\]</span></p>
<p>Equivalently:</p>
<p><span class="math display">\[
\langle y - \hat{y}, v \rangle = 0 \quad \forall v \in V
\]</span></p>
</div>
<section id="equivalence-to-least-squares" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="equivalence-to-least-squares"><span class="header-section-number">2.4.1</span> Equivalence to Least Squares</h3>
<p>The geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).</p>
<div id="thm-best-approximation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 12 (Best Approximation Theorem (Least Squares Property))</strong></span> Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> and <span class="math inline">\(y \in \mathbb{R}^n\)</span>. Let <span class="math inline">\(\hat{y}\)</span> be the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V\)</span>. Then <span class="math inline">\(\hat{y}\)</span> is the closest point in <span class="math inline">\(V\)</span> to <span class="math inline">\(y\)</span>. That is, for any vector <span class="math inline">\(v \in V\)</span> such that <span class="math inline">\(v \ne \hat{y}\)</span>: <span class="math display">\[
\|y - \hat{y}\|^2 &lt; \|y - v\|^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(v\)</span> be any vector in <span class="math inline">\(V\)</span>. We can rewrite the difference vector <span class="math inline">\(y - v\)</span> by adding and subtracting the projection <span class="math inline">\(\hat{y}\)</span>: <span class="math display">\[
y - v = (y - \hat{y}) + (\hat{y} - v)
\]</span></p>
<p>Observe the properties of the two terms on the right-hand side:</p>
<ol type="1">
<li><strong>Residual:</strong> <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to <span class="math inline">\(V\)</span> by definition.</li>
<li><strong>Difference in Subspace:</strong> Since both <span class="math inline">\(\hat{y} \in V\)</span> and <span class="math inline">\(v \in V\)</span>, their difference <span class="math inline">\((\hat{y} - v)\)</span> is also in <span class="math inline">\(V\)</span>.</li>
</ol>
<p>Therefore, the two terms are orthogonal to each other:</p>
<p><span class="math display">\[
(y - \hat{y}) \perp (\hat{y} - v)
\]</span></p>
<p>Applying the Pythagorean Theorem:</p>
<p><span class="math display">\[
\|y - v\|^2 = \|y - \hat{y}\|^2 + \|\hat{y} - v\|^2
\]</span></p>
<p>Since squared norms are non-negative, and <span class="math inline">\(\|\hat{y} - v\|^2 &gt; 0\)</span> (because <span class="math inline">\(v \ne \hat{y}\)</span>):</p>
<p><span class="math display">\[
\|y - v\|^2 &gt; \|y - \hat{y}\|^2
\]</span> The projection <span class="math inline">\(\hat{y}\)</span> minimizes the squared error distance (and error distance itself).</p>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-3d-proof" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3d-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-3d-proof-1.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3d-proof-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visualization of the Best Approximation Theorem
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="uniqueness-of-projection" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="uniqueness-of-projection"><span class="header-section-number">2.4.2</span> Uniqueness of Projection</h3>
<p>While the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.</p>
<div id="thm-projection-uniqueness" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13 (Uniqueness of Orthogonal Projection)</strong></span> For a given vector <span class="math inline">\(y\)</span> and subspace <span class="math inline">\(V\)</span>, the projection vector <span class="math inline">\(\hat{y}\)</span> satisfying <span class="math inline">\((y - \hat{y}) \perp V\)</span> is unique.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Assume there are two vectors <span class="math inline">\(\hat{y}_1 \in V\)</span> and <span class="math inline">\(\hat{y}_2 \in V\)</span> that both satisfy the orthogonality condition. <span class="math display">\[
(y - \hat{y}_1) \perp V \quad \text{and} \quad (y - \hat{y}_2) \perp V
\]</span> This means that for any <span class="math inline">\(v \in V\)</span>, both inner products are zero: <span class="math display">\[
\langle y - \hat{y}_1, v \rangle = 0
\]</span> <span class="math display">\[
\langle y - \hat{y}_2, v \rangle = 0
\]</span></p>
<p>Subtracting the second equation from the first:</p>
<p><span class="math display">\[
\langle y - \hat{y}_1, v \rangle - \langle y - \hat{y}_2, v \rangle = 0
\]</span> Using the linearity of the inner product: <span class="math display">\[
\langle (y - \hat{y}_1) - (y - \hat{y}_2), v \rangle = 0
\]</span> <span class="math display">\[
\langle \hat{y}_2 - \hat{y}_1, v \rangle = 0
\]</span></p>
<p>This equation holds for <strong>all</strong> <span class="math inline">\(v \in V\)</span>. Since <span class="math inline">\(\hat{y}_1\)</span> and <span class="math inline">\(\hat{y}_2\)</span> are both in <span class="math inline">\(V\)</span>, their difference <span class="math inline">\(d = \hat{y}_2 - \hat{y}_1\)</span> must also be in <span class="math inline">\(V\)</span>. We can therefore choose <span class="math inline">\(v = d = \hat{y}_2 - \hat{y}_1\)</span>.</p>
<p><span class="math display">\[
\langle \hat{y}_2 - \hat{y}_1, \hat{y}_2 - \hat{y}_1 \rangle = 0 \implies \|\hat{y}_2 - \hat{y}_1\|^2 = 0
\]</span> The only vector with a norm of zero is the zero vector itself. <span class="math display">\[
\hat{y}_2 - \hat{y}_1 = 0 \implies \hat{y}_1 = \hat{y}_2
\]</span> Thus, the projection is unique.</p>
</div>
</section>
</section>
<section id="projection-via-orthonormal-basis-q" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="projection-via-orthonormal-basis-q"><span class="header-section-number">2.5</span> Projection via Orthonormal Basis (<span class="math inline">\(Q\)</span>)</h2>
<section id="orthonomal-basis" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="orthonomal-basis"><span class="header-section-number">2.5.1</span> Orthonomal Basis</h3>
<p>Before discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.</p>
<div id="def-basis" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 22 (Basis)</strong></span> A set of vectors <span class="math inline">\(\{x_1, \dots, x_k\}\)</span> is a <strong>Basis</strong> for a vector space <span class="math inline">\(V\)</span> if:</p>
<ol type="1">
<li>The vectors span the space: <span class="math inline">\(V = L(x_1, \dots, x_k)\)</span>.</li>
<li>The vectors are linearly independent.</li>
</ol>
</div>
<p>The number of vectors in a basis is unique and is defined as the <strong>Dimension</strong> of <span class="math inline">\(V\)</span>.</p>
<p>Calculations become significantly simpler if we choose a basis with special geometric properties.</p>
<div id="def-orthonormal-basis" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 23 (Orthonormal Basis)</strong></span> A basis <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> is called an <strong>Orthonormal Basis</strong> if:</p>
<ol type="1">
<li><strong>Orthogonal:</strong> Each pair of vectors is perpendicular.</li>
</ol>
<p><span class="math display">\[
    q_i'q_j = 0 \quad \text{for } i \ne j
    \]</span></p>
<ol start="2" type="1">
<li><strong>Normalized:</strong> Each vector has unit length.</li>
</ol>
<p><span class="math display">\[
    ||q_i||^2 = q_i'q_i = 1
    \]</span></p>
<p>Combining these, we write <span class="math inline">\(q_i'q_j = \delta_{ij}\)</span> (Kronecker delta).</p>
</div>
<p>We now generalize the projection problem. Instead of projecting <span class="math inline">\(y\)</span> onto a single line, we project it onto a subspace <span class="math inline">\(V\)</span> of dimension <span class="math inline">\(k\)</span>.</p>
<p>If we have an orthonormal basis <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> for <span class="math inline">\(V\)</span>, the projection <span class="math inline">\(\hat{y}\)</span> is simply the sum of the projections onto the individual basis vectors.</p>
<div id="def-proj-orthonormal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 24 (Projection Defined with Orthonormal Basis)</strong></span> The projection of <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(V = L(q_1, \dots, q_k)\)</span> is: <span class="math display">\[
\hat{y} = \sum_{i=1}^k \text{proj}(y|q_i) = \sum_{i=1}^k (q_i'y) q_i
\]</span></p>
<p>Since the basis vectors are normalized, we do not need to divide by <span class="math inline">\(||q_i||^2\)</span>.</p>
</div>
<div id="thm-orthonormal-basis-proj" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14 (Projection via Orthonormal Basis)</strong></span> Let <span class="math inline">\(\{q_1, \dots, q_k\}\)</span> be an orthonormal basis for the subspace <span class="math inline">\(V \subseteq \mathbb{R}^n\)</span>. The vector defined by the sum of individual projections: <span class="math display">\[
\hat{y} = \sum_{i=1}^k \langle y, q_i \rangle q_i
\]</span> is indeed the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V\)</span>. That is, it satisfies <span class="math inline">\((y - \hat{y}) \perp V\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove this, we must check two conditions:</p>
<ol type="1">
<li><p><span class="math inline">\(\hat{y} \in V\)</span>: This is immediate because <span class="math inline">\(\hat{y}\)</span> is a linear combination of the basis vectors <span class="math inline">\(\{q_1, \dots, q_k\}\)</span>.</p></li>
<li><p><span class="math inline">\((y - \hat{y}) \perp V\)</span>: It suffices to show that the error vector <span class="math inline">\(e = y - \hat{y}\)</span> is orthogonal to every basis vector <span class="math inline">\(q_j\)</span> (for <span class="math inline">\(j = 1, \dots, k\)</span>).</p>
<p>Let’s calculate the inner product <span class="math inline">\(\langle y - \hat{y}, q_j \rangle\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\langle y - \hat{y}, q_j \rangle &amp;= \langle y, q_j \rangle - \langle \hat{y}, q_j \rangle \\
&amp;= \langle y, q_j \rangle - \left\langle \sum_{i=1}^k \langle y, q_i \rangle q_i, q_j \right\rangle \\
&amp;= \langle y, q_j \rangle - \sum_{i=1}^k \langle y, q_i \rangle \underbrace{\langle q_i, q_j \rangle}_{\delta_{ij}}
\end{aligned}
\]</span></p>
<p>Since the basis is orthonormal, <span class="math inline">\(\langle q_i, q_j \rangle\)</span> is 1 if <span class="math inline">\(i=j\)</span> and 0 otherwise. Thus, the summation collapses to a single term where <span class="math inline">\(i=j\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\langle y - \hat{y}, q_j \rangle &amp;= \langle y, q_j \rangle - \langle y, q_j \rangle \cdot 1 \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\((y - \hat{y})\)</span> is orthogonal to every basis vector <span class="math inline">\(q_j\)</span>, it is orthogonal to the entire subspace <span class="math inline">\(V\)</span>. Thus, <span class="math inline">\(\hat{y}\)</span> is the unique orthogonal projection.</p></li>
</ol>
</div>
</section>
<section id="projection-matrix-via-orthonomal-basis-q" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="projection-matrix-via-orthonomal-basis-q"><span class="header-section-number">2.5.2</span> Projection Matrix via Orthonomal Basis (<span class="math inline">\(Q\)</span>)</h3>
<p><strong>Matrix Form with Orthonormal Basis</strong></p>
<p>We can express the summation formula for <span class="math inline">\(\hat{y}\)</span> compactly using matrix notation.</p>
<p>Let <span class="math inline">\(Q\)</span> be an <span class="math inline">\(n \times k\)</span> matrix whose columns are the orthonormal basis vectors <span class="math inline">\(q_1, \dots, q_k\)</span>.</p>
<p><span class="math display">\[
Q = \begin{pmatrix} q_1 &amp; q_2 &amp; \dots &amp; q_k \end{pmatrix}
\]</span></p>
<p>Properties of <span class="math inline">\(Q\)</span>:</p>
<ul>
<li><span class="math inline">\(Q'Q = I_k\)</span> (Identity matrix of size <span class="math inline">\(k \times k\)</span>).</li>
<li><span class="math inline">\(QQ'\)</span> is <strong>not</strong> necessarily <span class="math inline">\(I_n\)</span> (unless <span class="math inline">\(k=n\)</span>).</li>
</ul>
<div id="def-proj-matrix-orthonormal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 25 (Projection Matrix in Terms of <span class="math inline">\(Q\)</span>)</strong></span> The projection <span class="math inline">\(\hat{y}\)</span> can be written as: <span class="math display">\[
\hat{y} = \begin{pmatrix} q_1 &amp; \dots &amp; q_k \end{pmatrix} \begin{pmatrix} q_1'y \\ \vdots \\ q_k'y \end{pmatrix} = Q (Q'y) = (QQ') y
\]</span></p>
<p>Thus, the projection matrix <span class="math inline">\(P\)</span> onto the subspace <span class="math inline">\(V\)</span> is:</p>
<p><span class="math display">\[
P = QQ'
\]</span></p>
</div>
<p><strong>Properties of Projection Matrices</strong></p>
<p>We have defined the projection matrix as <span class="math inline">\(P = X(X'X)^{-1}X'\)</span> (or <span class="math inline">\(P=QQ'\)</span> for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.</p>
<div id="thm-projection-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 15 (Symmeticity and Idempotence)</strong></span> A square matrix <span class="math inline">\(P\)</span> represents an orthogonal projection onto some subspace if and only if it satisfies:</p>
<ol type="1">
<li><strong>Idempotence:</strong> <span class="math inline">\(P^2 = P\)</span> (Applying the projection twice is the same as applying it once).</li>
<li><strong>Symmetry:</strong> <span class="math inline">\(P' = P\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(\hat{y} = Py\)</span> is already in the subspace <span class="math inline">\(\text{Col}(X)\)</span>, then projecting it again should not change it. <span class="math display">\[
P(Py) = Py \implies P^2 y = Py \quad \forall y
\]</span> Thus, <span class="math inline">\(P^2 = P\)</span>.</p>
</div>
<p><strong>Example: ANOVA (Analysis of Variance)</strong></p>
<p>One of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.</p>
<div id="exm-anova-projection" class="theorem example">
<p><span class="theorem-title"><strong>Example 5 (Finding Projection for One-way ANOVA)</strong></span> Consider a one-way ANOVA model with <span class="math inline">\(k\)</span> groups: <span class="math display">\[
y_{ij} = \mu_i + \epsilon_{ij}
\]</span> where <span class="math inline">\(i \in \{1, \dots, k\}\)</span> represents the group and <span class="math inline">\(j \in \{1, \dots, n_i\}\)</span> represents the observation within the group. Let <span class="math inline">\(N = \sum_{i=1}^k n_i\)</span> be the total number of observations.</p>
<ol type="1">
<li><p><strong>Matrix Definitions</strong></p>
<p>We define the data vector <span class="math inline">\(y\)</span> and the design matrix <span class="math inline">\(X\)</span> as follows:</p>
<ul>
<li><strong>Data Vector</strong> (<span class="math inline">\(y\)</span>): An <span class="math inline">\(N \times 1\)</span> vector containing all observations by group:</li>
</ul>
<p><span class="math display">\[
    y = \begin{pmatrix} y_{11} \\ \vdots \\ y_{1n_1} \\ y_{21} \\ \vdots \\ y_{kn_k} \end{pmatrix}
    \]</span></p>
<ul>
<li><strong>Design Matrix</strong> (<span class="math inline">\(X\)</span>): An <span class="math inline">\(N \times k\)</span> matrix constructed from <span class="math inline">\(k\)</span> column vectors, <span class="math inline">\(X = (x_1, x_2, \dots, x_k)\)</span>. Each vector <span class="math inline">\(x_g\)</span> is an <strong>indicator</strong> (dummy variable) for group <span class="math inline">\(g\)</span>:</li>
</ul>
<p><span class="math display">\[
    x_g = \begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix} \quad \leftarrow \text{Entries are 1 if observation belongs to group } g
    \]</span></p></li>
<li><p><strong>Orthogonality</strong></p>
<p>These column vectors <span class="math inline">\(x_1, \dots, x_k\)</span> are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero:</p>
<p><span class="math display">\[
\langle x_g, x_h \rangle = 0 \quad \text{for } g \neq h
\]</span> This allows us to find the projection onto the column space of <span class="math inline">\(X\)</span> by simply summing the projections onto each column individually.</p></li>
<li><p><strong>Calculating Individual Projections</strong></p>
<p>For a specific group vector <span class="math inline">\(x_g\)</span>, the projection is:</p>
<p><span class="math display">\[
\text{proj}(y|x_g) = \frac{\langle y, x_g \rangle}{\langle x_g, x_g \rangle} x_g
\]</span></p>
<p>We calculate the two scalar terms:</p>
<ul>
<li><strong>Denominator</strong> (<span class="math inline">\(\langle x_g, x_g \rangle\)</span>): The sum of squared elements of <span class="math inline">\(x_g\)</span>. Since <span class="math inline">\(x_g\)</span> contains <span class="math inline">\(n_g\)</span> ones and zeros elsewhere:</li>
</ul>
<p><span class="math display">\[
    \langle x_g, x_g \rangle = \sum \mathbb{1}_{\{i=g\}}^2 = n_g
    \]</span></p>
<ul>
<li><strong>Numerator</strong> (<span class="math inline">\(\langle y, x_g \rangle\)</span>): The dot product sums only the <span class="math inline">\(y\)</span> values belonging to group <span class="math inline">\(g\)</span>:</li>
</ul>
<p><span class="math display">\[
    \langle y, x_g \rangle = \sum_{i,j} y_{ij} \cdot \mathbb{1}_{\{i=g\}} = \sum_{j=1}^{n_g} y_{gj} = y_{g.} \quad (\text{Group Total})
    \]</span></p></li>
<li><p><strong>The Resulting Projection</strong></p>
<p>Substituting these back into the formula gives the coefficient for the vector <span class="math inline">\(x_g\)</span>:</p>
<p><span class="math display">\[
\text{proj}(y|x_g) = \frac{y_{g.}}{n_g} x_g = \bar{y}_{g.} x_g
\]</span></p>
<p>The total projection <span class="math inline">\(\hat{y}\)</span> is the sum over all groups:</p>
<p><span class="math display">\[
\hat{y} = \sum_{g=1}^k \bar{y}_{g.} x_g
\]</span> This confirms that the fitted value for any specific observation <span class="math inline">\(y_{ij}\)</span> is simply its group mean <span class="math inline">\(\bar{y}_{i.}\)</span>.</p></li>
</ol>
</div>
</section>
<section id="gram-schmidt-process" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="gram-schmidt-process"><span class="header-section-number">2.5.3</span> Gram-Schmidt Process</h3>
<p>To use the simplified formula <span class="math inline">\(P = QQ'\)</span>, we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.</p>
<div class="algorithm">
<p><strong>Gram-Schmidt Process</strong> Given linearly independent vectors <span class="math inline">\(x_1, \dots, x_p\)</span>:</p>
<ol type="1">
<li><strong>Step 1:</strong> Normalize the first vector.</li>
</ol>
<p><span class="math display">\[
    q_1 = \frac{x_1}{||x_1||}
    \]</span></p>
<ol start="2" type="1">
<li><strong>Step 2:</strong> Project <span class="math inline">\(x_2\)</span> onto <span class="math inline">\(q_1\)</span> and subtract it to find the orthogonal component.</li>
</ol>
<p><span class="math display">\[
    v_2 = x_2 - (x_2'q_1)q_1
    \]</span> Then normalize: <span class="math display">\[
    q_2 = \frac{v_2}{||v_2||}
    \]</span></p>
<ol start="3" type="1">
<li><strong>Step k:</strong> Subtract the projections onto all previous <span class="math inline">\(q\)</span> vectors.</li>
</ol>
<p><span class="math display">\[
    v_k = x_k - \sum_{j=1}^{k-1} (x_k'q_j)q_j
    \]</span> <span class="math display">\[
    q_k = \frac{v_k}{||v_k||}
    \]</span></p>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gram-schmidt-python" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gram-schmidt-python-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-gram-schmidt-python-3.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gram-schmidt-python-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Gram-Schmidt Process: Projecting <span class="math inline">\(x_2\)</span> onto <span class="math inline">\(x_1\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>This process leads to the <strong>QR Decomposition</strong> of a matrix: <span class="math inline">\(X = QR\)</span>, where <span class="math inline">\(Q\)</span> is orthogonal and <span class="math inline">\(R\)</span> is upper triangular.</p>
</section>
</section>
<section id="hat-matrix-projection-matrix-via-x" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="hat-matrix-projection-matrix-via-x"><span class="header-section-number">2.6</span> Hat Matrix (Projection Matrix via <span class="math inline">\(X\)</span>)</h2>
<section id="norm-equations" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="norm-equations"><span class="header-section-number">2.6.1</span> Norm Equations</h3>
<p>Let <span class="math inline">\(X = (x_1, \dots, x_p)\)</span> be an <span class="math inline">\(n \times p\)</span> matrix, where each column <span class="math inline">\(x_j\)</span> is a predictor vector.</p>
<p>We want to project the target vector <span class="math inline">\(y\)</span> onto the column space <span class="math inline">\(\text{Col}(X)\)</span>. This is equivalent to finding a coefficient vector <span class="math inline">\(\beta \in \mathbb{R}^p\)</span> such that the error vector (residual) is orthogonal to the entire subspace <span class="math inline">\(\text{Col}(X)\)</span>.</p>
<p><span class="math display">\[
y - X\beta \perp \text{Col}(X)
\]</span></p>
<p>Since the columns of <span class="math inline">\(X\)</span> span the subspace, the residual must be orthogonal to <strong>every</strong> column vector <span class="math inline">\(x_j\)</span> individually:</p>
<p><span class="math display">\[
y - X\beta \perp x_j \quad \text{for } j = 1, \dots, p
\]</span></p>
<p>Writing this geometric condition as an algebraic dot product (where <span class="math inline">\(x_j'\)</span> denotes the transpose):</p>
<p><span class="math display">\[
x_j'(y - X\beta) = 0 \quad \text{for each } j
\]</span></p>
<p>We can stack these <span class="math inline">\(p\)</span> separate linear equations into a single matrix equation. Since the rows of <span class="math inline">\(X'\)</span> are the columns of <span class="math inline">\(X\)</span>, this becomes:</p>
<p><span class="math display">\[
\begin{pmatrix} x_1' \\ \vdots \\ x_p' \end{pmatrix} (y - X\beta) = \mathbf{0}
\implies X'(y - X\beta) = 0
\]</span></p>
<p>Finally, we distribute the matrix transpose and rearrange terms to solve for <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
X'y - X'X\beta &amp;= 0 \\
X'X\beta &amp;= X'y
\end{aligned}
\]</span></p>
<p>This system is known as the <strong>Normal Equations</strong>.</p>
<div id="thm-least-squares-estimator" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16 (Least Squares Estimator)</strong></span> If <span class="math inline">\(X'X\)</span> is invertible (i.e., <span class="math inline">\(X\)</span> has full column rank), the unique solution for <span class="math inline">\(\beta\)</span> is: <span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'y
\]</span></p>
</div>
</section>
<section id="hat-matrix" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="hat-matrix"><span class="header-section-number">2.6.2</span> Hat Matrix</h3>
<p>Substituting the estimator <span class="math inline">\(\hat{\beta}\)</span> back into the equation for <span class="math inline">\(\hat{y}\)</span> gives us the projection matrix.</p>
<div id="def-projection-matrix-general" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 26 (Hat Matrix)</strong></span> The projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span> is given by: <span class="math display">\[
\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y
\]</span></p>
<p>Thus, the hat matrix <span class="math inline">\(H\)</span> is defined as:</p>
<p><span class="math display">\[
H = X(X'X)^{-1}X'
\]</span></p>
</div>
</section>
<section id="equivalence-of-hat-matrix-and-qq" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="equivalence-of-hat-matrix-and-qq"><span class="header-section-number">2.6.3</span> Equivalence of Hat Matrix and <span class="math inline">\(QQ'\)</span></h3>
<p>If we use the QR decomposition such that <span class="math inline">\(X = QR\)</span>, where the columns of <span class="math inline">\(Q\)</span> form an orthonormal basis for <span class="math inline">\(\text{Col}(X)\)</span>, the formula simplifies significantly.</p>
<p>Recall that for orthonormal columns, <span class="math inline">\(Q'Q = I\)</span>. Substituting <span class="math inline">\(X=QR\)</span> into the general formula:</p>
<p><span class="math display">\[
\begin{aligned}
H &amp;= QR((QR)'(QR))^{-1}(QR)' \\
  &amp;= QR(R'Q'QR)^{-1}R'Q' \\
  &amp;= QR(R' \underbrace{Q'Q}_{I} R)^{-1}R'Q' \\
  &amp;= QR(R'R)^{-1}R'Q' \\
  &amp;= QR R^{-1} (R')^{-1} R' Q' \\
  &amp;= Q \underbrace{R R^{-1}}_{I} \underbrace{(R')^{-1} R'}_{I} Q' \\
  &amp;= Q Q'
\end{aligned}
\]</span></p>
<p>This confirms that <span class="math inline">\(H = QQ'\)</span> is consistent with the general formula <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>.</p>
</section>
<section id="properties-of-hat-matrix" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="properties-of-hat-matrix"><span class="header-section-number">2.6.4</span> Properties of Hat Matrix</h3>
<p>We revisit the properties of projection matrices in this general context.</p>
<div id="thm-projection-properties-revisited" class="theorem">
<p><span class="theorem-title"><strong>Theorem 17 (Properties of Hat Matrix)</strong></span> The matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span> satisfies:</p>
<ol type="1">
<li><strong>Symmetric:</strong> <span class="math inline">\(H' = H\)</span></li>
<li><strong>Idempotent:</strong> <span class="math inline">\(H^2 = H\)</span></li>
<li><strong>Trace:</strong> The trace of a projection matrix equals the dimension of the subspace it projects onto. <span class="math display">\[
\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_p) = p
\]</span></li>
</ol>
</div>
</section>
</section>
<section id="projection-defined-with-orthogonal-projection-matrix" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="projection-defined-with-orthogonal-projection-matrix"><span class="header-section-number">2.7</span> Projection Defined with Orthogonal Projection Matrix</h2>
<p>Projection don’t have to be defined with a subspace or a matrix <span class="math inline">\(X\)</span> as we discussed before. Projection matrix is a self-contained definition of the subspace it projects onto.</p>
<section id="orthogonal-projection-matrix" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="orthogonal-projection-matrix"><span class="header-section-number">2.7.1</span> Orthogonal Projection Matrix</h3>
<div id="def-proj-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 27 (Orthogonal Projection Matrix)</strong></span> A square matrix <span class="math inline">\(P\)</span> is called an <strong>orthogonal projection matrix</strong> if it satisfies two conditions:</p>
<ol type="1">
<li><strong>Symmetry:</strong> <span class="math inline">\(P^\top = P\)</span></li>
<li><strong>Idempotency:</strong> <span class="math inline">\(P^2 = P\)</span></li>
</ol>
</div>
<div id="thm-proj-col" class="theorem">
<p><span class="theorem-title"><strong>Theorem 18 (Projection onto Column Space)</strong></span> Let <span class="math inline">\(P\)</span> be a <span class="math inline">\(p \times p\)</span> symmetric (<span class="math inline">\(P^\top = P\)</span>) and idempotent (<span class="math inline">\(P^2 = P\)</span>) matrix in <span class="math inline">\(\mathbb{R}^p\)</span>. Then <span class="math inline">\(P\)</span> represents the orthogonal projection onto its column space, <span class="math inline">\(\text{Col}(P)\)</span>.</p>
<p>Specifically, for any vector <span class="math inline">\(y \in \mathbb{R}^p\)</span>, the vector <span class="math inline">\(\hat{y} = Py\)</span> satisfies the definition of orthogonal projection:</p>
<ol type="1">
<li><span class="math inline">\(\hat{y} \in \text{Col}(P)\)</span></li>
<li><span class="math inline">\(y - \hat{y} \perp \text{Col}(P)\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove that <span class="math inline">\(P\)</span> is the orthogonal projector onto <span class="math inline">\(\text{Col}(P)\)</span>, we verify the two conditions for an arbitrary vector <span class="math inline">\(y \in \mathbb{R}^p\)</span>.</p>
<ol type="1">
<li><p>Condition: <span class="math inline">\(\hat{y} \in \text{Col}(P)\)</span></p>
<p>By the definition of matrix-vector multiplication, <span class="math inline">\(\hat{y} = Py\)</span> is a linear combination of the columns of <span class="math inline">\(P\)</span>. Therefore, <span class="math inline">\(\hat{y}\)</span> is, by definition, an element of <span class="math inline">\(\text{Col}(P)\)</span>.</p></li>
<li><p>Condition: <span class="math inline">\(y - \hat{y} \perp \text{Col}(P)\)</span></p>
<p>Let <span class="math inline">\(e = y - \hat{y} = (I_n - P)y\)</span>. To verify that <span class="math inline">\(e\)</span> is orthogonal to <span class="math inline">\(\text{Col}(P)\)</span>, it suffices to show that <span class="math inline">\(e\)</span> is orthogonal to every column of <span class="math inline">\(P\)</span>. In matrix notation, this is equivalent to showing <span class="math inline">\(e^\top P = 0\)</span>. We compute this directly:</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
e^\top P &amp;= [(I_p - P)y]^\top P \\
&amp;= y^\top (I_p - P)^\top P \\
&amp;= y^\top (I_p - P) P &amp; (\text{Symmetry: } P^\top = P) \\
&amp;= y^\top (P - P^2) \\
&amp;= y^\top (P - P) &amp; (\text{Idempotency: } P^2 = P) \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(e^\top P = 0\)</span>, the residual <span class="math inline">\(e\)</span> is orthogonal to every column of <span class="math inline">\(P\)</span>. Consequently, <span class="math inline">\(e\)</span> is orthogonal to the space spanned by those columns, <span class="math inline">\(\text{Col}(P)\)</span>.</p>
</div>
<div id="lem-projection-props" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 (0-1 Projection)</strong></span> Let <span class="math inline">\(P\)</span> be a <span class="math inline">\(n \times n\)</span> matrix. <span class="math inline">\(P\)</span> is the orthogonal projection matrix onto <span class="math inline">\(\text{Col}(P)\)</span> if and only if:</p>
<ol type="1">
<li><span class="math inline">\(Pv = v\)</span> for all <span class="math inline">\(v \in \text{Col}(P)\)</span>.</li>
<li><span class="math inline">\(Pv = 0\)</span> for all <span class="math inline">\(v \perp \text{Col}(P)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Forward Implication (<span class="math inline">\(\implies\)</span>):</strong> Given <span class="math inline">\(P\)</span> is an orthogonal projection (<span class="math inline">\(P^2=P, P^\top=P\)</span>).</p>
<ol type="1">
<li><strong>Proof of (1):</strong> Let <span class="math inline">\(v \in \text{Col}(P)\)</span>. Then <span class="math inline">\(v = Px\)</span> for some <span class="math inline">\(x\)</span>. <span class="math display">\[
   Pv = P(Px) = P^2 x = Px = v
   \]</span></li>
<li><strong>Proof of (2):</strong> Let <span class="math inline">\(v \perp \text{Col}(P)\)</span>. Then <span class="math inline">\(v\)</span> is orthogonal to every column of <span class="math inline">\(P\)</span>, so <span class="math inline">\(v^\top P = 0\)</span>. Since <span class="math inline">\(P\)</span> is symmetric:</li>
</ol>
<p><span class="math display">\[
   Pv = (v^\top P^\top)^\top = (v^\top P)^\top = 0^\top = 0
   \]</span></p>
<p><strong>Reverse Implication (<span class="math inline">\(\impliedby\)</span>):</strong> Given conditions (1) and (2) hold.</p>
<p>We must show that <span class="math inline">\(P\)</span> is idempotent (<span class="math inline">\(P^2=P\)</span>) and symmetric (<span class="math inline">\(P^\top=P\)</span>).</p>
<ol type="1">
<li><p><strong>Proof of Idempotence (<span class="math inline">\(P^2 = P\)</span>):</strong> For any vector <span class="math inline">\(x \in \mathbb{R}^n\)</span>, let <span class="math inline">\(y = Px\)</span>. By definition, <span class="math inline">\(y \in \text{Col}(P)\)</span>. Applying condition (1) to the vector <span class="math inline">\(y\)</span>: <span class="math display">\[
   Py = y \implies P(Px) = Px \implies P^2 x = Px
   \]</span> Since this holds for all <span class="math inline">\(x\)</span>, <span class="math inline">\(P^2 = P\)</span>.</p></li>
<li><p><strong>Proof of Symmetry (<span class="math inline">\(P^\top = P\)</span>):</strong> We decompose any two vectors <span class="math inline">\(x, y \in \mathbb{R}^n\)</span> into components inside and orthogonal to <span class="math inline">\(\text{Col}(P)\)</span>. Let <span class="math inline">\(x = x_1 + x_2\)</span> and <span class="math inline">\(y = y_1 + y_2\)</span>, where <span class="math inline">\(x_1, y_1 \in \text{Col}(P)\)</span> and <span class="math inline">\(x_2, y_2 \perp \text{Col}(P)\)</span>. Using conditions (1) and (2): <span class="math display">\[
   Px = P(x_1 + x_2) = Px_1 + Px_2 \stackrel{(1),(2)}{=} x_1 + 0 = x_1
   \]</span><br>
<span class="math display">\[
   Py = P(y_1 + y_2) = Py_1 + Py_2 \stackrel{(1),(2)}{=} y_1 + 0 = y_1
   \]</span> Now we compare the inner products <span class="math inline">\(\langle Px, y \rangle\)</span> and <span class="math inline">\(\langle x, Py \rangle\)</span>: <span class="math display">\[
   \langle Px, y \rangle = \langle x_1, y_1 + y_2 \rangle = \langle x_1, y_1 \rangle + \underbrace{\langle x_1, y_2 \rangle}_{0} = \langle x_1, y_1 \rangle
   \]</span></p></li>
</ol>
<p><span class="math display">\[
   \langle x, Py \rangle = \langle x_1 + x_2, y_1 \rangle = \langle x_1, y_1 \rangle + \underbrace{\langle x_2, y_1 \rangle}_{0} = \langle x_1, y_1 \rangle
   \]</span> Since <span class="math inline">\(\langle Px, y \rangle = \langle x, Py \rangle\)</span> implies <span class="math inline">\(x^\top P^\top y = x^\top P y\)</span> for all <span class="math inline">\(x, y\)</span>, we conclude <span class="math inline">\(P^\top = P\)</span>.</p>
<p>Since <span class="math inline">\(P\)</span> is symmetric and idempotent, it is the orthogonal projection matrix.</p>
</div>
</section>
<section id="projection-onto-complement-space" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="projection-onto-complement-space"><span class="header-section-number">2.7.2</span> Projection onto Complement Space</h3>
<div id="thm-complement-proj" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19 (Projection onto Orthogonal Complement)</strong></span> Let <span class="math inline">\(P\)</span> be a <span class="math inline">\(n \times n\)</span> orthogonal projection matrix operating in the space <span class="math inline">\(\mathbb{R}^n\)</span>. The matrix <span class="math inline">\(M\)</span> defined as: <span class="math display">\[
M = I_p - P
\]</span> is the orthogonal projection matrix onto the orthogonal complement of the column space of <span class="math inline">\(P\)</span>, denoted <span class="math inline">\(\text{Col}(P)^\perp \subseteq \mathbb{R}^n\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p><strong>Symmetry and Idempotency</strong> Since <span class="math inline">\(P\)</span> is a projection matrix, <span class="math inline">\(P^\top = P\)</span> and <span class="math inline">\(P^2 = P\)</span>. We verify these properties for <span class="math inline">\(M\)</span>: <span id="eq-m-sym"><span class="math display">\[
   M^\top = (I_p - P)^\top = I_p - P^\top = I_p - P = M
    \tag{1}\]</span></span> <span id="eq-m-idemp"><span class="math display">\[
   M^2 = (I_p - P)(I_p - P) = I_p - 2P + P^2 = I_p - 2P + P = I_p - P = M
    \tag{2}\]</span></span> By <a href="#eq-m-sym" class="quarto-xref">Equation&nbsp;1</a> and <a href="#eq-m-idemp" class="quarto-xref">Equation&nbsp;2</a>, <span class="math inline">\(M\)</span> is symmetric and idempotent, so it is an orthogonal projection matrix.</p></li>
<li><p><strong>Identifying the Subspace</strong> We now show that <span class="math inline">\(\text{Col}(M) = \text{Col}(P)^\perp\)</span> by mutual inclusion.</p>
<ol type="1">
<li><p><strong>Direction 1</strong>: <span class="math inline">\(\text{Col}(M) \subseteq \text{Col}(P)^\perp\)</span> Let <span class="math inline">\(v \in \text{Col}(M)\)</span>. Then <span class="math inline">\(v = Mx\)</span> for some vector <span class="math inline">\(x\)</span>. Multiplying by <span class="math inline">\(P\)</span>: <span class="math display">\[
  Pv = P(I_p - P)x = (P - P^2)x = 0
  \]</span> Since <span class="math inline">\(P\)</span> is symmetric (<span class="math inline">\(P = P'\)</span>), taking the transpose of <span class="math inline">\(Pv=0\)</span> gives <span class="math inline">\(v'P = 0\)</span>. This means <span class="math inline">\(v\)</span> is orthogonal to every column of <span class="math inline">\(P\)</span>. Therefore, <span class="math inline">\(v \in \text{Col}(P)^\perp\)</span>.</p></li>
<li><p><strong>Direction 2</strong>: <span class="math inline">\(\text{Col}(P)^\perp \subseteq \text{Col}(M)\)</span> Let <span class="math inline">\(v \in \text{Col}(P)^\perp\)</span>. By definition, <span class="math inline">\(v\)</span> is orthogonal to the columns of <span class="math inline">\(P\)</span>, so <span class="math inline">\(v'P = 0\)</span>. Taking the transpose and using symmetry (<span class="math inline">\(P' = P\)</span>), we get <span class="math inline">\(Pv = 0\)</span>.<br>
Now applying <span class="math inline">\(M\)</span> to <span class="math inline">\(v\)</span>: <span class="math display">\[
  Mv = (I_p - P)v = v - Pv = v
  \]</span> Since <span class="math inline">\(Mv = v\)</span>, <span class="math inline">\(v\)</span> lies in the column space of <span class="math inline">\(M\)</span>. Therefore, <span class="math inline">\(v \in \text{Col}(M)\)</span>.</p></li>
</ol></li>
</ol>
<p>Since both inclusions hold, <span class="math inline">\(\text{Col}(M) = \text{Col}(P)^\perp\)</span>.</p>
</div>
</section>
<section id="projections-onto-nested-subspaces" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="projections-onto-nested-subspaces"><span class="header-section-number">2.7.3</span> Projections onto Nested Subspaces</h3>
<section id="iterative-projections" class="level4" data-number="2.7.3.1">
<h4 data-number="2.7.3.1" class="anchored" data-anchor-id="iterative-projections"><span class="header-section-number">2.7.3.1</span> Iterative Projections</h4>
<div id="thm-nested-projections" class="theorem">
<p><span class="theorem-title"><strong>Theorem 20 (Iterative Projections)</strong></span> Let <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span> be <span class="math inline">\(n \times n\)</span> orthogonal projection matrices such that <span class="math inline">\(\text{Col}(P_0) \subseteq \text{Col}(P_1)\)</span>. Then:</p>
<ol type="1">
<li><span class="math inline">\(P_1 P_0 = P_0\)</span></li>
<li><span class="math inline">\(P_0 P_1 = P_0\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Method 1</strong>:</p>
<p>Proof of <span class="math inline">\(P_1 P_0 = P_0\)</span>:</p>
<p>Let <span class="math inline">\(y \in \mathbb{R}^n\)</span> be an arbitrary vector. By definition, the vector <span class="math inline">\(v = P_0 y\)</span> lies in <span class="math inline">\(\text{Col}(P_0)\)</span>. Given <span class="math inline">\(\text{Col}(P_0) \subseteq \text{Col}(P_1)\)</span>, it follows that <span class="math inline">\(v \in \text{Col}(P_1)\)</span>.</p>
<p>Using <strong>Lemma <a href="#lem-projection-props" class="quarto-xref">Lemma&nbsp;1</a></strong>, since <span class="math inline">\(v \in \text{Col}(P_1)\)</span>, <span class="math inline">\(P_1\)</span> acts as the identity on <span class="math inline">\(v\)</span>, so <span class="math inline">\(P_1 v = v\)</span>. Substituting <span class="math inline">\(v = P_0 y\)</span>:</p>
<p><span class="math display">\[
P_1 (P_0 y) = P_0 y
\]</span></p>
<p>Since <span class="math inline">\(P_1 P_0 y = P_0 y\)</span> holds for all <span class="math inline">\(y \in \mathbb{R}^n\)</span>, we conclude <span class="math inline">\(P_1 P_0 = P_0\)</span>.</p>
<p>Proof of <span class="math inline">\(P_0 P_1 = P_0\)</span>:</p>
<p>Taking the transpose of the result from part 1 and applying the symmetry property (<span class="math inline">\(P' = P\)</span>):</p>
<p><span class="math display">\[
(P_1 P_0)' = P_0' \implies P_0' P_1' = P_0' \implies P_0 P_1 = P_0
\]</span></p>
<p><strong>Method 2</strong>:</p>
<p>To prove <span class="math inline">\(P_0 P_1 = P_0\)</span>, for any <span class="math inline">\(y \in \mathbb{R}^n\)</span>, let <span class="math inline">\(\hat{y}_1 = P_1 y\)</span>, <span class="math inline">\(\hat{y}_0 = P_0 y\)</span>, <span class="math inline">\(e_1 = y - \hat{y}_1\)</span>, and <span class="math inline">\(e_0 = y - \hat{y}_0\)</span>. Note that both <span class="math inline">\(e_0\)</span> and <span class="math inline">\(e_1\)</span> are orthogonal to <span class="math inline">\(\text{Col}(P_0)\)</span> (since <span class="math inline">\(\text{Col}(P_0) \subseteq \text{Col}(P_1)\)</span>).</p>
<p>We have:</p>
<p><span class="math display">\[
P_0(P_1 - P_0)y = P_0(\hat{y}_1 - \hat{y}_0) = P_0 (e_0 - e_1) = 0
\]</span></p>
<p>This implies <span class="math inline">\(P_0 P_1 - P_0 = 0\)</span>, so <span class="math inline">\(P_0 P_1 = P_0\)</span>.</p>
</div>
</section>
<section id="difference-of-projections" class="level4" data-number="2.7.3.2">
<h4 data-number="2.7.3.2" class="anchored" data-anchor-id="difference-of-projections"><span class="header-section-number">2.7.3.2</span> Difference of Projections</h4>
<div id="thm-diff-projection" class="theorem">
<p><span class="theorem-title"><strong>Theorem 21 (Difference Projection)</strong></span> The matrix <span class="math inline">\(P_{\Delta} = P_1 - P_0\)</span> is an orthogonal projection matrix onto the subspace <span class="math inline">\(\text{Col}(P_1) \cap \text{Col}(P_0)^\perp\)</span>. This subspace represents the “extra” information in the full model that is orthogonal to the reduced model. Additionally, the following column space relationship holds: <span class="math display">\[
\text{Col}(P_1 - P_0) = \text{Col}(P_0)^\perp \cap \text{Col}(P_1)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><strong>Symmetry:</strong> Since <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_0\)</span> are symmetric:</li>
</ol>
<p><span class="math display">\[
(P_1 - P_0)' = P_1' - P_0' = P_1 - P_0
\]</span></p>
<ol start="2" type="1">
<li><strong>Idempotency:</strong></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
(P_1 - P_0)^2 &amp;= (P_1 - P_0)(P_1 - P_0) \\
&amp;= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2
\end{aligned}
\]</span> Using the projection property (<span class="math inline">\(P^2=P\)</span>) and the nested property (<span class="math inline">\(P_1 P_0 = P_0\)</span> and <span class="math inline">\(P_0 P_1 = P_0\)</span>): <span class="math display">\[
= P_1 - P_0 - P_0 + P_0 = P_1 - P_0
\]</span></p>
<ol start="3" type="1">
<li><strong>Orthogonality to <span class="math inline">\(P_0\)</span>:</strong></li>
</ol>
<p><span class="math display">\[
(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0
\]</span></p>
<ol start="4" type="1">
<li><p><strong>Column Space Identity:</strong> We show <span class="math inline">\(\text{Col}(P_1 - P_0) = \text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span> via double containment.</p>
<p><strong><span class="math inline">\((\subseteq)\)</span> Forward Containment:</strong> Let <span class="math inline">\(y \in \text{Col}(P_1 - P_0)\)</span>. By definition, <span class="math inline">\(y = (P_1 - P_0)x\)</span> for some <span class="math inline">\(x\)</span>.</p>
<ul>
<li>Check <span class="math inline">\(y \in \text{Col}(P_1)\)</span>: <span class="math inline">\(P_1 y = P_1(P_1 - P_0)x = (P_1 - P_0)x = y\)</span>. Thus <span class="math inline">\(y \in \text{Col}(P_1)\)</span>.</li>
<li>Check <span class="math inline">\(y \in \text{Col}(P_0)^\perp\)</span>: <span class="math inline">\(P_0 y = P_0(P_1 - P_0)x = (P_0 - P_0)x = 0\)</span>. Thus <span class="math inline">\(y \in \text{Col}(P_0)^\perp\)</span>.</li>
<li>Therefore, <span class="math inline">\(\text{Col}(P_1 - P_0) \subseteq \text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span>.</li>
</ul>
<p><strong><span class="math inline">\((\supseteq)\)</span> Reverse Containment</strong>: Let <span class="math inline">\(y \in \text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span>.</p>
<ul>
<li>Since <span class="math inline">\(y \in \text{Col}(P_1)\)</span>, <span class="math inline">\(P_1 y = y\)</span>.</li>
<li>Since <span class="math inline">\(y \in \text{Col}(P_0)^\perp\)</span>, <span class="math inline">\(P_0 y = 0\)</span>.</li>
<li>Observe <span class="math inline">\((P_1 - P_0)y = P_1 y - P_0 y = y - 0 = y\)</span>.</li>
<li>This implies <span class="math inline">\(y\)</span> is in the range of <span class="math inline">\((P_1 - P_0)\)</span>. Therefore, <span class="math inline">\(\text{Col}(P_0)^\perp \cap \text{Col}(P_1) \subseteq \text{Col}(P_1 - P_0)\)</span>.</li>
</ul></li>
</ol>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is important as we can use <span class="math inline">\(P_2-P_1\)</span> to construct the projection matrix and the space that it projects onto.</p>
</div>
</div>
<p><strong>Hat Matrix of Incremental Space</strong></p>
<div id="thm-hat-matrix-incremental" class="theorem">
<p><span class="theorem-title"><strong>Theorem 22 (Hat Matrix of Incremental Space)</strong></span> Let <span class="math inline">\(X_1\)</span> be a design matrix of dimension <span class="math inline">\(n \times k_1\)</span> and <span class="math inline">\(X_2\)</span> be a design matrix of dimension <span class="math inline">\(n \times k_2\)</span>, such that the combined matrix <span class="math inline">\(X = [X_1, X_2]\)</span> has full column rank. Let <span class="math inline">\(V_1 = \text{Col}(X_1)\)</span> and <span class="math inline">\(V_2 = \text{Col}([X_1, X_2])\)</span>. Let <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> be the orthogonal projection matrices onto <span class="math inline">\(V_1\)</span> and <span class="math inline">\(V_2\)</span>, respectively.</p>
<p>Define the matrix of residuals <span class="math inline">\(\tilde{X}_2\)</span> as:</p>
<p><span class="math display">\[
\tilde{X}_2 = (I - P_1) X_2
\]</span></p>
<p>Let <span class="math inline">\(W = \text{Col}(\tilde{X}_2)\)</span>. Let <span class="math inline">\(P_W\)</span> be the <span class="math inline">\(n \times n\)</span> projection matrix onto <span class="math inline">\(W\)</span>, which is the hat matrix constructed from <span class="math inline">\(\tilde{X}_2\)</span>:</p>
<p><span class="math display">\[
P_W = \tilde{X}_2 (\tilde{X}_2^T \tilde{X}_2)^{-1} \tilde{X}_2^T
\]</span></p>
<ol type="a">
<li>Let <span class="math inline">\(X^* = [X_1, \tilde{X}_2]\)</span>. Prove that the column space of the original design matrix <span class="math inline">\(X\)</span> is identical to the column space of the modified design matrix <span class="math inline">\(X^*\)</span>:</li>
</ol>
<p><span class="math display">\[
\text{Col}([X_1, X_2]) = \text{Col}([X_1, \tilde{X}_2])
\]</span></p>
<ol start="2" type="a">
<li>Using the result from Part (a) and the definition of the Hat Matrix, prove that:</li>
</ol>
<p><span class="math display">\[
P_W = P_2 - P_1
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Assignment question.</p>
</div>
</section>
</section>
<section id="projection-onto-three-multually-orthogonal-subspaces" class="level3" data-number="2.7.4">
<h3 data-number="2.7.4" class="anchored" data-anchor-id="projection-onto-three-multually-orthogonal-subspaces"><span class="header-section-number">2.7.4</span> Projection onto Three Multually Orthogonal Subspaces</h3>
<div id="thm-nested-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 23 (Orthogonal Decomposition)</strong></span> Let <span class="math inline">\(M_0 \subset M_1\)</span> be two nested linear models associated with orthogonal projection matrices <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span>, such that <span class="math inline">\(\text{Col}(P_0) \subset \text{Col}(P_1)\)</span>.</p>
<p>For any observation vector <span class="math inline">\(y\)</span>, we have the decomposition:</p>
<p><span class="math display">\[
y = \underbrace{P_0 y}_{\hat{y}_0} + \underbrace{(P_1 - P_0) y}_{\hat{y}_1 - \hat{y}_0} + \underbrace{(I - P_1) y}_{y - \hat{y}_1}
\]</span></p>
<p><strong>Geometric Interpretation:</strong></p>
<ol type="1">
<li><span class="math inline">\(\hat{y}_0 \in \text{Col}(P_0)\)</span>: The fit of the reduced model.</li>
<li><span class="math inline">\((\hat{y}_1 - \hat{y}_0) \in \text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span>: The additional fit provided by <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_0\)</span>.</li>
<li><span class="math inline">\((y - \hat{y}_1) \in \text{Col}(P_1)^\perp\)</span>: The projection of <span class="math inline">\(y\)</span> onto the <strong>orthogonal complement</strong> of <span class="math inline">\(\text{Col}(P_1)\)</span>.</li>
</ol>
<p>The three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm:</p>
<p><span class="math display">\[
\|y\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2 + \|y - \hat{y}_1\|^2
\]</span></p>
</div>
<div id="thm-nested-hatmatrix" class="theorem">
<p><span class="theorem-title"><strong>Theorem 24 (Orthogonal Decomposition)</strong></span> Let <span class="math inline">\(M_0 \subset M_1\)</span> be two nested linear models associated with orthogonal projection matrices <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span>, such that <span class="math inline">\(\text{Col}(P_0) \subset \text{Col}(P_1)\)</span>.</p>
<p>For any observation vector <span class="math inline">\(y\)</span>, we have the decomposition:</p>
<p><span class="math display">\[
y = \underbrace{P_0 y}_{\hat{y}_0} + \underbrace{(P_1 - P_0) y}_{\hat{y}_1 - \hat{y}_0} + \underbrace{(I - P_1) y}_{y - \hat{y}_1}
\]</span></p>
<p><strong>Geometric Interpretation:</strong></p>
<ol type="1">
<li><span class="math inline">\(\hat{y}_0 \in \text{Col}(P_0)\)</span>: The fit of the reduced model.</li>
<li><span class="math inline">\((\hat{y}_1 - \hat{y}_0) \in \text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span>: The additional fit provided by <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_0\)</span>.</li>
<li><span class="math inline">\((y - \hat{y}_1) \in \text{Col}(P_1)^\perp\)</span>: The projection of <span class="math inline">\(y\)</span> onto the <strong>orthogonal complement</strong> of <span class="math inline">\(\text{Col}(P_1)\)</span>.</li>
</ol>
<p>The three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm:</p>
<p><span class="math display">\[
\|y\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2 + \|y - \hat{y}_1\|^2
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p><strong>Definition of Vectors and Nested Spaces</strong></p>
<p>Let <span class="math inline">\(I\)</span> be the identity matrix, which is the orthogonal projection onto the entire space <span class="math inline">\(\mathbb{R}^n\)</span>. We effectively have a three-level nested sequence of subspaces:</p>
<p><span class="math display">\[
\text{Col}(P_0) \subset \text{Col}(P_1) \subset \mathbb{R}^n
\]</span> We define the components of the decomposition using successive difference projections:</p>
<ul>
<li><span class="math inline">\(v_0 = P_0 y\)</span></li>
<li><span class="math inline">\(v_1 = (P_1 - P_0) y\)</span></li>
<li><span class="math inline">\(v_2 = (I - P_1) y\)</span></li>
</ul>
<p>Summing these gives the identity: <span class="math inline">\(y = v_0 + v_1 + v_2\)</span>.</p></li>
<li><p><strong>Sequential Orthogonality via <a href="#thm-diff-projection" class="quarto-xref">Theorem&nbsp;21</a></strong></p>
<p>We apply the Difference Projection Theorem (<a href="#thm-diff-projection" class="quarto-xref">Theorem&nbsp;21</a>) to each successive pair of nested spaces to establish orthogonality.</p>
<ul>
<li><strong>Step 1: Verify <span class="math inline">\(v_1 \perp v_0\)</span></strong>
<ul>
<li>Consider the nested pair <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span>.</li>
<li>By <a href="#thm-diff-projection" class="quarto-xref">Theorem&nbsp;21</a>, the matrix <span class="math inline">\((P_1 - P_0)\)</span> projects onto <span class="math inline">\(\text{Col}(P_0)^\perp \cap \text{Col}(P_1)\)</span>.</li>
<li>Since <span class="math inline">\(v_0 \in \text{Col}(P_0)\)</span> and <span class="math inline">\(v_1\)</span> lies in the orthogonal complement <span class="math inline">\(\text{Col}(P_0)^\perp\)</span>, we have <strong><span class="math inline">\(v_1 \perp v_0\)</span></strong>.</li>
</ul></li>
<li><strong>Step 2: Verify <span class="math inline">\(v_2 \perp \{v_0, v_1\}\)</span></strong>
<ul>
<li>Consider the nested pair <span class="math inline">\(P_1\)</span> and <span class="math inline">\(I\)</span> (where <span class="math inline">\(I\)</span> projects onto <span class="math inline">\(\mathbb{R}^n\)</span>).</li>
<li>By <a href="#thm-diff-projection" class="quarto-xref">Theorem&nbsp;21</a>, the matrix <span class="math inline">\((I - P_1)\)</span> projects onto <span class="math inline">\(\text{Col}(P_1)^\perp \cap \mathbb{R}^n = \text{Col}(P_1)^\perp\)</span>.</li>
<li>Since both <span class="math inline">\(v_0\)</span> and <span class="math inline">\(v_1\)</span> reside within <span class="math inline">\(\text{Col}(P_1)\)</span> (as shown in Step 1), and <span class="math inline">\(v_2\)</span> lies in the orthogonal complement <span class="math inline">\(\text{Col}(P_1)^\perp\)</span>, it follows that <span class="math inline">\(v_2\)</span> is orthogonal to the entire subspace <span class="math inline">\(\text{Col}(P_1)\)</span>.</li>
<li>Therefore, <strong><span class="math inline">\(v_2 \perp v_0\)</span></strong> and <strong><span class="math inline">\(v_2 \perp v_1\)</span></strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>Conclusion</strong></p>
<p>Since <span class="math inline">\(\{v_0, v_1, v_2\}\)</span> are mutually orthogonal, the Pythagorean theorem applies:</p>
<p><span class="math display">\[
\|y\|^2 = \|v_0\|^2 + \|v_1\|^2 + \|v_2\|^2
\]</span> Substituting the original definitions back in: <span class="math display">\[
\|y\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2 + \|y - \hat{y}_1\|^2
\]</span></p></li>
</ol>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-anova-decomp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-anova-decomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-anova-decomp-5.png" class="img-fluid figure-img" data-fig-pos="H" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-anova-decomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of Projections onto Nested Subspaces
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-anova-ss" class="theorem example">
<p><span class="theorem-title"><strong>Example 6 (ANOVA Sum Squares)</strong></span> We apply the <strong>Nested Model Theorem</strong> (<span class="math inline">\(M_0 \subset M_1\)</span>) to the One-way ANOVA setting.</p>
<ol type="1">
<li><p>Notation and Definitions</p>
<p>Consider a dataset with <span class="math inline">\(k\)</span> groups. Let <span class="math inline">\(i = 1, \dots, k\)</span> index the groups, and <span class="math inline">\(j = 1, \dots, n_i\)</span> index the observations within group <span class="math inline">\(i\)</span>.</p>
<ul>
<li><p><span class="math inline">\(N\)</span>: Total number of observations, <span class="math inline">\(N = \sum_{i=1}^k n_i\)</span>.</p></li>
<li><p><span class="math inline">\(y_{ij}\)</span>: The <span class="math inline">\(j\)</span>-th observation in the <span class="math inline">\(i\)</span>-th group.</p></li>
<li><p><span class="math inline">\(\bar{y}_{i.}\)</span>: The sample mean of group <span class="math inline">\(i\)</span>.</p></li>
</ul>
<p><span class="math display">\[ \bar{y}_{i.} = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \]</span></p>
<ul>
<li><span class="math inline">\(\bar{y}_{..}\)</span>: The grand mean of all observations.</li>
</ul>
<p><span class="math display">\[ \bar{y}_{..} = \frac{1}{N} \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij} \]</span></p></li>
<li><p>The Data and Projection Vectors</p>
<div id="tbl-anova-vectors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-anova-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: ANOVA Vectors: Data, Null Model, and Full Model
</figcaption>
<div aria-describedby="tbl-anova-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Observation (<span class="math inline">\(y\)</span>)</th>
<th style="text-align: center;">Null Projection (<span class="math inline">\(\hat{y}_0\)</span>)</th>
<th style="text-align: center;">Full Projection (<span class="math inline">\(\hat{y}_1\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} y_{11} \\ \vdots \\ y_{1 n_1} \\ \hline \vdots \\ \hline y_{k1} \\ \vdots \\ y_{k n_k} \end{pmatrix}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \\ \hline \vdots \\ \hline \bar{y}_{..} \\ \vdots \\ \bar{y}_{..} \end{pmatrix}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\begin{pmatrix} \bar{y}_{1.} \\ \vdots \\ \bar{y}_{1.} \\ \hline \vdots \\ \hline \bar{y}_{k.} \\ \vdots \\ \bar{y}_{k.} \end{pmatrix}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<ol start="3" type="1">
<li>Decomposition and Sum of Squares</li>
</ol>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: left;">Vector Elements</th>
<th style="text-align: left;">Squared Norm (Sum of Squares)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Null Proj.</strong></td>
<td style="text-align: center;"><span class="math inline">\(\hat{y}_0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(P_0 y\)</span></td>
<td style="text-align: left;">Grand Mean (<span class="math inline">\(\bar{y}_{..}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\|\hat{y}_0\|^2 = N \bar{y}_{..}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Full Proj.</strong></td>
<td style="text-align: center;"><span class="math inline">\(\hat{y}_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(P_1 y\)</span></td>
<td style="text-align: left;">Group Means (<span class="math inline">\(\bar{y}_{i.}\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\|\hat{y}_1\|^2 = \sum_{i=1}^k n_i \bar{y}_{i.}^2\)</span></td>
</tr>
</tbody>
</table></li>
<li><p>Geometric Justification of Shortcut Formulas</p>
<p><strong>A. Total Sum of Squares (SST)</strong> Since <span class="math inline">\(\hat{y}_0 \perp (y - \hat{y}_0)\)</span>, we have <span class="math inline">\(\|y\|^2 = \|\hat{y}_0\|^2 + \|y - \hat{y}_0\|^2\)</span>:</p>
<p><span class="math display">\[ \text{SST} = \|y - \hat{y}_0\|^2 = \|y\|^2 - \|\hat{y}_0\|^2 \]</span> <span class="math display">\[ \text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij}^2 - N\bar{y}_{..}^2 \]</span></p>
<p><strong>B. Between Group Sum of Squares (SSB)</strong> Since <span class="math inline">\(\hat{y}_0 \perp (\hat{y}_1 - \hat{y}_0)\)</span>, we have <span class="math inline">\(\|\hat{y}_1\|^2 = \|\hat{y}_0\|^2 + \|\hat{y}_1 - \hat{y}_0\|^2\)</span>:</p>
<p><span class="math display">\[ \text{SSB} = \|\hat{y}_1 - \hat{y}_0\|^2 = \|\hat{y}_1\|^2 - \|\hat{y}_0\|^2 \]</span> <span class="math display">\[ \text{SSB} = \sum_{i=1}^k n_i\bar{y}_{i.}^2 - N\bar{y}_{..}^2 \]</span></p>
<p><strong>C. Within Group Sum of Squares (SSW)</strong> Since <span class="math inline">\(\hat{y}_1 \perp (y - \hat{y}_1)\)</span>, we have <span class="math inline">\(\|y\|^2 = \|\hat{y}_1\|^2 + \|y - \hat{y}_1\|^2\)</span>:</p>
<p><span class="math display">\[ \text{SSW} = \|y - \hat{y}_1\|^2 = \|y\|^2 - \|\hat{y}_1\|^2 \]</span> <span class="math display">\[ \text{SSW} = \sum_{i=1}^k \sum_{j=1}^{n_i} y_{ij}^2 - \sum_{i=1}^k n_i\bar{y}_{i.}^2 \]</span></p>
<p><strong>Conclusion:</strong></p>
<p><span class="math display">\[ \underbrace{\|y\|^2 - N\bar{y}_{..}^2}_{\text{SST}} = \underbrace{(\sum n_i\bar{y}_{i.}^2 - N\bar{y}_{..}^2)}_{\text{SSB}} + \underbrace{(\sum \sum y_{ij}^2 - \sum n_i\bar{y}_{i.}^2)}_{\text{SSW}} \]</span></p></li>
<li><p>Visualizing ANOVA Components in Data Space</p></li>
</ol>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate Data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>group_names <span class="op">=</span> [<span class="st">'A'</span>, <span class="st">'B'</span>, <span class="st">'C'</span>, <span class="st">'D'</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>n_i <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">8</span>, <span class="dv">15</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">12</span>, <span class="dv">18</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>std_dev <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define colors and markers for each group</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#1f77b4'</span>, <span class="st">'#ff7f0e'</span>, <span class="st">'#2ca02c'</span>, <span class="st">'#d62728'</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>markers <span class="op">=</span> [<span class="st">'o'</span>, <span class="st">'s'</span>, <span class="st">'^'</span>, <span class="st">'D'</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>data_x <span class="op">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>data_y <span class="op">=</span> []</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>group_boundaries <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>group_indices <span class="op">=</span> [] <span class="co"># To store indices for each group</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>current_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(n_i):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    group_data <span class="op">=</span> np.random.normal(means[i], std_dev, n)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.arange(current_idx, current_idx <span class="op">+</span> n)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    data_x.extend(indices)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    data_y.extend(group_data)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    group_indices.append(indices) <span class="co"># Store indices for plotting later</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    current_idx <span class="op">+=</span> n</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    group_boundaries.append(current_idx)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>data_x <span class="op">=</span> np.array(data_x)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>data_y <span class="op">=</span> np.array(data_y)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Stats</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>grand_mean <span class="op">=</span> np.mean(data_y)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>group_means <span class="op">=</span> [np.mean(data_y[group_boundaries[i]:group_boundaries[i<span class="op">+</span><span class="dv">1</span>]]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(n_i))]</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Plotting</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw Grand Mean (Full span)</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>grand_mean, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Grand Mean ($</span><span class="ch">\\</span><span class="ss">bar</span><span class="ch">{{</span><span class="ss">y</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="ss">..</span><span class="ch">}}</span><span class="ss">$ = </span><span class="sc">{</span>grand_mean<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through each group to plot points and means with matching colors</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(n_i)):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    start, end <span class="op">=</span> group_boundaries[i], group_boundaries[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> group_indices[i]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Scatter plot for the group with unique color and marker</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    plt.scatter(data_x[idx], data_y[idx], color<span class="op">=</span>colors[i], marker<span class="op">=</span>markers[i], </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.7</span>, s<span class="op">=</span><span class="dv">60</span>, label<span class="op">=</span><span class="ss">f'Group </span><span class="sc">{</span>group_names[i]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Horizontal line for group mean with the SAME color</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    plt.hlines(y<span class="op">=</span>group_means[i], xmin<span class="op">=</span>start, xmax<span class="op">=</span>end<span class="op">-</span><span class="dv">1</span>, color<span class="op">=</span>colors[i], linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Visualizing the "Within" residuals (faint lines)</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> idx:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        plt.vlines(x<span class="op">=</span>j, ymin<span class="op">=</span><span class="bu">min</span>(data_y[j], group_means[i]), </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                   ymax<span class="op">=</span><span class="bu">max</span>(data_y[j], group_means[i]), </span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>                   color<span class="op">=</span>colors[i], alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">':'</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Formatting</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"One-Way ANOVA: Data, Group Means, and Grand Mean"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Observation Index ($j$ grouped by $i$)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Value ($y_</span><span class="sc">{ij}</span><span class="st">$)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Set x-ticks at the center of each group</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.array(group_boundaries[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">+</span> np.array(n_i)<span class="op">/</span><span class="dv">2</span> <span class="op">-</span> <span class="fl">0.5</span>, </span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>           [<span class="ss">f"Group </span><span class="sc">{</span>g<span class="sc">}</span><span class="ch">\n</span><span class="ss">($n_</span><span class="ch">{{</span><span class="sc">{</span>g<span class="sc">.</span>lower()<span class="sc">}</span><span class="ch">}}</span><span class="ss">=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">$)"</span> <span class="cf">for</span> g, n <span class="kw">in</span> <span class="bu">zip</span>(group_names, n_i)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>plt.grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust legend to show group markers and the grand mean line</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>handles, labels <span class="op">=</span> plt.gca().get_legend_handles_labels()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Reorder legend: Groups first, then Grand Mean</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">0</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.legend([handles[idx] <span class="cf">for</span> idx <span class="kw">in</span> order], [labels[idx] <span class="cf">for</span> idx <span class="kw">in</span> order], </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>           bbox_to_anchor<span class="op">=</span>(<span class="fl">1.02</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>, borderaxespad<span class="op">=</span><span class="fl">0.</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-anova-data-space-colored" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-anova-data-space-colored-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-anova-data-space-colored-7.png" class="img-fluid figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-anova-data-space-colored-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Visualization of Group Means vs.&nbsp;Grand Mean
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="projections-onto-more-than-three-orthogonal-subspaces" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="projections-onto-more-than-three-orthogonal-subspaces"><span class="header-section-number">2.8</span> Projections onto More than Three Orthogonal Subspaces</h2>
<p>Finally, we consider the case where the entire space <span class="math inline">\(\mathbb{R}^n\)</span> is decomposed into mutually orthogonal subspaces.</p>
<div id="thm-orth-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 25 (General Orthogonal Projections)</strong></span> If <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of orthogonal subspaces <span class="math inline">\(V_1, V_2, \dots, V_k\)</span>: <span class="math display">\[
\mathbb{R}^n = V_1 \oplus V_2 \oplus \dots \oplus V_k
\]</span> where <span class="math inline">\(V_i \perp V_j\)</span> for all <span class="math inline">\(i \ne j\)</span>.</p>
<p>Then any vector <span class="math inline">\(y\)</span> can be uniquely written as:</p>
<p><span class="math display">\[
y = \hat{y}_1 + \hat{y}_2 + \dots + \hat{y}_k
\]</span> where <span class="math inline">\(\hat{y}_i \in V_i\)</span>.</p>
<p>Furthermore, each component <span class="math inline">\(\hat{y}_i\)</span> is simply the projection of <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(V_i\)</span>:</p>
<p><span class="math display">\[
\hat{y}_i = P_i y
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p>Existence: Since <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of <span class="math inline">\(V_1, \dots, V_k\)</span>, by definition, any vector <span class="math inline">\(y \in \mathbb{R}^n\)</span> can be written as a sum <span class="math inline">\(y = v_1 + \dots + v_k\)</span> where <span class="math inline">\(v_i \in V_i\)</span>.</p></li>
<li><p>Uniqueness: Suppose there are two such representations: <span class="math inline">\(y = \sum v_i = \sum w_i\)</span>, with <span class="math inline">\(v_i, w_i \in V_i\)</span>. Then <span class="math inline">\(\sum (v_i - w_i) = 0\)</span>. Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, <span class="math inline">\(v_i - w_i = 0 \implies v_i = w_i\)</span>. The representation is unique. Let <span class="math inline">\(\hat{y}_i = v_i\)</span>.</p></li>
<li><p>Projection Property: We claim that the <span class="math inline">\(i\)</span>-th component <span class="math inline">\(\hat{y}_i\)</span> is the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(V_i\)</span>. We must show that the residual <span class="math inline">\((y - \hat{y}_i)\)</span> is orthogonal to <span class="math inline">\(V_i\)</span>.</p>
<p><span class="math display">\[
y - \hat{y}_i = \sum_{j \ne i} \hat{y}_j
\]</span> Let <span class="math inline">\(z\)</span> be any vector in <span class="math inline">\(V_i\)</span>. We calculate the inner product: <span class="math display">\[
\langle y - \hat{y}_i, z \rangle = \left\langle \sum_{j \ne i} \hat{y}_j, z \right\rangle = \sum_{j \ne i} \langle \hat{y}_j, z \rangle
\]</span> Since <span class="math inline">\(\hat{y}_j \in V_j\)</span> and <span class="math inline">\(z \in V_i\)</span>, and the subspaces are mutually orthogonal (<span class="math inline">\(V_j \perp V_i\)</span> for <span class="math inline">\(j \ne i\)</span>), every term in the sum is zero. Therefore, <span class="math inline">\((y - \hat{y}_i) \perp V_i\)</span>. By the definition of orthogonal projection, <span class="math inline">\(\hat{y}_i = P_i y\)</span>.</p></li>
</ol>
</div>
<p>This implies that the identity matrix can be decomposed into a sum of projection matrices: <span class="math display">\[
I_n = P_1 + P_2 + \dots + P_k
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-orthogonal-decomp-rotated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-orthogonal-decomp-rotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-orthogonal-decomp-rotated-9.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-orthogonal-decomp-rotated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Orthogonal decomposition of vector y into subspaces
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Define Vectors ---</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>origin <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Projections (P_i y)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">0</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial Sums (P_i y + P_j y)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>sum_12 <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>sum_13 <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p3</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>sum_23 <span class="ot">&lt;-</span> p2 <span class="sc">+</span> p3</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Helper Functions ---</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to add a vector with an arrowhead (Cone)</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>add_vec_arrow <span class="ot">&lt;-</span> <span class="cf">function</span>(p, start, end, color, name) {</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  p <span class="sc">%&gt;%</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"scatter3d"</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>      <span class="at">mode =</span> <span class="st">"lines"</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">c</span>(start[<span class="dv">1</span>], end[<span class="dv">1</span>]),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">c</span>(start[<span class="dv">2</span>], end[<span class="dv">2</span>]),</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">z =</span> <span class="fu">c</span>(start[<span class="dv">3</span>], end[<span class="dv">3</span>]),</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fu">list</span>(<span class="at">color =</span> color, <span class="at">width =</span> <span class="dv">6</span>),</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>      <span class="at">showlegend =</span> <span class="cn">TRUE</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"cone"</span>,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> end[<span class="dv">1</span>], <span class="at">y =</span> end[<span class="dv">2</span>], <span class="at">z =</span> end[<span class="dv">3</span>],</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>      <span class="at">u =</span> end[<span class="dv">1</span>]<span class="sc">-</span>start[<span class="dv">1</span>], <span class="at">v =</span> end[<span class="dv">2</span>]<span class="sc">-</span>start[<span class="dv">2</span>], <span class="at">w =</span> end[<span class="dv">3</span>]<span class="sc">-</span>start[<span class="dv">3</span>],</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>      <span class="at">sizemode =</span> <span class="st">"absolute"</span>,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>      <span class="at">sizeref =</span> <span class="fl">0.5</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>      <span class="at">anchor =</span> <span class="st">"tip"</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>      <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(color, color)),</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>      <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>      <span class="at">showlegend =</span> <span class="cn">FALSE</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to add dashed "error" lines</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>add_dashed_line <span class="ot">&lt;-</span> <span class="cf">function</span>(p, start, end, color, name) {</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>  p <span class="sc">%&gt;%</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add_trace</span>(</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>      <span class="at">type =</span> <span class="st">"scatter3d"</span>,</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>      <span class="at">mode =</span> <span class="st">"lines"</span>,</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">c</span>(start[<span class="dv">1</span>], end[<span class="dv">1</span>]),</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">c</span>(start[<span class="dv">2</span>], end[<span class="dv">2</span>]),</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>      <span class="at">z =</span> <span class="fu">c</span>(start[<span class="dv">3</span>], end[<span class="dv">3</span>]),</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fu">list</span>(<span class="at">color =</span> color, <span class="at">width =</span> <span class="dv">3</span>, <span class="at">dash =</span> <span class="st">"dash"</span>),</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>      <span class="at">name =</span> name,</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>      <span class="at">hoverinfo =</span> <span class="st">"text"</span>,</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>      <span class="at">text =</span> name</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Build Plot ---</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> <span class="fu">plot_ly</span>()</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Main Vectors (Solid + Cones)</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p1, <span class="st">"red"</span>, <span class="st">"P1 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p2, <span class="st">"green"</span>, <span class="st">"P2 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, p3, <span class="st">"blue"</span>, <span class="st">"P3 y"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_vec_arrow</span>(origin, y_vec, <span class="st">"black"</span>, <span class="st">"y"</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Dashed Lines from y to Single Projections</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p1, <span class="st">"rgba(255, 0, 0, 0.5)"</span>, <span class="st">"y -&gt; P1"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p2, <span class="st">"rgba(0, 255, 0, 0.5)"</span>, <span class="st">"y -&gt; P2"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, p3, <span class="st">"rgba(0, 0, 255, 0.5)"</span>, <span class="st">"y -&gt; P3"</span>)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Dashed Lines from y to Partial Sums</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_12, <span class="st">"purple"</span>, <span class="st">"y -&gt; (P1+P2)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_13, <span class="st">"orange"</span>, <span class="st">"y -&gt; (P1+P3)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_dashed_line</span>(y_vec, sum_23, <span class="st">"cyan"</span>,   <span class="st">"y -&gt; (P2+P3)"</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Axes (Subspaces)</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>limit <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>axis_style <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">"gray"</span>, <span class="at">dash =</span> <span class="st">"dot"</span>, <span class="at">width =</span> <span class="dv">2</span>)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), </span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V1 (x)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), </span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V2 (y)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(<span class="at">type=</span><span class="st">"scatter3d"</span>, <span class="at">mode=</span><span class="st">"lines"</span>, <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">z=</span><span class="fu">c</span>(<span class="dv">0</span>, limit), </span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>            <span class="at">line=</span>axis_style, <span class="at">name=</span><span class="st">"V3 (z)"</span>)</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Layout ---</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>fig <span class="ot">&lt;-</span> fig <span class="sc">%&gt;%</span> <span class="fu">layout</span>(</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Orthogonal Decomposition Geometry"</span>,</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>  <span class="at">width =</span> <span class="dv">900</span>,</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>  <span class="at">height =</span> <span class="dv">700</span>,</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>  <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V1"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V2"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"V3"</span>, <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, limit)),</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    <span class="at">aspectmode =</span> <span class="st">"cube"</span>,</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    <span class="at">camera =</span> <span class="fu">list</span>(<span class="at">eye =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">1.5</span>, <span class="at">y =</span> <span class="fl">1.5</span>, <span class="at">z =</span> <span class="fl">1.2</span>))</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>  <span class="at">margin =</span> <span class="fu">list</span>(<span class="at">l =</span> <span class="dv">0</span>, <span class="at">r =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">t =</span> <span class="dv">50</span>),</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">0.75</span>, <span class="at">y =</span> <span class="fl">0.9</span>)</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>fig</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-orthogonal-decomp-r" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-orthogonal-decomp-r-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-796ae23e8702ce5fd734" style="width:100%;height:520px;"></div>
<script type="application/json" data-for="htmlwidget-796ae23e8702ce5fd734">{"x":{"visdat":{"1562593e24a8":["function () ","plotlyVisDat"]},"cur_data":"1562593e24a8","attrs":{"1562593e24a8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"red","width":6},"name":"P1 y","showlegend":true,"inherit":true},"1562593e24a8.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":3,"y":0,"z":0,"u":3,"v":0,"w":0,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["red","red"]],"showscale":false,"name":"P1 y","showlegend":false,"inherit":true},"1562593e24a8.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,4],"z":[0,0],"line":{"color":"green","width":6},"name":"P2 y","showlegend":true,"inherit":true},"1562593e24a8.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":0,"y":4,"z":0,"u":0,"v":4,"w":0,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["green","green"]],"showscale":false,"name":"P2 y","showlegend":false,"inherit":true},"1562593e24a8.4":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,5],"line":{"color":"blue","width":6},"name":"P3 y","showlegend":true,"inherit":true},"1562593e24a8.5":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":0,"y":0,"z":5,"u":0,"v":0,"w":5,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["blue","blue"]],"showscale":false,"name":"P3 y","showlegend":false,"inherit":true},"1562593e24a8.6":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,5],"line":{"color":"black","width":6},"name":"y","showlegend":true,"inherit":true},"1562593e24a8.7":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"cone","x":3,"y":4,"z":5,"u":3,"v":4,"w":5,"sizemode":"absolute","sizeref":0.5,"anchor":"tip","colorscale":[[0,1],["black","black"]],"showscale":false,"name":"y","showlegend":false,"inherit":true},"1562593e24a8.8":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,0],"line":{"color":"rgba(255, 0, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P1","hoverinfo":"text","text":"y -> P1","inherit":true},"1562593e24a8.9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,0],"line":{"color":"rgba(0, 255, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P2","hoverinfo":"text","text":"y -> P2","inherit":true},"1562593e24a8.10":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,0],"z":[5,5],"line":{"color":"rgba(0, 0, 255, 0.5)","width":3,"dash":"dash"},"name":"y -> P3","hoverinfo":"text","text":"y -> P3","inherit":true},"1562593e24a8.11":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,4],"z":[5,0],"line":{"color":"purple","width":3,"dash":"dash"},"name":"y -> (P1+P2)","hoverinfo":"text","text":"y -> (P1+P2)","inherit":true},"1562593e24a8.12":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,5],"line":{"color":"orange","width":3,"dash":"dash"},"name":"y -> (P1+P3)","hoverinfo":"text","text":"y -> (P1+P3)","inherit":true},"1562593e24a8.13":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,5],"line":{"color":"cyan","width":3,"dash":"dash"},"name":"y -> (P2+P3)","hoverinfo":"text","text":"y -> (P2+P3)","inherit":true},"1562593e24a8.14":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,6],"y":[0,0],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V1 (x)","inherit":true},"1562593e24a8.15":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,6],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V2 (y)","inherit":true},"1562593e24a8.16":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,6],"line":{"color":"gray","dash":"dot","width":2},"name":"V3 (z)","inherit":true}},"layout":{"width":900,"height":700,"margin":{"b":0,"l":0,"t":50,"r":0},"title":"Orthogonal Decomposition Geometry","scene":{"xaxis":{"title":"V1","range":[0,6]},"yaxis":{"title":"V2","range":[0,6]},"zaxis":{"title":"V3","range":[0,6]},"aspectmode":"cube","camera":{"eye":{"x":1.5,"y":1.5,"z":1.2}}},"legend":{"x":0.75,"y":0.90000000000000002},"xaxis":{"title":[]},"yaxis":{"title":[]},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"red","width":6},"name":"P1 y","showlegend":true,"marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"red"]],"showscale":false,"type":"cone","x":[3],"y":[0],"z":[0],"u":[3],"v":[0],"w":[0],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P1 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,4],"z":[0,0],"line":{"color":"green","width":6},"name":"P2 y","showlegend":true,"marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"green"],[1,"green"]],"showscale":false,"type":"cone","x":[0],"y":[4],"z":[0],"u":[0],"v":[4],"w":[0],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P2 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,5],"line":{"color":"blue","width":6},"name":"P3 y","showlegend":true,"marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"blue"],[1,"blue"]],"showscale":false,"type":"cone","x":[0],"y":[0],"z":[5],"u":[0],"v":[0],"w":[5],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"P3 y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,5],"line":{"color":"black","width":6},"name":"y","showlegend":true,"marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"black"],[1,"black"]],"showscale":false,"type":"cone","x":[3],"y":[4],"z":[5],"u":[3],"v":[4],"w":[5],"sizemode":"absolute","sizeref":0.5,"anchor":"tip","name":"y","showlegend":false,"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,0],"line":{"color":"rgba(255, 0, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P1","hoverinfo":["text","text"],"text":["y -> P1","y -> P1"],"marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,0],"line":{"color":"rgba(0, 255, 0, 0.5)","width":3,"dash":"dash"},"name":"y -> P2","hoverinfo":["text","text"],"text":["y -> P2","y -> P2"],"marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,0],"z":[5,5],"line":{"color":"rgba(0, 0, 255, 0.5)","width":3,"dash":"dash"},"name":"y -> P3","hoverinfo":["text","text"],"text":["y -> P3","y -> P3"],"marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,4],"z":[5,0],"line":{"color":"purple","width":3,"dash":"dash"},"name":"y -> (P1+P2)","hoverinfo":["text","text"],"text":["y -> (P1+P2)","y -> (P1+P2)"],"marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[5,5],"line":{"color":"orange","width":3,"dash":"dash"},"name":"y -> (P1+P3)","hoverinfo":["text","text"],"text":["y -> (P1+P3)","y -> (P1+P3)"],"marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[5,5],"line":{"color":"cyan","width":3,"dash":"dash"},"name":"y -> (P2+P3)","hoverinfo":["text","text"],"text":["y -> (P2+P3)","y -> (P2+P3)"],"marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,6],"y":[0,0],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V1 (x)","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,6],"z":[0,0],"line":{"color":"gray","dash":"dot","width":2},"name":"V2 (y)","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,0],"y":[0,0],"z":[0,6],"line":{"color":"gray","dash":"dot","width":2},"name":"V3 (z)","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-orthogonal-decomp-r-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Orthogonal decomposition geometry (R Plotly)
</figcaption>
</figure>
</div>
</div>
<div id="thm-complete-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 26 (Complete Orthogonal Decomposition of <span class="math inline">\(\mathbb{R}^n\)</span>)</strong></span> Let <span class="math inline">\(P_0, P_1, \dots, P_k\)</span> be a sequence of orthogonal projection matrices with nested column spaces: <span class="math display">\[
\text{Col}(P_0) \subseteq \text{Col}(P_1) \subseteq \dots \subseteq \text{Col}(P_k)
\]</span></p>
<p>Define the sequence of difference matrices <span class="math inline">\(\Delta P_i\)</span> and their column spaces <span class="math inline">\(V_i\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
\Delta P_0 &amp;= P_0, &amp; V_0 &amp;= \text{Col}(\Delta P_0) \\
\Delta P_i &amp;= P_i - P_{i-1} \quad (1 \le i \le k), &amp; V_i &amp;= \text{Col}(\Delta P_i) \\
\Delta P_{k+1} &amp;= I - P_k, &amp; V_{k+1} &amp;= \text{Col}(\Delta P_{k+1})
\end{align*}\]</span></p>
<p><strong>Conclusion:</strong></p>
<ol type="1">
<li><p><strong>Projection Property:</strong> Each <span class="math inline">\(\Delta P_i\)</span> is the orthogonal projection matrix onto <span class="math inline">\(V_i\)</span> for <span class="math inline">\(i = 0, \dots, k+1\)</span>.</p></li>
<li><p><strong>Mutual Orthogonality:</strong> The collection <span class="math inline">\(\{\Delta P_i\}\)</span> are mutually orthogonal operators:</p></li>
</ol>
<p><span class="math display">\[ \Delta P_i \Delta P_j = 0 \quad \text{for all } i \ne j \]</span></p>
<ol start="3" type="1">
<li><strong>Direct Sum Decomposition:</strong> The vector space <span class="math inline">\(\mathbb{R}^n\)</span> is the direct sum of these orthogonal subspaces:</li>
</ol>
<p><span class="math display">\[ \mathbb{R}^n = V_0 \oplus V_1 \oplus \dots \oplus V_{k+1} \]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p>Proof that <span class="math inline">\(\Delta P_i\)</span> is the Projection onto <span class="math inline">\(V_i\)</span></p>
<p>We must show each <span class="math inline">\(\Delta P_i\)</span> is symmetric and idempotent.</p>
<ul>
<li>For <span class="math inline">\(\Delta P_0 = P_0\)</span>: True by definition.</li>
<li>For <span class="math inline">\(\Delta P_i\)</span> (<span class="math inline">\(1 \le i \le k\)</span>):
<ul>
<li><strong>Symmetry:</strong> Difference of symmetric matrices ($P_i, P_{i-1} $) is symmetric.</li>
<li><strong>Idempotency:</strong> <span class="math inline">\((\Delta P_i)^2 = (P_i - P_{i-1})^2 = P_i^2    - P_i P_{i-1} - P_{i-1} P_i + P_{i-1}^2\)</span>. Using nested properties (<span class="math inline">\(P_i P_{i-1} = P_{i-1}\)</span>), this simplifies to <span class="math inline">\(P_i -    P_{i-1} = \Delta P_i\)</span>.</li>
</ul></li>
<li>For <span class="math inline">\(\Delta P_{k+1} = I - P_k\)</span>:
<ul>
<li><strong>Symmetry:</strong> <span class="math inline">\((I - P_k)' = I - P_k\)</span>.</li>
<li><strong>Idempotency:</strong> <span class="math inline">\((I - P_k)^2 = I - 2P_k + P_k^2 = I - P_k\)</span>.</li>
</ul></li>
</ul></li>
<li><p>Proof of Mutual Orthogonality</p>
<p>We show <span class="math inline">\(\Delta P_j \Delta P_i = 0\)</span> for <span class="math inline">\(i &lt; j\)</span>.</p>
<ul>
<li><strong>Case 1: Both indices</strong> <span class="math inline">\(\le k\)</span> (i.e., <span class="math inline">\(1 \le i &lt; j \le k\)</span>):</li>
</ul>
<p><span class="math display">\[    (P_j - P_{j-1})(P_i - P_{i-1}) = P_j P_i - P_j P_{i-1} - P_{j-1} P_i    + P_{j-1} P_{i-1} \]</span> Since <span class="math inline">\(\text{Col}(P_i) \subseteq \text{Col}(P_   {j-1})\)</span>, all terms reduce to <span class="math inline">\(P_i - P_{i-1} - P_i + P_{i-1} = 0\)</span>.</p>
<ul>
<li><strong>Case 2: One index is the residual</strong> (<span class="math inline">\(j = k+1\)</span>): We check <span class="math inline">\(\Delta P_{k+1} \Delta P_i = (I - P_k)\Delta P_i\)</span> for any <span class="math inline">\(i \le k\)</span>. Since <span class="math inline">\(V_i \subseteq \text{Col}(P_k)\)</span>, we have <span class="math inline">\(P_k \Delta P_i =    \Delta P_i\)</span>.</li>
</ul>
<p><span class="math display">\[ (I - P_k)\Delta P_i = \Delta P_i - P_k \Delta P_i =    \Delta P_i - \Delta P_i = 0 \]</span></p></li>
<li><p>Proof of Direct Sum</p>
<p>The sum of the difference matrices forms a telescoping series:</p>
<p><span class="math display">\[    \sum_{j=0}^{k+1} \Delta P_j = P_0 + \sum_{i=1}^k (P_i - P_{i-1}) +    (I - P_k) \]</span> <span class="math display">\[ = P_k + (I - P_k) = I \]</span> Since the identity operator <span class="math inline">\(I\)</span> (which maps <span class="math inline">\(\mathbb{R}^n\)</span> to itself) is the sum of mutually orthogonal projection operators, the space <span class="math inline">\(\mathbb{R}^n\)</span> decomposes into the direct sum of their respective image subspaces <span class="math inline">\(V_i\)</span>.</p></li>
</ol>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-venn-nested-projection" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 80% !important;" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-venn-nested-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-venn-nested-projection-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 80% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-venn-nested-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Venn Diagram of Nested Projections with Colored Increments
</figcaption>
</figure>
</div>
</div>
</div>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="matrix-algebra" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Matrix Algebra</h1>
<p>This chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, singular value decomposition.</p>
<section id="eigenvalues-and-eigenvectors" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">3.1</span> Eigenvalues and Eigenvectors</h2>
<div id="def-eigen" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 28 (Eigenvalues and Eigenvectors)</strong></span> For a square matrix <span class="math inline">\(A\)</span> (<span class="math inline">\(n \times n\)</span>), a scalar <span class="math inline">\(\lambda\)</span> is an <strong>eigenvalue</strong> and a non-zero vector <span class="math inline">\(x\)</span> is the corresponding <strong>eigenvector</strong> if:</p>
<p><span class="math display">\[
Ax = \lambda x \iff (A - \lambda I_n)x = 0
\]</span></p>
<p>The eigenvalues are found by solving the characteristic equation: <span class="math display">\[
|A - \lambda I_n| = 0
\]</span></p>
</div>
</section>
<section id="spectral-theory-for-symmetric-matrices" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="spectral-theory-for-symmetric-matrices"><span class="header-section-number">3.2</span> Spectral Theory for Symmetric Matrices</h2>
<section id="spectral-decomposition" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="spectral-decomposition"><span class="header-section-number">3.2.1</span> Spectral Decomposition</h3>
<p>For symmetric matrices, we have a powerful decomposition theorem.</p>
<div id="thm-spectral" class="theorem">
<p><span class="theorem-title"><strong>Theorem 27 (Spectral Decomposition)</strong></span> If <span class="math inline">\(A\)</span> is a symmetric <span class="math inline">\(n \times n\)</span> matrix, all its eigenvalues <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span> are real. Furthermore, there exists an orthogonal matrix <span class="math inline">\(Q\)</span> such that:</p>
<p><span class="math display">\[
A = Q \Lambda Q' = \sum_{i=1}^n \lambda_i q_i q_i'
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\)</span> contains the eigenvalues.</li>
<li><span class="math inline">\(Q = (q_1, \dots, q_n)\)</span> contains the corresponding orthonormal eigenvectors (<span class="math inline">\(q_i'q_j = \delta_{ij}\)</span>).</li>
</ul>
</div>
<p><strong>Explantion</strong>: This allows us to view the transformation <span class="math inline">\(Ax\)</span> as a rotation (<span class="math inline">\(Q'\)</span>), a scaling (<span class="math inline">\(\Lambda\)</span>), and a rotation back (<span class="math inline">\(Q\)</span>). For a symmetric matrix <span class="math inline">\(A\)</span>, we can write the spectral decomposition as a product of the eigenvector matrix <span class="math inline">\(Q\)</span> and eigenvalue matrix <span class="math inline">\(\Lambda\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A &amp;= Q \Lambda Q' \\
  &amp;= \begin{pmatrix} q_1 &amp; q_2 &amp; \cdots &amp; q_n \end{pmatrix}
     \begin{pmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}
     \begin{pmatrix} q_1' \\ q_2' \\ \vdots \\ q_n' \end{pmatrix} \\
  &amp;= \begin{pmatrix} \lambda_1 q_1 &amp; \lambda_2 q_2 &amp; \cdots &amp; \lambda_n q_n \end{pmatrix}
     \begin{pmatrix} q_1' \\ q_2' \\ \vdots \\ q_n' \end{pmatrix} \\
  &amp;= \lambda_1 q_1 q_1' + \lambda_2 q_2 q_2' + \cdots + \lambda_n q_n q_n' \\
  &amp;= \sum_{i=1}^n \lambda_i q_i q_i'
\end{aligned}
\]</span></p>
<p>where the eigenvectors <span class="math inline">\(q_i\)</span> satisfy the orthogonality conditions: <span class="math display">\[
q_i' q_j = \begin{cases} 1 &amp; \text{if } i=j \\ 0 &amp; \text{if } i \ne j \end{cases}
\]</span> And <span class="math inline">\(Q\)</span> is an orthogonal matrix: <span class="math inline">\(Q'Q = Q Q' = I_n\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. MATRIX SETUP ---</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Symmetric Matrix where eigenvectors are tilted</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">1.5</span>, <span class="fl">0.8</span>, <span class="fl">0.8</span>, <span class="fl">1.5</span>), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Decomposition A = QDQ'</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>eig <span class="ot">&lt;-</span> <span class="fu">eigen</span>(A)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> eig<span class="sc">$</span>vectors</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>D_mat <span class="ot">&lt;-</span> <span class="fu">diag</span>(eig<span class="sc">$</span>values)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. DEFINE THE 6 VECTORS ---</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 &amp; 2: Standard Axes (We will label these x1, x2)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>v1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>v2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 &amp; 4: Eigenvectors</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>v3 <span class="ot">&lt;-</span> Q[,<span class="dv">1</span>]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>v4 <span class="ot">&lt;-</span> Q[,<span class="dv">2</span>]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 &amp; 6: Filler vectors at random angles</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>v5 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">cos</span>(pi<span class="sc">/</span><span class="dv">3</span>), <span class="fu">sin</span>(pi<span class="sc">/</span><span class="dv">3</span>))</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>v6 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">cos</span>(<span class="dv">4</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">3</span>), <span class="fu">sin</span>(<span class="dv">4</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">3</span>))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine into starting matrix V_start</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>V_start <span class="ot">&lt;-</span> <span class="fu">cbind</span>(v1, v2, v3, v4, v5, v6)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define 6 Distinct Colors</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>my_colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"#E41A1C"</span>, <span class="st">"#377EB8"</span>, <span class="st">"#4DAF4A"</span>, <span class="st">"#984EA3"</span>, <span class="st">"#FF7F00"</span>, <span class="st">"#A65628"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(my_colors) <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Background Circle Points used for reference path in all plots</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>theta_c <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi, <span class="at">length.out =</span> <span class="dv">150</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>C_start <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">cos</span>(theta_c), <span class="fu">sin</span>(theta_c))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. DATA PROCESSING HELPER FUNCTION ---</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># This function prepares the data frames for ggplot for a given stage</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>prepare_data <span class="ot">&lt;-</span> <span class="cf">function</span>(V_mat, C_mat, stage_title, label_text_pair) {</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare Vectors data frame</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>  df_v <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">t</span>(V_mat))</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(df_v) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>vec_id <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>) <span class="co"># Unique ID for coloring</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Add labels only for vector 1 and 2</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>label <span class="ot">&lt;-</span> <span class="st">""</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>label[<span class="dv">1</span>] <span class="ot">&lt;-</span> label_text_pair[<span class="dv">1</span>]</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>label[<span class="dv">2</span>] <span class="ot">&lt;-</span> label_text_pair[<span class="dv">2</span>]</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate nudge for labels based on vector direction so they don't overlap arrow tip</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>nudge_x <span class="ot">&lt;-</span> <span class="fu">sign</span>(df_v<span class="sc">$</span>x) <span class="sc">*</span> <span class="fl">0.25</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>nudge_y <span class="ot">&lt;-</span> <span class="fu">sign</span>(df_v<span class="sc">$</span>y) <span class="sc">*</span> <span class="fl">0.25</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Don't nudge unlabelled vectors</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>nudge_x[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>  df_v<span class="sc">$</span>nudge_y[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare Background Path data frame</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>  df_c <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">t</span>(C_mat))</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(df_c) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"px"</span>, <span class="st">"py"</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">vecs =</span> df_v, <span class="at">path =</span> df_c, <span class="at">title =</span> stage_title)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. PERFORM TRANSFORMATIONS ---</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Stage 1: Start (x)</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">prepare_data</span>(V_start, C_start, </span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"1. Start (x)"</span>, <span class="fu">c</span>(<span class="st">"x[1]"</span>, <span class="st">"x[2]"</span>))</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Stage 2: Rotate (Q'x)</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>V2 <span class="ot">&lt;-</span> <span class="fu">t</span>(Q) <span class="sc">%*%</span> V_start</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>C2 <span class="ot">&lt;-</span> <span class="fu">t</span>(Q) <span class="sc">%*%</span> C_start</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">prepare_data</span>(V2, C2, </span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"2. Rotate (Q'x)"</span>, <span class="fu">c</span>(<span class="st">"z[1]"</span>, <span class="st">"z[2]"</span>))</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Stage 3: Stretch (DQ'x)</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>V3 <span class="ot">&lt;-</span> D_mat <span class="sc">%*%</span> V2</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>C3 <span class="ot">&lt;-</span> D_mat <span class="sc">%*%</span> C2</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>d3 <span class="ot">&lt;-</span> <span class="fu">prepare_data</span>(V3, C3, </span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"3. Stretch (DQ'x)"</span>, <span class="fu">c</span>(<span class="st">"y[1]"</span>, <span class="st">"y[2]"</span>))</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Stage 4: Rotate Back (QDQ'x)</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>V4 <span class="ot">&lt;-</span> Q <span class="sc">%*%</span> V3</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>C4 <span class="ot">&lt;-</span> Q <span class="sc">%*%</span> C3</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>d4 <span class="ot">&lt;-</span> <span class="fu">prepare_data</span>(V4, C4, </span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"4. Final (QDQ'x)"</span>, <span class="fu">c</span>(<span class="st">"w[1]"</span>, <span class="st">"w[2]"</span>))</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 5. PLOTTING FUNCTION ---</span></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>plot_stage_final <span class="ot">&lt;-</span> <span class="cf">function</span>(data_list) {</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Background path (gray dashed)</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_path</span>(<span class="at">data =</span> data_list<span class="sc">$</span>path, <span class="fu">aes</span>(<span class="at">x=</span>px, <span class="at">y=</span>py), </span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>              <span class="at">color=</span><span class="st">"gray70"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The 6 vectors</span></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="at">data =</span> data_list<span class="sc">$</span>vecs, <span class="fu">aes</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span><span class="dv">0</span>, <span class="at">xend=</span>x, <span class="at">yend=</span>y, <span class="at">color=</span>vec_id), </span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>                 <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.3</span>, <span class="st">"cm"</span>)), <span class="at">size=</span><span class="fl">1.1</span>) <span class="sc">+</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The labels for v1 and v2 using parsed expressions for subscripts</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="at">data =</span> data_list<span class="sc">$</span>vecs, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y, <span class="at">label=</span>label, <span class="at">color=</span>vec_id),</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>              <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">fontface=</span><span class="st">"bold"</span>, <span class="at">size=</span><span class="dv">5</span>,</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>              <span class="at">nudge_x =</span> data_list<span class="sc">$</span>vecs<span class="sc">$</span>nudge_x,</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>              <span class="at">nudge_y =</span> data_list<span class="sc">$</span>vecs<span class="sc">$</span>nudge_y) <span class="sc">+</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values =</span> my_colors) <span class="sc">+</span></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fixed coordinates to ensure realistic rotation/stretching view</span></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_fixed</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>)) <span class="sc">+</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>,</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>          <span class="at">panel.grid.minor =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>          <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face=</span><span class="st">"bold"</span>, <span class="at">hjust=</span><span class="fl">0.5</span>),</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>          <span class="at">axis.title =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> data_list<span class="sc">$</span>title)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the 4 plots</span></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_stage_final</span>(d1)</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_stage_final</span>(d2)</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_stage_final</span>(d3)</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">plot_stage_final</span>(d4)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Arrange them in a grid</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1, p2, p3, p4, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-eigen" class="quarto-float quarto-figure quarto-figure-center anchored" width="576">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eigen-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-eigen-1.png" id="fig-eigen" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-eigen-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="quadratic-form" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="quadratic-form"><span class="header-section-number">3.2.2</span> Quadratic Form</h3>
<div id="def-quadratic-form" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 29</strong></span> A <strong>quadratic form</strong> in <span class="math inline">\(n\)</span> variables <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> is a scalar function defined by a symmetric matrix <span class="math inline">\(A\)</span>: <span class="math display">\[
Q(x) = x'Ax = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j
\]</span></p>
</div>
</section>
<section id="positive-and-non-negative-definite-matrices" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="positive-and-non-negative-definite-matrices"><span class="header-section-number">3.2.3</span> Positive and Non-Negative Definite Matrices</h3>
<div id="def-pos-def" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 30 (Positive and Non-Negative Definite Matrices)</strong></span> A symmetric matrix <span class="math inline">\(A\)</span> is <strong>positive definite (p.d.)</strong> if: <span class="math display">\[
x'Ax &gt; 0 \quad \forall x \ne 0
\]</span> It is <strong>non-negative definite (n.n.d.)</strong> if: <span class="math display">\[
x'Ax \ge 0 \quad \forall x
\]</span></p>
</div>
<div id="thm-nnd-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 28 (Properties of Definite Matrices)</strong></span> Let <span class="math inline">\(A\)</span> be a symmetric <span class="math inline">\(n \times n\)</span> matrix with eigenvalues <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span>.</p>
<ol type="1">
<li><p><strong>Eigenvalue Characterization:</strong></p>
<ul>
<li><span class="math inline">\(A\)</span> is p.d. <span class="math inline">\(\iff\)</span> all <span class="math inline">\(\lambda_i &gt; 0\)</span>.</li>
<li><span class="math inline">\(A\)</span> is n.n.d. <span class="math inline">\(\iff\)</span> all <span class="math inline">\(\lambda_i \ge 0\)</span>.</li>
</ul></li>
<li><p><strong>Determinant and Inverse:</strong></p>
<ul>
<li>If <span class="math inline">\(A\)</span> is p.d., then <span class="math inline">\(|A| &gt; 0\)</span> and <span class="math inline">\(A^{-1}\)</span> exists.</li>
<li>If <span class="math inline">\(A\)</span> is n.n.d. and singular, then <span class="math inline">\(|A| = 0\)</span> (at least one <span class="math inline">\(\lambda_i = 0\)</span>).</li>
</ul></li>
<li><p><strong>Gram Matrices (<span class="math inline">\(B'B\)</span>):</strong> Let <span class="math inline">\(B\)</span> be an <span class="math inline">\(n \times p\)</span> matrix.</p>
<ul>
<li>If <span class="math inline">\(\text{rank}(B) = p\)</span>, then <span class="math inline">\(B'B\)</span> is p.d.</li>
<li>If <span class="math inline">\(\text{rank}(B) &lt; p\)</span>, then <span class="math inline">\(B'B\)</span> is n.n.d.</li>
</ul></li>
</ol>
</div>
</section>
<section id="properties-of-symmetric-matrices" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="properties-of-symmetric-matrices"><span class="header-section-number">3.2.4</span> Properties of Symmetric Matrices</h3>
<div id="thm-symmetric-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 29 (Properties of Symmetric Matrices)</strong></span> Let <span class="math inline">\(A\)</span> be a symmetric matrix with spectral decomposition <span class="math inline">\(A = Q \Lambda Q'\)</span>. The following properties hold:</p>
<ol type="1">
<li><strong>Trace:</strong> <span class="math inline">\(\text{tr}(A) = \sum \lambda_i\)</span>.</li>
<li><strong>Determinant:</strong> <span class="math inline">\(|A| = \prod \lambda_i\)</span>.</li>
<li><strong>Singularity:</strong> <span class="math inline">\(A\)</span> is singular if and only if at least one <span class="math inline">\(\lambda_i = 0\)</span>.</li>
<li><strong>Inverse:</strong> If <span class="math inline">\(A\)</span> is non-singular (<span class="math inline">\(\lambda_i \ne 0\)</span>), then <span class="math inline">\(A^{-1} = Q \Lambda^{-1} Q'\)</span>.</li>
<li><strong>Powers:</strong> <span class="math inline">\(A^k = Q \Lambda^k Q'\)</span>.
<ul>
<li><em>Square Root:</em> <span class="math inline">\(A^{1/2} = Q \Lambda^{1/2} Q'\)</span> (if <span class="math inline">\(\lambda_i \ge 0\)</span>).</li>
</ul></li>
<li><strong>Spectral Representation of Quadratic Forms:</strong> The quadratic form <span class="math inline">\(x'Ax\)</span> can be diagonalized using the eigenvectors of <span class="math inline">\(A\)</span>: <span class="math display">\[
x'Ax = x' Q \Lambda Q' x = y' \Lambda y = \sum_{i=1}^n \lambda_i y_i^2
\]</span> where <span class="math inline">\(y = Q'x\)</span> represents a rotation of the coordinate system.</li>
</ol>
</div>
</section>
<section id="spectral-representation-of-projection-matrices" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="spectral-representation-of-projection-matrices"><span class="header-section-number">3.2.5</span> Spectral Representation of Projection Matrices</h3>
<p>We revisit projection matrices in the context of eigenvalues.</p>
<div id="thm-proj-eigen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 30 (Eigenvalues of Projection Matrices)</strong></span> A symmetric matrix <span class="math inline">\(P\)</span> is a projection matrix (idempotent, <span class="math inline">\(P^2=P\)</span>) if and only if its eigenvalues are either 0 or 1.</p>
<p><span class="math display">\[
P^2 x = \lambda^2 x \quad \text{and} \quad Px = \lambda x \implies \lambda^2 = \lambda \implies \lambda \in \{0, 1\}
\]</span></p>
</div>
<p>For a projection matrix <span class="math inline">\(P\)</span>:</p>
<ul>
<li>If <span class="math inline">\(x \in \text{Col}(P)\)</span>, <span class="math inline">\(Px = x\)</span> (Eigenvalue 1).</li>
<li>If <span class="math inline">\(x \perp \text{Col}(P)\)</span>, <span class="math inline">\(Px = 0\)</span> (Eigenvalue 0).</li>
<li><span class="math inline">\(\text{rank}(P) = \text{tr}(P) = \sum \lambda_i\)</span> (Count of 1s).</li>
</ul>
<div id="exm-trace-P" class="theorem example">
<p><span class="theorem-title"><strong>Example 7</strong></span> For <span class="math inline">\(P = \frac{1}{n} J_n J_n'\)</span>, the rank is <span class="math inline">\(\text{tr}(P) = 1\)</span>.</p>
</div>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="singular-value-decomposition-svd"><span class="header-section-number">3.3</span> Singular Value Decomposition (SVD)</h2>
<div id="thm-svd" class="theorem">
<p><span class="theorem-title"><strong>Theorem 31 (Singular Value Decomposition (SVD))</strong></span> Let <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix with rank <span class="math inline">\(r \le \min(n, p)\)</span>. <span class="math inline">\(X\)</span> can be decomposed into the product of three matrices:</p>
<p><span class="math display">\[
X = U \mathbf{D} V'
\]</span></p>
<ol type="1">
<li>Partitioned Matrix Form</li>
</ol>
<p><span class="math display">\[
X = \underset{n \times n}{(U_1, U_2)}
\begin{pmatrix}
\Lambda_r &amp; O_{r \times (p-r)} \\
O_{(n-r) \times r} &amp; O_{(n-r) \times (p-r)}
\end{pmatrix}
\underset{p \times p}{
\begin{pmatrix}
V_1' \\
V_2'
\end{pmatrix}
}
\]</span></p>
<ol start="2" type="1">
<li>Detailed Matrix Form</li>
</ol>
<p>Expanding the diagonal matrix explicitly:</p>
<p><span class="math display">\[
X = \underset{n \times n}{(u_1, \dots, u_n)}
\left(
\begin{array}{cccc|c}
\lambda_1 &amp; 0 &amp; \dots &amp; 0 &amp;  \\
0 &amp; \lambda_2 &amp; \dots &amp; 0 &amp; O_{12} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp;  \\
0 &amp; 0 &amp; \dots &amp; \lambda_r &amp;  \\
\hline
&amp; O_{21} &amp; &amp; &amp; O_{22}
\end{array}
\right)
\underset{p \times p}{
\begin{pmatrix}
v_1' \\
\vdots \\
v_p'
\end{pmatrix}
}
\]</span></p>
<ol start="3" type="1">
<li>Reduced Form</li>
</ol>
<p><span class="math display">\[
X = U_1 \Lambda_r V_1' = \sum_{i=1}^r \lambda_i u_i v_i'
\]</span></p>
<p><strong>Properties:</strong></p>
<ol type="1">
<li><strong>Singular Values (<span class="math inline">\(\Lambda_r\)</span>):</strong> <span class="math inline">\(\Lambda_r = \text{diag}(\lambda_1, \dots, \lambda_r)\)</span> contains the singular values (<span class="math inline">\(\lambda_i &gt; 0\)</span>), which are the square roots of the non-zero eigenvalues of <span class="math inline">\(X'X\)</span>.</li>
<li><strong>Orthogonality:</strong>
<ul>
<li><span class="math inline">\(U\)</span> is <span class="math inline">\(n \times n\)</span> orthogonal (<span class="math inline">\(U'U = I_n\)</span>).</li>
<li><span class="math inline">\(V\)</span> is <span class="math inline">\(p \times p\)</span> orthogonal (<span class="math inline">\(V'V = I_p\)</span>).</li>
</ul></li>
</ol>
</div>
<section id="connection-to-gram-matrices" class="level4" data-number="3.3.0.1">
<h4 data-number="3.3.0.1" class="anchored" data-anchor-id="connection-to-gram-matrices"><span class="header-section-number">3.3.0.1</span> Connection to Gram Matrices</h4>
<p>The matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> provide the basis vectors (eigenvectors) for the Gram matrices of <span class="math inline">\(X\)</span>.</p>
<ol type="1">
<li><p><strong>Right Singular Vectors (<span class="math inline">\(V\)</span>):</strong> The columns of <span class="math inline">\(V\)</span> are the eigenvectors of the Gram matrix <span class="math inline">\(X'X\)</span>. <span class="math display">\[
X'X = (U \Lambda V')' (U \Lambda V') = V \Lambda U' U \Lambda V' = V \Lambda^2 V'
\]</span></p>
<ul>
<li>The eigenvalues of <span class="math inline">\(X'X\)</span> are the squared singular values <span class="math inline">\(\lambda_i^2\)</span>.</li>
</ul></li>
<li><p><strong>Left Singular Vectors (<span class="math inline">\(U\)</span>):</strong> The columns of <span class="math inline">\(U\)</span> are the eigenvectors of the Gram matrix <span class="math inline">\(XX'\)</span>. <span class="math display">\[
XX' = (U \Lambda V') (U \Lambda V')' = U \Lambda V' V \Lambda U' = U \Lambda^2 U'
\]</span></p>
<ul>
<li>The eigenvalues of <span class="math inline">\(XX'\)</span> are also <span class="math inline">\(\lambda_i^2\)</span> (for non-zero values).</li>
</ul></li>
</ol>
<div id="exm-svd" class="theorem example">
<p><span class="theorem-title"><strong>Example 8 (Example of SVD)</strong></span> Consider the matrix <span class="math inline">\(X = \begin{pmatrix} 1 &amp; 1 \\ 2 &amp; 2 \end{pmatrix}\)</span>.</p>
<ol type="1">
<li><p><strong>Compute <span class="math inline">\(X'X\)</span> and find <span class="math inline">\(V\)</span>:</strong> <span class="math display">\[
X'X = \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \end{pmatrix} \begin{pmatrix} 1 &amp; 1 \\ 2 &amp; 2 \end{pmatrix} = \begin{pmatrix} 5 &amp; 5 \\ 5 &amp; 5 \end{pmatrix}
\]</span></p>
<ul>
<li>Eigenvalues of <span class="math inline">\(X'X\)</span>: Trace is 10, Determinant is 0. Thus, <span class="math inline">\(\mu_1 = 10, \mu_2 = 0\)</span>.</li>
<li><strong>Singular Values:</strong> <span class="math inline">\(\lambda_1 = \sqrt{10}, \lambda_2 = 0\)</span>.</li>
<li>Eigenvector for <span class="math inline">\(\mu_1=10\)</span>: Normalized <span class="math inline">\(v_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>.</li>
<li>Eigenvector for <span class="math inline">\(\mu_2=0\)</span>: Normalized <span class="math inline">\(v_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix}\)</span>.</li>
<li>Therefore, <span class="math inline">\(V = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix}\)</span>.</li>
</ul></li>
<li><p><strong>Compute <span class="math inline">\(XX'\)</span> and find <span class="math inline">\(U\)</span>:</strong> <span class="math display">\[
XX' = \begin{pmatrix} 1 &amp; 1 \\ 2 &amp; 2 \end{pmatrix} \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \end{pmatrix} = \begin{pmatrix} 2 &amp; 4 \\ 4 &amp; 8 \end{pmatrix}
\]</span></p>
<ul>
<li>Eigenvalues are again 10 and 0.</li>
<li>Eigenvector for <span class="math inline">\(\mu_1=10\)</span>: Normalized <span class="math inline">\(u_1 = \frac{1}{\sqrt{5}}\begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span>.</li>
<li>Eigenvector for <span class="math inline">\(\mu_2=0\)</span>: Normalized <span class="math inline">\(u_2 = \frac{1}{\sqrt{5}}\begin{pmatrix} 2 \\ -1 \end{pmatrix}\)</span>.</li>
<li>Therefore, <span class="math inline">\(U = \frac{1}{\sqrt{5}}\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; -1 \end{pmatrix}\)</span>.</li>
</ul></li>
<li><p><strong>Verification:</strong> <span class="math display">\[
X = \sqrt{10} u_1 v_1' = \sqrt{10} \begin{pmatrix} \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \end{pmatrix} = \begin{pmatrix} 1 &amp; 1 \\ 2 &amp; 2 \end{pmatrix}
\]</span></p></li>
</ol>
</div>
</section>
</section>
<section id="cholesky-decomposition" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="cholesky-decomposition"><span class="header-section-number">3.4</span> Cholesky Decomposition</h2>
<p>A symmetric matrix <span class="math inline">\(A\)</span> has a Cholesky decomposition if and only if it is <strong>non-negative definite</strong> (i.e., <span class="math inline">\(x'Ax \ge 0\)</span> for all <span class="math inline">\(x\)</span>).</p>
<p><span class="math display">\[
A = B'B
\]</span></p>
<p>where <span class="math inline">\(B\)</span> is an <strong>upper triangular</strong> matrix with non-negative diagonal entries.</p>
<section id="matrix-representation-of-the-algorithm" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="matrix-representation-of-the-algorithm"><span class="header-section-number">3.4.1</span> Matrix Representation of the Algorithm</h3>
<p>To derive the algorithm, we equate the elements of <span class="math inline">\(A\)</span> with the product of the lower triangular matrix <span class="math inline">\(B'\)</span> and the upper triangular matrix <span class="math inline">\(B\)</span>.</p>
<p>For a <span class="math inline">\(3 \times 3\)</span> matrix, this looks like:</p>
<p><span class="math display">\[
\underbrace{\begin{pmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{pmatrix}}_{A}
=
\underbrace{\begin{pmatrix}
b_{11} &amp; 0 &amp; 0 \\
b_{12} &amp; b_{22} &amp; 0 \\
b_{13} &amp; b_{23} &amp; b_{33}
\end{pmatrix}}_{B'}
\underbrace{\begin{pmatrix}
b_{11} &amp; b_{12} &amp; b_{13} \\
0 &amp; b_{22} &amp; b_{23} \\
0 &amp; 0 &amp; b_{33}
\end{pmatrix}}_{B}
\]</span></p>
<p>Multiplying the matrices on the right yields the system of equations:</p>
<p><span class="math display">\[
A = \begin{pmatrix}
\mathbf{b_{11}^2} &amp; b_{11}b_{12} &amp; b_{11}b_{13} \\
b_{12}b_{11} &amp; \mathbf{b_{12}^2 + b_{22}^2} &amp; b_{12}b_{13} + b_{22}b_{23} \\
b_{13}b_{11} &amp; b_{13}b_{12} + b_{23}b_{22} &amp; \mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}
\end{pmatrix}
\]</span></p>
<p>By solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.</p>
<div id="alg-chosk-decomp" class="theorem algorithm">
<p><span class="theorem-title"><strong>Algorithm 1 (Choleski Decomposition)</strong></span> &nbsp;</p>
<ol type="1">
<li><strong>Row 1:</strong> Solve for <span class="math inline">\(b_{11}\)</span> using <span class="math inline">\(a_{11}\)</span>, then solve the rest of the row (<span class="math inline">\(b_{1j}\)</span>) by division.
<ul>
<li><span class="math inline">\(b_{11} = \sqrt{a_{11}}\)</span></li>
<li><span class="math inline">\(b_{1j} = a_{1j}/b_{11}\)</span></li>
</ul></li>
<li><strong>Row 2:</strong> Solve for <span class="math inline">\(b_{22}\)</span> using <span class="math inline">\(a_{22}\)</span> and the known <span class="math inline">\(b_{12}\)</span>, then solve <span class="math inline">\(b_{2j}\)</span>.
<ul>
<li><span class="math inline">\(b_{22} = \sqrt{a_{22} - b_{12}^2}\)</span></li>
<li><span class="math inline">\(b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}\)</span></li>
</ul></li>
<li><strong>Row 3:</strong> Solve for <span class="math inline">\(b_{33}\)</span> using <span class="math inline">\(a_{33}\)</span> and the known <span class="math inline">\(b_{13}, b_{23}\)</span>.
<ul>
<li><span class="math inline">\(b_{33} = \sqrt{a_{33} - b_{13}^2 - b_{23}^2}\)</span></li>
</ul></li>
</ol>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span><strong>Handling the Singular Case</strong></p>
<p>If <span class="math inline">\(A\)</span> is positive semi-definite (singular), a diagonal element <span class="math inline">\(b_{ii}\)</span> may evaluate to 0 (or a very small number close to 0 due to floating-point error). Standard algorithms often crash here because calculating off-diagonal terms involves division by <span class="math inline">\(b_{ii}\)</span>.</p>
<p>To handle this robustly without pivoting:</p>
<ul>
<li>If <span class="math inline">\(b_{ii} \approx 0\)</span>, it implies that the entire remaining row <span class="math inline">\(b_{i, i:n}\)</span> must be 0 for the matrix to remain consistent with being positive semi-definite.</li>
<li>The algorithm should explicitly set <span class="math inline">\(b_{ij} = 0\)</span> for all <span class="math inline">\(j \ge i\)</span> and proceed to the next row, rather than attempting division.</li>
</ul>
</div>
<div id="exm-chol" class="theorem example">
<p><span class="theorem-title"><strong>Example 9 (Example of Cholesky Decomposition)</strong></span> Consider the positive definite matrix <span class="math inline">\(A\)</span>: <span class="math display">\[
A = \begin{pmatrix}
4 &amp; 2 &amp; -2 \\
2 &amp; 10 &amp; 2 \\
-2 &amp; 2 &amp; 6
\end{pmatrix}
\]</span></p>
<p>We find <span class="math inline">\(B\)</span> such that <span class="math inline">\(A = B'B\)</span>:</p>
<ol type="1">
<li><strong>First Row of B (<span class="math inline">\(b_{11}, b_{12}, b_{13}\)</span>):</strong>
<ul>
<li><span class="math inline">\(b_{11} = \sqrt{4} = 2\)</span></li>
<li><span class="math inline">\(b_{12} = 2 / 2 = 1\)</span></li>
<li><span class="math inline">\(b_{13} = -2 / 2 = -1\)</span></li>
</ul></li>
<li><strong>Second Row of B (<span class="math inline">\(b_{22}, b_{23}\)</span>):</strong>
<ul>
<li><span class="math inline">\(b_{22} = \sqrt{10 - (1)^2} = \sqrt{9} = 3\)</span></li>
<li><span class="math inline">\(b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1\)</span></li>
</ul></li>
<li><strong>Third Row of B (<span class="math inline">\(b_{33}\)</span>):</strong>
<ul>
<li><span class="math inline">\(b_{33} = \sqrt{6 - (-1)^2 - (1)^2} = \sqrt{4} = 2\)</span></li>
</ul></li>
</ol>
<p><strong>Result:</strong> <span class="math display">\[
B = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
0 &amp; 3 &amp; 1 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\]</span></p>
</div>
</section>
<section id="applications-in-statistics" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="applications-in-statistics"><span class="header-section-number">3.4.2</span> Applications in Statistics</h3>
<p>Cholesky decomposition is preferred over other methods (like LU or SVD) for symmetric positive-definite matrices because it is numerically stable and roughly twice as fast.</p>
<ol type="1">
<li><p>Solving Linear Equations</p>
<p>In linear regression, we solve the normal equations <span class="math inline">\((X'X)\beta = X'y\)</span>. Since <span class="math inline">\(X'X\)</span> is symmetric and positive definite, we can decompose it as <span class="math inline">\(B'B\)</span>. The system becomes: <span class="math display">\[
B'B\beta = X'y
\]</span> This allows us to solve for <span class="math inline">\(\beta\)</span> using two efficient triangular substitutions (first solving <span class="math inline">\(B'z = X'y\)</span> for <span class="math inline">\(z\)</span>, then <span class="math inline">\(B\beta = z\)</span> for <span class="math inline">\(\beta\)</span>) without explicitly inverting the matrix, which is computationally expensive and unstable.</p></li>
<li><p>Computing the Determinant</p>
<p>The determinant of a triangular matrix is simply the product of its diagonal entries. Therefore, the determinant of <span class="math inline">\(A\)</span> can be computed instantly from <span class="math inline">\(B\)</span>: <span class="math display">\[
\det(A) = \det(B'B) = \det(B')\det(B) = \left(\prod_{i=1}^n b_{ii}\right)^2
\]</span> This is widely used in Maximum Likelihood Estimation (e.g., REML in mixed models) where log-determinants of large covariance matrices are required.</p></li>
<li><p>Generating Multivariate Normal Random Variables</p>
<p>To generate a random vector <span class="math inline">\(Y \sim N(\mu, \Sigma)\)</span>, we first generate a vector of independent standard normal variables <span class="math inline">\(Z \sim N(0, I)\)</span>. Using the Cholesky decomposition <span class="math inline">\(\Sigma = B'B\)</span>: <span class="math display">\[
Y = \mu + B'Z
\]</span> The covariance of <span class="math inline">\(Y\)</span> is confirmed by <span class="math inline">\(\text{Cov}(Y) = B' \text{Cov}(Z) B = B' I B = B'B = \Sigma\)</span>. This is the standard method used by functions like <code>mvrnorm</code> in R.</p></li>
</ol>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="multivariate-normal-distribution" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Multivariate Normal Distribution</h1>
<section id="motivation" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">4.1</span> Motivation</h2>
<p>Consider the linear model: <span class="math display">\[
y = X\beta + \epsilon, \quad \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<p>We are often interested in the distributional properties of the response vector <span class="math inline">\(y\)</span> and the residuals. Specifically, if <span class="math inline">\(y = (y_1, \dots, y_n)'\)</span>, we need to understand its multivariate distribution. <span class="math display">\[
\hat{y} = Py, \quad e = y - \hat{y} = (I_n - P)y
\]</span></p>
</section>
<section id="random-vectors-and-matrices" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="random-vectors-and-matrices"><span class="header-section-number">4.2</span> Random Vectors and Matrices</h2>
<div id="def-random-vector" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 31 (Random Vector and Matrix)</strong></span> A <strong>Random Vector</strong> is a vector whose elements are random variables. E.g., <span class="math display">\[
x_{k \times 1} = (x_1, x_2, \dots, x_k)^T
\]</span> where <span class="math inline">\(x_1, \dots, x_k\)</span> are each random variables.</p>
<p>A <strong>Random Matrix</strong> is a matrix whose elements are random variables. E.g., <span class="math inline">\(X_{n \times k} = (x_{ij})\)</span>, where <span class="math inline">\(x_{11}, \dots, x_{nk}\)</span> are each random variables.</p>
</div>
<div id="def-expected-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 32 (Expected Value)</strong></span> The expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.</p>
<p>For <span class="math inline">\(X_{n \times k}\)</span>: <span class="math display">\[
E(X) = \begin{pmatrix}
E(x_{11}) &amp; \dots &amp; E(x_{1k}) \\
\vdots &amp; \ddots &amp; \vdots \\
E(x_{n1}) &amp; \dots &amp; E(x_{nk})
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
E\left(\begin{pmatrix} x_1 \\ \vdots \\ x_k \end{pmatrix}\right) = \begin{pmatrix} E(x_1) \\ \vdots \\ E(x_k) \end{pmatrix}
\]</span></p>
</div>
<div id="def-variance-covariance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 33 (Variance-Covariance Matrix)</strong></span> For a random vector <span class="math inline">\(x_{k \times 1} = (x_1, \dots, x_k)^T\)</span>, the matrix is:</p>
<p><span class="math display">\[
\text{Var}(x) = \Sigma_x = \begin{pmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \dots &amp; \sigma_{1k} \\
\sigma_{21} &amp; \sigma_{22} &amp; \dots &amp; \sigma_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{k1} &amp; \sigma_{k2} &amp; \dots &amp; \sigma_{kk}
\end{pmatrix}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\sigma_{ij} = \text{Cov}(x_i, x_j) = E[(x_i - \mu_i)(x_j - \mu_j)]\)</span></li>
<li><span class="math inline">\(\sigma_{ii} = \text{Var}(x_i) = E[(x_i - \mu_i)^2]\)</span></li>
</ul>
<p>In matrix notation: <span class="math display">\[
\text{Var}(x) = E[(x - \mu_x)(x - \mu_x)^T]
\]</span> Note: <span class="math inline">\(\text{Var}(x)\)</span> is symmetric.</p>
</div>
<section id="derivation-of-covariance-matrix-structure" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="derivation-of-covariance-matrix-structure"><span class="header-section-number">4.2.1</span> Derivation of Covariance Matrix Structure</h3>
<p>Expanding the vector multiplication for variance: <span class="math display">\[
(x - \mu_x)(x - \mu_x)' \quad \text{where } \mu_x = (\mu_1, \dots, \mu_n)'
\]</span> <span class="math display">\[
= \begin{pmatrix} x_1 - \mu_1 \\ \vdots \\ x_n - \mu_n \end{pmatrix} (x_1 - \mu_1, \dots, x_n - \mu_n)
\]</span> This results in the matrix <span class="math inline">\(A = (a_{ij})\)</span> where <span class="math inline">\(a_{ij} = (x_i - \mu_i)(x_j - \mu_j)\)</span>. Taking expectations yields the covariance matrix elements <span class="math inline">\(\sigma_{ij}\)</span>.</p>
<div id="def-covariance-matrix-two" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 34 (Covariance Matrix (Two Vectors))</strong></span> For random vectors <span class="math inline">\(x_{k \times 1}\)</span> and <span class="math inline">\(y_{n \times 1}\)</span>, the covariance matrix is: <span class="math display">\[
\text{Cov}(x, y) = E[(x - \mu_x)(y - \mu_y)^T] = \begin{pmatrix}
\text{Cov}(x_1, y_1) &amp; \dots &amp; \text{Cov}(x_1, y_n) \\
\vdots &amp; \ddots &amp; \vdots \\
\text{Cov}(x_k, y_1) &amp; \dots &amp; \text{Cov}(x_k, y_n)
\end{pmatrix}
\]</span> Note that <span class="math inline">\(\text{Cov}(x, x) = \text{Var}(x)\)</span>.</p>
</div>
<div id="def-correlation-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 35 (Correlation Matrix)</strong></span> The correlation matrix of a random vector <span class="math inline">\(x\)</span> is: <span class="math display">\[
\text{corr}(x) = \begin{pmatrix}
1 &amp; \rho_{12} &amp; \dots &amp; \rho_{1k} \\
\vdots &amp; \ddots &amp; \vdots \\
\rho_{k1} &amp; \rho_{k2} &amp; \dots &amp; 1
\end{pmatrix}
\]</span> where <span class="math inline">\(\rho_{ij} = \text{corr}(x_i, x_j)\)</span>.</p>
<p><strong>Relationships:</strong> Let <span class="math inline">\(V_x = \text{diag}(\text{Var}(x_1), \dots, \text{Var}(x_k))\)</span>. <span class="math display">\[
\Sigma_x = V_x^{1/2} \rho_x V_x^{1/2} \quad \text{and} \quad \rho_x = (V_x^{1/2})^{-1} \Sigma_x (V_x^{1/2})^{-1}
\]</span> Similarly for two vectors: <span class="math display">\[
\Sigma_{xy} = V_x^{1/2} \rho_{xy} V_y^{1/2}
\]</span></p>
</div>
</section>
</section>
<section id="properties-of-mean-and-variance" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="properties-of-mean-and-variance"><span class="header-section-number">4.3</span> Properties of Mean and Variance</h2>
<p>We can derive several key algebraic properties for operations on random vectors.</p>
<ol type="1">
<li><span class="math inline">\(E(X + Y) = E(X) + E(Y)\)</span></li>
<li><span class="math inline">\(E(AXB) = A E(X) B\)</span> (In particular, <span class="math inline">\(E(AX) = A\mu_x\)</span>)</li>
<li><span class="math inline">\(\text{Cov}(x, y) = \text{Cov}(y, x)^T\)</span></li>
<li><span class="math inline">\(\text{Cov}(x + c, y + d) = \text{Cov}(x, y)\)</span></li>
<li><span class="math inline">\(\text{Cov}(Ax, By) = A \text{Cov}(x, y) B^T\)</span>
<ul>
<li>Special case for scalars: <span class="math inline">\(\text{Cov}(ax, by) = ab \cdot \text{Cov}(x, y)\)</span></li>
</ul></li>
<li><span class="math inline">\(\text{Cov}(x_1 + x_2, y_1) = \text{Cov}(x_1, y_1) + \text{Cov}(x_2, y_1)\)</span></li>
<li><span class="math inline">\(\text{Var}(x + c) = \text{Var}(x)\)</span></li>
<li><span class="math inline">\(\text{Var}(Ax) = A \text{Var}(x) A^T\)</span></li>
<li><span class="math inline">\(\text{Var}(x_1 + x_2) = \text{Var}(x_1) + \text{Cov}(x_1, x_2) + \text{Cov}(x_2, x_1) + \text{Var}(x_2)\)</span></li>
<li><span class="math inline">\(\text{Var}(\sum x_i) = \sum \text{Var}(x_i)\)</span> if independent.</li>
</ol>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Property 5 (Covariance of Linear Transformation):</strong> <span class="math display">\[
\begin{aligned}
\text{Cov}(Ax, By) &amp;= E[(Ax - A\mu_x)(By - B\mu_y)^T] \\
&amp;= A E[(x - \mu_x)(y - \mu_y)^T] B^T \\
&amp;= A \text{Cov}(x, y) B^T
\end{aligned}
\]</span> <strong>Property 2 (Expectation of Linear Transformation)</strong>:</p>
<p>To prove <span class="math inline">\(E(AXB) = A E(X) B\)</span>: First consider <span class="math inline">\(E(Ax_j)\)</span> where <span class="math inline">\(x_j\)</span> is a column of <span class="math inline">\(X\)</span>. <span class="math display">\[
E(Ax_j) = E\begin{pmatrix} a_1' x_j \\ \vdots \\ a_n' x_j \end{pmatrix} = \begin{pmatrix} E(a_1' x_j) \\ \vdots \\ E(a_n' x_j) \end{pmatrix}
\]</span> Since <span class="math inline">\(a_i\)</span> are constants: <span class="math display">\[
E(a_i' x_j) = E\left(\sum_{k=1}^p a_{ik} x_{kj}\right) = \sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)
\]</span> Thus <span class="math inline">\(E(Ax_j) = A E(x_j)\)</span>. Applying this to all columns of <span class="math inline">\(X\)</span>: <span class="math display">\[
E(AX) = [E(Ax_1), \dots, E(Ax_m)] = [AE(x_1), \dots, AE(x_m)] = A E(X)
\]</span> Similarly, <span class="math inline">\(E(XB) = E(X)B\)</span>.</p>
<p><strong>Proof of Property 9 (Variance of Sum):</strong></p>
<p><span class="math display">\[
\text{Var}(x_1 + x_2) = E[(x_1 + x_2 - \mu_1 - \mu_2)(x_1 + x_2 - \mu_1 - \mu_2)^T]
\]</span> Let centered variables be denoted by differences. <span class="math display">\[
= E[((x_1 - \mu_1) + (x_2 - \mu_2))((x_1 - \mu_1) + (x_2 - \mu_2))^T]
\]</span> Expanding terms: <span class="math display">\[
= E[(x_1 - \mu_1)(x_1 - \mu_1)^T + (x_1 - \mu_1)(x_2 - \mu_2)^T + (x_2 - \mu_2)(x_1 - \mu_1)^T + (x_2 - \mu_2)(x_2 - \mu_2)^T]
\]</span> <span class="math display">\[
= \text{Var}(x_1) + \text{Cov}(x_1, x_2) + \text{Cov}(x_2, x_1) + \text{Var}(x_2)
\]</span></p>
</div>
</section>
<section id="the-multivariate-normal-distribution" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="the-multivariate-normal-distribution"><span class="header-section-number">4.4</span> The Multivariate Normal Distribution</h2>
<section id="definition-and-density" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="definition-and-density"><span class="header-section-number">4.4.1</span> Definition and Density</h3>
<div id="def-standard-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 36 (Independent Standard Normal)</strong></span> Let <span class="math inline">\(z = (z_1, \dots, z_n)'\)</span> where <span class="math inline">\(z_i \sim N(0, 1)\)</span> are independent. We say <span class="math inline">\(z \sim N_n(0, I_n)\)</span>. The joint PDF is the product of marginals: <span class="math display">\[
f(z) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-\frac{z_i^2}{2}} = \frac{1}{(2\pi)^{n/2}} e^{-\frac{1}{2} z^T z}
\]</span> Properties: <span class="math inline">\(E(z) = 0\)</span> and <span class="math inline">\(\text{Var}(z) = I_n\)</span> (Covariance is 0 for <span class="math inline">\(i \ne j\)</span>, Variance is 1).</p>
</div>
<div id="def-mvn" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 37 (Multivariate Normal Distribution)</strong></span> A random vector <span class="math inline">\(x\)</span> (<span class="math inline">\(n \times 1\)</span>) has a <strong>multivariate normal distribution</strong> if it has the same distribution as: <span class="math display">\[
x = A_{n \times p} z_{p \times 1} + \mu_{n \times 1}
\]</span> where <span class="math inline">\(z \sim N_p(0, I_p)\)</span>, <span class="math inline">\(A\)</span> is a matrix of constants, and <span class="math inline">\(\mu\)</span> is a vector of constants. The moments are:</p>
<ul>
<li><span class="math inline">\(E(x) = \mu\)</span></li>
<li><span class="math inline">\(\text{Var}(x) = AA^T = \Sigma\)</span></li>
</ul>
</div>
</section>
<section id="geometric-interpretation" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="geometric-interpretation"><span class="header-section-number">4.4.2</span> Geometric Interpretation</h3>
<p>Using Spectral Decomposition, <span class="math inline">\(\Sigma = Q \Lambda Q'\)</span>. We can view the transformation <span class="math inline">\(x = Az + \mu\)</span> as:</p>
<ol type="1">
<li>Scaling by eigenvalues (<span class="math inline">\(\Lambda^{1/2}\)</span>).</li>
<li>Rotation by eigenvectors (<span class="math inline">\(Q\)</span>).</li>
<li>Shift by mean (<span class="math inline">\(\mu\)</span>).</li>
</ol>
<p><strong>An Shinely App for Visualizing Bivariate Normal</strong></p>
<p>Use the controls to construct the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> geometrically.</p>
<p>We define the transformation matrix <span class="math inline">\(\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}^{1/2}\)</span>, where <span class="math inline">\(\mathbf{Q}\)</span> is a rotation matrix and <span class="math inline">\(\mathbf{\Lambda}^{1/2}\)</span> is a diagonal scaling matrix. The resulting covariance is <span class="math inline">\(\boldsymbol{\Sigma} = \mathbf{A}\mathbf{A}'\)</span>.</p>
<pre class="{shinylive-r}"><code>#| standalone: true
#| viewerHeight: 700
#| echo: false


library(shiny)
library(bslib)
library(shinyWidgets)
library(munsell) 
library(scales)
library(tibble)
library(rlang)
library(ggplot2)
library(mvtnorm)

# --- 1. PRE-GENERATE FIXED Z POINTS ---
set.seed(123)
z_fixed &lt;- matrix(rnorm(50 * 2), ncol = 2)

ui &lt;- page_fillable(
  theme = bs_theme(version = 5),
  withMathJax(), 
  
  # --- ROW 1: CONTROLS (Compact Strip) ---
  card(
    class = "p-2", 
    layout_columns(
      col_widths = c(3, 2, 2, 2, 2),
      
      div(class = "text-center", tags$label(HTML("$$\\theta$$")), 
          noUiSliderInput("theta", label = NULL, min = 0, max = 360, value = 0, step = 5, 
                          orientation = "horizontal", width = "100%", height = "10px", color = "#0d6efd")),
      
      div(class = "text-center", tags$label(HTML("$$\\sqrt{\\lambda_1}$$")), 
          noUiSliderInput("L1", label = NULL, min = 0.5, max = 3, value = 2, step = 0.1, 
                          orientation = "horizontal", width = "100%", height = "10px", color = "#ffc107")),
      
      div(class = "text-center", tags$label(HTML("$$\\sqrt{\\lambda_2}$$")), 
          noUiSliderInput("L2", label = NULL, min = 0.5, max = 3, value = 1, step = 0.1, 
                          orientation = "horizontal", width = "100%", height = "10px", color = "#adb5bd")),
      
      div(class = "text-center", tags$label(HTML("$$\\mu_1$$")), 
          noUiSliderInput("mu1", label = NULL, min = -3, max = 3, value = 0, step = 0.5, 
                          orientation = "horizontal", width = "100%", height = "10px", color = "#6c757d")),
      
      div(class = "text-center", tags$label(HTML("$$\\mu_2$$")), 
          noUiSliderInput("mu2", label = NULL, min = -3, max = 3, value = 0, step = 0.5, 
                          orientation = "horizontal", width = "100%", height = "10px", color = "#6c757d"))
    )
  ),

  # --- ROW 2: SIDE-BY-SIDE (Plot &amp; Math) ---
  layout_columns(
    col_widths = c(8, 4), # 2/3 for Plot, 1/3 for Matrix
    
    # Left: Visualization
    card(
      full_screen = TRUE,
      plotOutput("contourPlot", height = "500px")
    ),
    
    # Right: The Math (Larger Font)
    card(
      class = "p-3 d-flex justify-content-center", # Center content vertically
      h5("Algebraic Representation", class = "mb-3 text-center"),
      
      # Use CSS to make the font larger and monospaced
      div(
        style = "font-family: 'Courier New', monospace; font-size: 1.1rem; line-height: 1.4;",
        verbatimTextOutput("matrixSide", placeholder = TRUE)
      )
    )
  )
)

server &lt;- function(input, output) {

  data &lt;- reactive({
    theta_rad &lt;- input$theta * pi / 180
    Q &lt;- matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), 2, 2)
    Lam_sqrt &lt;- diag(c(input$L1, input$L2))
    
    A &lt;- Q %*% Lam_sqrt
    Sigma &lt;- A %*% t(A)
    mu_vec &lt;- c(input$mu1, input$mu2)
    
    x_points &lt;- z_fixed %*% t(A)
    x_points[,1] &lt;- x_points[,1] + mu_vec[1]
    x_points[,2] &lt;- x_points[,2] + mu_vec[2]
    
    list(Q=Q, L=c(input$L1, input$L2), mu=mu_vec, Sigma=Sigma, A=A, points=as.data.frame(x_points))
  })

  output$matrixSide &lt;- renderText({
    M &lt;- data()
    A &lt;- round(M$A, 2)
    S &lt;- round(M$Sigma, 2)
    rho &lt;- cov2cor(M$Sigma)[1,2]
    
    # Formatted to fill vertical space comfortably
    paste0(
      "Linear Transform:\n",
      "x = A z + μ\n\n",
      
      "Matrix A:\n",
      sprintf("[%4.1f   %4.1f]\n", A[1,1], A[1,2]),
      sprintf("[%4.1f   %4.1f]\n", A[2,1], A[2,2]),
      "\n",
      
      "Covariance Σ:\n",
      "(Σ = AA')\n",
      sprintf("[%4.1f   %4.1f]\n", S[1,1], S[1,2]),
      sprintf("[%4.1f   %4.1f]\n", S[2,1], S[2,2]),
      "\n",
      
      "Correlation:\n",
      sprintf("ρ = %.3f", rho)
    )
  })

  output$contourPlot &lt;- renderPlot({
    req(data())
    M &lt;- data()
    
    grid_r &lt;- seq(-6, 6, length.out = 60)
    df_grid &lt;- expand.grid(x = grid_r, y = grid_r)
    df_grid$z &lt;- dmvnorm(as.matrix(df_grid), mean = M$mu, sigma = M$Sigma)
    
    v1 &lt;- M$Q[,1] * M$L[1]; v2 &lt;- M$Q[,2] * M$L[2]
    axes &lt;- tibble(x = M$mu[1], y = M$mu[2],
                   xend1 = M$mu[1] + v1[1], yend1 = M$mu[2] + v1[2],
                   xend2 = M$mu[1] + v2[1], yend2 = M$mu[2] + v2[2])
    
    ggplot() +
      geom_contour_filled(data = df_grid, aes(x, y, z = z), bins = 9, show.legend = FALSE) +
      geom_point(data = M$points, aes(V1, V2), color = "black", size = 2, alpha = 0.7) +
      geom_segment(data = axes, aes(x=x, y=y, xend=xend1, yend=yend1), 
                   color = "#ffc107", linewidth = 1.5, arrow = arrow(length = unit(0.3,"cm"))) +
      geom_segment(data = axes, aes(x=x, y=y, xend=xend2, yend=yend2), 
                   color = "white", linewidth = 1.5, arrow = arrow(length = unit(0.3,"cm"))) +
      coord_fixed(xlim = c(-6, 6), ylim = c(-6, 6)) +
      theme_minimal() +
      labs(x = "X", y = "Y")
  })
}

shinyApp(ui, server)</code></pre>
</section>
<section id="probability-density-function" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="probability-density-function"><span class="header-section-number">4.4.3</span> Probability Density Function</h3>
<p>If <span class="math inline">\(\Sigma\)</span> is positive definite, the PDF exists. We use the change of variable formula for <span class="math inline">\(x = Az + \mu\)</span>: <span class="math display">\[
f_x(x) = f_z(g^{-1}(x)) \cdot |J|
\]</span> where <span class="math inline">\(z = A^{-1}(x - \mu)\)</span> and <span class="math inline">\(J = \det(A^{-1}) = |A|^{-1}\)</span>.</p>
<p><span class="math display">\[
f_x(x) = (2\pi)^{-p/2} |A|^{-1} \exp \left\{ -\frac{1}{2} (A^{-1}(x-\mu))^T (A^{-1}(x-\mu)) \right\}
\]</span></p>
<p>Using <span class="math inline">\(|\Sigma| = |AA^T| = |A|^2\)</span> and <span class="math inline">\(\Sigma^{-1} = (AA^T)^{-1}\)</span>, we get: <span class="math display">\[
f_x(x) = (2\pi)^{-p/2} |\Sigma|^{-1/2} \exp \left\{ -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right\}
\]</span></p>
</section>
<section id="moment-generating-function" class="level3" data-number="4.4.4">
<h3 data-number="4.4.4" class="anchored" data-anchor-id="moment-generating-function"><span class="header-section-number">4.4.4</span> Moment Generating Function</h3>
<div id="def-mgf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 38 (Moment Generating Function (MGF))</strong></span> The MGF of a random vector <span class="math inline">\(x\)</span> is <span class="math inline">\(M_x(t) = E(e^{t^T x})\)</span>. For <span class="math inline">\(x = Az + \mu\)</span>: <span class="math display">\[
M_x(t) = E[e^{t^T(Az + \mu)}] = e^{t^T\mu} E[e^{(A^T t)^T z}] = e^{t^T\mu} M_z(A^T t)
\]</span> Since <span class="math inline">\(M_z(u) = e^{u^T u / 2}\)</span>: <span class="math display">\[
M_x(t) = e^{t^T\mu} \exp\left( \frac{1}{2} t^T (AA^T) t \right) = \exp \left( t^T\mu + \frac{1}{2} t^T \Sigma t \right)
\]</span></p>
</div>
<p>Key Properties:</p>
<ol type="1">
<li><p><strong>Uniqueness:</strong> Two random vectors with the same MGF have the same distribution.</p></li>
<li><p><strong>Independence:</strong> <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent iff <span class="math inline">\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\)</span>.</p></li>
</ol>
</section>
</section>
<section id="construction-and-linear-transformations" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="construction-and-linear-transformations"><span class="header-section-number">4.5</span> Construction and Linear Transformations</h2>
<div id="thm-construction" class="theorem">
<p><span class="theorem-title"><strong>Theorem 32 (Constructing MVN Random Vector)</strong></span> Let <span class="math inline">\(\mu \in \mathbb{R}^n\)</span> and <span class="math inline">\(\Sigma\)</span> be an <span class="math inline">\(n \times n\)</span> symmetric non-negative definitive (n.n.d) matrix. Then there exists a multivariate normal distribution with mean <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(\Sigma\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(\Sigma\)</span> is n.n.d., there exists <span class="math inline">\(B\)</span> such that <span class="math inline">\(\Sigma = BB^T\)</span> (e.g., via Cholesky or Spetral Decomposition). Let <span class="math inline">\(z \sim N_n(0, I)\)</span> and define <span class="math inline">\(x = Bz + \mu\)</span>.</p>
</div>
<div id="thm-linear-transform" class="theorem">
<p><span class="theorem-title"><strong>Theorem 33 (Linear Transformation Theorem)</strong></span> Let <span class="math inline">\(x \sim N_n(\mu, \Sigma)\)</span>. Let <span class="math inline">\(y = Cx + d\)</span> where <span class="math inline">\(C\)</span> is <span class="math inline">\(r \times n\)</span> and <span class="math inline">\(d\)</span> is <span class="math inline">\(r \times 1\)</span>. Then: <span class="math display">\[
y \sim N_r(C\mu + d, C \Sigma C^T)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(x = Az + \mu\)</span> where <span class="math inline">\(AA^T = \Sigma\)</span>. <span class="math display">\[
y = C(Az + \mu) + d = (CA)z + (C\mu + d)
\]</span> This fits the definition of MVN with mean <span class="math inline">\(C\mu + d\)</span> and variance <span class="math inline">\(C \Sigma C^T\)</span>.</p>
</div>
<section id="important-corollaries-of-thm-linear-transform" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="important-corollaries-of-thm-linear-transform"><span class="header-section-number">4.5.1</span> Important Corollaries of <a href="#thm-linear-transform" class="quarto-xref">Theorem&nbsp;33</a></h3>
<div id="cor-marginals" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 (Marginals)</strong></span> Any subvector of a multivariate normal vector is also multivariate normal.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If we partition <span class="math inline">\(x = (x_1', x_2')'\)</span>, we can use <span class="math inline">\(C = (I_r, 0)\)</span> to show <span class="math inline">\(x_1 \sim N(\mu_1, \Sigma_{11})\)</span>.</p>
</div>
<div id="cor-univariate" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2 (Univariate Combinations)</strong></span> Any linear combination <span class="math inline">\(a^T x\)</span> is univariate normal: <span class="math display">\[
a^T x \sim N(a^T \mu, a^T \Sigma a)
\]</span></p>
</div>
<div id="cor-orthogonal" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 3 (Orthogonal Transformations)</strong></span> If <span class="math inline">\(x \sim N(0, I_n)\)</span> and <span class="math inline">\(Q\)</span> is orthogonal (<span class="math inline">\(Q'Q = I\)</span>), then <span class="math inline">\(y = Q'x \sim N(0, I_n)\)</span>.</p>
</div>
<div id="cor-standardization" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4 (Standardization)</strong></span> If <span class="math inline">\(y \sim N_n(\mu, \Sigma)\)</span> and <span class="math inline">\(\Sigma\)</span> is positive definite: <span class="math display">\[
\Sigma^{-1/2}(y - \mu) \sim N_n(0, I_n)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(z = \Sigma^{-1/2}(y - \mu)\)</span>. Then <span class="math inline">\(\text{Var}(z) = \Sigma^{-1/2} \Sigma \Sigma^{-1/2} = I_n\)</span>.</p>
</div>
</section>
</section>
<section id="independence" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="independence"><span class="header-section-number">4.6</span> Independence</h2>
<div id="thm-independence" class="theorem">
<p><span class="theorem-title"><strong>Theorem 34 (Independence in MVN)</strong></span> Let <span class="math inline">\(y \sim N(\mu, \Sigma)\)</span> be partitioned into <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. <span class="math display">\[
\Sigma = \begin{pmatrix} \Sigma_{11} &amp; \Sigma_{12} \\ \Sigma_{21} &amp; \Sigma_{22} \end{pmatrix}
\]</span> Then <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are independent if and only if <span class="math inline">\(\Sigma_{12} = 0\)</span> (zero covariance).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li><p>Independence <span class="math inline">\(\implies\)</span> Covariance is 0: This holds generally for any distribution. <span class="math display">\[
\text{Cov}(y_1, y_2) = E[(y_1 - \mu_1)(y_2 - \mu_2)'] = 0
\]</span></p></li>
<li><p>Covariance is 0 <span class="math inline">\(\implies\)</span> Independence: This is specific to MVN. We use MGFs. If <span class="math inline">\(\Sigma_{12} = 0\)</span>, the quadratic form in the MGF splits: <span class="math display">\[
t^T \Sigma t = t_1^T \Sigma_{11} t_1 + t_2^T \Sigma_{22} t_2
\]</span> The MGF becomes: <span class="math display">\[
M_y(t) = \exp(t_1^T \mu_1 + \frac{1}{2} t_1^T \Sigma_{11} t_1) \times \exp(t_2^T \mu_2 + \frac{1}{2} t_2^T \Sigma_{22} t_2)
\]</span> <span class="math display">\[
M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)
\]</span> Thus, they are independent.</p></li>
</ol>
</div>
</section>
<section id="signal-noise-decomposition-for-multivariate-normal-distribution" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="signal-noise-decomposition-for-multivariate-normal-distribution"><span class="header-section-number">4.7</span> Signal-Noise Decomposition for Multivariate Normal Distribution</h2>
<p>We can formalize the relationship between two random vectors <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> through a decomposition theorem that separates the systematic signal from the stochastic noise.</p>
<div id="thm-reg-decomp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 35 (Regression Decomposition Theorem)</strong></span> Let the random vector <span class="math inline">\(V\)</span> of dimension <span class="math inline">\(p \times 1\)</span> be partitioned into two subvectors <span class="math inline">\(y\)</span> (<span class="math inline">\(p_1 \times 1\)</span>) and <span class="math inline">\(x\)</span> (<span class="math inline">\(p_2 \times 1\)</span>). Assume <span class="math inline">\(V\)</span> follows a multivariate normal distribution:</p>
<p><span class="math display">\[
\begin{pmatrix} y \\ x \end{pmatrix} \sim N_p\left( \begin{pmatrix} \mu_y \\ \mu_x \end{pmatrix}, \begin{pmatrix} \Sigma_{yy} &amp; \Sigma_{yx} \\ \Sigma_{xy} &amp; \Sigma_{xx} \end{pmatrix} \right)
\]</span></p>
<p>The response vector <span class="math inline">\(y\)</span> can be uniquely decomposed into a systematic component and a stochastic error: <span class="math display">\[
y = m(x) + e
\]</span> where we define the <strong>Regression Coefficient Matrix</strong> <span class="math inline">\(B\)</span> and the components as:</p>
<p><span class="math display">\[
B = \Sigma_{yx}\Sigma_{xx}^{-1}
\]</span></p>
<p><span class="math display">\[
m(x) = \mu_y + B(x - \mu_x)
\]</span></p>
<p><span class="math display">\[
e = y - m(x)
\]</span></p>
<p><strong>Properties:</strong></p>
<ol type="1">
<li><p><strong>Independence:</strong> The noise vector <span class="math inline">\(e\)</span> is statistically independent of the predictor <span class="math inline">\(x\)</span> (and consequently independent of <span class="math inline">\(m(x)\)</span>).</p></li>
<li><p><strong>Marginal Distributions:</strong></p>
<ul>
<li><span class="math inline">\(m(x) \sim N_{p_1}(\mu_y, \; B \Sigma_{xx} B^T)\)</span></li>
<li><span class="math inline">\(e \sim N_{p_1}(0, \; \Sigma_{yy} - B \Sigma_{xx} B^T)\)</span></li>
</ul></li>
<li><p><strong>Conditional Distribution:</strong> Since <span class="math inline">\(y = m(x) + e\)</span>, and <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(x\)</span>, the conditional distribution is: <span class="math display">\[
y | x \sim N_{p_1}(m(x), \Sigma_{y|x})
\]</span> where: <span class="math display">\[
m(x) = \mu_y + B(x - \mu_x) = \mu_y + \Sigma_{yx}\Sigma_{xx}^{-1}(x - \mu_x)
\]</span> <span class="math display">\[
\Sigma_{y|x} = \Sigma_{yy} - B \Sigma_{xx} B^T = \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}
\]</span></p></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We define a transformation from the input vector <span class="math inline">\(V = \begin{pmatrix} y \\ x \end{pmatrix}\)</span> to the target vector <span class="math inline">\(W = \begin{pmatrix} m(x) \\ e \end{pmatrix}\)</span>.</p>
<p>Using the linear transformation <span class="math inline">\(W = CV + d\)</span>:</p>
<p><span class="math display">\[
\underbrace{\begin{pmatrix} m(x) \\ e \end{pmatrix}}_{W} = \underbrace{\begin{pmatrix} 0 &amp; B \\ I &amp; -B \end{pmatrix}}_{C} \underbrace{\begin{pmatrix} y \\ x \end{pmatrix}}_{V} + \underbrace{\begin{pmatrix} \mu_y - B \mu_x \\ -(\mu_y - B \mu_x) \end{pmatrix}}_{d}
\]</span></p>
<ol type="1">
<li>Mean Vector</li>
</ol>
<p><span class="math display">\[
E[W] = C E[V] + d = \begin{pmatrix} 0 &amp; B \\ I &amp; -B \end{pmatrix} \begin{pmatrix} \mu_y \\ \mu_x \end{pmatrix} + \begin{pmatrix} \mu_y - B \mu_x \\ -\mu_y + B \mu_x \end{pmatrix}
= \begin{pmatrix} B \mu_x \\ \mu_y - B \mu_x \end{pmatrix} + \begin{pmatrix} \mu_y - B \mu_x \\ -\mu_y + B \mu_x \end{pmatrix}
= \begin{pmatrix} \mu_y \\ 0 \end{pmatrix}
\]</span></p>
<ol start="2" type="1">
<li>Covariance Matrix</li>
</ol>
<p>We compute <span class="math inline">\(\text{Var}(W) = C \Sigma C^T\)</span> directly:</p>
<p><span class="math display">\[
\begin{aligned}
C \Sigma C^T &amp;= \begin{pmatrix} 0 &amp; B \\ I &amp; -B \end{pmatrix} \begin{pmatrix} \Sigma_{yy} &amp; \Sigma_{yx} \\ \Sigma_{xy} &amp; \Sigma_{xx} \end{pmatrix} \begin{pmatrix} 0 &amp; I \\ B^T &amp; -B^T \end{pmatrix} \\
&amp;= \begin{pmatrix} B \Sigma_{xy} &amp; B \Sigma_{xx} \\ \Sigma_{yy} - B \Sigma_{xy} &amp; \Sigma_{yx} - B \Sigma_{xx} \end{pmatrix} \begin{pmatrix} 0 &amp; I \\ B^T &amp; -B^T \end{pmatrix} \\
&amp;= \begin{pmatrix} B \Sigma_{xx} B^T &amp; B \Sigma_{xy} - B \Sigma_{xx} B^T \\ \Sigma_{yx}B^T - B \Sigma_{xx} B^T &amp; (\Sigma_{yy} - B \Sigma_{xy}) - (\Sigma_{yx} - B \Sigma_{xx})B^T \end{pmatrix} \\
&amp;= \begin{pmatrix} B \Sigma_{xx} B^T &amp; 0 \\ 0 &amp; \Sigma_{yy} - B \Sigma_{xx} B^T \end{pmatrix}
\end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>Conditional Distribution</li>
</ol>
<p>We have established that <span class="math inline">\(y = m(x) + e\)</span> where <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(x\)</span>. To find the distribution of <span class="math inline">\(y\)</span> conditional on <span class="math inline">\(x\)</span>, we observe that <span class="math inline">\(m(x)\)</span> becomes a constant vector when <span class="math inline">\(x\)</span> is fixed, and the randomness comes solely from <span class="math inline">\(e\)</span>:</p>
<p><span class="math display">\[
E[y|x] = m(x) + E[e|x] = m(x) + 0 = m(x)
\]</span> <span class="math display">\[
\text{Var}(y|x) = \text{Var}(m(x)|x) + \text{Var}(e|x) = 0 + \text{Var}(e) = \Sigma_{y|x}
\]</span></p>
<p>Thus, <span class="math inline">\(y | x \sim N(m(x), \Sigma_{y|x})\)</span>.</p>
</div>
<section id="connections-with-other-formulas" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="connections-with-other-formulas"><span class="header-section-number">4.7.1</span> Connections with Other Formulas</h3>
<section id="rao-blackwell-decomposition-of-variance" class="level4" data-number="4.7.1.1">
<h4 data-number="4.7.1.1" class="anchored" data-anchor-id="rao-blackwell-decomposition-of-variance"><span class="header-section-number">4.7.1.1</span> Rao-Blackwell Decomposition of Variance</h4>
<p>The Law of Total Variance (Rao-Blackwell theorem) allows us to decompose the total variance of <span class="math inline">\(y\)</span> into two orthogonal components based on the predictor <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\text{Var}(y) = \underbrace{E[\text{Var}(y | x)]}_{\text{Unexplained (Noise)}} + \underbrace{\text{Var}[E(y | x)]}_{\text{Explained (Signal)}}
\]</span></p>
<p>In the Multivariate Normal case, this decomposition perfectly aligns with our regression model <span class="math inline">\(y = m(x) + e\)</span>.</p>
</section>
<section id="variance-of-noise" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="variance-of-noise">Variance of Noise</h4>
<p>This term represents the average variance remaining in <span class="math inline">\(y\)</span> after accounting for <span class="math inline">\(x\)</span>. It corresponds to the variance of the error term <span class="math inline">\(e\)</span>:</p>
<p><span class="math display">\[
E[\text{Var}(y | x)] = \text{Var}(e) = \Sigma_{yy} - B \Sigma_{xx} B^T
\]</span></p>
</section>
<section id="variance-of-signal" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="variance-of-signal">Variance of Signal</h4>
<p>This term represents the variability of the conditional mean <span class="math inline">\(m(x)\)</span> itself. Using the matrix <span class="math inline">\(B\)</span>, this takes the quadratic form:</p>
<p><span class="math display">\[
\text{Var}[E(y | x)] = \text{Var}[m(x)] = B \Sigma_{xx} B^T
\]</span></p>
</section>
<section id="total-variance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="total-variance">Total Variance</h4>
<p>Summing the Signal and Noise components recovers the total marginal variance of <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\Sigma_{yy} = \underbrace{\Sigma_{yy} - B \Sigma_{xx} B^T}_{\text{Unexplained (Noise)}} + \underbrace{B \Sigma_{xx} B^T}_{\text{Explained (Signal)}}
\]</span></p>
</section>
<section id="connection-to-ols-regression-estimators" class="level4" data-number="4.7.1.2">
<h4 data-number="4.7.1.2" class="anchored" data-anchor-id="connection-to-ols-regression-estimators"><span class="header-section-number">4.7.1.2</span> Connection to OLS Regression Estimators</h4>
<p>In OLS regression, centering the data allows us to separate the intercept from the slopes. Let <span class="math inline">\(\mathbf{y}_c\)</span> and <span class="math inline">\(\mathbf{X}_c\)</span> be the centered response and design matrices (where <span class="math inline">\(\mathbf{X}_c\)</span> <strong>excludes the column of 1s</strong>). Using this centered form, the total sum of squares decomposes exactly like the population variance:</p>
<p><span class="math display">\[
\text{SST} = \text{SSR} + \text{SSE}
\]</span></p>
<p>Comparing the sample quantities to their population counterparts:</p>
<ol type="1">
<li><p><strong>Regression Coefficients:</strong> <span class="math display">\[
  \hat{\beta}^T = (\mathbf{X}_c^T \mathbf{X}_c)^{-1} \mathbf{X}_c^T \mathbf{y}_c \approx B
  \]</span> <em>Note: <span class="math inline">\(\hat{\beta}\)</span> here represents only the slope coefficients, matching the dimensions of the covariance matrix <span class="math inline">\(\Sigma_{xx}\)</span>.</em></p></li>
<li><p><strong>Explained Variation (Signal):</strong> <span class="math display">\[
\text{SSR} = \hat{\beta}^T (\mathbf{X}_c^T \mathbf{X}_c) \hat{\beta} \quad \approx \quad (n-1) B \Sigma_{xx} B^T
\]</span></p></li>
<li><p><strong>Unexplained Variation (Noise):</strong> <span class="math display">\[
\text{SSE} = \mathbf{y}_c^T \mathbf{y}_c - \hat{\beta}^T (\mathbf{X}_c^T \mathbf{X}_c) \hat{\beta} \quad \approx \quad (n-1)(\Sigma_{yy} - B \Sigma_{xx} B^T)
\]</span></p></li>
</ol>
</section>
</section>
</section>
<section id="partial-and-multiple-correlation" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="partial-and-multiple-correlation"><span class="header-section-number">4.8</span> Partial and Multiple Correlation</h2>
<div id="def-partial-corr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 39 (Partial Correlation)</strong></span> The partial correlation between elements <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> given a set of variables <span class="math inline">\(x\)</span> is derived from the conditional covariance matrix <span class="math inline">\(\Sigma_{y|x}\)</span>: <span class="math display">\[
\rho_{ij|x} = \frac{\sigma_{ij|x}}{\sqrt{\sigma_{ii|x} \sigma_{jj|x}}}
\]</span> where <span class="math inline">\(\sigma_{ij|x}\)</span> are elements of <span class="math inline">\(\Sigma_{y|x} = \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}\)</span>.</p>
</div>
<div id="def-multiple-corr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 40 (Multiple Correlation (<span class="math inline">\(R^2\)</span>))</strong></span> For a scalar <span class="math inline">\(y\)</span> and vector <span class="math inline">\(x\)</span>, the squared multiple correlation is the proportion of variance of <span class="math inline">\(y\)</span> explained by the conditional mean: <span class="math display">\[
R^2_{y|x} = \frac{\text{Var}(E(y|x))}{\text{Var}(y)} = \frac{\Sigma_{yx} \Sigma_{xx}^{-1} \Sigma_{xy}}{\sigma^2_{y}}
\]</span></p>
</div>
<p>Note: this definition is the population or theretical <span class="math inline">\(R^2\)</span>, which is estimated by adjusted <span class="math inline">\(R^2\)</span> using sample in linear regression.</p>
</section>
<section id="examples-1" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="examples-1"><span class="header-section-number">4.9</span> Examples</h2>
<div id="exm-numerical" class="theorem example">
<p><span class="theorem-title"><strong>Example 10 (Bivariate Normal)</strong></span> Let the random vector <span class="math inline">\(\begin{pmatrix} y \\ x \end{pmatrix}\)</span> follow a bivariate normal distribution: <span class="math display">\[
\begin{pmatrix} y \\ x \end{pmatrix} \sim N \left( \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 &amp; 2 \\ 2 &amp; 4 \end{pmatrix} \right)
\]</span> Here, <span class="math inline">\(\mu_y = 1, \mu_x = 2, \Sigma_{yy} = 2, \Sigma_{xx} = 4\)</span>, and <span class="math inline">\(\Sigma_{yx} = 2\)</span>.</p>
<ol type="1">
<li><p>Finding the Regression Coefficient Matrix <span class="math inline">\(B\)</span> Using the population formula: <span class="math display">\[
B = \Sigma_{yx}\Sigma_{xx}^{-1} = 2(4)^{-1} = 0.5
\]</span></p></li>
<li><p>Finding the Conditional Mean <span class="math inline">\(m(x)\)</span> (The Signal) The systematic component represents the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span>: <span class="math display">\[
\begin{aligned}
m(x) &amp;= \mu_y + B(x - \mu_x) \\
&amp;= 1 + 0.5(x - 2) = 0.5x
\end{aligned}
\]</span></p></li>
<li><p>Variance of the Signal <span class="math inline">\(\text{Var}(m(x))\)</span> Using the quadratic form established in the theorem: <span class="math display">\[
\text{Var}(m(x)) = B \Sigma_{xx} B^T = 0.5(4)(0.5) = 1
\]</span></p></li>
<li><p>Variance of the Noise <span class="math inline">\(\text{Var}(y|x)\)</span> (The Residual) By the Signal-Noise Decomposition: <span class="math display">\[
\begin{aligned}
\text{Var}(y|x) &amp;= \Sigma_{yy} - \text{Var}(m(x)) \\
&amp;= 2 - 1 = 1
\end{aligned}
\]</span> Thus, <span class="math inline">\(y | x \sim N(m(x), 1)\)</span>. The total variance (2) is split equally between signal (1) and noise (1).</p></li>
<li><p>Multiple Correlation Coefficient (<span class="math inline">\(R^2\)</span>) <span class="math display">\[
R^2 = \frac{\text{Var}(m(x))}{\Sigma_{yy}} = \frac{1}{2} = 0.5
\]</span></p></li>
</ol>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-variance-decomp-scaled" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-variance-decomp-scaled-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-variance-decomp-scaled-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-variance-decomp-scaled-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Illustration of Rao-Blackwell Variance Decomposition in Bivariate Normal
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-trivariate" class="theorem example">
<p><span class="theorem-title"><strong>Example 11 (Trivariate Normal with 2 Predictors)</strong></span> Let <span class="math inline">\(V = (y, x_1, x_2)' \sim N_3(\mu, \Sigma)\)</span> with: <span class="math display">\[
\mu = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 10 &amp; 3 &amp; 4 \\ 3 &amp; 2 &amp; 1 \\ 4 &amp; 1 &amp; 4 \end{pmatrix}
\]</span> We partition these into <span class="math inline">\(\Sigma_{yy} = 10\)</span>, <span class="math inline">\(\Sigma_{yx} = \begin{pmatrix} 3 &amp; 4 \end{pmatrix}\)</span>, and <span class="math inline">\(\Sigma_{xx} = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 4 \end{pmatrix}\)</span>.</p>
<ol type="1">
<li><p>Finding the Regression Coefficient Matrix <span class="math inline">\(B\)</span> <span class="math display">\[
\Sigma_{xx}^{-1} = \frac{1}{7} \begin{pmatrix} 4 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \implies B = \Sigma_{yx} \Sigma_{xx}^{-1} = \begin{pmatrix} \frac{8}{7} &amp; \frac{5}{7} \end{pmatrix}
\]</span></p></li>
<li><p>Finding the Conditional Mean <span class="math inline">\(m(x)\)</span> (The Signal) <span class="math display">\[
m(x) = 1 + \frac{8}{7}(x_1 - 2) + \frac{5}{7}(x_2 - 3)
\]</span></p></li>
<li><p>Variance of the Signal <span class="math inline">\(\text{Var}(m(x))\)</span> <span class="math display">\[
\text{Var}(m(x)) = B \Sigma_{xx} B^T = \begin{pmatrix} \frac{8}{7} &amp; \frac{5}{7} \end{pmatrix} \begin{pmatrix} 3 \\ 4 \end{pmatrix} = \frac{44}{7} \approx 6.29
\]</span></p></li>
<li><p>Variance of the Noise <span class="math inline">\(\text{Var}(y|x)\)</span> (The Residual) Using the Signal-Noise Decomposition: <span class="math display">\[
\Sigma_{y|x} = \Sigma_{yy} - \text{Var}(m(x)) = 10 - 6.29 = 3.71
\]</span></p></li>
<li><p>Multiple Correlation Coefficient (<span class="math inline">\(R^2\)</span>) <span class="math display">\[
R^2 = \frac{6.29}{10} = 0.629
\]</span></p></li>
</ol>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">4</span>), <span class="at">nrow=</span><span class="dv">3</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>var_total <span class="ot">&lt;-</span> sigma[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>S_yx <span class="ot">&lt;-</span> <span class="fu">matrix</span>(sigma[<span class="dv">1</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">nrow=</span><span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>S_xx <span class="ot">&lt;-</span> sigma[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>B_mat <span class="ot">&lt;-</span> S_yx <span class="sc">%*%</span> <span class="fu">solve</span>(S_xx)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>var_signal <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(B_mat <span class="sc">%*%</span> S_xx <span class="sc">%*%</span> <span class="fu">t</span>(B_mat))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>var_noise <span class="ot">&lt;-</span> var_total <span class="sc">-</span> var_signal</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2024</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1000</span>, <span class="at">mean=</span>mu, <span class="at">sigma=</span>sigma)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y=</span>dat[,<span class="dv">1</span>], <span class="at">x1=</span>dat[,<span class="dv">2</span>], <span class="at">x2=</span>dat[,<span class="dv">3</span>])</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>m_x <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">8</span><span class="sc">/</span><span class="dv">7</span>)<span class="sc">*</span>(df<span class="sc">$</span>x1 <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">+</span> (<span class="dv">5</span><span class="sc">/</span><span class="dv">7</span>)<span class="sc">*</span>(df<span class="sc">$</span>x2 <span class="sc">-</span> <span class="dv">3</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>limit_min <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">12</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>limit_max <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>seq_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(limit_min, limit_max, <span class="at">length.out=</span><span class="dv">300</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>scale_factor <span class="ot">&lt;-</span> <span class="dv">20</span> </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>df_total <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> seq_vals, <span class="at">x =</span> <span class="dv">9</span> <span class="sc">+</span> <span class="fu">dnorm</span>(seq_vals, <span class="dv">1</span>, <span class="fu">sqrt</span>(var_total)) <span class="sc">*</span> scale_factor)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>df_signal <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> seq_vals, <span class="at">y =</span> <span class="sc">-</span><span class="dv">8</span> <span class="sc">-</span> <span class="fu">dnorm</span>(seq_vals, <span class="dv">1</span>, <span class="fu">sqrt</span>(var_signal)) <span class="sc">*</span> scale_factor)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>df_noise <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> seq_vals, <span class="at">x =</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fu">dnorm</span>(seq_vals, <span class="dv">5</span>, <span class="fu">sqrt</span>(var_noise)) <span class="sc">*</span> scale_factor)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>m_x, <span class="at">y=</span>y)) <span class="sc">+</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="dv">0</span>, <span class="at">slope=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">"red"</span>, <span class="at">linewidth=</span><span class="fl">0.5</span>, <span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.15</span>, <span class="at">size=</span><span class="fl">1.5</span>, <span class="at">color=</span><span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_polygon</span>(<span class="at">data=</span>df_signal, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">fill=</span><span class="st">"red"</span>, <span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">data=</span>df_signal, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">color=</span><span class="st">"red"</span>, <span class="at">linewidth=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x=</span><span class="dv">1</span>, <span class="at">y=</span><span class="sc">-</span><span class="dv">11</span>, <span class="at">label=</span><span class="st">"Signal Var</span><span class="sc">\n</span><span class="st">(m(x))"</span>, <span class="at">color=</span><span class="st">"red"</span>, <span class="at">size=</span><span class="dv">3</span>, <span class="at">fontface=</span><span class="st">"bold"</span>) <span class="sc">+</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_polygon</span>(<span class="at">data=</span>df_total, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">fill=</span><span class="st">"gray40"</span>, <span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">data=</span>df_total, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">color=</span><span class="st">"gray40"</span>, <span class="at">linewidth=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x=</span><span class="dv">11</span>, <span class="at">y=</span><span class="dv">6</span>, <span class="at">label=</span><span class="st">"Total Var</span><span class="sc">\n</span><span class="st">(y)"</span>, <span class="at">color=</span><span class="st">"gray40"</span>, <span class="at">size=</span><span class="dv">3</span>, <span class="at">fontface=</span><span class="st">"bold"</span>) <span class="sc">+</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_polygon</span>(<span class="at">data=</span>df_noise, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">fill=</span><span class="st">"blue"</span>, <span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">data=</span>df_noise, <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y), <span class="at">color=</span><span class="st">"blue"</span>, <span class="at">linewidth=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x=</span><span class="dv">6</span>, <span class="at">y=</span><span class="dv">9</span>, <span class="at">label=</span><span class="st">"Noise Var</span><span class="sc">\n</span><span class="st">(y|x)"</span>, <span class="at">color=</span><span class="st">"blue"</span>, <span class="at">size=</span><span class="dv">3</span>, <span class="at">fontface=</span><span class="st">"bold"</span>) <span class="sc">+</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">12</span>, <span class="dv">14</span>)) <span class="sc">+</span> <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">14</span>, <span class="dv">12</span>)) <span class="sc">+</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">ratio=</span><span class="dv">1</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Signal m(x)"</span>, <span class="at">y =</span> <span class="st">"Observed y"</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-trivariate-refined" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trivariate-refined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-trivariate-refined-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trivariate-refined-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Signal-Noise Variance Decomposition in Multivariate Normal
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>x1_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(df<span class="sc">$</span>x1), <span class="fu">max</span>(df<span class="sc">$</span>x1), <span class="at">length.out=</span><span class="dv">20</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>x2_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(df<span class="sc">$</span>x2), <span class="fu">max</span>(df<span class="sc">$</span>x2), <span class="at">length.out=</span><span class="dv">20</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span>x1_seq, <span class="at">x2=</span>x2_seq)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>y_pred <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> (<span class="dv">8</span><span class="sc">/</span><span class="dv">7</span>)<span class="sc">*</span>(grid<span class="sc">$</span>x1 <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">+</span> (<span class="dv">5</span><span class="sc">/</span><span class="dv">7</span>)<span class="sc">*</span>(grid<span class="sc">$</span>x2 <span class="sc">-</span> <span class="dv">3</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>z_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(grid<span class="sc">$</span>y_pred, <span class="at">nrow=</span><span class="dv">20</span>, <span class="at">ncol=</span><span class="dv">20</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">data =</span> df, <span class="at">x =</span> <span class="sc">~</span>x1, <span class="at">y =</span> <span class="sc">~</span>x2, <span class="at">z =</span> <span class="sc">~</span>y,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">color =</span> <span class="st">'#444'</span>, <span class="at">opacity =</span> <span class="fl">0.5</span>),</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">name =</span> <span class="st">"Observed (Total)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x =</span> x1_seq, <span class="at">y =</span> x2_seq, <span class="at">z =</span> z_matrix,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">opacity =</span> <span class="fl">0.6</span>, <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"red"</span>)),</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale =</span> <span class="cn">FALSE</span>, <span class="at">name =</span> <span class="st">"Signal (m(x))"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"x1"</span>), <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"x2"</span>), <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">"y"</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-plotly-plane" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plotly-plane-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-b0ab243c41e272d8b9cd" style="width:100%;height:650px;"></div>
<script type="application/json" data-for="htmlwidget-b0ab243c41e272d8b9cd">{"x":{"visdat":{"1562c2deb32":["function () ","plotlyVisDat"],"15621aac25dd":["function () ","data"]},"cur_data":"15621aac25dd","attrs":{"15621aac25dd":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"size":3,"color":"#444","opacity":0.5},"name":"Observed (Total)","inherit":true},"15621aac25dd.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[-7.3813696184952322,-6.9528670828384112,-6.5243645471815892,-6.0958620115247673,-5.6673594758679453,-5.2388569402111234,-4.8103544045543014,-4.3818518688974795,-3.9533493332406584,-3.5248467975838369,-3.0963442619270154,-2.667841726270193,-2.2393391906133715,-1.81083665495655,-1.3823341192997276,-0.95383158364290654,-0.5253290479860846,-0.096826512329262204,0.33167602332755841,0.76017855898438125],[-6.8838325191426568,-6.4553299834858358,-6.0268274478290138,-5.5983249121721919,-5.1698223765153699,-4.741319840858548,-4.312817305201726,-3.8843147695449045,-3.455812233888083,-3.0273096982312615,-2.59880716257444,-2.1703046269176176,-1.7418020912607961,-1.3132995556039746,-0.88479701994715221,-0.45629448429033115,-0.0277919486335092,0.40071058702331319,0.82921312268013381,1.2577156583369566],[-6.3862954197900814,-5.9577928841332604,-5.5292903484764384,-5.1007878128196165,-4.6722852771627945,-4.2437827415059726,-3.8152802058491511,-3.3867776701923291,-2.9582751345355076,-2.5297725988786861,-2.1012700632218646,-1.6727675275650422,-1.2442649919082207,-0.8157624562513992,-0.38725992059457681,0.041242615062244248,0.4697451507190662,0.89824768637588859,1.3267502220327092,1.755252757689532],[-5.8887583204375051,-5.4602557847806841,-5.031753249123863,-4.6032507134670402,-4.1747481778102191,-3.7462456421533972,-3.3177431064965752,-2.8892405708397533,-2.4607380351829318,-2.0322354995261103,-1.6037329638692888,-1.1752304282124664,-0.74672789255564487,-0.31822535689882336,0.11027717875799903,0.53877971441482009,0.96728225007164204,1.3957847857284644,1.824287321385285,2.2527898570421079],[-5.3912212210849297,-4.9627186854281078,-4.5342161497712867,-4.1057136141144639,-3.6772110784576428,-3.2487085428008213,-2.8202060071439994,-2.3917034714871774,-1.9632009358303559,-1.5346984001735344,-1.1061958645167129,-0.67769332885989053,-0.24919079320306903,0.17931174245375248,0.60781427811057487,1.0363168137673959,1.4648193494242179,1.8933218850810403,2.3218244207378609,2.7503269563946837],[-4.8936841217323543,-4.4651815860755324,-4.0366790504187104,-3.6081765147618885,-3.179673979105067,-2.7511714434482455,-2.3226689077914235,-1.8941663721346016,-1.4656638364777801,-1.0371613008209586,-0.60865876516413697,-0.18015622950731469,0.24834630614950681,0.67684884180632832,1.1053513774631507,1.5338539131199718,1.9623564487767937,2.3908589844336161,2.8193615200904367,3.2478640557472596],[-4.3961470223797781,-3.967644486722957,-3.5391419510661351,-3.1106394154093131,-2.6821368797524912,-2.2536343440956701,-1.8251318084388479,-1.396629272782026,-0.96812673712520447,-0.53962420146838297,-0.11112166581156135,0.31738086984526093,0.74588340550208243,1.1743859411589039,1.6028884768157263,2.0313910124725476,2.4598935481293696,2.8883960837861915,3.3168986194430126,3.7454011550998354],[-3.8986099230272027,-3.4701073873703812,-3.0416048517135597,-2.6131023160567373,-2.1845997803999158,-1.7560972447430943,-1.3275947090862723,-0.89909217342945047,-0.47058963777262897,-0.042087102115807407,0.38641543354101415,0.81491796919783643,1.243420504854658,1.6719230405114796,2.1004255761683019,2.528928111825123,2.957430647481945,3.3859331831387673,3.814435718795588,4.2429382544524108],[-3.4010728236746273,-2.9725702880178053,-2.5440677523609843,-2.1155652167041614,-1.6870626810473401,-1.2585601453905186,-0.83005760973369669,-0.40155507407687474,0.026947461579946763,0.45544999723676832,0.88395253289358988,1.3124550685504122,1.7409576042072337,2.1694601398640554,2.5979626755208773,3.0264652111776984,3.4549677468345203,3.8834702824913432,4.3119728181481634,4.7404753538049862],[-2.9035357243220514,-2.4750331886652299,-2.0465306530084084,-1.618028117351586,-1.1895255816947645,-0.76102304603794302,-0.33252051038112107,0.095982025275700877,0.52448456093252238,0.95298709658934389,1.3814896322461654,1.8099921679029878,2.2384947035598093,2.6669972392166308,3.0954997748734532,3.5240023105302742,3.9525048461870962,4.3810073818439186,4.8095099175007388,5.2380124531575625],[-2.4059986249694756,-1.9774960893126539,-1.5489935536558324,-1.12049101799901,-0.69198848234218846,-0.26348594668536696,0.16501658897145499,0.59351912462827694,1.0220216602850984,1.4505241959419199,1.8790267315987417,2.3075292672555641,2.7360318029123851,3.1645343385692071,3.593036874226029,4.0215394098828501,4.450041945539672,4.8785444811964949,5.307047016853315,5.7355495525101379],[-1.9084615256169002,-1.4799589899600787,-1.0514564543032572,-0.62295391864643479,-0.19445138298961329,0.23405115266720822,0.66255368832403017,1.0910562239808521,1.5195587596376736,1.9480612952944951,2.3765638309513166,2.805066366608139,3.2335689022649605,3.662071437921782,4.0905739735786044,4.5190765092354255,4.9475790448922474,5.3760815805490694,5.8045841162058904,6.2330866518627133],[-1.4109244262643239,-0.9824218906075024,-0.5539193549506809,-0.12541681929385851,0.303085716362963,0.7315882520197845,1.1600907876766064,1.5885933233334284,2.0170958589902499,2.4455983946470714,2.8741009303038929,3.3026034659607153,3.7311060016175368,4.1596085372743588,4.5881110729311807,5.0166136085880018,5.4451161442448237,5.8736186799016465,6.3021212155584667,6.7306237512152896],[-0.91338732691174895,-0.48488479125492745,-0.056382255598105946,0.37212028005871645,0.80062281571553795,1.2291253513723595,1.6576278870291814,2.0861304226860033,2.5146329583428249,2.9431354939996464,3.3716380296564679,3.8001405653132903,4.2286431009701122,4.6571456366269333,5.0856481722837561,5.5141507079405763,5.9426532435973982,6.3711557792542211,6.7996583149110421,7.2281608505678641],[-0.41585022755917223,0.012652308097649279,0.44115484375447078,0.86965737941129317,1.2981599150681147,1.7266624507249362,2.1551649863817581,2.5836675220385801,3.0121700576954016,3.4406725933522231,3.8691751290090446,4.2976776646658674,4.7261802003226885,5.1546827359795095,5.5831852716363324,6.0116878072931534,6.4401903429499754,6.8686928786067973,7.2971954142636184,7.7256979499204412],[0.081686871793401394,0.5101894074502229,0.9386919431070444,1.3671944787638668,1.7956970144206883,2.2241995500775098,2.6527020857343317,3.0812046213911537,3.5097071570479752,3.9382096927047967,4.3667122283616182,4.795214764018441,5.2237172996752621,5.6522198353320832,6.080722370988906,6.5092249066457271,6.937727442302549,7.366229977959371,7.794732513616192,8.2232350492730149],[0.57922397114597768,1.0077265068027992,1.4362290424596207,1.8647315781164431,2.2932341137732646,2.7217366494300861,3.150239185086908,3.57874172074373,4.0072442564005515,4.4357467920573734,4.8642493277141945,5.2927518633710164,5.7212543990278384,6.1497569346846603,6.5782594703414823,7.0067620059983033,7.4352645416551253,7.8637670773119481,8.2922696129687683,8.720772148625592],[1.076761070498554,1.5052636061553755,1.933766141812197,2.3622686774690194,2.7907712131258409,3.2192737487826624,3.6477762844394843,4.0762788200963058,4.5047813557531278,4.9332838914099497,5.3617864270667708,5.7902889627235936,6.2187914983804147,6.6472940340372357,7.0757965696940586,7.5042991053508796,7.9328016410077016,8.3613041766645235,8.7898067123213437,9.2183092479781674],[1.5742981698511302,2.0028007055079518,2.4313032411647733,2.8598057768215956,3.2883083124784172,3.7168108481352387,4.145313383792061,4.573815919448883,5.0023184551057041,5.430820990762526,5.8593235264193471,6.287826062076169,6.716328597732991,7.1448311333898129,7.5733336690466349,8.001836204703455,8.430338740360277,8.8588412760171007,9.2873438116739209,9.7158463473307428],[2.0718352692037056,2.5003378048605271,2.9288403405173487,3.357342876174171,3.7858454118309925,4.2143479474878145,4.6428504831446364,5.0713530188014584,5.4998555544582794,5.9283580901151014,6.3568606257719225,6.7853631614287444,7.2138656970855664,7.6423682327423883,8.0708707683992102,8.4993733040560322,8.9278758397128541,9.3563783753696761,9.7848809110264963,10.21338344668332]],"type":"surface","x":[-1.8246779689413803,-1.3893330070078767,-0.95398804507437285,-0.51864308314086904,-0.08329812120736535,0.35204684072613857,0.78739180265964226,1.222736764593146,1.6580817265266496,2.0934266884601533,2.5287716503936575,2.9641166123271607,3.3994615742606649,3.8348065361941681,4.2701514981276727,4.7054964600611751,5.1408414219946792,5.5761863839281833,6.0115313458616875,6.4468763077951907],"y":[-2.6144327155871174,-2.0145291656675672,-1.4146256157480166,-0.81472206582846618,-0.21481851590891576,0.38508503401063443,0.98498858393018507,1.5848921338497357,2.1847956837692859,2.7846992336888361,3.3846027836083863,3.9845063335279374,4.5844098834474876,5.1843134333670378,5.7842169832865888,6.3841205332061381,6.9840240831256892,7.5839276330452403,8.1838311829647896,8.7837347328843407],"opacity":0.59999999999999998,"colorscale":[[0,1],["red","red"]],"showscale":false,"name":"Signal (m(x))","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[3.2328732910516438,3.464926418033845,2.0321322744750598,-0.75423217428671174,2.4541903564848964,0.82197637610765262,-1.4059543879062595,3.4664192000629699,0.35821291994596116,-0.07266212167377839,0.54053792749104801,2.2684887989747367,1.5469829542218005,1.2207953644475718,4.5893164750168776,2.5907717076252776,4.7412888167614486,0.53155111564149315,-0.097701401740093008,3.060660766682072,4.2263516657539562,4.1235704836112488,1.5825494901306634,1.7629624399478092,2.3236665155180694,3.6313808278741009,1.1269308432854452,3.4858824594889617,3.4392837712253002,1.6355660111721761,0.41634627365901533,2.8434552957934458,-0.25184280419364935,5.566496026658001,2.6851094671629814,2.1790362461024486,0.029310392189627388,1.9050321902995768,-0.40017628450747011,2.1695188791151829,1.2251008931458029,0.99492291100352404,-0.15951961422156558,1.1832971660846319,1.8458104668205375,0.023931849496779067,1.935122107776547,0.42092928192880752,3.7425004406845357,3.318907534760998,1.515077371779427,1.4808329814499721,1.7462943276503866,4.3363991786345517,3.8608018717037478,1.4840404068950241,5.4823528780277737,2.063034605906894,3.8736484397308195,2.9374151682278504,2.8866325619137676,1.1928378277457325,2.1425817075424485,1.452564893133578,3.8767461688566942,4.0238770560669277,1.3457483100626886,1.3999026752834964,1.3501321578276686,3.2433825454289167,2.1319819286342612,0.96911267290288006,3.033857462559149,2.1557548185193167,2.0055579663874026,2.1266746905774729,2.7773053079583407,2.138810761211706,2.0760868365036136,1.6094600469095721,3.6886799728481989,1.9335309555193709,1.7668974262447039,4.2544750977677879,2.4011304882962219,1.0799910744381755,0.92967857262560494,2.3052961882811234,0.94094706322010446,3.2970004872797114,2.1972239036090162,2.14843835346693,-0.51231076807009801,2.77056531742728,3.2976122231919751,0.78376631955283749,5.7637507164559203,4.5267885763529243,3.2887044045138927,-0.38763734895372792,1.6640060638735354,4.7529244998890103,2.8311275148642352,3.187943118406829,0.25463684217036553,1.6678114472453478,1.7709056561525034,2.5150683278008339,3.4707594731862725,0.017524443565026138,1.4839926689733676,2.8421053744988538,0.39027998251180751,1.2693039680487956,2.3728863406629959,1.6924044767235589,2.7489276374245524,3.3236791314851484,0.083338817726201109,3.8415035117916396,2.3987563641372187,2.0192249196889835,2.3463607923545879,3.5942322811599681,1.5411944419616437,1.5870972710094746,2.5831942663508207,2.1618134446773229,3.2915214813806464,2.5751198700148548,3.0531894167163389,3.0393591877144535,1.3234934408184993,4.4470407812951915,1.2502374155568159,0.87600304721552624,3.2901039965929781,3.1667759559299546,2.4436420953743281,-0.37855456965566159,2.7441181801779662,2.7089661486401067,1.4159129486637558,5.7583242722189993,1.6902717545923356,0.54478511786505912,1.4893303346168176,2.7412439620570312,1.7136252682052069,1.8139039207500283,2.199815966980573,1.5296931257431181,3.0515561204861568,3.7388319322521815,3.59002791032548,2.3272711069593255,2.6600174298953125,1.4639202853972231,1.7979501918353713,0.92557863138149954,1.3706812339771992,1.3568776550402548,2.0580824832278481,2.6396764317616768,1.889060076845279,1.5038831662509269,1.7525136059865101,2.4831738947441924,2.2088542181756505,2.9534650588586953,2.4606679647912952,1.7926398608043546,1.033417500980778,-0.51023538878085128,-0.98889473484637769,1.8710611034669173,2.7850868649335485,0.99244663677239964,4.6270125491998488,-0.80405220397992272,0.68176715184001813,1.0070022434162316,1.5722046321994556,2.9672845333165521,2.0133168023964703,0.29950542692556748,2.6266693187072638,2.5526797979184419,1.1610552538182013,2.3011681554276078,0.26206414441018611,2.226395040023768,1.5673547715485188,-0.36584097013179795,-0.25778703182927609,2.1468483131952971,5.0707097443256819,-0.44659048293678127,1.3746054262524097,3.6046444466868213,1.9019781232257482,2.2332123588434816,-0.66549756674562444,0.30958770977229322,3.5813180091331085,2.822058970346911,2.3282547396271278,0.7385621932840285,2.9052673378908964,0.99113414175456271,-0.42219020907196425,0.96113026800486967,2.1262205693007958,1.9285027378946464,3.4424776990810706,0.24661025985415397,-0.70311797878453408,0.88360628082947401,3.6201844799240437,1.5583673551443638,1.721331603317483,0.59122444748691438,2.5478074442645098,2.3183559466194916,0.81800286118556409,1.3334882698079946,0.66190586775865667,3.1381428053935361,2.1967312651362687,1.1611568506515604,1.5798496381324827,3.5153901818535669,4.0515015072723308,-0.09314970431914249,0.60240650765438497,1.1703903704960623,3.0304196033550959,1.8855666478922726,2.6370049252744328,3.9263239498883191,1.1124658264713325,1.0015542149692545,4.0043127932017306,0.81037881333140582,0.98113482148911535,3.1410141377683773,1.6553877241268062,2.0471426679938238,1.2098698601709923,2.9886602978271437,2.9127975194351099,3.2782038315037281,1.9578135587014081,2.3759225152493597,0.21544817411389316,1.047638530403004,-0.4001963004633442,2.0014861897654068,2.6608287793185998,1.983308819072505,3.7796112883261577,1.8053845337664174,0.94364951048694046,2.4426898071071377,3.5124536743663173,5.5388847005725772,2.8415240169458218,3.1614030134780582,2.4160282103770658,2.147675139329869,2.7715871602100215,1.352277263309003,0.95171865864691552,0.62453838629352343,2.6223502222666291,2.4388942634532773,-0.0095982721661829196,1.0162577398420605,1.7580215088094324,2.5158644560905437,3.8617762457895122,2.1807953866303569,3.7253715092842334,1.7907909184051838,2.4564709918969458,4.0662150306918488,0.99044571708110363,0.30822205568243555,2.0205296647725186,1.0194243951554542,3.6080127703834557,3.7103463043790983,1.880713433823993,2.2937764570683603,2.1104180945066102,3.171668935125513,3.5251294633203365,2.181100931264579,0.37562679647058284,5.1362302515354186,1.3654769703732588,3.3621629971004756,1.2612903167099723,3.1724075045295184,1.9383587245595708,2.297527479057055,2.5288953318593501,3.6553259048206148,0.93618373555230239,3.9706499825713548,2.7316980255901808,5.3092389050787325,3.0261391984356476,0.66459314910762401,0.9891831636993087,2.8550489621750508,2.3333605580905177,0.716056953764729,-0.38262691660606851,2.093803384798246,1.8301967269478148,0.055898555215085688,1.9378231184833996,2.0264218959026477,1.0073966480184473,2.6195700017766876,-1.2294314091353513,1.4831194648060246,1.4777328997504522,-0.47614927412753705,2.2982058949925008,2.0439039403717287,-0.12008433683639463,2.3918407168190341,0.8973378678604349,0.21553060036173921,1.5697475181638687,1.0221530224104525,-0.4031706449464818,-0.33537717941920553,1.1397538643072782,1.7284732528516589,3.6951112754088973,0.74802003488364188,0.74943617750647129,3.2794987171228613,2.3932817086431832,2.7831398796734206,3.5265791189159357,-0.9260068802153909,2.7142168406436475,4.5353726320491603,1.6708969565177778,2.9511616104621936,-1.1685115165909621,1.9197472877865194,3.2950416317608546,3.1665948709577316,3.7909378260198618,2.7885144781413658,3.3754358822281416,2.3563931047712772,1.5565792184471416,0.36896575486178707,1.6020045939623921,0.86649087503966915,-0.061147811167734822,1.1449529201890338,3.3649419711172746,1.6241276210320019,1.9840996567524576,2.0230086066153552,3.5081378926453306,2.2181026917550821,1.9824243753074902,3.4490604561432709,2.4897603479792982,2.2861121467686565,2.9971187786389719,2.2536140960109483,0.24880957804060189,1.2176776462759387,3.133137175197438,1.7642058873003612,3.4598940063082808,1.8262597146010078,1.2738750225230426,2.4514609828129688,0.71724081622104308,2.6120843400087366,3.4421021750625185,4.02399807276527,3.3433788377590279,2.4900035122450603,1.3875374332165369,1.6519027718144912,2.1972565600491647,3.30618584976256,2.8374736997948187,-0.034068195653317979,1.9502585235385101,1.4770421358867765,1.523716611623585,3.0675464267773194,1.9051493200424103,3.2934585835165016,-0.0207103549950447,0.92841465776347776,4.0419654369588942,2.0687883885198515,2.5548785914056942,2.066067256367909,0.7140289574547023,3.074570869669401,3.3200492814513813,0.20368767286792133,2.4617629376394619,-0.014140437687618856,1.3794637200829594,2.0281344641010302,0.14741138913510521,0.98423813209074629,0.88995541473577977,2.6650090188974511,4.9230381332704267,2.6839781914696332,0.12745409222563775,2.4680329485494603,5.4682525571431704,0.24286577798694764,-0.91387197957073285,2.0861570730587644,0.95990817710385246,2.8370695526434981,4.5713867125367162,3.9524190569938527,3.3159448676094208,3.6338414621268704,2.0308888242737844,2.260519974043417,3.0336875225018689,0.21435914170921833,1.1812333928738779,2.4978114301470384,1.8305325056798694,2.2267029862602614,3.5406583606772069,3.2165052072105338,1.5003157509406109,1.6837320077006079,4.5180971478887892,2.5898197514703627,4.9064634809036605,1.0236522497808811,3.9788524079030756,2.061355673349432,0.047966819385396153,3.317756087261885,0.98259823431915327,1.1795889811886258,2.418885804632172,0.27329599036117247,0.599710978867525,2.4973715428111429,2.9381426178827583,0.95801083894249617,3.5760230004549376,1.6324862713385979,4.5660404377928643,0.31062381137741024,3.8786563124861555,2.6068733790695617,1.3511749683447138,2.5972916337504284,2.5219874639668971,3.572705647311694,1.0192078373481426,2.3949758699542789,1.8461651001384609,1.1320682041072978,2.0394927943429466,2.6732610563766563,2.2108491939987522,3.1844310696659557,4.8916332664568429,2.7402268690802729,1.1236419068875136,2.3969372484824136,2.1696041867008469,1.9416631002235254,-0.73797827228849311,1.9060829182533607,2.3626866378582858,2.0842731031782256,1.421529439432397,1.1077314210506088,3.0186542816898179,3.7310118114241355,-1.1636587575194173,2.8931390657739366,3.6221444987493081,0.91923026634017946,2.7807118336426493,3.7068447580211776,0.99306964321004698,3.8496264472705599,3.0510639988683588,4.2576656611256452,0.79844203105284195,3.5414126939264374,0.47713461508418775,3.4504310571737866,1.6978658774412461,0.59235264197850834,5.0277736906612152,0.14731082110824656,2.8463859774757743,-0.013125393510405203,-1.5873742535782149,2.9497971468801523,1.6422927002399594,1.9243294158955195,2.4354359299351809,1.7803967456642671,2.9615797678802958,2.4128398536415236,0.12685816249768855,1.0574077108419186,-0.08901656301193972,2.883530142450998,3.2669574186228298,2.53914503859782,1.2520026609915345,2.4272109569799158,0.35727677786678269,2.3193188116308985,-0.36339516744338907,4.1288661146964998,3.0764716914311379,3.0121445409331353,1.4095496029597783,4.5535424364845376,0.37735363889563689,2.8137596693715681,2.0279178215947424,4.4557420288995164,2.8510033697615569,1.0954856784721969,2.4438285435311942,2.085760210398707,0.49132153673708712,2.4611334304012908,0.89681698442622437,1.1783543194516672,-0.4488116572066585,3.2770646777693635,0.6760030612332617,2.7280725347321049,-0.1560865974990211,2.7350199532182145,0.73499153242550097,1.3613430579931556,3.5654128461271419,2.0377358753042438,2.4610666656949962,0.069622555496905436,2.6964645410778654,0.77202522300928167,1.1797223053705319,2.8328211455715291,1.2939202187402112,2.3430499293789988,-0.1381145842310012,3.6179859283685025,0.83304604165555429,3.7718512349198829,2.9752078766947223,3.0170958891233401,0.37902125242296258,1.2312451456021472,1.8974390794081724,2.3725145019126508,1.2237799956658764,3.3213311571700426,0.82817939278467478,1.2398940821541227,2.0041111282189048,0.82273529601502404,-0.85938078017065189,2.827657440832859,-0.23493123558047957,2.2175207627824003,2.9959970399655553,2.7406547008347797,2.378016378135555,1.6795224002662004,1.3854913561969469,-1.3090685811234186,2.734149758608079,2.4113074312230456,2.6671351301391342,2.3767923684571799,1.4213252335820892,4.7965166171917613,0.15646896048622549,5.1116780091872931,1.8210673705263531,5.5539816749870168,4.4938024149120643,2.268224363828105,1.5365564496899924,0.08163501705541365,2.1748225934055223,1.783570665401216,4.3011432348607475,2.575178690171704,4.9645028154837778,1.1133917760121093,2.3378845856627262,3.9218481950607544,2.458443684790633,1.2535451259681034,-1.4862648595257926,1.9644337253781408,2.6787363006259266,4.8150895174872321,1.5997952061291181,2.0628934263842988,3.0388492858598797,0.15771745623759981,0.96511527612018533,0.14049225339859439,-0.44621773841895118,3.5876377703474338,3.2240430715671344,1.1532517460704548,1.8436394395521503,4.5388149176326316,1.2429893948273996,0.92467193692441119,1.4863441184858259,-0.11694791654614045,2.0916505291527394,1.3205634260937427,1.6162968708002357,2.393045165875809,4.5343260388444584,0.045339118707059978,2.801481351445787,0.96496689839398297,1.9534742130939089,1.9407340871357539,0.054335017707857203,0.58566111646915009,0.52451808857579763,2.9948310766241031,1.6845634228385593,1.5996254018416272,1.8504073994247232,3.1362890769396303,2.0818964302024101,3.120978690334252,3.4945752383464468,-1.4542197179203269,2.7031123989881127,2.0200331306248573,1.5368614565630334,0.054877886447015634,-0.60502517856358473,1.0432271398509976,0.45469493100706004,2.3926045052301022,1.7261800611936966,2.101374854160321,1.8616417370216287,1.1448705784779265,2.9888818914894055,1.0843380974251327,3.2607504091557615,2.8815531926309523,3.0723679790514149,2.2643080766147365,4.3061048707996132,5.5653511087035596,1.3186502196215606,1.521926001769784,1.8926194856294538,3.3139489040701982,1.0112911092455665,0.052962252099849616,0.18061065867725312,5.4251648609736511,3.0105643236703745,0.1884412428877702,3.1931578304979089,2.6904667857782312,2.3748728235563861,-0.66802032836186243,2.3799365011207776,1.5262273984401959,0.20452759928931452,1.3858680022123773,4.4333099445632786,1.306685471567762,2.72139805896169,3.651341732710673,2.0511582137187792,2.8697375693722855,2.7406373638434367,5.0024839621780828,3.4997543398677298,3.0394230773692241,1.5880801652453418,0.56474886237169875,3.4591888438627798,1.6619318087945512,-0.69759036637878635,1.7196388421120763,1.6163949727739442,3.5129079078096686,2.0683974008463579,-0.72566714337521665,3.4580884116560733,1.7172407698851992,2.2933884912708309,4.7318346746389199,0.95690018737942339,2.0746943943551233,1.5965180423125498,0.9493464980030486,1.6491618134040045,2.1733915206998633,0.077633213303674342,2.2795089718280943,-0.79050998832051755,4.8124862422029437,1.4191886401794809,2.6342969064408708,1.5917386435214922,2.375444476605332,1.3164631760854149,3.0613861171335195,1.8851507749352803,2.2321111058432814,1.077260149298287,0.097822501018629104,3.945678516605426,1.8020262070936237,4.2997693016220051,1.0640210910391112,4.518295756807742,1.5695298392384345,2.5082225668194633,2.6327941590665338,0.97068728936088267,2.43399670021281,1.7991406686344802,-0.38974954663608585,2.2501253030241459,2.3121394677339353,3.0286323526198911,2.1675424743902481,1.9412907046318502,1.9448959289158751,1.005339128117051,0.62068579031936011,2.7473754687280669,0.8964852966828214,2.9517435583065206,2.5730471137738409,1.5929432211463248,0.16036744081475418,2.5352750135088709,2.1771138857308596,0.24734899251744324,2.3083797514976689,3.5024968930468825,1.3067592599711271,0.81956424601669409,3.6244443446595742,5.576080805550891,-0.95081561599218745,3.8917942313757887,-0.71044559072841418,1.2536288821688117,3.0243984695982435,-0.54560496693133675,2.4988640713623864,4.0450476058754159,1.2499951882733058,3.4247691625883174,3.3786486286422823,1.7092514813385056,2.7247628103350072,-0.87170700027524539,1.5762628001169621,0.17851412551451307,3.0417026987270317,2.9592062719951797,3.1665427373349422,2.3429945186552654,0.9718348098369689,2.1282475231470595,3.5703788497452988,3.652899960681018,3.4658152411309526,1.1309455028360702,1.8101136825121502,2.2014615662552628,3.3672179371747326,2.2117047422153195,0.80913841394358088,5.1290169852842036,0.75045933591574654,1.9270710106252993,-1.8246779689413803,2.8319918622958671,2.5162229655046442,5.0691983170612129,3.5001923827006873,0.32104873992681759,3.1948516903317534,1.6574978722472069,1.7670835434114007,4.6838504708433657,1.8601991627767969,0.58058206249762567,3.5570797445376305,3.1271213251316778,3.5647178391861489,3.0947596664524273,2.253043773400949,4.3265728278366931,-1.0058858440844678,2.9109716080125825,3.0077034935236489,1.99324106087301,1.7016506493728445,2.6804526224065328,2.2686452421182213,3.5941587983882322,2.4671463792380188,3.2541612146605825,0.3024960953647724,3.2518717883973478,1.6943130135393121,-0.51659809323443229,2.9663678894139065,0.33568167422805928,3.0155418329954475,4.066197648251757,2.3701522800035018,2.7595221226754543,2.2850148520176652,2.7549966887101984,2.4447611119952257,0.54080219807455365,1.7034673987597071,1.5390538241762355,2.1856511477401153,1.7405788504258817,-0.27263078114877048,0.07426904954693625,2.0581207974596913,1.4477597817027985,3.1674311079727371,6.4468763077951907,2.4973715285647198,1.055881140706767,1.3246200087620486,3.9484144489228399,0.26970738067521105,0.7612964428777953,1.9145028996020823,0.12487534174432935,2.5771904983643896,3.1463489562374787,1.812352481366164,0.92282344697417251,-0.33942201271351724,3.9485489177595503,0.78445649819407892,3.7385038670637485,0.58092476432318207,1.1031078605813533,3.1390474484706576,1.2958335348607957,-0.008347780739830668,1.0712822624093734,2.0620621351748127,2.4542566801517824,2.4143768916497317,2.8139540518339898,2.232085572134026,0.66843512392452009,1.2922766782929282,3.7358844813556971,1.1443568740986767,0.63263540007062047,1.599690582624659,1.1349146146559494,0.91954673588939384,3.6731076435000469,1.9437815083812959,4.2303311661033867,1.4200016483558784,0.73246188272528912,4.8530652303799346,1.6645062307648726,2.1193626768599882,-0.0090780871893589854,4.1581461148837935,1.6985999968486849,2.5902678121573546,1.0410759789211126,5.0547975129957052,4.6694165704679751,1.0145053686385019,-0.21017076074978647,1.130739412240954,2.4457254122155163,2.0976559002750941,3.1128016580765512,0.79931294037445233,0.71033192053708172,0.97977083379332264,1.1500511833844187,3.7406096511483256,0.27839971558116039,3.4728874568924928,-1.5611613675453997,1.9049500119437792,1.2181879698682712,-0.79172278828514742,1.6156794295727872,0.69294553416667259,4.4141777882207194,2.306016110677219,0.76406638284813533,2.0900724789241214,0.81374969275559672,1.7275093396026944,2.327068348569604,0.13367566702551992,0.80098008642624641,1.4116299964482077,2.3175610625519911,0.35308901136705129,1.0968473837989414,2.4272987137259521,2.9126746246309372,1.7773683364282609,3.4631437922871111,0.40985900939078479,1.7807255143283511,4.9304231566918766,2.607824219437755,4.1044039425242325,0.19640919255276312,4.9833897225098909,2.0497098912631988,1.1472479200171968,2.6324990994729189,2.3258803495860976,2.9458694971059938,1.9103835608624484,2.5981136636792566,-0.87187390366087758,-0.57662236877657946,2.0387462619405663,0.38023445381623011,2.0098069055191314,2.392222099012292,0.66549063208887604,2.3489229526692741,1.9733914335389815,0.75827159296616253,2.258654046634665,4.7683285889229339,2.5773511561491533,2.8993967118840525,3.6091429632002496,2.9061536137018766,2.5141810650851584,2.0085192334730531,0.69016505115763516,-0.71271183480778699,0.17442821162728595,2.3724140333846231,1.0062201830113793,2.9680076531229398,3.8344361676083842,1.2473229617474779,2.8940973702696993,2.4859326490796403,1.6334991674456731,1.6470076726403449,3.8447326765002297,4.5433504463970484,2.8957490934122596],"y":[3.6693696769641413,5.3496218267772129,1.184631502547256,2.7000236009074019,-2.2459396169801158,3.5719248340633776,3.4792311931132773,3.0200969049130935,2.8829900989287869,3.0582470382723486,0.86228902808290853,4.6157560344517137,2.1632271719737974,3.2127279892924832,1.5089572354435403,4.3710017616903496,2.7910722871644986,0.41194981188585444,3.8696591438085077,2.8917780632367891,5.4001794911109577,3.1468327204222275,2.3780787924803208,-0.75828630891484572,3.6087010021514909,4.1975116539261661,-0.10663152892203387,0.73439728390182601,2.9493443535286104,4.2438008311395157,0.64308827146884662,-0.12516418795608608,1.8270596866706061,5.0585197433640046,1.5610282651417329,5.3788512977725347,0.022069223778906188,3.7595157272430959,5.6960720288165696,5.0446655238888836,0.36178765841770399,2.7888204141116968,0.93244220958847901,3.5244047052054488,5.5575254609627009,3.1024586702705621,4.2265285867460651,5.0471995416887117,6.9693408835303181,5.1524282332443541,6.4296733402138688,-1.4770610506966948,1.9250725837297442,3.485540923319828,2.5302358803334553,-0.63735579950999011,3.6415648583771136,4.0593357751295507,5.7543249716484688,2.5494325405953879,4.8227566344242323,3.7755687793496988,2.5367720601057107,1.3991307664368327,5.951839071084863,4.5476046703422419,4.6065972114804392,3.6473682635844429,2.3143000475013289,4.0309639781590736,1.4927894081874122,1.5923125105500444,2.5570178915897182,6.5068148226329008,3.1463889779078986,4.0183652101292244,5.3356338625531734,2.8380106117730817,2.870912735446145,0.92240813858452242,5.8219654454719088,7.2353570949699328,0.53475861221120669,3.6897874480980439,4.2889303635408726,1.4488908164571979,2.8467159360404168,1.4202810894736513,0.19544269287166438,5.3557383194520742,1.2561620789548142,2.4763509087509732,0.5609900439328368,3.4369747757034239,5.2526728934003888,2.6848751330671705,3.0029841990928441,3.1342852800039074,5.7681899566755543,2.0648199513492624,2.9068639598977892,4.917461301935651,3.3437109316100564,1.6958271303447687,6.9041962690557606,2.7210959378127102,1.8075354307744811,1.9810614455878761,3.2693718326182304,2.609300180770731,5.4153808544863384,0.015900254672822633,3.8276384092033671,0.36689028515504063,4.5911108657521407,4.9346595379333742,1.5126689896204468,2.3715003313161924,2.5967000496143697,1.8246474231101779,1.5155064566202916,1.5704173874475371,4.2319405951611335,1.902148563501312,2.7324792081005298,2.0208112144898278,5.0840058621694508,2.8960076481780965,1.3562226600190883,5.8451556998525813,4.7477853629106228,3.2821362782946828,0.17875970840252542,4.5782220385246362,4.778862696402749,1.170843610478923,5.9109373236303266,4.5160116354700985,1.6443241548688612,3.2318280228637986,1.6478417452371046,2.2391092827191552,5.4520814420091632,7.0874778873521924,5.4703558518265405,1.9381414968944213,1.3959988481172561,3.4273849787670376,1.1652012280586623,1.8565253596325215,0.65329721357796755,3.0246069160905855,1.2851164616233024,5.2528924144764044,0.74607258752677508,5.3481429374045195,5.8280535808397484,2.7102067024222003,2.7086987772850097,3.522941997571162,1.8961665426161609,3.9801766143303712,1.9501020507851314,3.5647790689436816,1.7330156696080437,4.7555594199053237,3.2571750412318474,2.55089198797087,0.88714438115811456,2.4641822808719791,1.4195686036863941,2.1013920032625055,1.9771787897371582,0.33366745222324123,1.422063730070845,5.6380423431123052,2.0852516043479312,3.1688385753286377,3.2123108161049707,2.3490179817985353,2.6185322340479296,4.9374774744667338,2.7628390387890893,3.8706240582010198,2.9353606820636684,5.7324649527295151,5.2786719179487971,4.316904941743565,1.5294728622768405,0.073360814048772927,3.5269755810898724,4.7156939994689644,4.5938897222890995,3.168733923084464,2.8013283510297753,1.3907117165789877,4.7093841729894494,2.6799340006313903,2.1170695439036562,3.7231176170536644,2.1885292352850461,1.612850874152969,2.4462868309350432,4.5950327665489068,3.5743534501969867,6.5616958569968915,4.5474060591616094,0.73202050774415728,2.4547689958629584,1.7486549747426208,-1.3366916869369385,4.8580127879848192,2.5367338768729049,3.1136227529963838,4.0300727970564809,5.4641477480601468,4.0960762284154466,1.7577817408047258,2.4209459596949285,2.7657404785324364,3.6667714227200663,4.3936823458283838,2.4266291074693238,2.4999296122641659,1.8952726274938876,2.1633562943331182,3.0359943223413723,8.7837347328843407,4.608240355364801,0.81910324353012154,4.1947075489736187,2.4495122567626382,6.104333795663246,3.7045515593692491,1.7671256754321591,1.1238138908779374,2.6107822581278546,3.2880242910473099,3.9184292785695467,4.9491752328151408,2.4520235399525321,4.6096265428486927,5.4102901043430789,4.6915481545150399,-1.0424245222258515,3.621453932719898,3.957376180504915,0.89406878765376074,1.6364163384490513,2.1300206952648497,6.4280028063698467,2.4037127450372942,0.76548103541381929,4.4971444506185225,-2.0479513233300013,2.7906970983908139,-0.28197023603381943,3.5046294912910914,0.6669614742857326,3.475443368966086,3.0160469168638211,0.8698877995043075,2.660429878342367,1.4911147036683667,2.6466171403772858,7.7359434335952608,2.1477094741031721,2.2599472086457384,0.16306272636259544,5.215430447336538,2.1922044261992699,1.8808819706740176,1.9874989201512774,1.7844462903892753,1.8344321304711688,3.5493010392342841,2.3353450989537046,3.4656080716244881,1.3661504742405863,0.46325541014524196,5.9941676001216813,4.6269639671432561,8.3339902262069785,2.3683049699292695,1.3496465727513551,2.1273610082218735,0.56955758192444783,2.1803688629920228,5.3787577445822521,2.8694380058960576,3.6310394765906877,6.3409407039553658,2.5522542002867339,-0.90659386964918243,3.9135827012715683,6.1408241611157681,2.9244844705536894,2.2141311045972034,3.78705115078251,5.1097122151340955,0.41455512312492004,3.3163705393539327,2.4958184903385385,4.1043170735468184,1.9587884421421593,2.8886193561565179,3.4140992039874485,3.7547605758144029,-0.48536702725491132,6.5867227447049199,0.31883301628209004,4.3903945912340978,2.5768650345293787,4.6091054070593618,2.8974588150018241,4.7382417981395974,5.3577727173287748,4.9979609974021022,2.8635052443587172,4.643478899073493,5.0605192449288854,0.74585863010021658,-0.49429956361732108,0.79927265393561964,3.0256146473260923,5.3262811548294362,-0.50414749523922531,-0.016157698038917978,2.0202361565854843,0.68134775464312058,1.7531629802841826,4.0266348087723429,-1.2705814602264578,2.7090057002372809,1.8692835770106646,-0.27309989167041326,4.137537874326525,2.8641923921433814,2.438099076900226,2.5722279624450528,-0.31371271904291254,2.4086104963929986,5.5173572039436038,0.85464990301799526,1.1753754751570422,0.39213093780398056,0.40395407602428657,3.2571649847045654,4.3358191478982677,1.0111973891389512,3.4236840319525714,2.5219473770545582,4.719703701314204,3.730341853067948,-1.9844990794304191,3.6453786282557044,-0.34951910612192583,3.6043329692807742,3.8670914253406194,1.6163936030081338,5.7444815144593786,6.0135860571083128,4.5063662893442586,5.0296764102149991,0.92765254795338548,0.27755740317372224,3.8551593419787844,1.6365924667447556,7.2995539661855782,1.6897668121974001,1.9049577804182012,1.6478491711684065,2.089600670534002,2.7609068985296243,5.7372705028541571,1.9292037885826525,4.6665823393521357,4.803750056652766,2.2856902127625034,2.2736759196950724,3.0216371520465288,2.0040125065298899,1.4838544038548456,4.4861366377048508,2.4794439467212017,2.7501786716751706,3.4174494576676566,3.7003472016161245,-2.6144327155871174,5.1327577965866187,3.2718769574191269,5.0970037560358605,4.6058609868018046,6.9217299501102243,3.7735908695966991,6.511729508462361,4.9127227386297152,4.1161755998657927,3.8271360607382383,1.8136430257728673,-0.76481521185537238,2.9058373665456148,3.3695286581553185,4.7394066863725559,7.2017113323236854,2.7744395579225611,5.572205170093234,2.3341835970413163,3.4775688909030782,3.7652807484781201,3.3305972668966861,1.5277342609673004,1.0682585251413856,2.6899478053780852,2.4175484491937294,-0.3914793782532815,3.9101160315471439,3.3894591608878906,4.4093164623295982,2.2881396766063311,-0.71995968193146442,3.7867546603337581,1.9158409775326888,4.9434540490331091,5.2459482150575951,4.9829367254279031,1.6754908571649059,0.85893811148672228,8.6856795159717546,2.4920497081425625,3.8629951082032128,5.9654438739292033,4.1450721956114567,4.4290211593405946,3.8058403993421184,7.3008363257930107,4.8624428882343143,3.3057001733950933,1.5140791206340285,1.2355735309063758,1.8115677391156855,1.372947971268808,3.9264178908755918,3.408644123390618,0.69535292959226469,2.2904733651761928,5.6467481190812983,6.1410195272135084,2.6137838871454573,3.7378239794835268,0.61272529445210466,5.525363846379296,3.7505010071457745,0.50567480618977934,5.2487313555992028,2.5672804983999655,1.5847369709606256,0.56695369592214373,2.0130752797985556,2.0966775353316596,1.4946761054437712,1.7966586957686526,1.8213268727438401,4.8130123285995161,4.8676770325811889,3.8379550102262798,2.5043993231491108,-0.6574446446252189,2.1342995782342511,0.28988313296371038,4.531968780958378,4.3524704083208405,3.0590863646136879,4.896846020415607,1.949221159759039,3.4626353201314175,1.5688300186150088,0.92634991777726317,4.6743296063756237,4.4777216029733458,1.1287350489185577,1.7571158128276416,4.5035202189474051,1.1676586289375823,3.8005282002327769,7.403524934329643,1.6911254900566888,6.8588966426532361,3.863156305711315,4.4194373013707651,1.9243530896940078,4.9000852804235944,5.6831641040057539,6.8769116019304644,2.3444895402315566,3.2981286750022947,3.3505438998226706,4.7461897029800353,1.8362421731056664,1.8588748790126906,6.3418391951230184,2.1618117881811516,7.060690132706271,6.7172087513985508,2.4028978876769171,1.9453998335629046,5.0826140927145893,5.8894132600973226,3.6323894017807707,4.2238557102063226,1.4297678998270362,3.6413426734488441,0.99829760284157265,1.3942492294050279,2.5085638492460283,4.7952788987569468,0.44696971703598942,-1.5256061623269632,-2.4101571767805758,2.5410275443058383,4.5542397123647937,3.8689150457575723,4.5648519837396835,1.7844739429933512,4.4663392595942657,2.2951555602994258,3.5530436056364403,4.4681792777808598,-0.40885256251585478,3.4634477067577709,0.62373523686485566,5.3406159192495375,4.273502800855649,5.8527977557504407,0.37458176146500399,0.3368502502905435,0.83687637993909547,4.9162185630534783,6.098836129286946,4.6978748371229448,2.1221376046394038,2.3310597203310217,-2.2501542796569023,6.1243167628633941,4.0428017937454106,3.4401756625569537,2.8832108689513753,5.0293858625399972,2.774662043536043,2.8608551624284386,2.4818735881550915,4.5398633186177211,1.7246819668273783,3.3099148402103475,-0.20660712421564442,5.8855859956411667,2.848669324601246,4.8305071593665252,1.6015065102856736,0.8818758964235025,3.8507266702658125,4.0660417816394983,1.6638393471098558,2.1080737003362593,2.69162257130403,0.523795893709468,1.6998971427215628,2.2867785387507693,5.4812997956513207,7.2462169992901444,3.0050428397248568,2.4889160083822808,0.37309187255996079,4.8970870197072562,3.0572360150877538,3.656276358623062,5.0328489500181899,5.2466095360087976,3.0843429274903889,2.6022899444049332,3.8713305195757983,3.2698833917680923,3.2968658411798777,3.935378322467499,2.0793099904192935,2.4900597212594731,3.2524897933410184,1.8781295063250794,2.9347678960999208,2.5370454578961903,5.0185501207126357,2.2134145078973146,3.9832750493770055,2.6887634878603688,0.74005262909791369,3.3368362731736823,5.1922156418303294,0.74595684261714457,1.6257105320272009,2.139073422583504,1.8378852479782763,2.8559580369888833,1.1168619199141958,1.1188505827130102,3.2531089442344516,5.0181286318117904,1.7940922273000286,5.1509751420324541,5.3336898437929037,4.7736472191967767,1.4621421283395153,4.5599691998477967,3.8406389391553439,0.93256975464475689,6.0652043223817973,4.0327540770868726,7.0965887540088319,2.2332606763853726,3.1411136241808881,5.7191495603249187,5.7449749362577389,3.4485800663293436,3.6393343127940141,0.079320870297566071,2.3775338119559986,4.0831734826357602,4.4763529320794966,2.46301326314601,4.5678402270675083,1.5781861172376903,2.9730342721289302,2.7578320736330633,2.1963903368484079,6.3674438244400378,2.8044378955299276,3.0821188028974444,4.0692675576242365,5.2401233821908715,0.51022771025135238,3.9792481596039133,3.1958145194862531,3.56274056233931,5.6398446975936789,2.6840988085172235,1.3879699298928962,5.9853077089599296,3.1456557075913025,2.2048124846942727,1.9618881605288498,5.7268269711327005,4.3328513920850504,0.55430219200437225,2.9046514345560444,2.1561312804486379,0.89464018876613727,3.2949498786583282,3.0508435764019963,2.7161420223939818,5.2789422951656046,4.3765464077897889,1.7100486612295476,4.5949166928572609,0.95673818105541963,-0.57078570084885127,3.8884405604442258,4.770665242585002,5.0613456623671382,-2.447759682449191,4.1849298501728915,1.9780762763823565,4.3094150713642945,3.5801961139503655,1.7459037590920046,5.970441024845428,2.4331988499430679,4.17695575832348,3.2978150436324709,5.5815259669123121,3.919218414992951,5.3335088074187862,1.5132068575069266,0.41220482822753368,2.1724581764075412,7.4320824337149949,4.1016640609570523,2.9851702608043591,4.4622347777056648,5.6874869870723916,1.0528767568878894,1.2998471759587706,-0.1784073090647027,4.5439286928830311,3.483029691923579,-0.57644035859544474,2.0786823144831867,4.9450245299457816,2.4708224016290279,2.0346665787938862,2.6023909497974653,3.5321444310804289,4.9241061394119976,3.1683648925465939,1.5560388236404799,3.7657202388210527,2.2310552288408649,3.6272742459987519,4.4800932249285239,4.9637597856340392,4.4449243488366772,1.8748983588505965,5.650022370870543,2.6698960867594992,4.0001052587132229,6.8574913023564861,5.8966288336560098,-0.39226532110450041,1.983015466403284,0.99420307494129601,1.4831509315773304,4.2845860248141667,0.28846210308147091,5.309673196205356,3.1951594014221261,-0.042781648983877485,2.8049987694116432,5.6662089894381786,2.2373593732951109,4.3354582378996263,-0.36764534953743455,3.7193581132186666,2.9741937481154173,3.3342364377293134,3.3728568017768046,2.0506273384342144,2.0715501228913009,5.8598314620718277,3.5673210750533784,2.4941424034544117,1.328449766492833,3.8266416105499914,4.9938149595837249,5.6762486026454759,4.3517613388615537,3.2628537214945688,2.162841917106828,1.9137994054370986,4.0588457466035583,4.1139250292457419,5.2055396179201452,1.8874301184457853,4.0347306382131993,3.5001981871069168,1.2515777948449882,4.3795498148922114,-0.56471670469483071,3.5003066722942662,2.5776134955405681,0.98623916418836632,4.0428403592806799,1.2838023563992982,4.0076448923700809,6.4297042923026293,5.1735414328605387,2.7627542395136602,2.8287538173670184,0.90639614263094437,4.1308143362299337,4.0947659817831008,4.4571329982215628,1.3212927178378773,5.9720737862707036,-0.85485005772559264,5.998670449681649,3.3704704152137936,0.063107839263262022,1.8387790418640644,1.4023215307123966,0.63247487131546576,3.2033283777938175,3.9700160145965615,7.0066163533561099,1.1079360053343583,6.7934589792878484,1.8492164661271904,-0.023609479952175771,6.439159816491177,2.0175054248310951,2.3544555060478953,-0.8417045575037565,0.24654066655832452,3.7501042991394895,2.8481658902502844,1.7491060012328592,7.5943866672284406,4.4452875835806021,2.2766274360300041,2.6109762707877562,1.8031108080079539,2.1343741341512317,2.1381737830937793,0.99370487845395461,4.7398439189254287,2.2510164012220031,4.719593175507689,5.4306269220618475,2.180358557785425,1.7746419076780067,2.9525281491016431,3.063193009075972,3.689401705675611,4.3744232328546477,3.9984099195579264,0.18009572414040065,4.8721426548316913,1.1773663204023566,0.44417313715336038,1.5502197553267614,2.4069534219630389,4.2263357388342442,3.258340151173277,-0.17988054455603208,3.6199246923733681,3.9081990090944032,1.5639790715630395,3.6682588351045569,2.4147672526212975,3.7749726860380446,2.7022558169600397,3.226856624126869,3.7135038066127568,4.1721535494005781,2.1778124854342531,4.8918988190236918,3.3197080073958944,4.0562527614005388,2.1141723854416314,0.53646491716169553,1.4874655087862203,2.6634768031949125,7.6685810693312568,2.3565859491721408,0.98239778397504685,3.1171800191408461,4.0434237831122148,4.0648316783339764,5.2381567948783125,-0.21857701090322434,1.0584699341128567,3.0878357146878366,3.6378658273370323,4.0453909656040219,1.6933143336253118,4.389751274184837,4.6147164247247874,4.2090419338630802,1.6075632258179422,2.6584349150049156,0.67579815183555159,3.9598390655977762,2.7124347485341209,6.352284569922479,-1.574415465297001,2.1536181435604851,1.9124712605972771,1.2370875317427881,1.6625423237258588,3.2942895560026497,6.3736044989008249,1.4165740978003203,3.5838735062570786,5.8158691794354542,5.1115957307225184,2.1095498896579432,4.8375841108402504,4.8662159253183699,2.4868880116863661,3.8098822252083737,0.55343571645443035,2.9563082039363002,0.85820593589829564,0.36851285697162073,4.1895018736399727,3.2191354749499328,1.4437543693981831,2.8947061265330167,3.9968138214106665,1.1762380393834886,1.1165972375108582,4.6822440846291524,1.7384275490478269,2.5707233123594371,2.3817802564949297,3.6367096949433764,4.2792917705825371,-0.67319109417488665,1.7922678139526182,-0.78235114759506486,4.1468390022203634,5.6038223112931949,4.9736676053560602,3.2049374130295325,1.0527140291055399,3.3396379905871578,4.3945388515844979,5.8080164296866537,0.63138665566890095,3.0564103962589875,6.3420513300359609,4.794823429408714,1.594349072352381,3.8892415762271195,4.0776638223127426,6.5158337226441034,3.9539564619189624,2.5687041217851343,3.1920889091546831,2.1329989796997149,2.6774808576874278,-0.62586803990108431,5.1861032945995555,5.1659310315111817,0.96739646406943836,4.7813383823446793,3.2283552790524155,2.8352336384245187,4.4634833207296998,2.8987886381893255,-1.4826907700354042,4.4324584380814143,5.9894137110009682,-0.72534578696589014,6.0419010261151698,3.113662588655381,2.365419000353862,0.8848810853416671,1.8457242214176151,5.1473874618884183,2.8306498497983918,-0.38142138459662789,5.0089245134928966,5.3317671487725322,3.9587948975163592,-1.4802878394071621,0.29870283746340842,3.9986138413028218,1.6343500987287343,1.7305947398179238,1.2574955769355185,2.0162502072852959,5.9963548122071568,2.6842005523259953,3.0493977033842805,4.0807402313890622,3.7421100272711167,5.1486301460433692,8.1456604804186519,2.2777056069996178,7.3958220810177373,-0.33528435671922097,4.4249544730160917,2.8988618773399972,1.9065193641842635,4.0971246996717694,3.5060437699127345,2.6592260596088448,2.6267643329329555,1.8360637339695183,1.7212401375006479,0.53266410068357883,2.829809321548852,0.93640964907956725,2.205899113582058,1.9571665256595105,1.6300609360300249,6.2872072439191165,3.2115638061727854,1.8238386847522623,0.37462491498418826,2.8169402139433291,2.5549631280768983,5.5737835319359457,2.3582897029959251,4.2165053213173946,1.6814029156165982,0.055901322389011465,3.0566528647896294,2.8325346566237819,1.0068351551488799,3.1369089910148027,2.0892561178422611,3.2022868622096321,3.7726117935271715,4.3146965025358552,8.0215008158710717,2.1226132364232608,1.7137853753808321,1.6058998139486351,3.9018714691435834,5.3412658044047472,3.6387211803462227],"z":[4.1579735269154146,2.2094541005108046,1.5100166254053966,-3.1040268172091396,1.0186234121198281,0.29093486803202795,-4.2350941852818922,2.4814662469243434,1.0292084861088855,1.5421191909193168,-3.9216262144848359,4.5614172735077156,1.205728626678507,1.6743396531496737,2.8542659078701167,2.1568388050385519,3.7817773669210402,-2.5584984252243856,-1.2823315027882733,0.94681576999656647,6.771049011003786,2.5446875040486803,-4.0541742122560676,-5.5173196566857134,2.0921441137171359,1.5974982403049949,-3.0935279767078541,3.1198000068020422,2.0551256606109041,0.73164385796466502,-3.5853419644208451,1.7438464292353797,-3.304189648513101,6.30024420625855,3.1648593377711784,2.9165906493769418,-2.5033102561965777,3.8463935929553661,-0.56741949098638789,3.1340205658347911,-1.0744832200342564,-0.051510666182611908,-3.4074947847412798,1.9913169075217128,0.46401808957378066,-1.9041848814932201,2.2142537010724603,-1.8414718340987339,5.484294190417077,2.862931166850113,-0.47262396744990975,1.7824328919020842,1.1921330668311179,3.059538606030539,1.7475135059803795,-2.1930461938586436,5.8471004612700606,3.5910167953199679,3.9870082204627644,1.6765901092763147,3.790250422696424,-0.62594854248101495,3.1767711001952823,1.1724946316125548,4.4302242253214636,4.0006212252631013,6.5996030560067789,2.0499058980961538,2.742536872527066,2.383174670619054,-1.1328321674770616,-6.4028650496010302,2.9541670185682491,4.9839831122944833,1.3712718244538009,2.0277964762383496,1.7544642768999554,2.5593557712564268,0.91286843050206068,-1.2271815250240183,3.5046656913487069,5.2915861878803332,-1.41687831577867,2.7806173503030998,4.0864852834154082,-3.7179182508998396,1.6628924609454212,-0.27492631749775187,-6.5199817039288659,4.2919854394128363,2.4515492949263082,-1.0107628222975267,-2.6956795635230186,1.2967971823127233,5.7109981097966642,-0.64860537172486077,5.4424032650794034,4.1903619076555909,3.55637925940233,-3.4223961967835859,2.42667350704202,5.635248197619843,1.3141290117556945,4.1091544682658867,-2.0083976265521946,-0.25190780048913575,2.7975430600687368,1.3090115322204294,7.9697149830886227,1.4088923787514243,1.9087555033028343,-0.88050934746440901,3.6667109289431741,0.57888674083655323,2.5881097600713612,5.1073890241808444,-1.8557838928342609,3.2515229013948699,-4.8316354077029411,3.3368730807074551,0.037119825108118287,-1.0695120151252078,2.0699779727302192,-0.26752735289503526,-0.29089735179937581,0.093824778308062129,7.4901423136277439,0.47756510742479596,4.5052632250714613,4.3348187854897571,2.2782136702949494,-0.98665253079781778,-1.4413040929544283,2.4657249579498939,1.7758394815099341,-5.0523909257068045,3.5788296032092681,2.3854199149750057,2.3772618641291112,-1.9681344834732308,-3.1186496299609425,0.057950072740192593,5.0532864618929239,7.8279375260223958,4.0993255153915484,2.1005844449710729,-1.2702759192679505,5.143537870733315,1.3978739979134234,-0.76139522478096722,-2.4422172625605323,-4.4397766997590731,1.9199368691222545,5.6306710954419348,2.1331543545968317,2.0193765967788853,6.7392807224680702,0.13011027891977511,0.28335105091060986,-1.028570363222197,0.93556057682280691,2.4163337575029189,0.069343241231600516,3.8511398232829883,1.3182604659044261,1.7915738188439245,-2.519287184686215,-0.61746295972478049,5.3865621734119999,5.3302072709617052,0.76033828917617929,2.5745320337769462,-1.4569269651924794,-6.2396724240323378,-2.9235413554828873,5.8789797274294537,2.7104563299349467,-2.1134974274382761,2.9515150985101908,-2.6257822594731293,-2.4067889642882894,-1.1982052298307151,2.2476876234389502,1.9233594887123411,1.9834668218635021,1.7954465888393669,1.557988678627493,6.3321804790333678,-3.8635200409146924,1.1715626157133712,-1.9686815320439246,-1.5772610000470766,0.11985318326097205,-3.4472616107794511,-4.5017540397844176,0.82180047441665505,7.4226102890339591,-3.2134366478276997,-0.31261661289705023,5.5318220072167623,-5.3186556030138465,0.85592699423223784,-0.84083396942527133,-0.35254850643905566,0.53876231670646302,0.87294172704202622,1.9095626672393151,-3.0314487096836213,0.78291919823032941,-2.6744941568812264,-5.1613501874187273,0.46147452540829881,-1.6681704099953727,4.2454170173017456,3.7624817998693278,-1.8863488017830958,-1.0650220236512777,-1.9931733440861166,2.8642384424039502,-0.044670316157039647,2.4843331551251349,2.8061146651161213,-0.60597134838187228,0.03451875515989844,-0.65912757084447726,-1.7460431917999237,1.6822519172369423,5.0859551889351557,3.0593559130701702,-0.22098166936585262,0.49240824860237487,3.8269568501784565,2.3805341771722643,-3.4142477479748328,-0.60288073706348944,-2.5816885215721941,2.6118384427297796,0.0059300846548037134,5.4319921998552054,6.9117079015916172,2.30340825482013,1.0793632740921262,4.8571763345743779,0.77703424887557659,-4.2374191117239306,1.0011357226504627,5.1103438162112367,-2.8373263269174593,-2.851102039103659,0.37685531759193303,2.3544070976931293,1.3367330567408358,-0.15293670764082523,7.2338304927811521,-6.1497255270349438,-1.3974329115203252,-8.2606890553288252,2.9110513298521798,-0.42650895874334949,-1.3213748509366665,2.2544415502567863,-1.2596202201621467,-3.4772795851802405,-0.035808890995184361,5.2214115384834496,11.80907380747909,-0.87371431799080224,0.83666610189068702,-2.8582502963362151,0.73440346508537702,-0.28361274488350885,-2.3294859525990002,1.2098436067241927,-0.53129247359133203,0.63753063711793589,4.2303916558675798,-2.9161529320797457,-2.3613441805163848,-1.8038964136200888,-2.7679049310634172,-0.53635198024294906,-2.4023780676729585,6.679120974273312,-1.4432113816272509,4.3688553594446393,2.7081784175293713,-1.3308674675280083,-3.4296381923863803,2.9055208777878558,0.19139847411457023,4.5961180098413426,1.279854800266947,-0.6558530932750628,-1.8639338087074688,0.54223460252294509,4.2004386116865309,3.2243187514215852,4.1058565751877936,2.7779675037553089,8.7221075757036353,0.71424024023053301,2.3098269379884524,-1.1037567219256696,6.4436530228195004,-2.206623805940727,-0.55437668411525109,1.4737355526719911,3.5296114177470157,-2.1410955926884472,5.3376293488680089,-1.2173807606841902,5.2699901047461362,4.4614593615569014,-0.6008796657613924,-1.6435612223392124,5.0082781358746971,-0.086561549718095598,4.5421407507304199,-1.9285842177241546,1.2976986305378166,4.1652157829658139,-5.317931886167357,-1.2621031802376828,-3.579492406593344,-1.949415032941634,2.7586333984874907,-3.9985225935870243,0.032108049501776925,-2.755940686293032,-5.7299130004863867,-1.513925931503882,-1.3097347318522305,-6.3899764118191538,2.8541623071990925,-2.095958579552005,-2.930348585684051,2.1494676440422857,1.9122221503442147,-5.8282953409018248,-1.9887780142895148,-0.9829418084532624,0.060422484914437913,0.50044762551636901,-1.1458980045707428,0.44543668865428043,-0.68484492509421413,2.6342917459232469,3.9298532036795222,2.7457454860920421,-4.2215979565294282,2.9179654699305346,1.4514921750752801,2.7827959477470103,4.1082798272931687,-4.8525430161601459,2.565787399512637,-0.25689299899747819,0.73183383360285859,5.3922772167814754,0.87592603006941172,6.3564477497901706,2.8036017851956161,0.23242842367602068,1.8632560483252263,-1.2232519020478358,-1.7355598057532839,0.80486280557991985,0.68017029928294681,2.1824671914972837,-0.59513264428063795,0.85031041460641565,2.7423546162714416,0.35784566506097937,0.85638829668808814,1.9979957960733328,2.7266643013573431,5.271015657142379,1.334474648260813,-0.64161467742922462,-0.67457638983585944,-0.99962475600571166,4.7112081345554735,4.0443032453389796,1.9666575698451501,0.51169424668839958,-0.44672997281815885,0.25395945117392682,3.0210108330854766,-4.8041514747485632,1.8298799632043761,5.0980099496094278,5.5787030148786165,2.1545391742909641,1.0727998500729574,0.60576689118927618,5.0814398385374906,2.0602445513400123,1.3405112190999358,4.8257504441078503,-3.1649574002981193,-3.1234935064956764,3.0926725126660326,-1.417575308765588,3.419502105999741,1.4381341990492134,2.611892206877215,-2.2464750942258109,0.85022174305236664,1.7523460394064934,2.0272065381959661,2.8831967785739301,0.86240306918012366,-1.3523882741723487,-2.1741902232847297,3.3722385386544556,-1.5049363573331518,1.6870616006137014,2.5832815100202451,-0.99176428267324046,0.21477531403549066,-4.1164833388464084,-3.0166288462192012,-1.0375325422840502,4.3343153373094943,9.0099278051121061,5.3690183600266153,-2.1012459656211671,-2.5523985425412108,10.224681875559776,-4.6951383102177209,-0.060384507091764572,-0.11347978672266645,2.2092637280714218,0.52119506265544935,1.8081922368081758,5.4177065406844473,5.2468492348985762,1.6749589683595438,0.26642878512897894,2.1513365652827847,1.8428922823772731,-0.69433869347947574,3.0767445420285724,3.9313477681258933,-2.1514984164176396,0.63968439456949699,1.9731417189669678,8.4675294616011278,1.8604232483934562,1.5532773020031949,-0.19946438425731139,4.1406741461216292,7.5319974107579961,-0.11567222196524973,7.6123329282186702,-0.35545870781144395,-3.5316887577050338,-1.1704062027180586,-1.7285394990042326,-1.9681659451985003,-1.3072634120885716,-1.9091857678428137,-2.1497498475259684,2.187750582015819,5.2199119158355458,1.5527377029834275,0.85658197889958654,1.7350748626516699,4.3415145328683415,-4.5057642779748317,8.329869417469002,5.4847114230599177,1.6082760063703496,2.838953563721637,1.9504788380719227,3.4663811274460858,-2.64923758660108,1.50480389552535,6.4572095872152628,4.3144747179143916,2.1356811565844755,-0.80597164614181804,1.0470945895387813,0.31389986921262336,3.3712045064109351,4.1039849422348773,-1.1268884190424728,1.5442766357537496,3.4338088516466456,0.07040265490302966,-2.3596942182692029,1.2709642884411632,4.1388906733685875,4.1606720043268925,1.1227992568181235,2.3491009881361773,-0.20604459190390667,8.1879081713573125,-4.841231146009993,1.8116673034760742,9.0209377297646434,-2.3009154759145121,5.8797591955578898,6.5553776844265865,-0.1172805456588859,0.68474174097165164,3.8420970765302069,1.3611582625112488,-5.5506154643883816,5.6535959729755474,-0.2774832311788995,0.33238294875600605,1.0238398430365627,-0.45173742222299085,2.9323144584993779,1.952544932930766,1.4254012786993349,-3.0035442072362351,-8.5588798645200637,2.453984417827265,-0.49370405681123564,0.55039624945185928,2.7044016421224493,-3.7015990014788347,3.9192928588833453,-0.80167764647960249,-4.4571029598880623,-1.5506921513810594,-3.648634576129993,4.3311416693032125,-0.36532638988408683,4.4455723690377607,0.88892932726495755,0.78962062530246291,-3.2435997429428856,-1.802713307014812,-1.5374734513458415,3.4540644606606339,4.4675274677015171,2.8078931800147835,2.1605308114409314,1.4264436111380414,-5.1265292118954786,2.0327252120552366,3.6126167569789254,4.253806848923098,-0.21050994501272924,4.5801439300101663,0.76965414227071405,-3.282453785582339,-1.2804899636509406,2.6612944660203994,-1.1306166187965165,-1.8722073723820167,-4.4870592004415597,4.4399472305790457,-2.6536637752159939,4.5736662002471862,-0.16641550970395613,3.0317355922754059,-0.41831329382407345,3.8505170882459341,0.71471014431563118,2.3776311573448545,1.3825641654933276,-1.1798614214229795,0.55589549244456271,-1.347018993972608,1.7119878843038414,0.54207944244858708,-4.0706782472851364,1.9853008679391047,-3.0433275719137525,4.6242631204030955,0.56893068989888107,0.52844551260195238,5.3477094361962747,2.0697854258747572,0.42828784662090225,-0.70700191457627337,-0.30137288730316514,1.3628670962699787,-1.9917623445771055,5.4501554238063221,-1.9867755939859681,1.8423019706421759,0.12748883898172281,-0.37910763348033738,0.40417222287339816,-1.4076447492723347,1.0803547718356639,2.3599910391774492,0.45531364864329682,2.6085701817648417,-0.63724169192292446,-0.072056703905967634,-1.0112828392187794,-5.7809087259031848,3.2875895685832366,1.3973490582572683,-1.7163250855156109,2.980468370605422,0.79547844600056328,2.9282178904806591,-1.4604714327645469,6.5563845217888908,-1.2982706781442488,3.9327674541525224,2.6741362018497448,2.0972368317983685,-1.8939067397085481,-1.6019611949308792,-0.3786032755567541,1.7643944617666008,7.1249805975776468,4.1679528467747389,5.6742279759652412,4.3403120248381164,5.3226099763863859,5.0289168850447012,2.380991562029406,0.7518501274136109,-1.5052855603329394,0.6595978705894574,-0.16536842054108303,4.5659553176693475,0.9043882888584458,2.1369640325264392,2.6560873847821629,-0.532417856195329,-2.2575127363264409,-0.39377566454299751,-7.3387134752920868,4.3887680350949818,1.4842704954056281,-0.10081697036005299,2.7408663738518242,1.0805634157498283,-1.76992870154093,-2.516041052367985,2.4409792550234259,-2.6816216576100937,3.1710443843686709,-2.7800347464113107,-0.0080701838739010157,2.6702497233167684,3.1229511414794331,-2.9632599708379441,-4.0632974448542143,4.0709480218188947,3.5852504095853472,-1.123006891533167,-5.8437134599729799,-4.3657742849172942,-0.15951665201353094,0.42466410031702817,-0.95565229687120379,-0.16465641595977409,2.1844346522132456,1.3256030871372237,1.4902099204565185,0.87242859342859636,-1.0118170111097258,-6.1691060111441027,2.2282118487639853,0.54973181485230449,2.5117903938338255,-5.153658145808496,-0.71341990988476534,2.6649554333524179,-2.2389556734636917,4.0295237918711413,-0.65566054948722874,2.2294423030770685,-2.4572745403360359,-0.46777421112765083,1.6169432977703042,-1.7970750257881432,-0.40008430160423436,3.9005615033626122,1.9449275815343445,-1.8578921657135861,1.3177892631405694,4.9999370241161305,-0.83048496008442019,2.2107476004683626,2.2479136217913496,4.2620049536806714,-3.4172678376137098,-1.5137138699423138,-3.5143985162398454,7.769786550789882,1.2454586490395843,-2.1883272369096756,0.90372628818217182,5.2730264368862647,1.3944517559160492,-0.39595891784627657,2.9529216682653114,4.3658037829368741,0.75256204508353552,3.2132416795572638,2.2431054153845866,0.4128393983501577,3.1723620195343285,4.0097564448217842,1.7176120448752235,2.5096130665776393,4.9792142153189207,1.7369266384519917,5.8699898949084774,1.91151223995326,1.5378246274428409,4.1214668466251707,6.2703028891883088,-3.3039027972528832,-1.6379088687614489,-2.0986938619476256,-2.0167974282580321,3.6956560891272585,-1.1073833811134239,0.36494341338927272,3.8075168120058724,2.9091439975239597,-0.29102849769296646,5.7882877893976925,-1.3119724617633737,0.36736771461161066,-2.0551624695831738,1.1246442076437746,2.6422271868528426,0.3192189697622464,-3.3381450509303088,1.0889627364935106,-3.2327741342672303,4.3890669841602659,-1.5323512017002749,1.923301488362569,0.76834246583087995,-0.56631658252736972,3.9594798125758004,6.6748206737012268,-0.055974607033061119,1.9229651661077711,0.86170739778043315,-0.13479717821589321,8.1935491709852979,4.3088763795540634,6.2250684967281398,0.69824648800594957,4.1988653969152825,3.4412818306043218,-1.0769515217403005,1.8611214670301806,-0.91990512336116148,-0.075389866342423062,0.44927758582744293,-3.1230208377077551,1.1337236538313735,-1.0323237041867124,1.563428774753705,2.6385769571209767,0.44054495775516145,1.8129098453714239,-0.4389223978946859,0.65518464222906281,2.3614410841521671,2.4293400610497597,2.4291829379471448,2.2958713699026263,5.3694725158999264,-3.1152372525557723,2.8624537785565005,0.63001605107151848,-4.285671023550754,-0.45075830369464165,0.084331828972559197,-5.8306893169977183,-0.86325982070831531,2.4586198509806687,6.6262662277117075,-2.3812581244949165,9.0660292518087005,-4.7495500528333645,-2.2057653951378278,3.4279267079813041,-3.6179686654264653,-0.12261611259287442,-0.45807865307378215,-3.3487118508420224,3.0882973474203168,5.1198186771055001,-4.7521390210544459,6.2230278259873399,2.6421343499887397,-2.7328134943128606,-2.0508281621781244,1.5066428044731413,-0.42037184898463931,1.2985845196282515,-0.25793238849118572,0.7135476291459677,1.2908613267810871,5.6536607595703465,5.5401001723122816,2.72130451250047,0.72594972197469354,1.5434508731743302,1.6590302430243389,3.1860579191989453,0.16427975262509198,1.7213734076742406,3.9684903522762069,-2.3631057050717925,1.851974838836727,-5.2823877318794858,1.3806827703419431,2.4049041072252568,9.9620491253307879,2.4599483543924605,-4.5471131218665368,1.0923543679603105,-0.031794818333332975,-2.8442732908361954,4.9766038198957752,1.5173108253818817,-1.3246011879688586,1.8330489202065388,1.1923364234846396,3.9326742703657125,1.200561443116281,-0.39154458104858803,1.8428117080719169,-2.7784935432786995,3.5492088886602735,-0.14928619022450684,-3.5704924385538543,-1.0462429193271241,3.9351793570566955,6.6793167003791325,2.0614650860339432,-3.5282371408773923,0.42023084642931263,0.47208731469738563,5.6384379439292935,3.7605805587952186,-1.8905128999032277,-3.2328659596803178,-3.5551198814318434,2.1836085151558757,6.2913704723323951,-0.91084678345866465,2.2760256918156099,-0.16152620283732411,1.3613484488118008,-0.46275791001928468,-0.44877991085403846,0.46707264874759791,-2.073630555966874,2.0019086349525805,4.130264241017545,-5.2303052157948056,-2.3171727826472797,3.0800112785000366,-2.5552157317234796,-1.857451620645679,8.4902108865425454,2.9936833646704724,0.27535269233560167,0.16598122328103992,3.2537245195051852,-0.018932952576274698,2.1306847888625109,1.4233754049131027,3.4566831191675509,0.027069859734486723,3.5753980140127277,-1.3813675817181288,-1.8640633244208589,-0.66376425708776399,3.1057562777793186,0.78132410439217026,2.4853927968598022,-4.3702922767720498,0.86979624307122094,2.921713063170114,0.86752432772275578,-1.6609056104159556,-0.023731999404169368,0.12825585153646302,2.7019623989861259,-2.3566363842017699,0.99987873298706009,5.0766216689259682,-1.9890355926178902,-2.5199270770194713,0.11896877494914082,1.7540754944573047,3.2172696024483418,1.7750851888752004,-0.48026933663999727,-0.59807473020012858,7.1855626550657155,-0.43687872940374239,3.5291329785228704,-1.447829960113856,-0.0024380229947802423,6.5012731323674311,3.1265009722738331,0.14940511233025966,1.0312003630712923,0.82903666477641702,1.3500315566961607,2.0977163464827799,-5.6080765651589557,5.6702840445991178,3.5361197047696074,-2.3376333460585896,-8.6124119732633471,3.2433748719907567,4.7653711844344127,1.542725846343163,5.0059052888789815,2.3718410947333259,0.73331249685503574,-2.9943621452796094,0.19363297502705457,-0.78869233657094706,1.4739754128194718,4.6420227631254498,-3.6559300035647988,6.2738555911854652,-3.1942597526981276,-2.3324335498266691,-2.7113877284507812,-2.8545515987788073,4.2114924835275502,0.19995804851097654,-2.7782206261529909,-3.3928328309883407,0.79949108180599615,2.2050143328047973,-6.6797149079409444,-4.5232602017910679,-0.97747265533149319,-3.0284874939186919,-1.4467134137995514,-2.6775827814980944,-2.3051928488260822,2.1617855176045393,1.1225876892437707,0.33030898792036223,0.71093418168381528,1.3168442766017157,2.758288571312379,11.245035794544382,-0.05355264772663304,8.5966898775282914,-3.5940861505902069,7.1964047097287844,-1.3181289972363568,3.1794917974239048,-1.7702264018754419,2.9207217618514907,4.5696976040437827,-2.4742041484947714,2.8223868700330126,-4.2599679926431397,-1.982650535580345,-0.51662597533143551,1.1730250288889537,-1.8516126557832737,1.0796430416973939,-2.9461059605916087,1.5592764984036349,1.4324326830815997,2.746053485292463,0.12429830730383795,7.9487973370920235,1.1286406083599272,4.2873065677452873,1.6661106278312952,4.8347550658008531,-1.6160150723163267,-0.69385155720064629,-0.7298878158862665,-5.0361656017056138,-0.88924181870078445,3.2725634723241597,-3.8060422737779742,0.60285011863718663,1.1195304304154978,1.0628277623832889,6.2787896164226096,1.896884937388355,2.4524695854067762,0.41096281051258832,5.2894339507655168,4.2734482519585306,5.2887159340624761],"type":"scatter3d","mode":"markers","marker":{"color":"#444","size":3,"opacity":0.5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observed (Total)","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"y","ticklen":2},"colorscale":[[0,"red"],[1,"red"]],"showscale":false,"z":[[-7.3813696184952322,-6.9528670828384112,-6.5243645471815892,-6.0958620115247673,-5.6673594758679453,-5.2388569402111234,-4.8103544045543014,-4.3818518688974795,-3.9533493332406584,-3.5248467975838369,-3.0963442619270154,-2.667841726270193,-2.2393391906133715,-1.81083665495655,-1.3823341192997276,-0.95383158364290654,-0.5253290479860846,-0.096826512329262204,0.33167602332755841,0.76017855898438125],[-6.8838325191426568,-6.4553299834858358,-6.0268274478290138,-5.5983249121721919,-5.1698223765153699,-4.741319840858548,-4.312817305201726,-3.8843147695449045,-3.455812233888083,-3.0273096982312615,-2.59880716257444,-2.1703046269176176,-1.7418020912607961,-1.3132995556039746,-0.88479701994715221,-0.45629448429033115,-0.0277919486335092,0.40071058702331319,0.82921312268013381,1.2577156583369566],[-6.3862954197900814,-5.9577928841332604,-5.5292903484764384,-5.1007878128196165,-4.6722852771627945,-4.2437827415059726,-3.8152802058491511,-3.3867776701923291,-2.9582751345355076,-2.5297725988786861,-2.1012700632218646,-1.6727675275650422,-1.2442649919082207,-0.8157624562513992,-0.38725992059457681,0.041242615062244248,0.4697451507190662,0.89824768637588859,1.3267502220327092,1.755252757689532],[-5.8887583204375051,-5.4602557847806841,-5.031753249123863,-4.6032507134670402,-4.1747481778102191,-3.7462456421533972,-3.3177431064965752,-2.8892405708397533,-2.4607380351829318,-2.0322354995261103,-1.6037329638692888,-1.1752304282124664,-0.74672789255564487,-0.31822535689882336,0.11027717875799903,0.53877971441482009,0.96728225007164204,1.3957847857284644,1.824287321385285,2.2527898570421079],[-5.3912212210849297,-4.9627186854281078,-4.5342161497712867,-4.1057136141144639,-3.6772110784576428,-3.2487085428008213,-2.8202060071439994,-2.3917034714871774,-1.9632009358303559,-1.5346984001735344,-1.1061958645167129,-0.67769332885989053,-0.24919079320306903,0.17931174245375248,0.60781427811057487,1.0363168137673959,1.4648193494242179,1.8933218850810403,2.3218244207378609,2.7503269563946837],[-4.8936841217323543,-4.4651815860755324,-4.0366790504187104,-3.6081765147618885,-3.179673979105067,-2.7511714434482455,-2.3226689077914235,-1.8941663721346016,-1.4656638364777801,-1.0371613008209586,-0.60865876516413697,-0.18015622950731469,0.24834630614950681,0.67684884180632832,1.1053513774631507,1.5338539131199718,1.9623564487767937,2.3908589844336161,2.8193615200904367,3.2478640557472596],[-4.3961470223797781,-3.967644486722957,-3.5391419510661351,-3.1106394154093131,-2.6821368797524912,-2.2536343440956701,-1.8251318084388479,-1.396629272782026,-0.96812673712520447,-0.53962420146838297,-0.11112166581156135,0.31738086984526093,0.74588340550208243,1.1743859411589039,1.6028884768157263,2.0313910124725476,2.4598935481293696,2.8883960837861915,3.3168986194430126,3.7454011550998354],[-3.8986099230272027,-3.4701073873703812,-3.0416048517135597,-2.6131023160567373,-2.1845997803999158,-1.7560972447430943,-1.3275947090862723,-0.89909217342945047,-0.47058963777262897,-0.042087102115807407,0.38641543354101415,0.81491796919783643,1.243420504854658,1.6719230405114796,2.1004255761683019,2.528928111825123,2.957430647481945,3.3859331831387673,3.814435718795588,4.2429382544524108],[-3.4010728236746273,-2.9725702880178053,-2.5440677523609843,-2.1155652167041614,-1.6870626810473401,-1.2585601453905186,-0.83005760973369669,-0.40155507407687474,0.026947461579946763,0.45544999723676832,0.88395253289358988,1.3124550685504122,1.7409576042072337,2.1694601398640554,2.5979626755208773,3.0264652111776984,3.4549677468345203,3.8834702824913432,4.3119728181481634,4.7404753538049862],[-2.9035357243220514,-2.4750331886652299,-2.0465306530084084,-1.618028117351586,-1.1895255816947645,-0.76102304603794302,-0.33252051038112107,0.095982025275700877,0.52448456093252238,0.95298709658934389,1.3814896322461654,1.8099921679029878,2.2384947035598093,2.6669972392166308,3.0954997748734532,3.5240023105302742,3.9525048461870962,4.3810073818439186,4.8095099175007388,5.2380124531575625],[-2.4059986249694756,-1.9774960893126539,-1.5489935536558324,-1.12049101799901,-0.69198848234218846,-0.26348594668536696,0.16501658897145499,0.59351912462827694,1.0220216602850984,1.4505241959419199,1.8790267315987417,2.3075292672555641,2.7360318029123851,3.1645343385692071,3.593036874226029,4.0215394098828501,4.450041945539672,4.8785444811964949,5.307047016853315,5.7355495525101379],[-1.9084615256169002,-1.4799589899600787,-1.0514564543032572,-0.62295391864643479,-0.19445138298961329,0.23405115266720822,0.66255368832403017,1.0910562239808521,1.5195587596376736,1.9480612952944951,2.3765638309513166,2.805066366608139,3.2335689022649605,3.662071437921782,4.0905739735786044,4.5190765092354255,4.9475790448922474,5.3760815805490694,5.8045841162058904,6.2330866518627133],[-1.4109244262643239,-0.9824218906075024,-0.5539193549506809,-0.12541681929385851,0.303085716362963,0.7315882520197845,1.1600907876766064,1.5885933233334284,2.0170958589902499,2.4455983946470714,2.8741009303038929,3.3026034659607153,3.7311060016175368,4.1596085372743588,4.5881110729311807,5.0166136085880018,5.4451161442448237,5.8736186799016465,6.3021212155584667,6.7306237512152896],[-0.91338732691174895,-0.48488479125492745,-0.056382255598105946,0.37212028005871645,0.80062281571553795,1.2291253513723595,1.6576278870291814,2.0861304226860033,2.5146329583428249,2.9431354939996464,3.3716380296564679,3.8001405653132903,4.2286431009701122,4.6571456366269333,5.0856481722837561,5.5141507079405763,5.9426532435973982,6.3711557792542211,6.7996583149110421,7.2281608505678641],[-0.41585022755917223,0.012652308097649279,0.44115484375447078,0.86965737941129317,1.2981599150681147,1.7266624507249362,2.1551649863817581,2.5836675220385801,3.0121700576954016,3.4406725933522231,3.8691751290090446,4.2976776646658674,4.7261802003226885,5.1546827359795095,5.5831852716363324,6.0116878072931534,6.4401903429499754,6.8686928786067973,7.2971954142636184,7.7256979499204412],[0.081686871793401394,0.5101894074502229,0.9386919431070444,1.3671944787638668,1.7956970144206883,2.2241995500775098,2.6527020857343317,3.0812046213911537,3.5097071570479752,3.9382096927047967,4.3667122283616182,4.795214764018441,5.2237172996752621,5.6522198353320832,6.080722370988906,6.5092249066457271,6.937727442302549,7.366229977959371,7.794732513616192,8.2232350492730149],[0.57922397114597768,1.0077265068027992,1.4362290424596207,1.8647315781164431,2.2932341137732646,2.7217366494300861,3.150239185086908,3.57874172074373,4.0072442564005515,4.4357467920573734,4.8642493277141945,5.2927518633710164,5.7212543990278384,6.1497569346846603,6.5782594703414823,7.0067620059983033,7.4352645416551253,7.8637670773119481,8.2922696129687683,8.720772148625592],[1.076761070498554,1.5052636061553755,1.933766141812197,2.3622686774690194,2.7907712131258409,3.2192737487826624,3.6477762844394843,4.0762788200963058,4.5047813557531278,4.9332838914099497,5.3617864270667708,5.7902889627235936,6.2187914983804147,6.6472940340372357,7.0757965696940586,7.5042991053508796,7.9328016410077016,8.3613041766645235,8.7898067123213437,9.2183092479781674],[1.5742981698511302,2.0028007055079518,2.4313032411647733,2.8598057768215956,3.2883083124784172,3.7168108481352387,4.145313383792061,4.573815919448883,5.0023184551057041,5.430820990762526,5.8593235264193471,6.287826062076169,6.716328597732991,7.1448311333898129,7.5733336690466349,8.001836204703455,8.430338740360277,8.8588412760171007,9.2873438116739209,9.7158463473307428],[2.0718352692037056,2.5003378048605271,2.9288403405173487,3.357342876174171,3.7858454118309925,4.2143479474878145,4.6428504831446364,5.0713530188014584,5.4998555544582794,5.9283580901151014,6.3568606257719225,6.7853631614287444,7.2138656970855664,7.6423682327423883,8.0708707683992102,8.4993733040560322,8.9278758397128541,9.3563783753696761,9.7848809110264963,10.21338344668332]],"type":"surface","x":[-1.8246779689413803,-1.3893330070078767,-0.95398804507437285,-0.51864308314086904,-0.08329812120736535,0.35204684072613857,0.78739180265964226,1.222736764593146,1.6580817265266496,2.0934266884601533,2.5287716503936575,2.9641166123271607,3.3994615742606649,3.8348065361941681,4.2701514981276727,4.7054964600611751,5.1408414219946792,5.5761863839281833,6.0115313458616875,6.4468763077951907],"y":[-2.6144327155871174,-2.0145291656675672,-1.4146256157480166,-0.81472206582846618,-0.21481851590891576,0.38508503401063443,0.98498858393018507,1.5848921338497357,2.1847956837692859,2.7846992336888361,3.3846027836083863,3.9845063335279374,4.5844098834474876,5.1843134333670378,5.7842169832865888,6.3841205332061381,6.9840240831256892,7.5839276330452403,8.1838311829647896,8.7837347328843407],"opacity":0.59999999999999998,"name":"Signal (m(x))","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plotly-plane-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Interactive 3D Plot: Signal Plane vs Noise
</figcaption>
</figure>
</div>
</div>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="distribution-of-quadratic-forms" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Distribution of Quadratic Forms</h1>
<p>This chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.</p>
<section id="quadratic-forms" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="quadratic-forms"><span class="header-section-number">5.1</span> Quadratic Forms</h2>
<p>A quadratic form is a polynomial with terms all of degree two.</p>
<div id="def-quadratic-form-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 41 (Quadratic Form)</strong></span> Let <span class="math inline">\(y = (y_1, \dots, y_n)'\)</span> be a random vector and <span class="math inline">\(A\)</span> be a symmetric <span class="math inline">\(n \times n\)</span> matrix. The scalar quantity <span class="math inline">\(y'Ay\)</span> is called a <strong>quadratic form</strong> in <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[
y'Ay = \sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j
\]</span></p>
</div>
<p><strong>Examples:</strong></p>
<ul>
<li><strong>Squared Norm:</strong> If <span class="math inline">\(A = I_n\)</span>, then <span class="math inline">\(y'I_n y = y'y = \sum y_i^2 = ||y||^2\)</span>.</li>
<li><strong>Weighted Sum of Squares:</strong> If <span class="math inline">\(A\)</span> is diagonal with elements <span class="math inline">\(\lambda_i\)</span>, then <span class="math inline">\(y'Ay = \sum \lambda_i y_i^2\)</span>.</li>
<li><strong>Projection Sum of Squares:</strong> If <span class="math inline">\(P\)</span> is a projection matrix, <span class="math inline">\(||Py||^2 = (Py)'(Py) = y'P'Py = y'Py\)</span> (since <span class="math inline">\(P\)</span> is symmetric and idempotent).</li>
</ul>
</section>
<section id="mean-of-quadratic-forms" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="mean-of-quadratic-forms"><span class="header-section-number">5.2</span> Mean of Quadratic Forms</h2>
<p>We can find the expected value of a quadratic form without assuming normality.</p>
<div id="lem-simple-qf" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 (Mean of Simplified Quadratic Form)</strong></span> If <span class="math inline">\(y\)</span> is a random vector with mean <span class="math inline">\(E(y) = \mu\)</span> and covariance matrix <span class="math inline">\(\text{Var}(y) = I_n\)</span>, then: <span class="math display">\[
E(y'y) = \text{tr}(I_n) + \mu'\mu = n + \mu'\mu
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us decompose <span class="math inline">\(y\)</span> into its mean and a stochastic component: <span class="math inline">\(y = \mu + z\)</span>, where <span class="math inline">\(E(z) = 0\)</span> and <span class="math inline">\(\text{Var}(z) = E(zz') = I_n\)</span>. Substituting this into the quadratic form: <span class="math display">\[
\begin{aligned}
y'y &amp;= (\mu + z)'(\mu + z) \\
&amp;= \mu'\mu + \mu'z + z'\mu + z'z \\
&amp;= \mu'\mu + 2\mu'z + z'z
\end{aligned}
\]</span> Taking the expectation: <span class="math display">\[
\begin{aligned}
E(y'y) &amp;= \mu'\mu + 2\mu'E(z) + E(z'z) \\
&amp;= \mu'\mu + 0 + E\left(\sum_{i=1}^n z_i^2\right)
\end{aligned}
\]</span> Since <span class="math inline">\(\text{Var}(z_i) = E(z_i^2) - (E(z_i))^2 = 1 - 0 = 1\)</span>, we have <span class="math inline">\(E(\sum z_i^2) = \sum 1 = n\)</span>. Thus, <span class="math inline">\(E(y'y) = n + \mu'\mu\)</span>.</p>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Setup Data &amp; Parameters ---</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>sigma_val <span class="ot">&lt;-</span> <span class="dv">1</span>          </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">2</span>) <span class="sc">*</span> sigma_val<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>mu_orig <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>)      <span class="co"># Original Mean</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>y_orig  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">5</span>)      <span class="co"># Updated Point y</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 100 Points from N(mu, I)</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>data_orig <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n, mu_orig, Sigma)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Define Rotation Angles</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>angles <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">70</span>, <span class="dv">180</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. Process Data for Each Angle ---</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>points_list <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>vectors_list <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (deg <span class="cf">in</span> angles) {</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> deg <span class="sc">*</span> pi <span class="sc">/</span> <span class="dv">180</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>  rot_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fu">cos</span>(theta), <span class="sc">-</span><span class="fu">sin</span>(theta), </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">sin</span>(theta),  <span class="fu">cos</span>(theta)), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># A. Rotate Points</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>  data_rot <span class="ot">&lt;-</span> data_orig <span class="sc">%*%</span> <span class="fu">t</span>(rot_mat)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  df_pts <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> data_rot[,<span class="dv">1</span>], <span class="at">y =</span> data_rot[,<span class="dv">2</span>])</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>  df_pts<span class="sc">$</span>Angle <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">paste0</span>(deg, <span class="st">"°"</span>), <span class="at">levels =</span> <span class="fu">paste0</span>(angles, <span class="st">"°"</span>))</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>  points_list[[<span class="fu">length</span>(points_list) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> df_pts</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># B. Rotate Vectors (mu and y)</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>  mu_rot <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(rot_mat <span class="sc">%*%</span> mu_orig)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>  y_rot  <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(rot_mat <span class="sc">%*%</span> y_orig)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>  df_vec <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="at">Angle =</span> <span class="fu">factor</span>(<span class="fu">paste0</span>(deg, <span class="st">"°"</span>), <span class="at">levels =</span> <span class="fu">paste0</span>(angles, <span class="st">"°"</span>)),</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu_x =</span> mu_rot[<span class="dv">1</span>], <span class="at">mu_y =</span> mu_rot[<span class="dv">2</span>],</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="at">y_x  =</span> y_rot[<span class="dv">1</span>],  <span class="at">y_y  =</span> y_rot[<span class="dv">2</span>]</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>  vectors_list[[<span class="fu">length</span>(vectors_list) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> df_vec</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>all_points  <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, points_list)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>all_vectors <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, vectors_list)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Create Circle Data ---</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Radius Is the Length of Mu</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>radius_mu <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(mu_orig<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>circle_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>  <span class="at">x0 =</span> <span class="dv">0</span>, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">r =</span> radius_mu</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Generate the Plot ---</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1. Circle through the mu's (Centered at 0,0)</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>  ggforce<span class="sc">::</span><span class="fu">geom_circle</span>(<span class="fu">aes</span>(<span class="at">x0 =</span> <span class="dv">0</span>, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">r =</span> radius_mu), </span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>                       <span class="at">color =</span> <span class="st">"gray50"</span>, <span class="at">linetype =</span> <span class="st">"dotted"</span>, <span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2. Points (Data Cloud)</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> all_points, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> Angle), </span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 3. Vector mu (Origin -&gt; mu)</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">data =</span> all_vectors, </span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>               <span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> mu_x, <span class="at">yend =</span> mu_y, <span class="at">color =</span> Angle),</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>               <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.2</span>, <span class="st">"cm"</span>)), <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 4. Vector y (Origin -&gt; y)</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">data =</span> all_vectors, </span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>               <span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> y_x, <span class="at">yend =</span> y_y, <span class="at">color =</span> Angle),</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>               <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.2</span>, <span class="st">"cm"</span>)), <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 5. Vector y - mu (mu -&gt; y)</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">data =</span> all_vectors, </span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>               <span class="fu">aes</span>(<span class="at">x =</span> mu_x, <span class="at">y =</span> mu_y, <span class="at">xend =</span> y_x, <span class="at">yend =</span> y_y, <span class="at">color =</span> Angle),</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>               <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.15</span>, <span class="st">"cm"</span>)), </span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>               <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 6. Labels for mu, y, and y-mu</span></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> all_vectors, <span class="fu">aes</span>(<span class="at">x =</span> mu_x, <span class="at">y =</span> mu_y, <span class="at">label =</span> <span class="fu">expression</span>(mu), <span class="at">color =</span> Angle),</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>            <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> all_vectors, <span class="fu">aes</span>(<span class="at">x =</span> y_x, <span class="at">y =</span> y_y, <span class="at">label =</span> <span class="st">"y"</span>, <span class="at">color =</span> Angle),</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>            <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">hjust =</span> <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">fontface =</span> <span class="st">"italic"</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Label for y - mu (placed at midpoint)</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> all_vectors, <span class="fu">aes</span>(<span class="at">x =</span> (mu_x <span class="sc">+</span> y_x)<span class="sc">/</span><span class="dv">2</span>, <span class="at">y =</span> (mu_y <span class="sc">+</span> y_y)<span class="sc">/</span><span class="dv">2</span>, </span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">label =</span> <span class="fu">expression</span>(y <span class="sc">-</span> mu), <span class="at">color =</span> Angle),</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>            <span class="at">parse =</span> <span class="cn">TRUE</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">vjust =</span> <span class="fl">1.5</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 7. Origin Marker</span></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">y=</span><span class="dv">0</span>), <span class="at">color=</span><span class="st">"black"</span>, <span class="at">size=</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Formatting</span></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>() <span class="sc">+</span></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Rotations of Normal Cloud"</span>,</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"x"</span>, <span class="at">y =</span> <span class="st">"y"</span>) <span class="sc">+</span></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-qf-norm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qf-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-qf-norm-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qf-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Illustration of the Mean and Distribution of Quadratic Forms
</figcaption>
</figure>
</div>
</div>
</div>
<div id="thm-mean-qf" class="theorem">
<p><span class="theorem-title"><strong>Theorem 36 (Mean of Quadratic Form)</strong></span> If <span class="math inline">\(y\)</span> is a random vector with mean <span class="math inline">\(E(y) = \mu\)</span> and covariance matrix <span class="math inline">\(\text{Var}(y) = \Sigma\)</span>, and <span class="math inline">\(A\)</span> is a symmetric matrix of constants, then:</p>
<p><span class="math display">\[
E(y'Ay) = \text{tr}(A\Sigma) + \mu'A\mu
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We present three methods to derive the expectation of the quadratic form.</p>
<p><strong>Method 1: Using the Trace Trick</strong></p>
<p>Using the fact that a scalar is equal to its own trace (<span class="math inline">\(\text{tr}(c) = c\)</span>) and the linearity of expectation: <span class="math display">\[
\begin{aligned}
E(y'Ay) &amp;= E[\text{tr}(y'Ay)] \\
&amp;= E[\text{tr}(Ayy')] \quad \text{(cyclic property of trace)} \\
&amp;= \text{tr}(A E[yy']) \quad \text{(linearity of expectation)}
\end{aligned}
\]</span> Recall that the covariance matrix is defined as <span class="math inline">\(\Sigma = E[(y-\mu)(y-\mu)'] = E(yy') - \mu\mu'\)</span>. Rearranging this gives the second moment: <span class="math inline">\(E(yy') = \Sigma + \mu\mu'\)</span>. Substituting this back: <span class="math display">\[
\begin{aligned}
E(y'Ay) &amp;= \text{tr}(A(\Sigma + \mu\mu')) \\
&amp;= \text{tr}(A\Sigma) + \text{tr}(A\mu\mu') \\
&amp;= \text{tr}(A\Sigma) + \text{tr}(\mu'A\mu) \quad \text{(cyclic property on second term)} \\
&amp;= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
\]</span></p>
<p><strong>Method 2: Using Scalar Summation</strong></p>
<p>We can express the quadratic form in scalar notation using the entries of <span class="math inline">\(A=(a_{ij})\)</span>, <span class="math inline">\(\Sigma=(\sigma_{ij})\)</span>, and <span class="math inline">\(\mu=(\mu_i)\)</span>: <span class="math display">\[
\begin{aligned}
E(y'Ay) &amp;= E\left(\sum_{i=1}^n \sum_{j=1}^n a_{ij} y_i y_j\right) \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n a_{ij} E(y_i y_j) \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n a_{ij} (\sigma_{ij} + \mu_i \mu_j) \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n a_{ij} \sigma_{ji} + \sum_{i=1}^n \sum_{j=1}^n \mu_i a_{ij} \mu_j \quad (\text{since } \Sigma \text{ is symmetric, } \sigma_{ij}=\sigma_{ji}) \\
&amp;= \text{tr}(A\Sigma) + \mu'A\mu
\end{aligned}
\]</span></p>
<p><strong>Method 3: Using Spectral Decomposition of A</strong></p>
<p>Since <span class="math inline">\(A\)</span> is symmetric, we use its spectral decomposition <span class="math inline">\(A = \sum_{i=1}^n \lambda_i q_i q_i'\)</span>. Substituting this into the quadratic form: <span class="math display">\[
y'Ay = y' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) y = \sum_{i=1}^n \lambda_i (q_i' y)^2
\]</span> Let <span class="math inline">\(w_i = q_i' y\)</span>. This is a scalar random variable which is a linear transformation of <span class="math inline">\(y\)</span>. Its properties are:</p>
<ol type="1">
<li><strong>Mean:</strong> <span class="math inline">\(E(w_i) = q_i' E(y) = q_i' \mu\)</span>.</li>
<li><strong>Variance:</strong> <span class="math inline">\(\text{Var}(w_i) = \text{Var}(q_i' y) = q_i' \text{Var}(y) q_i = q_i' \Sigma q_i\)</span>.</li>
</ol>
<p>Using the relation <span class="math inline">\(E(w_i^2) = \text{Var}(w_i) + [E(w_i)]^2\)</span>, we have: <span class="math display">\[
E[(q_i' y)^2] = q_i' \Sigma q_i + (q_i' \mu)^2
\]</span> Summing over all <span class="math inline">\(i\)</span> weighted by <span class="math inline">\(\lambda_i\)</span>: <span class="math display">\[
\begin{aligned}
E(y'Ay) &amp;= \sum_{i=1}^n \lambda_i \left[ q_i' \Sigma q_i + (q_i' \mu)^2 \right] \\
&amp;= \sum_{i=1}^n \text{tr}(\lambda_i q_i' \Sigma q_i) + \mu' \left( \sum_{i=1}^n \lambda_i q_i q_i' \right) \mu \\
&amp;= \text{tr}\left( \Sigma \sum_{i=1}^n \lambda_i q_i q_i' \right) + \mu' A \mu \\
&amp;= \text{tr}(\Sigma A) + \mu' A \mu
\end{aligned}
\]</span></p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em> (Geometric Interpretation via Sigma). </span>If we further decompose <span class="math inline">\(\Sigma = \sum_{j=1}^n \gamma_j v_j v_j'\)</span> (where <span class="math inline">\(\gamma_j, v_j\)</span> are eigenvalues/vectors of <span class="math inline">\(\Sigma\)</span>), the trace term becomes: <span class="math display">\[
\text{tr}(A\Sigma) = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \gamma_j (q_i' v_j)^2
\]</span> Here, <span class="math inline">\((q_i' v_j)^2 = \cos^2(\theta_{ij})\)</span> represents the alignment between the axes of the quadratic form (<span class="math inline">\(A\)</span>) and the axes of the data covariance (<span class="math inline">\(\Sigma\)</span>). The expectation is maximized when the eigenspaces of <span class="math inline">\(A\)</span> and <span class="math inline">\(\Sigma\)</span> align.</p>
</div>
<div id="cor-projection-mean" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5 (Expectation with Projection Matrix)</strong></span> Consider the special case where:</p>
<ol type="1">
<li><span class="math inline">\(P\)</span> is a <strong>projection matrix</strong> (symmetric and idempotent, <span class="math inline">\(P^2=P\)</span>).</li>
<li>The covariance is <strong>spherical</strong>: <span class="math inline">\(\Sigma = \sigma^2 I_n\)</span>.</li>
</ol>
<p>Then the expectation simplifies to: <span class="math display">\[
E(y'Py) = \sigma^2 r + ||P\mu||^2
\]</span> where <span class="math inline">\(r = \text{rank}(P) = \text{tr}(P)\)</span>.</p>
<p><strong>Proof:</strong> Using <a href="#thm-mean-qf" class="quarto-xref">Theorem&nbsp;36</a> with <span class="math inline">\(A=P\)</span> and <span class="math inline">\(\Sigma=\sigma^2 I_n\)</span>:</p>
<ol type="1">
<li><strong>Trace Term:</strong> <span class="math inline">\(\text{tr}(P\Sigma) = \text{tr}(P(\sigma^2 I_n)) = \sigma^2 \text{tr}(P)\)</span>. Since <span class="math inline">\(P\)</span> is idempotent, its eigenvalues are either 0 or 1, so <span class="math inline">\(\text{tr}(P) = \text{rank}(P) = r\)</span>.</li>
<li><strong>Mean Term:</strong> Since <span class="math inline">\(P\)</span> is symmetric and idempotent (<span class="math inline">\(P'P = P^2 = P\)</span>), we can rewrite the quadratic form: <span class="math display">\[
\mu' P \mu = \mu' P' P \mu = (P\mu)' (P\mu) = ||P\mu||^2
\]</span></li>
</ol>
</div>
<div id="exm-mean-ss-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 12 (Expectation of Sum of Squares Decomposition (i.i.d. Case))</strong></span> Consider a random vector <span class="math inline">\(y = (y_1, \dots, y_n)'\)</span> with mean vector <span class="math inline">\(\mu_y = \mu j_n\)</span> and covariance <span class="math inline">\(\Sigma = \sigma^2 I_n\)</span>. We analyze the two components of the total sum of squares by projecting <span class="math inline">\(y\)</span> onto the mean space (<span class="math inline">\(P_{j_n}\)</span>) and the residual space (<span class="math inline">\(I-P_{j_n}\)</span>).</p>
<ol type="1">
<li>The Projection Vectors</li>
</ol>
<p>First, we write the explicit forms of the projected vectors using <span class="math inline">\(P_{j_n} = \frac{1}{n}j_n j_n'\)</span>:</p>
<ul>
<li><p><strong>Mean Vector (<span class="math inline">\(P_{j_n}y\)</span>):</strong> Projecting <span class="math inline">\(y\)</span> onto the column space of <span class="math inline">\(j_n\)</span> replaces every element with the sample mean <span class="math inline">\(\bar{y}\)</span>. <span class="math display">\[
  P_{j_n}y = \bar{y} j_n = \begin{pmatrix} \bar{y} \\ \bar{y} \\ \vdots \\ \bar{y} \end{pmatrix}
  \]</span></p></li>
<li><p><strong>Residual Vector (<span class="math inline">\((I-P_{j_n})y\)</span>):</strong> Subtracting the mean projection from <span class="math inline">\(y\)</span> yields the deviations. <span class="math display">\[
  (I-P_{j_n})y = y - \bar{y}j_n = \begin{pmatrix} y_1 - \bar{y} \\ y_2 - \bar{y} \\ \vdots \\ y_n - \bar{y} \end{pmatrix}
  \]</span></p></li>
</ul>
<ol start="2" type="1">
<li>Expectations of Squared Norms</li>
</ol>
<p>We now find the expectation of the squared length of these vectors using <a href="#cor-projection-mean" class="quarto-xref">Corollary&nbsp;5</a>.</p>
<p><strong>Part A: Sum of Squares for Mean</strong> The quadratic form is the squared norm of the projected mean vector: <span class="math display">\[
y'P_{j_n}y = ||P_{j_n}y||^2 = \sum_{i=1}^n \bar{y}^2 = n\bar{y}^2
\]</span> Applying the corollary with <span class="math inline">\(P=P_{j_n}\)</span>:</p>
<ul>
<li><strong>Rank:</strong> <span class="math inline">\(\text{tr}(P_{j_n}) = 1\)</span>.</li>
<li><strong>Mean:</strong> <span class="math inline">\(P_{j_n}\mu_y = P_{j_n}(\mu j_n) = \mu j_n\)</span>. The squared norm is <span class="math inline">\(n\mu^2\)</span>.</li>
</ul>
<p><span class="math display">\[
E[||P_{j_n}y||^2] = \sigma^2(1) + n\mu^2
\]</span></p>
<p><strong>Part B: Sum of Squared Errors (SSE)</strong> The quadratic form is the squared norm of the residual vector: <span class="math display">\[
y'(I-P_{j_n})y = ||(I-P_{j_n})y||^2 = \sum_{i=1}^n (y_i - \bar{y})^2
\]</span> Applying the corollary with <span class="math inline">\(P=I-P_{j_n}\)</span>:</p>
<ul>
<li><strong>Rank:</strong> <span class="math inline">\(\text{tr}(I-P_{j_n}) = n - 1\)</span>.</li>
<li><strong>Mean:</strong> <span class="math inline">\((I-P_{j_n})\mu_y = \mu_y - P_{j_n}\mu_y = \mu j_n - \mu j_n = 0\)</span>. The squared norm is <span class="math inline">\(0\)</span>.</li>
</ul>
<p><span class="math display">\[
E[||(I-P_{j_n})y||^2] = \sigma^2(n-1) + 0
\]</span></p>
<p><strong>Conclusion</strong> These results confirm the standard properties: <span class="math inline">\(E(\bar{y}^2) = \frac{\sigma^2}{n} + \mu^2\)</span> and <span class="math inline">\(E(S^2) = \sigma^2\)</span>.</p>
</div>
<div id="exm-mean-sst-regression" class="theorem example">
<p><span class="theorem-title"><strong>Example 13 (Expectation of Total Sum of Squares (Regression Case))</strong></span> Consider now a regression setting where the mean of <span class="math inline">\(y\)</span> depends on covariates (e.g., <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span>). The mean vector <span class="math inline">\(\mu_y\)</span> is <strong>not</strong> proportional to <span class="math inline">\(j_n\)</span>. We are interested in the expectation of the <strong>Total Sum of Squares (SST)</strong>.</p>
<ol type="1">
<li><p>Identification The SST measures the variation of <span class="math inline">\(y\)</span> around the <em>global sample mean</em> <span class="math inline">\(\bar{y}\)</span>, ignoring the covariates: <span class="math display">\[
\text{SST} = \sum_{i=1}^n (y_i - \bar{y})^2 = y'(I - P_{j_n})y
\]</span> This is the same quadratic form as Part B in the previous example, but the underlying mean <span class="math inline">\(\mu_y\)</span> has changed.</p></li>
<li><p>Calculation We apply <a href="#cor-projection-mean" class="quarto-xref">Corollary&nbsp;5</a> with <span class="math inline">\(P = I - P_{j_n}\)</span> and general <span class="math inline">\(\mu_y\)</span>:</p></li>
</ol>
<ul>
<li><strong>Rank Term:</strong> Same as before, <span class="math inline">\(\text{tr}(I - P_{j_n}) = n - 1\)</span>.</li>
<li><strong>Mean Term:</strong> The projection of the mean vector is no longer zero. <span class="math display">\[
  (I - P_{j_n})\mu_y = \mu_y - \bar{\mu}j_n = \begin{pmatrix} \mu_1 - \bar{\mu} \\ \vdots \\ \mu_n - \bar{\mu} \end{pmatrix}
  \]</span> where <span class="math inline">\(\bar{\mu} = \frac{1}{n}\sum \mu_i\)</span> is the average of the true means. The squared norm is the sum of squared deviations of the true means: <span class="math display">\[
  ||(I - P_{j_n})\mu_y||^2 = \sum_{i=1}^n (\mu_i - \bar{\mu})^2
  \]</span></li>
</ul>
<p><strong>Conclusion</strong> <span class="math display">\[
E(\text{SST}) = (n-1)\sigma^2 + \sum_{i=1}^n (\mu_i - \bar{\mu})^2
\]</span> This shows that in regression, the SST estimates <span class="math inline">\((n-1)\sigma^2\)</span> <em>plus</em> the variability introduced by the regression signal (the spread of the true means <span class="math inline">\(\mu_i\)</span>).</p>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Data Generation ---</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 1: I.i.d. Case (common Mean)</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>mu_iid <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">3</span>, n)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>y_iid <span class="ot">&lt;-</span> mu_iid <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Case 2: Regression Case (sorted Mean)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Mu_i Is Sampled from N(3, Sd=3) and Sorted</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>mu_reg <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">rnorm</span>(n, <span class="dv">3</span>, <span class="dv">3</span>)) </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>y_reg <span class="ot">&lt;-</span> mu_reg <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>df_iid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y_iid,</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu =</span> mu_iid,</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">y_bar =</span> <span class="fu">mean</span>(y_iid),</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"i.i.d. Case (Common Mean)"</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>df_reg <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">id =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y_reg,</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu =</span> mu_reg,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">y_bar =</span> <span class="fu">mean</span>(y_reg),</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"Regression Case (Sorted Mean)"</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine Common Y Limits for Comparison Across Both Plots</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>y_min <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="fu">c</span>(df_iid<span class="sc">$</span>y, df_reg<span class="sc">$</span>y, df_iid<span class="sc">$</span>mu, df_reg<span class="sc">$</span>mu)) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>y_max <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">c</span>(df_iid<span class="sc">$</span>y, df_reg<span class="sc">$</span>y, df_iid<span class="sc">$</span>mu, df_reg<span class="sc">$</span>mu)) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>y_lims <span class="ot">&lt;-</span> <span class="fu">c</span>(y_min, y_max)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plotting Function ---</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>plot_func <span class="ot">&lt;-</span> <span class="cf">function</span>(df, title, ylims) {</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> id)) <span class="sc">+</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Vertical lines for the deviations (y_i - y_bar)</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> id, <span class="at">y =</span> y_bar, <span class="at">yend =</span> y), </span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>                 <span class="at">color =</span> <span class="st">"gray50"</span>, <span class="at">linetype =</span> <span class="st">"solid"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True means mu_i (red X)</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> mu, <span class="at">shape =</span> <span class="st">"True Mean (μ_i)"</span>), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Observations y_i (black dots)</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y, <span class="at">shape =</span> <span class="st">"Observed (y_i)"</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Global Sample Mean line (y_bar)</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> y_bar, <span class="at">linetype =</span> <span class="st">"Sample Mean (ȳ)"</span>), </span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linewidth =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_shape_manual</span>(<span class="at">name =</span> <span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"True Mean (μ_i)"</span> <span class="ot">=</span> <span class="dv">4</span>, <span class="st">"Observed (y_i)"</span> <span class="ot">=</span> <span class="dv">16</span>)) <span class="sc">+</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_linetype_manual</span>(<span class="at">name =</span> <span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Sample Mean (ȳ)"</span> <span class="ot">=</span> <span class="st">"dashed"</span>)) <span class="sc">+</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> ylims) <span class="sc">+</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> title, <span class="at">x =</span> <span class="st">"Observation Index (Sorted by μ_i)"</span>, <span class="at">y =</span> <span class="st">"Value"</span>) <span class="sc">+</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Combine Plots ---</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_func</span>(df_iid, <span class="st">"i.i.d. Case: E(SST) = (n-1)σ²"</span>, y_lims)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_func</span>(df_reg, <span class="st">"Regression Case: E(SST) = (n-1)σ² + Σ(μ_i - μ̄)²"</span>, y_lims)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2 <span class="sc">+</span> <span class="fu">plot_layout</span>(<span class="at">guides =</span> <span class="st">"collect"</span>) <span class="sc">&amp;</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-sst-comparison-sorted-v2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sst-comparison-sorted-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-sst-comparison-sorted-v2-1.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sst-comparison-sorted-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Comparison of SST components with increased variation in the true means. The vertical lines represent the deviations <span class="math inline">\((y_i - \bar{y})\)</span>. With <span class="math inline">\(\text{sd}(\mu_i) = 3\)</span>, the regression case (right) shows significantly larger deviations, illustrating how the systematic spread of the means dominates the Total Sum of Squares.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="non-central-chi2-distribution" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="non-central-chi2-distribution"><span class="header-section-number">5.3</span> Non-central <span class="math inline">\(\chi^2\)</span> Distribution</h2>
<p>To understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.</p>
<div id="def-nc-chisq" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 42 (Non-central <span class="math inline">\(\chi^2\)</span> Distribution)</strong></span> Let <span class="math inline">\(y \sim N_n(\mu, I_n)\)</span>. The random variable <span class="math inline">\(V = y'y = \sum y_i^2\)</span> follows a <strong>non-central chi-square distribution</strong> with <span class="math inline">\(n\)</span> degrees of freedom and non-centrality parameter <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
V \sim \chi^2(n, \lambda) \quad \text{where } \lambda = \mu'\mu = ||\mu||^2
\]</span></p>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Note on NCP Definition:</strong> Some definitions of non-central <span class="math inline">\(\chi^2\)</span> use <span class="math inline">\(\lambda = \frac{1}{2}\mu'\mu\)</span>. In this course, we use <span class="math inline">\(\lambda = \mu'\mu\)</span>. With this convention, the Poisson-mixture representation below uses Poisson(<span class="math inline">\(\lambda/2\)</span>) weights.</p>
</div>
</div>
<section id="visualizing-chi2-distributions" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="visualizing-chi2-distributions"><span class="header-section-number">5.3.1</span> Visualizing <span class="math inline">\(\chi^2\)</span> Distributions</h3>
<p>Here is a plot visualizing the difference between central and non-central Chi-square distributions.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-plot-chisq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot-chisq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-plot-chisq-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot-chisq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Central vs Non-central Chi-square Distribution
</figcaption>
</figure>
</div>
</div>
</div>
<p>The density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter <span class="math inline">\(\lambda\)</span> increases.</p>
</section>
<section id="mean-variance-and-mgf" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="mean-variance-and-mgf"><span class="header-section-number">5.3.2</span> Mean, Variance, and MGF</h3>
<p>We summarize the key properties of the non-central chi-square distribution.</p>
<div id="thm-chisq-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 37 (Properties of Non-central Chi-square)</strong></span> Let <span class="math inline">\(V \sim \chi^2(n, \lambda)\)</span>. Then:</p>
<ol type="1">
<li><strong>Mean:</strong> <span class="math inline">\(E(V) = n + \lambda\)</span></li>
<li><strong>Variance:</strong> <span class="math inline">\(\text{Var}(V) = 2n + 4\lambda\)</span></li>
<li><strong>Moment Generating Function (MGF):</strong> <span class="math display">\[
m_V(t) = \frac{\exp\left[ -\frac{\lambda}{2} \left\{1 - \frac{1}{1-2t}\right\} \right]}{(1-2t)^{n/2}} \quad \text{for } t &lt; 1/2
\]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (Mean). </span>By definition, <span class="math inline">\(V \sim \chi^2(n, \lambda)\)</span> is the distribution of <span class="math inline">\(y'y\)</span> where <span class="math inline">\(y \sim N_n(\mu, I_n)\)</span> and the non-centrality parameter is <span class="math inline">\(\lambda = \mu'\mu = ||\mu||^2\)</span>. Applying <a href="#lem-simple-qf" class="quarto-xref">Lemma&nbsp;2</a> to the random vector <span class="math inline">\(y\)</span>: <span class="math display">\[
E(V) = E(y'y) = n + \mu'\mu = n + \lambda
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (MGF). </span>Since the components <span class="math inline">\(y_i\)</span> of the vector <span class="math inline">\(y\)</span> are independent <span class="math inline">\(N(\mu_i, 1)\)</span>, and <span class="math inline">\(V = \sum_{i=1}^n y_i^2\)</span>, the MGF of <span class="math inline">\(V\)</span> is the product of the MGFs of each <span class="math inline">\(y_i^2\)</span>: <span class="math display">\[
m_V(t) = E[e^{t \sum y_i^2}] = \prod_{i=1}^n E[e^{t y_i^2}]
\]</span> Consider a single component <span class="math inline">\(y_i \sim N(\mu_i, 1)\)</span>. Its squared expectation is: <span class="math display">\[
\begin{aligned}
E[e^{t y_i^2}] &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ty^2} e^{-\frac{1}{2}(y-\mu_i)^2} dy \\
&amp;= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left\{ -\frac{1}{2} \left[ (1-2t)y^2 - 2\mu_i y + \mu_i^2 \right] \right\} dy
\end{aligned}
\]</span> Completing the square in the exponent for <span class="math inline">\(y\)</span> (assuming <span class="math inline">\(t &lt; 1/2\)</span>): <span class="math display">\[
(1-2t)y^2 - 2\mu_i y + \mu_i^2 = (1-2t)\left(y - \frac{\mu_i}{1-2t}\right)^2 + \mu_i^2 - \frac{\mu_i^2}{1-2t}
\]</span> The integral of the Gaussian kernel <span class="math inline">\(\exp\{ -\frac{1}{2}(1-2t)(y - \dots)^2 \}\)</span> yields <span class="math inline">\(\sqrt{\frac{2\pi}{1-2t}}\)</span>. The remaining constant term is: <span class="math display">\[
\exp\left\{ -\frac{1}{2} \left( \mu_i^2 - \frac{\mu_i^2}{1-2t} \right) \right\} = \exp\left\{ \frac{\mu_i^2}{2} \left( \frac{1}{1-2t} - 1 \right) \right\} = \exp\left\{ \frac{\mu_i^2 t}{1-2t} \right\}
\]</span> Thus, for a single component: <span class="math display">\[
m_{y_i^2}(t) = (1-2t)^{-1/2} \exp\left( \frac{\mu_i^2 t}{1-2t} \right)
\]</span> Multiplying the MGFs for all <span class="math inline">\(n\)</span> components: <span class="math display">\[
\begin{aligned}
m_V(t) &amp;= \prod_{i=1}^n (1-2t)^{-1/2} \exp\left( \frac{\mu_i^2 t}{1-2t} \right) \\
&amp;= (1-2t)^{-n/2} \exp\left( \frac{t \sum \mu_i^2}{1-2t} \right)
\end{aligned}
\]</span> Substituting <span class="math inline">\(\lambda = \sum \mu_i^2\)</span> (so <span class="math inline">\(\sum \mu_i^2 = \lambda\)</span>): <span class="math display">\[
m_V(t) = (1-2t)^{-n/2} \exp\left( \frac{\lambda t}{1-2t} \right)
\]</span> Note that <span class="math inline">\(\displaystyle \frac{\lambda t}{1-2t} = -\frac{\lambda}{2}\left(1 - \frac{1}{1-2t}\right)\)</span>, which leads to the Poisson-mixture representation with <span class="math inline">\(J \sim \text{Poisson}(\lambda/2)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (Variance). </span>We use the <strong>Cumulant Generating Function</strong>, <span class="math inline">\(K_V(t) = \ln m_V(t)\)</span>, as its derivatives yield the mean and variance directly: <span class="math display">\[
K_V(t) = -\frac{n}{2} \ln(1-2t) + \frac{\lambda t}{1-2t}
\]</span> First derivative (Mean): <span class="math display">\[
\begin{aligned}
K'_V(t) &amp;= -\frac{n}{2} \left(\frac{-2}{1-2t}\right) + \lambda \left[ \frac{1(1-2t) - t(-2)}{(1-2t)^2} \right] \\
&amp;= \frac{n}{1-2t} + \frac{\lambda}{(1-2t)^2}
\end{aligned}
\]</span> Second derivative (Variance): <span class="math display">\[
\begin{aligned}
K''_V(t) &amp;= n(-1)(1-2t)^{-2}(-2) + \lambda(-2)(1-2t)^{-3}(-2) \\
&amp;= \frac{2n}{(1-2t)^2} + \frac{4\lambda}{(1-2t)^3}
\end{aligned}
\]</span> Evaluating at <span class="math inline">\(t=0\)</span>: <span class="math display">\[
\text{Var}(V) = K''_V(0) = 2n + 4\lambda
\]</span></p>
</div>
</section>
<section id="additivity" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="additivity"><span class="header-section-number">5.3.3</span> Additivity</h3>
<div id="thm-chisq-additivity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 38 (Additivity of Chi-square)</strong></span> If <span class="math inline">\(v_1, \dots, v_k\)</span> are independent random variables distributed as <span class="math inline">\(\chi^2(n_i, \lambda_i)\)</span>, then their sum follows a chi-square distribution:</p>
<p><span class="math display">\[
\sum_{i=1}^k v_i \sim \chi^2\left(\sum_{i=1}^k n_i, \sum_{i=1}^k \lambda_i\right)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Method 1: Using MGFs</strong></p>
<p>The moment generating function of <span class="math inline">\(v_i \sim \chi^2(n_i, \lambda_i)\)</span> is: <span class="math display">\[
M_{v_i}(t) = \frac{\exp\left[-\frac{\lambda_i}{2} \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}}
\]</span></p>
<p>Since <span class="math inline">\(v_1, \dots, v_k\)</span> are independent, the MGF of their sum <span class="math inline">\(V = \sum v_i\)</span> is the product of their individual MGFs:</p>
<p><span class="math display">\[
\begin{aligned}
M_V(t) &amp;= \prod_{i=1}^k M_{v_i}(t) \\
&amp;= \prod_{i=1}^k \frac{\exp\left[-\frac{\lambda_i}{2} \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{n_i/2}} \\
&amp;= \frac{\exp\left[-\frac{\sum \lambda_i}{2} \left(1 - \frac{1}{1-2t}\right)\right]}{(1-2t)^{\sum n_i/2}}
\end{aligned}
\]</span></p>
<p>This is the MGF of a non-central chi-square distribution with degrees of freedom <span class="math inline">\(\sum n_i\)</span> and non-centrality parameter <span class="math inline">\(\sum \lambda_i\)</span>.</p>
<p><strong>Method 2: Geometric Interpretation</strong></p>
<p>Let <span class="math inline">\(v_i = ||y_i||^2\)</span> where <span class="math inline">\(y_i \sim N_{n_i}(\mu_i, I_{n_i})\)</span>. Since the vectors <span class="math inline">\(y_i\)</span> are independent, we can stack them into a larger vector <span class="math inline">\(y = (y_1', \dots, y_k')'\)</span>.</p>
<p><span class="math display">\[
y \sim N_{\sum n_i}(\mu, I_{\sum n_i}) \quad \text{where } \mu = (\mu_1', \dots, \mu_k')'
\]</span></p>
<p>The sum of squares is: <span class="math display">\[
\sum v_i = \sum ||y_i||^2 = ||y||^2
\]</span></p>
<p>By definition, <span class="math inline">\(||y||^2\)</span> follows a non-central chi-square distribution with degrees of freedom equal to the dimension of <span class="math inline">\(y\)</span> (<span class="math inline">\(\sum n_i\)</span>) and non-centrality parameter <span class="math inline">\(\lambda = ||\mu||^2\)</span>.</p>
<p><span class="math display">\[
\lambda = \sum_{i=1}^k ||\mu_i||^2 = \sum_{i=1}^k \lambda_i
\]</span></p>
</div>
</section>
<section id="poisson-mixture-representation" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="poisson-mixture-representation"><span class="header-section-number">5.3.4</span> Poisson Mixture Representation</h3>
<div id="thm-chisq-poisson-mixture" class="theorem">
<p><span class="theorem-title"><strong>Theorem 39 (Poisson Mixture Representation)</strong></span> Let <span class="math inline">\(v \sim \chi^2(n, \lambda)\)</span> be a non-central chi-square random variable. Its probability density function can be represented as a Poisson-weighted sum of central chi-square density functions:</p>
<p><span class="math display">\[
f(v; n, \lambda) = \sum_{j=0}^{\infty} \left( \frac{e^{-\lambda/2} (\lambda/2)^j}{j!} \right) f(v; n+2j, 0)
\]</span></p>
<p>where <span class="math inline">\(f(v; \nu, 0)\)</span> is the density of a central chi-square distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We use the Moment Generating Function (MGF) approach. The MGF of a non-central chi-square distribution <span class="math inline">\(v \sim \chi^2(n, \lambda)\)</span> is: <span class="math display">\[
M_v(t) = (1-2t)^{-n/2} \exp\left( \frac{\lambda}{2} \left[ \frac{1}{1-2t} - 1 \right] \right)
\]</span></p>
<p>We can expand the exponential term using the power series <span class="math inline">\(e^x = \sum_{j=0}^\infty \frac{x^j}{j!}\)</span>: <span class="math display">\[
\begin{aligned}
M_v(t) &amp;= (1-2t)^{-n/2} e^{-\lambda/2} \exp\left( \frac{\lambda/2}{1-2t} \right) \\
&amp;= e^{-\lambda/2} (1-2t)^{-n/2} \sum_{j=0}^{\infty} \frac{1}{j!} \left( \frac{\lambda/2}{1-2t} \right)^j \\
&amp;= \sum_{j=0}^{\infty} \left( \frac{e^{-\lambda/2} (\lambda/2)^j}{j!} \right) (1-2t)^{-(n+2j)/2}
\end{aligned}
\]</span></p>
<p>Recognizing the terms:</p>
<ol type="1">
<li>The term in parentheses, <span class="math inline">\(P(J=j) = \frac{e^{-\lambda/2} (\lambda/2)^j}{j!}\)</span>, is the probability mass function of a <strong>Poisson</strong> random variable <span class="math inline">\(J \sim \text{Poisson}(\lambda/2)\)</span>.</li>
<li>The term <span class="math inline">\((1-2t)^{-(n+2j)/2}\)</span> is the MGF of a <strong>central chi-square</strong> distribution with <span class="math inline">\(n+2j\)</span> degrees of freedom.</li>
</ol>
<p>Since the MGF of the mixture is the sum of the MGFs of the components weighted by the mixture probabilities, the density must follow the same mixture structure.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>This theorem implies a hierarchical model for generating a non-central chi-square variable:</p>
<ol type="1">
<li>Sample <span class="math inline">\(J \sim \text{Poisson}(\lambda/2)\)</span>.</li>
<li>Given <span class="math inline">\(J=j\)</span>, sample <span class="math inline">\(V \sim \chi^2(n+2j, 0)\)</span>.</li>
</ol>
<p>This is particularly useful for numerical computation, as it allows the non-central CDF to be approximated by a finite sum of central chi-square CDFs.</p>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">4</span>          <span class="co"># Base degrees of freedom</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">4</span>     <span class="co"># Non-centrality parameter (lambda = ||mu||^2)</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>x_limit <span class="ot">&lt;-</span> <span class="dv">25</span>   <span class="co"># X-axis range</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>j_values <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">15</span> <span class="co"># Sequence of J = 0, 1, 2...</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Data for the Mixture Components</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, x_limit, <span class="at">length.out =</span> <span class="dv">400</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>mixture_df <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, <span class="fu">lapply</span>(j_values, <span class="cf">function</span>(j) {</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  weight <span class="ot">&lt;-</span> <span class="fu">dpois</span>(j, lambda<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> x,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">dchisq</span>(x, <span class="at">df =</span> n <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>j),</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">j =</span> j,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight =</span> weight</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate Data for the True Non-central Chi-square</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># R Uses Ncp = ||mu||^2 (we set lambda = ||mu||^2)</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>true_nc <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> x,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">dchisq</span>(x, <span class="at">df =</span> n, <span class="at">ncp =</span> lambda)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Draw weighted central chi-square curves (the "cloud")</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> mixture_df, </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">group =</span> j, <span class="at">alpha =</span> weight), </span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">"black"</span>, </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>            <span class="at">linewidth =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Draw the true non-central chi-square density</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> true_nc, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>            <span class="at">color =</span> <span class="st">"blue"</span>, </span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>            <span class="at">linewidth =</span> <span class="fl">1.3</span>) <span class="sc">+</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Aesthetics</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_alpha_continuous</span>(<span class="at">range =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.8</span>), <span class="at">guide =</span> <span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Poisson Mixture Representation of Non-central Chi-square"</span>,</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="fu">paste0</span>(<span class="st">"n = "</span>, n, <span class="st">", λ = "</span>, lambda, <span class="st">" (Blue line = True Non-central)"</span>),</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Value (v)"</span>,</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Density"</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-chisq-poisson-mixture-fixed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chisq-poisson-mixture-fixed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-chisq-poisson-mixture-fixed-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chisq-poisson-mixture-fixed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: The non-central chi-square distribution as a Poisson mixture. The black curves represent central chi-square densities with <span class="math inline">\(df = n + 2j\)</span>, with transparency (alpha) proportional to the Poisson weight <span class="math inline">\(P(J=j)\)</span>. The solid blue line is the true non-central chi-square density.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="distribution-of-quadratic-forms-1" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="distribution-of-quadratic-forms-1"><span class="header-section-number">5.4</span> Distribution of Quadratic Forms</h2>
<section id="mgf-of-quadratic-forms" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="mgf-of-quadratic-forms"><span class="header-section-number">5.4.1</span> MGF of Quadratic Forms</h3>
<p>To determine the distribution of general quadratic forms <span class="math inline">\(y'Ay\)</span>, we look at their MGF.</p>
<div id="thm-mgf-quad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 40 (MGF of Quadratic Form)</strong></span> If <span class="math inline">\(y \sim N_p(\mu, \Sigma)\)</span>, then the MGF of <span class="math inline">\(Q = y'Ay\)</span> is:</p>
<p><span class="math display">\[
M_Q(t) = |I - 2tA\Sigma|^{-1/2} \exp\left(-\frac{1}{2} \mu' [I - (I - 2tA\Sigma)^{-1}] \Sigma^{-1} \mu\right)
\]</span></p>
</div>
</section>
<section id="distribution-of-the-sum-squares-of-projected-spherical-normal" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="distribution-of-the-sum-squares-of-projected-spherical-normal"><span class="header-section-number">5.4.2</span> Distribution of the Sum Squares of Projected Spherical Normal</h3>
<p>We will prove a simplified version of <a href="#thm-dist-quad" class="quarto-xref">Theorem&nbsp;42</a> first.</p>
<div id="thm-proj-matrix" class="theorem">
<p><span class="theorem-title"><strong>Theorem 41 (Distribution of Projected Spherical Normal)</strong></span> If <span class="math inline">\(y \sim N_n(\mu, \sigma^2 I_n)\)</span> and <span class="math inline">\(P_V\)</span> is a projection matrix onto a subspace <span class="math inline">\(V\)</span> of dimension <span class="math inline">\(r\)</span>, then:</p>
<p><span class="math display">\[
\frac{1}{\sigma^2} y'P_V y = \frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{\sigma^2}\right)
\]</span></p>
<p>This holds because <span class="math inline">\(\frac{1}{\sigma^2} P_V (\sigma^2 I) = P_V\)</span>, which is idempotent.</p>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Crucial Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is one of the most important theorems in the course, establishing the fundamental conditions under which a quadratic form follows a chi-square distribution.</p>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>When <span class="math inline">\(\sigma^2=1\)</span></strong></p>
<p>Let <span class="math inline">\(P_V\)</span> be the projection matrix. We know <span class="math inline">\(P_V = QQ'\)</span> where <span class="math inline">\(Q = (q_1, \dots, q_r)\)</span> is an <span class="math inline">\(n \times r\)</span> matrix with orthonormal columns (<span class="math inline">\(Q'Q = I_r\)</span>).</p>
<p>The projection of vector <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(V\)</span> can be expressed using the orthonormal basis vectors: <span class="math display">\[
P_V y = Q Q' y = (q_1, \dots, q_r) \begin{pmatrix} q_1' y \\ \vdots \\ q_r' y \end{pmatrix} = \sum_{i=1}^r (q_i' y) q_i
\]</span></p>
<p>The squared norm of the projection is: <span class="math display">\[
y' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2
\]</span></p>
<p>Since <span class="math inline">\(y \sim N(\mu, I_n)\)</span>, the linear transformation <span class="math inline">\(w = Q'y\)</span> follows: <span class="math display">\[
w \sim N(Q'\mu, Q' I_n Q) = N(Q'\mu, I_r)
\]</span></p>
<p>Thus, <span class="math inline">\(w\)</span> is a vector of <span class="math inline">\(r\)</span> independent normal variables with variance 1. The sum of squares <span class="math inline">\(||w||^2\)</span> is by definition non-central chi-square:</p>
<p><span class="math display">\[
||w||^2 \sim \chi^2(r, \lambda)
\]</span> where the non-centrality parameter is: <span class="math display">\[
\lambda = ||E(w)||^2 = ||Q'\mu||^2
\]</span></p>
<p>Note that <span class="math inline">\(||Q'\mu||^2 = \mu' Q Q' \mu = \mu' P_V \mu = ||P_V \mu||^2\)</span>.</p>
<p>Thus, <span class="math inline">\(y' P_V y \sim \chi^2(r, ||P_V \mu||^2)\)</span>.</p>
<p><strong>When <span class="math inline">\(\sigma^2\not=1\)</span></strong></p>
<p>If <span class="math inline">\(y \sim N(\mu, \sigma^2 I_n)\)</span>, we standardize by dividing by <span class="math inline">\(\sigma\)</span>.</p>
<p>Let <span class="math inline">\(w = y/\sigma\)</span>. Then <span class="math inline">\(w \sim N(\mu/\sigma, I_n)\)</span>. Applying the previous result to <span class="math inline">\(w\)</span>:</p>
<p><span class="math display">\[
w' P_V w = \frac{y' P_V y}{\sigma^2} \sim \chi^2\left(r, \left|\left| P_V \frac{\mu}{\sigma} \right|\right|^2\right)
\]</span> which simplifies to: <span class="math display">\[
\frac{||P_V y||^2}{\sigma^2} \sim \chi^2\left(r, \frac{||P_V \mu||^2}{\sigma^2}\right)
\]</span></p>
</div>
<div class="callout callout-style-default callout-important callout-titled" name="Scale of the Quadratic Form">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term <span class="math inline">\(\|P_V y\|^2\)</span> itself is <strong>not</strong> a standard chi-square variable; it is a scaled chi-square variable. Its mean is:</p>
<p><span class="math display">\[
E(\|P_V y\|^2) = \sigma^2 \left(r + \frac{\|P_V \mu\|^2}{\sigma^2}\right) = r\sigma^2 + \|P_V \mu\|^2
\]</span></p>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Generate Data</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>) </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">3</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n, mu, sigma)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(data)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>, <span class="st">"z"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Project Points</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>proj_points <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(data, <span class="dv">1</span>, <span class="cf">function</span>(p) {</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(p <span class="sc">*</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>)) <span class="sc">*</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>) <span class="sc">+</span> <span class="fu">sum</span>(p <span class="sc">*</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)) <span class="sc">*</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>df_proj <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(proj_points)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df_proj) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"px"</span>, <span class="st">"py"</span>, <span class="st">"pz"</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Setup Axis Styles</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>ax_style <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">""</span>,</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">showgrid =</span> <span class="cn">TRUE</span>,        </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">gridcolor =</span> <span class="st">"gray"</span>,</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">gridwidth =</span> <span class="fl">0.5</span>,</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">zeroline =</span> <span class="cn">FALSE</span>,       </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">showline =</span> <span class="cn">FALSE</span>,       </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">showticklabels =</span> <span class="cn">FALSE</span>,</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">showbackground =</span> <span class="cn">FALSE</span>,</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">showspikes =</span> <span class="cn">FALSE</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Create the Plot with EXPLICIT DIMENSIONS</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>(</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- FIX IS HERE: Force the pixel dimensions ---</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">width =</span> <span class="dv">800</span>, </span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">height =</span> <span class="dv">450</span> </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Optional: Floor Plane ---</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="sc">-</span><span class="dv">2</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">8</span>), <span class="at">z =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">"mesh3d"</span>, <span class="at">opacity =</span> <span class="fl">0.05</span>, <span class="at">color =</span> <span class="st">'gray'</span>, </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="at">hoverinfo =</span> <span class="st">"none"</span>, <span class="at">showlegend =</span> <span class="cn">FALSE</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Original Data (y) ---</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> df, <span class="at">x =</span> <span class="sc">~</span>x, <span class="at">y =</span> <span class="sc">~</span>y, <span class="at">z =</span> <span class="sc">~</span>z,</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">'scatter3d'</span>, <span class="at">mode =</span> <span class="st">'markers'</span>,</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">color =</span> <span class="st">'blue'</span>, <span class="at">opacity =</span> <span class="fl">0.6</span>),</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">'&lt;i&gt;y&lt;/i&gt;'</span> </span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Projected Shadow (P_V y) ---</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_trace</span>(</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> df_proj, <span class="at">x =</span> <span class="sc">~</span>px, <span class="at">y =</span> <span class="sc">~</span>py, <span class="at">z =</span> <span class="sc">~</span>pz,</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">'scatter3d'</span>, <span class="at">mode =</span> <span class="st">'markers'</span>,</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">color =</span> <span class="st">'red'</span>, <span class="at">opacity =</span> <span class="fl">0.8</span>),</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"&lt;i&gt;P&lt;/i&gt;&lt;sub&gt;V&lt;/sub&gt;&lt;i&gt;y&lt;/i&gt;"</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Residual Lines ---</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_segments</span>(</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> df<span class="sc">$</span>x, <span class="at">xend =</span> df_proj<span class="sc">$</span>px,</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> df<span class="sc">$</span>y, <span class="at">yend =</span> df_proj<span class="sc">$</span>py,</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>    <span class="at">z =</span> df<span class="sc">$</span>z, <span class="at">zend =</span> df_proj<span class="sc">$</span>pz,</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">line =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">'gray'</span>, <span class="at">width =</span> <span class="dv">1</span>),</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>    <span class="at">showlegend =</span> <span class="cn">FALSE</span>, <span class="at">hoverinfo =</span> <span class="st">"none"</span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Manual Labels ---</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_text</span>(</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">c</span>(<span class="fl">8.5</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">8.5</span>, <span class="dv">0</span>), <span class="at">z =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">8.5</span>),</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>    <span class="at">text =</span> <span class="fu">c</span>(<span class="st">"&lt;i&gt;q&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;"</span>, <span class="st">"&lt;i&gt;q&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;"</span>, <span class="st">"&lt;i&gt;V&lt;/i&gt;&lt;sup&gt;\u22A5&lt;/sup&gt;"</span>),</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>    <span class="at">textfont =</span> <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">15</span>, <span class="at">color =</span> <span class="st">"black"</span>),</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>    <span class="at">showlegend =</span> <span class="cn">FALSE</span></span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>    <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>      <span class="at">xaxis =</span> ax_style,</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>      <span class="at">yaxis =</span> ax_style,</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>      <span class="at">zaxis =</span> ax_style,</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>      <span class="at">aspectmode =</span> <span class="st">"cube"</span>,</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>      <span class="at">camera =</span> <span class="fu">list</span>(<span class="at">eye =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">1.6</span>, <span class="at">y =</span> <span class="fl">1.6</span>, <span class="at">z =</span> <span class="fl">1.3</span>))</span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Projection of Trivariate Normal onto 2D Subspace"</span>,</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>    <span class="at">margin =</span> <span class="fu">list</span>(<span class="at">l=</span><span class="dv">0</span>, <span class="at">r=</span><span class="dv">0</span>, <span class="at">b=</span><span class="dv">0</span>, <span class="at">t=</span><span class="dv">30</span>),</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure layout autosize is off so it respects the width/height above</span></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>    <span class="at">autosize =</span> <span class="cn">FALSE</span> </span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="htmlwidget-cc2b35d4981c935c0ccc" style="width:100%;height:365px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-cc2b35d4981c935c0ccc">{"x":{"visdat":{"216e28498d2d":["function () ","plotlyVisDat"],"216e74d39a04":["function () ","data"],"216e7024b887":["function () ","data"]},"cur_data":"216e7024b887","attrs":{"216e28498d2d":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-2,8,8,-2],"y":[-2,-2,8,8],"z":[0,0,0,0],"type":"mesh3d","opacity":0.050000000000000003,"color":"gray","hoverinfo":"none","showlegend":false,"inherit":true},"216e74d39a04":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"size":3,"color":"blue","opacity":0.59999999999999998},"name":"<i>y<\/i>","inherit":true},"216e7024b887":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"size":3,"color":"red","opacity":0.80000000000000004},"name":"<i>P<\/i><sub>V<\/sub><i>y<\/i>","inherit":true},"216e7024b887.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[1.9264439808635216,0.83134857557564446,1.3652517350910904,1.9711584470716856,2.6706959687339298,0.34945345657489035,1.6502457607887315,2.7564064385454836,1.4611908400224074,2.2272919216729421,2.4922285700644311,2.2678350153316833,2.6532576794713814,1.87729133902262,1.5863234864078797,-0.64314895202897882,1.9070589815205279,2.4302846963599869,2.53539884086815,1.4447216486867482,3.7795029097751489,2.2864244196288248,2.1263158584588857,3.2722667794685192,1.2815337786807408,1.5496613760752882,4.397452480049763,2.0111291872208881,3.6335684214083068,0.56149335508547682,1.8094831979570984,2.3784239036368318,2.3000385452057976,0.99436374048916321,2.0192592746253708,0.92257934688801813,2.7127033252442825,3.0847750898606461,-0.22498769648741268,3.2356934623001434,0.75895550319150895,2.4547692689826328,2.6599026381426083,1.8001101718985668,1.3548860431402083,2.1653210212420504,2.4388187007144535,2.883302819948705,-0.052336983901952738,0.36362073194097544,3.4304023411881834,3.0466288471115464,2.4352889489076821,2.7151784071110603,2.9171749176410033,-0.66092279846568136,3.11027709663882,1.5150124034376753,2.230616830631202,1.7048421991164227,2.8719649540787442,1.651527551044389,2.5185037658882479,1.609315021361021,0.90721279089849438,3.2100105104402648,2.7409000112742739,3.7242622391564528,2.0651539325793031,3.1250027458281737,3.9754190540190781,1.7185178849610334,0.6770488872447844,1.7606484328618839,1.7859587600146167,2.1516805045093199,3.7123049773192824,1.6738561074946519,2.3730046558049156,1.7723159351044688,2.0204507085250327,2.3140576635062624,3.3282146960369863,2.1213183774907782,2.7128423200311134,2.7788600297835346,2.9147732708556342,1.4256054478156166,3.6268812141946771,1.619043260705987,1.8942158323502483,3.4040502677141715,3.2940839061455804,0.91000812758876193,1.1269289995825362,0.64192094062938554,2.181847192619832,2.1648408679184663,2.3641146873550629,2.5521577142185579,1.3981071540149834,1.0063014089012858,3.0267850560749041,2.7510613026063231,0.4908334626712989,1.9048525491507133,1.1040521772435012,-0.070751070541964278,2.1501201311881433,1.9207882907414118,1.9026307324869189,2.2161525418012324,2.8824651643965873,2.2055975044833867,1.3835641571264716,1.2652007480787191,1.8681972069588191,2.3100169865789519,0.96031964729795849,1.8156911311576163,2.9672672601679135,1.8917199087655139,1.3015793324056097,1.7240548315744604,3.1146485453619803,2.5500439612141359,3.2366758000816054,2.1390978571617709,2.4102750964904249,1.4415430876518671,2.6053706689303331,1.4936664578107606,0.57943449530425561,2.1279929656841259,3.9458512177314358,2.8009143395730618,3.16525338994626,2.3588557230931522,1.3914428216837758,1.7977591448897163,1.7267518931880748,1.5313002202179129,2.7041672839297157,0.80263649763458811,2.8663661321127734,2.8641524862258931,0.80137764093125141,2.6394919979005338,4.4302266523445084,1.4427845180583918,2.8449042414080719,1.2177981543805965,3.1107114182028006,2.2498247199265626,3.6519153916231648,0.54102927146308444,1.9487021138746388,1.4730748238744023,1.802735130923701,1.3704212573139951,1.1661564194649994,2.5787223746164498,0.91241928592307309,3.4840309315562834,0.81379341468624955,2.1010791513952798,2.5329892868264805,2.5867353390926251,1.6982533361336196,2.0795019953284011,2.9612641518096776,0.54353408292357419,1.2182602884679179,2.3204023143166101,1.5552180216630085,3.3700039939709998,2.6732538633548355,2.072166752414756,0.49224268246331992,2.0261002253262417,1.6835841318030069,1.8976534864794021,0.81844077294431461,2.4986580438299244,0.96104355969296695,1.7737780185575904,2.3814258294513522,1.2164842109869523,2.5829914056800645,0.68348959834830758,-0.80977467896054156,2.4649679907858526,2.8405398269609998,1.7141545764086161,2.5041262545916081,0.84408347491632862,1.8728513931743378,0.058481623959382167,3.1811808914385216,3.859910861543161],"y":[5.1988103488837201,4.3124129764335102,2.7348549433036471,3.5431940592320892,2.5856600520811401,2.5237531053844218,2.2113971621497575,2.4053827325404891,4.6509074673366904,2.9459718749145596,3.119245236427584,3.2436874295990918,4.2324758784853378,2.48393616905522,2.0074928496079627,4.6756969324031896,2.5588367830947147,2.2769340300601262,1.7637268811167071,1.7152842776822035,2.4260265207020133,3.6179858171665282,4.1098481389297188,3.7075883538355878,2.6363427029047477,3.0597499373846007,2.2954035363199301,2.2827818384259899,3.8846504989769199,1.9844074213964555,4.9552939654924639,2.9096804060341483,3.2145388266292163,2.2614722952604267,2.4256113102367305,1.6829838676947582,2.8170746116272727,3.4189824049244639,3.32430434416138,2.2184635129452488,2.2113780291459983,2.4978012816571393,4.4960606698463508,1.862696379334255,2.8209484056198018,4.9023618216789266,2.8990251146711916,1.6401592961786062,2.3352305647259377,3.4854599789048781,2.6243971283302265,2.438123636450217,2.6560827658715409,3.0904966471392212,4.5985087711458261,2.911434887861116,4.080799496151517,3.6307541156505665,2.8863601044938592,1.4670979971094018,2.4788826824474799,2.5101295468615255,3.0471544327615274,4.3001986776668204,5.2930789738310935,4.5475810589837691,2.8668490356710556,1.2434726044423627,2.6112201359282565,3.0892072230732945,3.845013004067436,3.9625279684842711,3.6843094294164644,1.6047256502005327,3.8496430456333552,2.5534427835727778,3.1748027001612558,3.0745511771737344,3.4281667649705057,3.0246749828261401,1.3325249024143371,3.7364959647734421,3.386026568349676,2.7343483747217778,3.1181445110466806,3.1340386453684634,3.2210194685610021,4.6408461659774858,2.7809496210665241,3.1680653838846573,4.168383873069093,4.0541810233769189,4.1452631103803572,2.422531998940443,5.0024827302928294,3.0667008709301831,4.8668518447068623,1.6490973139692886,3.0209835863542374,4.2499145709692154,2.2847578127772037,2.247311031782258,2.0614612963931065,1.9474867206612636,2.5628404668196012,3.3311791729589819,0.98578950207927996,3.2119804333722919,4.2366750464165701,5.0375740182404352,4.3011759922005872,3.7567747637959616,1.2732696008856701,2.3984932919932183,2.6479535434173833,3.7035239027568942,2.8943286659962255,1.7413513719398284,4.6844357080941146,3.9113912917959626,3.237430272491026,4.2181086103258121,1.6612257127650272,3.6608202977898006,2.477087623686578,3.6837455218507125,2.9391780453399257,3.6329607130314505,4.3355176150593921,3.0072900903168986,4.0175586369520921,1.811565964852021,2.2783955595639847,4.519217711388178,3.3773879730239296,0.94777717956626972,1.6359625479176245,2.7992189844108788,3.865779404334488,2.8981167442847777,3.6241874720206528,3.9590053777878254,4.671054828862939,3.0560167332749621,2.9480180938191012,1.2467626408577306,3.0993275940878275,2.4281499421044366,2.0259904171959087,2.8200937689524599,4.0149431727436573,1.0072515113114173,2.5727207127945713,3.1166372835827065,2.1067924299450471,3.333902942499225,3.4114299206157308,2.9669638407240084,0.53410180623997272,5.5714581458666377,2.7947007425318007,3.6511932815876702,3.273766491036548,4.0246732348183505,3.8176594463740878,2.7902068287714914,3.3781677722085068,2.0545911688761072,3.8569230108993184,2.5389616611156458,5.4167733537882103,1.3489511043118125,2.5360127570339852,3.8253798627592426,3.5101325468786637,2.4105189614849998,2.0032192577924777,3.1444757047106968,2.9856925868330868,1.2097187627359396,3.0345510671338509,3.1902303156924572,3.1747263969818413,1.9449829573973181,3.4761332783026289,4.3785701369592402,3.4562364031798127,1.8644115296256567,2.564354530308095,3.3461036195536069,2.3529543686817309,0.84235366498472297,3.8842508200282091,2.1705223883754821,2.426439729231717,4.5039006090045355,2.2258550703945952,3.8457315401892957,1.7393171211812251,2.6454575969260223],"xend":[1.9264439808635216,0.83134857557564446,1.3652517350910904,1.9711584470716856,2.6706959687339298,0.34945345657489035,1.6502457607887315,2.7564064385454836,1.4611908400224074,2.2272919216729421,2.4922285700644311,2.2678350153316833,2.6532576794713814,1.87729133902262,1.5863234864078797,-0.64314895202897882,1.9070589815205279,2.4302846963599869,2.53539884086815,1.4447216486867482,3.7795029097751489,2.2864244196288248,2.1263158584588857,3.2722667794685192,1.2815337786807408,1.5496613760752882,4.397452480049763,2.0111291872208881,3.6335684214083068,0.56149335508547682,1.8094831979570984,2.3784239036368318,2.3000385452057976,0.99436374048916321,2.0192592746253708,0.92257934688801813,2.7127033252442825,3.0847750898606461,-0.22498769648741268,3.2356934623001434,0.75895550319150895,2.4547692689826328,2.6599026381426083,1.8001101718985668,1.3548860431402083,2.1653210212420504,2.4388187007144535,2.883302819948705,-0.052336983901952738,0.36362073194097544,3.4304023411881834,3.0466288471115464,2.4352889489076821,2.7151784071110603,2.9171749176410033,-0.66092279846568136,3.11027709663882,1.5150124034376753,2.230616830631202,1.7048421991164227,2.8719649540787442,1.651527551044389,2.5185037658882479,1.609315021361021,0.90721279089849438,3.2100105104402648,2.7409000112742739,3.7242622391564528,2.0651539325793031,3.1250027458281737,3.9754190540190781,1.7185178849610334,0.6770488872447844,1.7606484328618839,1.7859587600146167,2.1516805045093199,3.7123049773192824,1.6738561074946519,2.3730046558049156,1.7723159351044688,2.0204507085250327,2.3140576635062624,3.3282146960369863,2.1213183774907782,2.7128423200311134,2.7788600297835346,2.9147732708556342,1.4256054478156166,3.6268812141946771,1.619043260705987,1.8942158323502483,3.4040502677141715,3.2940839061455804,0.91000812758876193,1.1269289995825362,0.64192094062938554,2.181847192619832,2.1648408679184663,2.3641146873550629,2.5521577142185579,1.3981071540149834,1.0063014089012858,3.0267850560749041,2.7510613026063231,0.4908334626712989,1.9048525491507133,1.1040521772435012,-0.070751070541964278,2.1501201311881433,1.9207882907414118,1.9026307324869189,2.2161525418012324,2.8824651643965873,2.2055975044833867,1.3835641571264716,1.2652007480787191,1.8681972069588191,2.3100169865789519,0.96031964729795849,1.8156911311576163,2.9672672601679135,1.8917199087655139,1.3015793324056097,1.7240548315744604,3.1146485453619803,2.5500439612141359,3.2366758000816054,2.1390978571617709,2.4102750964904249,1.4415430876518671,2.6053706689303331,1.4936664578107606,0.57943449530425561,2.1279929656841259,3.9458512177314358,2.8009143395730618,3.16525338994626,2.3588557230931522,1.3914428216837758,1.7977591448897163,1.7267518931880748,1.5313002202179129,2.7041672839297157,0.80263649763458811,2.8663661321127734,2.8641524862258931,0.80137764093125141,2.6394919979005338,4.4302266523445084,1.4427845180583918,2.8449042414080719,1.2177981543805965,3.1107114182028006,2.2498247199265626,3.6519153916231648,0.54102927146308444,1.9487021138746388,1.4730748238744023,1.802735130923701,1.3704212573139951,1.1661564194649994,2.5787223746164498,0.91241928592307309,3.4840309315562834,0.81379341468624955,2.1010791513952798,2.5329892868264805,2.5867353390926251,1.6982533361336196,2.0795019953284011,2.9612641518096776,0.54353408292357419,1.2182602884679179,2.3204023143166101,1.5552180216630085,3.3700039939709998,2.6732538633548355,2.072166752414756,0.49224268246331992,2.0261002253262417,1.6835841318030069,1.8976534864794021,0.81844077294431461,2.4986580438299244,0.96104355969296695,1.7737780185575904,2.3814258294513522,1.2164842109869523,2.5829914056800645,0.68348959834830758,-0.80977467896054156,2.4649679907858526,2.8405398269609998,1.7141545764086161,2.5041262545916081,0.84408347491632862,1.8728513931743378,0.058481623959382167,3.1811808914385216,3.859910861543161],"yend":[5.1988103488837201,4.3124129764335102,2.7348549433036471,3.5431940592320892,2.5856600520811401,2.5237531053844218,2.2113971621497575,2.4053827325404891,4.6509074673366904,2.9459718749145596,3.119245236427584,3.2436874295990918,4.2324758784853378,2.48393616905522,2.0074928496079627,4.6756969324031896,2.5588367830947147,2.2769340300601262,1.7637268811167071,1.7152842776822035,2.4260265207020133,3.6179858171665282,4.1098481389297188,3.7075883538355878,2.6363427029047477,3.0597499373846007,2.2954035363199301,2.2827818384259899,3.8846504989769199,1.9844074213964555,4.9552939654924639,2.9096804060341483,3.2145388266292163,2.2614722952604267,2.4256113102367305,1.6829838676947582,2.8170746116272727,3.4189824049244639,3.32430434416138,2.2184635129452488,2.2113780291459983,2.4978012816571393,4.4960606698463508,1.862696379334255,2.8209484056198018,4.9023618216789266,2.8990251146711916,1.6401592961786062,2.3352305647259377,3.4854599789048781,2.6243971283302265,2.438123636450217,2.6560827658715409,3.0904966471392212,4.5985087711458261,2.911434887861116,4.080799496151517,3.6307541156505665,2.8863601044938592,1.4670979971094018,2.4788826824474799,2.5101295468615255,3.0471544327615274,4.3001986776668204,5.2930789738310935,4.5475810589837691,2.8668490356710556,1.2434726044423627,2.6112201359282565,3.0892072230732945,3.845013004067436,3.9625279684842711,3.6843094294164644,1.6047256502005327,3.8496430456333552,2.5534427835727778,3.1748027001612558,3.0745511771737344,3.4281667649705057,3.0246749828261401,1.3325249024143371,3.7364959647734421,3.386026568349676,2.7343483747217778,3.1181445110466806,3.1340386453684634,3.2210194685610021,4.6408461659774858,2.7809496210665241,3.1680653838846573,4.168383873069093,4.0541810233769189,4.1452631103803572,2.422531998940443,5.0024827302928294,3.0667008709301831,4.8668518447068623,1.6490973139692886,3.0209835863542374,4.2499145709692154,2.2847578127772037,2.247311031782258,2.0614612963931065,1.9474867206612636,2.5628404668196012,3.3311791729589819,0.98578950207927996,3.2119804333722919,4.2366750464165701,5.0375740182404352,4.3011759922005872,3.7567747637959616,1.2732696008856701,2.3984932919932183,2.6479535434173833,3.7035239027568942,2.8943286659962255,1.7413513719398284,4.6844357080941146,3.9113912917959626,3.237430272491026,4.2181086103258121,1.6612257127650272,3.6608202977898006,2.477087623686578,3.6837455218507125,2.9391780453399257,3.6329607130314505,4.3355176150593921,3.0072900903168986,4.0175586369520921,1.811565964852021,2.2783955595639847,4.519217711388178,3.3773879730239296,0.94777717956626972,1.6359625479176245,2.7992189844108788,3.865779404334488,2.8981167442847777,3.6241874720206528,3.9590053777878254,4.671054828862939,3.0560167332749621,2.9480180938191012,1.2467626408577306,3.0993275940878275,2.4281499421044366,2.0259904171959087,2.8200937689524599,4.0149431727436573,1.0072515113114173,2.5727207127945713,3.1166372835827065,2.1067924299450471,3.333902942499225,3.4114299206157308,2.9669638407240084,0.53410180623997272,5.5714581458666377,2.7947007425318007,3.6511932815876702,3.273766491036548,4.0246732348183505,3.8176594463740878,2.7902068287714914,3.3781677722085068,2.0545911688761072,3.8569230108993184,2.5389616611156458,5.4167733537882103,1.3489511043118125,2.5360127570339852,3.8253798627592426,3.5101325468786637,2.4105189614849998,2.0032192577924777,3.1444757047106968,2.9856925868330868,1.2097187627359396,3.0345510671338509,3.1902303156924572,3.1747263969818413,1.9449829573973181,3.4761332783026289,4.3785701369592402,3.4562364031798127,1.8644115296256567,2.564354530308095,3.3461036195536069,2.3529543686817309,0.84235366498472297,3.8842508200282091,2.1705223883754821,2.426439729231717,4.5039006090045355,2.2258550703945952,3.8457315401892957,1.7393171211812251,2.6454575969260223],"type":"scatter","mode":"lines","z":[4.4395243534477871,4.76982251051672,6.5587083141491238,5.0705083914245757,5.129287735160946,6.7150649868832808,5.4609162059892027,3.7349387653934665,4.3131471481064736,4.554338029900042,6.224081797439462,5.3598138270573639,5.4007714505940525,5.1106827159451198,4.4441588652459254,6.7869131368030784,5.497850478229239,3.0333828433703616,5.7013559015636854,4.527208592272066,3.9321762940131548,4.7820250853417052,3.9739955516927603,4.2711087707088602,4.3749607321507433,3.3133066892575869,5.837787044494525,5.153373117836515,3.8618630629880526,6.2538149210699263,5.426464221476814,4.7049285170077288,5.8951256610450224,5.8781334875330424,5.8215810816374871,5.6886402541000907,5.5539176535375887,4.9380882894232787,4.6940373362600836,4.6195289989876169,4.3052930210794873,4.7920827219804014,3.7346036484317358,7.1689559653385126,6.2079619983049907,3.8768914167966502,4.5971151647009236,4.533344646376781,5.7799651183363174,4.9166309335281708,5.2533185139947545,4.9714532446512969,4.9571295427086843,6.3686022840144574,4.7742290143407322,6.5164706044295393,3.4512471957697786,5.5846137496360688,5.1238542438446135,5.2159415687439727,5.3796394827598819,4.4976765468906974,4.6667926163305795,3.9814246168929115,3.9282087735244229,5.3035286414042577,5.4482097786294261,5.0530042267305042,5.9222674678797373,7.0500846856271444,4.5089688339434648,2.6908311243591871,6.0057385244622568,4.2907992374176072,4.311991383532642,6.0255713696966993,4.7152269929489909,3.7792822877454646,5.1813034797491504,4.8611086375609558,5.0057641858998867,5.3852804011263302,4.6293399682075904,5.6443765485188333,4.7795134381812492,5.3317819639156969,6.0968390131493475,5.435181490833803,4.6740684144687732,6.1488076184510945,5.9935038559621194,5.5483969595080698,5.2387317351114415,4.3720939239606285,6.3606524485300078,4.3997404128528732,7.187332993016577,6.5326106261851891,4.7642996408995231,3.9735790996932194,4.2895934363006987,5.2568837091565292,4.7533081215376267,4.652457400602267,4.0483814327349847,4.9549722751910794,4.2150955305429241,3.3320580634118633,4.6197734797122374,5.9189966090607662,4.4246530373916082,5.6079643222250333,3.382117291710836,4.9444380344754606,5.5194072039434623,5.3011533621667146,5.105676194148943,4.3592939916946234,4.1502956539664178,3.9758712093950868,5.1176465971001255,4.0525253858151977,4.5094425562993319,4.7439078078017527,6.8438620052322072,4.348050098304542,5.2353865722848569,5.077960849563711,4.0381433658698711,4.9286919138764009,6.4445508584233488,5.4515040530792147,5.0412329219929397,4.5775031676603755,2.9467527784594845,6.1313372134141755,3.5393599290751778,5.739947510877335,6.9091035692174838,3.5561068390282005,5.7017843353747111,4.7378025105975317,3.4278558408545123,3.4853323462182484,3.3984638264254068,4.4690934778296967,3.5382444150041001,5.6879167729758278,7.1001089405256721,3.7129695239648211,5.7877388474751781,5.7690422410009097,5.3322025789501177,3.9916233917229924,4.8805473933693415,4.7196046648297534,5.5629895332204802,4.6275612438961709,5.9769733866856205,4.6254191422329862,6.0527114655793319,3.9508229933339338,3.7398447552418879,8.2410399349424051,4.5831424118395683,5.2982275915407158,5.6365696740338489,4.5162193742912562,5.5168620443136094,5.3689645273850859,4.7846194923583063,5.0652930335253155,4.9659327462615357,7.1284518990161807,4.2586639037271716,3.9040037329253359,5.0377883991710792,5.310480749443137,5.4365234789101828,4.5416346672888945,3.936673866028809,6.2631851760894897,4.6503496120464449,4.1344871373466257,4.7637204310589034,4.8028241056514478,6.1099202897136404,5.0847372921971967,5.7540537851845217,4.5007079828277394,5.2144453095816017,4.6753140885091655,5.0945835281735716,4.1046366420224585,3.6891984666720283,6.9972133847479663,5.6007088236724174,3.7487286383750562,4.3888340833195789,3.8145199154026894],"zend":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"line":{"color":"gray","width":1},"showlegend":false,"hoverinfo":"none","inherit":true},"216e7024b887.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[8.5,0,0],"y":[0,8.5,0],"z":[0,0,8.5],"text":["<i>q<\/i><sub>1<\/sub>","<i>q<\/i><sub>2<\/sub>","<i>V<\/i><sup>⊥<\/sup>"],"type":"scatter3d","mode":"text","textfont":{"size":15,"color":"black"},"showlegend":false,"inherit":true}},"layout":{"width":800,"height":450,"margin":{"b":0,"l":0,"t":30,"r":0},"scene":{"xaxis":{"title":"","showgrid":true,"gridcolor":"gray","gridwidth":0.5,"zeroline":false,"showline":false,"showticklabels":false,"showbackground":false,"showspikes":false},"yaxis":{"title":"","showgrid":true,"gridcolor":"gray","gridwidth":0.5,"zeroline":false,"showline":false,"showticklabels":false,"showbackground":false,"showspikes":false},"zaxis":{"title":"","showgrid":true,"gridcolor":"gray","gridwidth":0.5,"zeroline":false,"showline":false,"showticklabels":false,"showbackground":false,"showspikes":false},"aspectmode":"cube","camera":{"eye":{"x":1.6000000000000001,"y":1.6000000000000001,"z":1.3}}},"title":"Projection of Trivariate Normal onto 2D Subspace","autosize":false,"xaxis":{"domain":[0,1],"automargin":true,"title":[]},"yaxis":{"domain":[0,1],"automargin":true,"title":[]},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-2,8,8,-2],"y":[-2,-2,8,8],"z":[0,0,0,0],"type":"mesh3d","opacity":0.050000000000000003,"hoverinfo":"none","showlegend":false,"name":"gray","frame":null},{"x":[1.9264439808635216,0.83134857557564446,1.3652517350910904,1.9711584470716856,2.6706959687339298,0.34945345657489035,1.6502457607887315,2.7564064385454836,1.4611908400224074,2.2272919216729421,2.4922285700644311,2.2678350153316833,2.6532576794713814,1.87729133902262,1.5863234864078797,-0.64314895202897882,1.9070589815205279,2.4302846963599869,2.53539884086815,1.4447216486867482,3.7795029097751489,2.2864244196288248,2.1263158584588857,3.2722667794685192,1.2815337786807408,1.5496613760752882,4.397452480049763,2.0111291872208881,3.6335684214083068,0.56149335508547682,1.8094831979570984,2.3784239036368318,2.3000385452057976,0.99436374048916321,2.0192592746253708,0.92257934688801813,2.7127033252442825,3.0847750898606461,-0.22498769648741268,3.2356934623001434,0.75895550319150895,2.4547692689826328,2.6599026381426083,1.8001101718985668,1.3548860431402083,2.1653210212420504,2.4388187007144535,2.883302819948705,-0.052336983901952738,0.36362073194097544,3.4304023411881834,3.0466288471115464,2.4352889489076821,2.7151784071110603,2.9171749176410033,-0.66092279846568136,3.11027709663882,1.5150124034376753,2.230616830631202,1.7048421991164227,2.8719649540787442,1.651527551044389,2.5185037658882479,1.609315021361021,0.90721279089849438,3.2100105104402648,2.7409000112742739,3.7242622391564528,2.0651539325793031,3.1250027458281737,3.9754190540190781,1.7185178849610334,0.6770488872447844,1.7606484328618839,1.7859587600146167,2.1516805045093199,3.7123049773192824,1.6738561074946519,2.3730046558049156,1.7723159351044688,2.0204507085250327,2.3140576635062624,3.3282146960369863,2.1213183774907782,2.7128423200311134,2.7788600297835346,2.9147732708556342,1.4256054478156166,3.6268812141946771,1.619043260705987,1.8942158323502483,3.4040502677141715,3.2940839061455804,0.91000812758876193,1.1269289995825362,0.64192094062938554,2.181847192619832,2.1648408679184663,2.3641146873550629,2.5521577142185579,1.3981071540149834,1.0063014089012858,3.0267850560749041,2.7510613026063231,0.4908334626712989,1.9048525491507133,1.1040521772435012,-0.070751070541964278,2.1501201311881433,1.9207882907414118,1.9026307324869189,2.2161525418012324,2.8824651643965873,2.2055975044833867,1.3835641571264716,1.2652007480787191,1.8681972069588191,2.3100169865789519,0.96031964729795849,1.8156911311576163,2.9672672601679135,1.8917199087655139,1.3015793324056097,1.7240548315744604,3.1146485453619803,2.5500439612141359,3.2366758000816054,2.1390978571617709,2.4102750964904249,1.4415430876518671,2.6053706689303331,1.4936664578107606,0.57943449530425561,2.1279929656841259,3.9458512177314358,2.8009143395730618,3.16525338994626,2.3588557230931522,1.3914428216837758,1.7977591448897163,1.7267518931880748,1.5313002202179129,2.7041672839297157,0.80263649763458811,2.8663661321127734,2.8641524862258931,0.80137764093125141,2.6394919979005338,4.4302266523445084,1.4427845180583918,2.8449042414080719,1.2177981543805965,3.1107114182028006,2.2498247199265626,3.6519153916231648,0.54102927146308444,1.9487021138746388,1.4730748238744023,1.802735130923701,1.3704212573139951,1.1661564194649994,2.5787223746164498,0.91241928592307309,3.4840309315562834,0.81379341468624955,2.1010791513952798,2.5329892868264805,2.5867353390926251,1.6982533361336196,2.0795019953284011,2.9612641518096776,0.54353408292357419,1.2182602884679179,2.3204023143166101,1.5552180216630085,3.3700039939709998,2.6732538633548355,2.072166752414756,0.49224268246331992,2.0261002253262417,1.6835841318030069,1.8976534864794021,0.81844077294431461,2.4986580438299244,0.96104355969296695,1.7737780185575904,2.3814258294513522,1.2164842109869523,2.5829914056800645,0.68348959834830758,-0.80977467896054156,2.4649679907858526,2.8405398269609998,1.7141545764086161,2.5041262545916081,0.84408347491632862,1.8728513931743378,0.058481623959382167,3.1811808914385216,3.859910861543161],"y":[5.1988103488837201,4.3124129764335102,2.7348549433036471,3.5431940592320892,2.5856600520811401,2.5237531053844218,2.2113971621497575,2.4053827325404891,4.6509074673366904,2.9459718749145596,3.119245236427584,3.2436874295990918,4.2324758784853378,2.48393616905522,2.0074928496079627,4.6756969324031896,2.5588367830947147,2.2769340300601262,1.7637268811167071,1.7152842776822035,2.4260265207020133,3.6179858171665282,4.1098481389297188,3.7075883538355878,2.6363427029047477,3.0597499373846007,2.2954035363199301,2.2827818384259899,3.8846504989769199,1.9844074213964555,4.9552939654924639,2.9096804060341483,3.2145388266292163,2.2614722952604267,2.4256113102367305,1.6829838676947582,2.8170746116272727,3.4189824049244639,3.32430434416138,2.2184635129452488,2.2113780291459983,2.4978012816571393,4.4960606698463508,1.862696379334255,2.8209484056198018,4.9023618216789266,2.8990251146711916,1.6401592961786062,2.3352305647259377,3.4854599789048781,2.6243971283302265,2.438123636450217,2.6560827658715409,3.0904966471392212,4.5985087711458261,2.911434887861116,4.080799496151517,3.6307541156505665,2.8863601044938592,1.4670979971094018,2.4788826824474799,2.5101295468615255,3.0471544327615274,4.3001986776668204,5.2930789738310935,4.5475810589837691,2.8668490356710556,1.2434726044423627,2.6112201359282565,3.0892072230732945,3.845013004067436,3.9625279684842711,3.6843094294164644,1.6047256502005327,3.8496430456333552,2.5534427835727778,3.1748027001612558,3.0745511771737344,3.4281667649705057,3.0246749828261401,1.3325249024143371,3.7364959647734421,3.386026568349676,2.7343483747217778,3.1181445110466806,3.1340386453684634,3.2210194685610021,4.6408461659774858,2.7809496210665241,3.1680653838846573,4.168383873069093,4.0541810233769189,4.1452631103803572,2.422531998940443,5.0024827302928294,3.0667008709301831,4.8668518447068623,1.6490973139692886,3.0209835863542374,4.2499145709692154,2.2847578127772037,2.247311031782258,2.0614612963931065,1.9474867206612636,2.5628404668196012,3.3311791729589819,0.98578950207927996,3.2119804333722919,4.2366750464165701,5.0375740182404352,4.3011759922005872,3.7567747637959616,1.2732696008856701,2.3984932919932183,2.6479535434173833,3.7035239027568942,2.8943286659962255,1.7413513719398284,4.6844357080941146,3.9113912917959626,3.237430272491026,4.2181086103258121,1.6612257127650272,3.6608202977898006,2.477087623686578,3.6837455218507125,2.9391780453399257,3.6329607130314505,4.3355176150593921,3.0072900903168986,4.0175586369520921,1.811565964852021,2.2783955595639847,4.519217711388178,3.3773879730239296,0.94777717956626972,1.6359625479176245,2.7992189844108788,3.865779404334488,2.8981167442847777,3.6241874720206528,3.9590053777878254,4.671054828862939,3.0560167332749621,2.9480180938191012,1.2467626408577306,3.0993275940878275,2.4281499421044366,2.0259904171959087,2.8200937689524599,4.0149431727436573,1.0072515113114173,2.5727207127945713,3.1166372835827065,2.1067924299450471,3.333902942499225,3.4114299206157308,2.9669638407240084,0.53410180623997272,5.5714581458666377,2.7947007425318007,3.6511932815876702,3.273766491036548,4.0246732348183505,3.8176594463740878,2.7902068287714914,3.3781677722085068,2.0545911688761072,3.8569230108993184,2.5389616611156458,5.4167733537882103,1.3489511043118125,2.5360127570339852,3.8253798627592426,3.5101325468786637,2.4105189614849998,2.0032192577924777,3.1444757047106968,2.9856925868330868,1.2097187627359396,3.0345510671338509,3.1902303156924572,3.1747263969818413,1.9449829573973181,3.4761332783026289,4.3785701369592402,3.4562364031798127,1.8644115296256567,2.564354530308095,3.3461036195536069,2.3529543686817309,0.84235366498472297,3.8842508200282091,2.1705223883754821,2.426439729231717,4.5039006090045355,2.2258550703945952,3.8457315401892957,1.7393171211812251,2.6454575969260223],"z":[4.4395243534477871,4.76982251051672,6.5587083141491238,5.0705083914245757,5.129287735160946,6.7150649868832808,5.4609162059892027,3.7349387653934665,4.3131471481064736,4.554338029900042,6.224081797439462,5.3598138270573639,5.4007714505940525,5.1106827159451198,4.4441588652459254,6.7869131368030784,5.497850478229239,3.0333828433703616,5.7013559015636854,4.527208592272066,3.9321762940131548,4.7820250853417052,3.9739955516927603,4.2711087707088602,4.3749607321507433,3.3133066892575869,5.837787044494525,5.153373117836515,3.8618630629880526,6.2538149210699263,5.426464221476814,4.7049285170077288,5.8951256610450224,5.8781334875330424,5.8215810816374871,5.6886402541000907,5.5539176535375887,4.9380882894232787,4.6940373362600836,4.6195289989876169,4.3052930210794873,4.7920827219804014,3.7346036484317358,7.1689559653385126,6.2079619983049907,3.8768914167966502,4.5971151647009236,4.533344646376781,5.7799651183363174,4.9166309335281708,5.2533185139947545,4.9714532446512969,4.9571295427086843,6.3686022840144574,4.7742290143407322,6.5164706044295393,3.4512471957697786,5.5846137496360688,5.1238542438446135,5.2159415687439727,5.3796394827598819,4.4976765468906974,4.6667926163305795,3.9814246168929115,3.9282087735244229,5.3035286414042577,5.4482097786294261,5.0530042267305042,5.9222674678797373,7.0500846856271444,4.5089688339434648,2.6908311243591871,6.0057385244622568,4.2907992374176072,4.311991383532642,6.0255713696966993,4.7152269929489909,3.7792822877454646,5.1813034797491504,4.8611086375609558,5.0057641858998867,5.3852804011263302,4.6293399682075904,5.6443765485188333,4.7795134381812492,5.3317819639156969,6.0968390131493475,5.435181490833803,4.6740684144687732,6.1488076184510945,5.9935038559621194,5.5483969595080698,5.2387317351114415,4.3720939239606285,6.3606524485300078,4.3997404128528732,7.187332993016577,6.5326106261851891,4.7642996408995231,3.9735790996932194,4.2895934363006987,5.2568837091565292,4.7533081215376267,4.652457400602267,4.0483814327349847,4.9549722751910794,4.2150955305429241,3.3320580634118633,4.6197734797122374,5.9189966090607662,4.4246530373916082,5.6079643222250333,3.382117291710836,4.9444380344754606,5.5194072039434623,5.3011533621667146,5.105676194148943,4.3592939916946234,4.1502956539664178,3.9758712093950868,5.1176465971001255,4.0525253858151977,4.5094425562993319,4.7439078078017527,6.8438620052322072,4.348050098304542,5.2353865722848569,5.077960849563711,4.0381433658698711,4.9286919138764009,6.4445508584233488,5.4515040530792147,5.0412329219929397,4.5775031676603755,2.9467527784594845,6.1313372134141755,3.5393599290751778,5.739947510877335,6.9091035692174838,3.5561068390282005,5.7017843353747111,4.7378025105975317,3.4278558408545123,3.4853323462182484,3.3984638264254068,4.4690934778296967,3.5382444150041001,5.6879167729758278,7.1001089405256721,3.7129695239648211,5.7877388474751781,5.7690422410009097,5.3322025789501177,3.9916233917229924,4.8805473933693415,4.7196046648297534,5.5629895332204802,4.6275612438961709,5.9769733866856205,4.6254191422329862,6.0527114655793319,3.9508229933339338,3.7398447552418879,8.2410399349424051,4.5831424118395683,5.2982275915407158,5.6365696740338489,4.5162193742912562,5.5168620443136094,5.3689645273850859,4.7846194923583063,5.0652930335253155,4.9659327462615357,7.1284518990161807,4.2586639037271716,3.9040037329253359,5.0377883991710792,5.310480749443137,5.4365234789101828,4.5416346672888945,3.936673866028809,6.2631851760894897,4.6503496120464449,4.1344871373466257,4.7637204310589034,4.8028241056514478,6.1099202897136404,5.0847372921971967,5.7540537851845217,4.5007079828277394,5.2144453095816017,4.6753140885091655,5.0945835281735716,4.1046366420224585,3.6891984666720283,6.9972133847479663,5.6007088236724174,3.7487286383750562,4.3888340833195789,3.8145199154026894],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":3,"opacity":0.59999999999999998,"line":{"color":"rgba(255,127,14,1)"}},"name":"<i>y<\/i>","error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"x":[1.9264439808635216,0.83134857557564446,1.3652517350910904,1.9711584470716856,2.6706959687339298,0.34945345657489035,1.6502457607887315,2.7564064385454836,1.4611908400224074,2.2272919216729421,2.4922285700644311,2.2678350153316833,2.6532576794713814,1.87729133902262,1.5863234864078797,-0.64314895202897882,1.9070589815205279,2.4302846963599869,2.53539884086815,1.4447216486867482,3.7795029097751489,2.2864244196288248,2.1263158584588857,3.2722667794685192,1.2815337786807408,1.5496613760752882,4.397452480049763,2.0111291872208881,3.6335684214083068,0.56149335508547682,1.8094831979570984,2.3784239036368318,2.3000385452057976,0.99436374048916321,2.0192592746253708,0.92257934688801813,2.7127033252442825,3.0847750898606461,-0.22498769648741268,3.2356934623001434,0.75895550319150895,2.4547692689826328,2.6599026381426083,1.8001101718985668,1.3548860431402083,2.1653210212420504,2.4388187007144535,2.883302819948705,-0.052336983901952738,0.36362073194097544,3.4304023411881834,3.0466288471115464,2.4352889489076821,2.7151784071110603,2.9171749176410033,-0.66092279846568136,3.11027709663882,1.5150124034376753,2.230616830631202,1.7048421991164227,2.8719649540787442,1.651527551044389,2.5185037658882479,1.609315021361021,0.90721279089849438,3.2100105104402648,2.7409000112742739,3.7242622391564528,2.0651539325793031,3.1250027458281737,3.9754190540190781,1.7185178849610334,0.6770488872447844,1.7606484328618839,1.7859587600146167,2.1516805045093199,3.7123049773192824,1.6738561074946519,2.3730046558049156,1.7723159351044688,2.0204507085250327,2.3140576635062624,3.3282146960369863,2.1213183774907782,2.7128423200311134,2.7788600297835346,2.9147732708556342,1.4256054478156166,3.6268812141946771,1.619043260705987,1.8942158323502483,3.4040502677141715,3.2940839061455804,0.91000812758876193,1.1269289995825362,0.64192094062938554,2.181847192619832,2.1648408679184663,2.3641146873550629,2.5521577142185579,1.3981071540149834,1.0063014089012858,3.0267850560749041,2.7510613026063231,0.4908334626712989,1.9048525491507133,1.1040521772435012,-0.070751070541964278,2.1501201311881433,1.9207882907414118,1.9026307324869189,2.2161525418012324,2.8824651643965873,2.2055975044833867,1.3835641571264716,1.2652007480787191,1.8681972069588191,2.3100169865789519,0.96031964729795849,1.8156911311576163,2.9672672601679135,1.8917199087655139,1.3015793324056097,1.7240548315744604,3.1146485453619803,2.5500439612141359,3.2366758000816054,2.1390978571617709,2.4102750964904249,1.4415430876518671,2.6053706689303331,1.4936664578107606,0.57943449530425561,2.1279929656841259,3.9458512177314358,2.8009143395730618,3.16525338994626,2.3588557230931522,1.3914428216837758,1.7977591448897163,1.7267518931880748,1.5313002202179129,2.7041672839297157,0.80263649763458811,2.8663661321127734,2.8641524862258931,0.80137764093125141,2.6394919979005338,4.4302266523445084,1.4427845180583918,2.8449042414080719,1.2177981543805965,3.1107114182028006,2.2498247199265626,3.6519153916231648,0.54102927146308444,1.9487021138746388,1.4730748238744023,1.802735130923701,1.3704212573139951,1.1661564194649994,2.5787223746164498,0.91241928592307309,3.4840309315562834,0.81379341468624955,2.1010791513952798,2.5329892868264805,2.5867353390926251,1.6982533361336196,2.0795019953284011,2.9612641518096776,0.54353408292357419,1.2182602884679179,2.3204023143166101,1.5552180216630085,3.3700039939709998,2.6732538633548355,2.072166752414756,0.49224268246331992,2.0261002253262417,1.6835841318030069,1.8976534864794021,0.81844077294431461,2.4986580438299244,0.96104355969296695,1.7737780185575904,2.3814258294513522,1.2164842109869523,2.5829914056800645,0.68348959834830758,-0.80977467896054156,2.4649679907858526,2.8405398269609998,1.7141545764086161,2.5041262545916081,0.84408347491632862,1.8728513931743378,0.058481623959382167,3.1811808914385216,3.859910861543161],"y":[5.1988103488837201,4.3124129764335102,2.7348549433036471,3.5431940592320892,2.5856600520811401,2.5237531053844218,2.2113971621497575,2.4053827325404891,4.6509074673366904,2.9459718749145596,3.119245236427584,3.2436874295990918,4.2324758784853378,2.48393616905522,2.0074928496079627,4.6756969324031896,2.5588367830947147,2.2769340300601262,1.7637268811167071,1.7152842776822035,2.4260265207020133,3.6179858171665282,4.1098481389297188,3.7075883538355878,2.6363427029047477,3.0597499373846007,2.2954035363199301,2.2827818384259899,3.8846504989769199,1.9844074213964555,4.9552939654924639,2.9096804060341483,3.2145388266292163,2.2614722952604267,2.4256113102367305,1.6829838676947582,2.8170746116272727,3.4189824049244639,3.32430434416138,2.2184635129452488,2.2113780291459983,2.4978012816571393,4.4960606698463508,1.862696379334255,2.8209484056198018,4.9023618216789266,2.8990251146711916,1.6401592961786062,2.3352305647259377,3.4854599789048781,2.6243971283302265,2.438123636450217,2.6560827658715409,3.0904966471392212,4.5985087711458261,2.911434887861116,4.080799496151517,3.6307541156505665,2.8863601044938592,1.4670979971094018,2.4788826824474799,2.5101295468615255,3.0471544327615274,4.3001986776668204,5.2930789738310935,4.5475810589837691,2.8668490356710556,1.2434726044423627,2.6112201359282565,3.0892072230732945,3.845013004067436,3.9625279684842711,3.6843094294164644,1.6047256502005327,3.8496430456333552,2.5534427835727778,3.1748027001612558,3.0745511771737344,3.4281667649705057,3.0246749828261401,1.3325249024143371,3.7364959647734421,3.386026568349676,2.7343483747217778,3.1181445110466806,3.1340386453684634,3.2210194685610021,4.6408461659774858,2.7809496210665241,3.1680653838846573,4.168383873069093,4.0541810233769189,4.1452631103803572,2.422531998940443,5.0024827302928294,3.0667008709301831,4.8668518447068623,1.6490973139692886,3.0209835863542374,4.2499145709692154,2.2847578127772037,2.247311031782258,2.0614612963931065,1.9474867206612636,2.5628404668196012,3.3311791729589819,0.98578950207927996,3.2119804333722919,4.2366750464165701,5.0375740182404352,4.3011759922005872,3.7567747637959616,1.2732696008856701,2.3984932919932183,2.6479535434173833,3.7035239027568942,2.8943286659962255,1.7413513719398284,4.6844357080941146,3.9113912917959626,3.237430272491026,4.2181086103258121,1.6612257127650272,3.6608202977898006,2.477087623686578,3.6837455218507125,2.9391780453399257,3.6329607130314505,4.3355176150593921,3.0072900903168986,4.0175586369520921,1.811565964852021,2.2783955595639847,4.519217711388178,3.3773879730239296,0.94777717956626972,1.6359625479176245,2.7992189844108788,3.865779404334488,2.8981167442847777,3.6241874720206528,3.9590053777878254,4.671054828862939,3.0560167332749621,2.9480180938191012,1.2467626408577306,3.0993275940878275,2.4281499421044366,2.0259904171959087,2.8200937689524599,4.0149431727436573,1.0072515113114173,2.5727207127945713,3.1166372835827065,2.1067924299450471,3.333902942499225,3.4114299206157308,2.9669638407240084,0.53410180623997272,5.5714581458666377,2.7947007425318007,3.6511932815876702,3.273766491036548,4.0246732348183505,3.8176594463740878,2.7902068287714914,3.3781677722085068,2.0545911688761072,3.8569230108993184,2.5389616611156458,5.4167733537882103,1.3489511043118125,2.5360127570339852,3.8253798627592426,3.5101325468786637,2.4105189614849998,2.0032192577924777,3.1444757047106968,2.9856925868330868,1.2097187627359396,3.0345510671338509,3.1902303156924572,3.1747263969818413,1.9449829573973181,3.4761332783026289,4.3785701369592402,3.4562364031798127,1.8644115296256567,2.564354530308095,3.3461036195536069,2.3529543686817309,0.84235366498472297,3.8842508200282091,2.1705223883754821,2.426439729231717,4.5039006090045355,2.2258550703945952,3.8457315401892957,1.7393171211812251,2.6454575969260223],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":3,"opacity":0.80000000000000004,"line":{"color":"rgba(44,160,44,1)"}},"name":"<i>P<\/i><sub>V<\/sub><i>y<\/i>","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"x":[1.9264439808635216,1.9264439808635216,null,0.83134857557564446,0.83134857557564446,null,1.3652517350910904,1.3652517350910904,null,1.9711584470716856,1.9711584470716856,null,2.6706959687339298,2.6706959687339298,null,0.34945345657489035,0.34945345657489035,null,1.6502457607887315,1.6502457607887315,null,2.7564064385454836,2.7564064385454836,null,1.4611908400224074,1.4611908400224074,null,2.2272919216729421,2.2272919216729421,null,2.4922285700644311,2.4922285700644311,null,2.2678350153316833,2.2678350153316833,null,2.6532576794713814,2.6532576794713814,null,1.87729133902262,1.87729133902262,null,1.5863234864078797,1.5863234864078797,null,-0.64314895202897882,-0.64314895202897882,null,1.9070589815205279,1.9070589815205279,null,2.4302846963599869,2.4302846963599869,null,2.53539884086815,2.53539884086815,null,1.4447216486867482,1.4447216486867482,null,3.7795029097751489,3.7795029097751489,null,2.2864244196288248,2.2864244196288248,null,2.1263158584588857,2.1263158584588857,null,3.2722667794685192,3.2722667794685192,null,1.2815337786807408,1.2815337786807408,null,1.5496613760752882,1.5496613760752882,null,4.397452480049763,4.397452480049763,null,2.0111291872208881,2.0111291872208881,null,3.6335684214083068,3.6335684214083068,null,0.56149335508547682,0.56149335508547682,null,1.8094831979570984,1.8094831979570984,null,2.3784239036368318,2.3784239036368318,null,2.3000385452057976,2.3000385452057976,null,0.99436374048916321,0.99436374048916321,null,2.0192592746253708,2.0192592746253708,null,0.92257934688801813,0.92257934688801813,null,2.7127033252442825,2.7127033252442825,null,3.0847750898606461,3.0847750898606461,null,-0.22498769648741268,-0.22498769648741268,null,3.2356934623001434,3.2356934623001434,null,0.75895550319150895,0.75895550319150895,null,2.4547692689826328,2.4547692689826328,null,2.6599026381426083,2.6599026381426083,null,1.8001101718985668,1.8001101718985668,null,1.3548860431402083,1.3548860431402083,null,2.1653210212420504,2.1653210212420504,null,2.4388187007144535,2.4388187007144535,null,2.883302819948705,2.883302819948705,null,-0.052336983901952738,-0.052336983901952738,null,0.36362073194097544,0.36362073194097544,null,3.4304023411881834,3.4304023411881834,null,3.0466288471115464,3.0466288471115464,null,2.4352889489076821,2.4352889489076821,null,2.7151784071110603,2.7151784071110603,null,2.9171749176410033,2.9171749176410033,null,-0.66092279846568136,-0.66092279846568136,null,3.11027709663882,3.11027709663882,null,1.5150124034376753,1.5150124034376753,null,2.230616830631202,2.230616830631202,null,1.7048421991164227,1.7048421991164227,null,2.8719649540787442,2.8719649540787442,null,1.651527551044389,1.651527551044389,null,2.5185037658882479,2.5185037658882479,null,1.609315021361021,1.609315021361021,null,0.90721279089849438,0.90721279089849438,null,3.2100105104402648,3.2100105104402648,null,2.7409000112742739,2.7409000112742739,null,3.7242622391564528,3.7242622391564528,null,2.0651539325793031,2.0651539325793031,null,3.1250027458281737,3.1250027458281737,null,3.9754190540190781,3.9754190540190781,null,1.7185178849610334,1.7185178849610334,null,0.6770488872447844,0.6770488872447844,null,1.7606484328618839,1.7606484328618839,null,1.7859587600146167,1.7859587600146167,null,2.1516805045093199,2.1516805045093199,null,3.7123049773192824,3.7123049773192824,null,1.6738561074946519,1.6738561074946519,null,2.3730046558049156,2.3730046558049156,null,1.7723159351044688,1.7723159351044688,null,2.0204507085250327,2.0204507085250327,null,2.3140576635062624,2.3140576635062624,null,3.3282146960369863,3.3282146960369863,null,2.1213183774907782,2.1213183774907782,null,2.7128423200311134,2.7128423200311134,null,2.7788600297835346,2.7788600297835346,null,2.9147732708556342,2.9147732708556342,null,1.4256054478156166,1.4256054478156166,null,3.6268812141946771,3.6268812141946771,null,1.619043260705987,1.619043260705987,null,1.8942158323502483,1.8942158323502483,null,3.4040502677141715,3.4040502677141715,null,3.2940839061455804,3.2940839061455804,null,0.91000812758876193,0.91000812758876193,null,1.1269289995825362,1.1269289995825362,null,0.64192094062938554,0.64192094062938554,null,2.181847192619832,2.181847192619832,null,2.1648408679184663,2.1648408679184663,null,2.3641146873550629,2.3641146873550629,null,2.5521577142185579,2.5521577142185579,null,1.3981071540149834,1.3981071540149834,null,1.0063014089012858,1.0063014089012858,null,3.0267850560749041,3.0267850560749041,null,2.7510613026063231,2.7510613026063231,null,0.4908334626712989,0.4908334626712989,null,1.9048525491507133,1.9048525491507133,null,1.1040521772435012,1.1040521772435012,null,-0.070751070541964278,-0.070751070541964278,null,2.1501201311881433,2.1501201311881433,null,1.9207882907414118,1.9207882907414118,null,1.9026307324869189,1.9026307324869189,null,2.2161525418012324,2.2161525418012324,null,2.8824651643965873,2.8824651643965873,null,2.2055975044833867,2.2055975044833867,null,1.3835641571264716,1.3835641571264716,null,1.2652007480787191,1.2652007480787191,null,1.8681972069588191,1.8681972069588191,null,2.3100169865789519,2.3100169865789519,null,0.96031964729795849,0.96031964729795849,null,1.8156911311576163,1.8156911311576163,null,2.9672672601679135,2.9672672601679135,null,1.8917199087655139,1.8917199087655139,null,1.3015793324056097,1.3015793324056097,null,1.7240548315744604,1.7240548315744604,null,3.1146485453619803,3.1146485453619803,null,2.5500439612141359,2.5500439612141359,null,3.2366758000816054,3.2366758000816054,null,2.1390978571617709,2.1390978571617709,null,2.4102750964904249,2.4102750964904249,null,1.4415430876518671,1.4415430876518671,null,2.6053706689303331,2.6053706689303331,null,1.4936664578107606,1.4936664578107606,null,0.57943449530425561,0.57943449530425561,null,2.1279929656841259,2.1279929656841259,null,3.9458512177314358,3.9458512177314358,null,2.8009143395730618,2.8009143395730618,null,3.16525338994626,3.16525338994626,null,2.3588557230931522,2.3588557230931522,null,1.3914428216837758,1.3914428216837758,null,1.7977591448897163,1.7977591448897163,null,1.7267518931880748,1.7267518931880748,null,1.5313002202179129,1.5313002202179129,null,2.7041672839297157,2.7041672839297157,null,0.80263649763458811,0.80263649763458811,null,2.8663661321127734,2.8663661321127734,null,2.8641524862258931,2.8641524862258931,null,0.80137764093125141,0.80137764093125141,null,2.6394919979005338,2.6394919979005338,null,4.4302266523445084,4.4302266523445084,null,1.4427845180583918,1.4427845180583918,null,2.8449042414080719,2.8449042414080719,null,1.2177981543805965,1.2177981543805965,null,3.1107114182028006,3.1107114182028006,null,2.2498247199265626,2.2498247199265626,null,3.6519153916231648,3.6519153916231648,null,0.54102927146308444,0.54102927146308444,null,1.9487021138746388,1.9487021138746388,null,1.4730748238744023,1.4730748238744023,null,1.802735130923701,1.802735130923701,null,1.3704212573139951,1.3704212573139951,null,1.1661564194649994,1.1661564194649994,null,2.5787223746164498,2.5787223746164498,null,0.91241928592307309,0.91241928592307309,null,3.4840309315562834,3.4840309315562834,null,0.81379341468624955,0.81379341468624955,null,2.1010791513952798,2.1010791513952798,null,2.5329892868264805,2.5329892868264805,null,2.5867353390926251,2.5867353390926251,null,1.6982533361336196,1.6982533361336196,null,2.0795019953284011,2.0795019953284011,null,2.9612641518096776,2.9612641518096776,null,0.54353408292357419,0.54353408292357419,null,1.2182602884679179,1.2182602884679179,null,2.3204023143166101,2.3204023143166101,null,1.5552180216630085,1.5552180216630085,null,3.3700039939709998,3.3700039939709998,null,2.6732538633548355,2.6732538633548355,null,2.072166752414756,2.072166752414756,null,0.49224268246331992,0.49224268246331992,null,2.0261002253262417,2.0261002253262417,null,1.6835841318030069,1.6835841318030069,null,1.8976534864794021,1.8976534864794021,null,0.81844077294431461,0.81844077294431461,null,2.4986580438299244,2.4986580438299244,null,0.96104355969296695,0.96104355969296695,null,1.7737780185575904,1.7737780185575904,null,2.3814258294513522,2.3814258294513522,null,1.2164842109869523,1.2164842109869523,null,2.5829914056800645,2.5829914056800645,null,0.68348959834830758,0.68348959834830758,null,-0.80977467896054156,-0.80977467896054156,null,2.4649679907858526,2.4649679907858526,null,2.8405398269609998,2.8405398269609998,null,1.7141545764086161,1.7141545764086161,null,2.5041262545916081,2.5041262545916081,null,0.84408347491632862,0.84408347491632862,null,1.8728513931743378,1.8728513931743378,null,0.058481623959382167,0.058481623959382167,null,3.1811808914385216,3.1811808914385216,null,3.859910861543161,3.859910861543161],"y":[5.1988103488837201,5.1988103488837201,null,4.3124129764335102,4.3124129764335102,null,2.7348549433036471,2.7348549433036471,null,3.5431940592320892,3.5431940592320892,null,2.5856600520811401,2.5856600520811401,null,2.5237531053844218,2.5237531053844218,null,2.2113971621497575,2.2113971621497575,null,2.4053827325404891,2.4053827325404891,null,4.6509074673366904,4.6509074673366904,null,2.9459718749145596,2.9459718749145596,null,3.119245236427584,3.119245236427584,null,3.2436874295990918,3.2436874295990918,null,4.2324758784853378,4.2324758784853378,null,2.48393616905522,2.48393616905522,null,2.0074928496079627,2.0074928496079627,null,4.6756969324031896,4.6756969324031896,null,2.5588367830947147,2.5588367830947147,null,2.2769340300601262,2.2769340300601262,null,1.7637268811167071,1.7637268811167071,null,1.7152842776822035,1.7152842776822035,null,2.4260265207020133,2.4260265207020133,null,3.6179858171665282,3.6179858171665282,null,4.1098481389297188,4.1098481389297188,null,3.7075883538355878,3.7075883538355878,null,2.6363427029047477,2.6363427029047477,null,3.0597499373846007,3.0597499373846007,null,2.2954035363199301,2.2954035363199301,null,2.2827818384259899,2.2827818384259899,null,3.8846504989769199,3.8846504989769199,null,1.9844074213964555,1.9844074213964555,null,4.9552939654924639,4.9552939654924639,null,2.9096804060341483,2.9096804060341483,null,3.2145388266292163,3.2145388266292163,null,2.2614722952604267,2.2614722952604267,null,2.4256113102367305,2.4256113102367305,null,1.6829838676947582,1.6829838676947582,null,2.8170746116272727,2.8170746116272727,null,3.4189824049244639,3.4189824049244639,null,3.32430434416138,3.32430434416138,null,2.2184635129452488,2.2184635129452488,null,2.2113780291459983,2.2113780291459983,null,2.4978012816571393,2.4978012816571393,null,4.4960606698463508,4.4960606698463508,null,1.862696379334255,1.862696379334255,null,2.8209484056198018,2.8209484056198018,null,4.9023618216789266,4.9023618216789266,null,2.8990251146711916,2.8990251146711916,null,1.6401592961786062,1.6401592961786062,null,2.3352305647259377,2.3352305647259377,null,3.4854599789048781,3.4854599789048781,null,2.6243971283302265,2.6243971283302265,null,2.438123636450217,2.438123636450217,null,2.6560827658715409,2.6560827658715409,null,3.0904966471392212,3.0904966471392212,null,4.5985087711458261,4.5985087711458261,null,2.911434887861116,2.911434887861116,null,4.080799496151517,4.080799496151517,null,3.6307541156505665,3.6307541156505665,null,2.8863601044938592,2.8863601044938592,null,1.4670979971094018,1.4670979971094018,null,2.4788826824474799,2.4788826824474799,null,2.5101295468615255,2.5101295468615255,null,3.0471544327615274,3.0471544327615274,null,4.3001986776668204,4.3001986776668204,null,5.2930789738310935,5.2930789738310935,null,4.5475810589837691,4.5475810589837691,null,2.8668490356710556,2.8668490356710556,null,1.2434726044423627,1.2434726044423627,null,2.6112201359282565,2.6112201359282565,null,3.0892072230732945,3.0892072230732945,null,3.845013004067436,3.845013004067436,null,3.9625279684842711,3.9625279684842711,null,3.6843094294164644,3.6843094294164644,null,1.6047256502005327,1.6047256502005327,null,3.8496430456333552,3.8496430456333552,null,2.5534427835727778,2.5534427835727778,null,3.1748027001612558,3.1748027001612558,null,3.0745511771737344,3.0745511771737344,null,3.4281667649705057,3.4281667649705057,null,3.0246749828261401,3.0246749828261401,null,1.3325249024143371,1.3325249024143371,null,3.7364959647734421,3.7364959647734421,null,3.386026568349676,3.386026568349676,null,2.7343483747217778,2.7343483747217778,null,3.1181445110466806,3.1181445110466806,null,3.1340386453684634,3.1340386453684634,null,3.2210194685610021,3.2210194685610021,null,4.6408461659774858,4.6408461659774858,null,2.7809496210665241,2.7809496210665241,null,3.1680653838846573,3.1680653838846573,null,4.168383873069093,4.168383873069093,null,4.0541810233769189,4.0541810233769189,null,4.1452631103803572,4.1452631103803572,null,2.422531998940443,2.422531998940443,null,5.0024827302928294,5.0024827302928294,null,3.0667008709301831,3.0667008709301831,null,4.8668518447068623,4.8668518447068623,null,1.6490973139692886,1.6490973139692886,null,3.0209835863542374,3.0209835863542374,null,4.2499145709692154,4.2499145709692154,null,2.2847578127772037,2.2847578127772037,null,2.247311031782258,2.247311031782258,null,2.0614612963931065,2.0614612963931065,null,1.9474867206612636,1.9474867206612636,null,2.5628404668196012,2.5628404668196012,null,3.3311791729589819,3.3311791729589819,null,0.98578950207927996,0.98578950207927996,null,3.2119804333722919,3.2119804333722919,null,4.2366750464165701,4.2366750464165701,null,5.0375740182404352,5.0375740182404352,null,4.3011759922005872,4.3011759922005872,null,3.7567747637959616,3.7567747637959616,null,1.2732696008856701,1.2732696008856701,null,2.3984932919932183,2.3984932919932183,null,2.6479535434173833,2.6479535434173833,null,3.7035239027568942,3.7035239027568942,null,2.8943286659962255,2.8943286659962255,null,1.7413513719398284,1.7413513719398284,null,4.6844357080941146,4.6844357080941146,null,3.9113912917959626,3.9113912917959626,null,3.237430272491026,3.237430272491026,null,4.2181086103258121,4.2181086103258121,null,1.6612257127650272,1.6612257127650272,null,3.6608202977898006,3.6608202977898006,null,2.477087623686578,2.477087623686578,null,3.6837455218507125,3.6837455218507125,null,2.9391780453399257,2.9391780453399257,null,3.6329607130314505,3.6329607130314505,null,4.3355176150593921,4.3355176150593921,null,3.0072900903168986,3.0072900903168986,null,4.0175586369520921,4.0175586369520921,null,1.811565964852021,1.811565964852021,null,2.2783955595639847,2.2783955595639847,null,4.519217711388178,4.519217711388178,null,3.3773879730239296,3.3773879730239296,null,0.94777717956626972,0.94777717956626972,null,1.6359625479176245,1.6359625479176245,null,2.7992189844108788,2.7992189844108788,null,3.865779404334488,3.865779404334488,null,2.8981167442847777,2.8981167442847777,null,3.6241874720206528,3.6241874720206528,null,3.9590053777878254,3.9590053777878254,null,4.671054828862939,4.671054828862939,null,3.0560167332749621,3.0560167332749621,null,2.9480180938191012,2.9480180938191012,null,1.2467626408577306,1.2467626408577306,null,3.0993275940878275,3.0993275940878275,null,2.4281499421044366,2.4281499421044366,null,2.0259904171959087,2.0259904171959087,null,2.8200937689524599,2.8200937689524599,null,4.0149431727436573,4.0149431727436573,null,1.0072515113114173,1.0072515113114173,null,2.5727207127945713,2.5727207127945713,null,3.1166372835827065,3.1166372835827065,null,2.1067924299450471,2.1067924299450471,null,3.333902942499225,3.333902942499225,null,3.4114299206157308,3.4114299206157308,null,2.9669638407240084,2.9669638407240084,null,0.53410180623997272,0.53410180623997272,null,5.5714581458666377,5.5714581458666377,null,2.7947007425318007,2.7947007425318007,null,3.6511932815876702,3.6511932815876702,null,3.273766491036548,3.273766491036548,null,4.0246732348183505,4.0246732348183505,null,3.8176594463740878,3.8176594463740878,null,2.7902068287714914,2.7902068287714914,null,3.3781677722085068,3.3781677722085068,null,2.0545911688761072,2.0545911688761072,null,3.8569230108993184,3.8569230108993184,null,2.5389616611156458,2.5389616611156458,null,5.4167733537882103,5.4167733537882103,null,1.3489511043118125,1.3489511043118125,null,2.5360127570339852,2.5360127570339852,null,3.8253798627592426,3.8253798627592426,null,3.5101325468786637,3.5101325468786637,null,2.4105189614849998,2.4105189614849998,null,2.0032192577924777,2.0032192577924777,null,3.1444757047106968,3.1444757047106968,null,2.9856925868330868,2.9856925868330868,null,1.2097187627359396,1.2097187627359396,null,3.0345510671338509,3.0345510671338509,null,3.1902303156924572,3.1902303156924572,null,3.1747263969818413,3.1747263969818413,null,1.9449829573973181,1.9449829573973181,null,3.4761332783026289,3.4761332783026289,null,4.3785701369592402,4.3785701369592402,null,3.4562364031798127,3.4562364031798127,null,1.8644115296256567,1.8644115296256567,null,2.564354530308095,2.564354530308095,null,3.3461036195536069,3.3461036195536069,null,2.3529543686817309,2.3529543686817309,null,0.84235366498472297,0.84235366498472297,null,3.8842508200282091,3.8842508200282091,null,2.1705223883754821,2.1705223883754821,null,2.426439729231717,2.426439729231717,null,4.5039006090045355,4.5039006090045355,null,2.2258550703945952,2.2258550703945952,null,3.8457315401892957,3.8457315401892957,null,1.7393171211812251,1.7393171211812251,null,2.6454575969260223,2.6454575969260223],"type":"scatter","mode":"lines","z":[4.4395243534477871,4.76982251051672,6.5587083141491238,5.0705083914245757,5.129287735160946,6.7150649868832808,5.4609162059892027,3.7349387653934665,4.3131471481064736,4.554338029900042,6.224081797439462,5.3598138270573639,5.4007714505940525,5.1106827159451198,4.4441588652459254,6.7869131368030784,5.497850478229239,3.0333828433703616,5.7013559015636854,4.527208592272066,3.9321762940131548,4.7820250853417052,3.9739955516927603,4.2711087707088602,4.3749607321507433,3.3133066892575869,5.837787044494525,5.153373117836515,3.8618630629880526,6.2538149210699263,5.426464221476814,4.7049285170077288,5.8951256610450224,5.8781334875330424,5.8215810816374871,5.6886402541000907,5.5539176535375887,4.9380882894232787,4.6940373362600836,4.6195289989876169,4.3052930210794873,4.7920827219804014,3.7346036484317358,7.1689559653385126,6.2079619983049907,3.8768914167966502,4.5971151647009236,4.533344646376781,5.7799651183363174,4.9166309335281708,5.2533185139947545,4.9714532446512969,4.9571295427086843,6.3686022840144574,4.7742290143407322,6.5164706044295393,3.4512471957697786,5.5846137496360688,5.1238542438446135,5.2159415687439727,5.3796394827598819,4.4976765468906974,4.6667926163305795,3.9814246168929115,3.9282087735244229,5.3035286414042577,5.4482097786294261,5.0530042267305042,5.9222674678797373,7.0500846856271444,4.5089688339434648,2.6908311243591871,6.0057385244622568,4.2907992374176072,4.311991383532642,6.0255713696966993,4.7152269929489909,3.7792822877454646,5.1813034797491504,4.8611086375609558,5.0057641858998867,5.3852804011263302,4.6293399682075904,5.6443765485188333,4.7795134381812492,5.3317819639156969,6.0968390131493475,5.435181490833803,4.6740684144687732,6.1488076184510945,5.9935038559621194,5.5483969595080698,5.2387317351114415,4.3720939239606285,6.3606524485300078,4.3997404128528732,7.187332993016577,6.5326106261851891,4.7642996408995231,3.9735790996932194,4.2895934363006987,5.2568837091565292,4.7533081215376267,4.652457400602267,4.0483814327349847,4.9549722751910794,4.2150955305429241,3.3320580634118633,4.6197734797122374,5.9189966090607662,4.4246530373916082,5.6079643222250333,3.382117291710836,4.9444380344754606,5.5194072039434623,5.3011533621667146,5.105676194148943,4.3592939916946234,4.1502956539664178,3.9758712093950868,5.1176465971001255,4.0525253858151977,4.5094425562993319,4.7439078078017527,6.8438620052322072,4.348050098304542,5.2353865722848569,5.077960849563711,4.0381433658698711,4.9286919138764009,6.4445508584233488,5.4515040530792147,5.0412329219929397,4.5775031676603755,2.9467527784594845,6.1313372134141755,3.5393599290751778,5.739947510877335,6.9091035692174838,3.5561068390282005,5.7017843353747111,4.7378025105975317,3.4278558408545123,3.4853323462182484,3.3984638264254068,4.4690934778296967,3.5382444150041001,5.6879167729758278,7.1001089405256721,3.7129695239648211,5.7877388474751781,5.7690422410009097,5.3322025789501177,3.9916233917229924,4.8805473933693415,4.7196046648297534,5.5629895332204802,4.6275612438961709,5.9769733866856205,4.6254191422329862,6.0527114655793319,3.9508229933339338,3.7398447552418879,8.2410399349424051,4.5831424118395683,5.2982275915407158,5.6365696740338489,4.5162193742912562,5.5168620443136094,5.3689645273850859,4.7846194923583063,5.0652930335253155,4.9659327462615357,7.1284518990161807,4.2586639037271716,3.9040037329253359,5.0377883991710792,5.310480749443137,5.4365234789101828,4.5416346672888945,3.936673866028809,6.2631851760894897,4.6503496120464449,4.1344871373466257,4.7637204310589034,4.8028241056514478,6.1099202897136404,5.0847372921971967,5.7540537851845217,4.5007079828277394,5.2144453095816017,4.6753140885091655,5.0945835281735716,4.1046366420224585,3.6891984666720283,6.9972133847479663,5.6007088236724174,3.7487286383750562,4.3888340833195789,3.8145199154026894],"zend":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"line":{"color":"gray","width":1},"showlegend":false,"hoverinfo":["none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none",null,"none","none"],"marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[8.5,0,0],"y":[0,8.5,0],"z":[0,0,8.5],"text":["<i>q<\/i><sub>1<\/sub>","<i>q<\/i><sub>2<\/sub>","<i>V<\/i><sup>⊥<\/sup>"],"type":"scatter3d","mode":"text","textfont":{"size":15,"color":"black"},"showlegend":false,"marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Visualization of Projected Trivariate Normal Cloud</p>
</div>
</div>
</section>
<section id="distribution-of-general-quadratic-forms" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="distribution-of-general-quadratic-forms"><span class="header-section-number">5.4.3</span> Distribution of General Quadratic Forms</h3>
<div id="lem-idempotent-sigma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3 (Idempotent Matrix Property)</strong></span> Let <span class="math inline">\(\Sigma\)</span> be a positive definite matrix such that <span class="math inline">\(\Sigma = \Sigma^{1/2}\Sigma^{1/2}\)</span>. The matrix <span class="math inline">\(A\Sigma\)</span> is idempotent if and only if <span class="math inline">\(\Sigma^{1/2}A\Sigma^{1/2}\)</span> is idempotent.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\((\Rightarrow)\)</span> Assume <span class="math inline">\(A\Sigma\)</span> is idempotent, so <span class="math inline">\(A\Sigma A\Sigma = A\Sigma\)</span>. Then: <span class="math display">\[
\begin{aligned}
(\Sigma^{1/2}A\Sigma^{1/2})^2 &amp;= \Sigma^{1/2}A(\Sigma^{1/2}\Sigma^{1/2})A\Sigma^{1/2} \\
&amp;= \Sigma^{1/2}(A\Sigma A)\Sigma^{1/2}
\end{aligned}
\]</span> From the assumption <span class="math inline">\(A\Sigma A\Sigma = A\Sigma\)</span>, post-multiplying by <span class="math inline">\(\Sigma^{-1}\)</span> gives <span class="math inline">\(A\Sigma A = A\)</span>. Substituting this back: <span class="math display">\[
\Sigma^{1/2}(A)\Sigma^{1/2} = \Sigma^{1/2}A\Sigma^{1/2}
\]</span></p>
<p><span class="math inline">\((\Leftarrow)\)</span> Assume <span class="math inline">\(\Sigma^{1/2}A\Sigma^{1/2}\)</span> is idempotent. Then: <span class="math display">\[
(\Sigma^{1/2}A\Sigma^{1/2})(\Sigma^{1/2}A\Sigma^{1/2}) = \Sigma^{1/2}A\Sigma^{1/2}
\]</span> Expanding the left side: <span class="math display">\[
\Sigma^{1/2}A(\Sigma^{1/2}\Sigma^{1/2})A\Sigma^{1/2} = \Sigma^{1/2}A\Sigma A\Sigma^{1/2}
\]</span> Equating this to the right side: <span class="math display">\[
\Sigma^{1/2}A\Sigma A\Sigma^{1/2} = \Sigma^{1/2}A\Sigma^{1/2}
\]</span> Pre-multiply by <span class="math inline">\(\Sigma^{-1/2}\)</span> and post-multiply by <span class="math inline">\(\Sigma^{1/2}\)</span> (which exist since <span class="math inline">\(\Sigma\)</span> is positive definite): <span class="math display">\[
\begin{aligned}
\Sigma^{-1/2}(\Sigma^{1/2}A\Sigma A\Sigma^{1/2})\Sigma^{1/2} &amp;= \Sigma^{-1/2}(\Sigma^{1/2}A\Sigma^{1/2})\Sigma^{1/2} \\
I(A\Sigma A)\Sigma &amp;= I(A)\Sigma \\
A\Sigma A\Sigma &amp;= A\Sigma
\end{aligned}
\]</span></p>
</div>
<div id="lem-rank-sigma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 4 (Rank Invariance)</strong></span> Under the conditions of <a href="#lem-idempotent-sigma" class="quarto-xref">Lemma&nbsp;3</a>, if <span class="math inline">\(A\Sigma\)</span> is idempotent, then: <span class="math display">\[
\text{rank}(A\Sigma) = \text{rank}(\Sigma^{1/2}A\Sigma^{1/2}) = \text{tr}(A\Sigma)
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since <span class="math inline">\(A\Sigma\)</span> and <span class="math inline">\(\Sigma^{1/2}A\Sigma^{1/2}\)</span> are both idempotent (by <a href="#lem-idempotent-sigma" class="quarto-xref">Lemma&nbsp;3</a>), their ranks are equal to their traces.</p>
<p>Using the cyclic property of the trace operator (<span class="math inline">\(\text{tr}(XYZ) = \text{tr}(ZXY)\)</span>): <span class="math display">\[
\begin{aligned}
\text{rank}(A\Sigma) &amp;= \text{tr}(A\Sigma) \\
&amp;= \text{tr}(A \Sigma^{1/2} \Sigma^{1/2}) \\
&amp;= \text{tr}(\Sigma^{1/2} A \Sigma^{1/2}) \\
&amp;= \text{rank}(\Sigma^{1/2}A\Sigma^{1/2})
\end{aligned}
\]</span> Alternatively, notice that <span class="math inline">\(A\Sigma\)</span> is similar to <span class="math inline">\(\Sigma^{1/2}A\Sigma^{1/2}\)</span>: <span class="math display">\[
A\Sigma = \Sigma^{-1/2} (\Sigma^{1/2}A\Sigma^{1/2}) \Sigma^{1/2}
\]</span> Since similar matrices have the same rank, the equality holds.</p>
</div>
<div id="thm-dist-quad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 42 (Distribution of y’Ay)</strong></span> Let <span class="math inline">\(y \sim N_p(\mu, \Sigma)\)</span>. Let <span class="math inline">\(A\)</span> be a symmetric matrix of rank <span class="math inline">\(r\)</span>. Then <span class="math inline">\(y'Ay \sim \chi^2(r, \lambda)\)</span> with <span class="math inline">\(\lambda = \mu' A \mu\)</span> <strong>if and only if</strong> <span class="math inline">\(A\Sigma\)</span> is idempotent (<span class="math inline">\(A\Sigma A\Sigma = A\Sigma\)</span>).</p>
<p><strong>Special Case (<span class="math inline">\(\Sigma = I\)</span>):</strong> If <span class="math inline">\(\Sigma = I\)</span>, the condition simplifies to <span class="math inline">\(A\)</span> being idempotent (<span class="math inline">\(A^2 = A\)</span>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(y^* = \Sigma^{-1/2}y\)</span>, so <span class="math inline">\(y^* \sim N_n(\Sigma^{-1/2}\mu, I_n)\)</span>. We rewrite the quadratic form: <span class="math display">\[y'Ay = y' \Sigma^{-1/2} (\Sigma^{1/2} A \Sigma^{1/2}) \Sigma^{-1/2} y = (y^*)' P_V y^* = \|P_V y^*\|^2\]</span> Since <span class="math inline">\(A\Sigma\)</span> is idempotent, <span class="math inline">\(P_V = \Sigma^{1/2} A \Sigma^{1/2}\)</span> is a projection matrix with rank <span class="math inline">\(r\)</span>. By the definition of the non-central chi-square, <span class="math inline">\(y'Ay \sim \chi^2(r, \|P_V \Sigma^{-1/2}\mu\|^2)\)</span>. The non-centrality parameter simplifies to <span class="math inline">\(\lambda = \mu'A\mu\)</span>.</p>
</div>
</section>
<section id="standardized-distance-distribution" class="level3" data-number="5.4.4">
<h3 data-number="5.4.4" class="anchored" data-anchor-id="standardized-distance-distribution"><span class="header-section-number">5.4.4</span> Standardized Distance Distribution</h3>
<div id="cor-standardized-mvn" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 6 (Standardized Distance Distribution)</strong></span> Suppose <span class="math inline">\(y \sim N_n(\mu, \Sigma)\)</span>. Then the quadratic form representing the standardized distance from a constant vector <span class="math inline">\(\mu_0\)</span> follows a non-central chi-square distribution: <span class="math display">\[(y-\mu_0)'\Sigma^{-1}(y-\mu_0) \sim \chi^2(n, \lambda = (\mu-\mu_0)'\Sigma^{-1}(\mu-\mu_0))\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(A = \Sigma^{-1}\)</span>. Then <span class="math inline">\(A\Sigma = \Sigma^{-1}\Sigma = I_n\)</span>, which is clearly idempotent. Alternatively, let <span class="math inline">\(w = \Sigma^{-1/2}(y-\mu_0)\)</span>, then <span class="math inline">\(w \sim N_n(\Sigma^{-1/2}(\mu-\mu_0), I_n)\)</span>. By the definition of chi-square, <span class="math inline">\(\|w\|^2 = (y-\mu_0)'\Sigma^{-1}(y-\mu_0)\)</span> follows the stated distribution.</p>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Crucial Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is an important theorem we will use later.</p>
</div>
</div>
</section>
</section>
<section id="distributions-of-projections-of-spherical-normal" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="distributions-of-projections-of-spherical-normal"><span class="header-section-number">5.5</span> Distributions of Projections of Spherical Normal</h2>
<div id="thm-proj-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 43 (Distribution of Projections)</strong></span> Let <span class="math inline">\(V\)</span> be a <span class="math inline">\(k\)</span>-dimensional subspace of <span class="math inline">\(\mathcal{R}^n\)</span> with projection matrix <span class="math inline">\(P_V\)</span>, and let <span class="math inline">\(y\)</span> be a random vector in <span class="math inline">\(\mathcal{R}^n\)</span> with mean <span class="math inline">\(E(y)=\mu\)</span>. Then:</p>
<ol type="1">
<li><span class="math inline">\(E(P_V y) = P_V \mu\)</span>.</li>
<li>If <span class="math inline">\(\text{Var}(y)=\sigma^2 I_n\)</span>, then <span class="math inline">\(\text{Var}(P_V y) = \sigma^2 P_V\)</span> and <span class="math inline">\(E(\|P_V y\|^2) = \sigma^2 k + \|P_V \mu\|^2\)</span>.</li>
<li>If <span class="math inline">\(y \sim N_n(\mu, \sigma^2 I_n)\)</span>, then <span class="math inline">\(\frac{1}{\sigma^2}\|P_V y\|^2 = \frac{1}{\sigma^2}y'P_Vy \sim \chi^2(k, \frac{1}{\sigma^2}\|P_V \mu\|^2)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li>Since the projection operation is linear, <span class="math inline">\(E(P_V y) = P_V E(y) = P_V \mu\)</span>.</li>
<li><span class="math inline">\(\text{Var}(P_V y) = P_V \text{Var}(y) P_V^T = P_V \sigma^2 I_n P_V = \sigma^2 P_V\)</span>. The expectation of the squared norm follows from the mean of a quadratic form: <span class="math inline">\(E(y'P_Vy) = \text{tr}(P_V \sigma^2 I) + \mu'P_V\mu = \sigma^2 k + \|P_V \mu\|^2\)</span>.</li>
<li>This is a special case of the general quadratic distribution theorem where <span class="math inline">\(A = \frac{1}{\sigma^2} P_V\)</span> and <span class="math inline">\(A(\sigma^2 I) = P_V\)</span>, which is idempotent.</li>
</ol>
</div>
<div id="thm-ortho-indep" class="theorem">
<p><span class="theorem-title"><strong>Theorem 44 (Orthogonal Projections)</strong></span> Let <span class="math inline">\(V_1, \dots, V_k\)</span> be mutually orthogonal subspaces with dimensions d_i and projection matrices <span class="math inline">\(P_i\)</span>. If <span class="math inline">\(y \sim N_n(\mu, \sigma^2 I_n)\)</span>, then:</p>
<ol type="1">
<li>The projections <span class="math inline">\(\hat{y}_i = P_i y\)</span> are independent with <span class="math inline">\(\hat{y}_i \sim N(P_i \mu, \sigma^2 P_i)\)</span>.</li>
<li>The squared norms <span class="math inline">\(\|\hat{y}_i\|^2\)</span> are mutually independent.</li>
<li><span class="math inline">\(\frac{1}{\sigma^2}\|\hat{y}_i\|^2 \sim \chi^2(d_i, \frac{1}{\sigma^2}\|P_i \mu\|^2)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="1">
<li>For <span class="math inline">\(i \ne j\)</span>, <span class="math inline">\(\text{Cov}(P_i y, P_j y) = \sigma^2 P_i P_j = 0\)</span> because orthogonal projection matrices satisfy <span class="math inline">\(P_i P_j = 0\)</span>. Under normality, zero covariance implies independence.</li>
<li>Since <span class="math inline">\(\hat{y}_i\)</span> are independent, any measurable functions of them, such as their squared norms, are also independent.</li>
<li>This follows directly from applying the projection distribution theorem to each independent subspace.</li>
</ol>
</div>
<section id="independence-of-forms" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="independence-of-forms"><span class="header-section-number">5.5.1</span> Independence of Forms</h3>
<div id="thm-indep-mvn" class="theorem">
<p><span class="theorem-title"><strong>Theorem 45 (Independence Conditions)</strong></span> Suppose <span class="math inline">\(y \sim N_n(\mu, \Sigma)\)</span>.</p>
<ul>
<li><strong>Linear and Quadratic:</strong> <span class="math inline">\(By\)</span> and <span class="math inline">\(y'Ay\)</span> (where <span class="math inline">\(A\)</span> is symmetric) are independent if and only if <span class="math inline">\(B\Sigma A = 0\)</span>.</li>
<li><strong>Quadratic and Quadratic:</strong> <span class="math inline">\(y'Ay\)</span> and <span class="math inline">\(y'By\)</span> (where <span class="math inline">\(A, B\)</span> are symmetric) are independent if and only if <span class="math inline">\(A\Sigma B = 0\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(B\Sigma A = 0\)</span>, the normal vectors <span class="math inline">\(By\)</span> and <span class="math inline">\(Ay\)</span> have zero covariance and are independent. Because <span class="math inline">\(By\)</span> is independent of <span class="math inline">\(Ay\)</span>, it is also independent of any measurable function of <span class="math inline">\(Ay\)</span>, specifically <span class="math inline">\(y'Ay = \|Ay\|^2\)</span> (if <span class="math inline">\(A\)</span> is idempotent).</p>
</div>
</section>
<section id="cochrans-theorem" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="cochrans-theorem"><span class="header-section-number">5.5.2</span> Cochran’s Theorem</h3>
<div id="thm-cochran-result" class="theorem">
<p><span class="theorem-title"><strong>Theorem 46 (Cochran’s Result)</strong></span> Let <span class="math inline">\(y \sim N_n(\mu, \sigma^2 I)\)</span> and <span class="math inline">\(y'y = \sum y'A_iy\)</span>. The quadratic forms <span class="math inline">\(y^T A_i y / \sigma^2\)</span> are mutually independent <span class="math inline">\(\chi^2(r_i, \lambda_i)\)</span> if and only if any one of the following holds:</p>
<ul>
<li>Each <span class="math inline">\(A_i\)</span> is idempotent.</li>
<li><span class="math inline">\(A_i A_j = 0\)</span> for all <span class="math inline">\(i \ne j\)</span>.</li>
<li><span class="math inline">\(n = \sum r_i\)</span>.</li>
</ul>
</div>
</section>
</section>
<section id="non-central-distributions-derived-from-non-central-chi2" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="non-central-distributions-derived-from-non-central-chi2"><span class="header-section-number">5.6</span> Non-central Distributions Derived from Non-central <span class="math inline">\(\chi^2\)</span></h2>
<p>We begin by defining two independent Chi-squared random variables that form the building blocks for statistical power analysis.</p>
<ul>
<li><p><strong>Non-central Component (<span class="math inline">\(X_1\)</span>):</strong> <span class="math inline">\(X_1 \sim \chi^2(\text{df}_1, \lambda)\)</span>. Here, <span class="math inline">\(\lambda\)</span> is the non-centrality parameter, defined as the sum of squared means, <span class="math inline">\(\lambda = ||\mu||^2\)</span>. This is consistent with the definition used throughout this chapter. <em>(Note: This definition is also used by R’s <code>ncp</code> argument.)</em></p></li>
<li><p><strong>Central Component (<span class="math inline">\(X_2\)</span>):</strong> <span class="math inline">\(X_2 \sim \chi^2(\text{df}_2)\)</span>. <span class="math inline">\(X_2\)</span> often represents the <strong>Noise Sum of Squares</strong>, SSE<span class="math inline">\(_1\)</span> of an adequate model, which is assume to follow a central <span class="math inline">\(\chi^2\)</span>,</p></li>
</ul>
<p>We visualize these components as using the follow diagram.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-variance-partition" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 85% !important;" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-variance-partition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-variance-partition-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width: 85% !important;" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-variance-partition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: A diagram of two independent <span class="math inline">\(\chi^2\)</span> random variables
</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-non-central-f-distribution-ftextdf_1-textdf_2-lambda" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="the-non-central-f-distribution-ftextdf_1-textdf_2-lambda"><span class="header-section-number">5.6.1</span> The Non-central F-distribution <span class="math inline">\(F(\text{df}_1, \text{df}_2, \lambda)\)</span></h3>
<div id="def-noncentral-f" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 43 (Non-central F)</strong></span> Let <span class="math inline">\(X_1 \sim \chi^2(\text{df}_1, \lambda)\)</span> and <span class="math inline">\(X_2 \sim \chi^2(\text{df}_2)\)</span> be independent. The random variable <span class="math inline">\(F\)</span> follows a <strong>non-central F-distribution</strong>: <span class="math display">\[F = \frac{X_1/\text{df}_1}{X_2/\text{df}_2} \sim F(\text{df}_1, \text{df}_2, \lambda)\]</span></p>
</div>
<ul>
<li><strong>Expectation:</strong>
<ul>
<li><strong>Under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\lambda=0\)</span>):</strong> Exact mean is <span class="math inline">\(\frac{\text{df}_2}{\text{df}_2 - 2}\)</span> (for <span class="math inline">\(\text{df}_2 &gt; 2\)</span>).</li>
<li><strong>Under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\lambda \neq 0\)</span>):</strong> Approximate mean is <span class="math inline">\(1 + \frac{\lambda}{\text{df}_1}\)</span>.</li>
</ul></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-nc-f" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nc-f-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-nc-f-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nc-f-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Densities of Non-Central F (<span class="math inline">\(\lambda\)</span> defined as sum of squares).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="type-i-non-central-beta-textbeta_1textdf_12-textdf_22-lambda" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="type-i-non-central-beta-textbeta_1textdf_12-textdf_22-lambda"><span class="header-section-number">5.6.2</span> Type I Non-central Beta <span class="math inline">\(\text{Beta}_1(\text{df}_1/2, \text{df}_2/2, \lambda)\)</span></h3>
<div id="def-noncentral-beta1" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 44 (Type I Non-central Beta)</strong></span> The random variable <span class="math inline">\(B_I\)</span> follows a <strong>Type I non-central Beta distribution</strong>, defined as the signal’s proportion of the total sum (<span class="math inline">\(R^2\)</span>): <span class="math display">\[B_I = \frac{X_1}{X_1 + X_2} \sim \text{Beta}_1\left(\frac{\text{df}_1}{2}, \frac{\text{df}_2}{2}, \lambda\right)\]</span></p>
</div>
<ul>
<li><strong>Relationship to F:</strong> <span class="math inline">\(B_I = \frac{(\text{df}_1/\text{df}_2) F}{1 + (\text{df}_1/\text{df}_2) F}\)</span></li>
<li><strong>Expectation:</strong>
<ul>
<li><strong>Under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\lambda=0\)</span>):</strong> Exact mean is <span class="math inline">\(\frac{\text{df}_1}{\text{df}_1 + \text{df}_2}\)</span>.</li>
<li><strong>Under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\lambda \neq 0\)</span>):</strong> Approximate mean is <span class="math inline">\(\frac{\text{df}_1 + \lambda}{\text{df}_1 + \text{df}_2 + \lambda}\)</span>.</li>
</ul></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-nc-beta1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nc-beta1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-nc-beta1-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nc-beta1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Densities of Type I Beta (<span class="math inline">\(R^2\)</span>).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="type-ii-non-central-beta-textbeta_2textdf_22-textdf_12-lambda" class="level3" data-number="5.6.3">
<h3 data-number="5.6.3" class="anchored" data-anchor-id="type-ii-non-central-beta-textbeta_2textdf_22-textdf_12-lambda"><span class="header-section-number">5.6.3</span> Type II Non-central Beta <span class="math inline">\(\text{Beta}_2(\text{df}_2/2, \text{df}_1/2, \lambda)\)</span></h3>
<div id="def-noncentral-beta2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 45 (Type II Non-central Beta)</strong></span> <span class="math display">\[B_{II} = \frac{X_2}{X_1 + X_2} = 1 - B_I \sim \text{Beta}_2\left(\frac{\text{df}_2}{2}, \frac{\text{df}_1}{2}, \lambda\right)\]</span></p>
</div>
<ul>
<li><strong>Relationship to F:</strong> <span class="math inline">\(B_{II} = \frac{1}{1 + (\text{df}_1/\text{df}_2) F}\)</span></li>
<li><strong>Expectation:</strong>
<ul>
<li><strong>Under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\lambda=0\)</span>):</strong> Exact mean is <span class="math inline">\(\frac{\text{df}_2}{\text{df}_1 + \text{df}_2}\)</span>.</li>
<li><strong>Under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\lambda \neq 0\)</span>):</strong> Approximate mean is <span class="math inline">\(\frac{\text{df}_2}{\text{df}_1 + \text{df}_2 + \lambda}\)</span>.</li>
</ul></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-beta-ii" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-beta-ii-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-beta-ii-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beta-ii-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Densities of Type II Beta (<span class="math inline">\(SSE/SST\)</span>). Support is [0, 1].
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="scaled-type-ii-beta-textscaled-beta_2textdf_22-textdf_12-lambda" class="level3" data-number="5.6.4">
<h3 data-number="5.6.4" class="anchored" data-anchor-id="scaled-type-ii-beta-textscaled-beta_2textdf_22-textdf_12-lambda"><span class="header-section-number">5.6.4</span> Scaled Type II Beta <span class="math inline">\(\text{Scaled-Beta}_2(\text{df}_2/2, \text{df}_1/2, \lambda)\)</span></h3>
<div id="def-scaled-beta" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 46 (Scaled Type II Beta)</strong></span> <span class="math display">\[S = \frac{X_2/\text{df}_2}{(X_1+X_2)/(\text{df}_1+\text{df}_2)} \sim \text{Scaled-Beta}_2\]</span></p>
</div>
<ul>
<li><strong>Relationship to F:</strong> <span class="math inline">\(S = \frac{\text{df}_1+\text{df}_2}{\text{df}_2 + \text{df}_1 F}\)</span></li>
<li><strong>Expectation:</strong>
<ul>
<li><strong>Under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\lambda=0\)</span>):</strong> Exact mean is <span class="math inline">\(1\)</span>.</li>
<li><strong>Under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\lambda \neq 0\)</span>):</strong> Approximate mean is <span class="math inline">\(\frac{\text{df}_1+\text{df}_2}{\text{df}_1+\text{df}_2+\lambda}\)</span>.</li>
</ul></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-scaled-beta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaled-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-scaled-beta-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaled-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Densities of Scaled Type II Beta (<span class="math inline">\(MSE/MST\)</span>).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-non-central-t-distribution-ttextdf_2-delta" class="level3" data-number="5.6.5">
<h3 data-number="5.6.5" class="anchored" data-anchor-id="the-non-central-t-distribution-ttextdf_2-delta"><span class="header-section-number">5.6.5</span> The Non-central t-distribution <span class="math inline">\(t(\text{df}_2, \delta)\)</span></h3>
<div id="def-noncentral-t" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 47 (Non-central t)</strong></span> Let <span class="math inline">\(Z \sim N(\delta, 1)\)</span> and <span class="math inline">\(X_2 \sim \chi^2(\text{df}_2)\)</span> be independent. The random variable <span class="math inline">\(T\)</span> follows a <strong>non-central t-distribution</strong>: <span class="math display">\[T = \frac{Z}{\sqrt{X_2/\text{df}_2}} \sim t(\text{df}_2, \delta)\]</span></p>
</div>
<ul>
<li><strong>Relationship to F:</strong> <span class="math inline">\(F = T^2\)</span> (when <span class="math inline">\(\text{df}_1=1\)</span>). Note <span class="math inline">\(\delta^2 = \lambda\)</span>.</li>
<li><strong>Expectation:</strong>
<ul>
<li><strong>Under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\delta=0\)</span>):</strong> Exact mean is <span class="math inline">\(0\)</span>.</li>
<li><strong>Under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\delta \neq 0\)</span>):</strong> Approximate mean is <span class="math inline">\(\delta\)</span>.</li>
</ul></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-nc-t" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nc-t-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-nc-t-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nc-t-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Densities of Non-Central t (<span class="math inline">\(df=20\)</span>).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="example-inference-of-the-mean-of-normal-sample" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="example-inference-of-the-mean-of-normal-sample"><span class="header-section-number">5.7</span> Example: Inference of the Mean of Normal Sample</h2>
<p>Consider a random sample <span class="math inline">\(y \sim N_n(\mu j_n, \sigma^2 I_n)\)</span>. We wish to test:</p>
<ul>
<li><strong><span class="math inline">\(M_1\)</span> (Full Model):</strong> <span class="math inline">\(\mu\)</span> is unknown.</li>
<li><strong><span class="math inline">\(M_0\)</span> (Reduced Model):</strong> <span class="math inline">\(\mu = \mu_0\)</span>.</li>
</ul>
<p>Let’s define the transformed vector <span class="math inline">\(y^* = y - \mu_0 j_n\)</span>. Note that <span class="math inline">\(y^* \sim N_n((\mu - \mu_0)j_n, \sigma^2 I_n)\)</span>.</p>
<section id="sum-of-squares-and-their-distributions" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="sum-of-squares-and-their-distributions"><span class="header-section-number">5.7.1</span> Sum of Squares and Their Distributions</h3>
<p>We use the projection matrix <span class="math inline">\(P_{j_n} = \frac{1}{n}j_n j_n'\)</span> and its complement <span class="math inline">\((I_n - P_{j_n})\)</span> to partition the transformed vector.</p>
<ul>
<li><p><strong>Total SSE (<span class="math inline">\(SSE_0\)</span> for <span class="math inline">\(M_0\)</span>):</strong> <span class="math display">\[SSE_0 = \|I_n y^*\|^2 = \sum_{i=1}^n (Y_i - \mu_0)^2\]</span> This follows a non-central distribution with <span class="math inline">\(\text{df}_{\text{total}} = n\)</span>: <span class="math display">\[\frac{SSE_0}{\sigma^2} \sim \chi^2(n, \lambda) \quad \text{where } \lambda = \frac{n(\mu - \mu_0)^2}{\sigma^2}\]</span></p></li>
<li><p><strong>Residual SSE (<span class="math inline">\(SSE_1\)</span> for <span class="math inline">\(M_1\)</span>):</strong> <span class="math display">\[SSE_1 = \|(I_n - P_{j_n})y^*\|^2 = \sum_{i=1}^n (Y_i - \bar{Y})^2\]</span> This captures the random noise (central component) with <span class="math inline">\(\text{df}_2 = n-1\)</span>: <span class="math display">\[\frac{SSE_1}{\sigma^2} \sim \chi^2(n-1)\]</span></p></li>
<li><p><strong>Difference SS (<span class="math inline">\(SS_{\text{diff}}\)</span>):</strong> <span class="math display">\[SS_{\text{diff}} = \|P_{j_n} y^*\|^2 = n(\bar{Y} - \mu_0)^2\]</span> This captures the signal (non-central component) with <span class="math inline">\(\text{df}_1 = 1\)</span>: <span class="math display">\[\frac{SS_{\text{diff}}}{\sigma^2} \sim \chi^2(1, \lambda)\]</span></p></li>
</ul>
</section>
<section id="distributions-of-equivalent-statistics" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="distributions-of-equivalent-statistics"><span class="header-section-number">5.7.2</span> Distributions of Equivalent Statistics</h3>
<p>We can construct five equivalent statistics to compare <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span>.</p>
<ul>
<li><p><strong>The t-statistic (<span class="math inline">\(T\)</span>):</strong> <span class="math display">\[T = \frac{\bar{Y} - \mu_0}{S/\sqrt{n}}\]</span></p></li>
<li><p><strong>The F-statistic (<span class="math inline">\(F\)</span>):</strong> <span class="math display">\[F = \frac{n(\bar{Y} - \mu_0)^2}{S^2} = T^2\]</span></p></li>
<li><p><strong>The Type I Beta statistic (<span class="math inline">\(B_I\)</span>):</strong> <span class="math display">\[B_I = \frac{SS_{\text{diff}}}{SSE_0} = \frac{n(\bar{Y} - \mu_0)^2}{\sum (Y_i - \mu_0)^2}\]</span></p></li>
<li><p><strong>The Type II Beta statistic (<span class="math inline">\(B_{II}\)</span>):</strong> <span class="math display">\[B_{II} = \frac{SSE_1}{SSE_0} = \frac{\sum (Y_i - \bar{Y})^2}{\sum (Y_i - \mu_0)^2} = 1 - B_I\]</span></p></li>
<li><p><strong>The Scaled Type II Beta statistic (<span class="math inline">\(S_{\text{scaled}}\)</span>):</strong> <span class="math display">\[S_{\text{scaled}} = \frac{SSE_1/(n-1)}{SSE_0/n} = \left( \frac{n}{n-1} \right) B_{II}\]</span></p></li>
</ul>
</section>
<section id="expectations-under-m_1-and-m_0" class="level3" data-number="5.7.3">
<h3 data-number="5.7.3" class="anchored" data-anchor-id="expectations-under-m_1-and-m_0"><span class="header-section-number">5.7.3</span> Expectations Under <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_0\)</span></h3>
<p>The table below contrasts the distributions and expected values of these statistics. We assume the sample size <span class="math inline">\(n\)</span> is large enough for the mean of <span class="math inline">\(F\)</span> to exist (<span class="math inline">\(n &gt; 3\)</span>).</p>
<ul>
<li><strong>Degrees of Freedom:</strong> <span class="math inline">\(\text{df}_1 = 1\)</span>, <span class="math inline">\(\text{df}_2 = n-1\)</span>.</li>
<li><strong>Non-centrality:</strong> <span class="math inline">\(\delta = \frac{\sqrt{n}(\mu - \mu_0)}{\sigma}\)</span> and <span class="math inline">\(\lambda = \delta^2 = \frac{n(\mu - \mu_0)^2}{\sigma^2}\)</span>.</li>
</ul>
<div id="tbl-expected-values" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-expected-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Expected Values of Test Statistics Under Null and Alternative Hypotheses
</figcaption>
<div aria-describedby="tbl-expected-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Statistic</th>
<th style="text-align: left;">Distribution under <span class="math inline">\(H_1\)</span> (<span class="math inline">\(\mu \neq \mu_0\)</span>)</th>
<th style="text-align: left;">Exact Mean under <span class="math inline">\(H_0\)</span> (<span class="math inline">\(\mu=\mu_0\)</span>)</th>
<th style="text-align: left;">Approximate Mean under <span class="math inline">\(H_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(T\)</span></strong></td>
<td style="text-align: left;"><span class="math inline">\(t(n-1, \delta)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{\sqrt{n}(\mu - \mu_0)}{\sigma}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\(F\)</span></strong></td>
<td style="text-align: left;"><span class="math inline">\(F(1, n-1, \lambda)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{n-1}{n-3} \approx 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1 + \frac{n(\mu - \mu_0)^2}{\sigma^2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(B_I\)</span></strong></td>
<td style="text-align: left;"><span class="math inline">\(\text{Beta}_1\left(\frac{1}{2}, \frac{n-1}{2}, \lambda\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1/n + \frac{(\mu - \mu_0)^2}{\sigma^2}}{1 + \frac{(\mu - \mu_0)^2}{\sigma^2}}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\(B_{II}\)</span></strong></td>
<td style="text-align: left;"><span class="math inline">\(\text{Beta}_2\left(\frac{n-1}{2}, \frac{1}{2}, \lambda\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{n-1}{n}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{(n-1)/n}{1 + \frac{(\mu - \mu_0)^2}{\sigma^2}}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(S_{\text{scaled}}\)</span></strong></td>
<td style="text-align: left;"><span class="math inline">\(\text{Scaled-Beta}_2\left(\frac{n-1}{2}, \frac{1}{2}, \lambda\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{1 + \frac{(\mu - \mu_0)^2}{\sigma^2}}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Key Interpretation:</strong> All statistics are functionally driven by the signal energy. Notably, for <strong><span class="math inline">\(S_{\text{scaled}}\)</span></strong>, the sample size <span class="math inline">\(n\)</span> cancels out in the approximate mean. This makes it a direct measure of the ratio between Noise Variance and Total Variance (Noise + Signal) in the population distributions, connected to the Rao-Blackwell decomposition of variances.</p>
<div style="page-break-after: always;"></div>
<hr>
</section>
</section>
</section>
<section id="inference-for-a-multiple-linear-regression-model" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Inference for A Multiple Linear Regression Model</h1>
<section id="linear-models-and-least-square-estimator" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="linear-models-and-least-square-estimator"><span class="header-section-number">6.1</span> Linear Models and Least Square Estimator</h2>
<section id="assumptions-in-linear-models" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="assumptions-in-linear-models"><span class="header-section-number">6.1.1</span> Assumptions in Linear Models</h3>
<p>Suppose that on a random sample of <span class="math inline">\(n\)</span> units (patients, animals, trees, etc.) we observe a response variable <span class="math inline">\(Y\)</span> and explanatory variables <span class="math inline">\(X_{1},...,X_{k}\)</span>. Our data are then <span class="math inline">\((y_{i},x_{i1},...,x_{ik})\)</span>, <span class="math inline">\(i=1,...,n\)</span>, or in vector/matrix form <span class="math inline">\(y, x_{1},...,x_{k}\)</span> where <span class="math inline">\(y=(y_{1},...,y_{n})\)</span> and <span class="math inline">\(x_{j}=(x_{1j},...,x_{nj})^{T}\)</span> or <span class="math inline">\(y, X\)</span> where <span class="math inline">\(X=(x_{1},...,x_{k})\)</span>.</p>
<p>Either by design or by conditioning on their observed values, <span class="math inline">\(x_{1},...,x_{k}\)</span> are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:</p>
<p><strong>Assumptions on Linear Models</strong></p>
<ul>
<li><p><strong>A1. (Additive Error)</strong> <span class="math inline">\(y=\mu+e\)</span> where <span class="math inline">\(e=(e_{1},...,e_{n})^{T}\)</span> is an unobserved random vector with <span class="math inline">\(E(e)=0\)</span>. This implies that <span class="math inline">\(\mu=E(y)\)</span> is the unknown mean of <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>A2. (Linearity)</strong> <span class="math inline">\(\mu=\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}=X\beta\)</span> where <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span> are unknown parameters. This assumption says that <span class="math inline">\(E(y)=\mu\in\text{Col}(X)\)</span> (lies in the column space of <span class="math inline">\(X\)</span>); i.e., it is a linear combination of explanatory vectors <span class="math inline">\(x_{1},...,x_{k}\)</span> with coefficients the unknown parameters in <span class="math inline">\(\beta=(\beta_{1},...,\beta_{k})^{T}\)</span>. Note that it is linear in <span class="math inline">\(\beta_{1},...,\beta_{k}\)</span>, not necessarily in the <span class="math inline">\(x\)</span>’s.</p></li>
<li><p><strong>A3. (Independence)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> are independent random variables (and therefore so are <span class="math inline">\(y_{1},...,y_{n})\)</span>.</p></li>
<li><p><strong>A4. (Homoscedasticity)</strong> <span class="math inline">\(e_{1},...,e_{n}\)</span> all have the same variance <span class="math inline">\(\sigma^{2}\)</span>; that is, <span class="math inline">\(\text{Var}(e_{1})=\cdot\cdot\cdot=\text{Var}(e_{n})=\sigma^{2}\)</span> which implies <span class="math inline">\(\text{Var}(y_{1})=\cdot\cdot\cdot=\text{Var}(y_{n})=\sigma^{2}\)</span>.</p></li>
<li><p><strong>A5. (Normality)</strong> <span class="math inline">\(e\sim N_{n}(0,\sigma^{2}I_{n})\)</span>.</p></li>
</ul>
</section>
<section id="matrix-formulation" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="matrix-formulation"><span class="header-section-number">6.1.2</span> Matrix Formulation</h3>
<p>The model can be written algebraically as: <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdot\cdot\cdot+\beta_{k}x_{ik}, \quad i=1,...,n\]</span></p>
<p>Or in matrix notation: <span class="math display">\[
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdot\cdot\cdot &amp; x_{1k}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdot\cdot\cdot &amp; x_{2k}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdot\cdot\cdot &amp; x_{nk}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{pmatrix}
+
\begin{pmatrix}
e_{1}\\
e_{2}\\
\vdots\\
e_{n}
\end{pmatrix}
\]</span></p>
<p>This is expressed compactly as: <span class="math display">\[y=X\beta+e\]</span> where <span class="math inline">\(X\)</span> is the design matrix, and <span class="math inline">\(e \sim N_n(0, \sigma^2 I)\)</span>. Alternatively: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\cdot\cdot\cdot+\beta_{k}x_{k}+e\]</span></p>
<p>Taken together, all five assumptions can be stated more succinctly as: <span class="math display">\[y\sim N_{n}(X\beta,\sigma^{2}I)\]</span> with the mean vector <span class="math inline">\(\mu_{y}=X\beta\in \text{Col}(X)\)</span>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Coefficients and Variance of Reduced Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The effect of a parameter and the magnitude of the error variance depend upon what other explanatory variables are present in the model. For example, the coefficients <span class="math inline">\(\beta_{0}, \beta_{1}\)</span> and error standard deviation <span class="math inline">\(\sigma\)</span> in the model: <span class="math display">\[y=\beta_{0}j_{n}+\beta_{1}x_{1}+\beta_{2}x_{2}+e, \quad \text{Var}(e) = \sigma^2 I\]</span> will typically be different than <span class="math inline">\(\beta_{0}^{*}, \beta_{1}^{*}\)</span> and <span class="math inline">\(\sigma^*\)</span> in the model: <span class="math display">\[y=\beta_{0}^{*}j_{n}+\beta_{1}^{*}x_{1}+e^*, \quad \text{Var}(e^*) = (\sigma^*)^2 I\]</span> In this context, <span class="math inline">\(\beta_0^*\)</span> and <span class="math inline">\(\beta_1^*\)</span> are the population-projected coefficients of the full model. Furthermore, <span class="math inline">\(\sigma^*\)</span> will typically be larger than <span class="math inline">\(\sigma\)</span>, as the error term <span class="math inline">\(e^*\)</span> absorbs the variation previously explained by <span class="math inline">\(x_2\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will first consider the case that <span class="math inline">\(\text{rank}(X)=k+1\)</span>.</p>
</div>
</div>
</section>
<section id="least-squares-estimator-of-beta-and-fitted-value-hat-y" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="least-squares-estimator-of-beta-and-fitted-value-hat-y"><span class="header-section-number">6.1.3</span> Least Squares Estimator of <span class="math inline">\(\beta\)</span> and Fitted Value <span class="math inline">\(\hat Y\)</span></h3>
<div id="def-least-squares" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 48 (Least Squares Estimator)</strong></span> The <strong>Least Squares Estimator (LSE)</strong> of <span class="math inline">\(\beta\)</span>, denoted as <span class="math inline">\(\hat{\beta}\)</span>, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses <span class="math inline">\(y\)</span> and the fitted values <span class="math inline">\(X\hat{\beta}\)</span>. <span class="math display">\[
Q(\beta) = \sum_{i=1}^n (y_i - x_i^T \beta)^2 = (y - X\beta)'(y - X\beta)
\]</span></p>
</div>
<div id="thm-leastsquare" class="theorem">
<p><span class="theorem-title"><strong>Theorem 47 (Least Squares Estimator)</strong></span> Consider the linear model <span class="math inline">\(y = X\beta + e\)</span>, where <span class="math inline">\(X\)</span> is of full column rank. The Ordinary Least Squares (OLS) estimator <span class="math inline">\(\hat{\beta}\)</span> is given by the closed-form solution:</p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1}X'y\]</span></p>
<p>Consequently, the vector of fitted values <span class="math inline">\(\hat{y}\)</span> is the orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>:</p>
<p><span class="math display">\[\hat{y} = X\hat{\beta} = Hy\]</span></p>
<p>where <span class="math inline">\(H = X(X'X)^{-1}X'\)</span> is the orthogonal projection matrix (hat matrix).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The derivation relies on the geometry of orthogonal projections.</p>
<p><strong>1. Obtaining the Fitted Values <span class="math inline">\(\hat{y}\)</span></strong></p>
<p>In the linear model, the systematic component <span class="math inline">\(E[y]\)</span> is constrained to lie in the column space of <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(\text{Col}(X)\)</span>. We seek the vector in <span class="math inline">\(\text{Col}(X)\)</span> that is “closest” to the observed data <span class="math inline">\(y\)</span>. This vector is the <strong>orthogonal projection</strong> of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>, denoted as <span class="math inline">\(\hat{y}\)</span>. Using the projection matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>, we have:</p>
<p><span class="math display">\[\hat{y} = Hy = X(X'X)^{-1}X' y\]</span></p>
<p><strong>2. Obtaining <span class="math inline">\(\hat{\beta}\)</span> by Solving <span class="math inline">\(X\beta = \hat{y}\)</span></strong></p>
<p>Since <span class="math inline">\(\hat{y}\)</span> is a projection onto <span class="math inline">\(\text{Col}(X)\)</span>, the system <span class="math inline">\(X\hat{\beta} = \hat{y}\)</span> is consistent. To isolate <span class="math inline">\(\hat{\beta}\)</span>, we pre-multiply both sides by <span class="math inline">\((X'X)^{-1}X'\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
(X'X)^{-1}X' (X\hat{\beta}) &amp;= (X'X)^{-1}X' \hat{y} \\
\underbrace{(X'X)^{-1}(X'X)}_{I} \hat{\beta} &amp;= (X'X)^{-1}X' \hat{y} \\
\hat{\beta} &amp;= (X'X)^{-1}X' \hat{y}
\end{aligned}
\]</span></p>
<p>Finally, we express the estimator in terms of the observed <span class="math inline">\(y\)</span>. Because <span class="math inline">\(\hat{y}\)</span> is an orthogonal projection, the residual <span class="math inline">\(y - \hat{y}\)</span> is orthogonal to the columns of <span class="math inline">\(X\)</span>, implying <span class="math inline">\(X'\hat{y} = X'y\)</span>. Substituting this into the equation above yields the result:</p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1}X'y\]</span></p>
</div>
</section>
<section id="properties-of-the-estimator-hat-beta" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="properties-of-the-estimator-hat-beta"><span class="header-section-number">6.1.4</span> Properties of the Estimator <span class="math inline">\(\hat \beta\)</span></h3>
<div id="thm-unbiased" class="theorem">
<p><span class="theorem-title"><strong>Theorem 48 (Unbiasedness of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span>, then <span class="math inline">\(\hat{\beta}\)</span> is an unbiased estimator for <span class="math inline">\(\beta\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
E(\hat{\beta}) &amp;= E[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}E(y) \quad \text{[using linearity of expectation]} \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}X\beta \\
&amp;= \beta
\end{aligned}
\]</span></p>
</div>
<div id="thm-covariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 49 (Variance of <span class="math inline">\(\hat \beta\)</span>)</strong></span> If <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the covariance matrix for <span class="math inline">\(\hat{\beta}\)</span> is given by <span class="math inline">\(\sigma^{2}(X^{\prime}X)^{-1}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\beta}) &amp;= \text{Var}[(X^{\prime}X)^{-1}X^{\prime}y] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}\text{Var}(y)[(X^{\prime}X)^{-1}X^{\prime}]^{\prime} \quad \text{[using } \text{Var}(Ay) = A \text{Var}(y) A'] \\
&amp;= (X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1} \\
&amp;= \sigma^{2}(X^{\prime}X)^{-1}
\end{aligned}
\]</span></p>
</div>
<p><strong>Note:</strong> These theorems require no assumption of normality.</p>
</section>
</section>
<section id="best-linear-unbiased-estimator-blue" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="best-linear-unbiased-estimator-blue"><span class="header-section-number">6.2</span> Best Linear Unbiased Estimator (BLUE)</h2>
<div id="thm-gauss-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 50 (Gauss-Markov Theorem)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the least-squares estimators <span class="math inline">\(\hat{\beta}_{j}, j=0,1,...,k\)</span> have minimum variance among all linear unbiased estimators.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We consider a linear estimator <span class="math inline">\(Ay\)</span> of <span class="math inline">\(\beta\)</span> and seek the matrix <span class="math inline">\(A\)</span> for which <span class="math inline">\(Ay\)</span> is a minimum variance unbiased estimator.</p>
<p><strong>1. Unbiasedness Condition:</strong> In order for <span class="math inline">\(Ay\)</span> to be an unbiased estimator of <span class="math inline">\(\beta\)</span>, we must have <span class="math inline">\(E(Ay)=\beta\)</span>. Using the assumption <span class="math inline">\(E(y)=X\beta\)</span>, this is expressed as: <span class="math display">\[E(Ay) = A E(y) = AX\beta = \beta\]</span> which implies the condition <span class="math inline">\(AX=I_{k+1}\)</span> since the relationship must hold for any <span class="math inline">\(\beta\)</span>.</p>
<p><strong>2. Minimizing Variance:</strong> The covariance matrix for the estimator <span class="math inline">\(Ay\)</span> is: <span class="math display">\[\text{Var}(Ay) = A \text{Var}(y) A' = A(\sigma^2 I) A' = \sigma^2 AA'\]</span> We need to choose <span class="math inline">\(A\)</span> (subject to <span class="math inline">\(AX=I\)</span>) so that the diagonal elements of <span class="math inline">\(AA'\)</span> are minimized.</p>
<p>To relate <span class="math inline">\(Ay\)</span> to <span class="math inline">\(\hat{\beta}=(X'X)^{-1}X'y\)</span>, we define <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span> and write <span class="math inline">\(A = (A - \hat{A}) + \hat{A}\)</span>. Then: <span class="math display">\[AA' = [(A - \hat{A}) + \hat{A}] [(A - \hat{A}) + \hat{A}]'\]</span> Expanding this, the cross terms vanish because <span class="math inline">\((A - \hat{A})\hat{A}' = A\hat{A}' - \hat{A}\hat{A}'\)</span>. Note that <span class="math inline">\(\hat{A}\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}\)</span>. Also, <span class="math inline">\(A\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}\)</span> (since <span class="math inline">\(AX=I\)</span>). Thus, <span class="math inline">\((A - \hat{A})\hat{A}' = 0\)</span>.</p>
<p>The expansion simplifies to: <span class="math display">\[AA' = (A - \hat{A})(A - \hat{A})' + \hat{A}\hat{A}'\]</span> The matrix <span class="math inline">\((A - \hat{A})(A - \hat{A})'\)</span> is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of <span class="math inline">\(AA'\)</span>, we must set <span class="math inline">\(A - \hat{A} = 0\)</span>, which implies <span class="math inline">\(A = \hat{A}\)</span>.</p>
<p>Thus, the minimum variance estimator is: <span class="math display">\[Ay = (X'X)^{-1}X'y = \hat{\beta}\]</span></p>
</div>
<section id="notes-on-gauss-markov" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="notes-on-gauss-markov"><span class="header-section-number">6.2.1</span> Notes on Gauss-markov</h3>
<ol type="1">
<li><p><strong>Distributional Generality:</strong> The remarkable feature of the Gauss-Markov theorem is that it holds for <em>any</em> distribution of <span class="math inline">\(y\)</span>; normality is not required. The only assumptions used are linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>).</p></li>
<li><p><strong>Extension to All Linear Combinations:</strong> The theorem extends beyond just the parameter vector <span class="math inline">\(\beta\)</span> to any linear combination of the parameters.</p></li>
<li><p><strong>Scaling Invariance:</strong> The predictions made by the model are invariant to the scaling of the explanatory variables.</p></li>
</ol>
<div id="cor-linear-combo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 7 (BLUE for All Linear Combinations)</strong></span> If <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, the best linear unbiased estimator of the scalar <span class="math inline">\(a'\beta\)</span> is <span class="math inline">\(a'\hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> is the least-squares estimator.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\tilde{\beta} = Ay\)</span> be any other linear unbiased estimator of <span class="math inline">\(\beta\)</span>. The variance of the linear combination <span class="math inline">\(a'\tilde{\beta}\)</span> is: <span class="math display">\[
\frac{1}{\sigma^2}\text{Var}(a'\tilde{\beta}) = \frac{1}{\sigma^2}\text{Var}(a'Ay) = a'AA'a
\]</span> From the proof of the Gauss-Markov theorem, we established that <span class="math inline">\(AA' = (A-\hat{A})(A-\hat{A})' + (X'X)^{-1}\)</span> where <span class="math inline">\(\hat{A} = (X'X)^{-1}X'\)</span>. Substituting this into the variance equation: <span class="math display">\[
a'AA'a = a'(A-\hat{A})(A-\hat{A})'a + a'(X'X)^{-1}a
\]</span> The term <span class="math inline">\(a'(A-\hat{A})(A-\hat{A})'a\)</span> is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore: <span class="math display">\[
a'AA'a \ge a'(X'X)^{-1}a = \frac{1}{\sigma^2}\text{Var}(a'\hat{\beta})
\]</span> The variance is minimized when <span class="math inline">\(A=\hat{A}\)</span> (specifically when the first term is zero), proving that <span class="math inline">\(a'\hat{\beta}\)</span> has the minimum variance among all linear unbiased estimators.</p>
</div>
<div id="thm-scaling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 51 (Scaling Explanatory Variables)</strong></span> If <span class="math inline">\(x=(1,x_{1},...,x_{k})'\)</span> and <span class="math inline">\(z=(1,c_{1}x_{1},...,c_{k}x_{k})'\)</span>, then the fitted values are identical: <span class="math inline">\(\hat{y} = \hat{\beta}'x = \hat{\beta}_{z}'z\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(D = \text{diag}(1, c_1, ..., c_k)\)</span> such that the design matrix is transformed to <span class="math inline">\(Z = XD\)</span>. The LSE for the transformed data is: <span class="math display">\[
\begin{aligned}
\hat{\beta}_z &amp;= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\
&amp;= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\
&amp;= D^{-1}(X'X)^{-1}X'y = D^{-1}\hat{\beta}
\end{aligned}
\]</span> . Then, the prediction is: <span class="math display">\[
\hat{\beta}_z' z = (D^{-1}\hat{\beta})' (Dx) = \hat{\beta}' (D^{-1})' D x = \hat{\beta}'x
\]</span> .</p>
</div>
</section>
<section id="limitations-restriction-to-unbiased-estimators" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="limitations-restriction-to-unbiased-estimators"><span class="header-section-number">6.2.2</span> Limitations: Restriction to Unbiased Estimators</h3>
<p>It is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of <strong>linear</strong> and <strong>unbiased</strong> estimators.</p>
<ul>
<li><strong>Assumption Sensitivity:</strong> If the assumptions of linearity (<span class="math inline">\(E(y)=X\beta\)</span>) and homoscedasticity (<span class="math inline">\(\text{Var}(y)=\sigma^2 I\)</span>) do not hold, <span class="math inline">\(\hat{\beta}\)</span> may be biased or may have a larger variance than other estimators.</li>
<li><strong>Unbiasedness Constraint:</strong> The theorem does not compare <span class="math inline">\(\hat{\beta}\)</span> to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only “best” (minimum variance) among those estimators that satisfy the unbiasedness constraint.</li>
</ul>
</section>
</section>
<section id="estimator-of-error-variance" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="estimator-of-error-variance"><span class="header-section-number">6.3</span> Estimator of Error Variance</h2>
<p>We estimate <span class="math inline">\(\sigma^{2}\)</span> by the residual mean square:</p>
<div id="def-s2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 49 (Residual Variance Estimator)</strong></span> <span class="math display">\[s^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}(y_{i}-x_{i}'\hat{\beta})^{2} = \frac{\text{SSE}}{n-k-1}\]</span> where <span class="math inline">\(\text{SSE} = (y-X\hat{\beta})'(y-X\hat{\beta})\)</span>.</p>
</div>
<p>Alternatively, SSE can be written as: <span class="math display">\[\text{SSE} = y'y - \hat{\beta}'X'y\]</span> This is often useful for computation (<span class="math inline">\(y'y\)</span> is the total sum of squares of the raw data).</p>
<section id="unbiasedness-of-s2" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="unbiasedness-of-s2"><span class="header-section-number">6.3.1</span> Unbiasedness of <span class="math inline">\(s^2\)</span></h3>
<div id="thm-unbiased-s2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 52 (Unbiasedness of s-squared)</strong></span> If <span class="math inline">\(s^{2}\)</span> is defined as above, and if <span class="math inline">\(E(y)=X\beta\)</span> and <span class="math inline">\(\text{Var}(y)=\sigma^{2}I\)</span>, then <span class="math inline">\(E(s^{2})=\sigma^{2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We use the Hat Matrix <span class="math inline">\(H = X(X'X)^{-1}X'\)</span>, which projects <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>. Thus, <span class="math inline">\(\hat{y} = Hy\)</span>. The residuals are <span class="math inline">\(y - \hat{y} = (I - H)y\)</span>. The Sum of Squared Errors is: <span class="math display">\[\text{SSE} = \|(I-H)y\|^2 = y'(I-H)'(I-H)y\]</span> Since <span class="math inline">\(H\)</span> is symmetric and idempotent, <span class="math inline">\((I-H)\)</span> is also symmetric and idempotent. Thus: <span class="math display">\[\text{SSE} = y'(I-H)y\]</span></p>
<p>To find the expectation, we use the trace trick for quadratic forms: <span class="math inline">\(E[y'Ay] = \text{tr}(A\text{Var}(y)) + E[y]'A E[y]\)</span>. <span class="math display">\[
\begin{aligned}
E(\text{SSE}) &amp;= E[y'(I-H)y] \\
&amp;= \text{tr}((I-H)\sigma^2 I) + (X\beta)'(I-H)(X\beta) \\
&amp;= \sigma^2 \text{tr}(I-H) + \beta'X'(I-H)X\beta
\end{aligned}
\]</span> <strong>Trace Term:</strong> <span class="math inline">\(\text{tr}(I_n - H) = \text{tr}(I_n) - \text{tr}(H) = n - (k+1)\)</span>, since <span class="math inline">\(\text{tr}(H) = \text{tr}(X(X'X)^{-1}X') = \text{tr}((X'X)^{-1}X'X) = \text{tr}(I_{k+1}) = k+1\)</span>.</p>
<p><strong>Non-centrality Term:</strong> Since <span class="math inline">\(HX = X\)</span>, we have <span class="math inline">\((I-H)X = 0\)</span>. Therefore, the second term vanishes: <span class="math inline">\(\beta'X'(I-H)X\beta = 0\)</span>.</p>
<p>Combining these: <span class="math display">\[E(\text{SSE}) = \sigma^2(n - k - 1)\]</span> Dividing by the degrees of freedom <span class="math inline">\((n-k-1)\)</span>, we get <span class="math inline">\(E(s^2) = \sigma^2\)</span>.</p>
</div>
</section>
</section>
<section id="distributions-under-normality" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="distributions-under-normality"><span class="header-section-number">6.4</span> Distributions Under Normality</h2>
<p>If we add Assumption A5 (<span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>), we can derive the exact sampling distributions.</p>
<div id="cor-cov-beta" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 8 (Estimated Covariance of Beta)</strong></span> An unbiased estimator of <span class="math inline">\(\text{Cov}(\hat{\beta})\)</span> is given by: <span class="math display">\[\widehat{\text{Cov}}(\hat{\beta}) = s^{2}(X'X)^{-1}\]</span></p>
</div>
<div id="thm-sampling-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 53 (Sampling Distributions)</strong></span> Under assumptions A1-A5:</p>
<ol type="1">
<li><span class="math inline">\(\hat{\beta} \sim N_{k+1}(\beta, \sigma^{2}(X'X)^{-1})\)</span>.</li>
<li><span class="math inline">\((n-k-1)s^{2}/\sigma^{2} \sim \chi^{2}(n-k-1)\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(s^{2}\)</span> are independent.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part (i):</strong> Since <span class="math inline">\(\hat{\beta} = (X'X)^{-1}X'y\)</span> is a linear transformation of the normal vector <span class="math inline">\(y\)</span>, it is also normally distributed. We already established its mean and variance in <a href="#thm-unbiased" class="quarto-xref">Theorem&nbsp;48</a> and <a href="#thm-covariance" class="quarto-xref">Theorem&nbsp;49</a>.</p>
<p><strong>Part (ii):</strong> We showed <span class="math inline">\(\text{SSE} = y'(I-H)y\)</span>. Since <span class="math inline">\((I-H)\)</span> is idempotent with rank <span class="math inline">\(n-k-1\)</span>, and <span class="math inline">\((I-H)X\beta = 0\)</span>, by the theory of quadratic forms in normal variables, <span class="math inline">\(\text{SSE}/\sigma^2 \sim \chi^2(n-k-1)\)</span>.</p>
<p><strong>Part (iii):</strong> <span class="math inline">\(\hat{\beta}\)</span> depends on <span class="math inline">\(Hy\)</span> (or <span class="math inline">\(X'y\)</span>), while <span class="math inline">\(s^2\)</span> depends on <span class="math inline">\((I-H)y\)</span>. Since <span class="math inline">\(H(I-H) = H - H^2 = 0\)</span>, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.</p>
</div>
</section>
<section id="maximum-likelihood-estimator-mle" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="maximum-likelihood-estimator-mle"><span class="header-section-number">6.5</span> Maximum Likelihood Estimator (MLE)</h2>
<div id="thm-mle" class="theorem">
<p><span class="theorem-title"><strong>Theorem 54 (MLE for Linear Regression)</strong></span> If <span class="math inline">\(y \sim N_n(X\beta, \sigma^2 I)\)</span>, the Maximum Likelihood Estimators are: <span class="math display">\[
\hat{\beta}_{\text{MLE}} = (X'X)^{-1}X'y
\]</span> <span class="math display">\[
\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}(y - X\hat{\beta})'(y - X\hat{\beta}) = \frac{\text{SSE}}{n}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The log-likelihood function is: <span class="math display">\[ \ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta) \]</span> Maximizing this with respect to <span class="math inline">\(\beta\)</span> is equivalent to minimizing the quadratic term <span class="math inline">\((y - X\beta)'(y - X\beta)\)</span>, which yields the Least Squares Estimator. Differentiating with respect to <span class="math inline">\(\sigma^2\)</span> and setting to zero yields <span class="math inline">\(\hat{\sigma}^2 = \text{SSE}/n\)</span>.</p>
</div>
<p><strong>Note:</strong> The MLE for <span class="math inline">\(\sigma^2\)</span> is biased (denominator <span class="math inline">\(n\)</span>), whereas <span class="math inline">\(s^2\)</span> is unbiased (denominator <span class="math inline">\(n-k-1\)</span>).</p>
</section>
<section id="linear-models-in-centered-form" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="linear-models-in-centered-form"><span class="header-section-number">6.6</span> Linear Models in Centered Form</h2>
<p>The regression model can be written in a centered form by subtracting the means of the explanatory variables: <span class="math display">\[y_{i}=\alpha+\beta_{1}(x_{i1}-\overline{x}_{1})+\beta_{2}(x_{i2}-\overline{x}_{2})+\cdot\cdot\cdot+\beta_{k}(x_{ik}-\overline{x}_{k})+e_{i}\]</span> for <span class="math inline">\(i=1,...,n\)</span>, where the intercept term is adjusted: <span class="math display">\[\alpha=\beta_{0}+\beta_{1}\overline{x}_{1}+\beta_{2}\overline{x}_{2}+\cdot\cdot\cdot+\beta_{k}\overline{x}_{k}\]</span> and <span class="math inline">\(\overline{x}_{j}=\frac{1}{n}\sum_{i=1}^{n}x_{ij}\)</span>.</p>
<section id="matrix-formulation-1" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="matrix-formulation-1"><span class="header-section-number">6.6.1</span> Matrix Formulation</h3>
<p>In matrix form, the equivalence between the original model and the centered model is: <span class="math display">\[y = X\beta + e = (j_n, X_c)\begin{pmatrix} \alpha \\ \beta_{1} \end{pmatrix} + e\]</span> where <span class="math inline">\(\beta_{1}=(\beta_{1},...,\beta_{k})^{T}\)</span> represents the slope coefficients, and <span class="math inline">\(X_c\)</span> is the centered design matrix: <span class="math display">\[X_c = (I - P_{j_n})X_1\]</span> Here, <span class="math inline">\(X_1\)</span> consists of the original columns of <span class="math inline">\(X\)</span> excluding the intercept column.</p>
<p>To see the structure of <span class="math inline">\(X_c\)</span>, we first calculate the projection of the data onto the intercept space, <span class="math inline">\(P_{j_n}X_1\)</span>: <span class="math display">\[
\begin{aligned}
P_{j_n}X_1 &amp;= \frac{1}{n}j_n j_n' X_1 \\
&amp;= \begin{pmatrix} 1/n &amp; 1/n &amp; \cdots &amp; 1/n \\ 1/n &amp; 1/n &amp; \cdots &amp; 1/n \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1/n &amp; 1/n &amp; \cdots &amp; 1/n \end{pmatrix} \begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk} \end{pmatrix} \\
&amp;= \begin{pmatrix} \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \end{pmatrix}
\end{aligned}
\]</span> This results in a matrix where every row is the vector of column means. Subtracting this from <span class="math inline">\(X_1\)</span> gives <span class="math inline">\(X_c\)</span>: <span class="math display">\[
\begin{aligned}
X_c &amp;= X_1 - P_{j_n}X_1 \\
&amp;= \begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk} \end{pmatrix} - \begin{pmatrix} \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \bar{x}_1 &amp; \bar{x}_2 &amp; \cdots &amp; \bar{x}_k \end{pmatrix} \\
&amp;= \begin{pmatrix} x_{11} - \bar{x}_1 &amp; x_{12} - \bar{x}_2 &amp; \cdots &amp; x_{1k} - \bar{x}_k \\ x_{21} - \bar{x}_1 &amp; x_{22} - \bar{x}_2 &amp; \cdots &amp; x_{2k} - \bar{x}_k \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n1} - \bar{x}_1 &amp; x_{n2} - \bar{x}_2 &amp; \cdots &amp; x_{nk} - \bar{x}_k \end{pmatrix}
\end{aligned}
\]</span></p>
</section>
<section id="estimation-in-centered-form" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="estimation-in-centered-form"><span class="header-section-number">6.6.2</span> Estimation in Centered Form</h3>
<p>Because the column space of the intercept <span class="math inline">\(j_n\)</span> is orthogonal to the columns of <span class="math inline">\(X_c\)</span> (since columns of <span class="math inline">\(X_c\)</span> sum to zero), the cross-product matrix becomes block diagonal: <span class="math display">\[
\begin{pmatrix} j_n' \\ X_c' \end{pmatrix} (j_n, X_c) = \begin{pmatrix} j_n'j_n &amp; j_n'X_c \\ X_c'j_n &amp; X_c'X_c \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}
\]</span></p>
<div id="thm-centered-estimators" class="theorem">
<p><span class="theorem-title"><strong>Theorem 55 (Centered Estimators)</strong></span> The least squares estimators for the centered parameters are: <span class="math display">\[
\begin{pmatrix} \hat{\alpha} \\ \hat{\beta}_{1} \end{pmatrix} = \begin{pmatrix} n &amp; 0 \\ 0 &amp; X_c'X_c \end{pmatrix}^{-1} \begin{pmatrix} j_n'y \\ X_c'y \end{pmatrix} = \begin{pmatrix} \bar{y} \\ (X_c'X_c)^{-1}X_c'y \end{pmatrix}
\]</span> Thus:</p>
<ol type="1">
<li><span class="math inline">\(\hat{\alpha} = \bar{y}\)</span> (The sample mean of <span class="math inline">\(y\)</span>).</li>
<li><span class="math inline">\(\hat{\beta}_{1} = S_{xx}^{-1}S_{xy}\)</span>, using the sample covariance notations.</li>
</ol>
</div>
<p>Recovering the original intercept: <span class="math display">\[ \hat{\beta}_0 = \hat{\alpha} - \hat{\beta}_1 \bar{x}_1 - \dots - \hat{\beta}_k \bar{x}_k = \bar{y} - \hat{\beta}_{1}'\bar{x} \]</span></p>
</section>
</section>
<section id="decomposition-of-sum-of-squares" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="decomposition-of-sum-of-squares"><span class="header-section-number">6.7</span> Decomposition of Sum of Squares</h2>
<p>We partition the total variation based on the orthogonal subspaces.</p>
<div id="def-ss-components" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 50 (Sum of Squares Components)</strong></span> The total variation is decomposed as <span class="math inline">\(\text{SST} = \text{SSR} + \text{SSE}\)</span>.</p>
<ol type="1">
<li><p><strong>Total Sum of Squares (SST):</strong> The squared length of the centered response vector. <span class="math display">\[\text{SST} = \|y - \bar{y}j_n\|^2 = \|(I - P_{j_n})y\|^2\]</span></p></li>
<li><p><strong>Regression Sum of Squares (SSR):</strong> The variation explained by the regressors <span class="math inline">\(X_c\)</span>. <span class="math display">\[\text{SSR} = \|\hat{y} - \bar{y}j_n\|^2 = \|P_{X_c}y\|^2 = \hat{\beta}_1' X_c' X_c \hat{\beta}_1\]</span></p></li>
<li><p><strong>Sum of Squared Errors (SSE):</strong> The residual variation. <span class="math display">\[\text{SSE} = \|y - \hat{y}\|^2 = \|(I - H)y\|^2\]</span></p></li>
</ol>
</div>
<section id="d-visualization-of-decomposition-of-y" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="d-visualization-of-decomposition-of-y"><span class="header-section-number">6.7.1</span> 3D Visualization of Decomposition of <span class="math inline">\(y\)</span></h3>
<p>We partition the total variation in <span class="math inline">\(y\)</span> based on the orthogonal subspaces.</p>
<ol type="1">
<li><strong>Space of the Mean:</strong> <span class="math inline">\(L(j_n)\)</span>, spanned by the intercept vector <span class="math inline">\(j_n\)</span>.</li>
<li><strong>Space of the Regressors:</strong> <span class="math inline">\(L(X_c)\)</span>, spanned by the centered predictors <span class="math inline">\(X_c\)</span>.</li>
<li><strong>Error Space:</strong> <span class="math inline">\(\text{Col}(X)^\perp\)</span>, orthogonal to the model space.</li>
</ol>
<p>The vector <span class="math inline">\(y\)</span> can be decomposed into three orthogonal components: <span class="math display">\[y = \bar{y}j_n + P_{X_c}y + (y - \hat{y})\]</span> Visually, this corresponds to projecting the vector <span class="math inline">\(y\)</span> onto three orthogonal axes.</p>
<p><strong>Interactive Visualization:</strong></p>
<p>We generate a cloud of 100 observations of <span class="math inline">\(y\)</span> from <span class="math inline">\(N(\mu, \sigma=1)\)</span> where <span class="math inline">\(\mu = (5,5,0)\)</span>. The projections onto the Model Plane (<span class="math inline">\(z=0\)</span>) are highlighted in <strong>red</strong>, and the projections onto the error axis (<span class="math inline">\(z\)</span>) are in <strong>yellow</strong>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Effect Exists (signal)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">No Effect (noise)</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-3635e756f5f9cd0427a7" style="width:100%;height:576px;"></div>
<script type="application/json" data-for="htmlwidget-3635e756f5f9cd0427a7">{"x":{"visdat":{"156215daaa87":["function () ","plotlyVisDat"],"156246f19046":["function () ","data"],"15621d37080d":["function () ","data"],"15627f19c77":["function () ","data"],"15627574cb4":["function () ","data"],"1562746cc12f":["function () ","data"]},"cur_data":"1562746cc12f","attrs":{"156215daaa87":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"colorscale":[[0,1],["steelblue","steelblue"]],"showscale":false,"name":"Model Space","inherit":true},"156246f19046":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"red","symbol":"diamond","opacity":0.80000000000000004},"name":"Proj on Floor","inherit":true},"15621d37080d":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":{},"marker":{"size":4,"color":"black","symbol":"circle","opacity":0.59999999999999998},"name":"Data Cloud","inherit":true},"15627f19c77":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"blue","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(jn)","inherit":true},"15627574cb4":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"green","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(Xc)","inherit":true},"1562746cc12f":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":{},"marker":{"size":4,"color":"gold","symbol":"circle-open","opacity":0.80000000000000004},"name":"Error","inherit":true},"1562746cc12f.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","inherit":true},"1562746cc12f.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","inherit":true},"1562746cc12f.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","inherit":true}},"layout":{"margin":{"b":0,"l":0,"t":30,"r":0},"title":"Scenario A: Effect Exists","scene":{"xaxis":{"title":"L(j<sub>n<\/sub>)","range":[0,8]},"yaxis":{"title":"L(X<sub>c<\/sub>)","range":[-4,8]},"zaxis":{"title":"Col(X)<sup>&perp;<\/sup>","range":[-4,4]},"aspectmode":"cube","camera":{"eye":{"x":1.6000000000000001,"y":1.6000000000000001,"z":0.59999999999999998}}},"showlegend":false,"hovermode":"closest"},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z<br />z","ticklen":2},"colorscale":[[0,"steelblue"],[1,"steelblue"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"name":"Model Space","frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"red","size":4,"symbol":"diamond","opacity":0.80000000000000004,"line":{"color":"rgba(255,127,14,1)"}},"name":"Proj on Floor","error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"black","size":4,"symbol":"circle","opacity":0.59999999999999998,"line":{"color":"rgba(44,160,44,1)"}},"name":"Data Cloud","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"blue","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(214,39,40,1)"}},"name":"Proj L(jn)","error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.4641043867622114,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727325,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466099],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"green","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(148,103,189,1)"}},"name":"Proj L(Xc)","error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"gold","size":4,"symbol":"circle-open","opacity":0.80000000000000004,"line":{"color":"rgba(140,86,75,1)"}},"name":"Error","error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,4],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[4,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[4,4],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Scenario 1: Significant regression effect (<span class="math inline">\(\beta_1
ot= 0\)</span>). The mean vector projects significantly onto the predictor space.</p>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-88e2994b1094f786c85e" style="width:100%;height:576px;"></div>
<script type="application/json" data-for="htmlwidget-88e2994b1094f786c85e">{"x":{"visdat":{"1562528cf66e":["function () ","plotlyVisDat"],"15622486da19":["function () ","data"],"15621158b20b":["function () ","data"],"15625610f912":["function () ","data"],"1562745038e2":["function () ","data"],"15623ec6b916":["function () ","data"]},"cur_data":"15623ec6b916","attrs":{"1562528cf66e":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"colorscale":[[0,1],["steelblue","steelblue"]],"showscale":false,"name":"Model Space","inherit":true},"15622486da19":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"red","symbol":"diamond","opacity":0.80000000000000004},"name":"Proj on Floor","inherit":true},"15621158b20b":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":{},"z":{},"marker":{"size":4,"color":"black","symbol":"circle","opacity":0.59999999999999998},"name":"Data Cloud","inherit":true},"15625610f912":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":{},"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"blue","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(jn)","inherit":true},"1562745038e2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":{},"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"size":4,"color":"green","symbol":"circle-open","opacity":0.59999999999999998},"name":"Proj L(Xc)","inherit":true},"15623ec6b916":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":{},"marker":{"size":4,"color":"gold","symbol":"circle-open","opacity":0.80000000000000004},"name":"Error","inherit":true},"15623ec6b916.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","inherit":true},"15623ec6b916.2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,3],"y":[0,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","inherit":true},"15623ec6b916.3":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","x":[3,0],"y":[0,0],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","inherit":true}},"layout":{"margin":{"b":0,"l":0,"t":30,"r":0},"title":"Scenario B: No Effect","scene":{"xaxis":{"title":"L(j<sub>n<\/sub>)","range":[0,8]},"yaxis":{"title":"L(X<sub>c<\/sub>)","range":[-4,8]},"zaxis":{"title":"Col(X)<sup>&perp;<\/sup>","range":[-4,4]},"aspectmode":"cube","camera":{"eye":{"x":1.6000000000000001,"y":1.6000000000000001,"z":0.59999999999999998}}},"showlegend":false,"hovermode":"closest"},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z<br />z","ticklen":2},"colorscale":[[0,"steelblue"],[1,"steelblue"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"type":"surface","x":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],"y":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],"opacity":0.29999999999999999,"name":"Model Space","frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"red","size":4,"symbol":"diamond","opacity":0.80000000000000004,"line":{"color":"rgba(255,127,14,1)"}},"name":"Proj on Floor","error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"black","size":4,"symbol":"circle","opacity":0.59999999999999998,"line":{"color":"rgba(44,160,44,1)"}},"name":"Data Cloud","error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.674025049152271,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386675,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"blue","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(214,39,40,1)"}},"name":"Proj L(jn)","error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0.12665925699737743,-0.014273377674351514,-0.021435228645658035,0.68430114200722891,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803452,0.061927121922306892,0.10797078437198633,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354425,-0.53589561323778867,0.151764320702129,0.22410488931471309,0.026502113365252069,0.46113373393986878,1.0250423428135722,-0.24551558302826762,-1.1545844378204064,0.50286926223112838,-0.3546003812911962,-0.34400430823367895,0.51278568484834941,-0.14238650352550439,-0.6103588561272677,0.090651739874575116,-0.069445681219522312,0.0028820929499434669,0.19264020056316528,-0.18533001589620471,0.3221882742594166,-0.11024328090937531,0.16589098195784846,0.54841950657467398,0.21759074541690146,-0.1629657927656134,0.57440380922554701,0.49675192798105972,0.27419847975403489,0.11936586755572062,-0.31395303801968572,0.68032622426500367,-0.30012979357356334,1.0936664965082887,0.76630531309259464,-0.11785017955023847,-0.51321045015339029],"z":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"marker":{"color":"green","size":4,"symbol":"circle-open","opacity":0.59999999999999998,"line":{"color":"rgba(148,103,189,1)"}},"name":"Proj L(Xc)","error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"type":"scatter3d","mode":"markers","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"z":[-0.2802378232761063,-0.11508874474163999,0.77935415707456202,0.035254195712287995,0.064643867580473122,0.85753249344164062,0.2304581029946012,-0.63253061730326687,-0.34342642594676298,-0.22283098504997906,0.61204089871973089,0.17990691352868191,0.20038572529702606,0.055341357972559839,-0.27792056737703746,0.89345656840153909,0.24892523911461972,-0.9833085783148191,0.35067795078184272,-0.23639570386396702,-0.53391185299342259,-0.10898745732914752,-0.51300222415361985,-0.36444561464557002,-0.31251963392462845,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.1475357414961356,0.44756283052251122,0.43906674376652111,0.41079054081874361,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619131,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.5615542916016748,-0.20144241764953799,-0.23332767681160937,0.38998255916815894,-0.041684533235914624],"marker":{"color":"gold","size":4,"symbol":"circle-open","opacity":0.80000000000000004,"line":{"color":"rgba(140,86,75,1)"}},"name":"Error","error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[0,3],"y":[0,0],"z":[0,0],"line":{"color":"black","width":6},"name":"Mean Vector","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,3],"y":[0,0],"z":[0,0],"line":{"color":"blue","width":4,"dash":"dash"},"name":"Link to X","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"frame":null},{"type":"scatter3d","mode":"lines","x":[3,0],"y":[0,0],"z":[0,0],"line":{"color":"green","width":4,"dash":"dash"},"name":"Link to Y","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Scenario 2: No regression effect (<span class="math inline">\(\beta_1 = 0\)</span>). The mean vector lies purely on the intercept axis.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-diagram-to-show-decomposition-of-sum-of-squares" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="a-diagram-to-show-decomposition-of-sum-of-squares"><span class="header-section-number">6.7.2</span> A Diagram to Show Decomposition of Sum of Squares</h3>
<p>The decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ss-decomposition-legend-v2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ss-decomposition-legend-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-ss-decomposition-legend-v2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ss-decomposition-legend-v2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Geometric Decomposition: SST = SSR + SSE
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="distribution-of-sum-of-squares" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="distribution-of-sum-of-squares"><span class="header-section-number">6.7.3</span> Distribution of Sum of Squares</h3>
<p>We apply the general theory of projections to the specific components defined in <a href="#def-ss-components" class="quarto-xref">Definition&nbsp;50</a>.</p>
<div id="thm-distribution-ss-v2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 56 (Distribution of Sum of Squares)</strong></span> Let <span class="math inline">\(y \sim N(\mu, \sigma^2 I_n)\)</span>, where <span class="math inline">\(\mu \in \text{Col}(X)\)</span>. Consider the decomposition defined by the projection matrices <span class="math inline">\(P_{X_c}\)</span> and <span class="math inline">\(M = I - H\)</span>.</p>
<ul>
<li><p><strong>Independence:</strong> The quadratic forms <span class="math inline">\(\text{SSR}\)</span> and <span class="math inline">\(\text{SSE}\)</span> are statistically independent because the subspaces <span class="math inline">\(L(X_c)\)</span> and <span class="math inline">\(\text{Col}(X)^\perp\)</span> are orthogonal.</p></li>
<li><p><strong>Distribution of SSE:</strong> The scaled sum of squared errors follows a central Chi-squared distribution: <span class="math display">\[ \frac{\text{SSE}}{\sigma^2} = \frac{\|(I - H)y\|^2}{\sigma^2} \sim \chi^2(n-k-1) \]</span> <strong>Mean:</strong> <span class="math display">\[ E[\text{SSE}] = \sigma^2(n-k-1) \]</span></p></li>
<li><p><strong>Distribution of SSR:</strong> The scaled regression sum of squares follows a <strong>non-central</strong> Chi-squared distribution: <span class="math display">\[ \frac{\text{SSR}}{\sigma^2} = \frac{\|P_{X_c}y\|^2}{\sigma^2} \sim \chi^2(k, \lambda) \]</span> <strong>Mean:</strong> <span class="math display">\[ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 \]</span></p></li>
</ul>
<p><strong>Non-centrality Parameter (<span class="math inline">\(\lambda\)</span>):</strong> <span class="math display">\[ \lambda = \frac{1}{\sigma^2} \|P_{X_c} \mu\|^2 \]</span> where <span class="math display">\[\|P_{X_c} \mu\|^2 = \|X_c \beta_1\|^2 = (X_c \beta_1)' (X_c \beta_1) = \beta_1' X_c' X_c \beta_1\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We apply <a href="#thm-proj-dist" class="quarto-xref">Theorem&nbsp;43</a> to the specific projection matrices identified in the definitions.</p>
<ul>
<li><p><strong>For SSE (Error Space):</strong> <span class="math inline">\(\text{SSE}\)</span> is defined by the projection matrix <span class="math inline">\(P_V = I - H\)</span>.</p>
<ul>
<li><strong>Dimension:</strong> The rank of <span class="math inline">\((I - H)\)</span> is <span class="math inline">\(n - \text{rank}(X) = n - (k+1) = n - k - 1\)</span>.</li>
<li><strong>Non-centrality:</strong> Since <span class="math inline">\(\mu \in \text{Col}(X)\)</span>, the projection onto the orthogonal complement is zero: <span class="math inline">\(\|(I - H)\mu\|^2 = 0\)</span>. Thus, <span class="math inline">\(\lambda = 0\)</span>.</li>
<li><strong>Expectation:</strong> Using Part 2 of <a href="#thm-proj-dist" class="quarto-xref">Theorem&nbsp;43</a> (<span class="math inline">\(E(\|P_V y\|^2) = \sigma^2 \text{rank}(P_V) + \|P_V \mu\|^2\)</span>): <span class="math display">\[ E[\text{SSE}] = \sigma^2(n-k-1) + 0 = \sigma^2(n-k-1) \]</span></li>
</ul></li>
<li><p><strong>For SSR (Regression Space):</strong> <span class="math inline">\(\text{SSR}\)</span> is defined by the projection matrix <span class="math inline">\(P_V = P_{X_c}\)</span>.</p>
<ul>
<li><p><strong>Dimension:</strong> The rank of <span class="math inline">\(P_{X_c}\)</span> is <span class="math inline">\((k+1) - 1 = k\)</span>.</p></li>
<li><p><strong>Non-centrality:</strong> The projection of <span class="math inline">\(\mu\)</span> onto <span class="math inline">\(L(X_c)\)</span> is <span class="math inline">\(P_{X_c}\mu\)</span>. <span class="math display">\[ \lambda = \frac{1}{2\sigma^2} \|P_{X_c} \mu\|^2 \]</span></p></li>
<li><p><strong>Expectation:</strong> Using Part 2 of <a href="#thm-proj-dist" class="quarto-xref">Theorem&nbsp;43</a>: <span class="math display">\[ E[\text{SSR}] = \sigma^2 k + \|P_{X_c}\mu\|^2 \]</span></p></li>
</ul>
<p>This shows that while <span class="math inline">\(E[\text{SSE}]\)</span> depends only on the noise variance and sample size, <span class="math inline">\(E[\text{SSR}]\)</span> is inflated by the magnitude of the true regression signal <span class="math inline">\(\|P_{X_c}\mu\|^2\)</span>.</p></li>
</ul>
</div>
</section>
</section>
<section id="f-test-for-testing-overall-regression-effect" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="f-test-for-testing-overall-regression-effect"><span class="header-section-number">6.8</span> F-test for Testing Overall Regression Effect</h2>
<p>We wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.</p>
<p><strong>Hypotheses:</strong></p>
<ul>
<li><p><strong>Null Hypothesis (<span class="math inline">\(H_0\)</span>):</strong> <span class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_k = 0\)</span> (No regression effect). This implies <span class="math inline">\(\mu \in \text{span}(j_n)\)</span> and the true signal variance <span class="math inline">\(\|X_c\beta_1\|^2 = 0\)</span>.</p></li>
<li><p><strong>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>):</strong> At least one <span class="math inline">\(\beta_j \neq 0\)</span>.</p></li>
</ul>
<section id="the-f-statistic" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-f-statistic">The F-statistic</h3>
<p>We construct the test statistic using the ratio of the Mean Squares defined previously:</p>
<p><span class="math display">\[F = \frac{\text{MSR}}{\text{MSE}} = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}\]</span></p>
</section>
<section id="understanding-f-via-expectations" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="understanding-f-via-expectations">Understanding <span class="math inline">\(F\)</span> via Expectations</h3>
<p>The logic of the F-test is transparent when we examine the expected values of the numerator and denominator:</p>
<p><span class="math display">\[
\begin{aligned}
E[\text{MSE}] &amp;= \sigma^2 \\
E[\text{MSR}] &amp;= \sigma^2 + \frac{\|X_c \beta_1\|^2}{k}
\end{aligned}
\]</span></p>
<ul>
<li><strong>If <span class="math inline">\(H_0\)</span> is true:</strong> The signal term is zero. Both Mean Squares estimate <span class="math inline">\(\sigma^2\)</span> unbiasedly. We expect <span class="math inline">\(F \approx 1\)</span>.</li>
<li><strong>If <span class="math inline">\(H_1\)</span> is true:</strong> The numerator includes the positive term <span class="math inline">\(\frac{\|X_c \beta_1\|^2}{k}\)</span>. We expect <span class="math inline">\(F &gt; 1\)</span>.</li>
</ul>
<p>Therefore, we reject <span class="math inline">\(H_0\)</span> for sufficiently large values of <span class="math inline">\(F\)</span>. Specifically, we reject at level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(F_{obs} &gt; F_{\alpha}(k, n-k-1)\)</span>.</p>
</section>
<section id="distributional-theory" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="distributional-theory"><span class="header-section-number">6.8.1</span> Distributional Theory</h3>
<p>To derive the exact sampling distribution, we rely on the independence of the sums of squares (from <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;56</a>) and the definition of the non-central F-distribution given in <strong><a href="#def-noncentral-f" class="quarto-xref">Definition&nbsp;43</a></strong>.</p>
<div id="thm-regression-f-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 57 (Distribution of Regression F-Statistic)</strong></span> Under the assumption of normality, the regression F-statistic follows a <strong>non-central F-distribution</strong>:</p>
<p><span class="math display">\[ F \sim F(k, n-k-1, \lambda) \]</span></p>
<p>The non-centrality parameter <span class="math inline">\(\lambda\)</span> is determined by the ratio of the signal sum of squares to the error variance: <span class="math display">\[ \lambda = \frac{\|X_c \beta_1\|^2}{\sigma^2} \]</span></p>
<p><strong>Special Cases:</strong></p>
<ol type="1">
<li><strong>Under <span class="math inline">\(H_1\)</span> (Signal exists):</strong> <span class="math inline">\(\lambda &gt; 0\)</span>, so <span class="math inline">\(F\)</span> follows the non-central distribution.</li>
<li><strong>Under <span class="math inline">\(H_0\)</span> (No signal):</strong> <span class="math inline">\(\beta_1 = 0 \implies \lambda = 0\)</span>. The distribution collapses to the <strong>central F-distribution</strong>: <span class="math display">\[ F \sim F(k, n-k-1) \]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We identify the components from <a href="#def-noncentral-f" class="quarto-xref">Definition&nbsp;43</a>:</p>
<ol type="1">
<li><strong>Numerator (<span class="math inline">\(X_1\)</span>):</strong> Let <span class="math inline">\(X_1 = \text{SSR}/\sigma^2\)</span>. From <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;56</a>, <span class="math inline">\(X_1 \sim \chi^2(k, \lambda)\)</span>.</li>
<li><strong>Denominator (<span class="math inline">\(X_2\)</span>):</strong> Let <span class="math inline">\(X_2 = \text{SSE}/\sigma^2\)</span>. From <a href="#thm-distribution-ss-v2" class="quarto-xref">Theorem&nbsp;56</a>, <span class="math inline">\(X_2 \sim \chi^2(n-k-1)\)</span>.</li>
<li><strong>Independence:</strong> <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</li>
</ol>
<p>Substituting these into the F-statistic: <span class="math display">\[
F = \frac{\text{MSR}}{\text{MSE}} = \frac{(\text{SSR}/\sigma^2)/k}{(\text{SSE}/\sigma^2)/(n-k-1)} = \frac{X_1/k}{X_2/(n-k-1)}
\]</span> By definition <a href="#def-noncentral-f" class="quarto-xref">Definition&nbsp;43</a>, this ratio follows <span class="math inline">\(F(k, n-k-1, \lambda)\)</span>.</p>
</div>
</section>
<section id="visualization-of-the-rejection-region" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="visualization-of-the-rejection-region"><span class="header-section-number">6.8.2</span> Visualization of the Rejection Region</h3>
<p>The following plot illustrates the central F-distribution (valid under <span class="math inline">\(H_0\)</span>) for <span class="math inline">\(k=3\)</span> predictors and <span class="math inline">\(n=20\)</span> observations (<span class="math inline">\(df_1 = 3, df_2 = 16\)</span>). An observed statistic of <span class="math inline">\(F=2\)</span> is marked, with the p-value represented by the shaded tail area.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-f-dist-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-f-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-f-dist-example-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-f-dist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="raw-coefficient-of-determination-r2" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="raw-coefficient-of-determination-r2"><span class="header-section-number">6.9</span> Raw Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h2>
<section id="definition" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">6.9.1</span> Definition</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic measures the proportion of total variation explained by the regression model.</p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 51 (R-Squared)</strong></span> <span class="math display">\[R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}\]</span> Since <span class="math inline">\(0 \le \text{SSE} \le \text{SST}\)</span>, it follows that <span class="math inline">\(0 \le R^2 \le 1\)</span>.</p>
</div>
</section>
<section id="expectation-and-bias" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="expectation-and-bias"><span class="header-section-number">6.9.2</span> Expectation and Bias</h3>
<p>To understand the bias in <span class="math inline">\(R^2\)</span>, it is more illuminating to analyze the expectation of the <strong>unexplained variance</strong> (<span class="math inline">\(1 - R^2\)</span>). This term represents the ratio of error sum of squares to the total sum of squares:</p>
<p><span class="math display">\[ E[1 - R^2] = E\left[ \frac{\text{SSE}}{\text{SST}} \right] \]</span></p>
<p>Using the first-order approximation <span class="math inline">\(E[X/Y] \approx E[X]/E[Y]\)</span>, we examine the numerator and denominator separately:</p>
<p><span class="math display">\[
\begin{aligned}
E[\text{SSE}] &amp;= \sigma^2(n-k-1) \\
E[\text{SST}] &amp;= \sigma^2(n-1) + \sigma^2\lambda = \sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)
\end{aligned}
\]</span></p>
<p>Substituting these back, we approximate the expected unexplained fraction:</p>
<p><span class="math display">\[ E[1 - R^2] \approx \frac{\sigma^2(n-k-1)}{\sigma^2 \left( (n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2} \right)} = \frac{n-k-1}{(n-1) + \frac{\|X_c \beta_1\|^2}{\sigma^2}} \]</span></p>
<p><strong>Behavior under Null Hypothesis (<span class="math inline">\(H_0\)</span>):</strong> When there is no true signal (<span class="math inline">\(\beta_1 = 0\)</span>), the term <span class="math inline">\(\frac{\|X_c \beta_1\|^2}{\sigma^2}\)</span> vanishes. The expected proportion of unexplained variance becomes:</p>
<p><span class="math display">\[ E[1 - R^2 | H_0] \approx \frac{n-k-1}{n-1} \]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This result reveals the source of the bias:</p>
<ol type="1">
<li>Ideally, if predictors are noise, the model should explain nothing, and <span class="math inline">\(E[1-R^2]\)</span> should be <span class="math inline">\(1\)</span>.</li>
<li>Instead, the expected error ratio is <strong>less than 1</strong>, specifically scaled by <span class="math inline">\(\frac{n-k-1}{n-1}\)</span>.</li>
<li>This scaling factor is exactly what the <strong>Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</strong> attempts to correct by multiplying the observed ratio by the inverse <span class="math inline">\(\frac{n-1}{n-k-1}\)</span>.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="exact-distribution" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="exact-distribution"><span class="header-section-number">6.9.3</span> Exact Distribution</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.</p>
<div id="thm-r2-dist" class="theorem">
<p><span class="theorem-title"><strong>Theorem 58 (Distribution of R-Squared)</strong></span> <span class="math display">\[ R^2 \sim \text{Beta}_1\left( \frac{k}{2}, \frac{n-k-1}{2}, \lambda \right) \]</span> where <span class="math inline">\(\text{df}_1 = k\)</span> and <span class="math inline">\(\text{df}_2 = n-k-1\)</span>.</p>
</div>
</section>
</section>
<section id="adjusted-r-squared-r2_a" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="adjusted-r-squared-r2_a"><span class="header-section-number">6.10</span> Adjusted R-squared (<span class="math inline">\(R^2_a\)</span>)</h2>
<p>To correct for the inflation of <span class="math inline">\(R^2\)</span> due to model complexity (<span class="math inline">\(k\)</span>), we introduce the Adjusted <span class="math inline">\(R^2\)</span>. This statistic penalizes the sum of squares by their degrees of freedom:</p>
<p><span class="math display">\[ R^2_a = 1 - \frac{\text{SSE}/(n-k-1)}{\text{SST}/(n-1)} = 1 - \frac{\text{MSE}}{\text{MST}} = 1 - (1 - R^2) \frac{n-1}{n-k-1} \]</span></p>
<p><strong>Expectation:</strong></p>
<p>Under <span class="math inline">\(H_0\)</span>, since <span class="math inline">\(E[\text{MSE}] = E[\text{MST}] = \sigma^2\)</span>, the estimator is asymptotically unbiased:</p>
<p><span class="math display">\[ E[R^2_a | H_0] \approx 0 \]</span></p>
<p><strong>Variance and Stability:</strong></p>
<p>While <span class="math inline">\(R^2_a\)</span> corrects the bias, it introduces instability. The variance of <span class="math inline">\(R^2_a\)</span> under <span class="math inline">\(H_0\)</span> can be derived from the variance of the Beta distribution:</p>
<p><span class="math display">\[ \text{Var}(R^2_a | H_0) = \left( \frac{n-1}{n-k-1} \right)^2 \text{Var}(R^2 | H_0) \]</span></p>
<p>Substituting <span class="math inline">\(\text{Var}(R^2 | H_0) = \frac{2k(n-k-1)}{(n-1)^2(n+1)}\)</span>, we obtain:</p>
<p><span class="math display">\[ \text{Var}(R^2_a | H_0) = \frac{2k}{(n-k-1)(n+1)} \]</span></p>
<p><strong>Key Insight:</strong></p>
<p>As the model complexity <span class="math inline">\(k\)</span> increases relative to <span class="math inline">\(n\)</span>:</p>
<ol type="1">
<li>The denominator <span class="math inline">\((n-k-1)\)</span> shrinks.</li>
<li>The variance <span class="math inline">\(\text{Var}(R^2_a)\)</span> explodes.</li>
</ol>
<p>This implies that for high-dimensional models (large <span class="math inline">\(k/n\)</span>), <span class="math inline">\(R^2_a\)</span> becomes an extremely noisy estimator, often yielding large negative values even for null models.</p>
</section>
<section id="population-proportion-of-signals-rho2" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="population-proportion-of-signals-rho2"><span class="header-section-number">6.11</span> Population Proportion of Signals (<span class="math inline">\(\rho^2\)</span>)</h2>
<p>The formula for the expected Adjusted <span class="math inline">\(R^2\)</span> reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation <span class="math inline">\(Y_i\)</span> into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let <span class="math inline">\(\sigma^2_\mu\)</span> denote the signal variance and <span class="math inline">\(\sigma^2\)</span> denote the noise variance:</p>
<p><span class="math display">\[ \text{Var}(Y_i) = E[\text{Var}(Y_i|x_{(i)})] + \text{Var}(E[Y_i|x_{(i)}]) \]</span> <span class="math display">\[ \sigma^2_Y = \sigma^2 + \sigma^2_\mu \]</span></p>
<p>In our derived expectation for <span class="math inline">\(R^2_a\)</span>: <span class="math display">\[ E[R^2_a] \approx \frac{\frac{\|X_c\beta_1\|^2}{n-1}}{\sigma^2 + \frac{\|X_c\beta_1\|^2}{n-1}} \]</span></p>
<p>The term in the numerator, <span class="math inline">\(\frac{\|X_c\beta_1\|^2}{n-1}\)</span>, is precisely the <strong>sample variance of the true means</strong> <span class="math inline">\(\mu_i\)</span>. Let <span class="math inline">\(\mu = X\beta\)</span>. We can expand the centered signal vector <span class="math inline">\(X_c\beta_1\)</span> to see this explicitly. Since <span class="math inline">\(\mu \in \text{Col}(X)\)</span>, we know <span class="math inline">\(H\mu = \mu\)</span>:</p>
<p><span class="math display">\[
X_c\beta_1 = P_{X_c} \mu = (H - P_{j_n})\mu = H\mu - P_{j_n}\mu = \mu - \bar{\mu}j_n =
\begin{pmatrix}
\mu_1 - \bar{\mu} \\
\mu_2 - \bar{\mu} \\
\vdots \\
\mu_n - \bar{\mu}
\end{pmatrix}
\]</span></p>
<p>This vector represents the deviation of each observation’s true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is: <span class="math display">\[ \frac{\|X_c\beta_1\|^2}{n-1} = \frac{\sum_{i=1}^n (\mu_i - \bar{\mu})^2}{n-1} = \sigma^2_\mu \]</span></p>
<p>Thus, <span class="math inline">\(R^2_a\)</span> is therefore an unbiased estimator for the <strong>proportion of variance explained by the signal</strong> in the population: <span class="math display">\[ E[R^2_a] \approx \frac{\sigma^2_\mu}{\sigma^2 + \sigma^2_\mu}\]</span></p>
<p>We will denote this ‘parameter’ by <span class="math inline">\(\rho^2\)</span>:</p>
<p><span class="math display">\[ \rho^2 = 1 - \frac{\sigma^2}{\sigma^2_Y} = \frac{\sigma^2_\mu}{\sigma^2_Y} \]</span></p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>In the fixed covariate framework, the ‘parameter’ <span class="math inline">\(\rho^2\)</span> is a function of the specific design matrix <span class="math inline">\(X\)</span>, the coefficients <span class="math inline">\(\beta\)</span>, and the sample size <span class="math inline">\(n\)</span>. If we assume the <span class="math inline">\(x_i\)</span> are random draws from a population, then as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\sigma^2_\mu\)</span> converges to <span class="math inline">\(\text{Var}(x^T\beta)\)</span> (where <span class="math inline">\(x\)</span> is a random vector), and <span class="math inline">\(\rho^2\)</span> converges to the true population proportion of variance explained.</p>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="MSR Is Not a Variance Estimator">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>MSR Is Not a Variance Estimator
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Observing that <span class="math inline">\(E[\text{MST}] \approx \sigma^2 + \sigma^2_\mu\)</span> and <span class="math inline">\(E[\text{MSE}] = \sigma^2\)</span>, we can see that the difference <span class="math inline">\(\text{MST} - \text{MSE}\)</span> provides a direct method-of-moments estimator for the variance of the signal itself (<span class="math inline">\(\sigma^2_\mu\)</span>).</p></li>
<li><p>It is important to recognize that the commonly used <strong>Mean Square Regression (MSR)</strong>, defined as <span class="math inline">\(\text{SSR}/k\)</span>, is <strong>not</strong> an estimator of the signal variance. Because <span class="math inline">\(E[\text{MSR}] = \sigma^2 + \frac{\|X_c\beta_1\|^2}{k}\)</span>, it scales with the sample size <span class="math inline">\(n\)</span> (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting <em>existence</em> of signal), not for estimating the <em>magnitude</em> of the signal variance.</p></li>
</ul>
</div>
</div>
</section>
<section id="relationship-between-r2-and-f-test" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="relationship-between-r2-and-f-test"><span class="header-section-number">6.12</span> Relationship between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(F\)</span> Test</h2>
<p>The <span class="math inline">\(F\)</span>-statistic for the overall regression effect is a monotonic function of the coefficient of determination. We can express <span class="math inline">\(F\)</span> directly in terms of both the standard <span class="math inline">\(R^2\)</span> and the adjusted <span class="math inline">\(R^2_a\)</span>, as well as relate its expected value to the population variance components.</p>
<ol type="1">
<li><p><strong>Expressing <span class="math inline">\(F\)</span> via Standard <span class="math inline">\(R^2\)</span>:</strong> Since <span class="math inline">\(R^2 = \text{SSR}/\text{SST}\)</span> and <span class="math inline">\(1 - R^2 = \text{SSE}/\text{SST}\)</span>, we can substitute these into the definition of <span class="math inline">\(F\)</span>: <span class="math display">\[
F = \frac{\text{SSR}/k}{\text{SSE}/(n-k-1)} = \frac{(R^2 \cdot \text{SST}) / k}{((1 - R^2) \cdot \text{SST}) / (n - k - 1)} = \frac{R^2}{1 - R^2} \cdot \frac{n - k - 1}{k}
\]</span></p></li>
<li><p><strong>Expressing <span class="math inline">\(F\)</span> via Adjusted <span class="math inline">\(R^2_a\)</span>:</strong> The relationship becomes structurally identical to the population expectation if we use the estimated Signal-to-Noise Ratio. Since <span class="math inline">\(\frac{R^2_a}{1 - R^2_a} = \frac{\hat{\sigma}^2_\mu}{\hat{\sigma}^2}\)</span>, we have: <span class="math display">\[
F = 1 + \frac{n-1}{k} \left( \frac{R^2_a}{1 - R^2_a} \right)
\]</span> This form highlights that <span class="math inline">\(F\)</span> starts at a baseline of 1 (pure noise) and increases proportional to the estimated signal strength.</p></li>
<li><p><strong>Expected Value of <span class="math inline">\(F\)</span> as a function of <span class="math inline">\(\sigma^2_\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>:</strong> Using the population signal variance <span class="math inline">\(\sigma^2_\mu\)</span> and noise variance <span class="math inline">\(\sigma^2\)</span>, the expected value of the <span class="math inline">\(F\)</span>-statistic (using the first-order approximation <span class="math inline">\(E[F] \approx E[\text{MSR}]/E[\text{MSE}]\)</span>) is: <span class="math display">\[
E[F] \approx 1 + \frac{n-1}{k} \left( \frac{\sigma^2_\mu}{\sigma^2} \right)
\]</span> The exact mean, derived from the non-central <span class="math inline">\(F\)</span> distribution, is: <span class="math display">\[
E[F] = \frac{n-k-1}{n-k-3} \left( 1 + \frac{n-1}{k} \frac{\sigma^2_\mu}{\sigma^2} \right), \quad \text{for } n-k-1 &gt; 3
\]</span></p></li>
</ol>
</section>
<section id="confidence-interval-of-population-rho2" class="level2" data-number="6.13">
<h2 data-number="6.13" class="anchored" data-anchor-id="confidence-interval-of-population-rho2"><span class="header-section-number">6.13</span> Confidence Interval of Population <span class="math inline">\(\rho^2\)</span></h2>
<p>While <span class="math inline">\(R^2_a\)</span> provides a point estimate, we can construct an exact confidence interval for <span class="math inline">\(\rho^2\)</span> by exploiting the distribution of the <span class="math inline">\(F\)</span>-statistic.</p>
<p><strong>1. The link between <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\rho^2\)</span>:</strong></p>
<p>Recall that the <span class="math inline">\(F\)</span>-statistic follows a non-central distribution <span class="math inline">\(F(k, n-k-1, \lambda)\)</span>. The non-centrality parameter <span class="math inline">\(\lambda\)</span> is directly related to the population <span class="math inline">\(\rho^2\)</span>. Using the variance decomposition derived above:</p>
<p><span class="math display">\[ \lambda = \frac{\|X_c \beta_1\|^2}{\sigma^2} = (n-1) \left( \frac{\sigma^2_\mu}{\sigma^2} \right) \]</span></p>
<p>Substituting the signal-to-noise ratio <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2} = \frac{\rho^2}{1-\rho^2}\)</span>, we obtain a one-to-one mapping between <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\rho^2\)</span>:</p>
<p><span class="math display">\[ \lambda(\rho^2) = (n-1) \left( \frac{\rho^2}{1-\rho^2} \right) \]</span></p>
<p>To recover <span class="math inline">\(\rho^2\)</span> from <span class="math inline">\(\lambda\)</span>, we invert the mapping:</p>
<p><span class="math display">\[ \rho^2(\lambda) = \frac{\lambda}{\lambda + n - 1} \]</span></p>
<p><strong>2. Inverting the Test Statistic:</strong></p>
<p>We find a confidence interval <span class="math inline">\([\lambda_L, \lambda_U]\)</span> for <span class="math inline">\(\lambda\)</span> by “inverting” the observed <span class="math inline">\(F\)</span>-statistic (<span class="math inline">\(F_{obs}\)</span>). We search for two specific non-central F-distributions: one where <span class="math inline">\(F_{obs}\)</span> cuts off the upper <span class="math inline">\(\alpha/2\)</span> tail, and one where it cuts off the lower <span class="math inline">\(\alpha/2\)</span> tail.</p>
<ul>
<li><strong>Lower Bound (<span class="math inline">\(\lambda_L\)</span>):</strong> The non-centrality parameter such that <span class="math inline">\(F_{obs}\)</span> is the <span class="math inline">\(1-\alpha/2\)</span> quantile.</li>
<li><strong>Upper Bound (<span class="math inline">\(\lambda_U\)</span>):</strong> The non-centrality parameter such that <span class="math inline">\(F_{obs}\)</span> is the <span class="math inline">\(\alpha/2\)</span> quantile.</li>
</ul>
<p>This concept is illustrated in the figure below.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ci-inversion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-inversion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodel-lli_files/figure-html/fig-ci-inversion-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-inversion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Illustration of constructing a confidence interval for the non-centrality parameter <span class="math inline">\(\lambda\)</span> by inverting the F-test. The observed <span class="math inline">\(F_{obs}\)</span> (dashed line) is the <span class="math inline">\(97.5^{th}\)</span> percentile of the distribution defined by the lower bound <span class="math inline">\(\lambda_L\)</span> (blue), and the <span class="math inline">\(2.5^{th}\)</span> percentile of the distribution defined by the upper bound <span class="math inline">\(\lambda_U\)</span> (red). The shaded areas each represent <span class="math inline">\(\alpha/2\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>3. The Interval for <span class="math inline">\(\rho^2\)</span>:</strong></p>
<p>Once <span class="math inline">\([\lambda_L, \lambda_U]\)</span> are found numerically, we map them back to the population <span class="math inline">\(R^2\)</span> scale using the inverse relationship:</p>
<p><span class="math display">\[ \rho^2 = \frac{\lambda}{\lambda + (n-1)} \]</span></p>
<p>This produces an exact confidence interval <span class="math inline">\([\rho^2_L, \rho^2_U]\)</span> for the proportion of variance explained by the model in the population.</p>
</section>
<section id="an-animation-for-illustrating-r2_a-under-h_0-and-h_1" class="level2" data-number="6.14">
<h2 data-number="6.14" class="anchored" data-anchor-id="an-animation-for-illustrating-r2_a-under-h_0-and-h_1"><span class="header-section-number">6.14</span> An Animation for Illustrating <span class="math inline">\(R^2_a\)</span> Under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></h2>
<p>We simulate a dataset with <span class="math inline">\(n=30\)</span> observations and consider a sequence of nested models adding groups of predictors.</p>
<p><strong>Predictor Groups:</strong></p>
<ol type="1">
<li><strong>Group 1 (<span class="math inline">\(k=1\)</span>):</strong> Add <span class="math inline">\(x_1\)</span>. (Signal under <span class="math inline">\(H_1\)</span>).</li>
<li><strong>Group 2 (<span class="math inline">\(k=6\)</span>):</strong> Add <span class="math inline">\(x_2, \dots, x_6\)</span> (Noise).</li>
<li><strong>Group 3 (<span class="math inline">\(k=11\)</span>):</strong> Add <span class="math inline">\(x_7, \dots, x_{11}\)</span> (Noise).</li>
<li><strong>Group 4 (<span class="math inline">\(k=20\)</span>):</strong> Add <span class="math inline">\(x_{12}, \dots, x_{20}\)</span> (Noise).</li>
</ol>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true" href="">Null Hypothesis (<span class="math inline">\(H_0\)</span>)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false" href="">Alternative Hypothesis (<span class="math inline">\(H_1\)</span>)</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Under <span class="math inline">\(H_0\)</span>, the true coefficient for <span class="math inline">\(x_1\)</span> is <span class="math inline">\(\beta_1 = 0\)</span>. All predictors are noise.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<video controls="controls" width="100%">
<source src="figs/rss-h0-v6.mp4" type="video/mp4">
</video>
<p>Simulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable.</p>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>Under <span class="math inline">\(H_1\)</span>, <span class="math inline">\(x_1\)</span> is a true predictor (<span class="math inline">\(\beta_1 = 2\)</span>). The subsequent groups (<span class="math inline">\(x_2 \dots x_{20}\)</span>) remain noise.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<video controls="controls" width="100%">
<source src="figs/rss-h1-v6.mp4" type="video/mp4">
</video>
<p>Simulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-data-example-with-house-price-valuation" class="level2" data-number="6.15">
<h2 data-number="6.15" class="anchored" data-anchor-id="a-data-example-with-house-price-valuation"><span class="header-section-number">6.15</span> A Data Example with House Price Valuation</h2>
<p>A real estate agency wants to refine their pricing model. They regress the selling price of houses (<span class="math inline">\(y\)</span>) on five predictors (<span class="math inline">\(X\)</span>): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.</p>
<p>We assume the data has been collected and saved to <code>house_prices_5pred.csv</code>.</p>
<section id="visualize-the-data" class="level3" data-number="6.15.1">
<h3 data-number="6.15.1" class="anchored" data-anchor-id="visualize-the-data"><span class="header-section-number">6.15.1</span> Visualize the Data</h3>
<p>First, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"house_prices_5pred.csv"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional Display</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (knitr<span class="sc">::</span><span class="fu">is_html_output</span>()) {</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  rmarkdown<span class="sc">::</span><span class="fu">paged_table</span>(df)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(df, <span class="dv">10</span>), <span class="at">caption =</span> <span class="st">"First 10 rows of House Prices"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["Price"],"name":[1],"type":["int"],"align":["right"]},{"label":["Size"],"name":[2],"type":["int"],"align":["right"]},{"label":["Age"],"name":[3],"type":["int"],"align":["right"]},{"label":["Beds"],"name":[4],"type":["int"],"align":["right"]},{"label":["Garage"],"name":[5],"type":["int"],"align":["right"]},{"label":["Lawn"],"name":[6],"type":["int"],"align":["right"]}],"data":[{"1":"497808","2":"3092","3":"4","4":"3","5":"2","6":"426"},{"1":"364297","2":"1802","3":"26","4":"5","5":"0","6":"88"},{"1":"610217","2":"2701","3":"22","4":"4","5":"1","6":"403"},{"1":"536122","2":"2745","3":"38","4":"4","5":"0","6":"437"},{"1":"347259","2":"2143","3":"18","4":"2","5":"1","6":"141"},{"1":"343784","2":"2754","3":"49","4":"5","5":"1","6":"186"},{"1":"379522","2":"2039","3":"53","4":"4","5":"0","6":"451"},{"1":"341432","2":"1758","3":"43","4":"5","5":"1","6":"832"},{"1":"515913","2":"3191","3":"19","4":"4","5":"0","6":"276"},{"1":"292732","2":"1298","3":"17","4":"2","5":"2","6":"804"},{"1":"646447","2":"3255","3":"39","4":"5","5":"0","6":"536"},{"1":"583891","2":"3383","3":"12","4":"4","5":"0","6":"185"},{"1":"485637","2":"2759","3":"38","4":"4","5":"1","6":"184"},{"1":"496960","2":"2289","3":"27","4":"4","5":"0","6":"533"},{"1":"353135","2":"2124","3":"19","4":"2","5":"0","6":"181"},{"1":"533796","2":"3106","3":"43","4":"5","5":"3","6":"443"},{"1":"326891","2":"1276","3":"17","4":"5","5":"2","6":"320"},{"1":"504254","2":"3158","3":"2","4":"4","5":"3","6":"333"},{"1":"350065","2":"1328","3":"40","4":"3","5":"3","6":"713"},{"1":"457862","2":"1739","3":"13","4":"3","5":"1","6":"53"},{"1":"219294","2":"1118","3":"11","4":"2","5":"0","6":"208"},{"1":"333496","2":"1280","3":"15","4":"4","5":"0","6":"109"},{"1":"454666","2":"2701","3":"30","4":"3","5":"2","6":"575"},{"1":"587078","2":"3125","3":"1","4":"4","5":"2","6":"735"},{"1":"496126","2":"2993","3":"41","4":"2","5":"0","6":"295"},{"1":"386942","2":"1731","3":"29","4":"2","5":"2","6":"603"},{"1":"500227","2":"2538","3":"59","4":"4","5":"1","6":"539"},{"1":"554355","2":"3039","3":"18","4":"3","5":"3","6":"235"},{"1":"335301","2":"1001","3":"7","4":"5","5":"3","6":"957"},{"1":"191900","2":"1053","3":"52","4":"3","5":"0","6":"989"},{"1":"452794","2":"2103","3":"33","4":"4","5":"3","6":"240"},{"1":"335370","2":"1726","3":"55","4":"3","5":"2","6":"127"},{"1":"339842","2":"1441","3":"38","4":"3","5":"3","6":"159"},{"1":"535777","2":"3433","3":"2","4":"2","5":"0","6":"104"},{"1":"471557","2":"2688","3":"45","4":"4","5":"3","6":"716"},{"1":"512092","2":"2702","3":"11","4":"3","5":"0","6":"492"},{"1":"308528","2":"1135","3":"44","4":"3","5":"3","6":"646"},{"1":"566085","2":"3415","3":"30","4":"4","5":"1","6":"163"},{"1":"217797","2":"1055","3":"58","4":"4","5":"3","6":"413"},{"1":"483245","2":"2618","3":"34","4":"2","5":"3","6":"989"},{"1":"527446","2":"3182","3":"17","4":"3","5":"2","6":"724"},{"1":"509378","2":"3276","3":"40","4":"3","5":"3","6":"608"},{"1":"485767","2":"2560","3":"13","4":"2","5":"2","6":"151"},{"1":"409876","2":"1980","3":"32","4":"3","5":"0","6":"586"},{"1":"609642","2":"3129","3":"16","4":"4","5":"0","6":"252"},{"1":"376774","2":"2120","3":"49","4":"2","5":"0","6":"397"},{"1":"441467","2":"2039","3":"9","4":"2","5":"1","6":"961"},{"1":"363941","2":"1698","3":"4","4":"3","5":"0","6":"987"},{"1":"561867","2":"2636","3":"15","4":"2","5":"0","6":"816"},{"1":"403582","2":"2626","3":"48","4":"5","5":"3","6":"86"},{"1":"257529","2":"1153","3":"5","4":"2","5":"1","6":"779"},{"1":"528588","2":"2853","3":"37","4":"3","5":"3","6":"865"},{"1":"330393","2":"2132","3":"49","4":"2","5":"2","6":"961"},{"1":"467629","2":"2224","3":"5","4":"4","5":"3","6":"749"},{"1":"570547","2":"2883","3":"24","4":"5","5":"0","6":"577"},{"1":"386896","2":"2790","3":"54","4":"2","5":"0","6":"414"},{"1":"214403","2":"1050","3":"50","4":"5","5":"1","6":"818"},{"1":"341410","2":"1480","3":"10","4":"5","5":"0","6":"752"},{"1":"435568","2":"2141","3":"29","4":"3","5":"1","6":"861"},{"1":"411208","2":"1696","3":"16","4":"5","5":"3","6":"229"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</section>
<section id="fit-the-model" class="level3" data-number="6.15.2">
<h3 data-number="6.15.2" class="anchored" data-anchor-id="fit-the-model"><span class="header-section-number">6.15.2</span> Fit the Model</h3>
<p>We will solve for the coefficients <span class="math inline">\(\hat{\beta}\)</span> using three distinct methods.</p>
<section id="method-1-naive-matrix-formula" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="method-1-naive-matrix-formula">Method 1: Naive Matrix Formula</h4>
<p>This method solves the normal equations directly on the raw data: <span class="math inline">\(\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}y\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define Y and X (add Column of 1s for Intercept)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(df<span class="sc">$</span>Price)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: "lawn" Is Included Here, Even Though It Is Irrelevant</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>X_naive <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="at">Intercept =</span> <span class="dv">1</span>, </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                           df[, <span class="fu">c</span>(<span class="st">"Size"</span>, <span class="st">"Age"</span>, <span class="st">"Beds"</span>, <span class="st">"Garage"</span>, <span class="st">"Lawn"</span>)]))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Compute Intermediate Matrices</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> X_naive</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>Xty <span class="ot">&lt;-</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> y</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display Intermediate Steps</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Matrix X'X (Cross-products of predictors):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Matrix X'X (Cross-products of predictors):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(XtX, <span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          Intercept      Size     Age   Beds Garage     Lawn
Intercept        60    136483    1674    206     80    29392
Size         136483 343078981 3738402 469757 177877 63939128
Age            1674   3738402   63528   5874   2353   827130
Beds            206    469757    5874    776    281    98738
Garage           80    177877    2353    281    196    41915
Lawn          29392  63939128  827130  98738  41915 19306096</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Matrix X'y (Cross-products with response):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Matrix X'y (Cross-products with response):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Xty, <span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 [,1]
Intercept    25884407
Size      63115001244
Age         694594579
Beds         89683035
Garage       34067413
Lawn      12402228016</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Solve Beta</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>beta_naive <span class="ot">&lt;-</span> <span class="fu">solve</span>(XtX) <span class="sc">%*%</span> Xty</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display Result</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Solved Coefficients (Beta):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Solved Coefficients (Beta):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">t</span>(beta_naive))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     Intercept     Size       Age     Beds   Garage    Lawn
[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443</code></pre>
</div>
</div>
</section>
<section id="method-2-centralized-formula" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="method-2-centralized-formula">Method 2: Centralized Formula</h4>
<p>This method reduces multicollinearity issues. Formula: <span class="math inline">\(\hat{\beta}_{\text{slope}} = (X_c^{\prime}X_c)^{-1}X_c^{\prime}y_c\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Center the Data</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>y_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(y)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>X_raw <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(df[, <span class="fu">c</span>(<span class="st">"Size"</span>, <span class="st">"Age"</span>, <span class="st">"Beds"</span>, <span class="st">"Garage"</span>, <span class="st">"Lawn"</span>)])</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>X_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(X_raw)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>y_c <span class="ot">&lt;-</span> y <span class="sc">-</span> y_bar</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>X_c <span class="ot">&lt;-</span> <span class="fu">sweep</span>(X_raw, <span class="dv">2</span>, X_means) </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Compute Intermediate Matrices</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>XctXc <span class="ot">&lt;-</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> X_c</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>Xctyc <span class="ot">&lt;-</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y_c</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Display Intermediate Steps</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Matrix X_c'X_c (Centered Sum of Squares):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Matrix X_c'X_c (Centered Sum of Squares):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(XctXc, <span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           Size    Age  Beds Garage     Lawn
Size   32618826 -69474  1165  -4100 -2919344
Age      -69474  16823   127    121     7093
Beds       1165    127    69      6    -2175
Garage    -4100    121     6     89     2726
Lawn   -2919344   7093 -2175   2726  4907935</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Matrix X_c'y_c (Centered Cross-products):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Matrix X_c'y_c (Centered Cross-products):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Xctyc, <span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]
Size   4235309234
Age     -27580376
Beds       813238
Garage    -445130
Lawn   -277680160</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Solve for Slope Coefficients</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>beta_slope <span class="ot">&lt;-</span> <span class="fu">solve</span>(XctXc) <span class="sc">%*%</span> Xctyc</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Recover Intercept</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> y_bar <span class="sc">-</span> <span class="fu">sum</span>(X_means <span class="sc">*</span> beta_slope)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>beta_central <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="at">Intercept =</span> beta_0, beta_slope)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display Result</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Solved Coefficients (Beta):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Solved Coefficients (Beta):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">t</span>(beta_central))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     Intercept     Size       Age     Beds   Garage    Lawn
[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443</code></pre>
</div>
</div>
</section>
<section id="method-3-using-rs-lm-function" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="method-3-using-rs-lm-function">Method 3: Using R’s <code>lm</code> Function</h4>
<p>This is the standard approach for practitioners.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Model</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>model_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Price <span class="sc">~</span> ., <span class="at">data =</span> df)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>y_hat_lm <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model_lm)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract Coefficients</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(model_lm))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Price ~ ., data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-135178  -36006    1710   26401  111967 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 113185.971  35675.435   3.173  0.00249 ** 
Size           129.343      8.927  14.490  &lt; 2e-16 ***
Age          -1218.352    386.414  -3.153  0.00264 ** 
Beds         12664.157   6064.435   2.088  0.04150 *  
Garage         875.115   5316.490   0.165  0.86987    
Lawn            27.244     23.243   1.172  0.24629    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 49360 on 54 degrees of freedom
Multiple R-squared:  0.8161,    Adjusted R-squared:  0.799 
F-statistic: 47.92 on 5 and 54 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
</section>
<section id="visualization-of-fitted-values-vs-mean" class="level3" data-number="6.15.3">
<h3 data-number="6.15.3" class="anchored" data-anchor-id="visualization-of-fitted-values-vs-mean"><span class="header-section-number">6.15.3</span> Visualization of Fitted Values vs Mean</h3>
<p>We define <span class="math inline">\(\hat{y}_0\)</span> as the vector of the mean of <span class="math inline">\(y\)</span> (<span class="math inline">\(\bar{y}\)</span>). We plot the actual <span class="math inline">\(y\)</span> against our fitted model <span class="math inline">\(\hat{y}\)</span>, using a green line to represent the “Null Model” (<span class="math inline">\(\hat{y}_0\)</span>).</p>
<p><em>Note: Axes have been set so that X = Predicted Value and Y = Actual Value.</em></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define y_hat_0 (The Null Model) - for conceptual clarity</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>y_hat_0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(y), <span class="fu">length</span>(y))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot (Axes reversed: x=fitted, y=actual)</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y_hat_lm, y,</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Actual vs Fitted Prices"</span>,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Fitted Price (y_hat)"</span>,</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Actual Price (y)"</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add 1:1 line (Perfect fit area, remains y=x)</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"gray"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add Mean line representing the null model</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Since y-axis is 'actual y', a horizontal line at mean(y) represents y_bar</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span> (y), <span class="at">h =</span> <span class="fu">mean</span>(y), <span class="at">col =</span> <span class="st">"green"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Data"</span>, <span class="st">"Mean (y_bar)"</span>),</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"green"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="linearmodel-lli_files/figure-html/plot-y-vs-yhat-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p><strong>Question:</strong></p>
<p><span class="math display">\[ \bar y = \bar{\hat{y}} ?\]</span></p>
</section>
<section id="computing-sums-of-squares-sse-sst-ssr" class="level3" data-number="6.15.4">
<h3 data-number="6.15.4" class="anchored" data-anchor-id="computing-sums-of-squares-sse-sst-ssr"><span class="header-section-number">6.15.4</span> Computing Sums of Squares (SSE, SST, SSR)</h3>
<p>We compare different methods to calculate the sources of variation.</p>
<section id="naive-sum-of-squared-errors" class="level4" data-number="6.15.4.1">
<h4 data-number="6.15.4.1" class="anchored" data-anchor-id="naive-sum-of-squared-errors"><span class="header-section-number">6.15.4.1</span> 1. Naive Sum of Squared Errors</h4>
<p>This uses the standard summation definitions: <span class="math inline">\(\sum (Difference)^2\)</span>.</p>
<ul>
<li><strong>SST (Total):</strong> Variation of <span class="math inline">\(y\)</span> around <span class="math inline">\(\hat{y}_0\)</span> (Mean).</li>
<li><strong>SSR (Regression):</strong> Variation of <span class="math inline">\(\hat{y}\)</span> around <span class="math inline">\(\hat{y}_0\)</span> (Mean).</li>
<li><strong>SSE (Error):</strong> Variation of <span class="math inline">\(y\)</span> around <span class="math inline">\(\hat{y}\)</span> (Model).</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectors</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>y_vec <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y_hat_lm)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>y_bar_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(y), <span class="fu">length</span>(y))</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculations</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>SST_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>SSR_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>SSE_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Naive Calculation:</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Naive Calculation:</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SST:"</span>, SST_naive, <span class="st">" SSR:"</span>, SSR_naive, <span class="st">" SSE:"</span>, SSE_naive, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SST: 715333529746  SSR: 583756306788  SSE: 131577222958 </code></pre>
</div>
</div>
</section>
<section id="pythagorean-shortcut-vector-lengths" class="level4" data-number="6.15.4.2">
<h4 data-number="6.15.4.2" class="anchored" data-anchor-id="pythagorean-shortcut-vector-lengths"><span class="header-section-number">6.15.4.2</span> 2. Pythagorean Shortcut (Vector Lengths)</h4>
<p>Based on the geometry of least squares, we can treat the variables as vectors. Because the vectors are orthogonal, we can use squared lengths (dot products with themselves).</p>
<p>Formula: <span class="math inline">\(SSR = ||\hat{y}||^2 - ||\hat{y}_0||^2\)</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for squared Euclidean norm (length squared)</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>len_sq <span class="ot">&lt;-</span> <span class="cf">function</span>(v) <span class="fu">sum</span>(v<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># SST = ||y||^2 - ||y_0||^2</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>SST_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_vec) <span class="sc">-</span> <span class="fu">len_sq</span>(y_bar_vec)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># SSR = ||y_hat||^2 - ||y_0||^2</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>SSR_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_hat) <span class="sc">-</span> <span class="fu">len_sq</span>(y_bar_vec)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># SSE = ||y||^2 - ||y_hat||^2</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>SSE_pyth <span class="ot">&lt;-</span> <span class="fu">len_sq</span>(y_vec) <span class="sc">-</span> <span class="fu">len_sq</span>(y_hat)</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Pythagorean Calculation:</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Pythagorean Calculation:</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SST:"</span>, SST_pyth, <span class="st">" SSR:"</span>, SSR_pyth, <span class="st">" SSE:"</span>, SSE_pyth, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SST: 715333529746  SSR: 583756306788  SSE: 131577222958 </code></pre>
</div>
</div>
</section>
<section id="matrix-algebra-shortcuts" class="level4" data-number="6.15.4.3">
<h4 data-number="6.15.4.3" class="anchored" data-anchor-id="matrix-algebra-shortcuts"><span class="header-section-number">6.15.4.3</span> Matrix Algebra Shortcuts</h4>
<p>These formulas use the <span class="math inline">\(\beta\)</span> and <span class="math inline">\(X\)</span> matrices directly. This is computationally efficient for large datasets.</p>
<ul>
<li>Formula A (Centered with <span class="math inline">\(y_c\)</span>): <span class="math inline">\(SSR = \hat{\beta}_c^{\prime} X_c^{\prime} y_c\)</span></li>
<li>Formula B (Alternative with <span class="math inline">\(y\)</span>): <span class="math inline">\(SSR = \hat{\beta}_c^{\prime} X_c^{\prime} y\)</span></li>
<li>Formula C (Uncentered): <span class="math inline">\(SSR = \hat{\beta}^{\prime} X^{\prime} y - n\bar{y}^2\)</span></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>term_correction <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">mean</span>(y)<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># --- SSR Calculations ---</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. SSR Formula A (Centered, using y_c)</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>SSR_centered_yc <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_slope) <span class="sc">%*%</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y_c</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. SSR Formula A (Alternative, using raw y)</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Since X_c is centered, X_c' * 1 = 0, so X_c'y_c is equivalent to X_c'y</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>SSR_centered_y <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_slope) <span class="sc">%*%</span> <span class="fu">t</span>(X_c) <span class="sc">%*%</span> y</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. SSR Formula B (Uncentered Matrix)</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co"># beta_naive includes intercept, X_naive includes column of 1s</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>term_beta_X_y <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_naive) <span class="sc">%*%</span> <span class="fu">t</span>(X_naive) <span class="sc">%*%</span> y</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>SSR_uncentered <span class="ot">&lt;-</span> term_beta_X_y <span class="sc">-</span> term_correction</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Equivalence Check Table ---</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>results_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">Metric =</span> <span class="fu">c</span>(<span class="st">"SSR (Centered $X_c,y_c$)"</span>, </span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>             <span class="st">"SSR (Centered $X_c$)"</span>, </span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>             <span class="st">"SSR (Uncentered)"</span>),</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">Formula =</span> <span class="fu">c</span>(<span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}_c' X_c' y_c$"</span>, </span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>              <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}_c' X_c' y$"</span>, </span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>              <span class="st">"$</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">beta}' X' y - n</span><span class="sc">\\</span><span class="st">bar{y}^2$"</span>),</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">Value =</span> <span class="fu">c</span>(<span class="fu">as.numeric</span>(SSR_centered_yc), </span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.numeric</span>(SSR_centered_y), </span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.numeric</span>(SSR_uncentered))</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Render the table</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(results_table, </span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>             <span class="at">digits =</span> <span class="dv">4</span>, </span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Demonstration of SSR Formula Equivalence"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Demonstration of SSR Formula Equivalence</caption>
<colgroup>
<col style="width: 35%">
<col style="width: 46%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SSR (Centered <span class="math inline">\(X_c,y_c\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}_c' X_c' y_c\)</span></td>
<td style="text-align: right;">583756306788</td>
</tr>
<tr class="even">
<td style="text-align: left;">SSR (Centered <span class="math inline">\(X_c\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}_c' X_c' y\)</span></td>
<td style="text-align: right;">583756306788</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SSR (Uncentered)</td>
<td style="text-align: left;"><span class="math inline">\(\hat{\beta}' X' y - n\bar{y}^2\)</span></td>
<td style="text-align: right;">583756306788</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
<section id="analysis-of-variance-anova" class="level3" data-number="6.15.5">
<h3 data-number="6.15.5" class="anchored" data-anchor-id="analysis-of-variance-anova"><span class="header-section-number">6.15.5</span> Analysis of Variance (ANOVA)</h3>
<p>We now evaluate the sources of variation to test the overall model significance.</p>
<section id="computing-sums-of-squares" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="computing-sums-of-squares">1. Computing Sums of Squares</h4>
<p>We calculate the following components:</p>
<ul>
<li>Total Sum of Squares: <span class="math inline">\(\text{SST} = \sum (y_i - \bar{y})^2\)</span></li>
<li>Regression Sum of Squares: <span class="math inline">\(\text{SSR} = \sum (\hat{y}_i - \bar{y})^2\)</span></li>
<li>Sum of Squared Errors: <span class="math inline">\(\text{SSE} = \sum (y_i - \hat{y}_i)^2\)</span></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectors</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>y_vec <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(y_hat_lm)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>y_bar_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(y), <span class="fu">length</span>(y))</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculations</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>SST_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>SSR_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat <span class="sc">-</span> y_bar_vec)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>SSE_naive <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_vec <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"SST:"</span>, SST_naive, <span class="st">" SSR:"</span>, SSR_naive, <span class="st">" SSE:"</span>, SSE_naive, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SST: 715333529746  SSR: 583756306788  SSE: 131577222958 </code></pre>
</div>
</div>
</section>
<section id="manual-anova-construction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="manual-anova-construction">2. Manual ANOVA Construction</h4>
<p>We build the table manually using the sums of squares and degrees of freedom. We calculate the Mean Squares and the F-statistic:</p>
<ul>
<li><span class="math inline">\(\text{MSR} = \text{SSR} / k\)</span></li>
<li><span class="math inline">\(\text{MSE} = \text{SSE} / (n - k - 1)\)</span></li>
<li><span class="math inline">\(\text{MST} = \text{SST} / (n - 1)\)</span></li>
<li><span class="math inline">\(F = \text{MSR} / \text{MSE}\)</span></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span>             <span class="co"># Predictors</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>df_e <span class="ot">&lt;-</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>  <span class="co"># Error DF</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>df_t <span class="ot">&lt;-</span> n <span class="sc">-</span> <span class="dv">1</span>      <span class="co"># Total DF</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean Squares</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>MSR <span class="ot">&lt;-</span> SSR_naive <span class="sc">/</span> k</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> SSE_naive <span class="sc">/</span> df_e</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>MST <span class="ot">&lt;-</span> SST_naive <span class="sc">/</span> df_t <span class="co"># Mean Square Total (Variance of Y)</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="co"># F-statistic</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>F_stat <span class="ot">&lt;-</span> MSR <span class="sc">/</span> MSE</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co"># P-value</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>p_val <span class="ot">&lt;-</span> <span class="fu">pf</span>(F_stat, k, df_e, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Assemble Table</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>anova_manual <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">Source =</span> <span class="fu">c</span>(<span class="st">"Regression (Model)"</span>, <span class="st">"Error (Residual)"</span>, <span class="st">"Total"</span>),</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">DF =</span> <span class="fu">c</span>(k, df_e, df_t),</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">SS =</span> <span class="fu">c</span>(SSR_naive, SSE_naive, SST_naive),</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">MS =</span> <span class="fu">c</span>(MSR, MSE, MST), <span class="co"># Included MST here</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">F_Statistic =</span> <span class="fu">c</span>(F_stat, <span class="cn">NA</span>, <span class="cn">NA</span>),</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">P_Value =</span> <span class="fu">c</span>(p_val, <span class="cn">NA</span>, <span class="cn">NA</span>)</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(anova_manual, <span class="at">digits =</span> <span class="dv">4</span>, <span class="at">caption =</span> <span class="st">"Manual ANOVA Table"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Manual ANOVA Table</caption>
<colgroup>
<col style="width: 27%">
<col style="width: 4%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 17%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Source</th>
<th style="text-align: right;">DF</th>
<th style="text-align: right;">SS</th>
<th style="text-align: right;">MS</th>
<th style="text-align: right;">F_Statistic</th>
<th style="text-align: right;">P_Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Regression (Model)</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">583756306788</td>
<td style="text-align: right;">116751261358</td>
<td style="text-align: right;">47.9153</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Error (Residual)</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">131577222958</td>
<td style="text-align: right;">2436615240</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">715333529746</td>
<td style="text-align: right;">12124297114</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="standard-r-output-anova" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="standard-r-output-anova">3. Standard R Output (<code>anova</code>)</h4>
<p>We display the standard <code>summary()</code> which provides the coefficients, t-tests, and the overall F-statistic found at the bottom. We also show <code>anova()</code> which gives the sequential sum of squares.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit an intercept-only (null) model and compare to the fitted model</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>model_null <span class="ot">&lt;-</span> <span class="fu">lm</span>(Price <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> df)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">ANOVA comparing intercept-only model to fitted model:</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
ANOVA comparing intercept-only model to fitted model:</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">anova</span>(model_null, model_lm))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: Price ~ 1
Model 2: Price ~ Size + Age + Beds + Garage + Lawn
  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
1     59 7.1533e+11                                   
2     54 1.3158e+11  5 5.8376e+11 47.915 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One can call anova directly to model_lm</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">anova</span>(model_lm))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Response: Price
          Df     Sum Sq    Mean Sq  F value    Pr(&gt;F)    
Size       1 5.4992e+11 5.4992e+11 225.6914 &lt; 2.2e-16 ***
Age        1 2.0657e+10 2.0657e+10   8.4777  0.005216 ** 
Beds       1 9.5872e+09 9.5872e+09   3.9346  0.052396 .  
Garage     1 2.4151e+08 2.4151e+08   0.0991  0.754107    
Lawn       1 3.3476e+09 3.3476e+09   1.3739  0.246291    
Residuals 54 1.3158e+11 2.4366e+09                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
</section>
<section id="coefficient-of-determination-and-variance-decomposition" class="level3" data-number="6.15.6">
<h3 data-number="6.15.6" class="anchored" data-anchor-id="coefficient-of-determination-and-variance-decomposition"><span class="header-section-number">6.15.6</span> Coefficient of Determination and Variance Decomposition</h3>
<p>We calculate <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span>, and then present them in a <strong>Variance Decomposition Table</strong>.</p>
<section id="calculation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="calculation">1. Calculation</h4>
<p>We calculate the coefficients of determination:</p>
<ul>
<li>Standard <span class="math inline">\(R^2 = 1 - \frac{\text{SSE}}{\text{SST}}\)</span></li>
<li>Adjusted <span class="math inline">\(R^2_a = 1 - \frac{\text{MSE}}{\text{MST}}\)</span></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard R-squared</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>R2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (SSE_naive <span class="sc">/</span> SST_naive)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjusted R-squared</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Formula: 1 - (MSE / MST)</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>R2_adj <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (MSE <span class="sc">/</span> MST)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Standard R^2:  "</span>, <span class="fu">round</span>(R2, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Standard R^2:   0.8161 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Adjusted R^2:  "</span>, <span class="fu">round</span>(R2_adj, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Adjusted R^2:   0.799 </code></pre>
</div>
</div>
</section>
<section id="variance-decomposition-table" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="variance-decomposition-table">2. Variance Decomposition Table</h4>
<p>This table extends standard ANOVA. While ANOVA focuses on <strong>Mean Squares (MS)</strong> for hypothesis testing (is <span class="math inline">\(MSR &gt; MSE\)</span>?), this table focuses on <strong>Variance Components (<span class="math inline">\(\hat{\sigma}^2\)</span>)</strong> for estimation (how much variance is Signal vs.&nbsp;Noise?). We estimate the variance components as follows:</p>
<ul>
<li><p>Signal Variance: <span class="math inline">\(\hat{\sigma}^2_\mu = \text{MST} - \text{MSE}\)</span></p></li>
<li><p>Noise Variance: <span class="math inline">\(\hat{\sigma}^2 = \text{MSE}\)</span></p></li>
<li><p>Total Variance: <span class="math inline">\(\hat{\sigma}^2_Y = \text{MST}\)</span></p></li>
<li><p><strong>Signal Variance (<span class="math inline">\(\hat{\sigma}^2_\mu\)</span>):</strong> Estimated by <span class="math inline">\(MST - MSE\)</span>. (Note: <span class="math inline">\(MSR\)</span> is biased and overestimates signal).</p></li>
<li><p><strong>Noise Variance (<span class="math inline">\(\hat{\sigma}^2\)</span>):</strong> Estimated by <span class="math inline">\(MSE\)</span>.</p></li>
<li><p><strong>Total Variance (<span class="math inline">\(\hat{\sigma}^2_Y\)</span>):</strong> Estimated by <span class="math inline">\(MST\)</span>.</p></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance Component Estimators (Method of Moments)</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>sigma2_noise_est  <span class="ot">&lt;-</span> MSE</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>sigma2_total_est  <span class="ot">&lt;-</span> MST</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>sigma2_signal_est <span class="ot">&lt;-</span> MST <span class="sc">-</span> MSE</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportions</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>prop_signal <span class="ot">&lt;-</span> sigma2_signal_est <span class="sc">/</span> sigma2_total_est <span class="co"># Equals R^2_adj</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>prop_noise  <span class="ot">&lt;-</span> sigma2_noise_est <span class="sc">/</span> sigma2_total_est  <span class="co"># Equals 1 - R^2_adj</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Assemble Table</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>decomp_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">Component =</span> <span class="fu">c</span>(<span class="st">"Signal (Model)"</span>, <span class="st">"Noise (Error)"</span>, <span class="st">"Total (Y)"</span>),</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">DF =</span> <span class="fu">c</span>(k, df_e, df_t),</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">SS =</span> <span class="fu">c</span>(SSR_naive, SSE_naive, SST_naive),</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">MS =</span> <span class="fu">c</span>(<span class="cn">NA</span>, MSE, MST),</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">Estimator_Sigma2 =</span> <span class="fu">c</span>(sigma2_signal_est, sigma2_noise_est, sigma2_total_est),</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">Proportion =</span> <span class="fu">c</span>(prop_signal, prop_noise, <span class="fl">1.0</span>)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Display</span></span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(decomp_table, </span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">digits =</span> <span class="dv">4</span>, </span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>             <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Component"</span>, <span class="st">"DF"</span>, <span class="st">"SS"</span>, <span class="st">"MS"</span>, <span class="st">"Value ($</span><span class="sc">\\</span><span class="st">hat{</span><span class="sc">\\</span><span class="st">sigma}^2$)"</span>, <span class="st">"Proportion"</span>),</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Variance Decomposition Table: Estimating Signal vs. Noise"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Variance Decomposition Table: Estimating Signal vs.&nbsp;Noise</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 3%">
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 31%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: right;">DF</th>
<th style="text-align: right;">SS</th>
<th style="text-align: right;">MS</th>
<th style="text-align: right;">Value (<span class="math inline">\(\hat{\sigma}^2\)</span>)</th>
<th style="text-align: right;">Proportion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Signal (Model)</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">583756306788</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">9687681874</td>
<td style="text-align: right;">0.799</td>
</tr>
<tr class="even">
<td style="text-align: left;">Noise (Error)</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">131577222958</td>
<td style="text-align: right;">2436615240</td>
<td style="text-align: right;">2436615240</td>
<td style="text-align: right;">0.201</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total (Y)</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">715333529746</td>
<td style="text-align: right;">12124297114</td>
<td style="text-align: right;">12124297114</td>
<td style="text-align: right;">1.000</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
<section id="confidence-interval-for-population-r2-rho2" class="level3" data-number="6.15.7">
<h3 data-number="6.15.7" class="anchored" data-anchor-id="confidence-interval-for-population-r2-rho2"><span class="header-section-number">6.15.7</span> Confidence Interval for Population <span class="math inline">\(R^2\)</span> (<span class="math inline">\(\rho^2\)</span>)</h3>
<p>We construct a 95% confidence interval for the population proportion of variance explained (<span class="math inline">\(\rho^2\)</span>).</p>
<section id="manual-inversion-method" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="manual-inversion-method">1. Manual Inversion Method</h4>
<p>We solve for the non-centrality parameters <span class="math inline">\(\lambda_L\)</span> and <span class="math inline">\(\lambda_U\)</span> such that our observed <span class="math inline">\(F_{obs}\)</span> corresponds to the appropriate quantiles.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define Helper Function to Find Lambda</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We want to find lambda such that: pf(F_stat, df1, df2, ncp = lambda) = target_prob</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>get_lambda <span class="ot">&lt;-</span> <span class="cf">function</span>(target_prob, F_val, df1, df2) {</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>  f_root <span class="ot">&lt;-</span> <span class="cf">function</span>(lam) {</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pf</span>(F_val, df1, df2, <span class="at">ncp =</span> lam) <span class="sc">-</span> target_prob</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tryCatch</span>({</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    res <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f_root, <span class="at">interval =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1000</span>))<span class="sc">$</span>root</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(res)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>  }, <span class="at">error =</span> <span class="cf">function</span>(e) <span class="fu">return</span>(<span class="cn">NA</span>))</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Calculate Lambda Bounds (95% CI -&gt; alpha = 0.05)</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Lower Bound Lambda: F_obs is the (1 - alpha/2) quantile</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>lambda_Lower <span class="ot">&lt;-</span> <span class="fu">get_lambda</span>(<span class="dv">1</span> <span class="sc">-</span> alpha<span class="sc">/</span><span class="dv">2</span>, F_stat, k, df_e)</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Upper Bound Lambda: F_obs is the (alpha/2) quantile</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>lambda_Upper <span class="ot">&lt;-</span> <span class="fu">get_lambda</span>(alpha<span class="sc">/</span><span class="dv">2</span>, F_stat, k, df_e)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">is.na</span>(lambda_Lower)) lambda_Lower <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert Lambda to Rho^2</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Formula for Fixed Predictors: rho^2 = lambda / (lambda + n)</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>rho2_Lower <span class="ot">&lt;-</span> lambda_Lower <span class="sc">/</span> (lambda_Lower <span class="sc">+</span> n)</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>rho2_Upper <span class="ot">&lt;-</span> lambda_Upper <span class="sc">/</span> (lambda_Upper <span class="sc">+</span> n)</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Manual Calculation:</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Manual Calculation:</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"95% CI for Population Rho^2: ["</span>, <span class="fu">round</span>(rho2_Lower, <span class="dv">4</span>), <span class="st">", "</span>, <span class="fu">round</span>(rho2_Upper, <span class="dv">4</span>), <span class="st">"]</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>95% CI for Population Rho^2: [ 0.6982 ,  0.8556 ]</code></pre>
</div>
</div>
</section>
<section id="using-r-package-mbess" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="using-r-package-mbess">2. Using R Package <code>MBESS</code></h4>
<p>The <code>MBESS</code> package automates this procedure. We use <code>Random.Predictors = FALSE</code> to match the fixed-predictor assumption used in our manual calculation.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">requireNamespace</span>(<span class="st">"MBESS"</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Use N (sample size) and p (number of predictors) </span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># instead of df.1/df.2 to avoid the redundancy error.</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>  ci_res <span class="ot">&lt;-</span> MBESS<span class="sc">::</span><span class="fu">ci.R2</span>(<span class="at">F.value =</span> F_stat, </span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">p =</span> k,      <span class="co"># Number of predictors</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>                         <span class="at">N =</span> n,      <span class="co"># Sample size</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>                         <span class="at">conf.level =</span> <span class="fl">0.95</span>,</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Random.Predictors =</span> <span class="cn">FALSE</span>)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(ci_res)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Package 'MBESS' is not installed."</span>)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$Lower.Conf.Limit.R2
[1] 0.6982442

$Prob.Less.Lower
[1] 0.025

$Upper.Conf.Limit.R2
[1] 0.8555948

$Prob.Greater.Upper
[1] 0.025</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="underfitting-and-overfitting" class="level2" data-number="6.16">
<h2 data-number="6.16" class="anchored" data-anchor-id="underfitting-and-overfitting"><span class="header-section-number">6.16</span> Underfitting and Overfitting</h2>
<p>We compare the properties of two competing estimators for the mean response vector <span class="math inline">\(\mu = E[y]\)</span>.</p>
<section id="notation-and-setup" class="level3" data-number="6.16.1">
<h3 data-number="6.16.1" class="anchored" data-anchor-id="notation-and-setup"><span class="header-section-number">6.16.1</span> Notation and Setup</h3>
<p>We consider the general linear model: <span class="math display">\[
y = X\beta + e = X_1\beta_1 + X_2\beta_2 + e
\]</span> where <span class="math inline">\(X_1\)</span> is <span class="math inline">\(n \times p_1\)</span>, <span class="math inline">\(X_2\)</span> is <span class="math inline">\(n \times p_2\)</span>, and <span class="math inline">\(\text{Var}(e) = \sigma^2 I\)</span>.</p>
<p>We distinguish between two estimation approaches based on this model:</p>
<p><strong>1. Full Model (<span class="math inline">\(M_1\)</span>)</strong> We estimate <span class="math inline">\(\beta\)</span> without restrictions. The estimator projects <span class="math inline">\(y\)</span> onto the full column space <span class="math inline">\(\text{Col}(X)\)</span>. <span class="math display">\[
\begin{aligned}
P_1 &amp;= X(X^T X)^{-1}X^T &amp; (\text{Projection onto } \text{Col}(X)) \\
\hat{y}_1 &amp;= P_1 y &amp; (\text{Unrestricted Estimator})
\end{aligned}
\]</span></p>
<p><strong>2. Reduced Model (<span class="math inline">\(M_0\)</span>)</strong> We estimate <span class="math inline">\(\beta\)</span> subject to the constraint: <span class="math display">\[
M_0: \beta_2 = 0
\]</span> This effectively reduces the model to <span class="math inline">\(y = X_1\beta_1 + e\)</span>, projecting <span class="math inline">\(y\)</span> onto the subspace <span class="math inline">\(\text{Col}(X_1)\)</span>. <span class="math display">\[
\begin{aligned}
P_0 &amp;= X_1(X_1^T X_1)^{-1}X_1^T &amp; (\text{Projection onto } \text{Col}(X_1)) \\
\hat{y}_0 &amp;= P_0 y &amp; (\text{Restricted Estimator})
\end{aligned}
\]</span></p>
<p><strong>Key Geometric Property:</strong> Since the constraint <span class="math inline">\(\beta_2=0\)</span> restricts the estimation to a subspace (<span class="math inline">\(\text{Col}(X_1) \subset \text{Col}(X)\)</span>), we have the nesting property: <span class="math display">\[
P_1 P_0 = P_0 \quad \text{and} \quad P_1 - P_0 \text{ is a projection matrix.}
\]</span></p>
</section>
<section id="case-1-underfitting" class="level3" data-number="6.16.2">
<h3 data-number="6.16.2" class="anchored" data-anchor-id="case-1-underfitting"><span class="header-section-number">6.16.2</span> Case 1: Underfitting</h3>
<p><strong>The Truth:</strong> The Full Model (<span class="math inline">\(M_1\)</span>) is correct. <span class="math display">\[
y = X_1\beta_1 + X_2\beta_2 + e, \quad \beta_2 \neq 0
\]</span> The true mean is <span class="math inline">\(\mu = X_1\beta_1 + X_2\beta_2\)</span>.</p>
<p>We analyze the properties of the <strong>Reduced Estimator</strong> <span class="math inline">\(\hat{y}_0\)</span> (from <span class="math inline">\(M_0\)</span>) compared to the correct Full Estimator <span class="math inline">\(\hat{y}_1\)</span> (from <span class="math inline">\(M_1\)</span>).</p>
<div id="thm-underfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 59 (Bias-Variance Tradeoff in Underfitting)</strong></span> When <span class="math inline">\(M_1\)</span> is true:</p>
<ol type="1">
<li><strong>Bias:</strong> The estimator <span class="math inline">\(\hat{y}_0\)</span> is <strong>biased</strong>, while <span class="math inline">\(\hat{y}_1\)</span> is unbiased. <span class="math display">\[ \text{Bias}(\hat{y}_0) = -(I - P_0) X_2 \beta_2 \]</span></li>
<li><strong>Variance:</strong> The estimator <span class="math inline">\(\hat{y}_0\)</span> has <strong>smaller variance</strong> (matrix difference is positive semidefinite). <span class="math display">\[ \text{Var}(\hat{y}_1) - \text{Var}(\hat{y}_0) = \sigma^2 (P_1 - P_0) \ge 0 \]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part 1 (Bias):</strong> <span class="math display">\[
\begin{aligned}
E[\hat{y}_0] &amp;= P_0 E[y] = P_0(X_1\beta_1 + X_2\beta_2) \\
&amp;= X_1\beta_1 + P_0 X_2 \beta_2 \quad (\text{Since } P_0 X_1 = X_1)
\end{aligned}
\]</span> The bias is: <span class="math display">\[
\text{Bias} = E[\hat{y}_0] - \mu = (X_1\beta_1 + P_0 X_2 \beta_2) - (X_1\beta_1 + X_2\beta_2) = -(I - P_0)X_2\beta_2
\]</span></p>
<p><strong>Part 2 (Variance):</strong> <span class="math display">\[
\text{Var}(\hat{y}_1) = \sigma^2 P_1, \quad \text{Var}(\hat{y}_0) = \sigma^2 P_0
\]</span> The difference is <span class="math inline">\(\sigma^2(P_1 - P_0)\)</span>. Since <span class="math inline">\(\text{Col}(X_1) \subset \text{Col}(X)\)</span>, the difference <span class="math inline">\(P_1 - P_0\)</span> projects onto the orthogonal complement of <span class="math inline">\(\text{Col}(X_1)\)</span> within <span class="math inline">\(\text{Col}(X)\)</span>. It is idempotent and positive semidefinite.</p>
</div>
<p><strong>Remark: Scalar Variance and Coefficients</strong></p>
<p>From the matrix inequality above, we can state that for any arbitrary vector <span class="math inline">\(a\)</span>, the scalar variance of the linear combination <span class="math inline">\(a^T \hat{y}\)</span> is always smaller in the reduced model: <span class="math display">\[
\text{Var}(a^T \hat{y}_0) \le \text{Var}(a^T \hat{y}_1)
\]</span></p>
<p>We can extend this property to the regression coefficients <span class="math inline">\(\hat{\beta}\)</span>. Since <span class="math inline">\(\hat{y} = X\hat{\beta}\)</span>, we can recover the coefficients from the fitted values using the left pseudo-inverse:</p>
<p><span class="math display">\[
\begin{aligned}
(X^T X)^{-1}X^T (X\hat{\beta}) &amp;= (X^T X)^{-1}X^T \hat{y} \\
\underbrace{(X^T X)^{-1}(X^T X)}_{I} \hat{\beta} &amp;= (X^T X)^{-1}X^T \hat{y}
\end{aligned}
\]</span></p>
<div id="cor-beta-variance" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 9 (Variance of Coefficients)</strong></span> Because <span class="math inline">\(\hat{\beta}\)</span> is a linear transformation of <span class="math inline">\(\hat{y}\)</span>, the variance reduction in <span class="math inline">\(\hat{y}_0\)</span> propagates to the coefficients.</p>
<p>For any specific coefficient <span class="math inline">\(\beta_j\)</span> included in the reduced model (i.e., <span class="math inline">\(\beta_j \in \beta_1\)</span>), the variance of the estimator is smaller in the reduced model than in the full model: <span class="math display">\[ \text{Var}(\hat{\beta}_{j, reduced}) \le \text{Var}(\hat{\beta}_{j, full}) \]</span></p>
</div>
<p><strong>Conclusion:</strong> Using <span class="math inline">\(M_0\)</span> when <span class="math inline">\(M_1\)</span> is true introduces bias but reduces variance for both the fitted values and the estimated coefficients.</p>
</section>
<section id="case-2-overfitting" class="level3" data-number="6.16.3">
<h3 data-number="6.16.3" class="anchored" data-anchor-id="case-2-overfitting"><span class="header-section-number">6.16.3</span> Case 2: Overfitting</h3>
<p><strong>The Truth:</strong> The Reduced Model (<span class="math inline">\(M_0\)</span>) is correct. <span class="math display">\[
y = X_1\beta_1 + e \quad (\text{i.e., } \beta_2 = 0)
\]</span> The true mean is <span class="math inline">\(\mu = X_1\beta_1\)</span>.</p>
<p>We analyze the properties of the <strong>Full Estimator</strong> <span class="math inline">\(\hat{y}_1\)</span> (from <span class="math inline">\(M_1\)</span>) compared to the correct Reduced Estimator <span class="math inline">\(\hat{y}_0\)</span> (from <span class="math inline">\(M_0\)</span>).</p>
<div id="thm-overfitting" class="theorem">
<p><span class="theorem-title"><strong>Theorem 60 (Variance Inflation in Overfitting)</strong></span> When <span class="math inline">\(M_0\)</span> is true:</p>
<ol type="1">
<li><strong>Bias:</strong> Both estimators are <strong>unbiased</strong>. <span class="math display">\[ E[\hat{y}_1] = \mu \quad \text{and} \quad E[\hat{y}_0] = \mu \]</span></li>
<li><strong>Variance:</strong> The estimator <span class="math inline">\(\hat{y}_1\)</span> has <strong>unnecessarily higher variance</strong>. <span class="math display">\[ \text{Var}(\hat{y}_1) \ge \text{Var}(\hat{y}_0) \]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Part 1 (Bias):</strong> Since <span class="math inline">\(\mu = X_1\beta_1\)</span>: <span class="math display">\[
E[\hat{y}_1] = P_1 X_1\beta_1 = X_1\beta_1 = \mu \quad (\text{Since } X_1 \in \text{Col}(X))
\]</span> <span class="math display">\[
E[\hat{y}_0] = P_0 X_1\beta_1 = X_1\beta_1 = \mu \quad (\text{Since } X_1 \in \text{Col}(X_1))
\]</span></p>
<p><strong>Part 2 (Variance):</strong> As shown in Case 1, the difference is <span class="math inline">\(\sigma^2(P_1 - P_0)\)</span>. The cost of overfitting is purely variance inflation. The total variance (trace) increases by the number of unnecessary parameters (<span class="math inline">\(p_2\)</span>): <span class="math display">\[
\text{tr}(\text{Var}(\hat{y}_1)) - \text{tr}(\text{Var}(\hat{y}_0)) = \sigma^2 (\text{tr}(P_1) - \text{tr}(P_0)) = \sigma^2 (p_{full} - p_{reduced}) = \sigma^2 p_2
\]</span></p>
</div>
<p><strong>Conclusion:</strong> Using <span class="math inline">\(M_1\)</span> when <span class="math inline">\(M_0\)</span> is true offers no benefit in bias but strictly increases estimation variance.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="generalized-inverses" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Generalized Inverses</h1>
<section id="motivation-1" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="motivation-1"><span class="header-section-number">7.1</span> Motivation</h2>
<p>Consider the linear system <span class="math inline">\(X\beta = y\)</span>. In <span class="math inline">\(\mathbb{R}^2\)</span>, if <span class="math inline">\(X = [x_1, x_2]\)</span> is invertible, the solution is unique: <span class="math inline">\(\beta = X^{-1}y\)</span>. This satisfies <span class="math inline">\(X(X^{-1}y) = y\)</span>.However, if <span class="math inline">\(X\)</span> is not square or not invertible (e.g., <span class="math inline">\(X\)</span> is <span class="math inline">\(2 \times 3\)</span>), <span class="math inline">\(X\beta = y\)</span> does not have a unique solution. We seek a matrix <span class="math inline">\(G\)</span> such that <span class="math inline">\(\beta = Gy\)</span> provides a solution whenever <span class="math inline">\(y \in C(X)\)</span> (the column space of X). Substituting <span class="math inline">\(\beta = Gy\)</span> into the equation <span class="math inline">\(X\beta = y\)</span>: <span class="math display">\[
X(Gy) = y \quad \forall y \in C(X)
\]</span> Since any <span class="math inline">\(y \in C(X)\)</span> can be written as <span class="math inline">\(Xw\)</span> for some vector <span class="math inline">\(w\)</span>: <span class="math display">\[
XGXw = Xw \quad \forall w
\]</span> This implies the defining condition: <span class="math display">\[
XGX = X
\]</span></p>
</section>
<section id="definition-of-generalized-inverse" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="definition-of-generalized-inverse"><span class="header-section-number">7.2</span> Definition of Generalized Inverse</h2>
<div id="def-gen-inverse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 52 (Generalized Inverse)</strong></span> Let <span class="math inline">\(X\)</span> be an <span class="math inline">\(n \times p\)</span> matrix. A matrix <span class="math inline">\(X^-\)</span> of size <span class="math inline">\(p \times n\)</span> is called a <strong>generalized inverse</strong> of <span class="math inline">\(X\)</span> if it satisfies: <span class="math display">\[
XX^-X = X
\]</span></p>
</div>
<div id="exm-ginverse" class="theorem example">
<p><span class="theorem-title"><strong>Example 14 (Examples of Generalized Inverse)</strong></span> &nbsp;</p>
<ul>
<li><p><strong>Example 1: Diagonal Matrix</strong> If <span class="math inline">\(X = \text{diag}(\lambda_1, \lambda_2, 0, 0)\)</span>, we can write it in matrix form as: <span class="math display">\[
  X = \begin{pmatrix}
  \lambda_1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; \lambda_2 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0
  \end{pmatrix}
  \]</span> A generalized inverse is obtained by inverting the non-zero elements: <span class="math display">\[
  X^- = \begin{pmatrix}
  \lambda_1^{-1} &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; \lambda_2^{-1} &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0
  \end{pmatrix}
  \]</span></p></li>
<li><p><strong>Example 2: Row Vector</strong> Let <span class="math inline">\(X = (1, 2, 3)\)</span>. One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of <span class="math inline">\(X\)</span> (which is <span class="math inline">\(1\)</span>), and others are zero: <span class="math display">\[
  X^- = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}
  \]</span> <strong>Verification:</strong> <span class="math display">\[
  XX^-X = (1, 2, 3) \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} (1, 2, 3) = (1) \cdot (1, 2, 3) = (1, 2, 3) = X
  \]</span> Other valid generalized inverses include <span class="math inline">\(\begin{pmatrix} 0 \\ 1/2 \\ 0 \end{pmatrix}\)</span> or <span class="math inline">\(\begin{pmatrix} 0 \\ 0 \\ 1/3 \end{pmatrix}\)</span>.</p></li>
<li><p><strong>Example 3: Rank Deficient Matrix</strong> Let <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 1 \\ 3 &amp; 2 &amp; 4 \end{pmatrix}\)</span>. Note that Row 3 = Row 1 + Row 2, so Rank<span class="math inline">\((A) = 2\)</span>.</p>
<p><strong>Solution:</strong> A generalized inverse can be found by locating a non-singular <span class="math inline">\(2 \times 2\)</span> submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor <span class="math inline">\(M = \begin{pmatrix} 2 &amp; 2 \\ 1 &amp; 0 \end{pmatrix}\)</span>. The inverse is <span class="math inline">\(M^{-1} = \frac{1}{-2}\begin{pmatrix} 0 &amp; -2 \\ -1 &amp; 2 \end{pmatrix} = \begin{pmatrix} 0 &amp; 1 \\ 0.5 &amp; -1 \end{pmatrix}\)</span>.</p>
<p>Placing this in the corresponding position in <span class="math inline">\(A^-\)</span> and setting the rest to 0: <span class="math display">\[
  A^- = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 0.5 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}
  \]</span></p>
<p><strong>Verification (<span class="math inline">\(AA^-A = A\)</span>):</strong> First, compute <span class="math inline">\(AA^-\)</span>: <span class="math display">\[
  AA^- = \begin{pmatrix} 2 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 1 \\ 3 &amp; 2 &amp; 4 \end{pmatrix} \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 0.5 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 0 \end{pmatrix}
  \]</span> Then multiply by <span class="math inline">\(A\)</span>: <span class="math display">\[
  (AA^-)A = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 2 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 1 \\ 3 &amp; 2 &amp; 4 \end{pmatrix} = \begin{pmatrix} 2 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 1 \\ 3 &amp; 2 &amp; 4 \end{pmatrix} = A
  \]</span></p></li>
</ul>
</div>
</section>
<section id="a-procedure-to-find-a-generalized-inverse" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="a-procedure-to-find-a-generalized-inverse"><span class="header-section-number">7.3</span> A Procedure to Find a Generalized Inverse</h2>
<p>If we can partition <span class="math inline">\(X\)</span> (possibly after permuting rows/columns) such that <span class="math inline">\(R_{11}\)</span> is a non-singular rank <span class="math inline">\(r\)</span> submatrix:</p>
<p><span class="math display">\[
X = \begin{pmatrix} R_{11} &amp; R_{12} \\ R_{21} &amp; R_{22} \end{pmatrix}
\]</span></p>
<p>Then a generalized inverse is:</p>
<p><span class="math display">\[
X^- = \begin{pmatrix} R_{11}^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix}
\]</span></p>
<p><strong>Verification:</strong></p>
<p><span class="math display">\[
\begin{aligned}
XX^-X &amp;= \begin{pmatrix} R_{11} &amp; R_{12} \\ R_{21} &amp; R_{22} \end{pmatrix} \begin{pmatrix} R_{11}^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \begin{pmatrix} R_{11} &amp; R_{12} \\ R_{21} &amp; R_{22} \end{pmatrix} \\
&amp;= \begin{pmatrix} I_r &amp; 0 \\ R_{21}R_{11}^{-1} &amp; 0 \end{pmatrix} \begin{pmatrix} R_{11} &amp; R_{12} \\ R_{21} &amp; R_{22} \end{pmatrix} \\
&amp;= \begin{pmatrix} R_{11} &amp; R_{12} \\ R_{21} &amp; R_{21}R_{11}^{-1}R_{12} \end{pmatrix}
\end{aligned}
\]</span> Note that since rank<span class="math inline">\((X) = \text{rank}(R_{11})\)</span>, the rows of <span class="math inline">\([R_{21}, R_{22}]\)</span> are linear combinations of <span class="math inline">\([R_{11}, R_{12}]\)</span>, implying <span class="math inline">\(R_{22} = R_{21}R_{11}^{-1}R_{12}\)</span>. Thus, <span class="math inline">\(XX^-X = X\)</span>.</p>
<p><strong>An Algorithm for Finding a Generalized Inverse</strong></p>
<p>A systematic procedure to find a generalized inverse <span class="math inline">\(A^-\)</span> for any matrix <span class="math inline">\(A\)</span>:</p>
<ol type="1">
<li>Find any non-singular <span class="math inline">\(r \times r\)</span> submatrix <span class="math inline">\(C\)</span>, where <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(A\)</span>. It is not necessary for the elements of <span class="math inline">\(C\)</span> to occupy adjacent rows and columns in <span class="math inline">\(A\)</span>.</li>
<li>Find <span class="math inline">\(C^{-1}\)</span> and <span class="math inline">\((C^{-1})'\)</span>.</li>
<li>Replace the elements of <span class="math inline">\(C\)</span> in <span class="math inline">\(A\)</span> with the elements of <span class="math inline">\((C^{-1})'\)</span>.</li>
<li>Replace all other elements in <span class="math inline">\(A\)</span> with zeros.</li>
<li>Transpose the resulting matrix.</li>
</ol>
<p><strong>Matrix Visual Representation</strong> <span class="math display">\[
\underset{\text{Original } A}{\begin{pmatrix}
\times &amp; \otimes &amp; \times &amp; \otimes \\
\times &amp; \otimes &amp; \times &amp; \otimes \\
\times &amp; \times &amp; \times &amp; \times
\end{pmatrix}}
\xrightarrow[\text{with } (C^{-1})']{\text{Replace } C}
\underset{\text{Intermediate}}{\begin{pmatrix}
\times &amp; \triangle &amp; \times &amp; \triangle \\
\times &amp; \triangle &amp; \times &amp; \triangle \\
\times &amp; \times &amp; \times &amp; \times
\end{pmatrix}}
\xrightarrow[\text{Result}]{\text{Transpose}}
\underset{\text{Final } A^-}{\begin{pmatrix}
\times &amp; \times &amp; \times \\
\square &amp; \square &amp; \times \\
\times &amp; \times &amp; \times \\
\square &amp; \square &amp; \times
\end{pmatrix}}
\]</span></p>
<p><strong>Legend:</strong></p>
<ul>
<li><span class="math inline">\(\otimes\)</span>: Elements of submatrix <span class="math inline">\(C\)</span></li>
<li><span class="math inline">\(\triangle\)</span>: Elements of <span class="math inline">\((C^{-1})'\)</span></li>
<li><span class="math inline">\(\square\)</span>: Elements of <span class="math inline">\(C^{-1}\)</span> (after transposition)</li>
<li><span class="math inline">\(\times\)</span>: Other elements (replaced by 0 in the final calculation)</li>
</ul>
</section>
<section id="moore-penrose-inverse" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="moore-penrose-inverse"><span class="header-section-number">7.4</span> Moore-Penrose Inverse</h2>
<p>The Moore-Penrose inverse (denoted <span class="math inline">\(X^+\)</span>) is a unique generalized inverse defined via Singular Value Decomposition (SVD).</p>
<p>If <span class="math inline">\(X\)</span> has SVD: <span class="math display">\[
X = U \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} V'
\]</span></p>
<p>Then the Moore-Penrose inverse is: <span class="math display">\[
X^+ = V \begin{pmatrix} \Lambda_r^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} U'
\]</span></p>
<p>where <span class="math inline">\(\Lambda_r = \text{diag}(\lambda_1, \dots, \lambda_r)\)</span> contains the singular values. Unlike standard generalized inverses, <span class="math inline">\(X^+\)</span> is unique.</p>
<p><strong>Verification:</strong></p>
<p>We verify that <span class="math inline">\(X^+\)</span> satisfies the condition <span class="math inline">\(XX^+X = X\)</span>.</p>
<ol type="1">
<li><p><strong>Substitute definitions:</strong> <span class="math display">\[
XX^+X = \left[ U \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} V' \right] \left[ V \begin{pmatrix} \Lambda_r^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} U' \right] \left[ U \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} V' \right]
\]</span></p></li>
<li><p><strong>Apply orthogonality:</strong> Recall that <span class="math inline">\(V'V = I\)</span> and <span class="math inline">\(U'U = I\)</span>. <span class="math display">\[
= U \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \underbrace{(V'V)}_{I} \begin{pmatrix} \Lambda_r^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \underbrace{(U'U)}_{I} \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} V'
\]</span></p></li>
<li><p><strong>Multiply diagonal matrices:</strong> <span class="math display">\[
= U \left[ \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \begin{pmatrix} \Lambda_r^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \right] V'
\]</span> Since <span class="math inline">\(\Lambda_r \Lambda_r^{-1} \Lambda_r = I \cdot \Lambda_r = \Lambda_r\)</span>: <span class="math display">\[
= U \begin{pmatrix} \Lambda_r &amp; 0 \\ 0 &amp; 0 \end{pmatrix} V' = X
\]</span></p></li>
</ol>
</section>
<section id="solving-linear-systems-with-generalized-inverse" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="solving-linear-systems-with-generalized-inverse"><span class="header-section-number">7.5</span> Solving Linear Systems with Generalized Inverse</h2>
<p>We apply generalized inverses to solve systems of linear equations <span class="math inline">\(X\beta = c\)</span> where <span class="math inline">\(X\)</span> is <span class="math inline">\(n \times p\)</span>.</p>
<div id="def-consistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 53 (Consistency and Solution)</strong></span> The system <span class="math inline">\(X\beta = c\)</span> is consistent if and only if <span class="math inline">\(c \in \text{Col}(X)\)</span> (the column space of <span class="math inline">\(X\)</span>). If consistent, <span class="math inline">\(\beta = X^- c\)</span> is a solution.</p>
</div>
<p><strong>Proof:</strong> If the system is consistent, there exists some <span class="math inline">\(b\)</span> such that <span class="math inline">\(Xb = c\)</span>. Using the definition <span class="math inline">\(XX^-X = X\)</span>: <span class="math display">\[
X(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c
\]</span> Thus, <span class="math inline">\(X^-c\)</span> is a solution. Note that the solution is not unique if <span class="math inline">\(X\)</span> is not full rank.</p>
<div id="exm-gi-sol-ls" class="theorem example">
<p><span class="theorem-title"><strong>Example 15 (Examples of Solutions of Linear System with Generalized Inverse)</strong></span> &nbsp;</p>
<ul>
<li><p><strong>Example 1: Underdetermined System</strong></p>
<p>Let <span class="math inline">\(X = \begin{pmatrix} 1 &amp; 2 &amp; 3 \end{pmatrix}\)</span> and we want to solve <span class="math inline">\(X\beta = 4\)</span>.</p>
<p><strong>Solution 1:</strong> Using the generalized inverse <span class="math inline">\(X^- = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\)</span>: <span class="math display">\[
\beta = X^- \cdot 4 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} 4 = \begin{pmatrix} 4 \\ 0 \\ 0 \end{pmatrix}
\]</span> <strong>Verification:</strong> <span class="math display">\[
X\beta = \begin{pmatrix} 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 4 \\ 0 \\ 0 \end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \quad \checkmark
\]</span></p>
<p><strong>Solution 2:</strong> Using another generalized inverse <span class="math inline">\(X^- = \begin{pmatrix} 0 \\ 0 \\ 1/3 \end{pmatrix}\)</span>: <span class="math display">\[
\beta = X^- \cdot 4 = \begin{pmatrix} 0 \\ 0 \\ 1/3 \end{pmatrix} 4 = \begin{pmatrix} 0 \\ 0 \\ 4/3 \end{pmatrix}
\]</span> <strong>Verification:</strong> <span class="math display">\[
X\beta = \begin{pmatrix} 1 &amp; 2 &amp; 3 \end{pmatrix} \begin{pmatrix} 0 \\ 0 \\ 4/3 \end{pmatrix} = 0 + 0 + 3(4/3) = 4 \quad \checkmark
\]</span></p></li>
<li><p><strong>Example 2: Overdetermined System</strong></p>
<p>Let <span class="math inline">\(X = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}\)</span>. Solve <span class="math inline">\(X\beta = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix} = c\)</span>. Here <span class="math inline">\(c = 2X\)</span>, so the system is consistent. Since <span class="math inline">\(X\)</span> is a column vector, <span class="math inline">\(\beta\)</span> is a scalar.</p>
<p><strong>Solution:</strong> Using the generalized inverse <span class="math inline">\(X^- = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix}\)</span>: <span class="math display">\[
\beta = X^- c = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix} = 1(2) + 0(4) + 0(6) = 2
\]</span> <strong>Verification:</strong> <span class="math display">\[
X\beta = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} (2) = \begin{pmatrix} 2 \\ 4 \\ 6 \end{pmatrix} = c \quad \checkmark
\]</span></p></li>
</ul>
</div>
</section>
<section id="least-squares-for-non-full-rank-x-with-generalized-inverse" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="least-squares-for-non-full-rank-x-with-generalized-inverse"><span class="header-section-number">7.6</span> Least Squares for Non-full-rank <span class="math inline">\(X\)</span> with Generalized Inverse</h2>
<section id="projection-matrix-with-generalized-inverse-of-xx" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="projection-matrix-with-generalized-inverse-of-xx"><span class="header-section-number">7.6.1</span> Projection Matrix with Generalized Inverse of <span class="math inline">\(X'X\)</span></h3>
<p>For the normal equations <span class="math inline">\((X'X)\beta = X'y\)</span>, a solution is given by: <span class="math display">\[
\hat{\beta} = (X'X)^- X'y
\]</span> The fitted values are <span class="math display">\[\hat{y} = X\hat{\beta} = X(X'X)^- X'y.\]</span> This <span class="math inline">\(\hat{y}\)</span> represents the unique orthogonal projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(\text{Col}(X)\)</span>.</p>
</section>
<section id="invariance-and-uniqueness-of-the-projection-matrix" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="invariance-and-uniqueness-of-the-projection-matrix"><span class="header-section-number">7.6.2</span> Invariance and Uniqueness of “the” Projection Matrix</h3>
<div id="thm-transpose" class="theorem">
<p><span class="theorem-title"><strong>Theorem 61 (Transpose Property of Generalized Inverses)</strong></span> <span class="math inline">\((X^-)'\)</span> is a version of <span class="math inline">\((X')^-\)</span>. That is, <span class="math inline">\((X^-)'\)</span> is a generalized inverse of <span class="math inline">\(X'\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By definition, a generalized inverse <span class="math inline">\(X^-\)</span> satisfies the property: <span class="math display">\[
X X^- X = X
\]</span></p>
<p>To verify that <span class="math inline">\((X^-)'\)</span> is a generalized inverse of <span class="math inline">\(X'\)</span>, we need to show that it satisfies the condition <span class="math inline">\(A G A = A\)</span> where <span class="math inline">\(A = X'\)</span> and <span class="math inline">\(G = (X^-)'\)</span>.</p>
<ol type="1">
<li><p>Start with the fundamental definition: <span class="math display">\[
X X^- X = X
\]</span></p></li>
<li><p>Take the transpose of both sides of the equation: <span class="math display">\[
(X X^- X)' = X'
\]</span></p></li>
<li><p>Apply the reverse order law for transposes, <span class="math inline">\((ABC)' = C' B' A'\)</span>: <span class="math display">\[
X' (X^-)' X' = X'
\]</span></p></li>
</ol>
<p>Since substituting <span class="math inline">\((X^-)'\)</span> into the generalized inverse equation for <span class="math inline">\(X'\)</span> yields <span class="math inline">\(X'\)</span>, <span class="math inline">\((X^-)'\)</span> is a valid generalized inverse of <span class="math inline">\(X'\)</span>.</p>
</div>
<div id="lem-invariance" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 5 (Invariance of Generalized Least Squares)</strong></span> For any version of the generalized inverse <span class="math inline">\((X'X)^-\)</span>, the matrix <span class="math inline">\(X'(X'X)^- X'\)</span> is invariant and equals <span class="math inline">\(X'\)</span>. <span class="math display">\[
X'X(X'X)^- X' = X'
\]</span></p>
</div>
<p><strong>Proof (using Projection):</strong> Let <span class="math inline">\(P = X(X'X)^- X'\)</span>. This is the projection matrix onto <span class="math inline">\(\text{Col}(X)\)</span>. By definition of projection, <span class="math inline">\(Px = x\)</span> for any <span class="math inline">\(x \in \text{Col}(X)\)</span>. Since columns of <span class="math inline">\(X\)</span> are in <span class="math inline">\(\text{Col}(X)\)</span>, <span class="math inline">\(PX = X\)</span>. Taking the transpose: <span class="math inline">\((PX)' = X' \implies X'P' = X'\)</span>. Since projection matrices are symmetric (<span class="math inline">\(P=P'\)</span>), <span class="math inline">\(X'P = X'\)</span>. Substituting <span class="math inline">\(P\)</span>: <span class="math inline">\(X' X (X'X)^- X' = X'\)</span>.</p>
<p><strong>Proof (Direct Matrix Manipulation):</strong> Decompose <span class="math inline">\(y = X\beta + e\)</span> where <span class="math inline">\(e \perp \text{Col}(X)\)</span> (i.e., <span class="math inline">\(X'e = 0\)</span>). <span class="math display">\[
\begin{aligned}
X'X(X'X)^- X' y &amp;= X'X(X'X)^- X' (X\beta + e) \\
&amp;= X'X(X'X)^- X'X\beta + X'X(X'X)^- X'e
\end{aligned}
\]</span> Using the property <span class="math inline">\(A A^- A = A\)</span> (where <span class="math inline">\(A=X'X\)</span>), the first term becomes <span class="math inline">\(X'X\beta\)</span>. The second term is 0 because <span class="math inline">\(X'e = 0\)</span>. Thus, the expression simplifies to <span class="math inline">\(X'X\beta = X'(X\beta) = X'\hat{y}_{\text{proj}}\)</span>. This implies the operator acts as <span class="math inline">\(X'\)</span>.</p>
<div id="thm-proj-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 62 (Properties of Projection Matrix <span class="math inline">\(P\)</span>)</strong></span> Let <span class="math inline">\(P = X(X'X)^- X'\)</span>. This matrix has the following properties:</p>
<ol type="1">
<li><p><strong>Symmetry:</strong> <span class="math inline">\(P = P'\)</span>.</p></li>
<li><p><strong>Idempotence:</strong> <span class="math inline">\(P^2 = P\)</span>. <span class="math display">\[
P^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')
\]</span> Using the identity from <a href="#lem-invariance" class="quarto-xref">Lemma&nbsp;5</a> (<span class="math inline">\(X'X(X'X)^- X' = X'\)</span>), this simplifies to: <span class="math display">\[
X(X'X)^- X' = P
\]</span></p></li>
<li><p><strong>Uniqueness:</strong> <span class="math inline">\(P\)</span> is unique and invariant to the choice of the generalized inverse <span class="math inline">\((X'X)^-\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><strong>Proof of Uniqueness:</strong></p>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two different generalized inverses of <span class="math inline">\(X'X\)</span>. Define <span class="math inline">\(P_A = X A X'\)</span> and <span class="math inline">\(P_B = X B X'\)</span>. From <a href="#lem-invariance" class="quarto-xref">Lemma&nbsp;5</a>, we know that <span class="math inline">\(X' P_A = X'\)</span> and <span class="math inline">\(X' P_B = X'\)</span>.</p>
<p>Subtracting these two equations: <span class="math display">\[
X' (P_A - P_B) = 0
\]</span> Taking the transpose, we get <span class="math inline">\((P_A - P_B) X = 0\)</span>. This implies that the columns of the difference matrix <span class="math inline">\(D = P_A - P_B\)</span> are orthogonal to the columns of <span class="math inline">\(X\)</span> (i.e., <span class="math inline">\(D \perp \text{Col}(X)\)</span>).</p>
<p>However, by definition, the columns of <span class="math inline">\(P_A\)</span> and <span class="math inline">\(P_B\)</span> (and thus <span class="math inline">\(D\)</span>) are linear combinations of the columns of <span class="math inline">\(X\)</span> (i.e., <span class="math inline">\(D \in \text{Col}(X)\)</span>).</p>
<p>The only matrix that lies <em>in</em> the column space of <span class="math inline">\(X\)</span> but is also <em>orthogonal</em> to the column space of <span class="math inline">\(X\)</span> is the zero matrix. Therefore: <span class="math display">\[
P_A - P_B = 0 \implies P_A = P_B
\]</span></p>
</div>
</section>
</section>
<section id="the-left-inverse-view-recovering-hatbeta-from-haty" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="the-left-inverse-view-recovering-hatbeta-from-haty"><span class="header-section-number">7.7</span> The Left Inverse View: Recovering <span class="math inline">\(\hat{\beta}\)</span> from <span class="math inline">\(\hat{y}\)</span></h2>
<p>While the geometric properties of the linear model are most naturally established via the unique orthogonal projection <span class="math inline">\(\hat{y}\)</span>, we require a functional mapping—a statistical “bridge”—to translate the distribution of these fitted values back into the parameter space of <span class="math inline">\(\hat{\beta}\)</span>. This bridge is provided by the generalized left inverse.</p>
<section id="the-generalized-left-inverse" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="the-generalized-left-inverse"><span class="header-section-number">7.7.1</span> The Generalized Left Inverse</h3>
<p>To recover the parameter estimates directly from the fitted values, we define the generalized left inverse, denoted as <span class="math inline">\(X_{\text{left}}^-\)</span>, such that:</p>
<p><span class="math display">\[
\hat{\beta} = X_{\text{left}}^- \hat{y}
\]</span></p>
<p>A standard choice for this operator, derived from the normal equations, is:</p>
<p><span class="math display">\[
X_{\text{left}}^- = (X' X)^- X'
\]</span></p>
<p>When <span class="math inline">\(X\)</span> is full-rank, the <span class="math inline">\(X_{\text{left}}^-\)</span> is unique, which is given by</p>
<p><span class="math display">\[
X_{\text{left}}^- = (X' X)^{-1} X'
\]</span></p>
</section>
<section id="verification-of-the-inverse-property" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="verification-of-the-inverse-property"><span class="header-section-number">7.7.2</span> Verification of the Inverse Property</h3>
<p>To verify that <span class="math inline">\(X_{\text{left}}^-\)</span> acts as a valid generalized inverse of <span class="math inline">\(X\)</span>, it must satisfy the condition <span class="math inline">\(X X_{\text{left}}^- X = X\)</span>. Substituting our definition:</p>
<p><span class="math display">\[
X \underbrace{\left[ (X' X)^- X' \right]}_{X_{\text{left}}^-} X = X (X' X)^- (X' X)
\]</span></p>
<p>Using the property of generalized inverses for symmetric matrices where <span class="math inline">\((X' X)(X' X)^- X' = X'\)</span>, the transpose of this identity gives <span class="math inline">\(X (X' X)^- (X' X) = X\)</span>. Thus, the condition holds:</p>
<p><span class="math display">\[
X X_{\text{left}}^- X = X
\]</span></p>
</section>
<section id="recovering-the-estimator" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="recovering-the-estimator"><span class="header-section-number">7.7.3</span> Recovering the Estimator</h3>
<p>We can now demonstrate that applying this left inverse to the fitted values <span class="math inline">\(\hat{y}\)</span> yields the standard solution to the normal equations.</p>
<p>Substituting the projection formula <span class="math inline">\(\hat{y} = X(X' X)^- X' y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
X_{\text{left}}^- \hat{y} &amp;= \left[ (X' X)^- X' \right] \left[ X(X' X)^- X' y \right] \\
&amp;= (X' X)^- \underbrace{(X' X) (X' X)^- (X' X)}_{\text{Property } A A^- A = A} (X' X)^- X' y
\end{aligned}
\]</span></p>
<p>Simplifying using the generalized inverse property <span class="math inline">\(A^- A A^- = A^-\)</span> (where <span class="math inline">\(A = X' X\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
X_{\text{left}}^- \hat{y} &amp;= \underbrace{(X' X)^- (X' X) (X' X)^-}_{(X' X)^-} X' y \\
&amp;= (X' X)^- X' y
\end{aligned}
\]</span></p>
<p>Thus, we recover the standard estimator used in the normal equations:</p>
<p><span class="math display">\[
\mathbf{\hat{\beta} = (X' X)^- X' y}
\]</span></p>
</section>
</section>
<section id="non-full-rank-least-squares-with-qr-decomposition" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="non-full-rank-least-squares-with-qr-decomposition"><span class="header-section-number">7.8</span> Non-full-rank Least Squares with QR Decomposition</h2>
<p>When <span class="math inline">\(X\)</span> has rank <span class="math inline">\(r &lt; p\)</span> (where <span class="math inline">\(X\)</span> is <span class="math inline">\(n \times p\)</span>), we can derive the least squares estimator using partitioned matrices.</p>
<p>Assume the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(X\)</span> are linearly independent. We can partition <span class="math inline">\(X\)</span> as: <span class="math display">\[
X = Q (R_1, R_2)
\]</span> where <span class="math inline">\(Q\)</span> is an <span class="math inline">\(n \times r\)</span> matrix with orthogonal columns (<span class="math inline">\(Q'Q = I_r\)</span>), <span class="math inline">\(R_1\)</span> is an <span class="math inline">\(r \times r\)</span> non-singular matrix, and <span class="math inline">\(R_2\)</span> is <span class="math inline">\(r \times (p-r)\)</span>.</p>
<p>The normal equations are: <span class="math display">\[
X'X\beta = X'y \implies \begin{pmatrix} R_1' \\ R_2' \end{pmatrix} Q' Q (R_1, R_2) \beta = \begin{pmatrix} R_1' \\ R_2' \end{pmatrix} Q'y
\]</span> Simplifying (<span class="math inline">\(Q'Q = I_r\)</span>): <span class="math display">\[
\begin{pmatrix} R_1'R_1 &amp; R_1'R_2 \\ R_2'R_1 &amp; R_2'R_2 \end{pmatrix} \beta = \begin{pmatrix} R_1'Q'y \\ R_2'Q'y \end{pmatrix}
\]</span></p>
<section id="constructing-a-solution-by-solving-normal-equations" class="level3" data-number="7.8.1">
<h3 data-number="7.8.1" class="anchored" data-anchor-id="constructing-a-solution-by-solving-normal-equations"><span class="header-section-number">7.8.1</span> Constructing a Solution by Solving Normal Equations</h3>
<p>One specific generalized inverse of <span class="math inline">\(X'X\)</span> can be found by focusing on the non-singular block <span class="math inline">\(R_1'R_1\)</span>: <span class="math display">\[
(X'X)^- = \begin{pmatrix} (R_1'R_1)^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix}
\]</span></p>
<p>Using this generalized inverse, the estimator <span class="math inline">\(\hat{\beta}\)</span> becomes: <span class="math display">\[
\hat{\beta} = (X'X)^- X'y = \begin{pmatrix} (R_1'R_1)^{-1} &amp; 0 \\ 0 &amp; 0 \end{pmatrix} \begin{pmatrix} R_1'Q'y \\ R_2'Q'y \end{pmatrix}
\]</span> <span class="math display">\[
\hat{\beta} = \begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\ 0 \end{pmatrix} = \begin{pmatrix} R_1^{-1} Q'y \\ 0 \end{pmatrix}
\]</span></p>
<p>The fitted values are: <span class="math display">\[
\hat{y} = X\hat{\beta} = Q(R_1, R_2) \begin{pmatrix} R_1^{-1} Q'y \\ 0 \end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y
\]</span> This confirms that <span class="math inline">\(\hat{y}\)</span> is the projection of <span class="math inline">\(y\)</span> onto the column space of <span class="math inline">\(Q\)</span> (which is the same as the column space of <span class="math inline">\(X\)</span>).</p>
</section>
<section id="constructing-a-solution-by-solving-reparametrized-beta" class="level3" data-number="7.8.2">
<h3 data-number="7.8.2" class="anchored" data-anchor-id="constructing-a-solution-by-solving-reparametrized-beta"><span class="header-section-number">7.8.2</span> Constructing a Solution by Solving Reparametrized <span class="math inline">\(\beta\)</span></h3>
<p>We can view the model as: <span class="math display">\[
y = Q(R_1, R_2)\beta + \epsilon = Qb + \epsilon
\]</span> where <span class="math inline">\(b = R_1\beta_1 + R_2\beta_2\)</span>.</p>
<p>Since the columns of <span class="math inline">\(Q\)</span> are orthogonal, the least squares estimate for <span class="math inline">\(b\)</span> is simply: <span class="math display">\[
\hat{b} = (Q'Q)^{-1}Q'y = Q'y
\]</span></p>
<p>To find <span class="math inline">\(\beta\)</span>, we solve the underdetermined system: <span class="math display">\[
R_1\beta_1 + R_2\beta_2 = \hat{b} = Q'y
\]</span></p>
<p><strong>Solution 1:</strong> Set <span class="math inline">\(\beta_2 = 0\)</span>. Then: <span class="math display">\[
R_1\beta_1 = Q'y \implies \hat{\beta}_1 = R_1^{-1}Q'y
\]</span> This yields the same result as the generalized inverse method above: <span class="math inline">\(\hat{\beta} = \begin{pmatrix} R_1^{-1}Q'y \\ 0 \end{pmatrix}\)</span>.</p>
<p><strong>Solution 2:</strong> Using the generalized inverse of <span class="math inline">\(R = (R_1, R_2)\)</span>: <span class="math display">\[
R^- = \begin{pmatrix} R_1^{-1} \\ 0 \end{pmatrix}
\]</span> <span class="math display">\[
\hat{\beta} = R^- Q'y = \begin{pmatrix} R_1^{-1}Q'y \\ 0 \end{pmatrix}
\]</span> This demonstrates that finding a solution to the normal equations using <span class="math inline">\((X'X)^-\)</span> is equivalent to solving the reparameterized system <span class="math inline">\(b = R\beta\)</span>.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>