[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theory of Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Theory of Linear Models",
    "section": "Key Features",
    "text": "Key Features\nThis text adopts a geometric approach to the statistical theory of linear models, aiming to provide a deeper understanding than standard algebraic treatments. Key features include:\n\nProjection Perspective: We prioritize the geometric interpretation of least squares, viewing estimation as a projection of the response vector onto a model subspace. This visual framework unifies diverse topics—from simple regression to complex ANOVA designs—under a single theoretical umbrella.\nInteractive Visualizations: Abstract concepts are brought to life through interactive 3D plots. Readers can rotate and inspect vector spaces, residual planes, and projection geometries to build a tangible intuition for high-dimensional operations.\nComputational Integration: Theory is seamlessly integrated with practice. The text provides implementation examples using R (and Python), demonstrating how theoretical matrix equations translate directly into computational code.\nRigorous Foundations: While visually driven, the text maintains mathematical rigor, covering essential topics such as spectral theory, the generalized inverseand the multivariate normal distribution to ensure a solid theoretical grounding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Theory of Linear Models",
    "section": "Overview",
    "text": "Overview\nThis course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Theory of Linear Models",
    "section": "Audience",
    "text": "Audience\nThis book is designed for graduate students and advanced undergraduate students in statistics, data science, and related quantitative fields. It serves as a bridge between applied regression analysis and the theoretical foundations of linear models. Researchers and practitioners seeking a deeper geometric and algebraic understanding of the statistical methods they use daily will also find this text valuable.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Theory of Linear Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, readers should have a comfortable grasp of the following topics:\nLinear Algebra: An elementary understanding of matrix operations is essential. You should be familiar with matrix multiplication, determinants, inversion, and the basic concepts of vector spaces (such as linear independence, basis vectors, and subspaces). While we review key spectral theory concepts (like eigenvalues and the singular value decomposition) in the early chapters, prior exposure to these ideas is helpful.\nProbability and Statistics: A standard introductory course in probability and mathematical statistics is required. Readers should be familiar with random variables, expectation, variance, covariance, common probability distributions (especially the Normal distribution), and fundamental concepts of hypothesis testing and estimation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Multiple Linear Regression\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nwhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n),\n\\]\nin which \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\] The scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introlm.html#examples",
    "href": "introlm.html#examples",
    "title": "1  Introduction",
    "section": "1.2 Examples",
    "text": "1.2 Examples\n\n1.2.1 Polynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n\n1.2.2 Design Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]\n\n\n1.2.3 One-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\\[\n\\overset{G_1}{\n  \\boxed{\n    \\begin{matrix} Y_{11} \\\\ Y_{12} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_2}{\n  \\boxed{\n    \\begin{matrix} Y_{21} \\\\ Y_{22} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_3}{\n  \\boxed{\n    \\begin{matrix} Y_{31} \\\\ Y_{32} \\end{matrix}\n  }\n}\n\\]\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]\n\n\n1.2.4 Analysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,\\text{cont}} & 1 & 0 \\\\\nX_{2,\\text{cont}} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,\\text{cont}} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introlm.html#least-squares-estimation",
    "href": "introlm.html#least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Least Squares Estimation",
    "text": "1.3 Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\n\\text{SSE} = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{\\text{SST} - \\text{SSE}}{\\text{SST}}\n\\] where \\(\\text{SST} = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introlm.html#geometric-perspective-of-least-square-estimation",
    "href": "introlm.html#geometric-perspective-of-least-square-estimation",
    "title": "1  Introduction",
    "section": "1.4 Geometric Perspective of Least Square Estimation",
    "text": "1.4 Geometric Perspective of Least Square Estimation\nWe align the coordinate system to the models for clarity:\n\nReduced Model (\\(M_0\\)): Represented by the X-axis (labeled \\(j_3\\)).\n\n\\(\\hat{y}_0\\) is the projection of \\(y\\) onto this axis.\n\nFull Model (\\(M_1\\)): Represented by the XY-plane (the floor).\n\n\\(\\hat{y}_1\\) is the projection of \\(y\\) onto this plane (\\(z=0\\)).\n\nObserved Data (\\(y\\)): A point in 3D space.\n\nThe “improvement” due to adding predictors is the distance between \\(\\hat{y}_0\\) and \\(\\hat{y}_1\\).\n\n\n\n\n\n\n\n\nFigure 1.1: Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)\n\n\n\n\nThe geometric perspective is not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:\n\nStatistical Clarity: Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.\nComputational Stability: A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic “Normal Equations” (\\((X'X)^{-1}X'y\\)) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.\nGeneralizability: The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html",
    "href": "lec1-vecspace.html",
    "title": "2  Projection in Vector Space",
    "section": "",
    "text": "2.1 Vector and Projection onto a Line",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#vector-and-projection-onto-a-line",
    "href": "lec1-vecspace.html#vector-and-projection-onto-a-line",
    "title": "2  Projection in Vector Space",
    "section": "",
    "text": "2.1.1 Vectors and Operations\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\n\nDefinition 2.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\nVector Arithmetic: Vectors can be manipulated geometrically:\n\nDefinition 2.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 2.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n2.1.2 Scalar Multiplication and Distance\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\n\nDefinition 2.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\nc x = \\begin{pmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{pmatrix}\n\\]\n\nWe often need to quantify the “size” of a vector. This is done using the concept of length, or norm.\n\nDefinition 2.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n2.1.3 Angle and Inner Product\nTo understand the relationship between two vectors \\(x\\) and \\(y\\) beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the classic Law of Cosines to this triangle, we can relate the geometric angle to the vector lengths.\n\nTheorem 2.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation provides a critical link between the geometric angle \\(\\theta\\) and the algebraic norms of the vectors.\nDerivation of Inner Product\nWe can express the squared distance term \\(||y - x||^2\\) purely algebraically by expanding the components:\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the Inner Product (or dot product).\n\nDefinition 2.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\n\n\n2.1.4 Coordinate (Scalar) Projection\nThe inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of this projection is given by:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis expression can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector in the direction of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]\n\n\n2.1.5 Vector Projection Formula\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\n\nDefinition 2.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n2.1.6 Perpendicularity (Orthogonality)\nA special case of the angle between vectors arises when \\(\\theta = 90^\\circ\\). This geometric concept of perpendicularity is central to the theory of projections and least squares.\n\nDefinition 2.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 2.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n2.1.7 Projection onto a Line (Subspace)\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\n\nDefinition 2.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\nThe projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that it is the closest point on the line to \\(y\\). This implies that the error vector (or residual) must be perpendicular to the line itself.\n\nDefinition 2.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\nDerivation: To find the value of the scalar \\(c\\), we apply the orthogonality condition:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\nExpanding this inner product gives:\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\), we obtain:\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\nAlternative Forms of the Projection Formula\nWe can express the projection vector \\(\\hat{y}\\) in several equivalent ways to highlight different geometric interpretations.\n\nDefinition 2.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n2.1.8 Projection Matrix (\\(P_x\\))\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector \\(y\\). This allows us to define a Projection Matrix.\nWe can rewrite the formula for \\(\\hat{y}\\) by factoring out \\(y\\):\n\\[\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n\\]\nThis leads to the definition of the projection matrix \\(P_x\\).\n\nDefinition 2.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\nLet’s apply these concepts to a concrete example.\n\nExample 2.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\nExample: Projection onto the Ones Vector (\\(j_n\\))\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\n\nExample 2.3 (Projection onto the Ones Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n2.1.9 Pythagorean Theorem\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\n\nTheorem 2.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\nProof. We expand the squared norm using the inner product:\n\\[\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n\\]\nSince \\(x \\perp y\\), the inner product \\(x'y = 0\\). Thus, the term \\(2x'y\\) vanishes, leaving:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\nThe proof after defining inner product to represent \\(\\cos(\\theta)\\) is trivival. Figure 2.1 shows a geometric proof of the fundamental Pythagorean Theorem (aka 勾股定理).\n\n\n\n\n\n\n\n\nFigure 2.1: Proof of Pythagorean Theorem using Area Scaling\n\n\n\n\n\n\n\n2.1.10 Least Square Property\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector \\(y\\) and the subspace (or line) onto which it is projected.\n\nTheorem 2.3 (Least Square Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\nProof. Since both \\(\\hat{y}\\) and \\(y^*\\) lie on the line \\(L(x)\\), their difference \\((\\hat{y} - y^*)\\) also lies on \\(L(x)\\). From the definition of projection, the residual \\((y - \\hat{y})\\) is orthogonal to the line \\(L(x)\\). Therefore:\n\\[\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n\\]\nWe can write the vector \\((y - y^*)\\) as: \\[\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n\\]\nApplying the Pythagorean Theorem: \\[\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n\\]\nSince \\(||\\hat{y} - y^*||^2 \\ge 0\\), it follows that: \\[\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#vector-space",
    "href": "lec1-vecspace.html#vector-space",
    "title": "2  Projection in Vector Space",
    "section": "2.2 Vector Space",
    "text": "2.2 Vector Space\nWe now generalize our discussion from lines to broader spaces.\n\nDefinition 2.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\nIt follows that the zero vector \\(0\\) must belong to any subspace (by choosing \\(c=0\\)).\n\n2.2.1 Spanned Vector Space\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\nDefinition 2.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\n\n\n2.2.2 Column Space and Row Space\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\nDefinition 2.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(\\text{Col}(X)\\), is the vector space spanned by its columns:\n\\[\n\\text{Col}(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 2.16 (Row Space) The Row Space, denoted \\(\\text{Row}(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\n\n\n2.2.3 Linear Independence and Rank\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\nDefinition 2.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "href": "lec1-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "title": "2  Projection in Vector Space",
    "section": "2.3 Rank of Matrices and Dim of Vector Space",
    "text": "2.3 Rank of Matrices and Dim of Vector Space\n\nDefinition 2.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(\\text{Col}(X))\n\\]\n\nThere are several fundamental properties regarding the rank of a matrix.\n\nExample 2.4 (Example of the Equality of Row and Col Rank) Consider the following \\(3 \\times 4\\) matrix (\\(n=3, p=4\\)): \\[\nX = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}\n\\] Notice that the third row is the sum of the first two (\\(r_3 = r_1 + r_2\\)).\n1. Row Rank and Basis \\(U\\) The first two rows are linearly independent. We set the row rank \\(r=2\\) and use these rows as our basis matrix \\(U\\) (\\(2 \\times 4\\)): \\[\nU = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1\n\\end{pmatrix}\n\\]\n2. Coefficient Matrix \\(C\\) We express every row of \\(X\\) as a linear combination of the rows of \\(U\\):\n\nRow 1: \\(1 \\cdot u_1 + 0 \\cdot u_2\\)\nRow 2: \\(0 \\cdot u_1 + 1 \\cdot u_2\\)\nRow 3: \\(1 \\cdot u_1 + 1 \\cdot u_2\\)\n\nThese coefficients form the matrix \\(C\\) (\\(3 \\times 2\\)): \\[\nC = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\]\n3. The Decomposition (\\(X = CU\\)) We verify that \\(X\\) is the product of \\(C\\) and \\(U\\): \\[\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}}_{X \\ (3 \\times 4)}\n=\n\\underbrace{\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}}_{C \\ (3 \\times 2)}\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix}}_{U \\ (2 \\times 4)}\n\\]\n4. Conclusion on Column Rank The columns of \\(X\\) are linear combinations of the columns of \\(C\\). \\[\n\\text{Col}(X) \\subseteq \\text{Col}(C)\n\\] Since \\(C\\) has only 2 columns, the dimension of its column space (and thus \\(X\\)’s column space) cannot exceed 2. \\[\n\\text{Dim}(\\text{Col}(X)) \\le 2\n\\] This confirms that Row Rank (2) \\(\\ge\\) Column Rank. (By symmetry, they are equal).\n\n\nTheorem 2.4 (Row Rank equals Column Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(\\text{Col}(X)) = \\text{Dim}(\\text{Row}(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\n\n2.3.1 Orthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 2.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 2.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n2.3.2 Kernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 2.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = \\text{Col}(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 2.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n\\]\n\n\nProof. Let \\(x \\in \\mathbb{R}^p\\). \\(x \\in \\text{Ker}(X)\\) if and only if \\(Xx = 0\\). If we denote the rows of \\(X\\) as \\(r_1', \\dots, r_n'\\), then the equation \\(Xx = 0\\) is equivalent to the system of equations: \\[\n\\begin{pmatrix} r_1' \\\\ \\vdots \\\\ r_n' \\end{pmatrix} x = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\iff r_i' x = 0 \\text{ for all } i = 1, \\dots, n\n\\] This means \\(x\\) is orthogonal to every row of \\(X\\). Since the rows span the row space \\(\\text{Row}(X)\\), being orthogonal to every generator \\(r_i\\) implies \\(x\\) is orthogonal to the entire space \\(\\text{Row}(X)\\). Thus, \\(\\text{Ker}(X) = \\{ x \\mid x \\perp \\text{Row}(X) \\} = [\\text{Row}(X)]^\\perp\\).\n\n\n\n2.3.3 Nullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 2.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\] where \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\nProof. From the previous theorem, we established that the kernel is the orthogonal complement of the row space: \\[\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n\\]\nSince the row space is a subspace of \\(\\mathbb{R}^p\\), the entire space can be decomposed into the direct sum of the row space and its orthogonal complement: \\[\n\\mathbb{R}^p = \\text{Row}(X) \\oplus [\\text{Row}(X)]^\\perp = \\text{Row}(X) \\oplus \\text{Ker}(X)\n\\]\nTaking the dimensions of these spaces: \\[\n\\text{Dim}(\\mathbb{R}^p) = \\text{Dim}(\\text{Row}(X)) + \\text{Dim}(\\text{Ker}(X))\n\\]\nSubstituting the definitions of Rank (dimension of row/column space) and Nullity: \\[\np = \\text{Rank}(X) + \\text{Nullity}(X)\n\\]\n\nComparing Ranks via Kernel Containment\nThe Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices \\(A\\) and \\(B\\) (with the same number of columns) by inspecting their null spaces.\n\nTheorem 2.7 (Kernel Containment and Rank Inequality) Let \\(A\\) and \\(B\\) be two matrices with \\(p\\) columns. If the kernel of \\(A\\) is contained within the kernel of \\(B\\), then the rank of \\(A\\) is greater than or equal to the rank of \\(B\\).\n\\[\n\\text{Ker}(A) \\subseteq \\text{Ker}(B) \\implies \\text{Rank}(A) \\ge \\text{Rank}(B)\n\\]\n\n\nProof. From the subspace inclusion \\(\\text{Ker}(A) \\subseteq \\text{Ker}(B)\\), it follows that the dimension of the smaller space cannot exceed the dimension of the larger space: \\[\n\\text{Nullity}(A) \\le \\text{Nullity}(B)\n\\] Using the Rank-Nullity Theorem (\\(\\text{Rank} = p - \\text{Nullity}\\)), we reverse the inequality: \\[\np - \\text{Nullity}(A) \\ge p - \\text{Nullity}(B)\n\\] \\[\n\\text{Rank}(A) \\ge \\text{Rank}(B)\n\\]\n\n\n\n2.3.4 Rank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 2.8 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\n\nProof. The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\n\\text{Col}(XZ) \\subseteq \\text{Col}(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\n\\text{Row}(XZ) \\subseteq \\text{Row}(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 2.9 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\n\nProof. From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\n\n\n2.3.5 Rank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 2.10 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\n\nProof. We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 2.11 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\n\\text{Col}(XX') = \\text{Col}(X)\n\\]\n\n\nProof. \n\nForward (\\(\\subseteq\\)): Let \\(z \\in \\text{Col}(XX')\\). Then \\(z = XX'w\\) for some vector \\(w\\). We can rewrite this as \\(z = X(X'w)\\). Since \\(z\\) is a linear combination of columns of \\(X\\) (with coefficients \\(X'w\\)), \\(z \\in \\text{Col}(X)\\). Thus, \\(\\text{Col}(XX') \\subseteq \\text{Col}(X)\\).\nEquality via Rank: From the previous theorem, we know that \\(\\text{Rank}(XX') = \\text{Rank}(X)\\). Since \\(\\text{Col}(XX')\\) is a subspace of \\(\\text{Col}(X)\\) and they have the same finite dimension (Rank), the subspaces must be identical.\n\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(\\text{Col}(X)\\) lies in the same space as the projection onto \\(\\text{Col}(XX')\\). This is vital for the existence of solutions in generalized least squares.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#orthogonal-projection-onto-a-subspace",
    "href": "lec1-vecspace.html#orthogonal-projection-onto-a-subspace",
    "title": "2  Projection in Vector Space",
    "section": "2.4 Orthogonal Projection onto a Subspace",
    "text": "2.4 Orthogonal Projection onto a Subspace\n\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\n\n2.4.1 Equivalence to Least Squares\nThe geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).\n\nTheorem 2.12 (Best Approximation Theorem (Least Squares Property)) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) and \\(y \\in \\mathbb{R}^n\\). Let \\(\\hat{y}\\) be the orthogonal projection of \\(y\\) onto \\(V\\). Then \\(\\hat{y}\\) is the closest point in \\(V\\) to \\(y\\). That is, for any vector \\(v \\in V\\) such that \\(v \\ne \\hat{y}\\):\n\\[\n\\|y - \\hat{y}\\|^2 &lt; \\|y - v\\|^2\n\\]\n\n\nProof. Let \\(v\\) be any vector in \\(V\\). We can rewrite the difference vector \\(y - v\\) by adding and subtracting the projection \\(\\hat{y}\\): \\[\ny - v = (y - \\hat{y}) + (\\hat{y} - v)\n\\]\nObserve the properties of the two terms on the right-hand side:\n\nResidual: \\((y - \\hat{y})\\) is orthogonal to \\(V\\) by definition.\nDifference in Subspace: Since both \\(\\hat{y} \\in V\\) and \\(v \\in V\\), their difference \\((\\hat{y} - v)\\) is also in \\(V\\).\n\nTherefore, the two terms are orthogonal to each other: \\[\n(y - \\hat{y}) \\perp (\\hat{y} - v)\n\\]\nApplying the Pythagorean Theorem: \\[\n\\|y - v\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - v\\|^2\n\\]\nSince squared norms are non-negative, and \\(\\|\\hat{y} - v\\|^2 &gt; 0\\) (because \\(v \\ne \\hat{y}\\)): \\[\n\\|y - v\\|^2 &gt; \\|y - \\hat{y}\\|^2\n\\] The projection \\(\\hat{y}\\) minimizes the squared error distance (and error distance itself).\n\n\n\n\n\n\n\n\n\nFigure 2.2: Visualization of the Best Approximation Theorem\n\n\n\n\n\n\n\n2.4.2 Uniqueness of Projection\nWhile the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.\n\nTheorem 2.13 (Uniqueness of Orthogonal Projection) For a given vector \\(y\\) and subspace \\(V\\), the projection vector \\(\\hat{y}\\) satisfying \\((y - \\hat{y}) \\perp V\\) is unique.\n\n\nProof. Assume there are two vectors \\(\\hat{y}_1 \\in V\\) and \\(\\hat{y}_2 \\in V\\) that both satisfy the orthogonality condition. \\[\n(y - \\hat{y}_1) \\perp V \\quad \\text{and} \\quad (y - \\hat{y}_2) \\perp V\n\\] This means that for any \\(v \\in V\\), both inner products are zero: \\[\n\\langle y - \\hat{y}_1, v \\rangle = 0\n\\] \\[\n\\langle y - \\hat{y}_2, v \\rangle = 0\n\\]\nSubtracting the second equation from the first: \\[\n\\langle y - \\hat{y}_1, v \\rangle - \\langle y - \\hat{y}_2, v \\rangle = 0\n\\] Using the linearity of the inner product: \\[\n\\langle (y - \\hat{y}_1) - (y - \\hat{y}_2), v \\rangle = 0\n\\] \\[\n\\langle \\hat{y}_2 - \\hat{y}_1, v \\rangle = 0\n\\]\nThis equation holds for all \\(v \\in V\\). Since \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) are both in \\(V\\), their difference \\(d = \\hat{y}_2 - \\hat{y}_1\\) must also be in \\(V\\). We can therefore choose \\(v = d = \\hat{y}_2 - \\hat{y}_1\\). \\[\n\\langle \\hat{y}_2 - \\hat{y}_1, \\hat{y}_2 - \\hat{y}_1 \\rangle = 0 \\implies \\|\\hat{y}_2 - \\hat{y}_1\\|^2 = 0\n\\] The only vector with a norm of zero is the zero vector itself. \\[\n\\hat{y}_2 - \\hat{y}_1 = 0 \\implies \\hat{y}_1 = \\hat{y}_2\n\\] Thus, the projection is unique.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#projection-via-orthonormal-basis-q",
    "href": "lec1-vecspace.html#projection-via-orthonormal-basis-q",
    "title": "2  Projection in Vector Space",
    "section": "2.5 Projection via Orthonormal Basis (\\(Q\\))",
    "text": "2.5 Projection via Orthonormal Basis (\\(Q\\))\n\n2.5.1 Orthonomal Basis\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 2.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 2.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 2.24 (Projection Defined with Orthonormal Basis) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\nTheorem 2.14 (Projection via Orthonormal Basis) Let \\(\\{q_1, \\dots, q_k\\}\\) be an orthonormal basis for the subspace \\(V \\subseteq \\mathbb{R}^n\\). The vector defined by the sum of individual projections: \\[\n\\hat{y} = \\sum_{i=1}^k \\langle y, q_i \\rangle q_i\n\\] is indeed the orthogonal projection of \\(y\\) onto \\(V\\). That is, it satisfies \\((y - \\hat{y}) \\perp V\\).\n\n\nProof. To prove this, we must check two conditions:\n\n\\(\\hat{y} \\in V\\): This is immediate because \\(\\hat{y}\\) is a linear combination of the basis vectors \\(\\{q_1, \\dots, q_k\\}\\).\n\\((y - \\hat{y}) \\perp V\\): It suffices to show that the error vector \\(e = y - \\hat{y}\\) is orthogonal to every basis vector \\(q_j\\) (for \\(j = 1, \\dots, k\\)).\nLet’s calculate the inner product \\(\\langle y - \\hat{y}, q_j \\rangle\\): \\[\n\\begin{aligned}\n\\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle \\hat{y}, q_j \\rangle \\\\\n&= \\langle y, q_j \\rangle - \\left\\langle \\sum_{i=1}^k \\langle y, q_i \\rangle q_i, q_j \\right\\rangle \\\\\n&= \\langle y, q_j \\rangle - \\sum_{i=1}^k \\langle y, q_i \\rangle \\underbrace{\\langle q_i, q_j \\rangle}_{\\delta_{ij}}\n\\end{aligned}\n\\]\nSince the basis is orthonormal, \\(\\langle q_i, q_j \\rangle\\) is 1 if \\(i=j\\) and 0 otherwise. Thus, the summation collapses to a single term where \\(i=j\\): \\[\n\\begin{aligned}\n\\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle y, q_j \\rangle \\cdot 1 \\\\\n&= 0\n\\end{aligned}\n\\]\nSince \\((y - \\hat{y})\\) is orthogonal to every basis vector \\(q_j\\), it is orthogonal to the entire subspace \\(V\\). Thus, \\(\\hat{y}\\) is the unique orthogonal projection.\n\n\n\n\n2.5.2 Projection Matrix via Orthonomal Basis (\\(Q\\))\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\):\n\n\\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)).\n\\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).\n\n\nDefinition 2.25 (Projection Matrix in Terms of \\(Q\\)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 2.15 (Symmeticity and Idempotence) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\n\nProof. If \\(\\hat{y} = Py\\) is already in the subspace \\(\\text{Col}(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\n\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 2.5 (Finding Projection for One-way ANOVA) Consider a one-way ANOVA model with \\(k\\) groups: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i \\in \\{1, \\dots, k\\}\\) represents the group and \\(j \\in \\{1, \\dots, n_i\\}\\) represents the observation within the group. Let \\(N = \\sum_{i=1}^k n_i\\) be the total number of observations.\n1. Matrix Definitions We define the data vector \\(y\\) and the design matrix \\(X\\) as follows:\n\nData Vector (\\(y\\)): An \\(N \\times 1\\) vector containing all observations stacked by group: \\[\n  y = \\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1n_1} \\\\ y_{21} \\\\ \\vdots \\\\ y_{kn_k} \\end{pmatrix}\n  \\]\nDesign Matrix (\\(X\\)): An \\(N \\times k\\) matrix constructed from \\(k\\) column vectors, \\(X = (x_1, x_2, \\dots, x_k)\\). Each vector \\(x_g\\) is an indicator variable (dummy variable) for group \\(g\\): \\[\n  x_g = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\quad \\leftarrow \\text{Entries are 1 if observation belongs to group } g\n  \\]\n\n2. Orthogonality These column vectors \\(x_1, \\dots, x_k\\) are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero: \\[\n\\langle x_g, x_h \\rangle = 0 \\quad \\text{for } g \\neq h\n\\] This allows us to find the projection onto the column space of \\(X\\) by simply summing the projections onto each column individually.\n3. Calculating Individual Projections For a specific group vector \\(x_g\\), the projection is: \\[\n\\text{proj}(y|x_g) = \\frac{\\langle y, x_g \\rangle}{\\langle x_g, x_g \\rangle} x_g\n\\]\nWe calculate the two scalar terms:\n\nDenominator (\\(\\langle x_g, x_g \\rangle\\)): The sum of squared elements of \\(x_g\\). Since \\(x_g\\) contains \\(n_g\\) ones and zeros elsewhere: \\[\n  \\langle x_g, x_g \\rangle = \\sum \\mathbb{1}_{\\{i=g\\}}^2 = n_g\n  \\]\nNumerator (\\(\\langle y, x_g \\rangle\\)): The dot product sums only the \\(y\\) values belonging to group \\(g\\): \\[\n  \\langle y, x_g \\rangle = \\sum_{i,j} y_{ij} \\cdot \\mathbb{1}_{\\{i=g\\}} = \\sum_{j=1}^{n_g} y_{gj} = y_{g.} \\quad (\\text{Group Total})\n  \\]\n\n4. The Resulting Projection Substituting these back into the formula gives the coefficient for the vector \\(x_g\\): \\[\n\\text{proj}(y|x_g) = \\frac{y_{g.}}{n_g} x_g = \\bar{y}_{g.} x_g\n\\]\nThe total projection \\(\\hat{y}\\) is the sum over all groups: \\[\n\\hat{y} = \\sum_{g=1}^k \\bar{y}_{g.} x_g\n\\] This confirms that the fitted value for any specific observation \\(y_{ij}\\) is simply its group mean \\(\\bar{y}_{i.}\\).\n\n\n\n2.5.3 Gram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Gram-Schmidt Process: Projecting \\(x_2\\) onto \\(x_1\\)\n\n\n\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#hat-matrix-projection-matrix-via-x",
    "href": "lec1-vecspace.html#hat-matrix-projection-matrix-via-x",
    "title": "2  Projection in Vector Space",
    "section": "2.6 Hat Matrix (Projection Matrix via \\(X\\))",
    "text": "2.6 Hat Matrix (Projection Matrix via \\(X\\))\n\n2.6.1 Norm Equations\nLet \\(X = (x_1, \\dots, x_p)\\) be an \\(n \\times p\\) matrix, where each column \\(x_j\\) is a predictor vector.\nWe want to project the target vector \\(y\\) onto the column space \\(\\text{Col}(X)\\). This is equivalent to finding a coefficient vector \\(\\beta \\in \\mathbb{R}^p\\) such that the error vector (residual) is orthogonal to the entire subspace \\(\\text{Col}(X)\\).\n\\[\ny - X\\beta \\perp \\text{Col}(X)\n\\]\nSince the columns of \\(X\\) span the subspace, the residual must be orthogonal to every column vector \\(x_j\\) individually:\n\\[\ny - X\\beta \\perp x_j \\quad \\text{for } j = 1, \\dots, p\n\\]\nWriting this geometric condition as an algebraic dot product (where \\(x_j'\\) denotes the transpose):\n\\[\nx_j'(y - X\\beta) = 0 \\quad \\text{for each } j\n\\]\nWe can stack these \\(p\\) separate linear equations into a single matrix equation. Since the rows of \\(X'\\) are the columns of \\(X\\), this becomes:\n\\[\n\\begin{pmatrix} x_1' \\\\ \\vdots \\\\ x_p' \\end{pmatrix} (y - X\\beta) = \\mathbf{0}\n\\implies X'(y - X\\beta) = 0\n\\]\nFinally, we distribute the matrix transpose and rearrange terms to solve for \\(\\beta\\):\n\\[\n\\begin{aligned}\nX'y - X'X\\beta &= 0 \\\\\nX'X\\beta &= X'y\n\\end{aligned}\n\\]\nThis system is known as the Normal Equations.\n\nTheorem 2.16 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n2.6.2 Hat Matrix\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 2.26 (Hat Matrix) The projection of \\(y\\) onto \\(\\text{Col}(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the hat matrix \\(H\\) is defined as:\n\\[\nH = X(X'X)^{-1}X'\n\\]\n\n\n\n2.6.3 Equivalence of Hat Matrix and \\(QQ'\\)\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(\\text{Col}(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\n\\begin{aligned}\nH &= QR((QR)'(QR))^{-1}(QR)' \\\\\n  &= QR(R'Q'QR)^{-1}R'Q' \\\\\n  &= QR(R' \\underbrace{Q'Q}_{I} R)^{-1}R'Q' \\\\\n  &= QR(R'R)^{-1}R'Q' \\\\\n  &= QR R^{-1} (R')^{-1} R' Q' \\\\\n  &= Q \\underbrace{R R^{-1}}_{I} \\underbrace{(R')^{-1} R'}_{I} Q' \\\\\n  &= Q Q'\n\\end{aligned}\n\\]\nThis confirms that \\(H = QQ'\\) is consistent with the general formula \\(H = X(X'X)^{-1}X'\\).\n\n\n2.6.4 Properties of Hat Matrix\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 2.17 (Properties of Hat Matrix) The matrix \\(H = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(H' = H\\)\nIdempotent: \\(H^2 = H\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#projection-defined-with-orthogonal-projection-matrix",
    "href": "lec1-vecspace.html#projection-defined-with-orthogonal-projection-matrix",
    "title": "2  Projection in Vector Space",
    "section": "2.7 Projection Defined with Orthogonal Projection Matrix",
    "text": "2.7 Projection Defined with Orthogonal Projection Matrix\nProjection don’t have to be defined with a subspace or a matrix \\(X\\) as we discussed before. Projection matrix is a self-contained definition of the subspace it projects onto.\n\n2.7.1 Orthogonal Projection Matrix\n\nDefinition 2.27 (Orthogonal Projection Matrix) A square matrix \\(P\\) is called an orthogonal projection matrix if it satisfies two conditions:\n\nSymmetry: \\(P^\\top = P\\)\nIdempotency: \\(P^2 = P\\)\n\n\n\nTheorem 2.18 (Projection onto Column Space) If a matrix \\(P\\) is symmetric and idempotent, then \\(P\\) represents the orthogonal projection onto its column space, \\(\\text{Col}(P)\\).\nSpecifically, for any vector \\(y\\), the vector \\(\\hat{y} = Py\\) is the unique vector in \\(\\text{Col}(P)\\) such that the residual \\(e = y - \\hat{y}\\) is orthogonal to \\(\\text{Col}(P)\\).\n\n\nProof. Let \\(y \\in \\mathbb{R}^n\\). We decompose \\(y\\) as \\(y = Py + (I - P)y\\). We must show that the residual term \\((I-P)y\\) is orthogonal to any vector \\(z \\in \\text{Col}(P)\\).\nSince \\(z \\in \\text{Col}(P)\\), there exists a vector \\(x\\) such that \\(z = Px\\). The inner product between \\(z\\) and the residual is: \\[\n\\langle z, (I - P)y \\rangle = z^\\top (I - P)y = (Px)^\\top (I - P)y\n\\tag{2.1}\\]\nUsing the matrix transpose property \\((AB)^\\top = B^\\top A^\\top\\), we rewrite Equation 2.1 as: \\[\n\\langle z, (I - P)y \\rangle = x^\\top P^\\top (I - P)y\n\\tag{2.2}\\]\nSince \\(P\\) is symmetric (\\(P^\\top = P\\)), we can substitute \\(P\\) for \\(P^\\top\\) in Equation 2.2: \\[\n\\langle z, (I - P)y \\rangle = x^\\top P (I - P)y = x^\\top (P - P^2)y\n\\tag{2.3}\\]\nFinally, utilizing the idempotency of \\(P\\) (where \\(P^2 = P\\)), the expression in Equation 2.3 simplifies to 0: \\[\nx^\\top (P - P)y = x^\\top (0)y = 0\n\\tag{2.4}\\]\nSince the inner product is 0, the residual is orthogonal to every vector in \\(\\text{Col}(P)\\). Thus, \\(P\\) is the orthogonal projector.\n\n\n\n2.7.2 Projection onto Complement Space\n\nTheorem 2.19 (Projection onto Orthogonal Complement) Let \\(P\\) be an orthogonal projection matrix. The matrix \\(M\\) defined as: \\[\nM = I - P\n\\] is the orthogonal projection matrix onto the orthogonal complement of the column space of \\(P\\), denoted \\(\\text{Col}(P)^\\perp\\).\n\n\nProof. 1. Symmetry and Idempotency Since \\(P\\) is a projection matrix, \\(P^\\top = P\\) and \\(P^2 = P\\). We verify these properties for \\(M\\): \\[\nM^\\top = (I - P)^\\top = I - P^\\top = I - P = M\n\\tag{2.5}\\] \\[\nM^2 = (I - P)(I - P) = I - 2P + P^2 = I - 2P + P = I - P = M\n\\tag{2.6}\\] By Equation 2.5 and Equation 2.6, \\(M\\) is symmetric and idempotent, so it is an orthogonal projection matrix.\n2. Identifying the Subspace By Theorem 2.18, \\(M\\) projects onto its own column space, \\(\\text{Col}(M)\\). A vector \\(v\\) is in \\(\\text{Col}(M)\\) if and only if it is fixed by the projection (\\(Mv = v\\)). \\[\nMv = v\n\\tag{2.7}\\]\nSubstituting \\(M = I - P\\) into Equation 2.7 gives: \\[\n(I - P)v = v\n\\tag{2.8}\\]\nRearranging Equation 2.8, we find the condition for \\(v\\): \\[\nv - Pv = v \\implies Pv = 0\n\\tag{2.9}\\]\nThe condition \\(Pv = 0\\) in Equation 2.9 implies that \\(v\\) belongs to the null space of \\(P\\), denoted \\(\\text{Null}(P)\\). By the Fundamental Theorem of Linear Algebra for symmetric matrices, the null space is the orthogonal complement of the column space: \\[\n\\text{Null}(P) = \\text{Col}(P^\\top)^\\perp = \\text{Col}(P)^\\perp\n\\] Thus, the image of \\(M\\) is exactly \\(\\text{Col}(P)^\\perp\\).\n\n\nExercise 2.1 (Column Space of the Hat Matrix) Let \\(H = X(X^\\top X)^{-1}X^\\top\\) be the hat matrix.\n\nProve that the column space of \\(H\\) is identical to the column space of \\(X\\): \\[ \\text{Col}(H) = \\text{Col}(X) \\]\nUsing the result above, show that the column space of the residual maker matrix \\(M = I - H\\) is the orthogonal complement of \\(\\text{Col}(X)\\): \\[ \\text{Col}(M) = \\text{Col}(X)^\\perp \\]\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n1. Equivalence of Column Spaces To prove \\(\\text{Col}(H) = \\text{Col}(X)\\), we show inclusion in both directions.\n\nForward (\\(\\text{Col}(H) \\subseteq \\text{Col}(X)\\)): By definition, \\(H = X [(X^\\top X)^{-1}X^\\top]\\). Any column of \\(H\\) is a linear combination of the columns of \\(X\\) (weighted by the matrix in brackets). Therefore, any vector in the image of \\(H\\) must lie in \\(\\text{Col}(X)\\).\nReverse (\\(\\text{Col}(X) \\subseteq \\text{Col}(H)\\)): Take any vector \\(v \\in \\text{Col}(X)\\). By definition, \\(v = Xb\\) for some vector \\(b\\). Apply \\(H\\) to \\(v\\): \\[\n  Hv = X(X^\\top X)^{-1}X^\\top (Xb) = X(X^\\top X)^{-1}(X^\\top X)b = X(I)b = Xb = v\n  \\] Since \\(Hv = v\\), the vector \\(v\\) lies in the column space of \\(H\\) (specifically, it is an eigenvector with eigenvalue 1).\n\nSince both inclusions hold, \\(\\text{Col}(H) = \\text{Col}(X)\\).\n2. Orthogonal Complements From part 1, we know the subspaces are identical. Therefore, their orthogonal complements must also be identical: \\[\n\\text{Col}(H)^\\perp = \\text{Col}(X)^\\perp\n\\] We previously established in Theorem 2.19 that for any projection matrix \\(P\\), the complement projection \\(M = I - P\\) projects onto \\(\\text{Col}(P)^\\perp\\). Substituting \\(H\\) for \\(P\\): \\[\n\\text{Col}(M) = \\text{Col}(H)^\\perp\n\\] Combining these results gives the required equality: \\[\n\\text{Col}(M) = \\text{Col}(X)^\\perp\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#projection-onto-nested-subspaces",
    "href": "lec1-vecspace.html#projection-onto-nested-subspaces",
    "title": "2  Projection in Vector Space",
    "section": "2.8 Projection onto Nested Subspaces",
    "text": "2.8 Projection onto Nested Subspaces\n\n2.8.1 Nested Models and Subspaces\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\nDefinition 2.28 (Nested Models) Consider two models:\n\nReduced Model (\\(M_0\\)): \\(y \\in \\text{Col}(X_0)\\)\nFull Model (\\(M_1\\)): \\(y \\in \\text{Col}(X_1)\\)\n\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\n\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\n\\]\n\nUsually, \\(X_1\\) is constructed by adding columns to \\(X_0\\): \\(X_1 = [X_0, X_{\\text{new}}]\\).\n\n\n2.8.2 Projections onto Nested Subspaces\nLet \\(P_0\\) be the projection matrix onto \\(\\text{Col}(X_0)\\) and \\(P_1\\) be the projection matrix onto \\(\\text{Col}(X_1)\\). Since \\(\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\\), we have important relationships between these matrices.\n\nTheorem 2.20 (Composition of Projections) If \\(\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\n\nProof. 1. Proof of \\(P_1 P_0 = P_0\\): For any vector \\(y \\in \\mathbb{R}^n\\), the vector \\(v = P_0 y\\) lies in \\(\\text{Col}(X_0)\\). Since \\(\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\\), the vector \\(v\\) also lies in \\(\\text{Col}(X_1)\\). A projection matrix \\(P_1\\) acts as the identity operator for any vector already in its column space. Therefore, \\(P_1 v = v\\). Substituting \\(v = P_0 y\\), we get \\(P_1 P_0 y = P_0 y\\) for all \\(y\\). Thus, \\(P_1 P_0 = P_0\\).\n2. Proof of \\(P_0 P_1 = P_0\\): Take the transpose of the previous result (\\(P_1 P_0 = P_0\\)). \\[\n(P_1 P_0)' = P_0'\n\\] Using the property that projection matrices are symmetric (\\(P' = P\\)): \\[\nP_0' P_1' = P_0' \\implies P_0 P_1 = P_0\n\\]\n\nDifference of Projections\nThe difference between the two projection matrices, \\(P_1 - P_0\\), is itself a projection matrix.\n\nTheorem 2.21 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(\\text{Col}(X_1) \\cap \\text{Col}(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\n\nProof. 1. Symmetry: Since \\(P_1\\) and \\(P_0\\) are symmetric: \\((P_1 - P_0)' = P_1' - P_0' = P_1 - P_0\\).\n2. Idempotency: \\[\n\\begin{aligned}\n(P_1 - P_0)^2 &= (P_1 - P_0)(P_1 - P_0) \\\\\n&= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2\n\\end{aligned}\n\\] Using the projection properties (\\(P^2=P\\)) and the nested property (\\(P_1 P_0 = P_0\\) and \\(P_0 P_1 = P_0\\)): \\[\n= P_1 - P_0 - P_0 + P_0 = P_1 - P_0\n\\]\n3. Orthogonality to \\(P_0\\): \\[\n(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0\n\\] Since \\((P_1 - P_0)\\) is symmetric and idempotent, it is an orthogonal projection matrix. Since it is orthogonal to \\(P_0\\) (the space of \\(M_0\\)) but is derived from \\(P_1\\), it projects onto the subspace of \\(M_1\\) that is orthogonal to \\(M_0\\).\n\n\n\n2.8.3 Decomposition of Projections and their Sum Squares\n\nTheorem 2.22 (Orthogonal Decomposition) Let \\(M_0 \\subset M_1\\) be two nested linear models with corresponding design matrices \\(X_0\\) and \\(X_1\\) such that \\(\\text{Col}(X_0) \\subset \\text{Col}(X_1)\\). Let \\(P_0\\) and \\(P_1\\) be the orthogonal projection matrices onto \\(\\text{Col}(X_0)\\) and \\(\\text{Col}(X_1)\\) respectively.\nFor any observation vector \\(y\\), we have the decomposition: \\[\ny = \\underbrace{P_0 y}_{\\hat{y}_0} + \\underbrace{(P_1 - P_0) y}_{\\hat{y}_1 - \\hat{y}_0} + \\underbrace{(I - P_1) y}_{y - \\hat{y}_1}\n\\]\nGeometric Interpretation:\n\n\\(\\hat{y}_0 \\in \\text{Col}(X_0)\\): The fit of the reduced model.\n\\((\\hat{y}_1 - \\hat{y}_0) \\in \\text{Col}(X_0)^\\perp \\cap \\text{Col}(X_1)\\): The additional fit provided by \\(M_1\\) over \\(M_0\\).\n\\((y - \\hat{y}_1) \\in \\text{Col}(X_1)^\\perp\\): The projection of \\(y\\) onto the orthogonal complement of \\(\\text{Col}(X_1)\\).\n\nThe three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm: \\[\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n\\]\n\n\nProof. 1. Definitions We define the three components as vectors \\(v_1, v_2, v_3\\):\n\n\\(v_1 = \\hat{y}_0 = P_0 y\\).\n\\(v_2 = \\hat{y}_1 - \\hat{y}_0 = (P_1 - P_0)y\\).\n\\(v_3 = y - \\hat{y}_1 = (I - P_1)y\\).\n\nNote: Since \\(P_1\\) projects onto \\(\\text{Col}(X_1)\\), the matrix \\((I - P_1)\\) projects onto the orthogonal complement \\(\\text{Col}(X_1)^\\perp\\). Thus, \\(v_3 \\in \\text{Col}(I - P_1)\\).\n\n\nNote that since \\(\\text{Col}(X_0) \\subset \\text{Col}(X_1)\\), we have the property \\(P_1 P_0 = P_0 P_1 = P_0\\). (Projecting onto the smaller subspace \\(M_0\\) is unchanged if we first project onto the enclosing subspace \\(M_1\\)).\n2. Orthogonality of \\(v_1\\) and \\(v_2\\) We check the inner product \\(\\langle v_1, v_2 \\rangle = v_1' v_2\\): \\[\n\\begin{aligned}\nv_1' v_2 &= (P_0 y)' (P_1 - P_0) y \\\\\n&= y' P_0' (P_1 - P_0) y \\\\\n&= y' (P_0 P_1 - P_0^2) y \\quad (\\text{Since } P_0 \\text{ is symmetric}) \\\\\n&= y' (P_0 - P_0) y \\quad (\\text{Since } P_0 P_1 = P_0 \\text{ and } P_0^2 = P_0) \\\\\n&= 0\n\\end{aligned}\n\\]\n3. Orthogonality of \\((v_1 + v_2)\\) and \\(v_3\\) Note that \\(v_1 + v_2 = P_1 y = \\hat{y}_1\\). We check if the total fit \\(\\hat{y}_1\\) is orthogonal to the residual \\(v_3\\): \\[\n\\begin{aligned}\n\\hat{y}_1' v_3 &= (P_1 y)' (I - P_1) y \\\\\n&= y' P_1 (I - P_1) y \\\\\n&= y' (P_1 - P_1^2) y \\\\\n&= y' (P_1 - P_1) y \\\\\n&= 0\n\\end{aligned}\n\\] Since \\(\\hat{y}_1\\) is orthogonal to \\(v_3\\), and \\(\\hat{y}_0\\) is a component of \\(\\hat{y}_1\\), it follows that all three pieces are mutually orthogonal.\n4. Sum of Squares By the Pythagorean theorem applied twice to these orthogonal vectors, the equality of squared norms follows immediately.\n\n\n\n\n\n\n\n\n\nFigure 2.4: Illustration of Projections onto Nested Subspaces\n\n\n\n\n\n\nExample 2.6 (ANOVA Sum Squares) We apply the Nested Model Theorem (\\(M_0 \\subset M_1\\)) to the One-way ANOVA setting.\n1. Notation and Definitions\nConsider a dataset with \\(k\\) groups. Let \\(i = 1, \\dots, k\\) index the groups, and \\(j = 1, \\dots, n_i\\) index the observations within group \\(i\\).\n\n\\(N\\): Total number of observations, \\(N = \\sum_{i=1}^k n_i\\).\n\\(y_{ij}\\): The \\(j\\)-th observation in the \\(i\\)-th group.\n\\(\\bar{y}_{i.}\\): The sample mean of group \\(i\\). \\[ \\bar{y}_{i.} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij} \\]\n\\(\\bar{y}_{..}\\): The grand mean of all observations. \\[ \\bar{y}_{..} = \\frac{1}{N} \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij} \\]\n\n2. The Data and Projection Vectors\n\n\n\nTable 2.1: ANOVA Vectors: Data, Null Model, and Full Model\n\n\n\n\n\n\n\n\n\n\nObservation (\\(y\\))\nNull Projection (\\(\\hat{y}_0\\))\nFull Projection (\\(\\hat{y}_1\\))\n\n\n\n\n\\(\\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1 n_1} \\\\ \\hline \\vdots \\\\ \\hline y_{k1} \\\\ \\vdots \\\\ y_{k n_k} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\bar{y}_{1.} \\\\ \\vdots \\\\ \\bar{y}_{1.} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{k.} \\\\ \\vdots \\\\ \\bar{y}_{k.} \\end{pmatrix}\\)\n\n\n\n\n\n\n3. Decomposition and Sum of Squares\n\n\n\n\n\n\n\n\n\n\nComponent\nNotation\nDefinition\nVector Elements\nSquared Norm (Sum of Squares)\n\n\n\n\nNull Proj.\n\\(\\hat{y}_0\\)\n\\(P_0 y\\)\nGrand Mean (\\(\\bar{y}_{..}\\))\n\\(\\|\\hat{y}_0\\|^2 = N \\bar{y}_{..}^2\\)\n\n\nFull Proj.\n\\(\\hat{y}_1\\)\n\\(P_1 y\\)\nGroup Means (\\(\\bar{y}_{i.}\\))\n\\(\\|\\hat{y}_1\\|^2 = \\sum_{i=1}^k n_i \\bar{y}_{i.}^2\\)\n\n\n\n4. Geometric Justification of Shortcut Formulas\nA. Total Sum of Squares (SST) Since \\(\\hat{y}_0 \\perp (y - \\hat{y}_0)\\), we have \\(\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|y - \\hat{y}_0\\|^2\\): \\[ \\text{SST} = \\|y - \\hat{y}_0\\|^2 = \\|y\\|^2 - \\|\\hat{y}_0\\|^2 \\] \\[ \\text{SST} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - N\\bar{y}_{..}^2 \\]\nB. Between Group Sum of Squares (SSB) Since \\(\\hat{y}_0 \\perp (\\hat{y}_1 - \\hat{y}_0)\\), we have \\(\\|\\hat{y}_1\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2\\): \\[ \\text{SSB} = \\|\\hat{y}_1 - \\hat{y}_0\\|^2 = \\|\\hat{y}_1\\|^2 - \\|\\hat{y}_0\\|^2 \\] \\[ \\text{SSB} = \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2 \\]\nC. Within Group Sum of Squares (SSW) Since \\(\\hat{y}_1 \\perp (y - \\hat{y}_1)\\), we have \\(\\|y\\|^2 = \\|\\hat{y}_1\\|^2 + \\|y - \\hat{y}_1\\|^2\\): \\[ \\text{SSW} = \\|y - \\hat{y}_1\\|^2 = \\|y\\|^2 - \\|\\hat{y}_1\\|^2 \\] \\[ \\text{SSW} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 \\]\nConclusion: \\[ \\underbrace{\\|y\\|^2 - N\\bar{y}_{..}^2}_{\\text{SST}} = \\underbrace{(\\sum n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2)}_{\\text{SSB}} + \\underbrace{(\\sum \\sum y_{ij}^2 - \\sum n_i\\bar{y}_{i.}^2)}_{\\text{SSW}} \\] 5. Visualizing ANOVA Components in Data Space\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Generate Data\nnp.random.seed(42)\ngroup_names = ['A', 'B', 'C', 'D']\nn_i = [10, 12, 8, 15]\nmeans = [10, 15, 12, 18]\nstd_dev = 1.5\n\n# Define colors and markers for each group\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\nmarkers = ['o', 's', '^', 'D']\n\ndata_x = []\ndata_y = []\ngroup_boundaries = [0]\ngroup_indices = [] # To store indices for each group\n\ncurrent_idx = 0\nfor i, n in enumerate(n_i):\n    group_data = np.random.normal(means[i], std_dev, n)\n    indices = np.arange(current_idx, current_idx + n)\n    data_x.extend(indices)\n    data_y.extend(group_data)\n    group_indices.append(indices) # Store indices for plotting later\n    current_idx += n\n    group_boundaries.append(current_idx)\n\ndata_x = np.array(data_x)\ndata_y = np.array(data_y)\n\n# Calculate Stats\ngrand_mean = np.mean(data_y)\ngroup_means = [np.mean(data_y[group_boundaries[i]:group_boundaries[i+1]]) for i in range(len(n_i))]\n\n# 2. Plotting\nplt.figure(figsize=(12, 6))\n\n# Draw Grand Mean (Full span)\nplt.axhline(y=grand_mean, color='red', linestyle='--', linewidth=2, label=f'Grand Mean ($\\\\bar{{y}}_{{..}}$ = {grand_mean:.2f})')\n\n# Iterate through each group to plot points and means with matching colors\nfor i in range(len(n_i)):\n    start, end = group_boundaries[i], group_boundaries[i+1]\n    idx = group_indices[i]\n    \n    # 1. Scatter plot for the group with unique color and marker\n    plt.scatter(data_x[idx], data_y[idx], color=colors[i], marker=markers[i], \n                alpha=0.7, s=60, label=f'Group {group_names[i]}')\n    \n    # 2. Horizontal line for group mean with the SAME color\n    plt.hlines(y=group_means[i], xmin=start, xmax=end-1, color=colors[i], linewidth=3)\n    \n    # 3. Visualizing the \"Within\" residuals (faint lines)\n    for j in idx:\n        plt.vlines(x=j, ymin=min(data_y[j], group_means[i]), \n                   ymax=max(data_y[j], group_means[i]), \n                   color=colors[i], alpha=0.3, linestyle=':')\n\n# Formatting\nplt.title(\"One-Way ANOVA: Data, Group Means, and Grand Mean\", fontsize=14)\nplt.xlabel(\"Observation Index ($j$ grouped by $i$)\", fontsize=12)\nplt.ylabel(\"Value ($y_{ij}$)\", fontsize=12)\n# Set x-ticks at the center of each group\nplt.xticks(np.array(group_boundaries[:-1]) + np.array(n_i)/2 - 0.5, \n           [f\"Group {g}\\n($n_{{{g.lower()}}}={n}$)\" for g, n in zip(group_names, n_i)])\n\n\nCode\nplt.grid(axis='y', alpha=0.3)\n\n# Adjust legend to show group markers and the grand mean line\nhandles, labels = plt.gca().get_legend_handles_labels()\n# Reorder legend: Groups first, then Grand Mean\norder = [1, 2, 3, 4, 0]\nplt.legend([handles[idx] for idx in order], [labels[idx] for idx in order], \n           bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.5: Visualization of Group Means vs. Grand Mean",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#projections-onto-orthogonal-subspaces",
    "href": "lec1-vecspace.html#projections-onto-orthogonal-subspaces",
    "title": "2  Projection in Vector Space",
    "section": "2.9 Projections onto Orthogonal Subspaces",
    "text": "2.9 Projections onto Orthogonal Subspaces\nFinally, we consider the case where the entire space \\(\\mathbb{R}^n\\) is decomposed into mutually orthogonal subspaces.\n\nTheorem 2.23 (General Orthogonal Projections) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = \\hat{y}_1 + \\hat{y}_2 + \\dots + \\hat{y}_k\n\\] where \\(\\hat{y}_i \\in V_i\\).\nFurthermore, each component \\(\\hat{y}_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\n\\hat{y}_i = P_i y\n\\]\n\n\nProof. 1. Existence: Since \\(\\mathbb{R}^n\\) is the direct sum of \\(V_1, \\dots, V_k\\), by definition, any vector \\(y \\in \\mathbb{R}^n\\) can be written as a sum \\(y = v_1 + \\dots + v_k\\) where \\(v_i \\in V_i\\).\n2. Uniqueness: Suppose there are two such representations: \\(y = \\sum v_i = \\sum w_i\\), with \\(v_i, w_i \\in V_i\\). Then \\(\\sum (v_i - w_i) = 0\\). Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, \\(v_i - w_i = 0 \\implies v_i = w_i\\). The representation is unique. Let \\(\\hat{y}_i = v_i\\).\n3. Projection Property: We claim that the \\(i\\)-th component \\(\\hat{y}_i\\) is the orthogonal projection of \\(y\\) onto \\(V_i\\). We must show that the residual \\((y - \\hat{y}_i)\\) is orthogonal to \\(V_i\\). \\[\ny - \\hat{y}_i = \\sum_{j \\ne i} \\hat{y}_j\n\\] Let \\(z\\) be any vector in \\(V_i\\). We calculate the inner product: \\[\n\\langle y - \\hat{y}_i, z \\rangle = \\left\\langle \\sum_{j \\ne i} \\hat{y}_j, z \\right\\rangle = \\sum_{j \\ne i} \\langle \\hat{y}_j, z \\rangle\n\\] Since \\(\\hat{y}_j \\in V_j\\) and \\(z \\in V_i\\), and the subspaces are mutually orthogonal (\\(V_j \\perp V_i\\) for \\(j \\ne i\\)), every term in the sum is zero. Therefore, \\((y - \\hat{y}_i) \\perp V_i\\). By the definition of orthogonal projection, \\(\\hat{y}_i = P_i y\\).\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: \\[\nI_n = P_1 + P_2 + \\dots + P_k\n\\]\n\n\n\n\n\n\n\n\nFigure 2.6: Orthogonal decomposition of vector y into subspaces\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\n# --- Define Vectors ---\ny_vec &lt;- c(3, 4, 5)\norigin &lt;- c(0, 0, 0)\n\n# Projections (P_i y)\np1 &lt;- c(3, 0, 0)\np2 &lt;- c(0, 4, 0)\np3 &lt;- c(0, 0, 5)\n\n# Partial Sums (P_i y + P_j y)\nsum_12 &lt;- p1 + p2\nsum_13 &lt;- p1 + p3\nsum_23 &lt;- p2 + p3\n\n# --- Helper Functions ---\n\n# Function to add a vector with an arrowhead (Cone)\nadd_vec_arrow &lt;- function(p, start, end, color, name) {\n  p %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 6),\n      name = name,\n      showlegend = TRUE\n    ) %&gt;%\n    add_trace(\n      type = \"cone\",\n      x = end[1], y = end[2], z = end[3],\n      u = end[1]-start[1], v = end[2]-start[2], w = end[3]-start[3],\n      sizemode = \"absolute\",\n      sizeref = 0.5,\n      anchor = \"tip\",\n      colorscale = list(c(0, 1), c(color, color)),\n      showscale = FALSE,\n      name = name,\n      showlegend = FALSE\n    )\n}\n\n# Function to add dashed \"error\" lines\nadd_dashed_line &lt;- function(p, start, end, color, name) {\n  p %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 3, dash = \"dash\"),\n      name = name,\n      hoverinfo = \"text\",\n      text = name\n    )\n}\n\n# --- Build Plot ---\nfig &lt;- plot_ly()\n\n# 1. Main Vectors (Solid + Cones)\nfig &lt;- fig %&gt;%\n  add_vec_arrow(origin, p1, \"red\", \"P1 y\") %&gt;%\n  add_vec_arrow(origin, p2, \"green\", \"P2 y\") %&gt;%\n  add_vec_arrow(origin, p3, \"blue\", \"P3 y\") %&gt;%\n  add_vec_arrow(origin, y_vec, \"black\", \"y\")\n\n# 2. Dashed Lines from y to Single Projections\nfig &lt;- fig %&gt;%\n  add_dashed_line(y_vec, p1, \"rgba(255, 0, 0, 0.5)\", \"y -&gt; P1\") %&gt;%\n  add_dashed_line(y_vec, p2, \"rgba(0, 255, 0, 0.5)\", \"y -&gt; P2\") %&gt;%\n  add_dashed_line(y_vec, p3, \"rgba(0, 0, 255, 0.5)\", \"y -&gt; P3\")\n\n# 3. Dashed Lines from y to Partial Sums\nfig &lt;- fig %&gt;%\n  add_dashed_line(y_vec, sum_12, \"purple\", \"y -&gt; (P1+P2)\") %&gt;%\n  add_dashed_line(y_vec, sum_13, \"orange\", \"y -&gt; (P1+P3)\") %&gt;%\n  add_dashed_line(y_vec, sum_23, \"cyan\",   \"y -&gt; (P2+P3)\")\n\n# 4. Axes (Subspaces)\nlimit &lt;- 6\naxis_style &lt;- list(color = \"gray\", dash = \"dot\", width = 2)\n\nfig &lt;- fig %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0, limit), y=c(0,0), z=c(0,0), \n            line=axis_style, name=\"V1 (x)\") %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0, limit), z=c(0,0), \n            line=axis_style, name=\"V2 (y)\") %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0,0), z=c(0, limit), \n            line=axis_style, name=\"V3 (z)\")\n\n# --- Layout ---\nfig &lt;- fig %&gt;% layout(\n  title = \"Orthogonal Decomposition Geometry\",\n  width = 900,\n  height = 700,\n  scene = list(\n    xaxis = list(title = \"V1\", range = c(0, limit)),\n    yaxis = list(title = \"V2\", range = c(0, limit)),\n    zaxis = list(title = \"V3\", range = c(0, limit)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(x = 1.5, y = 1.5, z = 1.2))\n  ),\n  margin = list(l = 0, r = 0, b = 0, t = 50),\n  legend = list(x = 0.75, y = 0.9)\n)\n\nfig\n\n\n\n\n\n\n\n\nFigure 2.7: Orthogonal decomposition geometry (R Plotly)\n\n\n\n\n\nTheorem 2.24 (Complete Orthogonal Decomposition of \\(\\mathbb{R}^n\\)) Let \\(P_0, P_1, \\dots, P_k\\) be a sequence of orthogonal projection matrices with nested column spaces: \\[\n\\text{Col}(P_0) \\subseteq \\text{Col}(P_1) \\subseteq \\dots \\subseteq \\text{Col}(P_k)\n\\]\nDefine the sequence of difference matrices \\(\\Delta P_i\\) and their column spaces \\(V_i\\) as follows:\n\\[\\begin{align*}\n\\Delta P_0 &= P_0, & V_0 &= \\text{Col}(\\Delta P_0) \\\\\n\\Delta P_i &= P_i - P_{i-1} \\quad (1 \\le i \\le k), & V_i &= \\text{Col}(\\Delta P_i) \\\\\n\\Delta P_{k+1} &= I - P_k, & V_{k+1} &= \\text{Col}(\\Delta P_{k+1})\n\\end{align*}\\]\nConclusion:\n\nProjection Property: Each \\(\\Delta P_i\\) is the orthogonal projection matrix onto \\(V_i\\) for \\(i = 0, \\dots, k+1\\).\nMutual Orthogonality: The collection \\(\\{\\Delta P_i\\}\\) are mutually orthogonal operators: \\[ \\Delta P_i \\Delta P_j = 0 \\quad \\text{for all } i \\ne j \\]\nDirect Sum Decomposition: The vector space \\(\\mathbb{R}^n\\) is the direct sum of these orthogonal subspaces: \\[ \\mathbb{R}^n = V_0 \\oplus V_1 \\oplus \\dots \\oplus V_{k+1} \\]\n\n\n\nProof. 1. Proof that \\(\\Delta P_i\\) is the Projection onto \\(V_i\\) We must show each \\(\\Delta P_i\\) is symmetric and idempotent.\n\nFor \\(\\Delta P_0 = P_0\\): True by definition.\nFor \\(\\Delta P_i\\) (\\(1 \\le i \\le k\\)):\n\nSymmetry: Difference of symmetric matrices (\\(P_i, P_{i-1}\\)) is symmetric.\nIdempotency: \\((\\Delta P_i)^2 = (P_i - P_{i-1})^2 = P_i^2 - P_i P_{i-1} - P_{i-1} P_i + P_{i-1}^2\\). Using nested properties (\\(P_i P_{i-1} = P_{i-1}\\)), this simplifies to \\(P_i - P_{i-1} = \\Delta P_i\\).\n\nFor \\(\\Delta P_{k+1} = I - P_k\\):\n\nSymmetry: \\((I - P_k)' = I - P_k\\).\nIdempotency: \\((I - P_k)^2 = I - 2P_k + P_k^2 = I - P_k\\).\n\n\n2. Proof of Mutual Orthogonality We show \\(\\Delta P_j \\Delta P_i = 0\\) for \\(i &lt; j\\).\n\nCase 1: Both indices \\(\\le k\\) (i.e., \\(1 \\le i &lt; j \\le k\\)): \\[ (P_j - P_{j-1})(P_i - P_{i-1}) = P_j P_i - P_j P_{i-1} - P_{j-1} P_i + P_{j-1} P_{i-1} \\] Since \\(\\text{Col}(P_i) \\subseteq \\text{Col}(P_{j-1})\\), all terms reduce to \\(P_i - P_{i-1} - P_i + P_{i-1} = 0\\).\nCase 2: One index is the residual (\\(j = k+1\\)): We check \\(\\Delta P_{k+1} \\Delta P_i = (I - P_k)\\Delta P_i\\) for any \\(i \\le k\\). Since \\(V_i \\subseteq \\text{Col}(P_k)\\), we have \\(P_k \\Delta P_i = \\Delta P_i\\). \\[ (I - P_k)\\Delta P_i = \\Delta P_i - P_k \\Delta P_i = \\Delta P_i - \\Delta P_i = 0 \\]\n\n3. Proof of Direct Sum The sum of the difference matrices forms a telescoping series: \\[ \\sum_{j=0}^{k+1} \\Delta P_j = P_0 + \\sum_{i=1}^k (P_i - P_{i-1}) + (I - P_k) \\] \\[ = P_k + (I - P_k) = I \\] Since the identity operator \\(I\\) (which maps \\(\\mathbb{R}^n\\) to itself) is the sum of mutually orthogonal projection operators, the space \\(\\mathbb{R}^n\\) decomposes into the direct sum of their respective image subspaces \\(V_i\\).\n\n\n\n\n\n\n\n\n\nFigure 2.8: Venn Diagram of Nested Projections with Colored Increments",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html",
    "href": "lec2-matrix.html",
    "title": "3  Matrix Algebra",
    "section": "",
    "text": "3.1 Eigenvalues and Eigenvectors\nThis chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, singular value decomposition.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#spectral-theory",
    "href": "lec2-matrix.html#spectral-theory",
    "title": "3  Matrix Algebra",
    "section": "",
    "text": "3.1.1 Eigenvalues and Eigenvectors\n\nDefinition 3.1 (Eigenvalues and Eigenvectors) For a square matrix \\(A\\) (\\(n \\times n\\)), a scalar \\(\\lambda\\) is an eigenvalue and a non-zero vector \\(x\\) is the corresponding eigenvector if:\n\\[\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n\\]\nThe eigenvalues are found by solving the characteristic equation: \\[\n|A - \\lambda I_n| = 0\n\\]\n\n\n\n3.1.2 Spectral Decomposition\nFor symmetric matrices, we have a powerful decomposition theorem.\n\nTheorem 3.1 (Spectral Decomposition) If \\(A\\) is a symmetric \\(n \\times n\\) matrix, all its eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) are real. Furthermore, there exists an orthogonal matrix \\(Q\\) such that:\n\\[\nA = Q \\Lambda Q' = \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\]\nwhere:\n\n\\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) contains the eigenvalues.\n\\(Q = (q_1, \\dots, q_n)\\) contains the corresponding orthonormal eigenvectors (\\(q_i'q_j = \\delta_{ij}\\)).\n\n\nExplantion: This allows us to view the transformation \\(Ax\\) as a rotation (\\(Q'\\)), a scaling (\\(\\Lambda\\)), and a rotation back (\\(Q\\)). For a symmetric matrix \\(A\\), we can write the spectral decomposition as a product of the eigenvector matrix \\(Q\\) and eigenvalue matrix \\(\\Lambda\\):\n\\[\n\\begin{aligned}\nA &= Q \\Lambda Q' \\\\\n  &= \\begin{pmatrix} q_1 & q_2 & \\cdots & q_n \\end{pmatrix}\n     \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\begin{pmatrix} \\lambda_1 q_1 & \\lambda_2 q_2 & \\cdots & \\lambda_n q_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\lambda_1 q_1 q_1' + \\lambda_2 q_2 q_2' + \\cdots + \\lambda_n q_n q_n' \\\\\n  &= \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\end{aligned}\n\\]\nwhere the eigenvectors \\(q_i\\) satisfy the orthogonality conditions: \\[\nq_i' q_j = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{if } i \\ne j \\end{cases}\n\\] And \\(Q\\) is an orthogonal matrix: \\(Q'Q = Q Q' = I_n\\).\n\n\n3.1.3 Quadratic Form\n\nDefinition 3.2 A quadratic form in \\(n\\) variables \\(x_1, x_2, \\dots, x_n\\) is a scalar function defined by a symmetric matrix \\(A\\): \\[\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\]\n\n\n\n3.1.4 Positive and Non-Negative Definite Matrices\n\nDefinition 3.3 (Positive and Non-Negative Definite Matrices) A symmetric matrix \\(A\\) is positive definite (p.d.) if: \\[\nx'Ax &gt; 0 \\quad \\forall x \\ne 0\n\\] It is non-negative definite (n.n.d.) if: \\[\nx'Ax \\ge 0 \\quad \\forall x\n\\]\n\n\nTheorem 3.2 (Properties of Definite Matrices) Let \\(A\\) be a symmetric \\(n \\times n\\) matrix with eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\).\n\nEigenvalue Characterization:\n\n\\(A\\) is p.d. \\(\\iff\\) all \\(\\lambda_i &gt; 0\\).\n\\(A\\) is n.n.d. \\(\\iff\\) all \\(\\lambda_i \\ge 0\\).\n\nDeterminant and Inverse:\n\nIf \\(A\\) is p.d., then \\(|A| &gt; 0\\) and \\(A^{-1}\\) exists.\nIf \\(A\\) is n.n.d. and singular, then \\(|A| = 0\\) (at least one \\(\\lambda_i = 0\\)).\n\nGram Matrices (\\(B'B\\)): Let \\(B\\) be an \\(n \\times p\\) matrix.\n\nIf \\(\\text{rank}(B) = p\\), then \\(B'B\\) is p.d.\nIf \\(\\text{rank}(B) &lt; p\\), then \\(B'B\\) is n.n.d.\n\n\n\n\n\n3.1.5 Properties of Symmetric Matrices\n\nTheorem 3.3 (Properties of Symmetric Matrices) Let \\(A\\) be a symmetric matrix with spectral decomposition \\(A = Q \\Lambda Q'\\). The following properties hold:\n\nTrace: \\(\\text{tr}(A) = \\sum \\lambda_i\\).\nDeterminant: \\(|A| = \\prod \\lambda_i\\).\nSingularity: \\(A\\) is singular if and only if at least one \\(\\lambda_i = 0\\).\nInverse: If \\(A\\) is non-singular (\\(\\lambda_i \\ne 0\\)), then \\(A^{-1} = Q \\Lambda^{-1} Q'\\).\nPowers: \\(A^k = Q \\Lambda^k Q'\\).\n\nSquare Root: \\(A^{1/2} = Q \\Lambda^{1/2} Q'\\) (if \\(\\lambda_i \\ge 0\\)).\n\nSpectral Representation of Quadratic Forms: The quadratic form \\(x'Ax\\) can be diagonalized using the eigenvectors of \\(A\\): \\[\nx'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n\\] where \\(y = Q'x\\) represents a rotation of the coordinate system.\n\n\n\n\n3.1.6 Spectral Representation of Projection Matrices\nWe revisit projection matrices in the context of eigenvalues.\n\nTheorem 3.4 (Eigenvalues of Projection Matrices) A symmetric matrix \\(P\\) is a projection matrix (idempotent, \\(P^2=P\\)) if and only if its eigenvalues are either 0 or 1.\n\\[\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n\\]\n\nFor a projection matrix \\(P\\):\n\nIf \\(x \\in \\text{Col}(P)\\), \\(Px = x\\) (Eigenvalue 1).\nIf \\(x \\perp \\text{Col}(P)\\), \\(Px = 0\\) (Eigenvalue 0).\n\\(\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i\\) (Count of 1s).\n\n\nExample 3.1 For \\(P = \\frac{1}{n} J_n J_n'\\), the rank is \\(\\text{tr}(P) = 1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#cholesky-decomposition",
    "href": "lec2-matrix.html#cholesky-decomposition",
    "title": "3  Matrix Algebra",
    "section": "3.4 Cholesky Decomposition",
    "text": "3.4 Cholesky Decomposition\nA symmetric matrix \\(A\\) has a Cholesky decomposition if and only if it is non-negative definite (i.e., \\(x'Ax \\ge 0\\) for all \\(x\\)).\n\\[\nA = B'B\n\\]\nwhere \\(B\\) is an upper triangular matrix with non-negative diagonal entries.\n\n3.4.1 Matrix Representation of the Algorithm\nTo derive the algorithm, we equate the elements of \\(A\\) with the product of the lower triangular matrix \\(B'\\) and the upper triangular matrix \\(B\\).\nFor a \\(3 \\times 3\\) matrix, this looks like:\n\\[\n\\underbrace{\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}}_{A}\n=\n\\underbrace{\\begin{pmatrix}\nb_{11} & 0 & 0 \\\\\nb_{12} & b_{22} & 0 \\\\\nb_{13} & b_{23} & b_{33}\n\\end{pmatrix}}_{B'}\n\\underbrace{\\begin{pmatrix}\nb_{11} & b_{12} & b_{13} \\\\\n0 & b_{22} & b_{23} \\\\\n0 & 0 & b_{33}\n\\end{pmatrix}}_{B}\n\\]\nMultiplying the matrices on the right yields the system of equations:\n\\[\nA = \\begin{pmatrix}\n\\mathbf{b_{11}^2} & b_{11}b_{12} & b_{11}b_{13} \\\\\nb_{12}b_{11} & \\mathbf{b_{12}^2 + b_{22}^2} & b_{12}b_{13} + b_{22}b_{23} \\\\\nb_{13}b_{11} & b_{13}b_{12} + b_{23}b_{22} & \\mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}\n\\end{pmatrix}\n\\]\nBy solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.\n\n\n3.4.2 The Algorithm\n\nRow 1: Solve for \\(b_{11}\\) using \\(a_{11}\\), then solve the rest of the row (\\(b_{1j}\\)) by division.\n\n\\(b_{11} = \\sqrt{a_{11}}\\)\n\\(b_{1j} = a_{1j}/b_{11}\\)\n\nRow 2: Solve for \\(b_{22}\\) using \\(a_{22}\\) and the known \\(b_{12}\\), then solve \\(b_{2j}\\).\n\n\\(b_{22} = \\sqrt{a_{22} - b_{12}^2}\\)\n\\(b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}\\)\n\nRow 3: Solve for \\(b_{33}\\) using \\(a_{33}\\) and the known \\(b_{13}, b_{23}\\).\n\n\\(b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}\\)\n\n\n\n\n3.4.3 Numerical Example\nConsider the positive definite matrix \\(A\\): \\[\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n\\]\nWe find \\(B\\) such that \\(A = B'B\\):\n\nFirst Row of B (\\(b_{11}, b_{12}, b_{13}\\)):\n\n\\(b_{11} = \\sqrt{4} = 2\\)\n\\(b_{12} = 2 / 2 = 1\\)\n\\(b_{13} = -2 / 2 = -1\\)\n\nSecond Row of B (\\(b_{22}, b_{23}\\)):\n\n\\(b_{22} = \\sqrt{10 - (1)^2} = \\sqrt{9} = 3\\)\n\\(b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1\\)\n\nThird Row of B (\\(b_{33}\\)):\n\n\\(b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{4} = 2\\)\n\n\nResult: \\[\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#generalized-inverses",
    "href": "lec2-matrix.html#generalized-inverses",
    "title": "3  Spectral Theory and Generalized Inverse",
    "section": "3.3 Generalized Inverses",
    "text": "3.3 Generalized Inverses\n\n3.3.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]\n\n\n3.3.2 Definition of Generalized Inverse\n\nDefinition 3.4 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 3.2 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]\n\n\n\n\n3.3.3 A Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend:\n\n\\(\\otimes\\): Elements of submatrix \\(C\\)\n\\(\\triangle\\): Elements of \\((C^{-1})'\\)\n\\(\\square\\): Elements of \\(C^{-1}\\) (after transposition)\n\\(\\times\\): Other elements (replaced by 0 in the final calculation)\n\n\n\n3.3.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]\n\n\n\n3.3.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 3.5 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\mathcal{C}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 3.3 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "href": "lec2-matrix.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "title": "3  Spectral Theory and Generalized Inverse",
    "section": "3.4 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse",
    "text": "3.4 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse\n\n3.4.1 Projection Matrix with Generalized Inverse of \\(X'X\\)\nFor the normal equations \\((X'X)\\beta = X'y\\), a solution is given by: \\[\n\\hat{\\beta} = (X'X)^- X'y\n\\] The fitted values are \\[\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.\\] This \\(\\hat{y}\\) represents the unique orthogonal projection of \\(y\\) onto \\(\\text{Col}(X)\\).\n\n\n3.4.2 Invariance and Uniqueness of “the” Projection Matrix\n\nTheorem 3.6 (Transpose Property of Generalized Inverses) \\((X^-)'\\) is a version of \\((X')^-\\). That is, \\((X^-)'\\) is a generalized inverse of \\(X'\\).\n\n\nProof. By definition, a generalized inverse \\(X^-\\) satisfies the property: \\[\nX X^- X = X\n\\]\nTo verify that \\((X^-)'\\) is a generalized inverse of \\(X'\\), we need to show that it satisfies the condition \\(A G A = A\\) where \\(A = X'\\) and \\(G = (X^-)'\\).\n\nStart with the fundamental definition: \\[\nX X^- X = X\n\\]\nTake the transpose of both sides of the equation: \\[\n(X X^- X)' = X'\n\\]\nApply the reverse order law for transposes, \\((ABC)' = C' B' A'\\): \\[\nX' (X^-)' X' = X'\n\\]\n\nSince substituting \\((X^-)'\\) into the generalized inverse equation for \\(X'\\) yields \\(X'\\), \\((X^-)'\\) is a valid generalized inverse of \\(X'\\).\n\n\nLemma 3.1 (Invariance of Generalized Least Squares) For any version of the generalized inverse \\((X'X)^-\\), the matrix \\(X'(X'X)^- X'\\) is invariant and equals \\(X'\\). \\[\nX'X(X'X)^- X' = X'\n\\]\n\nProof (using Projection): Let \\(P = X(X'X)^- X'\\). This is the projection matrix onto \\(\\mathcal{C}(X)\\). By definition of projection, \\(Px = x\\) for any \\(x \\in \\text{Col}(X)\\). Since columns of \\(X\\) are in \\(\\text{Col}(X)\\), \\(PX = X\\). Taking the transpose: \\((PX)' = X' \\implies X'P' = X'\\). Since projection matrices are symmetric (\\(P=P'\\)), \\(X'P = X'\\). Substituting \\(P\\): \\(X' X (X'X)^- X' = X'\\).\nProof (Direct Matrix Manipulation): Decompose \\(y = X\\beta + e\\) where \\(e \\perp \\text{Col}(X)\\) (i.e., \\(X'e = 0\\)). \\[\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n\\] Using the property \\(A A^- A = A\\) (where \\(A=X'X\\)), the first term becomes \\(X'X\\beta\\). The second term is 0 because \\(X'e = 0\\). Thus, the expression simplifies to \\(X'X\\beta = X'(X\\beta) = X'\\hat{y}_{\\text{proj}}\\). This implies the operator acts as \\(X'\\).\n\nTheorem 3.7 (Properties of Projection Matrix \\(P\\)) Let \\(P = X(X'X)^- X'\\). This matrix has the following properties:\n\nSymmetry: \\(P = P'\\).\nIdempotence: \\(P^2 = P\\). \\[\nP^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n\\] Using the identity from Lemma 7.1 (\\(X'X(X'X)^- X' = X'\\)), this simplifies to: \\[\nX(X'X)^- X' = P\n\\]\nUniqueness: \\(P\\) is unique and invariant to the choice of the generalized inverse \\((X'X)^-\\).\n\n\n\nProof. Proof of Uniqueness:\nLet \\(A\\) and \\(B\\) be two different generalized inverses of \\(X'X\\). Define \\(P_A = X A X'\\) and \\(P_B = X B X'\\). From Lemma 7.1, we know that \\(X' P_A = X'\\) and \\(X' P_B = X'\\).\nSubtracting these two equations: \\[\nX' (P_A - P_B) = 0\n\\] Taking the transpose, we get \\((P_A - P_B) X = 0\\). This implies that the columns of the difference matrix \\(D = P_A - P_B\\) are orthogonal to the columns of \\(X\\) (i.e., \\(D \\perp \\text{Col}(X)\\)).\nHowever, by definition, the columns of \\(P_A\\) and \\(P_B\\) (and thus \\(D\\)) are linear combinations of the columns of \\(X\\) (i.e., \\(D \\in \\text{Col}(X)\\)).\nThe only matrix that lies in the column space of \\(X\\) but is also orthogonal to the column space of \\(X\\) is the zero matrix. Therefore: \\[\nP_A - P_B = 0 \\implies P_A = P_B\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "href": "lec2-matrix.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "title": "3  Spectral Theory and Generalized Inverse",
    "section": "3.5 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)",
    "text": "3.5 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)\nWhile the geometric properties of the linear model are most naturally established via the unique orthogonal projection \\(\\hat{y}\\), we require a functional mapping—a statistical “bridge”—to translate the distribution of these fitted values back into the parameter space of \\(\\hat{\\beta}\\). This bridge is provided by the generalized left inverse.\n\n3.5.1 The Generalized Left Inverse\nTo recover the parameter estimates directly from the fitted values, we define the generalized left inverse, denoted as \\(X_{\\text{left}}^-\\), such that:\n\\[\n\\hat{\\beta} = X_{\\text{left}}^- \\hat{y}\n\\]\nA standard choice for this operator, derived from the normal equations, is:\n\\[\nX_{\\text{left}}^- = (X' X)^- X'\n\\]\nWhen \\(X\\) is full-rank, the \\(X_{\\text{left}}^-\\) is unique, which is given by\n\\[\nX_{\\text{left}}^- = (X' X)^{-1} X'\n\\]\n\n\n3.5.2 Verification of the Inverse Property\nTo verify that \\(X_{\\text{left}}^-\\) acts as a valid generalized inverse of \\(X\\), it must satisfy the condition \\(X X_{\\text{left}}^- X = X\\). Substituting our definition:\n\\[\nX \\underbrace{\\left[ (X' X)^- X' \\right]}_{X_{\\text{left}}^-} X = X (X' X)^- (X' X)\n\\]\nUsing the property of generalized inverses for symmetric matrices where \\((X' X)(X' X)^- X' = X'\\), the transpose of this identity gives \\(X (X' X)^- (X' X) = X\\). Thus, the condition holds:\n\\[\nX X_{\\text{left}}^- X = X\n\\]\n\n\n3.5.3 Recovering the Estimator\nWe can now demonstrate that applying this left inverse to the fitted values \\(\\hat{y}\\) yields the standard solution to the normal equations.\nSubstituting the projection formula \\(\\hat{y} = X(X' X)^- X' y\\):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\left[ (X' X)^- X' \\right] \\left[ X(X' X)^- X' y \\right] \\\\\n&= (X' X)^- \\underbrace{(X' X) (X' X)^- (X' X)}_{\\text{Property } A A^- A = A} (X' X)^- X' y\n\\end{aligned}\n\\]\nSimplifying using the generalized inverse property \\(A^- A A^- = A^-\\) (where \\(A = X' X\\)):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\underbrace{(X' X)^- (X' X) (X' X)^-}_{(X' X)^-} X' y \\\\\n&= (X' X)^- X' y\n\\end{aligned}\n\\]\nThus, we recover the standard estimator used in the normal equations:\n\\[\n\\mathbf{\\hat{\\beta} = (X' X)^- X' y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#non-full-rank-least-squares-with-qr-decomposition",
    "href": "lec2-matrix.html#non-full-rank-least-squares-with-qr-decomposition",
    "title": "3  Spectral Theory and Generalized Inverse",
    "section": "3.6 Non-full-rank Least Squares with QR Decomposition",
    "text": "3.6 Non-full-rank Least Squares with QR Decomposition\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n3.6.1 Constructing a Solution by Solving Normal Equations\nOne specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n3.6.2 Constructing a Solution by Solving Reparametrized \\(\\beta\\)\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\nSolution 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html",
    "href": "lec3-mvn.html",
    "title": "4  Multivariate Normal Distribution",
    "section": "",
    "text": "4.1 Motivation\nConsider the linear model: \\[\ny = X\\beta + \\epsilon, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nWe are often interested in the distributional properties of the response vector \\(y\\) and the residuals. Specifically, if \\(y = (y_1, \\dots, y_n)'\\), we need to understand its multivariate distribution. \\[\n\\hat{y} = Py, \\quad e = y - \\hat{y} = (I_n - P)y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#random-vectors-and-matrices",
    "href": "lec3-mvn.html#random-vectors-and-matrices",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.2 Random Vectors and Matrices",
    "text": "4.2 Random Vectors and Matrices\n\nDefinition 4.1 (Random Vector and Matrix) A Random Vector is a vector whose elements are random variables. E.g., \\[\nx_{k \\times 1} = (x_1, x_2, \\dots, x_k)^T\n\\] where \\(x_1, \\dots, x_k\\) are each random variables.\nA Random Matrix is a matrix whose elements are random variables. E.g., \\(X_{n \\times k} = (x_{ij})\\), where \\(x_{11}, \\dots, x_{nk}\\) are each random variables.\n\n\nDefinition 4.2 (Expected Value) The expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.\nFor \\(X_{n \\times k}\\): \\[\nE(X) = \\begin{pmatrix}\nE(x_{11}) & \\dots & E(x_{1k}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE(x_{n1}) & \\dots & E(x_{nk})\n\\end{pmatrix}\n\\]\n\\[\nE\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}\\right) = \\begin{pmatrix} E(x_1) \\\\ \\vdots \\\\ E(x_k) \\end{pmatrix}\n\\]\n\n\nDefinition 4.3 (Variance-Covariance Matrix) For a random vector \\(x_{k \\times 1} = (x_1, \\dots, x_k)^T\\), the matrix is:\n\\[\n\\text{Var}(x) = \\Sigma_x = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2} & \\dots & \\sigma_{kk}\n\\end{pmatrix}\n\\]\nWhere:\n\n\\(\\sigma_{ij} = \\text{Cov}(x_i, x_j) = E[(x_i - \\mu_i)(x_j - \\mu_j)]\\)\n\\(\\sigma_{ii} = \\text{Var}(x_i) = E[(x_i - \\mu_i)^2]\\)\n\nIn matrix notation: \\[\n\\text{Var}(x) = E[(x - \\mu_x)(x - \\mu_x)^T]\n\\] Note: \\(\\text{Var}(x)\\) is symmetric.\n\n\n4.2.1 Derivation of Covariance Matrix Structure\nExpanding the vector multiplication for variance: \\[\n(x - \\mu_x)(x - \\mu_x)' \\quad \\text{where } \\mu_x = (\\mu_1, \\dots, \\mu_n)'\n\\] \\[\n= \\begin{pmatrix} x_1 - \\mu_1 \\\\ \\vdots \\\\ x_n - \\mu_n \\end{pmatrix} (x_1 - \\mu_1, \\dots, x_n - \\mu_n)\n\\] This results in the matrix \\(A = (a_{ij})\\) where \\(a_{ij} = (x_i - \\mu_i)(x_j - \\mu_j)\\). Taking expectations yields the covariance matrix elements \\(\\sigma_{ij}\\).\n\nDefinition 4.4 (Covariance Matrix (Two Vectors)) For random vectors \\(x_{k \\times 1}\\) and \\(y_{n \\times 1}\\), the covariance matrix is: \\[\n\\text{Cov}(x, y) = E[(x - \\mu_x)(y - \\mu_y)^T] = \\begin{pmatrix}\n\\text{Cov}(x_1, y_1) & \\dots & \\text{Cov}(x_1, y_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(x_k, y_1) & \\dots & \\text{Cov}(x_k, y_n)\n\\end{pmatrix}\n\\] Note that \\(\\text{Cov}(x, x) = \\text{Var}(x)\\).\n\n\nDefinition 4.5 (Correlation Matrix) The correlation matrix of a random vector \\(x\\) is: \\[\n\\text{corr}(x) = \\begin{pmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\dots & 1\n\\end{pmatrix}\n\\] where \\(\\rho_{ij} = \\text{corr}(x_i, x_j)\\).\nRelationships: Let \\(V_x = \\text{diag}(\\text{Var}(x_1), \\dots, \\text{Var}(x_k))\\). \\[\n\\Sigma_x = V_x^{1/2} \\rho_x V_x^{1/2} \\quad \\text{and} \\quad \\rho_x = (V_x^{1/2})^{-1} \\Sigma_x (V_x^{1/2})^{-1}\n\\] Similarly for two vectors: \\[\n\\Sigma_{xy} = V_x^{1/2} \\rho_{xy} V_y^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#properties-of-mean-and-variance",
    "href": "lec3-mvn.html#properties-of-mean-and-variance",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.3 Properties of Mean and Variance",
    "text": "4.3 Properties of Mean and Variance\nWe can derive several key algebraic properties for operations on random vectors.\n\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E(AXB) = A E(X) B\\) (In particular, \\(E(AX) = A\\mu_x\\))\n\\(\\text{Cov}(x, y) = \\text{Cov}(y, x)^T\\)\n\\(\\text{Cov}(x + c, y + d) = \\text{Cov}(x, y)\\)\n\\(\\text{Cov}(Ax, By) = A \\text{Cov}(x, y) B^T\\)\n\nSpecial case for scalars: \\(\\text{Cov}(ax, by) = ab \\cdot \\text{Cov}(x, y)\\)\n\n\\(\\text{Cov}(x_1 + x_2, y_1) = \\text{Cov}(x_1, y_1) + \\text{Cov}(x_2, y_1)\\)\n\\(\\text{Var}(x + c) = \\text{Var}(x)\\)\n\\(\\text{Var}(Ax) = A \\text{Var}(x) A^T\\)\n\\(\\text{Var}(x_1 + x_2) = \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)\\)\n\\(\\text{Var}(\\sum x_i) = \\sum \\text{Var}(x_i)\\) if independent.\n\n\nProof. Property 5 (Covariance of Linear Transformation): \\[\n\\begin{aligned}\n\\text{Cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{Cov}(x, y) B^T\n\\end{aligned}\n\\] Property 2 (Expectation of Linear Transformation):\nTo prove \\(E(AXB) = A E(X) B\\): First consider \\(E(Ax_j)\\) where \\(x_j\\) is a column of \\(X\\). \\[\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n\\] Since \\(a_i\\) are constants: \\[\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n\\] Thus \\(E(Ax_j) = A E(x_j)\\). Applying this to all columns of \\(X\\): \\[\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n\\] Similarly, \\(E(XB) = E(X)B\\).\nProof of Property 9 (Variance of Sum):\n\\[\n\\text{Var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n\\] Let centered variables be denoted by differences. \\[\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n\\] Expanding terms: \\[\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n\\] \\[\n= \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#the-multivariate-normal-distribution",
    "href": "lec3-mvn.html#the-multivariate-normal-distribution",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.4 The Multivariate Normal Distribution",
    "text": "4.4 The Multivariate Normal Distribution\n\n4.4.1 Definition and Density\n\nDefinition 4.6 (Independent Standard Normal) Let \\(z = (z_1, \\dots, z_n)'\\) where \\(z_i \\sim N(0, 1)\\) are independent. We say \\(z \\sim N_n(0, I_n)\\). The joint PDF is the product of marginals: \\[\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n\\] Properties: \\(E(z) = 0\\) and \\(\\text{Var}(z) = I_n\\) (Covariance is 0 for \\(i \\ne j\\), Variance is 1).\n\n\nDefinition 4.7 (Multivariate Normal Distribution) A random vector \\(x\\) (\\(n \\times 1\\)) has a multivariate normal distribution if it has the same distribution as: \\[\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n\\] where \\(z \\sim N_p(0, I_p)\\), \\(A\\) is a matrix of constants, and \\(\\mu\\) is a vector of constants. The moments are:\n\n\\(E(x) = \\mu\\)\n\\(\\text{Var}(x) = AA^T = \\Sigma\\)\n\n\n\n\n4.4.2 Geometric Interpretation\nUsing Spectral Decomposition, \\(\\Sigma = Q \\Lambda Q'\\). We can view the transformation \\(x = Az + \\mu\\) as:\n\nScaling by eigenvalues (\\(\\Lambda^{1/2}\\)).\nRotation by eigenvectors (\\(Q\\)).\nShift by mean (\\(\\mu\\)).\n\nAn Shinely App for Visualizing Bivariate Normal\nUse the controls to construct the covariance matrix \\(\\boldsymbol{\\Sigma}\\) geometrically.\nWe define the transformation matrix \\(\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}\\), where \\(\\mathbf{Q}\\) is a rotation matrix and \\(\\mathbf{\\Lambda}^{1/2}\\) is a diagonal scaling matrix. The resulting covariance is \\(\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}'\\).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n#| echo: false\n\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(shinyWidgets)\nlibrary(munsell) \nlibrary(scales)\nlibrary(tibble)\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# --- 1. PRE-GENERATE FIXED Z POINTS ---\nset.seed(123)\nz_fixed &lt;- matrix(rnorm(50 * 2), ncol = 2)\n\nui &lt;- page_fillable(\n  theme = bs_theme(version = 5),\n  withMathJax(), \n  \n  # --- ROW 1: CONTROLS (Compact Strip) ---\n  card(\n    class = \"p-2\", \n    layout_columns(\n      col_widths = c(3, 2, 2, 2, 2),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\theta$$\")), \n          noUiSliderInput(\"theta\", label = NULL, min = 0, max = 360, value = 0, step = 5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#0d6efd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_1}$$\")), \n          noUiSliderInput(\"L1\", label = NULL, min = 0.5, max = 3, value = 2, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#ffc107\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_2}$$\")), \n          noUiSliderInput(\"L2\", label = NULL, min = 0.5, max = 3, value = 1, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#adb5bd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_1$$\")), \n          noUiSliderInput(\"mu1\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_2$$\")), \n          noUiSliderInput(\"mu2\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\"))\n    )\n  ),\n\n  # --- ROW 2: SIDE-BY-SIDE (Plot & Math) ---\n  layout_columns(\n    col_widths = c(8, 4), # 2/3 for Plot, 1/3 for Matrix\n    \n    # Left: Visualization\n    card(\n      full_screen = TRUE,\n      plotOutput(\"contourPlot\", height = \"500px\")\n    ),\n    \n    # Right: The Math (Larger Font)\n    card(\n      class = \"p-3 d-flex justify-content-center\", # Center content vertically\n      h5(\"Algebraic Representation\", class = \"mb-3 text-center\"),\n      \n      # Use CSS to make the font larger and monospaced\n      div(\n        style = \"font-family: 'Courier New', monospace; font-size: 1.1rem; line-height: 1.4;\",\n        verbatimTextOutput(\"matrixSide\", placeholder = TRUE)\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n\n  data &lt;- reactive({\n    theta_rad &lt;- input$theta * pi / 180\n    Q &lt;- matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), 2, 2)\n    Lam_sqrt &lt;- diag(c(input$L1, input$L2))\n    \n    A &lt;- Q %*% Lam_sqrt\n    Sigma &lt;- A %*% t(A)\n    mu_vec &lt;- c(input$mu1, input$mu2)\n    \n    x_points &lt;- z_fixed %*% t(A)\n    x_points[,1] &lt;- x_points[,1] + mu_vec[1]\n    x_points[,2] &lt;- x_points[,2] + mu_vec[2]\n    \n    list(Q=Q, L=c(input$L1, input$L2), mu=mu_vec, Sigma=Sigma, A=A, points=as.data.frame(x_points))\n  })\n\n  output$matrixSide &lt;- renderText({\n    M &lt;- data()\n    A &lt;- round(M$A, 2)\n    S &lt;- round(M$Sigma, 2)\n    rho &lt;- cov2cor(M$Sigma)[1,2]\n    \n    # Formatted to fill vertical space comfortably\n    paste0(\n      \"Linear Transform:\\n\",\n      \"x = A z + μ\\n\\n\",\n      \n      \"Matrix A:\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[1,1], A[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[2,1], A[2,2]),\n      \"\\n\",\n      \n      \"Covariance Σ:\\n\",\n      \"(Σ = AA')\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[1,1], S[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[2,1], S[2,2]),\n      \"\\n\",\n      \n      \"Correlation:\\n\",\n      sprintf(\"ρ = %.3f\", rho)\n    )\n  })\n\n  output$contourPlot &lt;- renderPlot({\n    req(data())\n    M &lt;- data()\n    \n    grid_r &lt;- seq(-6, 6, length.out = 60)\n    df_grid &lt;- expand.grid(x = grid_r, y = grid_r)\n    df_grid$z &lt;- dmvnorm(as.matrix(df_grid), mean = M$mu, sigma = M$Sigma)\n    \n    v1 &lt;- M$Q[,1] * M$L[1]; v2 &lt;- M$Q[,2] * M$L[2]\n    axes &lt;- tibble(x = M$mu[1], y = M$mu[2],\n                   xend1 = M$mu[1] + v1[1], yend1 = M$mu[2] + v1[2],\n                   xend2 = M$mu[1] + v2[1], yend2 = M$mu[2] + v2[2])\n    \n    ggplot() +\n      geom_contour_filled(data = df_grid, aes(x, y, z = z), bins = 9, show.legend = FALSE) +\n      geom_point(data = M$points, aes(V1, V2), color = \"black\", size = 2, alpha = 0.7) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend1, yend=yend1), \n                   color = \"#ffc107\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend2, yend=yend2), \n                   color = \"white\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      coord_fixed(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      theme_minimal() +\n      labs(x = \"X\", y = \"Y\")\n  })\n}\n\nshinyApp(ui, server)\n\n\n4.4.3 Probability Density Function\nIf \\(\\Sigma\\) is positive definite, the PDF exists. We use the change of variable formula for \\(x = Az + \\mu\\): \\[\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n\\] where \\(z = A^{-1}(x - \\mu)\\) and \\(J = \\det(A^{-1}) = |A|^{-1}\\).\n\\[\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n\\]\nUsing \\(|\\Sigma| = |AA^T| = |A|^2\\) and \\(\\Sigma^{-1} = (AA^T)^{-1}\\), we get: \\[\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n\\]\n\n\n4.4.4 Moment Generating Function\n\nDefinition 4.8 (Moment Generating Function (MGF)) The MGF of a random vector \\(x\\) is \\(M_x(t) = E(e^{t^T x})\\). For \\(x = Az + \\mu\\): \\[\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n\\] Since \\(M_z(u) = e^{u^T u / 2}\\): \\[\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\n\nKey Properties:\n\nUniqueness: Two random vectors with the same MGF have the same distribution.\nIndependence: \\(y_1\\) and \\(y_2\\) are independent iff \\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#construction-and-linear-transformations",
    "href": "lec3-mvn.html#construction-and-linear-transformations",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.5 Construction and Linear Transformations",
    "text": "4.5 Construction and Linear Transformations\n\nTheorem 4.1 (Constructing MVN Random Vector) Let \\(\\mu \\in \\mathbb{R}^n\\) and \\(\\Sigma\\) be an \\(n \\times n\\) symmetric non-negative definitive (n.n.d) matrix. Then there exists a multivariate normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\).\n\n\nProof. Since \\(\\Sigma\\) is n.n.d., there exists \\(B\\) such that \\(\\Sigma = BB^T\\) (e.g., via Cholesky or Spetral Decomposition). Let \\(z \\sim N_n(0, I)\\) and define \\(x = Bz + \\mu\\).\n\n\nTheorem 4.2 (Linear Transformation Theorem) Let \\(x \\sim N_n(\\mu, \\Sigma)\\). Let \\(y = Cx + d\\) where \\(C\\) is \\(r \\times n\\) and \\(d\\) is \\(r \\times 1\\). Then: \\[\ny \\sim N_r(C\\mu + d, C \\Sigma C^T)\n\\]\n\n\nProof. \\(x = Az + \\mu\\) where \\(AA^T = \\Sigma\\). \\[\ny = C(Az + \\mu) + d = (CA)z + (C\\mu + d)\n\\] This fits the definition of MVN with mean \\(C\\mu + d\\) and variance \\(C \\Sigma C^T\\).\n\n\n4.5.1 Important Corollaries of Theorem 4.2\n\nCorollary 4.1 (Marginals) Any subvector of a multivariate normal vector is also multivariate normal.\n\n\nProof. If we partition \\(x = (x_1', x_2')'\\), we can use \\(C = (I_r, 0)\\) to show \\(x_1 \\sim N(\\mu_1, \\Sigma_{11})\\).\n\n\nCorollary 4.2 (Univariate Combinations) Any linear combination \\(a^T x\\) is univariate normal: \\[\na^T x \\sim N(a^T \\mu, a^T \\Sigma a)\n\\]\n\n\nCorollary 4.3 (Orthogonal Transformations) If \\(x \\sim N(0, I_n)\\) and \\(Q\\) is orthogonal (\\(Q'Q = I\\)), then \\(y = Q'x \\sim N(0, I_n)\\).\n\n\nCorollary 4.4 (Standardization) If \\(y \\sim N_n(\\mu, \\Sigma)\\) and \\(\\Sigma\\) is positive definite: \\[\n\\Sigma^{-1/2}(y - \\mu) \\sim N_n(0, I_n)\n\\]\n\n\nProof. Let \\(z = \\Sigma^{-1/2}(y - \\mu)\\). Then \\(\\text{Var}(z) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = I_n\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#independence",
    "href": "lec3-mvn.html#independence",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.6 Independence",
    "text": "4.6 Independence\n\nTheorem 4.3 (Independence in MVN) Let \\(y \\sim N(\\mu, \\Sigma)\\) be partitioned into \\(y_1\\) and \\(y_2\\). \\[\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n\\] Then \\(y_1\\) and \\(y_2\\) are independent if and only if \\(\\Sigma_{12} = 0\\) (zero covariance).\n\n\nProof. 1. Independence \\(\\implies\\) Covariance is 0: This holds generally for any distribution. \\[\n\\text{Cov}(y_1, y_2) = E[(y_1 - \\mu_1)(y_2 - \\mu_2)'] = 0\n\\]\n2. Covariance is 0 \\(\\implies\\) Independence: This is specific to MVN. We use MGFs. If \\(\\Sigma_{12} = 0\\), the quadratic form in the MGF splits: \\[\nt^T \\Sigma t = t_1^T \\Sigma_{11} t_1 + t_2^T \\Sigma_{22} t_2\n\\] The MGF becomes: \\[\nM_y(t) = \\exp(t_1^T \\mu_1 + \\frac{1}{2} t_1^T \\Sigma_{11} t_1) \\times \\exp(t_2^T \\mu_2 + \\frac{1}{2} t_2^T \\Sigma_{22} t_2)\n\\] \\[\nM_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\n\\] Thus, they are independent.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#signal-noise-decomposition-for-multivariate-normal-distribution",
    "href": "lec3-mvn.html#signal-noise-decomposition-for-multivariate-normal-distribution",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.7 Signal-Noise Decomposition for Multivariate Normal Distribution",
    "text": "4.7 Signal-Noise Decomposition for Multivariate Normal Distribution\nWe can formalize the relationship between two random vectors \\(y\\) and \\(x\\) through a decomposition theorem that separates the systematic signal from the stochastic noise.\n\nTheorem 4.4 (Regression Decomposition Theorem) Let the random vector \\(V\\) of dimension \\(p \\times 1\\) be partitioned into two subvectors \\(y\\) (\\(p_1 \\times 1\\)) and \\(x\\) (\\(p_2 \\times 1\\)). Assume \\(V\\) follows a multivariate normal distribution:\n\\[\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N_p\\left( \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\right)\n\\]\nThe response vector \\(y\\) can be uniquely decomposed into a systematic component and a stochastic error: \\[\ny = m(x) + e\n\\] where we define the Regression Coefficient Matrix \\(B\\) and the components as:\n\\[\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1}\n\\]\n\\[\nm(x) = \\mu_y + B(x - \\mu_x)\n\\]\n\\[\ne = y - m(x)\n\\]\nProperties:\n\nIndependence: The noise vector \\(e\\) is statistically independent of the predictor \\(x\\) (and consequently independent of \\(m(x)\\)).\nMarginal Distributions:\n\n\\(m(x) \\sim N_{p_1}(\\mu_y, \\; B \\Sigma_{xx} B^T)\\)\n\\(e \\sim N_{p_1}(0, \\; \\Sigma_{yy} - B \\Sigma_{xx} B^T)\\)\n\nConditional Distribution: Since \\(y = m(x) + e\\), and \\(e\\) is independent of \\(x\\), the conditional distribution is: \\[\ny | x \\sim N_{p_1}(m(x), \\Sigma_{y|x})\n\\] where: \\[\nm(x) = \\mu_y + B(x - \\mu_x) = \\mu_y + \\Sigma_{yx}\\Sigma_{xx}^{-1}(x - \\mu_x)\n\\] \\[\n\\Sigma_{y|x} = \\Sigma_{yy} - B \\Sigma_{xx} B^T = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\n\\]\n\n\n\nProof. We define a transformation from the input vector \\(V = \\begin{pmatrix} y \\\\ x \\end{pmatrix}\\) to the target vector \\(W = \\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}\\).\nUsing the linear transformation \\(W = CV + d\\):\n\\[\n\\underbrace{\\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}}_{W} = \\underbrace{\\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix}}_{C} \\underbrace{\\begin{pmatrix} y \\\\ x \\end{pmatrix}}_{V} + \\underbrace{\\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -(\\mu_y - B \\mu_x) \\end{pmatrix}}_{d}\n\\]\n1. Mean Vector\n\\[\nE[W] = C E[V] + d = \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix}\n= \\begin{pmatrix} B \\mu_x \\\\ \\mu_y - B \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix}\n= \\begin{pmatrix} \\mu_y \\\\ 0 \\end{pmatrix}\n\\]\n2. Covariance Matrix\nWe compute \\(\\text{Var}(W) = C \\Sigma C^T\\) directly:\n\\[\n\\begin{aligned}\nC \\Sigma C^T &= \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xy} & B \\Sigma_{xx} \\\\ \\Sigma_{yy} - B \\Sigma_{xy} & \\Sigma_{yx} - B \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & B \\Sigma_{xy} - B \\Sigma_{xx} B^T \\\\ \\Sigma_{yx}B^T - B \\Sigma_{xx} B^T & (\\Sigma_{yy} - B \\Sigma_{xy}) - (\\Sigma_{yx} - B \\Sigma_{xx})B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & 0 \\\\ 0 & \\Sigma_{yy} - B \\Sigma_{xx} B^T \\end{pmatrix}\n\\end{aligned}\n\\]\n3. Conditional Distribution\nWe have established that \\(y = m(x) + e\\) where \\(e\\) is independent of \\(x\\). To find the distribution of \\(y\\) conditional on \\(x\\), we observe that \\(m(x)\\) becomes a constant vector when \\(x\\) is fixed, and the randomness comes solely from \\(e\\):\n\\[\nE[y|x] = m(x) + E[e|x] = m(x) + 0 = m(x)\n\\] \\[\n\\text{Var}(y|x) = \\text{Var}(m(x)|x) + \\text{Var}(e|x) = 0 + \\text{Var}(e) = \\Sigma_{y|x}\n\\]\nThus, \\(y | x \\sim N(m(x), \\Sigma_{y|x})\\).\n\n\n4.7.1 Connections with Other Formulas\n\n4.7.1.1 Rao-Blackwell Decomposition of Variance\nThe Law of Total Variance (Rao-Blackwell theorem) allows us to decompose the total variance of \\(y\\) into two orthogonal components based on the predictor \\(x\\):\n\\[\n\\text{Var}(y) = \\underbrace{E[\\text{Var}(y | x)]}_{\\text{Unexplained (Noise)}} + \\underbrace{\\text{Var}[E(y | x)]}_{\\text{Explained (Signal)}}\n\\]\nIn the Multivariate Normal case, this decomposition perfectly aligns with our regression model \\(y = m(x) + e\\).\n\n\nVariance of Noise\nThis term represents the average variance remaining in \\(y\\) after accounting for \\(x\\). It corresponds to the variance of the error term \\(e\\):\n\\[\nE[\\text{Var}(y | x)] = \\text{Var}(e) = \\Sigma_{yy} - B \\Sigma_{xx} B^T\n\\]\n\n\nVariance of Signal\nThis term represents the variability of the conditional mean \\(m(x)\\) itself. Using the matrix \\(B\\), this takes the quadratic form:\n\\[\n\\text{Var}[E(y | x)] = \\text{Var}[m(x)] = B \\Sigma_{xx} B^T\n\\]\n\n\nTotal Variance\nSumming the Signal and Noise components recovers the total marginal variance of \\(y\\):\n\\[\n\\Sigma_{yy} = \\underbrace{\\Sigma_{yy} - B \\Sigma_{xx} B^T}_{\\text{Unexplained (Noise)}} + \\underbrace{B \\Sigma_{xx} B^T}_{\\text{Explained (Signal)}}\n\\]\n\n\n4.7.1.2 Connection to OLS Regression Estimators\nIn OLS regression, centering the data allows us to separate the intercept from the slopes. Let \\(\\mathbf{y}_c\\) and \\(\\mathbf{X}_c\\) be the centered response and design matrices (where \\(\\mathbf{X}_c\\) excludes the column of 1s). Using this centered form, the total sum of squares decomposes exactly like the population variance:\n\\[\n\\text{SST} = \\text{SSR} + \\text{SSE}\n\\]\nComparing the sample quantities to their population counterparts:\n\nRegression Coefficients: \\[\n  \\hat{\\beta}^T = (\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{y}_c \\approx B\n  \\] Note: \\(\\hat{\\beta}\\) here represents only the slope coefficients, matching the dimensions of the covariance matrix \\(\\Sigma_{xx}\\).\nExplained Variation (Signal): \\[\n\\text{SSR} = \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1) B \\Sigma_{xx} B^T\n\\]\nUnexplained Variation (Noise): \\[\n\\text{SSE} = \\mathbf{y}_c^T \\mathbf{y}_c - \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1)(\\Sigma_{yy} - B \\Sigma_{xx} B^T)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#partial-and-multiple-correlation",
    "href": "lec3-mvn.html#partial-and-multiple-correlation",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.8 Partial and Multiple Correlation",
    "text": "4.8 Partial and Multiple Correlation\n\nDefinition 4.9 (Partial Correlation) The partial correlation between elements \\(y_i\\) and \\(y_j\\) given a set of variables \\(x\\) is derived from the conditional covariance matrix \\(\\Sigma_{y|x}\\): \\[\n\\rho_{ij|x} = \\frac{\\sigma_{ij|x}}{\\sqrt{\\sigma_{ii|x} \\sigma_{jj|x}}}\n\\] where \\(\\sigma_{ij|x}\\) are elements of \\(\\Sigma_{y|x} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\\).\n\n\nDefinition 4.10 (Multiple Correlation (\\(R^2\\))) For a scalar \\(y\\) and vector \\(x\\), the squared multiple correlation is the proportion of variance of \\(y\\) explained by the conditional mean: \\[\nR^2_{y|x} = \\frac{\\text{Var}(E(y|x))}{\\text{Var}(y)} = \\frac{\\Sigma_{yx} \\Sigma_{xx}^{-1} \\Sigma_{xy}}{\\sigma^2_{y}}\n\\]\n\nNote: this definition is the population or theretical \\(R^2\\), which is estimated by adjusted \\(R^2\\) using sample in linear regression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#examples",
    "href": "lec3-mvn.html#examples",
    "title": "4  Multivariate Normal Distribution",
    "section": "4.9 Examples",
    "text": "4.9 Examples\n\nExample 4.1 (Bivariate Normal) Let the random vector \\(\\begin{pmatrix} y \\\\ x \\end{pmatrix}\\) follow a bivariate normal distribution: \\[\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 2 \\\\ 2 & 4 \\end{pmatrix} \\right)\n\\] Here, \\(\\mu_y = 1, \\mu_x = 2, \\Sigma_{yy} = 2, \\Sigma_{xx} = 4\\), and \\(\\Sigma_{yx} = 2\\).\n1. Finding the Regression Coefficient Matrix \\(B\\) Using the population formula: \\[\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1} = 2(4)^{-1} = 0.5\n\\]\n2. Finding the Conditional Mean \\(m(x)\\) (The Signal) The systematic component represents the projection of \\(y\\) onto \\(x\\): \\[\n\\begin{aligned}\nm(x) &= \\mu_y + B(x - \\mu_x) \\\\\n&= 1 + 0.5(x - 2) = 0.5x\n\\end{aligned}\n\\]\n3. Variance of the Signal \\(\\text{Var}(m(x))\\) Using the quadratic form established in the theorem: \\[\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = 0.5(4)(0.5) = 1\n\\]\n4. Variance of the Noise \\(\\text{Var}(y|x)\\) (The Residual) By the Signal-Noise Decomposition: \\[\n\\begin{aligned}\n\\text{Var}(y|x) &= \\Sigma_{yy} - \\text{Var}(m(x)) \\\\\n&= 2 - 1 = 1\n\\end{aligned}\n\\] Thus, \\(y | x \\sim N(m(x), 1)\\). The total variance (2) is split equally between signal (1) and noise (1).\n5. Multiple Correlation Coefficient (\\(R^2\\)) \\[\nR^2 = \\frac{\\text{Var}(m(x))}{\\Sigma_{yy}} = \\frac{1}{2} = 0.5\n\\]\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of Rao-Blackwell Variance Decomposition in Bivariate Normal\n\n\n\n\n\n\n\nExample 4.2 (Trivariate Normal with 2 Predictors) Let \\(V = (y, x_1, x_2)' \\sim N_3(\\mu, \\Sigma)\\) with: \\[\n\\mu = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 10 & 3 & 4 \\\\ 3 & 2 & 1 \\\\ 4 & 1 & 4 \\end{pmatrix}\n\\] We partition these into \\(\\Sigma_{yy} = 10\\), \\(\\Sigma_{yx} = \\begin{pmatrix} 3 & 4 \\end{pmatrix}\\), and \\(\\Sigma_{xx} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix}\\).\n1. Finding the Regression Coefficient Matrix \\(B\\) \\[\n\\Sigma_{xx}^{-1} = \\frac{1}{7} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2 \\end{pmatrix} \\implies B = \\Sigma_{yx} \\Sigma_{xx}^{-1} = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix}\n\\]\n2. Finding the Conditional Mean \\(m(x)\\) (The Signal) \\[\nm(x) = 1 + \\frac{8}{7}(x_1 - 2) + \\frac{5}{7}(x_2 - 3)\n\\]\n3. Variance of the Signal \\(\\text{Var}(m(x))\\) \\[\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{44}{7} \\approx 6.29\n\\]\n4. Variance of the Noise \\(\\text{Var}(y|x)\\) (The Residual) Using the Signal-Noise Decomposition: \\[\n\\Sigma_{y|x} = \\Sigma_{yy} - \\text{Var}(m(x)) = 10 - 6.29 = 3.71\n\\]\n5. Multiple Correlation Coefficient (\\(R^2\\)) \\[\nR^2 = \\frac{6.29}{10} = 0.629\n\\]\n\n\nCode\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\nmu &lt;- c(1, 2, 3)\nsigma &lt;- matrix(c(10, 3, 4, 3, 2, 1, 4, 1, 4), nrow=3, byrow=TRUE)\n\nvar_total &lt;- sigma[1,1]\nS_yx &lt;- matrix(sigma[1, 2:3], nrow=1)\nS_xx &lt;- sigma[2:3, 2:3]\nB_mat &lt;- S_yx %*% solve(S_xx)\nvar_signal &lt;- as.numeric(B_mat %*% S_xx %*% t(B_mat))\nvar_noise &lt;- var_total - var_signal\n\nset.seed(2024)\ndat &lt;- rmvnorm(1000, mean=mu, sigma=sigma)\ndf &lt;- data.frame(y=dat[,1], x1=dat[,2], x2=dat[,3])\ndf$m_x &lt;- 1 + (8/7)*(df$x1 - 2) + (5/7)*(df$x2 - 3)\n\nlimit_min &lt;- -12\nlimit_max &lt;- 12\nseq_vals &lt;- seq(limit_min, limit_max, length.out=300)\nscale_factor &lt;- 20 \n\ndf_total &lt;- data.frame(y = seq_vals, x = 9 + dnorm(seq_vals, 1, sqrt(var_total)) * scale_factor)\ndf_signal &lt;- data.frame(x = seq_vals, y = -8 - dnorm(seq_vals, 1, sqrt(var_signal)) * scale_factor)\ndf_noise &lt;- data.frame(y = seq_vals, x = 5 + dnorm(seq_vals, 5, sqrt(var_noise)) * scale_factor)\n\nggplot(df, aes(x=m_x, y=y)) +\n  geom_abline(intercept=0, slope=1, color=\"red\", linewidth=0.5, alpha=0.3) +\n  geom_point(alpha=0.15, size=1.5, color=\"black\") +\n  geom_polygon(data=df_signal, aes(x=x, y=y), fill=\"red\", alpha=0.3) +\n  geom_path(data=df_signal, aes(x=x, y=y), color=\"red\", linewidth=1) +\n  annotate(\"text\", x=1, y=-11, label=\"Signal Var\\n(m(x))\", color=\"red\", size=3, fontface=\"bold\") +\n  geom_polygon(data=df_total, aes(x=x, y=y), fill=\"gray40\", alpha=0.3) +\n  geom_path(data=df_total, aes(x=x, y=y), color=\"gray40\", linewidth=1) +\n  annotate(\"text\", x=11, y=6, label=\"Total Var\\n(y)\", color=\"gray40\", size=3, fontface=\"bold\") +\n  geom_polygon(data=df_noise, aes(x=x, y=y), fill=\"blue\", alpha=0.3) +\n  geom_path(data=df_noise, aes(x=x, y=y), color=\"blue\", linewidth=1) +\n  annotate(\"text\", x=6, y=9, label=\"Noise Var\\n(y|x)\", color=\"blue\", size=3, fontface=\"bold\") +\n  scale_x_continuous(limits=c(-12, 14)) + scale_y_continuous(limits=c(-14, 12)) +\n  coord_fixed(ratio=1) + labs(x = \"Signal m(x)\", y = \"Observed y\") + theme_minimal()\n\n\n\n\n\n\n\n\nFigure 4.2: Signal-Noise Variance Decomposition in Multivariate Normal\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\nx1_seq &lt;- seq(min(df$x1), max(df$x1), length.out=20)\nx2_seq &lt;- seq(min(df$x2), max(df$x2), length.out=20)\ngrid &lt;- expand.grid(x1=x1_seq, x2=x2_seq)\ngrid$y_pred &lt;- 1 + (8/7)*(grid$x1 - 2) + (5/7)*(grid$x2 - 3)\nz_matrix &lt;- matrix(grid$y_pred, nrow=20, ncol=20)\n\nplot_ly() %&gt;%\n  add_markers(data = df, x = ~x1, y = ~x2, z = ~y,\n              marker = list(size = 3, color = '#444', opacity = 0.5),\n              name = \"Observed (Total)\") %&gt;%\n  add_surface(x = x1_seq, y = x2_seq, z = z_matrix,\n              opacity = 0.6, colorscale = list(c(0, 1), c(\"red\", \"red\")),\n              showscale = FALSE, name = \"Signal (m(x))\") %&gt;%\n  layout(scene = list(xaxis = list(title = \"x1\"), yaxis = list(title = \"x2\"), zaxis = list(title = \"y\")))\n\n\n\n\n\n\n\n\nFigure 4.3: Interactive 3D Plot: Signal Plane vs Noise",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html",
    "href": "lec4-qf.html",
    "title": "5  Distribution of Quadratic Forms",
    "section": "",
    "text": "5.1 Quadratic Forms\nThis chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.\nA quadratic form is a polynomial with terms all of degree two.\nExamples:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#quadratic-forms",
    "href": "lec4-qf.html#quadratic-forms",
    "title": "5  Distribution of Quadratic Forms",
    "section": "",
    "text": "Definition 5.1 (Quadratic Form) Let \\(y = (y_1, \\dots, y_n)'\\) be a random vector and \\(A\\) be a symmetric \\(n \\times n\\) matrix. The scalar quantity \\(y'Ay\\) is called a quadratic form in \\(y\\).\n\\[\ny'Ay = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} y_i y_j\n\\]\n\n\n\nSquared Norm: If \\(A = I_n\\), then \\(y'I_n y = y'y = \\sum y_i^2 = ||y||^2\\).\nWeighted Sum of Squares: If \\(A\\) is diagonal with elements \\(\\lambda_i\\), then \\(y'Ay = \\sum \\lambda_i y_i^2\\).\nProjection Sum of Squares: If \\(P\\) is a projection matrix, \\(||Py||^2 = (Py)'(Py) = y'P'Py = y'Py\\) (since \\(P\\) is symmetric and idempotent).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#mean-of-quadratic-forms",
    "href": "lec4-qf.html#mean-of-quadratic-forms",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.2 Mean of Quadratic Forms",
    "text": "5.2 Mean of Quadratic Forms\nWe can find the expected value of a quadratic form without assuming normality.\n\nLemma 5.1 (Mean of Simplified Quadratic Form) If \\(y\\) is a random vector with mean \\(E(y) = \\mu\\) and covariance matrix \\(\\text{Var}(y) = I_n\\), then: \\[\nE(y'y) = \\text{tr}(I_n) + \\mu'\\mu = n + \\mu'\\mu\n\\]\n\n\nProof. Let us decompose \\(y\\) into its mean and a stochastic component: \\(y = \\mu + z\\), where \\(E(z) = 0\\) and \\(\\text{Var}(z) = E(zz') = I_n\\). Substituting this into the quadratic form: \\[\n\\begin{aligned}\ny'y &= (\\mu + z)'(\\mu + z) \\\\\n&= \\mu'\\mu + \\mu'z + z'\\mu + z'z \\\\\n&= \\mu'\\mu + 2\\mu'z + z'z\n\\end{aligned}\n\\] Taking the expectation: \\[\n\\begin{aligned}\nE(y'y) &= \\mu'\\mu + 2\\mu'E(z) + E(z'z) \\\\\n&= \\mu'\\mu + 0 + E\\left(\\sum_{i=1}^n z_i^2\\right)\n\\end{aligned}\n\\] Since \\(\\text{Var}(z_i) = E(z_i^2) - (E(z_i))^2 = 1 - 0 = 1\\), we have \\(E(\\sum z_i^2) = \\sum 1 = n\\). Thus, \\(E(y'y) = n + \\mu'\\mu\\).\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(dplyr)\n\n# --- 1. Setup Data & Parameters ---\nset.seed(42)\nn &lt;- 100\nsigma_val &lt;- 1          \nSigma &lt;- diag(2) * sigma_val^2\n\nmu_orig &lt;- c(5, 6)      # Original Mean\ny_orig  &lt;- c(7, 5)      # Updated Point y\n\n# Generate 100 Points from N(mu, I)\ndata_orig &lt;- mvrnorm(n, mu_orig, Sigma)\n\n# Define Rotation Angles\nangles &lt;- c(0, 70, 180)\n\n# --- 2. Process Data for Each Angle ---\npoints_list &lt;- list()\nvectors_list &lt;- list()\n\nfor (deg in angles) {\n  theta &lt;- deg * pi / 180\n  rot_mat &lt;- matrix(c(cos(theta), -sin(theta), \n                      sin(theta),  cos(theta)), nrow = 2, byrow = TRUE)\n  \n  # A. Rotate Points\n  data_rot &lt;- data_orig %*% t(rot_mat)\n  df_pts &lt;- data.frame(x = data_rot[,1], y = data_rot[,2])\n  df_pts$Angle &lt;- factor(paste0(deg, \"°\"), levels = paste0(angles, \"°\"))\n  points_list[[length(points_list) + 1]] &lt;- df_pts\n  \n  # B. Rotate Vectors (mu and y)\n  mu_rot &lt;- as.vector(rot_mat %*% mu_orig)\n  y_rot  &lt;- as.vector(rot_mat %*% y_orig)\n  \n  df_vec &lt;- data.frame(\n    Angle = factor(paste0(deg, \"°\"), levels = paste0(angles, \"°\")),\n    mu_x = mu_rot[1], mu_y = mu_rot[2],\n    y_x  = y_rot[1],  y_y  = y_rot[2]\n  )\n  vectors_list[[length(vectors_list) + 1]] &lt;- df_vec\n}\n\nall_points  &lt;- do.call(rbind, points_list)\nall_vectors &lt;- do.call(rbind, vectors_list)\n\n# --- 3. Create Circle Data ---\n# Radius Is the Length of Mu\nradius_mu &lt;- sqrt(sum(mu_orig^2))\ncircle_data &lt;- data.frame(\n  x0 = 0, y0 = 0, r = radius_mu\n)\n\n# --- 4. Generate the Plot ---\nggplot() +\n  # 1. Circle through the mu's (Centered at 0,0)\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = radius_mu), \n                       color = \"gray50\", linetype = \"dotted\", size = 0.5) +\n  \n  # 2. Points (Data Cloud)\n  geom_point(data = all_points, aes(x = x, y = y, color = Angle), \n             size = 0.5, alpha = 0.5) +\n  \n  # 3. Vector mu (Origin -&gt; mu)\n  geom_segment(data = all_vectors, \n               aes(x = 0, y = 0, xend = mu_x, yend = mu_y, color = Angle),\n               arrow = arrow(length = unit(0.2, \"cm\")), size = 0.8) +\n  \n  # 4. Vector y (Origin -&gt; y)\n  geom_segment(data = all_vectors, \n               aes(x = 0, y = 0, xend = y_x, yend = y_y, color = Angle),\n               arrow = arrow(length = unit(0.2, \"cm\")), size = 0.8) +\n  \n  # 5. Vector y - mu (mu -&gt; y)\n  geom_segment(data = all_vectors, \n               aes(x = mu_x, y = mu_y, xend = y_x, yend = y_y, color = Angle),\n               arrow = arrow(length = unit(0.15, \"cm\")), \n               linetype = \"dashed\", size = 0.6) +\n  \n  # 6. Labels for mu, y, and y-mu\n  geom_text(data = all_vectors, aes(x = mu_x, y = mu_y, label = expression(mu), color = Angle),\n            parse = TRUE, vjust = -0.5, size = 4, show.legend = FALSE) +\n  \n  geom_text(data = all_vectors, aes(x = y_x, y = y_y, label = \"y\", color = Angle),\n            vjust = -0.5, hjust = -0.2, size = 4, fontface = \"italic\", show.legend = FALSE) +\n  \n  # Label for y - mu (placed at midpoint)\n  geom_text(data = all_vectors, aes(x = (mu_x + y_x)/2, y = (mu_y + y_y)/2, \n                                    label = expression(y - mu), color = Angle),\n            parse = TRUE, size = 3, vjust = 1.5, show.legend = FALSE) +\n\n  # 7. Origin Marker\n  geom_point(aes(x=0, y=0), color=\"black\", size=2) +\n  \n  # Formatting\n  coord_fixed() +\n  theme_minimal() +\n  labs(title = \"Rotations of Normal Cloud\",\n       x = \"x\", y = \"y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 5.1: Illustration of the Mean and Distribution of Quadratic Forms\n\n\n\n\n\n\nTheorem 5.1 (Mean of Quadratic Form) If \\(y\\) is a random vector with mean \\(E(y) = \\mu\\) and covariance matrix \\(\\text{Var}(y) = \\Sigma\\), and \\(A\\) is a symmetric matrix of constants, then:\n\\[\nE(y'Ay) = \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\]\n\n\nProof. We present three methods to derive the expectation of the quadratic form.\nMethod 1: Using the Trace Trick\nUsing the fact that a scalar is equal to its own trace (\\(\\text{tr}(c) = c\\)) and the linearity of expectation: \\[\n\\begin{aligned}\nE(y'Ay) &= E[\\text{tr}(y'Ay)] \\\\\n&= E[\\text{tr}(Ayy')] \\quad \\text{(cyclic property of trace)} \\\\\n&= \\text{tr}(A E[yy']) \\quad \\text{(linearity of expectation)}\n\\end{aligned}\n\\] Recall that the covariance matrix is defined as \\(\\Sigma = E[(y-\\mu)(y-\\mu)'] = E(yy') - \\mu\\mu'\\). Rearranging this gives the second moment: \\(E(yy') = \\Sigma + \\mu\\mu'\\). Substituting this back: \\[\n\\begin{aligned}\nE(y'Ay) &= \\text{tr}(A(\\Sigma + \\mu\\mu')) \\\\\n&= \\text{tr}(A\\Sigma) + \\text{tr}(A\\mu\\mu') \\\\\n&= \\text{tr}(A\\Sigma) + \\text{tr}(\\mu'A\\mu) \\quad \\text{(cyclic property on second term)} \\\\\n&= \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\end{aligned}\n\\]\nMethod 2: Using Scalar Summation\nWe can express the quadratic form in scalar notation using the entries of \\(A=(a_{ij})\\), \\(\\Sigma=(\\sigma_{ij})\\), and \\(\\mu=(\\mu_i)\\): \\[\n\\begin{aligned}\nE(y'Ay) &= E\\left(\\sum_{i=1}^n \\sum_{j=1}^n a_{ij} y_i y_j\\right) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} E(y_i y_j) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} (\\sigma_{ij} + \\mu_i \\mu_j) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} \\sigma_{ji} + \\sum_{i=1}^n \\sum_{j=1}^n \\mu_i a_{ij} \\mu_j \\quad (\\text{since } \\Sigma \\text{ is symmetric, } \\sigma_{ij}=\\sigma_{ji}) \\\\\n&= \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\end{aligned}\n\\]\nMethod 3: Using Spectral Decomposition of A\nSince \\(A\\) is symmetric, we use its spectral decomposition \\(A = \\sum_{i=1}^n \\lambda_i q_i q_i'\\). Substituting this into the quadratic form: \\[\ny'Ay = y' \\left( \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) y = \\sum_{i=1}^n \\lambda_i (q_i' y)^2\n\\] Let \\(w_i = q_i' y\\). This is a scalar random variable which is a linear transformation of \\(y\\). Its properties are:\n\nMean: \\(E(w_i) = q_i' E(y) = q_i' \\mu\\).\nVariance: \\(\\text{Var}(w_i) = \\text{Var}(q_i' y) = q_i' \\text{Var}(y) q_i = q_i' \\Sigma q_i\\).\n\nUsing the relation \\(E(w_i^2) = \\text{Var}(w_i) + [E(w_i)]^2\\), we have: \\[\nE[(q_i' y)^2] = q_i' \\Sigma q_i + (q_i' \\mu)^2\n\\] Summing over all \\(i\\) weighted by \\(\\lambda_i\\): \\[\n\\begin{aligned}\nE(y'Ay) &= \\sum_{i=1}^n \\lambda_i \\left[ q_i' \\Sigma q_i + (q_i' \\mu)^2 \\right] \\\\\n&= \\sum_{i=1}^n \\text{tr}(\\lambda_i q_i' \\Sigma q_i) + \\mu' \\left( \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) \\mu \\\\\n&= \\text{tr}\\left( \\Sigma \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) + \\mu' A \\mu \\\\\n&= \\text{tr}(\\Sigma A) + \\mu' A \\mu\n\\end{aligned}\n\\]\n\n\nRemark (Geometric Interpretation via Sigma). If we further decompose \\(\\Sigma = \\sum_{j=1}^n \\gamma_j v_j v_j'\\) (where \\(\\gamma_j, v_j\\) are eigenvalues/vectors of \\(\\Sigma\\)), the trace term becomes: \\[\n\\text{tr}(A\\Sigma) = \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\gamma_j (q_i' v_j)^2\n\\] Here, \\((q_i' v_j)^2 = \\cos^2(\\theta_{ij})\\) represents the alignment between the axes of the quadratic form (\\(A\\)) and the axes of the data covariance (\\(\\Sigma\\)). The expectation is maximized when the eigenspaces of \\(A\\) and \\(\\Sigma\\) align.\n\n\nCorollary 5.1 (Expectation with Projection Matrix) Consider the special case where:\n\n\\(P\\) is a projection matrix (symmetric and idempotent, \\(P^2=P\\)).\nThe covariance is spherical: \\(\\Sigma = \\sigma^2 I_n\\).\n\nThen the expectation simplifies to: \\[\nE(y'Py) = \\sigma^2 r + ||P\\mu||^2\n\\] where \\(r = \\text{rank}(P) = \\text{tr}(P)\\).\nProof: Using Theorem 5.1 with \\(A=P\\) and \\(\\Sigma=\\sigma^2 I_n\\):\n\nTrace Term: \\(\\text{tr}(P\\Sigma) = \\text{tr}(P(\\sigma^2 I_n)) = \\sigma^2 \\text{tr}(P)\\). Since \\(P\\) is idempotent, its eigenvalues are either 0 or 1, so \\(\\text{tr}(P) = \\text{rank}(P) = r\\).\nMean Term: Since \\(P\\) is symmetric and idempotent (\\(P'P = P^2 = P\\)), we can rewrite the quadratic form: \\[\n\\mu' P \\mu = \\mu' P' P \\mu = (P\\mu)' (P\\mu) = ||P\\mu||^2\n\\]\n\n\n\nExample 5.1 (Expectation of Sum of Squares Decomposition (i.i.d. Case)) Consider a random vector \\(y = (y_1, \\dots, y_n)'\\) with mean vector \\(\\mu_y = \\mu j_n\\) and covariance \\(\\Sigma = \\sigma^2 I_n\\). We analyze the two components of the total sum of squares by projecting \\(y\\) onto the mean space (\\(P_{j_n}\\)) and the residual space (\\(I-P_{j_n}\\)).\n1. The Projection Vectors\nFirst, we write the explicit forms of the projected vectors using \\(P_{j_n} = \\frac{1}{n}j_n j_n'\\):\n\nMean Vector (\\(P_{j_n}y\\)): Projecting \\(y\\) onto the column space of \\(j_n\\) replaces every element with the sample mean \\(\\bar{y}\\). \\[\n  P_{j_n}y = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n  \\]\nResidual Vector (\\((I-P_{j_n})y\\)): Subtracting the mean projection from \\(y\\) yields the deviations. \\[\n  (I-P_{j_n})y = y - \\bar{y}j_n = \\begin{pmatrix} y_1 - \\bar{y} \\\\ y_2 - \\bar{y} \\\\ \\vdots \\\\ y_n - \\bar{y} \\end{pmatrix}\n  \\]\n\n2. Expectations of Squared Norms\nWe now find the expectation of the squared length of these vectors using Corollary 5.1.\nPart A: Sum of Squares for Mean The quadratic form is the squared norm of the projected mean vector: \\[\ny'P_{j_n}y = ||P_{j_n}y||^2 = \\sum_{i=1}^n \\bar{y}^2 = n\\bar{y}^2\n\\] Applying the corollary with \\(P=P_{j_n}\\):\n\nRank: \\(\\text{tr}(P_{j_n}) = 1\\).\nMean: \\(P_{j_n}\\mu_y = P_{j_n}(\\mu j_n) = \\mu j_n\\). The squared norm is \\(n\\mu^2\\).\n\n\\[\nE[||P_{j_n}y||^2] = \\sigma^2(1) + n\\mu^2\n\\]\nPart B: Sum of Squared Errors (SSE) The quadratic form is the squared norm of the residual vector: \\[\ny'(I-P_{j_n})y = ||(I-P_{j_n})y||^2 = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] Applying the corollary with \\(P=I-P_{j_n}\\):\n\nRank: \\(\\text{tr}(I-P_{j_n}) = n - 1\\).\nMean: \\((I-P_{j_n})\\mu_y = \\mu_y - P_{j_n}\\mu_y = \\mu j_n - \\mu j_n = 0\\). The squared norm is \\(0\\).\n\n\\[\nE[||(I-P_{j_n})y||^2] = \\sigma^2(n-1) + 0\n\\]\nConclusion These results confirm the standard properties: \\(E(\\bar{y}^2) = \\frac{\\sigma^2}{n} + \\mu^2\\) and \\(E(S^2) = \\sigma^2\\).\n\n\nExample 5.2 (Expectation of Total Sum of Squares (Regression Case)) Consider now a regression setting where the mean of \\(y\\) depends on covariates (e.g., \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\)). The mean vector \\(\\mu_y\\) is not proportional to \\(j_n\\). We are interested in the expectation of the Total Sum of Squares (SST).\n1. Identification The SST measures the variation of \\(y\\) around the global sample mean \\(\\bar{y}\\), ignoring the covariates: \\[\n\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2 = y'(I - P_{j_n})y\n\\] This is the same quadratic form as Part B in the previous example, but the underlying mean \\(\\mu_y\\) has changed.\n2. Calculation We apply Corollary 5.1 with \\(P = I - P_{j_n}\\) and general \\(\\mu_y\\):\n\nRank Term: Same as before, \\(\\text{tr}(I - P_{j_n}) = n - 1\\).\nMean Term: The projection of the mean vector is no longer zero. \\[\n  (I - P_{j_n})\\mu_y = \\mu_y - \\bar{\\mu}j_n = \\begin{pmatrix} \\mu_1 - \\bar{\\mu} \\\\ \\vdots \\\\ \\mu_n - \\bar{\\mu} \\end{pmatrix}\n  \\] where \\(\\bar{\\mu} = \\frac{1}{n}\\sum \\mu_i\\) is the average of the true means. The squared norm is the sum of squared deviations of the true means: \\[\n  ||(I - P_{j_n})\\mu_y||^2 = \\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2\n  \\]\n\nConclusion \\[\nE(\\text{SST}) = (n-1)\\sigma^2 + \\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2\n\\] This shows that in regression, the SST estimates \\((n-1)\\sigma^2\\) plus the variability introduced by the regression signal (the spread of the true means \\(\\mu_i\\)).\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\nset.seed(123)\nn &lt;- 20\nsigma &lt;- 1\n\n# --- Data Generation ---\n\n# Case 1: I.i.d. Case (common Mean)\nmu_iid &lt;- rep(3, n)\ny_iid &lt;- mu_iid + rnorm(n, 0, sigma)\n\n# Case 2: Regression Case (sorted Mean)\n# Mu_i Is Sampled from N(3, Sd=3) and Sorted\nmu_reg &lt;- sort(rnorm(n, 3, 3)) \ny_reg &lt;- mu_reg + rnorm(n, 0, sigma)\n\ndf_iid &lt;- data.frame(\n  id = 1:n,\n  y = y_iid,\n  mu = mu_iid,\n  y_bar = mean(y_iid),\n  type = \"i.i.d. Case (Common Mean)\"\n)\n\ndf_reg &lt;- data.frame(\n  id = 1:n,\n  y = y_reg,\n  mu = mu_reg,\n  y_bar = mean(y_reg),\n  type = \"Regression Case (Sorted Mean)\"\n)\n\n# Determine Common Y Limits for Comparison Across Both Plots\ny_min &lt;- min(c(df_iid$y, df_reg$y, df_iid$mu, df_reg$mu)) - 1\ny_max &lt;- max(c(df_iid$y, df_reg$y, df_iid$mu, df_reg$mu)) + 1\ny_lims &lt;- c(y_min, y_max)\n\n# --- Plotting Function ---\n\nplot_func &lt;- function(df, title, ylims) {\n  ggplot(df, aes(x = id)) +\n    # Vertical lines for the deviations (y_i - y_bar)\n    geom_segment(aes(xend = id, y = y_bar, yend = y), \n                 color = \"gray50\", linetype = \"solid\", alpha = 0.6) +\n    # True means mu_i (red X)\n    geom_point(aes(y = mu, shape = \"True Mean (μ_i)\"), color = \"red\", size = 3) +\n    # Observations y_i (black dots)\n    geom_point(aes(y = y, shape = \"Observed (y_i)\"), color = \"black\", size = 2) +\n    # Global Sample Mean line (y_bar)\n    geom_hline(aes(yintercept = y_bar, linetype = \"Sample Mean (ȳ)\"), \n               color = \"blue\", linewidth = 0.8) +\n    scale_shape_manual(name = \"\", values = c(\"True Mean (μ_i)\" = 4, \"Observed (y_i)\" = 16)) +\n    scale_linetype_manual(name = \"\", values = c(\"Sample Mean (ȳ)\" = \"dashed\")) +\n    scale_y_continuous(limits = ylims) +\n    labs(title = title, x = \"Observation Index (Sorted by μ_i)\", y = \"Value\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n}\n\n# --- Combine Plots ---\n\np1 &lt;- plot_func(df_iid, \"i.i.d. Case: E(SST) = (n-1)σ²\", y_lims)\np2 &lt;- plot_func(df_reg, \"Regression Case: E(SST) = (n-1)σ² + Σ(μ_i - μ̄)²\", y_lims)\n\np1 + p2 + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 5.2: Comparison of SST components with increased variation in the true means. The vertical lines represent the deviations \\((y_i - \\bar{y})\\). With \\(\\text{sd}(\\mu_i) = 3\\), the regression case (right) shows significantly larger deviations, illustrating how the systematic spread of the means dominates the Total Sum of Squares.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#non-central-chi2-distribution",
    "href": "lec4-qf.html#non-central-chi2-distribution",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.3 Non-central \\(\\chi^2\\) Distribution",
    "text": "5.3 Non-central \\(\\chi^2\\) Distribution\nTo understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.\n\nDefinition 5.2 (Non-central \\(\\chi^2\\) Distribution) Let \\(y \\sim N_n(\\mu, I_n)\\). The random variable \\(V = y'y = \\sum y_i^2\\) follows a non-central chi-square distribution with \\(n\\) degrees of freedom and non-centrality parameter \\(\\lambda\\).\n\\[\nV \\sim \\chi^2(n, \\lambda) \\quad \\text{where } \\lambda = \\mu'\\mu = ||\\mu||^2\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nNote on NCP Definition: Some definitions of non-central \\(\\chi^2\\) use \\(\\lambda = \\frac{1}{2}\\mu'\\mu\\). In this course, we use \\(\\lambda = \\mu'\\mu\\). With this convention, the Poisson-mixture representation below uses Poisson(\\(\\lambda/2\\)) weights.\n\n\n\n5.3.1 Visualizing \\(\\chi^2\\) Distributions\nHere is a plot visualizing the difference between central and non-central Chi-square distributions.\n\n\n\n\n\n\n\n\nFigure 5.3: Central vs Non-central Chi-square Distribution\n\n\n\n\n\nThe density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter \\(\\lambda\\) increases.\n\n\n5.3.2 Mean, Variance, and MGF\nWe summarize the key properties of the non-central chi-square distribution.\n\nTheorem 5.2 (Properties of Non-central Chi-square) Let \\(V \\sim \\chi^2(n, \\lambda)\\). Then:\n\nMean: \\(E(V) = n + \\lambda\\)\nVariance: \\(\\text{Var}(V) = 2n + 4\\lambda\\)\nMoment Generating Function (MGF): \\[\nm_V(t) = \\frac{\\exp\\left[ -\\frac{\\lambda}{2} \\left\\{1 - \\frac{1}{1-2t}\\right\\} \\right]}{(1-2t)^{n/2}} \\quad \\text{for } t &lt; 1/2\n\\]\n\n\n\nProof (Mean). By definition, \\(V \\sim \\chi^2(n, \\lambda)\\) is the distribution of \\(y'y\\) where \\(y \\sim N_n(\\mu, I_n)\\) and the non-centrality parameter is \\(\\lambda = \\mu'\\mu = ||\\mu||^2\\). Applying Lemma 5.1 to the random vector \\(y\\): \\[\nE(V) = E(y'y) = n + \\mu'\\mu = n + \\lambda\n\\]\n\n\nProof (MGF). Since the components \\(y_i\\) of the vector \\(y\\) are independent \\(N(\\mu_i, 1)\\), and \\(V = \\sum_{i=1}^n y_i^2\\), the MGF of \\(V\\) is the product of the MGFs of each \\(y_i^2\\): \\[\nm_V(t) = E[e^{t \\sum y_i^2}] = \\prod_{i=1}^n E[e^{t y_i^2}]\n\\] Consider a single component \\(y_i \\sim N(\\mu_i, 1)\\). Its squared expectation is: \\[\n\\begin{aligned}\nE[e^{t y_i^2}] &= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ty^2} e^{-\\frac{1}{2}(y-\\mu_i)^2} dy \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left\\{ -\\frac{1}{2} \\left[ (1-2t)y^2 - 2\\mu_i y + \\mu_i^2 \\right] \\right\\} dy\n\\end{aligned}\n\\] Completing the square in the exponent for \\(y\\) (assuming \\(t &lt; 1/2\\)): \\[\n(1-2t)y^2 - 2\\mu_i y + \\mu_i^2 = (1-2t)\\left(y - \\frac{\\mu_i}{1-2t}\\right)^2 + \\mu_i^2 - \\frac{\\mu_i^2}{1-2t}\n\\] The integral of the Gaussian kernel \\(\\exp\\{ -\\frac{1}{2}(1-2t)(y - \\dots)^2 \\}\\) yields \\(\\sqrt{\\frac{2\\pi}{1-2t}}\\). The remaining constant term is: \\[\n\\exp\\left\\{ -\\frac{1}{2} \\left( \\mu_i^2 - \\frac{\\mu_i^2}{1-2t} \\right) \\right\\} = \\exp\\left\\{ \\frac{\\mu_i^2}{2} \\left( \\frac{1}{1-2t} - 1 \\right) \\right\\} = \\exp\\left\\{ \\frac{\\mu_i^2 t}{1-2t} \\right\\}\n\\] Thus, for a single component: \\[\nm_{y_i^2}(t) = (1-2t)^{-1/2} \\exp\\left( \\frac{\\mu_i^2 t}{1-2t} \\right)\n\\] Multiplying the MGFs for all \\(n\\) components: \\[\n\\begin{aligned}\nm_V(t) &= \\prod_{i=1}^n (1-2t)^{-1/2} \\exp\\left( \\frac{\\mu_i^2 t}{1-2t} \\right) \\\\\n&= (1-2t)^{-n/2} \\exp\\left( \\frac{t \\sum \\mu_i^2}{1-2t} \\right)\n\\end{aligned}\n\\] Substituting \\(\\lambda = \\sum \\mu_i^2\\) (so \\(\\sum \\mu_i^2 = \\lambda\\)): \\[\nm_V(t) = (1-2t)^{-n/2} \\exp\\left( \\frac{\\lambda t}{1-2t} \\right)\n\\] Note that \\(\\displaystyle \\frac{\\lambda t}{1-2t} = -\\frac{\\lambda}{2}\\left(1 - \\frac{1}{1-2t}\\right)\\), which leads to the Poisson-mixture representation with \\(J \\sim \\text{Poisson}(\\lambda/2)\\).\n\n\nProof (Variance). We use the Cumulant Generating Function, \\(K_V(t) = \\ln m_V(t)\\), as its derivatives yield the mean and variance directly: \\[\nK_V(t) = -\\frac{n}{2} \\ln(1-2t) + \\frac{\\lambda t}{1-2t}\n\\] First derivative (Mean): \\[\n\\begin{aligned}\nK'_V(t) &= -\\frac{n}{2} \\left(\\frac{-2}{1-2t}\\right) + \\lambda \\left[ \\frac{1(1-2t) - t(-2)}{(1-2t)^2} \\right] \\\\\n&= \\frac{n}{1-2t} + \\frac{\\lambda}{(1-2t)^2}\n\\end{aligned}\n\\] Second derivative (Variance): \\[\n\\begin{aligned}\nK''_V(t) &= n(-1)(1-2t)^{-2}(-2) + \\lambda(-2)(1-2t)^{-3}(-2) \\\\\n&= \\frac{2n}{(1-2t)^2} + \\frac{4\\lambda}{(1-2t)^3}\n\\end{aligned}\n\\] Evaluating at \\(t=0\\): \\[\n\\text{Var}(V) = K''_V(0) = 2n + 4\\lambda\n\\]\n\n\n\n5.3.3 Additivity\n\nTheorem 5.3 (Additivity of Chi-square) If \\(v_1, \\dots, v_k\\) are independent random variables distributed as \\(\\chi^2(n_i, \\lambda_i)\\), then their sum follows a chi-square distribution:\n\\[\n\\sum_{i=1}^k v_i \\sim \\chi^2\\left(\\sum_{i=1}^k n_i, \\sum_{i=1}^k \\lambda_i\\right)\n\\]\n\n\nProof. Method 1: Using MGFs\nThe moment generating function of \\(v_i \\sim \\chi^2(n_i, \\lambda_i)\\) is: \\[\nM_{v_i}(t) = \\frac{\\exp\\left[-\\frac{\\lambda_i}{2} \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}}\n\\]\nSince \\(v_1, \\dots, v_k\\) are independent, the MGF of their sum \\(V = \\sum v_i\\) is the product of their individual MGFs:\n\\[\n\\begin{aligned}\nM_V(t) &= \\prod_{i=1}^k M_{v_i}(t) \\\\\n&= \\prod_{i=1}^k \\frac{\\exp\\left[-\\frac{\\lambda_i}{2} \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}} \\\\\n&= \\frac{\\exp\\left[-\\frac{\\sum \\lambda_i}{2} \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{\\sum n_i/2}}\n\\end{aligned}\n\\]\nThis is the MGF of a non-central chi-square distribution with degrees of freedom \\(\\sum n_i\\) and non-centrality parameter \\(\\sum \\lambda_i\\).\nMethod 2: Geometric Interpretation\nLet \\(v_i = ||y_i||^2\\) where \\(y_i \\sim N_{n_i}(\\mu_i, I_{n_i})\\). Since the vectors \\(y_i\\) are independent, we can stack them into a larger vector \\(y = (y_1', \\dots, y_k')'\\).\n\\[\ny \\sim N_{\\sum n_i}(\\mu, I_{\\sum n_i}) \\quad \\text{where } \\mu = (\\mu_1', \\dots, \\mu_k')'\n\\]\nThe sum of squares is: \\[\n\\sum v_i = \\sum ||y_i||^2 = ||y||^2\n\\]\nBy definition, \\(||y||^2\\) follows a non-central chi-square distribution with degrees of freedom equal to the dimension of \\(y\\) (\\(\\sum n_i\\)) and non-centrality parameter \\(\\lambda = ||\\mu||^2\\).\n\\[\n\\lambda = \\sum_{i=1}^k ||\\mu_i||^2 = \\sum_{i=1}^k \\lambda_i\n\\]\n\n\n\n5.3.4 Poisson Mixture Representation\n\nTheorem 5.4 (Poisson Mixture Representation) Let \\(v \\sim \\chi^2(n, \\lambda)\\) be a non-central chi-square random variable. Its probability density function can be represented as a Poisson-weighted sum of central chi-square density functions:\n\\[\nf(v; n, \\lambda) = \\sum_{j=0}^{\\infty} \\left( \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!} \\right) f(v; n+2j, 0)\n\\]\nwhere \\(f(v; \\nu, 0)\\) is the density of a central chi-square distribution with \\(\\nu\\) degrees of freedom.\n\n\nProof. We use the Moment Generating Function (MGF) approach. The MGF of a non-central chi-square distribution \\(v \\sim \\chi^2(n, \\lambda)\\) is: \\[\nM_v(t) = (1-2t)^{-n/2} \\exp\\left( \\frac{\\lambda}{2} \\left[ \\frac{1}{1-2t} - 1 \\right] \\right)\n\\]\nWe can expand the exponential term using the power series \\(e^x = \\sum_{j=0}^\\infty \\frac{x^j}{j!}\\): \\[\n\\begin{aligned}\nM_v(t) &= (1-2t)^{-n/2} e^{-\\lambda/2} \\exp\\left( \\frac{\\lambda/2}{1-2t} \\right) \\\\\n&= e^{-\\lambda/2} (1-2t)^{-n/2} \\sum_{j=0}^{\\infty} \\frac{1}{j!} \\left( \\frac{\\lambda/2}{1-2t} \\right)^j \\\\\n&= \\sum_{j=0}^{\\infty} \\left( \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!} \\right) (1-2t)^{-(n+2j)/2}\n\\end{aligned}\n\\]\nRecognizing the terms:\n\nThe term in parentheses, \\(P(J=j) = \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!}\\), is the probability mass function of a Poisson random variable \\(J \\sim \\text{Poisson}(\\lambda/2)\\).\nThe term \\((1-2t)^{-(n+2j)/2}\\) is the MGF of a central chi-square distribution with \\(n+2j\\) degrees of freedom.\n\nSince the MGF of the mixture is the sum of the MGFs of the components weighted by the mixture probabilities, the density must follow the same mixture structure.\n\n\nRemark. This theorem implies a hierarchical model for generating a non-central chi-square variable:\n\nSample \\(J \\sim \\text{Poisson}(\\lambda/2)\\).\nGiven \\(J=j\\), sample \\(V \\sim \\chi^2(n+2j, 0)\\).\n\nThis is particularly useful for numerical computation, as it allows the non-central CDF to be approximated by a finite sum of central chi-square CDFs.\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Parameters\nn &lt;- 4          # Base degrees of freedom\nlambda &lt;- 4     # Non-centrality parameter (lambda = ||mu||^2)\nx_limit &lt;- 25   # X-axis range\nj_values &lt;- 0:15 # Sequence of J = 0, 1, 2...\n\n# Generate Data for the Mixture Components\nx &lt;- seq(0, x_limit, length.out = 400)\nmixture_df &lt;- do.call(rbind, lapply(j_values, function(j) {\n  weight &lt;- dpois(j, lambda/2)\n  data.frame(\n    x = x,\n    y = dchisq(x, df = n + 2*j),\n    j = j,\n    weight = weight\n  )\n}))\n\n# Generate Data for the True Non-central Chi-square\n# R Uses Ncp = ||mu||^2 (we set lambda = ||mu||^2)\ntrue_nc &lt;- data.frame(\n  x = x,\n  y = dchisq(x, df = n, ncp = lambda)\n) \n\n# Plotting\nggplot() +\n  # Draw weighted central chi-square curves (the \"cloud\")\n  geom_line(data = mixture_df, \n            aes(x = x, y = y, group = j, alpha = weight), \n            color = \"black\", \n            linewidth = 0.8) +\n  # Draw the true non-central chi-square density\n  geom_line(data = true_nc, aes(x = x, y = y), \n            color = \"blue\", \n            linewidth = 1.3) +\n  # Aesthetics\n  scale_alpha_continuous(range = c(0.01, 0.8), guide = \"none\") +\n  labs(\n    title = \"Poisson Mixture Representation of Non-central Chi-square\",\n    subtitle = paste0(\"n = \", n, \", λ = \", lambda, \" (Blue line = True Non-central)\"),\n    x = \"Value (v)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 5.4: The non-central chi-square distribution as a Poisson mixture. The black curves represent central chi-square densities with \\(df = n + 2j\\), with transparency (alpha) proportional to the Poisson weight \\(P(J=j)\\). The solid blue line is the true non-central chi-square density.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#distribution-of-quadratic-forms",
    "href": "lec4-qf.html#distribution-of-quadratic-forms",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.4 Distribution of Quadratic Forms",
    "text": "5.4 Distribution of Quadratic Forms\n\n5.4.1 MGF of Quadratic Forms\nTo determine the distribution of general quadratic forms \\(y'Ay\\), we look at their MGF.\n\nTheorem 5.5 (MGF of Quadratic Form) If \\(y \\sim N_p(\\mu, \\Sigma)\\), then the MGF of \\(Q = y'Ay\\) is:\n\\[\nM_Q(t) = |I - 2tA\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2} \\mu' [I - (I - 2tA\\Sigma)^{-1}] \\Sigma^{-1} \\mu\\right)\n\\]\n\n\n\n5.4.2 Distribution of the Sum Squares of Projected Spherical Normal\nWe will prove a simplified version of Theorem 5.7 first.\n\nTheorem 5.6 (Distribution of Projected Spherical Normal) If \\(y \\sim N_n(\\mu, \\sigma^2 I_n)\\) and \\(P_V\\) is a projection matrix onto a subspace \\(V\\) of dimension \\(r\\), then:\n\\[\n\\frac{1}{\\sigma^2} y'P_V y = \\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{\\sigma^2}\\right)\n\\]\nThis holds because \\(\\frac{1}{\\sigma^2} P_V (\\sigma^2 I) = P_V\\), which is idempotent.\n\n\n\n\n\n\n\nCrucial Theorem\n\n\n\nThis is one of the most important theorems in the course, establishing the fundamental conditions under which a quadratic form follows a chi-square distribution.\n\n\n\nProof. When \\(\\sigma^2=1\\)\nLet \\(P_V\\) be the projection matrix. We know \\(P_V = QQ'\\) where \\(Q = (q_1, \\dots, q_r)\\) is an \\(n \\times r\\) matrix with orthonormal columns (\\(Q'Q = I_r\\)).\nThe projection of vector \\(y\\) onto the subspace \\(V\\) can be expressed using the orthonormal basis vectors: \\[\nP_V y = Q Q' y = (q_1, \\dots, q_r) \\begin{pmatrix} q_1' y \\\\ \\vdots \\\\ q_r' y \\end{pmatrix} = \\sum_{i=1}^r (q_i' y) q_i\n\\]\nThe squared norm of the projection is: \\[\ny' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2\n\\]\nSince \\(y \\sim N(\\mu, I_n)\\), the linear transformation \\(w = Q'y\\) follows: \\[\nw \\sim N(Q'\\mu, Q' I_n Q) = N(Q'\\mu, I_r)\n\\]\nThus, \\(w\\) is a vector of \\(r\\) independent normal variables with variance 1. The sum of squares \\(||w||^2\\) is by definition non-central chi-square:\n\\[\n||w||^2 \\sim \\chi^2(r, \\lambda)\n\\] where the non-centrality parameter is: \\[\n\\lambda = ||E(w)||^2 = ||Q'\\mu||^2\n\\]\nNote that \\(||Q'\\mu||^2 = \\mu' Q Q' \\mu = \\mu' P_V \\mu = ||P_V \\mu||^2\\).\nThus, \\(y' P_V y \\sim \\chi^2(r, ||P_V \\mu||^2)\\).\nWhen \\(\\sigma^2\\not=1\\)\nIf \\(y \\sim N(\\mu, \\sigma^2 I_n)\\), we standardize by dividing by \\(\\sigma\\).\nLet \\(w = y/\\sigma\\). Then \\(w \\sim N(\\mu/\\sigma, I_n)\\). Applying the previous result to \\(w\\):\n\\[\nw' P_V w = \\frac{y' P_V y}{\\sigma^2} \\sim \\chi^2\\left(r, \\left|\\left| P_V \\frac{\\mu}{\\sigma} \\right|\\right|^2\\right)\n\\] which simplifies to: \\[\n\\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{\\sigma^2}\\right)\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nThe term \\(\\|P_V y\\|^2\\) itself is not a standard chi-square variable; it is a scaled chi-square variable. Its mean is:\n\\[\nE(\\|P_V y\\|^2) = \\sigma^2 \\left(r + \\frac{\\|P_V \\mu\\|^2}{\\sigma^2}\\right) = r\\sigma^2 + \\|P_V \\mu\\|^2\n\\]\n\n\n\n\nCode\nlibrary(plotly)\nlibrary(MASS)\n\n# 1. Generate Data\nset.seed(123)\nn &lt;- 200\nmu &lt;- c(2, 3, 5) \nsigma &lt;- diag(3)\ndata &lt;- mvrnorm(n, mu, sigma)\ndf &lt;- as.data.frame(data)\ncolnames(df) &lt;- c(\"x\", \"y\", \"z\")\n\n# 2. Project Points\nproj_points &lt;- t(apply(data, 1, function(p) {\n  sum(p * c(1,0,0)) * c(1,0,0) + sum(p * c(0,1,0)) * c(0,1,0)\n}))\ndf_proj &lt;- as.data.frame(proj_points)\ncolnames(df_proj) &lt;- c(\"px\", \"py\", \"pz\")\n\n# 3. Setup Axis Styles\nax_style &lt;- list(\n  title = \"\",\n  showgrid = TRUE,        \n  gridcolor = \"gray\",\n  gridwidth = 0.5,\n  zeroline = FALSE,       \n  showline = FALSE,       \n  showticklabels = FALSE,\n  showbackground = FALSE,\n  showspikes = FALSE\n)\n\n# 4. Create the Plot with EXPLICIT DIMENSIONS\nplot_ly(\n    # --- FIX IS HERE: Force the pixel dimensions ---\n    width = 800, \n    height = 450 \n  ) %&gt;%\n  # --- Optional: Floor Plane ---\n  add_trace(\n    x = c(-2, 8, 8, -2), y = c(-2, -2, 8, 8), z = c(0, 0, 0, 0),\n    type = \"mesh3d\", opacity = 0.05, color = 'gray', \n    hoverinfo = \"none\", showlegend = FALSE\n  ) %&gt;%\n  # --- Original Data (y) ---\n  add_trace(\n    data = df, x = ~x, y = ~y, z = ~z,\n    type = 'scatter3d', mode = 'markers',\n    marker = list(size = 3, color = 'blue', opacity = 0.6),\n    name = '&lt;i&gt;y&lt;/i&gt;' \n  ) %&gt;%\n  # --- Projected Shadow (P_V y) ---\n  add_trace(\n    data = df_proj, x = ~px, y = ~py, z = ~pz,\n    type = 'scatter3d', mode = 'markers',\n    marker = list(size = 3, color = 'red', opacity = 0.8),\n    name = \"&lt;i&gt;P&lt;/i&gt;&lt;sub&gt;V&lt;/sub&gt;&lt;i&gt;y&lt;/i&gt;\"\n  ) %&gt;%\n  # --- Residual Lines ---\n  add_segments(\n    x = df$x, xend = df_proj$px,\n    y = df$y, yend = df_proj$py,\n    z = df$z, zend = df_proj$pz,\n    line = list(color = 'gray', width = 1),\n    showlegend = FALSE, hoverinfo = \"none\"\n  ) %&gt;%\n  # --- Manual Labels ---\n  add_text(\n    x = c(8.5, 0, 0), y = c(0, 8.5, 0), z = c(0, 0, 8.5),\n    text = c(\"&lt;i&gt;q&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;\", \"&lt;i&gt;q&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;\", \"&lt;i&gt;V&lt;/i&gt;&lt;sup&gt;\\u22A5&lt;/sup&gt;\"),\n    textfont = list(size = 15, color = \"black\"),\n    showlegend = FALSE\n  ) %&gt;%\n  layout(\n    scene = list(\n      xaxis = ax_style,\n      yaxis = ax_style,\n      zaxis = ax_style,\n      aspectmode = \"cube\",\n      camera = list(eye = list(x = 1.6, y = 1.6, z = 1.3))\n    ),\n    title = \"Projection of Trivariate Normal onto 2D Subspace\",\n    margin = list(l=0, r=0, b=0, t=30),\n    # Ensure layout autosize is off so it respects the width/height above\n    autosize = FALSE \n  )\n\n\n\n\nVisualization of Projected Trivariate Normal Cloud\n\n\n\n\n5.4.3 Distribution of General Quadratic Forms\n\nLemma 5.2 (Idempotent Matrix Property) Let \\(\\Sigma\\) be a positive definite matrix such that \\(\\Sigma = \\Sigma^{1/2}\\Sigma^{1/2}\\). The matrix \\(A\\Sigma\\) is idempotent if and only if \\(\\Sigma^{1/2}A\\Sigma^{1/2}\\) is idempotent.\n\n\nProof. \\((\\Rightarrow)\\) Assume \\(A\\Sigma\\) is idempotent, so \\(A\\Sigma A\\Sigma = A\\Sigma\\). Then: \\[\n\\begin{aligned}\n(\\Sigma^{1/2}A\\Sigma^{1/2})^2 &= \\Sigma^{1/2}A(\\Sigma^{1/2}\\Sigma^{1/2})A\\Sigma^{1/2} \\\\\n&= \\Sigma^{1/2}(A\\Sigma A)\\Sigma^{1/2}\n\\end{aligned}\n\\] From the assumption \\(A\\Sigma A\\Sigma = A\\Sigma\\), post-multiplying by \\(\\Sigma^{-1}\\) gives \\(A\\Sigma A = A\\). Substituting this back: \\[\n\\Sigma^{1/2}(A)\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma^{1/2}\n\\]\n\\((\\Leftarrow)\\) Assume \\(\\Sigma^{1/2}A\\Sigma^{1/2}\\) is idempotent. Then: \\[\n(\\Sigma^{1/2}A\\Sigma^{1/2})(\\Sigma^{1/2}A\\Sigma^{1/2}) = \\Sigma^{1/2}A\\Sigma^{1/2}\n\\] Expanding the left side: \\[\n\\Sigma^{1/2}A(\\Sigma^{1/2}\\Sigma^{1/2})A\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2}\n\\] Equating this to the right side: \\[\n\\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma^{1/2}\n\\] Pre-multiply by \\(\\Sigma^{-1/2}\\) and post-multiply by \\(\\Sigma^{1/2}\\) (which exist since \\(\\Sigma\\) is positive definite): \\[\n\\begin{aligned}\n\\Sigma^{-1/2}(\\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2})\\Sigma^{1/2} &= \\Sigma^{-1/2}(\\Sigma^{1/2}A\\Sigma^{1/2})\\Sigma^{1/2} \\\\\nI(A\\Sigma A)\\Sigma &= I(A)\\Sigma \\\\\nA\\Sigma A\\Sigma &= A\\Sigma\n\\end{aligned}\n\\]\n\n\nLemma 5.3 (Rank Invariance) Under the conditions of Lemma 5.2, if \\(A\\Sigma\\) is idempotent, then: \\[\n\\text{rank}(A\\Sigma) = \\text{rank}(\\Sigma^{1/2}A\\Sigma^{1/2}) = \\text{tr}(A\\Sigma)\n\\]\n\n\nProof. Since \\(A\\Sigma\\) and \\(\\Sigma^{1/2}A\\Sigma^{1/2}\\) are both idempotent (by Lemma 5.2), their ranks are equal to their traces.\nUsing the cyclic property of the trace operator (\\(\\text{tr}(XYZ) = \\text{tr}(ZXY)\\)): \\[\n\\begin{aligned}\n\\text{rank}(A\\Sigma) &= \\text{tr}(A\\Sigma) \\\\\n&= \\text{tr}(A \\Sigma^{1/2} \\Sigma^{1/2}) \\\\\n&= \\text{tr}(\\Sigma^{1/2} A \\Sigma^{1/2}) \\\\\n&= \\text{rank}(\\Sigma^{1/2}A\\Sigma^{1/2})\n\\end{aligned}\n\\] Alternatively, notice that \\(A\\Sigma\\) is similar to \\(\\Sigma^{1/2}A\\Sigma^{1/2}\\): \\[\nA\\Sigma = \\Sigma^{-1/2} (\\Sigma^{1/2}A\\Sigma^{1/2}) \\Sigma^{1/2}\n\\] Since similar matrices have the same rank, the equality holds.\n\n\nTheorem 5.7 (Distribution of y’Ay) Let \\(y \\sim N_p(\\mu, \\Sigma)\\). Let \\(A\\) be a symmetric matrix of rank \\(r\\). Then \\(y'Ay \\sim \\chi^2(r, \\lambda)\\) with \\(\\lambda = \\mu' A \\mu\\) if and only if \\(A\\Sigma\\) is idempotent (\\(A\\Sigma A\\Sigma = A\\Sigma\\)).\nSpecial Case (\\(\\Sigma = I\\)): If \\(\\Sigma = I\\), the condition simplifies to \\(A\\) being idempotent (\\(A^2 = A\\)).\n\n\nProof. Let \\(y^* = \\Sigma^{-1/2}y\\), so \\(y^* \\sim N_n(\\Sigma^{-1/2}\\mu, I_n)\\). We rewrite the quadratic form: \\[y'Ay = y' \\Sigma^{-1/2} (\\Sigma^{1/2} A \\Sigma^{1/2}) \\Sigma^{-1/2} y = (y^*)' P_V y^* = \\|P_V y^*\\|^2\\] Since \\(A\\Sigma\\) is idempotent, \\(P_V = \\Sigma^{1/2} A \\Sigma^{1/2}\\) is a projection matrix with rank \\(r\\). By the definition of the non-central chi-square, \\(y'Ay \\sim \\chi^2(r, \\|P_V \\Sigma^{-1/2}\\mu\\|^2)\\). The non-centrality parameter simplifies to \\(\\lambda = \\mu'A\\mu\\).\n\n\n\n5.4.4 Standardized Distance Distribution\n\nCorollary 5.2 (Standardized Distance Distribution) Suppose \\(y \\sim N_n(\\mu, \\Sigma)\\). Then the quadratic form representing the standardized distance from a constant vector \\(\\mu_0\\) follows a non-central chi-square distribution: \\[(y-\\mu_0)'\\Sigma^{-1}(y-\\mu_0) \\sim \\chi^2(n, \\lambda = (\\mu-\\mu_0)'\\Sigma^{-1}(\\mu-\\mu_0))\\]\n\n\nProof. Let \\(A = \\Sigma^{-1}\\). Then \\(A\\Sigma = \\Sigma^{-1}\\Sigma = I_n\\), which is clearly idempotent. Alternatively, let \\(w = \\Sigma^{-1/2}(y-\\mu_0)\\), then \\(w \\sim N_n(\\Sigma^{-1/2}(\\mu-\\mu_0), I_n)\\). By the definition of chi-square, \\(\\|w\\|^2 = (y-\\mu_0)'\\Sigma^{-1}(y-\\mu_0)\\) follows the stated distribution.\n\n\n\n\n\n\n\nCrucial Theorem\n\n\n\nThis is an important theorem we will use later.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#distributions-of-projections-of-spherical-normal",
    "href": "lec4-qf.html#distributions-of-projections-of-spherical-normal",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.5 Distributions of Projections of Spherical Normal",
    "text": "5.5 Distributions of Projections of Spherical Normal\n\nTheorem 5.8 (Distribution of Projections) Let \\(V\\) be a \\(k\\)-dimensional subspace of \\(\\mathcal{R}^n\\) with projection matrix \\(P_V\\), and let \\(y\\) be a random vector in \\(\\mathcal{R}^n\\) with mean \\(E(y)=\\mu\\). Then:\n\n\\(E(P_V y) = P_V \\mu\\).\nIf \\(\\text{Var}(y)=\\sigma^2 I_n\\), then \\(\\text{Var}(P_V y) = \\sigma^2 P_V\\) and \\(E(\\|P_V y\\|^2) = \\sigma^2 k + \\|P_V \\mu\\|^2\\).\nIf \\(y \\sim N_n(\\mu, \\sigma^2 I_n)\\), then \\(\\frac{1}{\\sigma^2}\\|P_V y\\|^2 = \\frac{1}{\\sigma^2}y'P_Vy \\sim \\chi^2(k, \\frac{1}{\\sigma^2}\\|P_V \\mu\\|^2)\\).\n\n\n\nProof. \n\nSince the projection operation is linear, \\(E(P_V y) = P_V E(y) = P_V \\mu\\).\n\\(\\text{Var}(P_V y) = P_V \\text{Var}(y) P_V^T = P_V \\sigma^2 I_n P_V = \\sigma^2 P_V\\). The expectation of the squared norm follows from the mean of a quadratic form: \\(E(y'P_Vy) = \\text{tr}(P_V \\sigma^2 I) + \\mu'P_V\\mu = \\sigma^2 k + \\|P_V \\mu\\|^2\\).\nThis is a special case of the general quadratic distribution theorem where \\(A = \\frac{1}{\\sigma^2} P_V\\) and \\(A(\\sigma^2 I) = P_V\\), which is idempotent.\n\n\n\nTheorem 5.9 (Orthogonal Projections) Let \\(V_1, \\dots, V_k\\) be mutually orthogonal subspaces with dimensions d_i and projection matrices \\(P_i\\). If \\(y \\sim N_n(\\mu, \\sigma^2 I_n)\\), then:\n\nThe projections \\(\\hat{y}_i = P_i y\\) are independent with \\(\\hat{y}_i \\sim N(P_i \\mu, \\sigma^2 P_i)\\).\nThe squared norms \\(\\|\\hat{y}_i\\|^2\\) are mutually independent.\n\\(\\frac{1}{\\sigma^2}\\|\\hat{y}_i\\|^2 \\sim \\chi^2(d_i, \\frac{1}{\\sigma^2}\\|P_i \\mu\\|^2)\\).\n\n\n\nProof. \n\nFor \\(i \\ne j\\), \\(\\text{Cov}(P_i y, P_j y) = \\sigma^2 P_i P_j = 0\\) because orthogonal projection matrices satisfy \\(P_i P_j = 0\\). Under normality, zero covariance implies independence.\nSince \\(\\hat{y}_i\\) are independent, any measurable functions of them, such as their squared norms, are also independent.\nThis follows directly from applying the projection distribution theorem to each independent subspace.\n\n\n\n5.5.1 Independence of Forms\n\nTheorem 5.10 (Independence Conditions) Suppose \\(y \\sim N_n(\\mu, \\Sigma)\\).\n\nLinear and Quadratic: \\(By\\) and \\(y'Ay\\) (where \\(A\\) is symmetric) are independent if and only if \\(B\\Sigma A = 0\\).\nQuadratic and Quadratic: \\(y'Ay\\) and \\(y'By\\) (where \\(A, B\\) are symmetric) are independent if and only if \\(A\\Sigma B = 0\\).\n\n\n\nProof. If \\(B\\Sigma A = 0\\), the normal vectors \\(By\\) and \\(Ay\\) have zero covariance and are independent. Because \\(By\\) is independent of \\(Ay\\), it is also independent of any measurable function of \\(Ay\\), specifically \\(y'Ay = \\|Ay\\|^2\\) (if \\(A\\) is idempotent).\n\n\n\n5.5.2 Cochran’s Theorem\n\nTheorem 5.11 (Cochran’s Result) Let \\(y \\sim N_n(\\mu, \\sigma^2 I)\\) and \\(y'y = \\sum y'A_iy\\). The quadratic forms \\(y^T A_i y / \\sigma^2\\) are mutually independent \\(\\chi^2(r_i, \\lambda_i)\\) if and only if any one of the following holds:\n\nEach \\(A_i\\) is idempotent.\n\\(A_i A_j = 0\\) for all \\(i \\ne j\\).\n\\(n = \\sum r_i\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#non-central-distributions-derived-from-non-central-chi2",
    "href": "lec4-qf.html#non-central-distributions-derived-from-non-central-chi2",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.6 Non-central Distributions Derived from Non-central \\(\\chi^2\\)",
    "text": "5.6 Non-central Distributions Derived from Non-central \\(\\chi^2\\)\nWe begin by defining two independent Chi-squared random variables that form the building blocks for statistical power analysis.\n\nNon-central Component (\\(X_1\\)): \\(X_1 \\sim \\chi^2(\\text{df}_1, \\lambda)\\). Here, \\(\\lambda\\) is the non-centrality parameter, defined as the sum of squared means, \\(\\lambda = ||\\mu||^2\\). This is consistent with the definition used throughout this chapter. (Note: This definition is also used by R’s ncp argument.)\nCentral Component (\\(X_2\\)): \\(X_2 \\sim \\chi^2(\\text{df}_2)\\). \\(X_2\\) often represents the Noise Sum of Squares, SSE\\(_1\\) of an adequate model, which is assume to follow a central \\(\\chi^2\\),\n\nWe visualize these components as using the follow diagram.\n\n\n\n\n\n\n\n\nFigure 5.5: A diagram of two independent \\(\\chi^2\\) random variables\n\n\n\n\n\n\n5.6.1 The Non-central F-distribution \\(F(\\text{df}_1, \\text{df}_2, \\lambda)\\)\n\nDefinition 5.3 (Non-central F) Let \\(X_1 \\sim \\chi^2(\\text{df}_1, \\lambda)\\) and \\(X_2 \\sim \\chi^2(\\text{df}_2)\\) be independent. The random variable \\(F\\) follows a non-central F-distribution: \\[F = \\frac{X_1/\\text{df}_1}{X_2/\\text{df}_2} \\sim F(\\text{df}_1, \\text{df}_2, \\lambda)\\]\n\n\nExpectation:\n\nUnder \\(H_0\\) (\\(\\lambda=0\\)): Exact mean is \\(\\frac{\\text{df}_2}{\\text{df}_2 - 2}\\) (for \\(\\text{df}_2 &gt; 2\\)).\nUnder \\(H_1\\) (\\(\\lambda \\neq 0\\)): Approximate mean is \\(1 + \\frac{\\lambda}{\\text{df}_1}\\).\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Densities of Non-Central F (\\(\\lambda\\) defined as sum of squares).\n\n\n\n\n\n\n\n5.6.2 Type I Non-central Beta \\(\\text{Beta}_1(\\text{df}_1/2, \\text{df}_2/2, \\lambda)\\)\n\nDefinition 5.4 (Type I Non-central Beta) The random variable \\(B_I\\) follows a Type I non-central Beta distribution, defined as the signal’s proportion of the total sum (\\(R^2\\)): \\[B_I = \\frac{X_1}{X_1 + X_2} \\sim \\text{Beta}_1\\left(\\frac{\\text{df}_1}{2}, \\frac{\\text{df}_2}{2}, \\lambda\\right)\\]\n\n\nRelationship to F: \\(B_I = \\frac{(\\text{df}_1/\\text{df}_2) F}{1 + (\\text{df}_1/\\text{df}_2) F}\\)\nExpectation:\n\nUnder \\(H_0\\) (\\(\\lambda=0\\)): Exact mean is \\(\\frac{\\text{df}_1}{\\text{df}_1 + \\text{df}_2}\\).\nUnder \\(H_1\\) (\\(\\lambda \\neq 0\\)): Approximate mean is \\(\\frac{\\text{df}_1 + \\lambda}{\\text{df}_1 + \\text{df}_2 + \\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Densities of Type I Beta (\\(R^2\\)).\n\n\n\n\n\n\n\n5.6.3 Type II Non-central Beta \\(\\text{Beta}_2(\\text{df}_2/2, \\text{df}_1/2, \\lambda)\\)\n\nDefinition 5.5 (Type II Non-central Beta) \\[B_{II} = \\frac{X_2}{X_1 + X_2} = 1 - B_I \\sim \\text{Beta}_2\\left(\\frac{\\text{df}_2}{2}, \\frac{\\text{df}_1}{2}, \\lambda\\right)\\]\n\n\nRelationship to F: \\(B_{II} = \\frac{1}{1 + (\\text{df}_1/\\text{df}_2) F}\\)\nExpectation:\n\nUnder \\(H_0\\) (\\(\\lambda=0\\)): Exact mean is \\(\\frac{\\text{df}_2}{\\text{df}_1 + \\text{df}_2}\\).\nUnder \\(H_1\\) (\\(\\lambda \\neq 0\\)): Approximate mean is \\(\\frac{\\text{df}_2}{\\text{df}_1 + \\text{df}_2 + \\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: Densities of Type II Beta (\\(SSE/SST\\)). Support is [0, 1].\n\n\n\n\n\n\n\n5.6.4 Scaled Type II Beta \\(\\text{Scaled-Beta}_2(\\text{df}_2/2, \\text{df}_1/2, \\lambda)\\)\n\nDefinition 5.6 (Scaled Type II Beta) \\[S = \\frac{X_2/\\text{df}_2}{(X_1+X_2)/(\\text{df}_1+\\text{df}_2)} \\sim \\text{Scaled-Beta}_2\\]\n\n\nRelationship to F: \\(S = \\frac{\\text{df}_1+\\text{df}_2}{\\text{df}_2 + \\text{df}_1 F}\\)\nExpectation:\n\nUnder \\(H_0\\) (\\(\\lambda=0\\)): Exact mean is \\(1\\).\nUnder \\(H_1\\) (\\(\\lambda \\neq 0\\)): Approximate mean is \\(\\frac{\\text{df}_1+\\text{df}_2}{\\text{df}_1+\\text{df}_2+\\lambda}\\).\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: Densities of Scaled Type II Beta (\\(MSE/MST\\)).\n\n\n\n\n\n\n\n5.6.5 The Non-central t-distribution \\(t(\\text{df}_2, \\delta)\\)\n\nDefinition 5.7 (Non-central t) Let \\(Z \\sim N(\\delta, 1)\\) and \\(X_2 \\sim \\chi^2(\\text{df}_2)\\) be independent. The random variable \\(T\\) follows a non-central t-distribution: \\[T = \\frac{Z}{\\sqrt{X_2/\\text{df}_2}} \\sim t(\\text{df}_2, \\delta)\\]\n\n\nRelationship to F: \\(F = T^2\\) (when \\(\\text{df}_1=1\\)). Note \\(\\delta^2 = \\lambda\\).\nExpectation:\n\nUnder \\(H_0\\) (\\(\\delta=0\\)): Exact mean is \\(0\\).\nUnder \\(H_1\\) (\\(\\delta \\neq 0\\)): Approximate mean is \\(\\delta\\).\n\n\n\n\n\n\n\n\n\n\nFigure 5.10: Densities of Non-Central t (\\(df=20\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#example-inference-of-the-mean-of-normal-sample",
    "href": "lec4-qf.html#example-inference-of-the-mean-of-normal-sample",
    "title": "5  Distribution of Quadratic Forms",
    "section": "5.7 Example: Inference of the Mean of Normal Sample",
    "text": "5.7 Example: Inference of the Mean of Normal Sample\nConsider a random sample \\(y \\sim N_n(\\mu j_n, \\sigma^2 I_n)\\). We wish to test:\n\n\\(M_1\\) (Full Model): \\(\\mu\\) is unknown.\n\\(M_0\\) (Reduced Model): \\(\\mu = \\mu_0\\).\n\nLet’s define the transformed vector \\(y^* = y - \\mu_0 j_n\\). Note that \\(y^* \\sim N_n((\\mu - \\mu_0)j_n, \\sigma^2 I_n)\\).\n\n5.7.1 Sum of Squares and Their Distributions\nWe use the projection matrix \\(P_{j_n} = \\frac{1}{n}j_n j_n'\\) and its complement \\((I_n - P_{j_n})\\) to partition the transformed vector.\n\nTotal SSE (\\(SSE_0\\) for \\(M_0\\)): \\[SSE_0 = \\|I_n y^*\\|^2 = \\sum_{i=1}^n (Y_i - \\mu_0)^2\\] This follows a non-central distribution with \\(\\text{df}_{\\text{total}} = n\\): \\[\\frac{SSE_0}{\\sigma^2} \\sim \\chi^2(n, \\lambda) \\quad \\text{where } \\lambda = \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}\\]\nResidual SSE (\\(SSE_1\\) for \\(M_1\\)): \\[SSE_1 = \\|(I_n - P_{j_n})y^*\\|^2 = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\] This captures the random noise (central component) with \\(\\text{df}_2 = n-1\\): \\[\\frac{SSE_1}{\\sigma^2} \\sim \\chi^2(n-1)\\]\nDifference SS (\\(SS_{\\text{diff}}\\)): \\[SS_{\\text{diff}} = \\|P_{j_n} y^*\\|^2 = n(\\bar{Y} - \\mu_0)^2\\] This captures the signal (non-central component) with \\(\\text{df}_1 = 1\\): \\[\\frac{SS_{\\text{diff}}}{\\sigma^2} \\sim \\chi^2(1, \\lambda)\\]\n\n\n\n5.7.2 Distributions of Equivalent Statistics\nWe can construct five equivalent statistics to compare \\(M_0\\) and \\(M_1\\).\n\nThe t-statistic (\\(T\\)): \\[T = \\frac{\\bar{Y} - \\mu_0}{S/\\sqrt{n}}\\]\nThe F-statistic (\\(F\\)): \\[F = \\frac{n(\\bar{Y} - \\mu_0)^2}{S^2} = T^2\\]\nThe Type I Beta statistic (\\(B_I\\)): \\[B_I = \\frac{SS_{\\text{diff}}}{SSE_0} = \\frac{n(\\bar{Y} - \\mu_0)^2}{\\sum (Y_i - \\mu_0)^2}\\]\nThe Type II Beta statistic (\\(B_{II}\\)): \\[B_{II} = \\frac{SSE_1}{SSE_0} = \\frac{\\sum (Y_i - \\bar{Y})^2}{\\sum (Y_i - \\mu_0)^2} = 1 - B_I\\]\nThe Scaled Type II Beta statistic (\\(S_{\\text{scaled}}\\)): \\[S_{\\text{scaled}} = \\frac{SSE_1/(n-1)}{SSE_0/n} = \\left( \\frac{n}{n-1} \\right) B_{II}\\]\n\n\n\n5.7.3 Expectations Under \\(M_1\\) and \\(M_0\\)\nThe table below contrasts the distributions and expected values of these statistics. We assume the sample size \\(n\\) is large enough for the mean of \\(F\\) to exist (\\(n &gt; 3\\)).\n\nDegrees of Freedom: \\(\\text{df}_1 = 1\\), \\(\\text{df}_2 = n-1\\).\nNon-centrality: \\(\\delta = \\frac{\\sqrt{n}(\\mu - \\mu_0)}{\\sigma}\\) and \\(\\lambda = \\delta^2 = \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}\\).\n\n\n\n\nTable 5.1: Expected Values of Test Statistics Under Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nDistribution under \\(H_1\\) (\\(\\mu \\neq \\mu_0\\))\nExact Mean under \\(H_0\\) (\\(\\mu=\\mu_0\\))\nApproximate Mean under \\(H_1\\)\n\n\n\n\n\\(T\\)\n\\(t(n-1, \\delta)\\)\n\\(0\\)\n\\(\\frac{\\sqrt{n}(\\mu - \\mu_0)}{\\sigma}\\)\n\n\n\\(F\\)\n\\(F(1, n-1, \\lambda)\\)\n\\(\\frac{n-1}{n-3} \\approx 1\\)\n\\(1 + \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}\\)\n\n\n\\(B_I\\)\n\\(\\text{Beta}_1\\left(\\frac{1}{2}, \\frac{n-1}{2}, \\lambda\\right)\\)\n\\(\\frac{1}{n}\\)\n\\(\\frac{1/n + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}\\)\n\n\n\\(B_{II}\\)\n\\(\\text{Beta}_2\\left(\\frac{n-1}{2}, \\frac{1}{2}, \\lambda\\right)\\)\n\\(\\frac{n-1}{n}\\)\n\\(\\frac{(n-1)/n}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}\\)\n\n\n\\(S_{\\text{scaled}}\\)\n\\(\\text{Scaled-Beta}_2\\left(\\frac{n-1}{2}, \\frac{1}{2}, \\lambda\\right)\\)\n\\(1\\)\n\\(\\frac{1}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}\\)\n\n\n\n\n\n\nKey Interpretation: All statistics are functionally driven by the signal energy. Notably, for \\(S_{\\text{scaled}}\\), the sample size \\(n\\) cancels out in the approximate mean. This makes it a direct measure of the ratio between Noise Variance and Total Variance (Noise + Signal) in the population distributions, connected to the Rao-Blackwell decomposition of variances.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec5-est.html",
    "href": "lec5-est.html",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "",
    "text": "6.1 Linear Models and Least Square Estimator",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#linear-models-and-least-square-estimator",
    "href": "lec5-est.html#linear-models-and-least-square-estimator",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "",
    "text": "6.1.1 Assumptions in Linear Models\nSuppose that on a random sample of \\(n\\) units (patients, animals, trees, etc.) we observe a response variable \\(Y\\) and explanatory variables \\(X_{1},...,X_{k}\\). Our data are then \\((y_{i},x_{i1},...,x_{ik})\\), \\(i=1,...,n\\), or in vector/matrix form \\(y, x_{1},...,x_{k}\\) where \\(y=(y_{1},...,y_{n})\\) and \\(x_{j}=(x_{1j},...,x_{nj})^{T}\\) or \\(y, X\\) where \\(X=(x_{1},...,x_{k})\\).\nEither by design or by conditioning on their observed values, \\(x_{1},...,x_{k}\\) are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:\nAssumptions on Linear Models\n\nA1. (Additive Error) \\(y=\\mu+e\\) where \\(e=(e_{1},...,e_{n})^{T}\\) is an unobserved random vector with \\(E(e)=0\\). This implies that \\(\\mu=E(y)\\) is the unknown mean of \\(y\\).\nA2. (Linearity) \\(\\mu=\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}=X\\beta\\) where \\(\\beta_{1},...,\\beta_{k}\\) are unknown parameters. This assumption says that \\(E(y)=\\mu\\in\\text{Col}(X)\\) (lies in the column space of \\(X\\)); i.e., it is a linear combination of explanatory vectors \\(x_{1},...,x_{k}\\) with coefficients the unknown parameters in \\(\\beta=(\\beta_{1},...,\\beta_{k})^{T}\\). Note that it is linear in \\(\\beta_{1},...,\\beta_{k}\\), not necessarily in the \\(x\\)’s.\nA3. (Independence) \\(e_{1},...,e_{n}\\) are independent random variables (and therefore so are \\(y_{1},...,y_{n})\\).\nA4. (Homoscedasticity) \\(e_{1},...,e_{n}\\) all have the same variance \\(\\sigma^{2}\\); that is, \\(\\text{Var}(e_{1})=\\cdot\\cdot\\cdot=\\text{Var}(e_{n})=\\sigma^{2}\\) which implies \\(\\text{Var}(y_{1})=\\cdot\\cdot\\cdot=\\text{Var}(y_{n})=\\sigma^{2}\\).\nA5. (Normality) \\(e\\sim N_{n}(0,\\sigma^{2}I_{n})\\).\n\n\n\n6.1.2 Matrix Formulation\nThe model can be written algebraically as: \\[y_{i}=\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdot\\cdot\\cdot+\\beta_{k}x_{ik}, \\quad i=1,...,n\\]\nOr in matrix notation: \\[\n\\begin{pmatrix}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_{11} & x_{12} & \\cdot\\cdot\\cdot & x_{1k}\\\\\n1 & x_{21} & x_{22} & \\cdot\\cdot\\cdot & x_{2k}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdot\\cdot\\cdot & x_{nk}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nThis is expressed compactly as: \\[y=X\\beta+e\\] where \\(X\\) is the design matrix, and \\(e \\sim N_n(0, \\sigma^2 I)\\). Alternatively: \\[y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}+e\\]\nTaken together, all five assumptions can be stated more succinctly as: \\[y\\sim N_{n}(X\\beta,\\sigma^{2}I)\\] with the mean vector \\(\\mu_{y}=X\\beta\\in \\text{Col}(X)\\).\n\n\n\n\n\n\nA Note on Coefficients\n\n\n\nThe effect of a parameter depends upon what other explanatory variables are present in the model. For example, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in the model: \\[y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+e\\] will typically be different than \\(\\beta_{0}^{*}\\) and \\(\\beta_{1}^{*}\\) in the model: \\[y=\\beta_{0}^{*}j_{n}+\\beta_{1}^{*}x_{1}+e^{*}\\] In this context, \\(\\beta_0^*\\) and \\(\\beta_1^*\\) are the population-projected coefficients of the full model, that is, \\(\\beta_0^*\\) and \\(\\beta_1^*\\) are the parameters that can best approximate the full model.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will first consider the case that \\(\\text{rank}(X)=k+1\\).\n\n\n\n\n6.1.3 Least Squares Estimator of \\(\\beta\\) and Fitted Value \\(\\hat Y\\)\n\nDefinition 6.1 (Least Squares Estimator) The Least Squares Estimator (LSE) of \\(\\beta\\), denoted as \\(\\hat{\\beta}\\), is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses \\(y\\) and the fitted values \\(X\\hat{\\beta}\\). \\[\nQ(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 = (y - X\\beta)'(y - X\\beta)\n\\]\n\nWe can derive the closed-form solution for \\(\\hat{\\beta}\\) using the geometry of projections discussed in previous chapters.\n\n1. Obtaining \\(\\hat Y\\)\nIn the linear model \\(y = X\\beta + e\\), the systematic component (the mean \\(E[y]\\)) is constrained to lie in the column space of \\(X\\), denoted as \\(\\text{Col}(X)\\). We seek the vector in \\(\\text{Col}(X)\\) that is “closest” to the observed data vector \\(y\\). As established in the theory of projections, this closest vector is the orthogonal projection of \\(y\\) onto \\(\\text{Col}(X)\\). Let \\(\\hat{y}\\) denote this fitted value vector. Using the explicit formula for the projection matrix \\[H = X(X'X)^{-1}X',\\] we have: \\[ \\hat{y} = Hy = X(X'X)^{-1}X' y.\\]\n\n\n2. Obtaining \\(\\hat{\\beta}\\) by Solving \\(x\\beta = \\hat{y}\\)\nSince the fitted vector \\(\\hat{y}\\) is a projection onto \\(\\text{Col}(X)\\), it must lie entirely within that column space. This guarantees that the linear system for the coefficients \\(\\hat{\\beta}\\) is consistent (has an exact solution): \\[ X\\hat{\\beta} = \\hat{y} \\]\nTo isolate \\(\\hat{\\beta}\\), we pre-multiply both sides by the left pseudo-inverse of \\(X\\), which is \\((X'X)^{-1}X'\\):\n\\[\n\\begin{aligned}\n(X'X)^{-1}X' (X\\hat{\\beta}) &= (X'X)^{-1}X' \\hat{y} \\\\\n\\underbrace{(X'X)^{-1}(X'X)}_{I} \\hat{\\beta} &= (X'X)^{-1}X' \\hat{y}\n\\end{aligned}\n\\]\nThis gives us the estimator expressed in terms of the fitted values:\n\\[\n\\boxed{\\hat{\\beta} = (X'X)^{-1}X' \\hat{y}}\n\\]\nHowever, we typically calculate the estimator from the observed data \\(y\\). Recall that because \\(\\hat{y}\\) is an orthogonal projection, the difference \\(y - \\hat{y}\\) is orthogonal to \\(X\\). This implies \\(X'\\hat{y} = X'y\\). Substituting this into the equation above yields the standard closed-form solution:\n\\[\n\\boxed{\\hat{\\beta} = (X'X)^{-1}X'y}\n\\]\n\n\n\n6.1.4 Properties of the Estimator \\(\\hat \\beta\\)\n\nTheorem 6.1 (Unbiasedness of \\(\\hat \\beta\\)) If \\(E(y)=X\\beta\\), then \\(\\hat{\\beta}\\) is an unbiased estimator for \\(\\beta\\).\n\n\nProof. \\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= E[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}E(y) \\quad \\text{[using linearity of expectation]} \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}X\\beta \\\\\n&= \\beta\n\\end{aligned}\n\\]\n\n\nTheorem 6.2 (Variance of \\(\\hat \\beta\\)) If \\(\\text{Var}(y)=\\sigma^{2}I\\), the covariance matrix for \\(\\hat{\\beta}\\) is given by \\(\\sigma^{2}(X^{\\prime}X)^{-1}\\).\n\n\nProof. \\[\n\\begin{aligned}\n\\text{Var}(\\hat{\\beta}) &= \\text{Var}[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}\\text{Var}(y)[(X^{\\prime}X)^{-1}X^{\\prime}]^{\\prime} \\quad \\text{[using } \\text{Var}(Ay) = A \\text{Var}(y) A'] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}(\\sigma^{2}I)X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}X^{\\prime}X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}\n\\end{aligned}\n\\]\n\nNote: These theorems require no assumption of normality.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#best-linear-unbiased-estimator-blue",
    "href": "lec5-est.html#best-linear-unbiased-estimator-blue",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.2 Best Linear Unbiased Estimator (BLUE)",
    "text": "6.2 Best Linear Unbiased Estimator (BLUE)\n\nTheorem 6.3 (Gauss-Markov Theorem) If \\(E(y)=X\\beta\\) and \\(\\text{Var}(y)=\\sigma^{2}I\\), the least-squares estimators \\(\\hat{\\beta}_{j}, j=0,1,...,k\\) have minimum variance among all linear unbiased estimators.\n\n\nProof. We consider a linear estimator \\(Ay\\) of \\(\\beta\\) and seek the matrix \\(A\\) for which \\(Ay\\) is a minimum variance unbiased estimator.\n1. Unbiasedness Condition: In order for \\(Ay\\) to be an unbiased estimator of \\(\\beta\\), we must have \\(E(Ay)=\\beta\\). Using the assumption \\(E(y)=X\\beta\\), this is expressed as: \\[E(Ay) = A E(y) = AX\\beta = \\beta\\] which implies the condition \\(AX=I_{k+1}\\) since the relationship must hold for any \\(\\beta\\).\n2. Minimizing Variance: The covariance matrix for the estimator \\(Ay\\) is: \\[\\text{Var}(Ay) = A \\text{Var}(y) A' = A(\\sigma^2 I) A' = \\sigma^2 AA'\\] We need to choose \\(A\\) (subject to \\(AX=I\\)) so that the diagonal elements of \\(AA'\\) are minimized.\nTo relate \\(Ay\\) to \\(\\hat{\\beta}=(X'X)^{-1}X'y\\), we define \\(\\hat{A} = (X'X)^{-1}X'\\) and write \\(A = (A - \\hat{A}) + \\hat{A}\\). Then: \\[AA' = [(A - \\hat{A}) + \\hat{A}] [(A - \\hat{A}) + \\hat{A}]'\\] Expanding this, the cross terms vanish because \\((A - \\hat{A})\\hat{A}' = A\\hat{A}' - \\hat{A}\\hat{A}'\\). Note that \\(\\hat{A}\\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}\\). Also, \\(A\\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}\\) (since \\(AX=I\\)). Thus, \\((A - \\hat{A})\\hat{A}' = 0\\).\nThe expansion simplifies to: \\[AA' = (A - \\hat{A})(A - \\hat{A})' + \\hat{A}\\hat{A}'\\] The matrix \\((A - \\hat{A})(A - \\hat{A})'\\) is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of \\(AA'\\), we must set \\(A - \\hat{A} = 0\\), which implies \\(A = \\hat{A}\\).\nThus, the minimum variance estimator is: \\[Ay = (X'X)^{-1}X'y = \\hat{\\beta}\\]\n\n\n6.2.1 Notes on Gauss-markov\n\nDistributional Generality: The remarkable feature of the Gauss-Markov theorem is that it holds for any distribution of \\(y\\); normality is not required. The only assumptions used are linearity (\\(E(y)=X\\beta\\)) and homoscedasticity (\\(\\text{Var}(y)=\\sigma^2 I\\)).\nExtension to All Linear Combinations: The theorem extends beyond just the parameter vector \\(\\beta\\) to any linear combination of the parameters.\n\n\nCorollary 6.1 (BLUE for All Linear Combinations) If \\(E(y)=X\\beta\\) and \\(\\text{Var}(y)=\\sigma^{2}I\\), the best linear unbiased estimator of the scalar \\(a'\\beta\\) is \\(a'\\hat{\\beta}\\), where \\(\\hat{\\beta}\\) is the least-squares estimator.\n\n\nProof. Let \\(\\tilde{\\beta} = Ay\\) be any other linear unbiased estimator of \\(\\beta\\). The variance of the linear combination \\(a'\\tilde{\\beta}\\) is: \\[\n\\frac{1}{\\sigma^2}\\text{Var}(a'\\tilde{\\beta}) = \\frac{1}{\\sigma^2}\\text{Var}(a'Ay) = a'AA'a\n\\] From the proof of the Gauss-Markov theorem, we established that \\(AA' = (A-\\hat{A})(A-\\hat{A})' + (X'X)^{-1}\\) where \\(\\hat{A} = (X'X)^{-1}X'\\). Substituting this into the variance equation: \\[\na'AA'a = a'(A-\\hat{A})(A-\\hat{A})'a + a'(X'X)^{-1}a\n\\] The term \\(a'(A-\\hat{A})(A-\\hat{A})'a\\) is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore: \\[\na'AA'a \\ge a'(X'X)^{-1}a = \\frac{1}{\\sigma^2}\\text{Var}(a'\\hat{\\beta})\n\\] The variance is minimized when \\(A=\\hat{A}\\) (specifically when the first term is zero), proving that \\(a'\\hat{\\beta}\\) has the minimum variance among all linear unbiased estimators.\n\n\nScaling Invariance: The predictions made by the model are invariant to the scaling of the explanatory variables.\n\n\nTheorem 6.4 (Scaling Explanatory Variables) If \\(x=(1,x_{1},...,x_{k})'\\) and \\(z=(1,c_{1}x_{1},...,c_{k}x_{k})'\\), then the fitted values are identical: \\(\\hat{y} = \\hat{\\beta}'x = \\hat{\\beta}_{z}'z\\).\n\n\nProof. Let \\(D = \\text{diag}(1, c_1, ..., c_k)\\) such that the design matrix is transformed to \\(Z = XD\\). The LSE for the transformed data is: \\[\n\\begin{aligned}\n\\hat{\\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\\\\n&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\\\\n&= D^{-1}(X'X)^{-1}X'y = D^{-1}\\hat{\\beta}\n\\end{aligned}\n\\] . Then, the prediction is: \\[\n\\hat{\\beta}_z' z = (D^{-1}\\hat{\\beta})' (Dx) = \\hat{\\beta}' (D^{-1})' D x = \\hat{\\beta}'x\n\\] .\n\n\nLimitations: Restriction to Unbiased Estimators\nIt is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of linear and unbiased estimators.\n\nAssumption Sensitivity: If the assumptions of linearity (\\(E(y)=X\\beta\\)) and homoscedasticity (\\(\\text{Var}(y)=\\sigma^2 I\\)) do not hold, \\(\\hat{\\beta}\\) may be biased or may have a larger variance than other estimators.\nUnbiasedness Constraint: The theorem does not compare \\(\\hat{\\beta}\\) to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only “best” (minimum variance) among those estimators that satisfy the unbiasedness constraint.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#estimator-of-error-variance",
    "href": "lec5-est.html#estimator-of-error-variance",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.3 Estimator of Error Variance",
    "text": "6.3 Estimator of Error Variance\nWe estimate \\(\\sigma^{2}\\) by the residual mean square:\n\nDefinition 6.2 (Residual Variance Estimator) \\[s^{2} = \\frac{1}{n-k-1} \\sum_{i=1}^{n}(y_{i}-x_{i}'\\hat{\\beta})^{2} = \\frac{\\text{SSE}}{n-k-1}\\] where \\(\\text{SSE} = (y-X\\hat{\\beta})'(y-X\\hat{\\beta})\\).\n\nAlternatively, SSE can be written as: \\[\\text{SSE} = y'y - \\hat{\\beta}'X'y\\] This is often useful for computation (\\(y'y\\) is the total sum of squares of the raw data).\n\n6.3.1 Unbiasedness of \\(s^2\\)\n\nTheorem 6.5 (Unbiasedness of s-squared) If \\(s^{2}\\) is defined as above, and if \\(E(y)=X\\beta\\) and \\(\\text{Var}(y)=\\sigma^{2}I\\), then \\(E(s^{2})=\\sigma^{2}\\).\n\n\nProof. We use the Hat Matrix \\(H = X(X'X)^{-1}X'\\), which projects \\(y\\) onto \\(\\text{Col}(X)\\). Thus, \\(\\hat{y} = Hy\\). The residuals are \\(y - \\hat{y} = (I - H)y\\). The Sum of Squared Errors is: \\[\\text{SSE} = \\|(I-H)y\\|^2 = y'(I-H)'(I-H)y\\] Since \\(H\\) is symmetric and idempotent, \\((I-H)\\) is also symmetric and idempotent. Thus: \\[\\text{SSE} = y'(I-H)y\\]\nTo find the expectation, we use the trace trick for quadratic forms: \\(E[y'Ay] = \\text{tr}(A\\text{Var}(y)) + E[y]'A E[y]\\). \\[\n\\begin{aligned}\nE(\\text{SSE}) &= E[y'(I-H)y] \\\\\n&= \\text{tr}((I-H)\\sigma^2 I) + (X\\beta)'(I-H)(X\\beta) \\\\\n&= \\sigma^2 \\text{tr}(I-H) + \\beta'X'(I-H)X\\beta\n\\end{aligned}\n\\] Trace Term: \\(\\text{tr}(I_n - H) = \\text{tr}(I_n) - \\text{tr}(H) = n - (k+1)\\), since \\(\\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_{k+1}) = k+1\\).\nNon-centrality Term: Since \\(HX = X\\), we have \\((I-H)X = 0\\). Therefore, the second term vanishes: \\(\\beta'X'(I-H)X\\beta = 0\\).\nCombining these: \\[E(\\text{SSE}) = \\sigma^2(n - k - 1)\\] Dividing by the degrees of freedom \\((n-k-1)\\), we get \\(E(s^2) = \\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#distributions-under-normality",
    "href": "lec5-est.html#distributions-under-normality",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.4 Distributions Under Normality",
    "text": "6.4 Distributions Under Normality\nIf we add Assumption A5 (\\(y \\sim N_n(X\\beta, \\sigma^2 I)\\)), we can derive the exact sampling distributions.\n\nCorollary 6.2 (Estimated Covariance of Beta) An unbiased estimator of \\(\\text{Cov}(\\hat{\\beta})\\) is given by: \\[\\widehat{\\text{Cov}}(\\hat{\\beta}) = s^{2}(X'X)^{-1}\\]\n\n\nTheorem 6.6 (Sampling Distributions) Under assumptions A1-A5:\n\n\\(\\hat{\\beta} \\sim N_{k+1}(\\beta, \\sigma^{2}(X'X)^{-1})\\).\n\\((n-k-1)s^{2}/\\sigma^{2} \\sim \\chi^{2}(n-k-1)\\).\n\\(\\hat{\\beta}\\) and \\(s^{2}\\) are independent.\n\n\n\nProof. Part (i): Since \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) is a linear transformation of the normal vector \\(y\\), it is also normally distributed. We already established its mean and variance in Theorem 6.1 and Theorem 6.2.\nPart (ii): We showed \\(\\text{SSE} = y'(I-H)y\\). Since \\((I-H)\\) is idempotent with rank \\(n-k-1\\), and \\((I-H)X\\beta = 0\\), by the theory of quadratic forms in normal variables, \\(\\text{SSE}/\\sigma^2 \\sim \\chi^2(n-k-1)\\).\nPart (iii): \\(\\hat{\\beta}\\) depends on \\(Hy\\) (or \\(X'y\\)), while \\(s^2\\) depends on \\((I-H)y\\). Since \\(H(I-H) = H - H^2 = 0\\), the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#maximum-likelihood-estimator-mle",
    "href": "lec5-est.html#maximum-likelihood-estimator-mle",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.5 Maximum Likelihood Estimator (MLE)",
    "text": "6.5 Maximum Likelihood Estimator (MLE)\n\nTheorem 6.7 (MLE for Linear Regression) If \\(y \\sim N_n(X\\beta, \\sigma^2 I)\\), the Maximum Likelihood Estimators are: \\[\n\\hat{\\beta}_{\\text{MLE}} = (X'X)^{-1}X'y\n\\] \\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}(y - X\\hat{\\beta})'(y - X\\hat{\\beta}) = \\frac{\\text{SSE}}{n}\n\\]\n\n\nProof. The log-likelihood function is: \\[ \\ln L(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta) \\] Maximizing this with respect to \\(\\beta\\) is equivalent to minimizing the quadratic term \\((y - X\\beta)'(y - X\\beta)\\), which yields the Least Squares Estimator. Differentiating with respect to \\(\\sigma^2\\) and setting to zero yields \\(\\hat{\\sigma}^2 = \\text{SSE}/n\\).\n\nNote: The MLE for \\(\\sigma^2\\) is biased (denominator \\(n\\)), whereas \\(s^2\\) is unbiased (denominator \\(n-k-1\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#linear-models-in-centered-form",
    "href": "lec5-est.html#linear-models-in-centered-form",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.6 Linear Models in Centered Form",
    "text": "6.6 Linear Models in Centered Form\nThe regression model can be written in a centered form by subtracting the means of the explanatory variables: \\[y_{i}=\\alpha+\\beta_{1}(x_{i1}-\\overline{x}_{1})+\\beta_{2}(x_{i2}-\\overline{x}_{2})+\\cdot\\cdot\\cdot+\\beta_{k}(x_{ik}-\\overline{x}_{k})+e_{i}\\] for \\(i=1,...,n\\), where the intercept term is adjusted: \\[\\alpha=\\beta_{0}+\\beta_{1}\\overline{x}_{1}+\\beta_{2}\\overline{x}_{2}+\\cdot\\cdot\\cdot+\\beta_{k}\\overline{x}_{k}\\] and \\(\\overline{x}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}\\).\n\n6.6.1 Matrix Formulation\nIn matrix form, the equivalence between the original model and the centered model is: \\[y = X\\beta + e = (j_n, X_c)\\begin{pmatrix} \\alpha \\\\ \\beta_{1} \\end{pmatrix} + e\\] where \\(\\beta_{1}=(\\beta_{1},...,\\beta_{k})^{T}\\) represents the slope coefficients, and \\(X_c\\) is the centered design matrix: \\[X_c = (I - P_{j_n})X_1\\] Here, \\(X_1\\) consists of the original columns of \\(X\\) excluding the intercept column.\nTo see the structure of \\(X_c\\), we first calculate the projection of the data onto the intercept space, \\(P_{j_n}X_1\\): \\[\n\\begin{aligned}\nP_{j_n}X_1 &= \\frac{1}{n}j_n j_n' X_1 \\\\\n&= \\begin{pmatrix} 1/n & 1/n & \\cdots & 1/n \\\\ 1/n & 1/n & \\cdots & 1/n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1/n & 1/n & \\cdots & 1/n \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n\\] This results in a matrix where every row is the vector of column means. Subtracting this from \\(X_1\\) gives \\(X_c\\): \\[\n\\begin{aligned}\nX_c &= X_1 - P_{j_n}X_1 \\\\\n&= \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} - \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix} \\\\\n&= \\begin{pmatrix} x_{11} - \\bar{x}_1 & x_{12} - \\bar{x}_2 & \\cdots & x_{1k} - \\bar{x}_k \\\\ x_{21} - \\bar{x}_1 & x_{22} - \\bar{x}_2 & \\cdots & x_{2k} - \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} - \\bar{x}_1 & x_{n2} - \\bar{x}_2 & \\cdots & x_{nk} - \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n6.6.2 Estimation in Centered Form\nBecause the column space of the intercept \\(j_n\\) is orthogonal to the columns of \\(X_c\\) (since columns of \\(X_c\\) sum to zero), the cross-product matrix becomes block diagonal: \\[\n\\begin{pmatrix} j_n' \\\\ X_c' \\end{pmatrix} (j_n, X_c) = \\begin{pmatrix} j_n'j_n & j_n'X_c \\\\ X_c'j_n & X_c'X_c \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}\n\\]\n\nTheorem 6.8 (Centered Estimators) The least squares estimators for the centered parameters are: \\[\n\\begin{pmatrix} \\hat{\\alpha} \\\\ \\hat{\\beta}_{1} \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}^{-1} \\begin{pmatrix} j_n'y \\\\ X_c'y \\end{pmatrix} = \\begin{pmatrix} \\bar{y} \\\\ (X_c'X_c)^{-1}X_c'y \\end{pmatrix}\n\\] Thus:\n\n\\(\\hat{\\alpha} = \\bar{y}\\) (The sample mean of \\(y\\)).\n\\(\\hat{\\beta}_{1} = S_{xx}^{-1}S_{xy}\\), using the sample covariance notations.\n\n\nRecovering the original intercept: \\[ \\hat{\\beta}_0 = \\hat{\\alpha} - \\hat{\\beta}_1 \\bar{x}_1 - \\dots - \\hat{\\beta}_k \\bar{x}_k = \\bar{y} - \\hat{\\beta}_{1}'\\bar{x} \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#sum-of-squares-decomposition",
    "href": "lec5-est.html#sum-of-squares-decomposition",
    "title": "6  Inference in A Multiple Linear Regression Model",
    "section": "6.7 Sum of Squares Decomposition",
    "text": "6.7 Sum of Squares Decomposition\nWe partition the total variation based on the orthogonal subspaces.\n\nDefinition 6.3 (Sum of Squares Components) The total variation is decomposed as \\(\\text{SST} = \\text{SSR} + \\text{SSE}\\).\n\nTotal Sum of Squares (SST): The squared length of the centered response vector. \\[\\text{SST} = \\|y - \\bar{y}j_n\\|^2 = \\|(I - P_{j_n})y\\|^2\\]\nRegression Sum of Squares (SSR): The variation explained by the regressors \\(X_c\\). \\[\\text{SSR} = \\|\\hat{y} - \\bar{y}j_n\\|^2 = \\|P_{X_c}y\\|^2 = \\hat{\\beta}_1' X_c' X_c \\hat{\\beta}_1\\]\nSum of Squared Errors (SSE): The residual variation. \\[\\text{SSE} = \\|y - \\hat{y}\\|^2 = \\|(I - H)y\\|^2\\]\n\n\n\n6.7.1 3D Visualization of Decomposition of \\(y\\)\nWe partition the total variation in \\(y\\) based on the orthogonal subspaces.\n\nSpace of the Mean: \\(L(j_n)\\), spanned by the intercept vector \\(j_n\\).\nSpace of the Regressors: \\(L(X_c)\\), spanned by the centered predictors \\(X_c\\).\nError Space: \\(\\text{Col}(X)^\\perp\\), orthogonal to the model space.\n\nThe vector \\(y\\) can be decomposed into three orthogonal components: \\[y = \\bar{y}j_n + P_{X_c}y + (y - \\hat{y})\\] Visually, this corresponds to projecting the vector \\(y\\) onto three orthogonal axes.\nInteractive Visualization:\nWe generate a cloud of 100 observations of \\(y\\) from \\(N(\\mu, \\sigma=1)\\) where \\(\\mu = (5,5,0)\\). The projections onto the Model Plane (\\(z=0\\)) are highlighted in red, and the projections onto the error axis (\\(z\\)) are in yellow.\n\nEffect Exists (signal)No Effect (noise)\n\n\n\n\n\n\nScenario 1: Significant regression effect (\\(\\beta_1\not= 0\\)). The mean vector projects significantly onto the predictor space.\n\n\n\n\n\n\n\n\nScenario 2: No regression effect (\\(\\beta_1 = 0\\)). The mean vector lies purely on the intercept axis.\n\n\n\n\n\n\n\n6.7.2 A Diagram to Show Decomposition of Sum of Squares\nThe decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).\n\n\n\n\n\n\n\n\nFigure 6.1: Geometric Decomposition: SST = SSR + SSE\n\n\n\n\n\n\n\n6.7.3 Distribution of Sum of Squares\nWe apply the general theory of projections to the specific components defined in Definition 6.3.\n\nTheorem 6.9 (Distribution of Sum of Squares) Let \\(y \\sim N(\\mu, \\sigma^2 I_n)\\), where \\(\\mu \\in \\text{Col}(X)\\). Consider the decomposition defined by the projection matrices \\(P_{X_c}\\) and \\(M = I - H\\).\n\nIndependence: The quadratic forms \\(\\text{SSR}\\) and \\(\\text{SSE}\\) are statistically independent because the subspaces \\(L(X_c)\\) and \\(\\text{Col}(X)^\\perp\\) are orthogonal.\nDistribution of SSE: The scaled sum of squared errors follows a central Chi-squared distribution: \\[ \\frac{\\text{SSE}}{\\sigma^2} = \\frac{\\|(I - H)y\\|^2}{\\sigma^2} \\sim \\chi^2(n-k-1) \\] Mean: \\[ E[\\text{SSE}] = \\sigma^2(n-k-1) \\]\nDistribution of SSR: The scaled regression sum of squares follows a non-central Chi-squared distribution: \\[ \\frac{\\text{SSR}}{\\sigma^2} = \\frac{\\|P_{X_c}y\\|^2}{\\sigma^2} \\sim \\chi^2(k, \\lambda) \\] Mean: \\[ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 \\]\n\nNon-centrality Parameter (\\(\\lambda\\)): \\[ \\lambda = \\frac{1}{\\sigma^2} \\|P_{X_c} \\mu\\|^2 \\] where \\[\\|P_{X_c} \\mu\\|^2 = \\|X_c \\beta_1\\|^2 = (X_c \\beta_1)' (X_c \\beta_1) = \\beta_1' X_c' X_c \\beta_1\\]\n\n\nProof. We apply Theorem 5.8 to the specific projection matrices identified in the definitions.\n\nFor SSE (Error Space): \\(\\text{SSE}\\) is defined by the projection matrix \\(P_V = I - H\\).\n\nDimension: The rank of \\((I - H)\\) is \\(n - \\text{rank}(X) = n - (k+1) = n - k - 1\\).\nNon-centrality: Since \\(\\mu \\in \\text{Col}(X)\\), the projection onto the orthogonal complement is zero: \\(\\|(I - H)\\mu\\|^2 = 0\\). Thus, \\(\\lambda = 0\\).\nExpectation: Using Part 2 of Theorem 5.8 (\\(E(\\|P_V y\\|^2) = \\sigma^2 \\text{rank}(P_V) + \\|P_V \\mu\\|^2\\)): \\[ E[\\text{SSE}] = \\sigma^2(n-k-1) + 0 = \\sigma^2(n-k-1) \\]\n\nFor SSR (Regression Space): \\(\\text{SSR}\\) is defined by the projection matrix \\(P_V = P_{X_c}\\).\n\nDimension: The rank of \\(P_{X_c}\\) is \\((k+1) - 1 = k\\).\nNon-centrality: The projection of \\(\\mu\\) onto \\(L(X_c)\\) is \\(P_{X_c}\\mu\\). \\[ \\lambda = \\frac{1}{2\\sigma^2} \\|P_{X_c} \\mu\\|^2 \\]\nExpectation: Using Part 2 of Theorem 5.8: \\[ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 \\]\n\nThis shows that while \\(E[\\text{SSE}]\\) depends only on the noise variance and sample size, \\(E[\\text{SSR}]\\) is inflated by the magnitude of the true regression signal \\(\\|P_{X_c}\\mu\\|^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference in A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#f-test-for-testing-overall-regression-effect",
    "href": "lec5-est.html#f-test-for-testing-overall-regression-effect",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.8 F-test for Testing Overall Regression Effect",
    "text": "6.8 F-test for Testing Overall Regression Effect\nWe wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.\nHypotheses:\n\nNull Hypothesis (\\(H_0\\)): \\(\\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\) (No regression effect). This implies \\(\\mu \\in \\text{span}(j_n)\\) and the true signal variance \\(\\|X_c\\beta_1\\|^2 = 0\\).\nAlternative Hypothesis (\\(H_1\\)): At least one \\(\\beta_j \\neq 0\\).\n\n\nThe F-statistic\nWe construct the test statistic using the ratio of the Mean Squares defined previously:\n\\[F = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)}\\]\n\n\nUnderstanding \\(F\\) via Expectations\nThe logic of the F-test is transparent when we examine the expected values of the numerator and denominator:\n\\[\n\\begin{aligned}\nE[\\text{MSE}] &= \\sigma^2 \\\\\nE[\\text{MSR}] &= \\sigma^2 + \\frac{\\|X_c \\beta_1\\|^2}{k}\n\\end{aligned}\n\\]\n\nIf \\(H_0\\) is true: The signal term is zero. Both Mean Squares estimate \\(\\sigma^2\\) unbiasedly. We expect \\(F \\approx 1\\).\nIf \\(H_1\\) is true: The numerator includes the positive term \\(\\frac{\\|X_c \\beta_1\\|^2}{k}\\). We expect \\(F &gt; 1\\).\n\nTherefore, we reject \\(H_0\\) for sufficiently large values of \\(F\\). Specifically, we reject at level \\(\\alpha\\) if \\(F_{obs} &gt; F_{\\alpha}(k, n-k-1)\\).\n\n\n6.8.1 Distributional Theory\nTo derive the exact sampling distribution, we rely on the independence of the sums of squares (from Theorem 6.9) and the definition of the non-central F-distribution given in Definition 5.3.\n\nTheorem 6.10 (Distribution of Regression F-Statistic) Under the assumption of normality, the regression F-statistic follows a non-central F-distribution:\n\\[ F \\sim F(k, n-k-1, \\lambda) \\]\nThe non-centrality parameter \\(\\lambda\\) is determined by the ratio of the signal sum of squares to the error variance: \\[ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\]\nSpecial Cases:\n\nUnder \\(H_1\\) (Signal exists): \\(\\lambda &gt; 0\\), so \\(F\\) follows the non-central distribution.\nUnder \\(H_0\\) (No signal): \\(\\beta_1 = 0 \\implies \\lambda = 0\\). The distribution collapses to the central F-distribution: \\[ F \\sim F(k, n-k-1) \\]\n\n\n\nProof. We identify the components from Definition 5.3:\n\nNumerator (\\(X_1\\)): Let \\(X_1 = \\text{SSR}/\\sigma^2\\). From Theorem 6.9, \\(X_1 \\sim \\chi^2(k, \\lambda)\\).\nDenominator (\\(X_2\\)): Let \\(X_2 = \\text{SSE}/\\sigma^2\\). From Theorem 6.9, \\(X_2 \\sim \\chi^2(n-k-1)\\).\nIndependence: \\(X_1\\) and \\(X_2\\) are independent.\n\nSubstituting these into the F-statistic: \\[\nF = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{(\\text{SSR}/\\sigma^2)/k}{(\\text{SSE}/\\sigma^2)/(n-k-1)} = \\frac{X_1/k}{X_2/(n-k-1)}\n\\] By definition Definition 5.3, this ratio follows \\(F(k, n-k-1, \\lambda)\\).\n\n\n\n6.8.2 Visualization of the Rejection Region\nThe following plot illustrates the central F-distribution (valid under \\(H_0\\)) for \\(k=3\\) predictors and \\(n=20\\) observations (\\(df_1 = 3, df_2 = 16\\)). An observed statistic of \\(F=2\\) is marked, with the p-value represented by the shaded tail area.\n\n\n\n\n\n\n\n\nFigure 6.2: Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#coefficient-of-determination-r2",
    "href": "lec5-est.html#coefficient-of-determination-r2",
    "title": "6  Inference in A Multiple Linear Regression Model",
    "section": "6.9 Coefficient of Determination (\\(R^2\\))",
    "text": "6.9 Coefficient of Determination (\\(R^2\\))\n\n6.9.1 Definition\nThe \\(R^2\\) statistic measures the proportion of total variation explained by the regression model.\n\nDefinition 6.4 (R-Squared) \\[R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\\] Since \\(0 \\le \\text{SSE} \\le \\text{SST}\\), it follows that \\(0 \\le R^2 \\le 1\\).\n\n\n\n6.9.2 Expectation and Bias\nTo understand the bias in \\(R^2\\), it is more illuminating to analyze the expectation of the unexplained variance (\\(1 - R^2\\)). This term represents the ratio of error sum of squares to the total sum of squares:\n\\[ E[1 - R^2] = E\\left[ \\frac{\\text{SSE}}{\\text{SST}} \\right] \\]\nUsing the first-order approximation \\(E[X/Y] \\approx E[X]/E[Y]\\), we examine the numerator and denominator separately:\n\\[\n\\begin{aligned}\nE[\\text{SSE}] &= \\sigma^2(n-k-1) \\\\\nE[\\text{SST}] &= \\sigma^2(n-1) + \\sigma^2\\lambda = \\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)\n\\end{aligned}\n\\]\nSubstituting these back, we approximate the expected unexplained fraction:\n\\[ E[1 - R^2] \\approx \\frac{\\sigma^2(n-k-1)}{\\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)} = \\frac{n-k-1}{(n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}} \\]\nBehavior under Null Hypothesis (\\(H_0\\)): When there is no true signal (\\(\\beta_1 = 0\\)), the term \\(\\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}\\) vanishes. The expected proportion of unexplained variance becomes:\n\\[ E[1 - R^2 | H_0] \\approx \\frac{n-k-1}{n-1} \\]\nThis result reveals the source of the bias:\n\nIdeally, if predictors are noise, the model should explain nothing, and \\(E[1-R^2]\\) should be \\(1\\).\nInstead, the expected error ratio is less than 1, specifically scaled by \\(\\frac{n-k-1}{n-1}\\).\nThis scaling factor is exactly what the Adjusted R-squared (\\(R^2_a\\)) attempts to correct by multiplying the observed ratio by the inverse \\(\\frac{n-1}{n-k-1}\\).\n\n\n\n6.9.3 Exact Distribution\nThe \\(R^2\\) statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.\n\nTheorem 6.11 (Distribution of R-Squared) \\[ R^2 \\sim \\text{Beta}_1\\left( \\frac{k}{2}, \\frac{n-k-1}{2}, \\lambda \\right) \\] where \\(\\text{df}_1 = k\\) and \\(\\text{df}_2 = n-k-1\\).\n\n\n\n6.9.4 Adjusted R-squared (\\(R^2_a\\))\nTo correct for the inflation of \\(R^2\\) due to model complexity (\\(k\\)), we introduce the Adjusted \\(R^2\\). This statistic penalizes the sum of squares by their degrees of freedom:\n\\[ R^2_a = 1 - \\frac{\\text{SSE}/(n-k-1)}{\\text{SST}/(n-1)} = 1 - \\frac{\\text{MSE}}{\\text{MST}} = 1 - (1 - R^2) \\frac{n-1}{n-k-1} \\]\nExpectation:\nUnder \\(H_0\\), since \\(E[\\text{MSE}] = E[\\text{MST}] = \\sigma^2\\), the estimator is asymptotically unbiased:\n\\[ E[R^2_a | H_0] \\approx 0 \\]\nVariance and Stability:\nWhile \\(R^2_a\\) corrects the bias, it introduces instability. The variance of \\(R^2_a\\) under \\(H_0\\) can be derived from the variance of the Beta distribution:\n\\[ \\text{Var}(R^2_a | H_0) = \\left( \\frac{n-1}{n-k-1} \\right)^2 \\text{Var}(R^2 | H_0) \\]\nSubstituting \\(\\text{Var}(R^2 | H_0) = \\frac{2k(n-k-1)}{(n-1)^2(n+1)}\\), we obtain:\n\\[ \\text{Var}(R^2_a | H_0) = \\frac{2k}{(n-k-1)(n+1)} \\]\nKey Insight:\nAs the model complexity \\(k\\) increases relative to \\(n\\):\n\nThe denominator \\((n-k-1)\\) shrinks.\nThe variance \\(\\text{Var}(R^2_a)\\) explodes.\n\nThis implies that for high-dimensional models (large \\(k/n\\)), \\(R^2_a\\) becomes an extremely noisy estimator, often yielding large negative values even for null models.\n\n\n6.9.5 Relationship with Rao-Blackwell Decomposition of Variances\nThe formula for the expected Adjusted \\(R^2\\) reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation \\(Y_i\\) into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let \\(\\sigma^2_\\mu\\) denote the signal variance and \\(\\sigma^2\\) denote the noise variance:\n\\[ \\text{Var}(Y_i) = E[\\text{Var}(Y_i|x_{(i)})] + \\text{Var}(E[Y_i|x_{(i)}]) \\] \\[ \\sigma^2_Y = \\sigma^2 + \\sigma^2_\\mu \\]\nIn our derived expectation for \\(R^2_a\\): \\[ E[R^2_a] \\approx \\frac{\\frac{\\|X_c\\beta_1\\|^2}{n-1}}{\\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{n-1}} \\]\nThe term in the numerator, \\(\\frac{\\|X_c\\beta_1\\|^2}{n-1}\\), is precisely the sample variance of the true means \\(\\mu_i\\). Let \\(\\mu = X\\beta\\). We can expand the centered signal vector \\(X_c\\beta_1\\) to see this explicitly. Since \\(\\mu \\in \\text{Col}(X)\\), we know \\(H\\mu = \\mu\\):\n\\[\nX_c\\beta_1 = P_{X_c} \\mu = (H - P_{j_n})\\mu = H\\mu - P_{j_n}\\mu = \\mu - \\bar{\\mu}j_n =\n\\begin{pmatrix}\n\\mu_1 - \\bar{\\mu} \\\\\n\\mu_2 - \\bar{\\mu} \\\\\n\\vdots \\\\\n\\mu_n - \\bar{\\mu}\n\\end{pmatrix}\n\\]\nThis vector represents the deviation of each observation’s true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is: \\[ \\frac{\\|X_c\\beta_1\\|^2}{n-1} = \\frac{\\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2}{n-1} = \\widehat{\\sigma}^2_\\mu \\]\nIf we view the rows of the design matrix \\(X\\) as random draws \\(x_{(1)}, \\dots, x_{(n)}\\) from a population of covariate vectors, this term estimates \\(\\text{Var}(x_{(i)}'\\beta)\\), which is the variance of the signal component \\(\\sigma^2_\\mu\\).\nThus, \\(R^2_a\\) can be interpreted as a method-of-moments estimator for the proportion of variance explained by the signal in the population: \\[ E[R^2_a] \\approx \\frac{\\sigma^2_\\mu}{\\sigma^2 + \\sigma^2_\\mu} = \\frac{\\text{Var}(E[Y_i|x_{(i)}])}{\\text{Var}(Y_i)} \\]\n\n\n\n\n\n\nMSR Is Not a Variance Estimator\n\n\n\n\nObserving that \\(E[\\text{MST}] \\approx \\sigma^2 + \\sigma^2_\\mu\\) and \\(E[\\text{MSE}] = \\sigma^2\\), we can see that the difference \\(\\text{MST} - \\text{MSE}\\) provides a direct method-of-moments estimator for the variance of the signal itself (\\(\\sigma^2_\\mu\\)).\nIt is important to recognize that the commonly used Mean Square Regression (MSR), defined as \\(\\text{SSR}/k\\), is not an estimator of the signal variance. Because \\(E[\\text{MSR}] = \\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{k}\\), it scales with the sample size \\(n\\) (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting existence of signal), not for estimating the magnitude of the signal variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference in A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#relationship-between-r2-and-f-test",
    "href": "lec5-est.html#relationship-between-r2-and-f-test",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.12 Relationship between \\(R^2\\) and \\(F\\) Test",
    "text": "6.12 Relationship between \\(R^2\\) and \\(F\\) Test\nThe \\(F\\)-statistic for the overall regression effect is a monotonic function of the coefficient of determination. We can express \\(F\\) directly in terms of both the standard \\(R^2\\) and the adjusted \\(R^2_a\\), as well as relate its expected value to the population variance components.\n\nExpressing \\(F\\) via Standard \\(R^2\\): Since \\(R^2 = \\text{SSR}/\\text{SST}\\) and \\(1 - R^2 = \\text{SSE}/\\text{SST}\\), we can substitute these into the definition of \\(F\\): \\[\nF = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)} = \\frac{(R^2 \\cdot \\text{SST}) / k}{((1 - R^2) \\cdot \\text{SST}) / (n - k - 1)} = \\frac{R^2}{1 - R^2} \\cdot \\frac{n - k - 1}{k}\n\\]\nExpressing \\(F\\) via Adjusted \\(R^2_a\\): The relationship becomes structurally identical to the population expectation if we use the estimated Signal-to-Noise Ratio. Since \\(\\frac{R^2_a}{1 - R^2_a} = \\frac{\\hat{\\sigma}^2_\\mu}{\\hat{\\sigma}^2}\\), we have: \\[\nF = 1 + \\frac{n-1}{k} \\left( \\frac{R^2_a}{1 - R^2_a} \\right)\n\\] This form highlights that \\(F\\) starts at a baseline of 1 (pure noise) and increases proportional to the estimated signal strength.\nExpected Value of \\(F\\) as a function of \\(\\sigma^2_\\mu\\) and \\(\\sigma^2\\): Using the population signal variance \\(\\sigma^2_\\mu\\) and noise variance \\(\\sigma^2\\), the expected value of the \\(F\\)-statistic (using the first-order approximation \\(E[F] \\approx E[\\text{MSR}]/E[\\text{MSE}]\\)) is: \\[\nE[F] \\approx 1 + \\frac{n-1}{k} \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right)\n\\] The exact mean, derived from the non-central \\(F\\) distribution, is: \\[\nE[F] = \\frac{n-k-1}{n-k-3} \\left( 1 + \\frac{n-1}{k} \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right), \\quad \\text{for } n-k-1 &gt; 3\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#an-animation-for-illustrating-r2_a-under-h_0-and-h_1",
    "href": "lec5-est.html#an-animation-for-illustrating-r2_a-under-h_0-and-h_1",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.14 An Animation for Illustrating \\(R^2_a\\) Under \\(H_0\\) and \\(H_1\\)",
    "text": "6.14 An Animation for Illustrating \\(R^2_a\\) Under \\(H_0\\) and \\(H_1\\)\nWe simulate a dataset with \\(n=30\\) observations and consider a sequence of nested models adding groups of predictors.\nPredictor Groups:\n\nGroup 1 (\\(k=1\\)): Add \\(x_1\\). (Signal under \\(H_1\\)).\nGroup 2 (\\(k=6\\)): Add \\(x_2, \\dots, x_6\\) (Noise).\nGroup 3 (\\(k=11\\)): Add \\(x_7, \\dots, x_{11}\\) (Noise).\nGroup 4 (\\(k=20\\)): Add \\(x_{12}, \\dots, x_{20}\\) (Noise).\n\n\nNull Hypothesis (\\(H_0\\))Alternative Hypothesis (\\(H_1\\))\n\n\nUnder \\(H_0\\), the true coefficient for \\(x_1\\) is \\(\\beta_1 = 0\\). All predictors are noise.\n\n\n\n\n\nSimulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable.\n\n\n\n\nUnder \\(H_1\\), \\(x_1\\) is a true predictor (\\(\\beta_1 = 2\\)). The subsequent groups (\\(x_2 \\dots x_{20}\\)) remain noise.\n\n\n\n\n\nSimulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#a-data-example-with-house-price-valuation",
    "href": "lec5-est.html#a-data-example-with-house-price-valuation",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.15 A Data Example with House Price Valuation",
    "text": "6.15 A Data Example with House Price Valuation\nA real estate agency wants to refine their pricing model. They regress the selling price of houses (\\(y\\)) on five predictors (\\(X\\)): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.\nWe assume the data has been collected and saved to house_prices_5pred.csv.\n\n6.15.1 Visualize the Data\nFirst, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.\n\n\nCode\n# Load Data\ndf &lt;- read.csv(\"house_prices_5pred.csv\")\n\n# Conditional Display\nif (knitr::is_html_output()) {\n  rmarkdown::paged_table(df)\n} else {\n  knitr::kable(head(df, 10), caption = \"First 10 rows of House Prices\")\n}\n\n\n\n  \n\n\n\n\n\n6.15.2 Fit the Model\nWe will solve for the coefficients \\(\\hat{\\beta}\\) using three distinct methods.\n\nMethod 1: Naive Matrix Formula\nThis method solves the normal equations directly on the raw data: \\(\\hat{\\beta} = (X^{\\prime}X)^{-1}X^{\\prime}y\\).\n\n\nCode\n# 1. Define Y and X (add Column of 1s for Intercept)\ny &lt;- as.matrix(df$Price)\n# Note: \"lawn\" Is Included Here, Even Though It Is Irrelevant\nX_naive &lt;- as.matrix(cbind(Intercept = 1, \n                           df[, c(\"Size\", \"Age\", \"Beds\", \"Garage\", \"Lawn\")]))\n\n# 2. Compute Intermediate Matrices\nXtX &lt;- t(X_naive) %*% X_naive\nXty &lt;- t(X_naive) %*% y\n\n# Display Intermediate Steps\ncat(\"Matrix X'X (Cross-products of predictors):\\n\")\n\n\nMatrix X'X (Cross-products of predictors):\n\n\nCode\nprint(round(XtX, 0))\n\n\n          Intercept      Size     Age   Beds Garage     Lawn\nIntercept        60    136483    1674    206     80    29392\nSize         136483 343078981 3738402 469757 177877 63939128\nAge            1674   3738402   63528   5874   2353   827130\nBeds            206    469757    5874    776    281    98738\nGarage           80    177877    2353    281    196    41915\nLawn          29392  63939128  827130  98738  41915 19306096\n\n\nCode\ncat(\"\\nMatrix X'y (Cross-products with response):\\n\")\n\n\n\nMatrix X'y (Cross-products with response):\n\n\nCode\nprint(round(Xty, 0))\n\n\n                 [,1]\nIntercept    25884407\nSize      63115001244\nAge         694594579\nBeds         89683035\nGarage       34067413\nLawn      12402228016\n\n\nCode\n# 3. Solve Beta\nbeta_naive &lt;- solve(XtX) %*% Xty\n\n# Display Result\ncat(\"\\nSolved Coefficients (Beta):\\n\")\n\n\n\nSolved Coefficients (Beta):\n\n\nCode\nprint(t(beta_naive))\n\n\n     Intercept     Size       Age     Beds   Garage    Lawn\n[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443\n\n\n\n\nMethod 2: Centralized Formula\nThis method reduces multicollinearity issues. Formula: \\(\\hat{\\beta}_{\\text{slope}} = (X_c^{\\prime}X_c)^{-1}X_c^{\\prime}y_c\\).\n\n\nCode\n# 1. Center the Data\ny_bar &lt;- mean(y)\nX_raw &lt;- as.matrix(df[, c(\"Size\", \"Age\", \"Beds\", \"Garage\", \"Lawn\")])\nX_means &lt;- colMeans(X_raw)\n\ny_c &lt;- y - y_bar\nX_c &lt;- sweep(X_raw, 2, X_means) \n\n# 2. Compute Intermediate Matrices\nXctXc &lt;- t(X_c) %*% X_c\nXctyc &lt;- t(X_c) %*% y_c\n\n# Display Intermediate Steps\ncat(\"Matrix X_c'X_c (Centered Sum of Squares):\\n\")\n\n\nMatrix X_c'X_c (Centered Sum of Squares):\n\n\nCode\nprint(round(XctXc, 0))\n\n\n           Size    Age  Beds Garage     Lawn\nSize   32618826 -69474  1165  -4100 -2919344\nAge      -69474  16823   127    121     7093\nBeds       1165    127    69      6    -2175\nGarage    -4100    121     6     89     2726\nLawn   -2919344   7093 -2175   2726  4907935\n\n\nCode\ncat(\"\\nMatrix X_c'y_c (Centered Cross-products):\\n\")\n\n\n\nMatrix X_c'y_c (Centered Cross-products):\n\n\nCode\nprint(round(Xctyc, 0))\n\n\n             [,1]\nSize   4235309234\nAge     -27580376\nBeds       813238\nGarage    -445130\nLawn   -277680160\n\n\nCode\n# 3. Solve for Slope Coefficients\nbeta_slope &lt;- solve(XctXc) %*% Xctyc\n\n# 4. Recover Intercept\nbeta_0 &lt;- y_bar - sum(X_means * beta_slope)\nbeta_central &lt;- rbind(Intercept = beta_0, beta_slope)\n\n# Display Result\ncat(\"\\nSolved Coefficients (Beta):\\n\")\n\n\n\nSolved Coefficients (Beta):\n\n\nCode\nprint(t(beta_central))\n\n\n     Intercept     Size       Age     Beds   Garage    Lawn\n[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443\n\n\n\n\nMethod 3: Using R’s lm Function\nThis is the standard approach for practitioners.\n\n\nCode\n# Fit Model\nmodel_lm &lt;- lm(Price ~ ., data = df)\ny_hat_lm &lt;- fitted(model_lm)\n\n# Extract Coefficients\nprint(summary(model_lm))\n\n\n\nCall:\nlm(formula = Price ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-135178  -36006    1710   26401  111967 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 113185.971  35675.435   3.173  0.00249 ** \nSize           129.343      8.927  14.490  &lt; 2e-16 ***\nAge          -1218.352    386.414  -3.153  0.00264 ** \nBeds         12664.157   6064.435   2.088  0.04150 *  \nGarage         875.115   5316.490   0.165  0.86987    \nLawn            27.244     23.243   1.172  0.24629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 49360 on 54 degrees of freedom\nMultiple R-squared:  0.8161,    Adjusted R-squared:  0.799 \nF-statistic: 47.92 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n6.15.3 Visualization of Fitted Values vs Mean\nWe define \\(\\hat{y}_0\\) as the vector of the mean of \\(y\\) (\\(\\bar{y}\\)). We plot the actual \\(y\\) against our fitted model \\(\\hat{y}\\), using a green line to represent the “Null Model” (\\(\\hat{y}_0\\)).\nNote: Axes have been set so that X = Predicted Value and Y = Actual Value.\n\n\nCode\n# Define y_hat_0 (The Null Model) - for conceptual clarity\ny_hat_0 &lt;- rep(mean(y), length(y))\n\n# Scatterplot (Axes reversed: x=fitted, y=actual)\nplot(y_hat_lm, y,\n     main = \"Actual vs Fitted Prices\",\n     xlab = \"Fitted Price (y_hat)\",\n     ylab = \"Actual Price (y)\",\n     pch = 19, col = \"blue\")\n\n# Add 1:1 line (Perfect fit area, remains y=x)\nabline(0, 1, col = \"gray\", lty = 2)\n\n# Add Mean line representing the null model\n# Since y-axis is 'actual y', a horizontal line at mean(y) represents y_bar\nabline(v=mean (y), h = mean(y), col = \"green\", lwd = 2)\n\nlegend(\"topleft\", legend = c(\"Data\", \"Mean (y_bar)\"),\n       col = c(\"blue\", \"green\"), pch = c(19, NA), lty = c(NA, 1))\n\n\n\n\n\n\n\n\n\nQuestion:\n\\[ \\bar y = \\bar{\\hat{y}} ?\\]\n\n\n6.15.4 Computing Sums of Squares (SSE, SST, SSR)\nWe compare different methods to calculate the sources of variation.\n\n6.15.4.1 1. Naive Sum of Squared Errors\nThis uses the standard summation definitions: \\(\\sum (Difference)^2\\).\n\nSST (Total): Variation of \\(y\\) around \\(\\hat{y}_0\\) (Mean).\nSSR (Regression): Variation of \\(\\hat{y}\\) around \\(\\hat{y}_0\\) (Mean).\nSSE (Error): Variation of \\(y\\) around \\(\\hat{y}\\) (Model).\n\n\n\nCode\n# Vectors\ny_vec &lt;- as.vector(y)\ny_hat &lt;- as.vector(y_hat_lm)\ny_bar_vec &lt;- rep(mean(y), length(y))\n\n# Calculations\nSST_naive &lt;- sum((y_vec - y_bar_vec)^2)\nSSR_naive &lt;- sum((y_hat - y_bar_vec)^2)\nSSE_naive &lt;- sum((y_vec - y_hat)^2)\n\ncat(\"Naive Calculation:\\n\")\n\n\nNaive Calculation:\n\n\nCode\ncat(\"SST:\", SST_naive, \" SSR:\", SSR_naive, \" SSE:\", SSE_naive, \"\\n\")\n\n\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n\n\n\n\n6.15.4.2 2. Pythagorean Shortcut (Vector Lengths)\nBased on the geometry of least squares, we can treat the variables as vectors. Because the vectors are orthogonal, we can use squared lengths (dot products with themselves).\nFormula: \\(SSR = ||\\hat{y}||^2 - ||\\hat{y}_0||^2\\)\n\n\nCode\n# Function for squared Euclidean norm (length squared)\nlen_sq &lt;- function(v) sum(v^2)\n\n# SST = ||y||^2 - ||y_0||^2\nSST_pyth &lt;- len_sq(y_vec) - len_sq(y_bar_vec)\n\n# SSR = ||y_hat||^2 - ||y_0||^2\nSSR_pyth &lt;- len_sq(y_hat) - len_sq(y_bar_vec)\n\n# SSE = ||y||^2 - ||y_hat||^2\nSSE_pyth &lt;- len_sq(y_vec) - len_sq(y_hat)\n\ncat(\"Pythagorean Calculation:\\n\")\n\n\nPythagorean Calculation:\n\n\nCode\ncat(\"SST:\", SST_pyth, \" SSR:\", SSR_pyth, \" SSE:\", SSE_pyth, \"\\n\")\n\n\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n\n\n\n\n6.15.4.3 Matrix Algebra Shortcuts\nThese formulas use the \\(\\beta\\) and \\(X\\) matrices directly. This is computationally efficient for large datasets.\n\nFormula A (Centered with \\(y_c\\)): \\(SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y_c\\)\nFormula B (Alternative with \\(y\\)): \\(SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y\\)\nFormula C (Uncentered): \\(SSR = \\hat{\\beta}^{\\prime} X^{\\prime} y - n\\bar{y}^2\\)\n\n\n\nCode\nn &lt;- length(y)\nterm_correction &lt;- n * mean(y)^2 \n\n# --- SSR Calculations ---\n\n# 1. SSR Formula A (Centered, using y_c)\nSSR_centered_yc &lt;- t(beta_slope) %*% t(X_c) %*% y_c\n\n# 2. SSR Formula A (Alternative, using raw y)\n# Since X_c is centered, X_c' * 1 = 0, so X_c'y_c is equivalent to X_c'y\nSSR_centered_y &lt;- t(beta_slope) %*% t(X_c) %*% y\n\n# 3. SSR Formula B (Uncentered Matrix)\n# beta_naive includes intercept, X_naive includes column of 1s\nterm_beta_X_y &lt;- t(beta_naive) %*% t(X_naive) %*% y\nSSR_uncentered &lt;- term_beta_X_y - term_correction\n\n# --- Equivalence Check Table ---\n\nresults_table &lt;- data.frame(\n  Metric = c(\"SSR (Centered $X_c,y_c$)\", \n             \"SSR (Centered $X_c$)\", \n             \"SSR (Uncentered)\"),\n  Formula = c(\"$\\\\hat{\\\\beta}_c' X_c' y_c$\", \n              \"$\\\\hat{\\\\beta}_c' X_c' y$\", \n              \"$\\\\hat{\\\\beta}' X' y - n\\\\bar{y}^2$\"),\n  Value = c(as.numeric(SSR_centered_yc), \n            as.numeric(SSR_centered_y), \n            as.numeric(SSR_uncentered))\n)\n\n# Render the table\nknitr::kable(results_table, \n             digits = 4, \n             caption = \"Demonstration of SSR Formula Equivalence\")\n\n\n\nDemonstration of SSR Formula Equivalence\n\n\n\n\n\n\n\nMetric\nFormula\nValue\n\n\n\n\nSSR (Centered \\(X_c,y_c\\))\n\\(\\hat{\\beta}_c' X_c' y_c\\)\n583756306788\n\n\nSSR (Centered \\(X_c\\))\n\\(\\hat{\\beta}_c' X_c' y\\)\n583756306788\n\n\nSSR (Uncentered)\n\\(\\hat{\\beta}' X' y - n\\bar{y}^2\\)\n583756306788\n\n\n\n\n\n\n\n\n6.15.5 Analysis of Variance (ANOVA)\nWe now evaluate the sources of variation to test the overall model significance.\n\n1. Computing Sums of Squares\nWe calculate the following components:\n\nTotal Sum of Squares: \\(\\text{SST} = \\sum (y_i - \\bar{y})^2\\)\nRegression Sum of Squares: \\(\\text{SSR} = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nSum of Squared Errors: \\(\\text{SSE} = \\sum (y_i - \\hat{y}_i)^2\\)\n\n\n\nCode\n# Vectors\ny_vec &lt;- as.vector(y)\ny_hat &lt;- as.vector(y_hat_lm)\ny_bar_vec &lt;- rep(mean(y), length(y))\n\n# Calculations\nSST_naive &lt;- sum((y_vec - y_bar_vec)^2)\nSSR_naive &lt;- sum((y_hat - y_bar_vec)^2)\nSSE_naive &lt;- sum((y_vec - y_hat)^2)\n\ncat(\"SST:\", SST_naive, \" SSR:\", SSR_naive, \" SSE:\", SSE_naive, \"\\n\")\n\n\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n\n\n\n\n2. Manual ANOVA Construction\nWe build the table manually using the sums of squares and degrees of freedom. We calculate the Mean Squares and the F-statistic:\n\n\\(\\text{MSR} = \\text{SSR} / k\\)\n\\(\\text{MSE} = \\text{SSE} / (n - k - 1)\\)\n\\(\\text{MST} = \\text{SST} / (n - 1)\\)\n\\(F = \\text{MSR} / \\text{MSE}\\)\n\n\n\nCode\n# Parameters\nk &lt;- 5             # Predictors\ndf_e &lt;- n - k - 1  # Error DF\ndf_t &lt;- n - 1      # Total DF\n\n# Mean Squares\nMSR &lt;- SSR_naive / k\nMSE &lt;- SSE_naive / df_e\nMST &lt;- SST_naive / df_t # Mean Square Total (Variance of Y)\n\n# F-statistic\nF_stat &lt;- MSR / MSE\n\n# P-value\np_val &lt;- pf(F_stat, k, df_e, lower.tail = FALSE)\n\n# Assemble Table\nanova_manual &lt;- data.frame(\n  Source = c(\"Regression (Model)\", \"Error (Residual)\", \"Total\"),\n  DF = c(k, df_e, df_t),\n  SS = c(SSR_naive, SSE_naive, SST_naive),\n  MS = c(MSR, MSE, MST), # Included MST here\n  F_Statistic = c(F_stat, NA, NA),\n  P_Value = c(p_val, NA, NA)\n)\n\nknitr::kable(anova_manual, digits = 4, caption = \"Manual ANOVA Table\")\n\n\n\nManual ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF_Statistic\nP_Value\n\n\n\n\nRegression (Model)\n5\n583756306788\n116751261358\n47.9153\n0\n\n\nError (Residual)\n54\n131577222958\n2436615240\nNA\nNA\n\n\nTotal\n59\n715333529746\n12124297114\nNA\nNA\n\n\n\n\n\n\n\n3. Standard R Output (anova)\nWe display the standard summary() which provides the coefficients, t-tests, and the overall F-statistic found at the bottom. We also show anova() which gives the sequential sum of squares.\n\n\nCode\n# Fit an intercept-only (null) model and compare to the fitted model\nmodel_null &lt;- lm(Price ~ 1, data = df)\ncat(\"\\nANOVA comparing intercept-only model to fitted model:\\n\")\n\n\n\nANOVA comparing intercept-only model to fitted model:\n\n\nCode\nprint(anova(model_null, model_lm))\n\n\nAnalysis of Variance Table\n\nModel 1: Price ~ 1\nModel 2: Price ~ Size + Age + Beds + Garage + Lawn\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1     59 7.1533e+11                                   \n2     54 1.3158e+11  5 5.8376e+11 47.915 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# One can call anova directly to model_lm\nprint(anova(model_lm))\n\n\nAnalysis of Variance Table\n\nResponse: Price\n          Df     Sum Sq    Mean Sq  F value    Pr(&gt;F)    \nSize       1 5.4992e+11 5.4992e+11 225.6914 &lt; 2.2e-16 ***\nAge        1 2.0657e+10 2.0657e+10   8.4777  0.005216 ** \nBeds       1 9.5872e+09 9.5872e+09   3.9346  0.052396 .  \nGarage     1 2.4151e+08 2.4151e+08   0.0991  0.754107    \nLawn       1 3.3476e+09 3.3476e+09   1.3739  0.246291    \nResiduals 54 1.3158e+11 2.4366e+09                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n6.15.6 Coefficient of Determination and Variance Decomposition\nWe calculate \\(R^2\\) and Adjusted \\(R^2\\), and then present them in a Variance Decomposition Table.\n\n1. Calculation\nWe calculate the coefficients of determination:\n\nStandard \\(R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\\)\nAdjusted \\(R^2_a = 1 - \\frac{\\text{MSE}}{\\text{MST}}\\)\n\n\n\nCode\n# Standard R-squared\nR2 &lt;- 1 - (SSE_naive / SST_naive)\n\n# Adjusted R-squared\n# Formula: 1 - (MSE / MST)\nR2_adj &lt;- 1 - (MSE / MST)\n\ncat(\"Standard R^2:  \", round(R2, 4), \"\\n\")\n\n\nStandard R^2:   0.8161 \n\n\nCode\ncat(\"Adjusted R^2:  \", round(R2_adj, 4), \"\\n\")\n\n\nAdjusted R^2:   0.799 \n\n\n\n\n2. Variance Decomposition Table\nThis table extends standard ANOVA. While ANOVA focuses on Mean Squares (MS) for hypothesis testing (is \\(MSR &gt; MSE\\)?), this table focuses on Variance Components (\\(\\hat{\\sigma}^2\\)) for estimation (how much variance is Signal vs. Noise?). We estimate the variance components as follows:\n\nSignal Variance: \\(\\hat{\\sigma}^2_\\mu = \\text{MST} - \\text{MSE}\\)\nNoise Variance: \\(\\hat{\\sigma}^2 = \\text{MSE}\\)\nTotal Variance: \\(\\hat{\\sigma}^2_Y = \\text{MST}\\)\nSignal Variance (\\(\\hat{\\sigma}^2_\\mu\\)): Estimated by \\(MST - MSE\\). (Note: \\(MSR\\) is biased and overestimates signal).\nNoise Variance (\\(\\hat{\\sigma}^2\\)): Estimated by \\(MSE\\).\nTotal Variance (\\(\\hat{\\sigma}^2_Y\\)): Estimated by \\(MST\\).\n\n\n\nCode\n# Variance Component Estimators (Method of Moments)\nsigma2_noise_est  &lt;- MSE\nsigma2_total_est  &lt;- MST\nsigma2_signal_est &lt;- MST - MSE\n\n# Proportions\nprop_signal &lt;- sigma2_signal_est / sigma2_total_est # Equals R^2_adj\nprop_noise  &lt;- sigma2_noise_est / sigma2_total_est  # Equals 1 - R^2_adj\n\n# Assemble Table\ndecomp_table &lt;- data.frame(\n  Component = c(\"Signal (Model)\", \"Noise (Error)\", \"Total (Y)\"),\n  DF = c(k, df_e, df_t),\n  SS = c(SSR_naive, SSE_naive, SST_naive),\n  MS = c(NA, MSE, MST),\n  Estimator_Sigma2 = c(sigma2_signal_est, sigma2_noise_est, sigma2_total_est),\n  Proportion = c(prop_signal, prop_noise, 1.0)\n)\n\n# Display\nknitr::kable(decomp_table, \n             digits = 4, \n             col.names = c(\"Component\", \"DF\", \"SS\", \"MS\", \"Value ($\\\\hat{\\\\sigma}^2$)\", \"Proportion\"),\n             caption = \"Variance Decomposition Table: Estimating Signal vs. Noise\")\n\n\n\nVariance Decomposition Table: Estimating Signal vs. Noise\n\n\n\n\n\n\n\n\n\n\nComponent\nDF\nSS\nMS\nValue (\\(\\hat{\\sigma}^2\\))\nProportion\n\n\n\n\nSignal (Model)\n5\n583756306788\nNA\n9687681874\n0.799\n\n\nNoise (Error)\n54\n131577222958\n2436615240\n2436615240\n0.201\n\n\nTotal (Y)\n59\n715333529746\n12124297114\n12124297114\n1.000\n\n\n\n\n\n\n\n\n6.15.7 Confidence Interval for Population \\(R^2\\) (\\(\\rho^2\\))\nWe construct a 95% confidence interval for the population proportion of variance explained (\\(\\rho^2\\)).\n\n1. Manual Inversion Method\nWe solve for the non-centrality parameters \\(\\lambda_L\\) and \\(\\lambda_U\\) such that our observed \\(F_{obs}\\) corresponds to the appropriate quantiles.\n\n\nCode\n# 1. Define Helper Function to Find Lambda\n# We want to find lambda such that: pf(F_stat, df1, df2, ncp = lambda) = target_prob\nget_lambda &lt;- function(target_prob, F_val, df1, df2) {\n  f_root &lt;- function(lam) {\n    pf(F_val, df1, df2, ncp = lam) - target_prob\n  }\n  tryCatch({\n    res &lt;- uniroot(f_root, interval = c(0, 1000))$root\n    return(res)\n  }, error = function(e) return(NA))\n}\n\n# 2. Calculate Lambda Bounds (95% CI -&gt; alpha = 0.05)\nalpha &lt;- 0.05\n# Lower Bound Lambda: F_obs is the (1 - alpha/2) quantile\nlambda_Lower &lt;- get_lambda(1 - alpha/2, F_stat, k, df_e)\n# Upper Bound Lambda: F_obs is the (alpha/2) quantile\nlambda_Upper &lt;- get_lambda(alpha/2, F_stat, k, df_e)\n\nif (is.na(lambda_Lower)) lambda_Lower &lt;- 0\n\n# 3. Convert Lambda to Rho^2\n# Formula for Fixed Predictors: rho^2 = lambda / (lambda + n)\nrho2_Lower &lt;- lambda_Lower / (lambda_Lower + n)\nrho2_Upper &lt;- lambda_Upper / (lambda_Upper + n)\n\ncat(\"Manual Calculation:\\n\")\n\n\nManual Calculation:\n\n\nCode\ncat(\"95% CI for Population Rho^2: [\", round(rho2_Lower, 4), \", \", round(rho2_Upper, 4), \"]\\n\")\n\n\n95% CI for Population Rho^2: [ 0.6982 ,  0.8556 ]\n\n\n\n\n2. Using R Package MBESS\nThe MBESS package automates this procedure. We use Random.Predictors = FALSE to match the fixed-predictor assumption used in our manual calculation.\n\n\nCode\nif (requireNamespace(\"MBESS\", quietly = TRUE)) {\n  \n  # Use N (sample size) and p (number of predictors) \n  # instead of df.1/df.2 to avoid the redundancy error.\n  ci_res &lt;- MBESS::ci.R2(F.value = F_stat, \n                         p = k,      # Number of predictors\n                         N = n,      # Sample size\n                         conf.level = 0.95,\n                         Random.Predictors = FALSE)\n  \n  print(ci_res)\n  \n} else {\n  cat(\"Package 'MBESS' is not installed.\")\n}\n\n\n$Lower.Conf.Limit.R2\n[1] 0.6982442\n\n$Prob.Less.Lower\n[1] 0.025\n\n$Upper.Conf.Limit.R2\n[1] 0.8555948\n\n$Prob.Greater.Upper\n[1] 0.025",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#overfitting",
    "href": "lec5-est.html#overfitting",
    "title": "6  Inference in A Multiple Linear Regression Model",
    "section": "6.16 Overfitting",
    "text": "6.16 Overfitting\nSuppose the reduced model \\(y = X_1\\beta_1^* + e\\) is true (i.e., \\(\\beta_2 = 0\\)), but we fit the full model \\((\\dagger)\\). Since the full model includes the true model as a special case, the estimator \\(\\hat{\\beta}\\) from the full model remains unbiased.\nHowever, fitting the extraneous variables affects the variance.\n\nTheorem 6.12 (Variance Comparison) Let \\(\\hat{\\beta}_1\\) be the estimator from the full model and \\(\\hat{\\beta}_1^*\\) be the estimator from the reduced model. Then: \\[ \\text{Var}(\\hat{\\beta}_1) - \\text{Var}(\\hat{\\beta}_1^*) = \\sigma^2 A B^{-1} A^T \\] where \\(A = (X_1^T X_1)^{-1}X_1^T X_2\\) and \\(B = X_2^T X_2 - X_2^T X_1 A\\). Since \\(A B^{-1} A^T\\) is positive semidefinite, \\(\\text{Var}(\\hat{\\beta}_1) \\ge \\text{Var}(\\hat{\\beta}_1^*)\\).\n\n\nProof. Using the inverse of a partitioned matrix, the top-left block of \\((X^T X)^{-1}\\) corresponding to \\(\\beta_1\\) is: \\[ H^{11} = (X_1^T X_1)^{-1} + (X_1^T X_1)^{-1} X_1^T X_2 B^{-1} X_2^T X_1 (X_1^T X_1)^{-1} \\] Since \\(\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 H^{11}\\) and \\(\\text{Var}(\\hat{\\beta}_1^*) = \\sigma^2 (X_1^T X_1)^{-1}\\), the difference is the second term: \\[ \\text{Var}(\\hat{\\beta}_1) - \\text{Var}(\\hat{\\beta}_1^*) = \\sigma^2 A B^{-1} A^T \\] .\n\n\n6.16.1 Summary\n\nUnderfitting: Reduces variance but introduces bias (unless variables are orthogonal).\nOverfitting: Keeps estimators unbiased but increases variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimation in Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "ginv.html",
    "href": "ginv.html",
    "title": "7  Generalized Inverses",
    "section": "",
    "text": "7.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#generalized-inverses",
    "href": "ginv.html#generalized-inverses",
    "title": "7  Generalized Inverse",
    "section": "",
    "text": "7.1.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]\n\n\n7.1.2 Definition of Generalized Inverse\n\nDefinition 7.1 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 7.1 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]\n\n\n\n\n7.1.3 A Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend:\n\n\\(\\otimes\\): Elements of submatrix \\(C\\)\n\\(\\triangle\\): Elements of \\((C^{-1})'\\)\n\\(\\square\\): Elements of \\(C^{-1}\\) (after transposition)\n\\(\\times\\): Other elements (replaced by 0 in the final calculation)\n\n\n\n7.1.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]\n\n\n\n7.1.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 7.2 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\mathcal{C}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 7.2 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "ginv.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "href": "ginv.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "title": "7  Generalized Inverses",
    "section": "7.6 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse",
    "text": "7.6 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse\n\n7.6.1 Projection Matrix with Generalized Inverse of \\(X'X\\)\nFor the normal equations \\((X'X)\\beta = X'y\\), a solution is given by: \\[\n\\hat{\\beta} = (X'X)^- X'y\n\\] The fitted values are \\[\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.\\] This \\(\\hat{y}\\) represents the unique orthogonal projection of \\(y\\) onto \\(\\text{Col}(X)\\).\n\n\n7.6.2 Invariance and Uniqueness of “the” Projection Matrix\n\nTheorem 7.1 (Transpose Property of Generalized Inverses) \\((X^-)'\\) is a version of \\((X')^-\\). That is, \\((X^-)'\\) is a generalized inverse of \\(X'\\).\n\n\nProof. By definition, a generalized inverse \\(X^-\\) satisfies the property: \\[\nX X^- X = X\n\\]\nTo verify that \\((X^-)'\\) is a generalized inverse of \\(X'\\), we need to show that it satisfies the condition \\(A G A = A\\) where \\(A = X'\\) and \\(G = (X^-)'\\).\n\nStart with the fundamental definition: \\[\nX X^- X = X\n\\]\nTake the transpose of both sides of the equation: \\[\n(X X^- X)' = X'\n\\]\nApply the reverse order law for transposes, \\((ABC)' = C' B' A'\\): \\[\nX' (X^-)' X' = X'\n\\]\n\nSince substituting \\((X^-)'\\) into the generalized inverse equation for \\(X'\\) yields \\(X'\\), \\((X^-)'\\) is a valid generalized inverse of \\(X'\\).\n\n\nLemma 7.1 (Invariance of Generalized Least Squares) For any version of the generalized inverse \\((X'X)^-\\), the matrix \\(X'(X'X)^- X'\\) is invariant and equals \\(X'\\). \\[\nX'X(X'X)^- X' = X'\n\\]\n\nProof (using Projection): Let \\(P = X(X'X)^- X'\\). This is the projection matrix onto \\(\\text{Col}(X)\\). By definition of projection, \\(Px = x\\) for any \\(x \\in \\text{Col}(X)\\). Since columns of \\(X\\) are in \\(\\text{Col}(X)\\), \\(PX = X\\). Taking the transpose: \\((PX)' = X' \\implies X'P' = X'\\). Since projection matrices are symmetric (\\(P=P'\\)), \\(X'P = X'\\). Substituting \\(P\\): \\(X' X (X'X)^- X' = X'\\).\nProof (Direct Matrix Manipulation): Decompose \\(y = X\\beta + e\\) where \\(e \\perp \\text{Col}(X)\\) (i.e., \\(X'e = 0\\)). \\[\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n\\] Using the property \\(A A^- A = A\\) (where \\(A=X'X\\)), the first term becomes \\(X'X\\beta\\). The second term is 0 because \\(X'e = 0\\). Thus, the expression simplifies to \\(X'X\\beta = X'(X\\beta) = X'\\hat{y}_{\\text{proj}}\\). This implies the operator acts as \\(X'\\).\n\nTheorem 7.2 (Properties of Projection Matrix \\(P\\)) Let \\(P = X(X'X)^- X'\\). This matrix has the following properties:\n\nSymmetry: \\(P = P'\\).\nIdempotence: \\(P^2 = P\\). \\[\nP^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n\\] Using the identity from Lemma 7.1 (\\(X'X(X'X)^- X' = X'\\)), this simplifies to: \\[\nX(X'X)^- X' = P\n\\]\nUniqueness: \\(P\\) is unique and invariant to the choice of the generalized inverse \\((X'X)^-\\).\n\n\n\nProof. Proof of Uniqueness:\nLet \\(A\\) and \\(B\\) be two different generalized inverses of \\(X'X\\). Define \\(P_A = X A X'\\) and \\(P_B = X B X'\\). From Lemma 7.1, we know that \\(X' P_A = X'\\) and \\(X' P_B = X'\\).\nSubtracting these two equations: \\[\nX' (P_A - P_B) = 0\n\\] Taking the transpose, we get \\((P_A - P_B) X = 0\\). This implies that the columns of the difference matrix \\(D = P_A - P_B\\) are orthogonal to the columns of \\(X\\) (i.e., \\(D \\perp \\text{Col}(X)\\)).\nHowever, by definition, the columns of \\(P_A\\) and \\(P_B\\) (and thus \\(D\\)) are linear combinations of the columns of \\(X\\) (i.e., \\(D \\in \\text{Col}(X)\\)).\nThe only matrix that lies in the column space of \\(X\\) but is also orthogonal to the column space of \\(X\\) is the zero matrix. Therefore: \\[\nP_A - P_B = 0 \\implies P_A = P_B\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "href": "ginv.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "title": "7  Generalized Inverses",
    "section": "7.7 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)",
    "text": "7.7 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)\nWhile the geometric properties of the linear model are most naturally established via the unique orthogonal projection \\(\\hat{y}\\), we require a functional mapping—a statistical “bridge”—to translate the distribution of these fitted values back into the parameter space of \\(\\hat{\\beta}\\). This bridge is provided by the generalized left inverse.\n\n7.7.1 The Generalized Left Inverse\nTo recover the parameter estimates directly from the fitted values, we define the generalized left inverse, denoted as \\(X_{\\text{left}}^-\\), such that:\n\\[\n\\hat{\\beta} = X_{\\text{left}}^- \\hat{y}\n\\]\nA standard choice for this operator, derived from the normal equations, is:\n\\[\nX_{\\text{left}}^- = (X' X)^- X'\n\\]\nWhen \\(X\\) is full-rank, the \\(X_{\\text{left}}^-\\) is unique, which is given by\n\\[\nX_{\\text{left}}^- = (X' X)^{-1} X'\n\\]\n\n\n7.7.2 Verification of the Inverse Property\nTo verify that \\(X_{\\text{left}}^-\\) acts as a valid generalized inverse of \\(X\\), it must satisfy the condition \\(X X_{\\text{left}}^- X = X\\). Substituting our definition:\n\\[\nX \\underbrace{\\left[ (X' X)^- X' \\right]}_{X_{\\text{left}}^-} X = X (X' X)^- (X' X)\n\\]\nUsing the property of generalized inverses for symmetric matrices where \\((X' X)(X' X)^- X' = X'\\), the transpose of this identity gives \\(X (X' X)^- (X' X) = X\\). Thus, the condition holds:\n\\[\nX X_{\\text{left}}^- X = X\n\\]\n\n\n7.7.3 Recovering the Estimator\nWe can now demonstrate that applying this left inverse to the fitted values \\(\\hat{y}\\) yields the standard solution to the normal equations.\nSubstituting the projection formula \\(\\hat{y} = X(X' X)^- X' y\\):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\left[ (X' X)^- X' \\right] \\left[ X(X' X)^- X' y \\right] \\\\\n&= (X' X)^- \\underbrace{(X' X) (X' X)^- (X' X)}_{\\text{Property } A A^- A = A} (X' X)^- X' y\n\\end{aligned}\n\\]\nSimplifying using the generalized inverse property \\(A^- A A^- = A^-\\) (where \\(A = X' X\\)):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\underbrace{(X' X)^- (X' X) (X' X)^-}_{(X' X)^-} X' y \\\\\n&= (X' X)^- X' y\n\\end{aligned}\n\\]\nThus, we recover the standard estimator used in the normal equations:\n\\[\n\\mathbf{\\hat{\\beta} = (X' X)^- X' y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#non-full-rank-least-squares-with-qr-decomposition",
    "href": "ginv.html#non-full-rank-least-squares-with-qr-decomposition",
    "title": "7  Generalized Inverses",
    "section": "7.8 Non-full-rank Least Squares with QR Decomposition",
    "text": "7.8 Non-full-rank Least Squares with QR Decomposition\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n7.8.1 Constructing a Solution by Solving Normal Equations\nOne specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n7.8.2 Constructing a Solution by Solving Reparametrized \\(\\beta\\)\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\nSolution 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#singular-value-decomposition-svd",
    "href": "lec2-matrix.html#singular-value-decomposition-svd",
    "title": "3  Matrix Algebra",
    "section": "3.3 Singular Value Decomposition (SVD)",
    "text": "3.3 Singular Value Decomposition (SVD)\n\nTheorem 3.5 (Singular Value Decomposition (SVD)) Let \\(X\\) be an \\(n \\times p\\) matrix with rank \\(r \\le \\min(n, p)\\). \\(X\\) can be decomposed into the product of three matrices:\n\\[\nX = U \\mathbf{D} V'\n\\]\n1. Partitioned Matrix Form\n\\[\nX = \\underset{n \\times n}{(U_1, U_2)}\n\\begin{pmatrix}\n\\Lambda_r & O_{r \\times (p-r)} \\\\\nO_{(n-r) \\times r} & O_{(n-r) \\times (p-r)}\n\\end{pmatrix}\n\\underset{p \\times p}{\n\\begin{pmatrix}\nV_1' \\\\\nV_2'\n\\end{pmatrix}\n}\n\\]\n2. Detailed Matrix Form\nExpanding the diagonal matrix explicitly:\n\\[\nX = \\underset{n \\times n}{(u_1, \\dots, u_n)}\n\\left(\n\\begin{array}{cccc|c}\n\\lambda_1 & 0 & \\dots & 0 &  \\\\\n0 & \\lambda_2 & \\dots & 0 & O_{12} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots &  \\\\\n0 & 0 & \\dots & \\lambda_r &  \\\\\n\\hline\n& O_{21} & & & O_{22}\n\\end{array}\n\\right)\n\\underset{p \\times p}{\n\\begin{pmatrix}\nv_1' \\\\\n\\vdots \\\\\nv_p'\n\\end{pmatrix}\n}\n\\]\n3. Reduced Form\n\\[\nX = U_1 \\Lambda_r V_1' = \\sum_{i=1}^r \\lambda_i u_i v_i'\n\\]\nProperties:\n\nSingular Values (\\(\\Lambda_r\\)): \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values (\\(\\lambda_i &gt; 0\\)), which are the square roots of the non-zero eigenvalues of \\(X'X\\).\nOrthogonality:\n\n\\(U\\) is \\(n \\times n\\) orthogonal (\\(U'U = I_n\\)).\n\\(V\\) is \\(p \\times p\\) orthogonal (\\(V'V = I_p\\)).\n\n\n\n\n3.3.0.1 Connection to Gram Matrices\nThe matrices \\(U\\) and \\(V\\) provide the basis vectors (eigenvectors) for the Gram matrices of \\(X\\).\n\nRight Singular Vectors (\\(V\\)): The columns of \\(V\\) are the eigenvectors of the Gram matrix \\(X'X\\). \\[\nX'X = (U \\Lambda V')' (U \\Lambda V') = V \\Lambda U' U \\Lambda V' = V \\Lambda^2 V'\n\\]\n\nThe eigenvalues of \\(X'X\\) are the squared singular values \\(\\lambda_i^2\\).\n\nLeft Singular Vectors (\\(U\\)): The columns of \\(U\\) are the eigenvectors of the Gram matrix \\(XX'\\). \\[\nXX' = (U \\Lambda V') (U \\Lambda V')' = U \\Lambda V' V \\Lambda U' = U \\Lambda^2 U'\n\\]\n\nThe eigenvalues of \\(XX'\\) are also \\(\\lambda_i^2\\) (for non-zero values).\n\n\n\n\n3.3.0.2 Numerical Example\nConsider the matrix \\(X = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\\).\n\nCompute \\(X'X\\) and find \\(V\\): \\[\nX'X = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 5 \\\\ 5 & 5 \\end{pmatrix}\n\\]\n\nEigenvalues of \\(X'X\\): Trace is 10, Determinant is 0. Thus, \\(\\mu_1 = 10, \\mu_2 = 0\\).\nSingular Values: \\(\\lambda_1 = \\sqrt{10}, \\lambda_2 = 0\\).\nEigenvector for \\(\\mu_1=10\\): Normalized \\(v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\).\n\nCompute \\(XX'\\) and find \\(U\\): \\[\nXX' = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n\\]\n\nEigenvalues are again 10 and 0.\nEigenvector for \\(\\mu_1=10\\): Normalized \\(u_1 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(U = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & 2 \\\\ 2 & -1 \\end{pmatrix}\\).\n\nVerification: \\[\nX = \\sqrt{10} u_1 v_1' = \\sqrt{10} \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#raw-coefficient-of-determination-r2",
    "href": "lec5-est.html#raw-coefficient-of-determination-r2",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.9 Raw Coefficient of Determination (\\(R^2\\))",
    "text": "6.9 Raw Coefficient of Determination (\\(R^2\\))\n\n6.9.1 Definition\nThe \\(R^2\\) statistic measures the proportion of total variation explained by the regression model.\n\nDefinition 6.4 (R-Squared) \\[R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\\] Since \\(0 \\le \\text{SSE} \\le \\text{SST}\\), it follows that \\(0 \\le R^2 \\le 1\\).\n\n\n\n6.9.2 Expectation and Bias\nTo understand the bias in \\(R^2\\), it is more illuminating to analyze the expectation of the unexplained variance (\\(1 - R^2\\)). This term represents the ratio of error sum of squares to the total sum of squares:\n\\[ E[1 - R^2] = E\\left[ \\frac{\\text{SSE}}{\\text{SST}} \\right] \\]\nUsing the first-order approximation \\(E[X/Y] \\approx E[X]/E[Y]\\), we examine the numerator and denominator separately:\n\\[\n\\begin{aligned}\nE[\\text{SSE}] &= \\sigma^2(n-k-1) \\\\\nE[\\text{SST}] &= \\sigma^2(n-1) + \\sigma^2\\lambda = \\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)\n\\end{aligned}\n\\]\nSubstituting these back, we approximate the expected unexplained fraction:\n\\[ E[1 - R^2] \\approx \\frac{\\sigma^2(n-k-1)}{\\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)} = \\frac{n-k-1}{(n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}} \\]\nBehavior under Null Hypothesis (\\(H_0\\)): When there is no true signal (\\(\\beta_1 = 0\\)), the term \\(\\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}\\) vanishes. The expected proportion of unexplained variance becomes:\n\\[ E[1 - R^2 | H_0] \\approx \\frac{n-k-1}{n-1} \\]\nThis result reveals the source of the bias:\n\nIdeally, if predictors are noise, the model should explain nothing, and \\(E[1-R^2]\\) should be \\(1\\).\nInstead, the expected error ratio is less than 1, specifically scaled by \\(\\frac{n-k-1}{n-1}\\).\nThis scaling factor is exactly what the Adjusted R-squared (\\(R^2_a\\)) attempts to correct by multiplying the observed ratio by the inverse \\(\\frac{n-1}{n-k-1}\\).\n\n\n\n6.9.3 Exact Distribution\nThe \\(R^2\\) statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.\n\nTheorem 6.11 (Distribution of R-Squared) \\[ R^2 \\sim \\text{Beta}_1\\left( \\frac{k}{2}, \\frac{n-k-1}{2}, \\lambda \\right) \\] where \\(\\text{df}_1 = k\\) and \\(\\text{df}_2 = n-k-1\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#adjusted-r-squared-r2_a",
    "href": "lec5-est.html#adjusted-r-squared-r2_a",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.10 Adjusted R-squared (\\(R^2_a\\))",
    "text": "6.10 Adjusted R-squared (\\(R^2_a\\))\nTo correct for the inflation of \\(R^2\\) due to model complexity (\\(k\\)), we introduce the Adjusted \\(R^2\\). This statistic penalizes the sum of squares by their degrees of freedom:\n\\[ R^2_a = 1 - \\frac{\\text{SSE}/(n-k-1)}{\\text{SST}/(n-1)} = 1 - \\frac{\\text{MSE}}{\\text{MST}} = 1 - (1 - R^2) \\frac{n-1}{n-k-1} \\]\nExpectation:\nUnder \\(H_0\\), since \\(E[\\text{MSE}] = E[\\text{MST}] = \\sigma^2\\), the estimator is asymptotically unbiased:\n\\[ E[R^2_a | H_0] \\approx 0 \\]\nVariance and Stability:\nWhile \\(R^2_a\\) corrects the bias, it introduces instability. The variance of \\(R^2_a\\) under \\(H_0\\) can be derived from the variance of the Beta distribution:\n\\[ \\text{Var}(R^2_a | H_0) = \\left( \\frac{n-1}{n-k-1} \\right)^2 \\text{Var}(R^2 | H_0) \\]\nSubstituting \\(\\text{Var}(R^2 | H_0) = \\frac{2k(n-k-1)}{(n-1)^2(n+1)}\\), we obtain:\n\\[ \\text{Var}(R^2_a | H_0) = \\frac{2k}{(n-k-1)(n+1)} \\]\nKey Insight:\nAs the model complexity \\(k\\) increases relative to \\(n\\):\n\nThe denominator \\((n-k-1)\\) shrinks.\nThe variance \\(\\text{Var}(R^2_a)\\) explodes.\n\nThis implies that for high-dimensional models (large \\(k/n\\)), \\(R^2_a\\) becomes an extremely noisy estimator, often yielding large negative values even for null models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#population-proportion-of-signals-rho2",
    "href": "lec5-est.html#population-proportion-of-signals-rho2",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.11 Population Proportion of Signals (\\(\\rho^2\\))",
    "text": "6.11 Population Proportion of Signals (\\(\\rho^2\\))\nThe formula for the expected Adjusted \\(R^2\\) reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation \\(Y_i\\) into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let \\(\\sigma^2_\\mu\\) denote the signal variance and \\(\\sigma^2\\) denote the noise variance:\n\\[ \\text{Var}(Y_i) = E[\\text{Var}(Y_i|x_{(i)})] + \\text{Var}(E[Y_i|x_{(i)}]) \\] \\[ \\sigma^2_Y = \\sigma^2 + \\sigma^2_\\mu \\]\nIn our derived expectation for \\(R^2_a\\): \\[ E[R^2_a] \\approx \\frac{\\frac{\\|X_c\\beta_1\\|^2}{n-1}}{\\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{n-1}} \\]\nThe term in the numerator, \\(\\frac{\\|X_c\\beta_1\\|^2}{n-1}\\), is precisely the sample variance of the true means \\(\\mu_i\\). Let \\(\\mu = X\\beta\\). We can expand the centered signal vector \\(X_c\\beta_1\\) to see this explicitly. Since \\(\\mu \\in \\text{Col}(X)\\), we know \\(H\\mu = \\mu\\):\n\\[\nX_c\\beta_1 = P_{X_c} \\mu = (H - P_{j_n})\\mu = H\\mu - P_{j_n}\\mu = \\mu - \\bar{\\mu}j_n =\n\\begin{pmatrix}\n\\mu_1 - \\bar{\\mu} \\\\\n\\mu_2 - \\bar{\\mu} \\\\\n\\vdots \\\\\n\\mu_n - \\bar{\\mu}\n\\end{pmatrix}\n\\]\nThis vector represents the deviation of each observation’s true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is: \\[ \\frac{\\|X_c\\beta_1\\|^2}{n-1} = \\frac{\\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2}{n-1} = \\sigma^2_\\mu \\]\nThus, \\(R^2_a\\) is therefore an unbiased estimator for the proportion of variance explained by the signal in the population: \\[ E[R^2_a] \\approx \\frac{\\sigma^2_\\mu}{\\sigma^2 + \\sigma^2_\\mu}\\]\nWe will denote this ‘parameter’ by \\(\\rho^2\\):\n\\[ \\rho^2 = 1 - \\frac{\\sigma^2}{\\sigma^2_Y} = \\frac{\\sigma^2_\\mu}{\\sigma^2_Y} \\]\n\nRemark. In the fixed covariate framework, the ‘parameter’ \\(\\rho^2\\) is a function of the specific design matrix \\(X\\), the coefficients \\(\\beta\\), and the sample size \\(n\\). If we assume the \\(x_i\\) are random draws from a population, then as \\(n \\to \\infty\\), \\(\\sigma^2_\\mu\\) converges to \\(\\text{Var}(x^T\\beta)\\) (where \\(x\\) is a random vector), and \\(\\rho^2\\) converges to the true population proportion of variance explained.\n\n\n\n\n\n\n\nMSR Is Not a Variance Estimator\n\n\n\n\nObserving that \\(E[\\text{MST}] \\approx \\sigma^2 + \\sigma^2_\\mu\\) and \\(E[\\text{MSE}] = \\sigma^2\\), we can see that the difference \\(\\text{MST} - \\text{MSE}\\) provides a direct method-of-moments estimator for the variance of the signal itself (\\(\\sigma^2_\\mu\\)).\nIt is important to recognize that the commonly used Mean Square Regression (MSR), defined as \\(\\text{SSR}/k\\), is not an estimator of the signal variance. Because \\(E[\\text{MSR}] = \\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{k}\\), it scales with the sample size \\(n\\) (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting existence of signal), not for estimating the magnitude of the signal variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#decomposition-of-sum-of-squares",
    "href": "lec5-est.html#decomposition-of-sum-of-squares",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.7 Decomposition of Sum of Squares",
    "text": "6.7 Decomposition of Sum of Squares\nWe partition the total variation based on the orthogonal subspaces.\n\nDefinition 6.3 (Sum of Squares Components) The total variation is decomposed as \\(\\text{SST} = \\text{SSR} + \\text{SSE}\\).\n\nTotal Sum of Squares (SST): The squared length of the centered response vector. \\[\\text{SST} = \\|y - \\bar{y}j_n\\|^2 = \\|(I - P_{j_n})y\\|^2\\]\nRegression Sum of Squares (SSR): The variation explained by the regressors \\(X_c\\). \\[\\text{SSR} = \\|\\hat{y} - \\bar{y}j_n\\|^2 = \\|P_{X_c}y\\|^2 = \\hat{\\beta}_1' X_c' X_c \\hat{\\beta}_1\\]\nSum of Squared Errors (SSE): The residual variation. \\[\\text{SSE} = \\|y - \\hat{y}\\|^2 = \\|(I - H)y\\|^2\\]\n\n\n\n6.7.1 3D Visualization of Decomposition of \\(y\\)\nWe partition the total variation in \\(y\\) based on the orthogonal subspaces.\n\nSpace of the Mean: \\(L(j_n)\\), spanned by the intercept vector \\(j_n\\).\nSpace of the Regressors: \\(L(X_c)\\), spanned by the centered predictors \\(X_c\\).\nError Space: \\(\\text{Col}(X)^\\perp\\), orthogonal to the model space.\n\nThe vector \\(y\\) can be decomposed into three orthogonal components: \\[y = \\bar{y}j_n + P_{X_c}y + (y - \\hat{y})\\] Visually, this corresponds to projecting the vector \\(y\\) onto three orthogonal axes.\nInteractive Visualization:\nWe generate a cloud of 100 observations of \\(y\\) from \\(N(\\mu, \\sigma=1)\\) where \\(\\mu = (5,5,0)\\). The projections onto the Model Plane (\\(z=0\\)) are highlighted in red, and the projections onto the error axis (\\(z\\)) are in yellow.\n\nEffect Exists (signal)No Effect (noise)\n\n\n\n\n\n\nScenario 1: Significant regression effect (\\(\\beta_1\not= 0\\)). The mean vector projects significantly onto the predictor space.\n\n\n\n\n\n\n\n\nScenario 2: No regression effect (\\(\\beta_1 = 0\\)). The mean vector lies purely on the intercept axis.\n\n\n\n\n\n\n\n6.7.2 A Diagram to Show Decomposition of Sum of Squares\nThe decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).\n\n\n\n\n\n\n\n\nFigure 6.1: Geometric Decomposition: SST = SSR + SSE\n\n\n\n\n\n\n\n6.7.3 Distribution of Sum of Squares\nWe apply the general theory of projections to the specific components defined in Definition 6.3.\n\nTheorem 6.9 (Distribution of Sum of Squares) Let \\(y \\sim N(\\mu, \\sigma^2 I_n)\\), where \\(\\mu \\in \\text{Col}(X)\\). Consider the decomposition defined by the projection matrices \\(P_{X_c}\\) and \\(M = I - H\\).\n\nIndependence: The quadratic forms \\(\\text{SSR}\\) and \\(\\text{SSE}\\) are statistically independent because the subspaces \\(L(X_c)\\) and \\(\\text{Col}(X)^\\perp\\) are orthogonal.\nDistribution of SSE: The scaled sum of squared errors follows a central Chi-squared distribution: \\[ \\frac{\\text{SSE}}{\\sigma^2} = \\frac{\\|(I - H)y\\|^2}{\\sigma^2} \\sim \\chi^2(n-k-1) \\] Mean: \\[ E[\\text{SSE}] = \\sigma^2(n-k-1) \\]\nDistribution of SSR: The scaled regression sum of squares follows a non-central Chi-squared distribution: \\[ \\frac{\\text{SSR}}{\\sigma^2} = \\frac{\\|P_{X_c}y\\|^2}{\\sigma^2} \\sim \\chi^2(k, \\lambda) \\] Mean: \\[ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 \\]\n\nNon-centrality Parameter (\\(\\lambda\\)): \\[ \\lambda = \\frac{1}{\\sigma^2} \\|P_{X_c} \\mu\\|^2 \\] where \\[\\|P_{X_c} \\mu\\|^2 = \\|X_c \\beta_1\\|^2 = (X_c \\beta_1)' (X_c \\beta_1) = \\beta_1' X_c' X_c \\beta_1\\]\n\n\nProof. We apply Theorem 5.8 to the specific projection matrices identified in the definitions.\n\nFor SSE (Error Space): \\(\\text{SSE}\\) is defined by the projection matrix \\(P_V = I - H\\).\n\nDimension: The rank of \\((I - H)\\) is \\(n - \\text{rank}(X) = n - (k+1) = n - k - 1\\).\nNon-centrality: Since \\(\\mu \\in \\text{Col}(X)\\), the projection onto the orthogonal complement is zero: \\(\\|(I - H)\\mu\\|^2 = 0\\). Thus, \\(\\lambda = 0\\).\nExpectation: Using Part 2 of Theorem 5.8 (\\(E(\\|P_V y\\|^2) = \\sigma^2 \\text{rank}(P_V) + \\|P_V \\mu\\|^2\\)): \\[ E[\\text{SSE}] = \\sigma^2(n-k-1) + 0 = \\sigma^2(n-k-1) \\]\n\nFor SSR (Regression Space): \\(\\text{SSR}\\) is defined by the projection matrix \\(P_V = P_{X_c}\\).\n\nDimension: The rank of \\(P_{X_c}\\) is \\((k+1) - 1 = k\\).\nNon-centrality: The projection of \\(\\mu\\) onto \\(L(X_c)\\) is \\(P_{X_c}\\mu\\). \\[ \\lambda = \\frac{1}{2\\sigma^2} \\|P_{X_c} \\mu\\|^2 \\]\nExpectation: Using Part 2 of Theorem 5.8: \\[ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 \\]\n\nThis shows that while \\(E[\\text{SSE}]\\) depends only on the noise variance and sample size, \\(E[\\text{SSR}]\\) is inflated by the magnitude of the true regression signal \\(\\|P_{X_c}\\mu\\|^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#confidence-interval-of-population-rho2",
    "href": "lec5-est.html#confidence-interval-of-population-rho2",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.13 Confidence Interval of Population \\(\\rho^2\\)",
    "text": "6.13 Confidence Interval of Population \\(\\rho^2\\)\nWhile \\(R^2_a\\) provides a point estimate, we can construct an exact confidence interval for \\(\\rho^2\\) by exploiting the distribution of the \\(F\\)-statistic.\n1. The link between \\(\\lambda\\) and \\(\\rho^2\\):\nRecall that the \\(F\\)-statistic follows a non-central distribution \\(F(k, n-k-1, \\lambda)\\). The non-centrality parameter \\(\\lambda\\) is directly related to the population \\(\\rho^2\\). Using the variance decomposition derived above:\n\\[ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} = (n-1) \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right) \\]\nSubstituting the signal-to-noise ratio \\(\\frac{\\sigma^2_\\mu}{\\sigma^2} = \\frac{\\rho^2}{1-\\rho^2}\\), we obtain a one-to-one mapping between \\(\\lambda\\) and \\(\\rho^2\\):\n\\[ \\lambda(\\rho^2) = (n-1) \\left( \\frac{\\rho^2}{1-\\rho^2} \\right) \\]\n2. Inverting the Test Statistic:\nWe find a confidence interval \\([\\lambda_L, \\lambda_U]\\) for \\(\\lambda\\) by “inverting” the observed \\(F\\)-statistic (\\(F_{obs}\\)). We search for two specific non-central F-distributions: one where \\(F_{obs}\\) cuts off the upper \\(\\alpha/2\\) tail, and one where it cuts off the lower \\(\\alpha/2\\) tail.\n\nLower Bound (\\(\\lambda_L\\)): The non-centrality parameter such that \\(F_{obs}\\) is the \\(1-\\alpha/2\\) quantile.\nUpper Bound (\\(\\lambda_U\\)): The non-centrality parameter such that \\(F_{obs}\\) is the \\(\\alpha/2\\) quantile.\n\nThis concept is illustrated in the figure below.\n\n\n\n\n\n\n\n\nFigure 6.3: Illustration of constructing a confidence interval for the non-centrality parameter \\(\\lambda\\) by inverting the F-test. The observed \\(F_{obs}\\) (dashed line) is the \\(97.5^{th}\\) percentile of the distribution defined by the lower bound \\(\\lambda_L\\) (blue), and the \\(2.5^{th}\\) percentile of the distribution defined by the upper bound \\(\\lambda_U\\) (red). The shaded areas each represent \\(\\alpha/2\\).\n\n\n\n\n\n3. The Interval for \\(\\rho^2\\):\nOnce \\([\\lambda_L, \\lambda_U]\\) are found numerically, we map them back to the population \\(R^2\\) scale using the inverse relationship:\n\\[ \\rho^2 = \\frac{\\lambda}{\\lambda + (n-1)} \\]\nThis produces an exact confidence interval \\([\\rho^2_L, \\rho^2_U]\\) for the proportion of variance explained by the model in the population.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#underfitting-and-overfitting",
    "href": "lec5-est.html#underfitting-and-overfitting",
    "title": "6  Inference for A Multiple Linear Regression Model",
    "section": "6.16 Underfitting and Overfitting",
    "text": "6.16 Underfitting and Overfitting\nWe compare the properties of two competing estimators for the mean response vector \\(\\mu = E[y]\\).\n\n6.16.1 Notation and Setup\nWe consider the general linear model: \\[\ny = X\\beta + e = X_1\\beta_1 + X_2\\beta_2 + e\n\\] where \\(X_1\\) is \\(n \\times p_1\\), \\(X_2\\) is \\(n \\times p_2\\), and \\(\\text{Var}(e) = \\sigma^2 I\\).\nWe distinguish between two estimation approaches based on this model:\n1. Full Model (\\(M_1\\)) We estimate \\(\\beta\\) without restrictions. The estimator projects \\(y\\) onto the full column space \\(\\text{Col}(X)\\). \\[\n\\begin{aligned}\nP_1 &= X(X^T X)^{-1}X^T & (\\text{Projection onto } \\text{Col}(X)) \\\\\n\\hat{y}_1 &= P_1 y & (\\text{Unrestricted Estimator})\n\\end{aligned}\n\\]\n2. Reduced Model (\\(M_0\\)) We estimate \\(\\beta\\) subject to the constraint: \\[\nM_0: \\beta_2 = 0\n\\] This effectively reduces the model to \\(y = X_1\\beta_1 + e\\), projecting \\(y\\) onto the subspace \\(\\text{Col}(X_1)\\). \\[\n\\begin{aligned}\nP_0 &= X_1(X_1^T X_1)^{-1}X_1^T & (\\text{Projection onto } \\text{Col}(X_1)) \\\\\n\\hat{y}_0 &= P_0 y & (\\text{Restricted Estimator})\n\\end{aligned}\n\\]\nKey Geometric Property: Since the constraint \\(\\beta_2=0\\) restricts the estimation to a subspace (\\(\\text{Col}(X_1) \\subset \\text{Col}(X)\\)), we have the nesting property: \\[\nP_1 P_0 = P_0 \\quad \\text{and} \\quad P_1 - P_0 \\text{ is a projection matrix.}\n\\]\n\n\n6.16.2 Case 1: Underfitting\nThe Truth: The Full Model (\\(M_1\\)) is correct. \\[\ny = X_1\\beta_1 + X_2\\beta_2 + e, \\quad \\beta_2 \\neq 0\n\\] The true mean is \\(\\mu = X_1\\beta_1 + X_2\\beta_2\\).\nWe analyze the properties of the Reduced Estimator \\(\\hat{y}_0\\) (from \\(M_0\\)) compared to the correct Full Estimator \\(\\hat{y}_1\\) (from \\(M_1\\)).\n\nTheorem 6.12 (Bias-Variance Tradeoff in Underfitting) When \\(M_1\\) is true:\n\nBias: The estimator \\(\\hat{y}_0\\) is biased, while \\(\\hat{y}_1\\) is unbiased. \\[ \\text{Bias}(\\hat{y}_0) = -(I - P_0) X_2 \\beta_2 \\]\nVariance: The estimator \\(\\hat{y}_0\\) has smaller variance (matrix difference is positive semidefinite). \\[ \\text{Var}(\\hat{y}_1) - \\text{Var}(\\hat{y}_0) = \\sigma^2 (P_1 - P_0) \\ge 0 \\]\n\n\n\nProof. Part 1 (Bias): \\[\n\\begin{aligned}\nE[\\hat{y}_0] &= P_0 E[y] = P_0(X_1\\beta_1 + X_2\\beta_2) \\\\\n&= X_1\\beta_1 + P_0 X_2 \\beta_2 \\quad (\\text{Since } P_0 X_1 = X_1)\n\\end{aligned}\n\\] The bias is: \\[\n\\text{Bias} = E[\\hat{y}_0] - \\mu = (X_1\\beta_1 + P_0 X_2 \\beta_2) - (X_1\\beta_1 + X_2\\beta_2) = -(I - P_0)X_2\\beta_2\n\\]\nPart 2 (Variance): \\[\n\\text{Var}(\\hat{y}_1) = \\sigma^2 P_1, \\quad \\text{Var}(\\hat{y}_0) = \\sigma^2 P_0\n\\] The difference is \\(\\sigma^2(P_1 - P_0)\\). Since \\(\\text{Col}(X_1) \\subset \\text{Col}(X)\\), the difference \\(P_1 - P_0\\) projects onto the orthogonal complement of \\(\\text{Col}(X_1)\\) within \\(\\text{Col}(X)\\). It is idempotent and positive semidefinite.\n\nRemark: Scalar Variance and Coefficients\nFrom the matrix inequality above, we can state that for any arbitrary vector \\(a\\), the scalar variance of the linear combination \\(a^T \\hat{y}\\) is always smaller in the reduced model: \\[\n\\text{Var}(a^T \\hat{y}_0) \\le \\text{Var}(a^T \\hat{y}_1)\n\\]\nWe can extend this property to the regression coefficients \\(\\hat{\\beta}\\). Since \\(\\hat{y} = X\\hat{\\beta}\\), we can recover the coefficients from the fitted values using the left pseudo-inverse:\n\\[\n\\begin{aligned}\n(X^T X)^{-1}X^T (X\\hat{\\beta}) &= (X^T X)^{-1}X^T \\hat{y} \\\\\n\\underbrace{(X^T X)^{-1}(X^T X)}_{I} \\hat{\\beta} &= (X^T X)^{-1}X^T \\hat{y}\n\\end{aligned}\n\\]\n\nCorollary 6.3 (Variance of Coefficients) Because \\(\\hat{\\beta}\\) is a linear transformation of \\(\\hat{y}\\), the variance reduction in \\(\\hat{y}_0\\) propagates to the coefficients.\nFor any specific coefficient \\(\\beta_j\\) included in the reduced model (i.e., \\(\\beta_j \\in \\beta_1\\)), the variance of the estimator is smaller in the reduced model than in the full model: \\[ \\text{Var}(\\hat{\\beta}_{j, reduced}) \\le \\text{Var}(\\hat{\\beta}_{j, full}) \\]\n\nConclusion: Using \\(M_0\\) when \\(M_1\\) is true introduces bias but reduces variance for both the fitted values and the estimated coefficients.\n\n\n6.16.3 Case 2: Overfitting\nThe Truth: The Reduced Model (\\(M_0\\)) is correct. \\[\ny = X_1\\beta_1 + e \\quad (\\text{i.e., } \\beta_2 = 0)\n\\] The true mean is \\(\\mu = X_1\\beta_1\\).\nWe analyze the properties of the Full Estimator \\(\\hat{y}_1\\) (from \\(M_1\\)) compared to the correct Reduced Estimator \\(\\hat{y}_0\\) (from \\(M_0\\)).\n\nTheorem 6.13 (Variance Inflation in Overfitting) When \\(M_0\\) is true:\n\nBias: Both estimators are unbiased. \\[ E[\\hat{y}_1] = \\mu \\quad \\text{and} \\quad E[\\hat{y}_0] = \\mu \\]\nVariance: The estimator \\(\\hat{y}_1\\) has unnecessarily higher variance. \\[ \\text{Var}(\\hat{y}_1) \\ge \\text{Var}(\\hat{y}_0) \\]\n\n\n\nProof. Part 1 (Bias): Since \\(\\mu = X_1\\beta_1\\): \\[\nE[\\hat{y}_1] = P_1 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\text{Col}(X))\n\\] \\[\nE[\\hat{y}_0] = P_0 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\text{Col}(X_1))\n\\]\nPart 2 (Variance): As shown in Case 1, the difference is \\(\\sigma^2(P_1 - P_0)\\). The cost of overfitting is purely variance inflation. The total variance (trace) increases by the number of unnecessary parameters (\\(p_2\\)): \\[\n\\text{tr}(\\text{Var}(\\hat{y}_1)) - \\text{tr}(\\text{Var}(\\hat{y}_0)) = \\sigma^2 (\\text{tr}(P_1) - \\text{tr}(P_0)) = \\sigma^2 (p_{full} - p_{reduced}) = \\sigma^2 p_2\n\\]\n\nConclusion: Using \\(M_1\\) when \\(M_0\\) is true offers no benefit in bias but strictly increases estimation variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference for A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "lec5-est.html#underfitting-and-overfitting-1",
    "href": "lec5-est.html#underfitting-and-overfitting-1",
    "title": "6  Inference in A Multiple Linear Regression Model",
    "section": "6.17 Underfitting and Overfitting",
    "text": "6.17 Underfitting and Overfitting\nWe compare the properties of two competing estimators for the mean response vector \\(\\mu = E[y]\\).\nConsider the Full Model matrix \\(X = [X_1 \\ X_2]\\). We define two estimators for the fitted values:\n\nReduced Estimator (\\(\\hat{y}_0\\)): Based on \\(X_1\\) only. \\[ \\hat{y}_0 = P_1 y \\quad \\text{where} \\quad P_1 = X_1(X_1^T X_1)^{-1}X_1^T \\]\nFull Estimator (\\(\\hat{y}_1\\)): Based on the full \\(X\\). \\[ \\hat{y}_1 = P y \\quad \\text{where} \\quad P = X(X^T X)^{-1}X^T \\]\n\nNote that \\(P_1\\) and \\(P\\) are symmetric projection matrices (idempotent), and since the column space of \\(X_1\\) is a subspace of \\(X\\), we have \\(P P_1 = P_1\\) and \\(P - P_1\\) is also a projection matrix.\n\n6.17.1 Case 1: Underfitting\nThe Truth: The full model is correct. \\[ y = X_1\\beta_1 + X_2\\beta_2 + e, \\quad \\beta_2 \\neq 0 \\] The true mean is \\(\\mu = X_1\\beta_1 + X_2\\beta_2\\).\nWe analyze the properties of the Reduced Estimator \\(\\hat{y}_0\\) (which “underfits” the data) compared to the correct Full Estimator \\(\\hat{y}_1\\).\n\nTheorem 6.14 (Bias-Variance Tradeoff in Underfitting) When the full model is true:\n\nBias: The reduced estimator \\(\\hat{y}_0\\) is biased, while \\(\\hat{y}_1\\) is unbiased. \\[ \\text{Bias}(\\hat{y}_0) = -(I - P_1)X_2\\beta_2 \\]\nVariance: The reduced estimator \\(\\hat{y}_0\\) has smaller variance (element-wise or matrix difference). \\[ \\text{Var}(\\hat{y}_1) - \\text{Var}(\\hat{y}_0) = \\sigma^2 (P - P_1) \\ge 0 \\]\n\n\n\nProof. Part 1 (Bias): \\[\n\\begin{aligned}\nE[\\hat{y}_0] &= P_1 E[y] = P_1(X_1\\beta_1 + X_2\\beta_2) \\\\\n&= X_1\\beta_1 + P_1 X_2 \\beta_2\n\\end{aligned}\n\\] The bias is \\(E[\\hat{y}_0] - \\mu = (X_1\\beta_1 + P_1 X_2 \\beta_2) - (X_1\\beta_1 + X_2\\beta_2) = -(I - P_1)X_2\\beta_2\\). This is non-zero unless \\(X_2\\) is already in the span of \\(X_1\\) or orthogonal to the residual space.\nPart 2 (Variance): \\[\n\\text{Var}(\\hat{y}_1) = \\sigma^2 P, \\quad \\text{Var}(\\hat{y}_0) = \\sigma^2 P_1\n\\] The difference is \\(\\sigma^2(P - P_1)\\). Since \\(\\mathcal{C}(X_1) \\subset \\mathcal{C}(X)\\), the difference \\(P - P_1\\) is the projection matrix onto the subspace of \\(X\\) orthogonal to \\(X_1\\). Projection matrices are positive semidefinite.\n\nConclusion: Underfitting introduces bias but reduces variance.\n\n\n6.17.2 Case 2: Overfitting\nThe Truth: The reduced model is correct. \\[ y = X_1\\beta_1 + e \\quad (\\text{i.e., } \\beta_2 = 0) \\] The true mean is \\(\\mu = X_1\\beta_1\\).\nWe analyze the properties of the Full Estimator \\(\\hat{y}_1\\) (which “overfits” the data) compared to the correct Reduced Estimator \\(\\hat{y}_0\\).\n\nTheorem 6.15 (Variance Inflation in Overfitting) When the reduced model is true:\n\nBias: Both estimators are unbiased. \\[ E[\\hat{y}_1] = \\mu \\quad \\text{and} \\quad E[\\hat{y}_0] = \\mu \\]\nVariance: The full estimator \\(\\hat{y}_1\\) has unnecessarily higher variance. \\[ \\text{Var}(\\hat{y}_1) \\ge \\text{Var}(\\hat{y}_0) \\]\n\n\n\nProof. Part 1 (Bias): Since \\(\\mu = X_1\\beta_1\\): \\[ E[\\hat{y}_1] = P X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\mathcal{C}(X)) \\] \\[ E[\\hat{y}_0] = P_1 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\mathcal{C}(X_1)) \\]\nPart 2 (Variance): As shown in Case 1, \\(\\text{Var}(\\hat{y}_1) - \\text{Var}(\\hat{y}_0) = \\sigma^2(P - P_1)\\). This matrix is positive semidefinite. Specifically, the total variance (trace) increases by the number of extra parameters (\\(p_2\\)): \\[ \\text{tr}(\\text{Var}(\\hat{y}_1)) - \\text{tr}(\\text{Var}(\\hat{y}_0)) = \\sigma^2 (\\text{tr}(P) - \\text{tr}(P_1)) = \\sigma^2 (p_{full} - p_{reduced}) = \\sigma^2 p_2 \\]\n\nConclusion: Overfitting offers no benefit in bias but strictly increases estimation variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference in A Multiple Linear Regression Model</span>"
    ]
  },
  {
    "objectID": "ginv.html#definition-of-generalized-inverse",
    "href": "ginv.html#definition-of-generalized-inverse",
    "title": "7  Generalized Inverses",
    "section": "7.2 Definition of Generalized Inverse",
    "text": "7.2 Definition of Generalized Inverse\n\nDefinition 7.1 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 7.1 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#a-procedure-to-find-a-generalized-inverse",
    "href": "ginv.html#a-procedure-to-find-a-generalized-inverse",
    "title": "7  Generalized Inverses",
    "section": "7.3 A Procedure to Find a Generalized Inverse",
    "text": "7.3 A Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend:\n\n\\(\\otimes\\): Elements of submatrix \\(C\\)\n\\(\\triangle\\): Elements of \\((C^{-1})'\\)\n\\(\\square\\): Elements of \\(C^{-1}\\) (after transposition)\n\\(\\times\\): Other elements (replaced by 0 in the final calculation)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#moore-penrose-inverse",
    "href": "ginv.html#moore-penrose-inverse",
    "title": "7  Generalized Inverses",
    "section": "7.4 Moore-Penrose Inverse",
    "text": "7.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "ginv.html#solving-linear-systems-with-generalized-inverse",
    "href": "ginv.html#solving-linear-systems-with-generalized-inverse",
    "title": "7  Generalized Inverses",
    "section": "7.5 Solving Linear Systems with Generalized Inverse",
    "text": "7.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 7.2 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\text{Col}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 7.2 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Generalized Inverses</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#eigenvalues-and-eigenvectors",
    "href": "lec2-matrix.html#eigenvalues-and-eigenvectors",
    "title": "3  Matrix Algebra",
    "section": "",
    "text": "Definition 3.1 (Eigenvalues and Eigenvectors) For a square matrix \\(A\\) (\\(n \\times n\\)), a scalar \\(\\lambda\\) is an eigenvalue and a non-zero vector \\(x\\) is the corresponding eigenvector if:\n\\[\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n\\]\nThe eigenvalues are found by solving the characteristic equation: \\[\n|A - \\lambda I_n| = 0\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#spectral-theory-for-symmetric-matrices",
    "href": "lec2-matrix.html#spectral-theory-for-symmetric-matrices",
    "title": "3  Matrix Algebra",
    "section": "3.2 Spectral Theory for Symmetric Matrices",
    "text": "3.2 Spectral Theory for Symmetric Matrices\n\n3.2.1 Spectral Decomposition\nFor symmetric matrices, we have a powerful decomposition theorem.\n\nTheorem 3.1 (Spectral Decomposition) If \\(A\\) is a symmetric \\(n \\times n\\) matrix, all its eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) are real. Furthermore, there exists an orthogonal matrix \\(Q\\) such that:\n\\[\nA = Q \\Lambda Q' = \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\]\nwhere:\n\n\\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) contains the eigenvalues.\n\\(Q = (q_1, \\dots, q_n)\\) contains the corresponding orthonormal eigenvectors (\\(q_i'q_j = \\delta_{ij}\\)).\n\n\nExplantion: This allows us to view the transformation \\(Ax\\) as a rotation (\\(Q'\\)), a scaling (\\(\\Lambda\\)), and a rotation back (\\(Q\\)). For a symmetric matrix \\(A\\), we can write the spectral decomposition as a product of the eigenvector matrix \\(Q\\) and eigenvalue matrix \\(\\Lambda\\):\n\\[\n\\begin{aligned}\nA &= Q \\Lambda Q' \\\\\n  &= \\begin{pmatrix} q_1 & q_2 & \\cdots & q_n \\end{pmatrix}\n     \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\begin{pmatrix} \\lambda_1 q_1 & \\lambda_2 q_2 & \\cdots & \\lambda_n q_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\lambda_1 q_1 q_1' + \\lambda_2 q_2 q_2' + \\cdots + \\lambda_n q_n q_n' \\\\\n  &= \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\end{aligned}\n\\]\nwhere the eigenvectors \\(q_i\\) satisfy the orthogonality conditions: \\[\nq_i' q_j = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{if } i \\ne j \\end{cases}\n\\] And \\(Q\\) is an orthogonal matrix: \\(Q'Q = Q Q' = I_n\\).\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# --- 1. MATRIX SETUP ---\n# Symmetric Matrix where eigenvectors are tilted\nA &lt;- matrix(c(1.5, 0.8, 0.8, 1.5), nrow = 2)\n\n# Decomposition A = QDQ'\neig &lt;- eigen(A)\nQ &lt;- eig$vectors\nD_mat &lt;- diag(eig$values)\n\n# --- 2. DEFINE THE 6 VECTORS ---\n\n# 1 & 2: Standard Axes (We will label these x1, x2)\nv1 &lt;- c(1, 0)\nv2 &lt;- c(0, 1)\n# 3 & 4: Eigenvectors\nv3 &lt;- Q[,1]\nv4 &lt;- Q[,2]\n# 5 & 6: Filler vectors at random angles\nv5 &lt;- c(cos(pi/3), sin(pi/3))\nv6 &lt;- c(cos(4*pi/3), sin(4*pi/3))\n\n# Combine into starting matrix V_start\nV_start &lt;- cbind(v1, v2, v3, v4, v5, v6)\n\n# Define 6 Distinct Colors\nmy_colors &lt;- c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF7F00\", \"#A65628\")\nnames(my_colors) &lt;- 1:6\n\n# Background Circle Points used for reference path in all plots\ntheta_c &lt;- seq(0, 2*pi, length.out = 150)\nC_start &lt;- rbind(cos(theta_c), sin(theta_c))\n\n# --- 3. DATA PROCESSING HELPER FUNCTION ---\n\n# This function prepares the data frames for ggplot for a given stage\nprepare_data &lt;- function(V_mat, C_mat, stage_title, label_text_pair) {\n  # Prepare Vectors data frame\n  df_v &lt;- data.frame(t(V_mat))\n  colnames(df_v) &lt;- c(\"x\", \"y\")\n  df_v$vec_id &lt;- factor(1:6) # Unique ID for coloring\n  \n  # Add labels only for vector 1 and 2\n  df_v$label &lt;- \"\"\n  df_v$label[1] &lt;- label_text_pair[1]\n  df_v$label[2] &lt;- label_text_pair[2]\n  \n  # Calculate nudge for labels based on vector direction so they don't overlap arrow tip\n  df_v$nudge_x &lt;- sign(df_v$x) * 0.25\n  df_v$nudge_y &lt;- sign(df_v$y) * 0.25\n  # Don't nudge unlabelled vectors\n  df_v$nudge_x[3:6] &lt;- 0\n  df_v$nudge_y[3:6] &lt;- 0\n\n  # Prepare Background Path data frame\n  df_c &lt;- data.frame(t(C_mat))\n  colnames(df_c) &lt;- c(\"px\", \"py\")\n  \n  list(vecs = df_v, path = df_c, title = stage_title)\n}\n\n\n# --- 4. PERFORM TRANSFORMATIONS ---\n\n# Stage 1: Start (x)\nd1 &lt;- prepare_data(V_start, C_start, \n                   \"1. Start (x)\", c(\"x[1]\", \"x[2]\"))\n\n# Stage 2: Rotate (Q'x)\nV2 &lt;- t(Q) %*% V_start\nC2 &lt;- t(Q) %*% C_start\nd2 &lt;- prepare_data(V2, C2, \n                   \"2. Rotate (Q'x)\", c(\"z[1]\", \"z[2]\"))\n\n# Stage 3: Stretch (DQ'x)\nV3 &lt;- D_mat %*% V2\nC3 &lt;- D_mat %*% C2\nd3 &lt;- prepare_data(V3, C3, \n                   \"3. Stretch (DQ'x)\", c(\"y[1]\", \"y[2]\"))\n\n# Stage 4: Rotate Back (QDQ'x)\nV4 &lt;- Q %*% V3\nC4 &lt;- Q %*% C3\nd4 &lt;- prepare_data(V4, C4, \n                   \"4. Final (QDQ'x)\", c(\"w[1]\", \"w[2]\"))\n\n\n# --- 5. PLOTTING FUNCTION ---\n\nplot_stage_final &lt;- function(data_list) {\n  ggplot() +\n    # Background path (gray dashed)\n    geom_path(data = data_list$path, aes(x=px, y=py), \n              color=\"gray70\", linetype=\"dashed\") +\n    # The 6 vectors\n    geom_segment(data = data_list$vecs, aes(x=0, y=0, xend=x, yend=y, color=vec_id), \n                 arrow = arrow(length = unit(0.3, \"cm\")), size=1.1) +\n    # The labels for v1 and v2 using parsed expressions for subscripts\n    geom_text(data = data_list$vecs, aes(x=x, y=y, label=label, color=vec_id),\n              parse = TRUE, fontface=\"bold\", size=5,\n              nudge_x = data_list$vecs$nudge_x,\n              nudge_y = data_list$vecs$nudge_y) +\n    scale_color_manual(values = my_colors) +\n    # Fixed coordinates to ensure realistic rotation/stretching view\n    coord_fixed(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +\n    theme_bw() +\n    theme(legend.position = \"none\",\n          panel.grid.minor = element_blank(),\n          plot.title = element_text(face=\"bold\", hjust=0.5),\n          axis.title = element_blank()) +\n    labs(title = data_list$title)\n}\n\n# Generate the 4 plots\np1 &lt;- plot_stage_final(d1)\np2 &lt;- plot_stage_final(d2)\np3 &lt;- plot_stage_final(d3)\np4 &lt;- plot_stage_final(d4)\n\n# Arrange them in a grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n\n\n3.2.2 Quadratic Form\n\nDefinition 3.2 A quadratic form in \\(n\\) variables \\(x_1, x_2, \\dots, x_n\\) is a scalar function defined by a symmetric matrix \\(A\\): \\[\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\]\n\n\n\n3.2.3 Positive and Non-Negative Definite Matrices\n\nDefinition 3.3 (Positive and Non-Negative Definite Matrices) A symmetric matrix \\(A\\) is positive definite (p.d.) if: \\[\nx'Ax &gt; 0 \\quad \\forall x \\ne 0\n\\] It is non-negative definite (n.n.d.) if: \\[\nx'Ax \\ge 0 \\quad \\forall x\n\\]\n\n\nTheorem 3.2 (Properties of Definite Matrices) Let \\(A\\) be a symmetric \\(n \\times n\\) matrix with eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\).\n\nEigenvalue Characterization:\n\n\\(A\\) is p.d. \\(\\iff\\) all \\(\\lambda_i &gt; 0\\).\n\\(A\\) is n.n.d. \\(\\iff\\) all \\(\\lambda_i \\ge 0\\).\n\nDeterminant and Inverse:\n\nIf \\(A\\) is p.d., then \\(|A| &gt; 0\\) and \\(A^{-1}\\) exists.\nIf \\(A\\) is n.n.d. and singular, then \\(|A| = 0\\) (at least one \\(\\lambda_i = 0\\)).\n\nGram Matrices (\\(B'B\\)): Let \\(B\\) be an \\(n \\times p\\) matrix.\n\nIf \\(\\text{rank}(B) = p\\), then \\(B'B\\) is p.d.\nIf \\(\\text{rank}(B) &lt; p\\), then \\(B'B\\) is n.n.d.\n\n\n\n\n\n3.2.4 Properties of Symmetric Matrices\n\nTheorem 3.3 (Properties of Symmetric Matrices) Let \\(A\\) be a symmetric matrix with spectral decomposition \\(A = Q \\Lambda Q'\\). The following properties hold:\n\nTrace: \\(\\text{tr}(A) = \\sum \\lambda_i\\).\nDeterminant: \\(|A| = \\prod \\lambda_i\\).\nSingularity: \\(A\\) is singular if and only if at least one \\(\\lambda_i = 0\\).\nInverse: If \\(A\\) is non-singular (\\(\\lambda_i \\ne 0\\)), then \\(A^{-1} = Q \\Lambda^{-1} Q'\\).\nPowers: \\(A^k = Q \\Lambda^k Q'\\).\n\nSquare Root: \\(A^{1/2} = Q \\Lambda^{1/2} Q'\\) (if \\(\\lambda_i \\ge 0\\)).\n\nSpectral Representation of Quadratic Forms: The quadratic form \\(x'Ax\\) can be diagonalized using the eigenvectors of \\(A\\): \\[\nx'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n\\] where \\(y = Q'x\\) represents a rotation of the coordinate system.\n\n\n\n\n3.2.5 Spectral Representation of Projection Matrices\nWe revisit projection matrices in the context of eigenvalues.\n\nTheorem 3.4 (Eigenvalues of Projection Matrices) A symmetric matrix \\(P\\) is a projection matrix (idempotent, \\(P^2=P\\)) if and only if its eigenvalues are either 0 or 1.\n\\[\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n\\]\n\nFor a projection matrix \\(P\\):\n\nIf \\(x \\in \\text{Col}(P)\\), \\(Px = x\\) (Eigenvalue 1).\nIf \\(x \\perp \\text{Col}(P)\\), \\(Px = 0\\) (Eigenvalue 0).\n\\(\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i\\) (Count of 1s).\n\n\nExample 3.1 For \\(P = \\frac{1}{n} J_n J_n'\\), the rank is \\(\\text{tr}(P) = 1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Algebra</span>"
    ]
  }
]