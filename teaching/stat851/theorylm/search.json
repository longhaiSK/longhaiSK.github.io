[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical Theory for Linear Models",
    "section": "Overview",
    "text": "Overview\nThis book has evolved from the lecture notes for STAT 851: Statistical Linear Models, a graduate-level course taught at the University of Saskatchewan. This course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#audience-and-prerequisites",
    "href": "index.html#audience-and-prerequisites",
    "title": "Statistical Theory for Linear Models",
    "section": "Audience and Prerequisites",
    "text": "Audience and Prerequisites",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Theory for Linear Models",
    "section": "Key Features",
    "text": "Key Features",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Statistical Theory for Linear Models",
    "section": "Structure of the Book",
    "text": "Structure of the Book",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Theory for Linear Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe lectures were built upon the lecture notes for stat 8260 by Danniel Hall and the textbook LINEAR MODELS IN STATISTICS, Second Edition, by Alvin C. Rencher and G. Bruce Schaalje, ISBN 978-0-471-75498-5 (cloth)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1 Overview\nThis lecture covers the foundations of Linear Statistical Models, including Multiple Linear Regression and ANOVA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#overview",
    "href": "introlm.html#overview",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1.1 Geometric Interpretation of Least Squares\nWe compare models based on the reduction of errors. Consider a full model and a reduced model (\\(K_1\\)).\nLet:\n\n\\(y\\) be the observed vector.\n\\(\\hat{y}_1\\) be the prediction from the reduced model.\n\\(\\hat{y}_2\\) be the prediction from the full model.\n\n\n\n\nGeometric Interpretation of Least Squares\n\n\nThe errors (residuals) are defined as: \\[\ne_1 = y - \\hat{y}_1\n\\] \\[\ne_2 = y - \\hat{y}_2\n\\]\nThe Sum of Squared Errors (SSE) representing the distance is: \\[\nSSE_1 = ||y - \\hat{y}_1||^2\n\\] \\[\nSSE_2 = ||y - \\hat{y}_2||^2\n\\]\nThe statistical test is often based on the comparison of \\(SSE_1\\) and \\(SSE_2\\) (or the reduction in sums of squares \\(||\\hat{y}_2 - \\hat{y}_1||^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#multiple-linear-regression",
    "href": "introlm.html#multiple-linear-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\n\n2.2.1 Matrix Formulation\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nWhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n)\n\\]\nAnd \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\]\nThe scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#polynomial-regression",
    "href": "introlm.html#polynomial-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.3 Polynomial Regression",
    "text": "2.3 Polynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n2.3.1 Design Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#one-way-anova",
    "href": "introlm.html#one-way-anova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.4 One-Way ANOVA",
    "text": "2.4 One-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\n\n\nOne-Way ANOVA Diagram\n\n\n\n2.4.1 Dummy Variable Matrix\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#analysis-of-covariance-ancova",
    "href": "introlm.html#analysis-of-covariance-ancova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.5 Analysis of Covariance (ANCOVA)",
    "text": "2.5 Analysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,cont} & 1 & 0 \\\\\nX_{2,cont} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,cont} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]\n\n2.5.1 Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\nSSE = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{SST - SSE}{SST}\n\\] where \\(SST = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html",
    "href": "lec2-vecspace.html",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "3.1 Vector and Projection onto a Line\nVectors and Operations\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\nVector Arithmetic: Vectors can be manipulated geometrically:\nScalar Multiplication and Length\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\nWe often need to quantify the “size” of a vector. This is done using the concept of length, or norm.\nAngle and Inner Product\nTo understand the relationship between two vectors \\(x\\) and \\(y\\) beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the classic Law of Cosines to this triangle, we can relate the geometric angle to the vector lengths.\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation provides a critical link between the geometric angle \\(\\theta\\) and the algebraic norms of the vectors.\nDerivation of Inner Product\nWe can express the squared distance term \\(||y - x||^2\\) purely algebraically by expanding the components:\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the Inner Product (or dot product).\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\nCoordinate (Scalar) Projection\nThe inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of this projection is given by:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis expression can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector in the direction of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]\nVector Projection Formula\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\nPerpendicularity (Orthogonality)\nA special case of the angle between vectors arises when \\(\\theta = 90^\\circ\\). This geometric concept of perpendicularity is central to the theory of projections and least squares.\nProjection onto a Line (Subspace)\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\nThe projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that it is the closest point on the line to \\(y\\). This implies that the error vector (or residual) must be perpendicular to the line itself.\nDerivation: To find the value of the scalar \\(c\\), we apply the orthogonality condition:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\nExpanding this inner product gives:\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\), we obtain:\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\nAlternative Forms of the Projection Formula\nWe can express the projection vector \\(\\hat{y}\\) in several equivalent ways to highlight different geometric interpretations.\nProjection Matrix (\\(P_x\\))\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector \\(y\\). This allows us to define a Projection Matrix.\nWe can rewrite the formula for \\(\\hat{y}\\) by factoring out \\(y\\):\n\\[\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n\\]\nThis leads to the definition of the projection matrix \\(P_x\\).\nExample: Projection in \\(\\mathbb{R}^2\\)\nLet’s apply these concepts to a concrete example.\nExample: Projection onto the Mean Vector\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\nPythagorean Theorem\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\nProof: We expand the squared norm using the inner product:\n\\[\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n\\]\nSince \\(x \\perp y\\), the inner product \\(x'y = 0\\). Thus, the term \\(2x'y\\) vanishes, leaving:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\nLeast Square Property\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector \\(y\\) and the subspace (or line) onto which it is projected.\nProof: Since both \\(\\hat{y}\\) and \\(y^*\\) lie on the line \\(L(x)\\), their difference \\((\\hat{y} - y^*)\\) also lies on \\(L(x)\\). From the definition of projection, the residual \\((y - \\hat{y})\\) is orthogonal to the line \\(L(x)\\). Therefore:\n\\[\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n\\]\nWe can write the vector \\((y - y^*)\\) as: \\[\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n\\]\nApplying the Pythagorean Theorem: \\[\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n\\]\nSince \\(||\\hat{y} - y^*||^2 \\ge 0\\), it follows that: \\[\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "href": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Least Square Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nLeast Square Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#general-vector-space",
    "href": "lec2-vecspace.html#general-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.2 General Vector Space",
    "text": "3.2 General Vector Space\nWe now generalize our discussion from lines to broader spaces.\n\nDefinition 3.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\nIt follows that the zero vector \\(0\\) must belong to any subspace (by choosing \\(c=0\\)).\nSpanned Vector Space\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\nDefinition 3.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\n\n\n\nSpanned Vector Space\n\n\nColumn Space and Row Space\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\nDefinition 3.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(Col(X)\\), is the vector space spanned by its columns:\n\\[\nCol(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 3.16 (Row Space) The Row Space, denoted \\(Row(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\n\n\n\nLinear Independence and Rank\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\nDefinition 3.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "href": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.3 Rank of Matrices and Dim of Vector Space",
    "text": "3.3 Rank of Matrices and Dim of Vector Space\n\nDefinition 3.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(Col(X))\n\\]\n\n\n3.3.0.1 Properties of Rank\nThere are several fundamental properties regarding the rank of a matrix.\n\nTheorem 3.4 (Properties of Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(Col(X)) = \\text{Dim}(Row(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\n\nProof. Let \\(X\\) be an \\(n \\times p\\) matrix. Let \\(r\\) be the row rank of \\(X\\). This means the dimension of the row space is \\(r\\). Let \\(u_1, \\dots, u_r\\) be a basis for the row space of \\(X\\) (these are row vectors). Since every row of \\(X\\) is in the row space, each row \\(x_{i.}\\) can be written as a linear combination of the basis vectors: \\[\n  x_{i.} = c_{i1}u_1 + c_{i2}u_2 + \\dots + c_{ir}u_r \\quad \\text{for } i=1,\\dots,n\n  \\]\nWe can write this in matrix notation as: \\[\n  X = C U\n  \\] where \\(C\\) is an \\(n \\times r\\) matrix of coefficients \\(c_{ij}\\), and \\(U\\) is an \\(r \\times p\\) matrix with rows \\(u_1, \\dots, u_r\\).\nNow consider the columns of \\(X\\). Since \\(X = CU\\), the columns of \\(X\\) are linear combinations of the columns of \\(C\\). Let \\(c^{(j)}\\) be the \\(j\\)-th column of \\(C\\). The columns of \\(X\\) lie in the space spanned by \\(\\{c^{(1)}, \\dots, c^{(r)}\\}\\). Thus, the column space of \\(X\\), \\(Col(X)\\), is a subspace of the column space of \\(C\\). \\[\n  \\text{Dim}(Col(X)) \\le \\text{Dim}(Col(C)) \\le r\n  \\] The dimension of the column space of \\(C\\) is at most \\(r\\) (since \\(C\\) has only \\(r\\) columns). Therefore, Column Rank \\(\\le\\) Row Rank.\nApplying the same logic to \\(X'\\), we get Row Rank \\(\\le\\) Column Rank. Combining these inequalities gives: Row Rank = Column Rank.\n\n\nExample: 2x3 Matrix\nConsider the following \\(2 \\times 3\\) matrix: \\[\nX = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n\\]\nRow Rank: The rows are \\(r_1 = (1, 0, 1)\\) and \\(r_2 = (0, 1, 1)\\). Neither is a multiple of the other, so they are linearly independent. \\[\n\\text{Row Rank} = 2\n\\]\nColumn Rank: The columns are \\(c_1 = \\binom{1}{0}\\), \\(c_2 = \\binom{0}{1}\\), and \\(c_3 = \\binom{1}{1}\\). Notice that \\(c_3 = c_1 + c_2\\). The third column is dependent on the first two. However, \\(c_1\\) and \\(c_2\\) are independent (standard basis vectors). \\[\n\\text{Column Rank} = 2\n\\]\nThus, Rank(Row) = Rank(Col) = 2.\n\nOrthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 3.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 3.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n\nOrthogonal Complement\n\n\nKernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 3.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = Col(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 3.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [Row(X)]^\\perp\n\\]\n\nNullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 3.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\]\nWhere \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\n\n\nRank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 3.7 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\nProof: The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\nCol(XZ) \\subseteq Col(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\nRow(XZ) \\subseteq Row(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\nCombining these gives the result.\n\n\n\nRank of Matrix Product\n\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 3.8 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\nProof: From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\n\n\nRank Preservation with Invertible Matrices\n\n\n–\nRank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 3.9 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\nProof Strategy: We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\n\n\nRank of Gram Matrix\n\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 3.10 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\nCol(XX') = Col(X)\n\\]\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(Col(X)\\) lies in the same space as the projection onto \\(Col(XX')\\). This is vital for the existence of solutions in generalized least squares.\n\n\n\nColumn Space Equivalence",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "href": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "3.4 Projection via Orthonormal Basis",
    "text": "3.4 Projection via Orthonormal Basis\nBasis and Dimension\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 3.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\nThe Projection Theorem\nThis theorem establishes the existence and uniqueness of the projection vector for any subspace, regardless of the basis used.\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\): * \\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)). * \\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-projection-matrices",
    "href": "lec2-vecspace.html#projection-via-projection-matrices",
    "title": "3  Vector Space and Projection",
    "section": "3.5 Projection via Projection Matrices",
    "text": "3.5 Projection via Projection Matrices\n\nTheorem 3.12 (Projection Matrix (Orthonormal)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 3.13 (Properties of Projection Matrices) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\nProof of Idempotence: If \\(\\hat{y} = Py\\) is already in the subspace \\(Col(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 3.4 (ANOVA as Projection) Consider a one-way ANOVA model: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i\\) represents the group and \\(j\\) represents the observation within the group.\nWe can define dummy variables (indicators) for each group. Let \\(x_1\\) be the indicator for group 1, \\(x_2\\) for group 2, etc. These vectors are mutually orthogonal because an observation cannot belong to two groups simultaneously.\nThe projection of the data vector \\(y\\) onto the space spanned by these indicators is the sum of the projections onto each group vector:\n\\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\]\nSince the indicators are orthogonal, this simplifies to calculating the mean for each group. The fitted value for any observation \\(y_{ij}\\) is simply the group mean \\(\\bar{y}_{i.}\\).\n\n\n\n\nANOVA Projection Geometry\n\n\nProjection onto Orthogonal Complement\nIf \\(P\\) is the projection matrix onto a subspace \\(V\\), we can easily define the projection onto the orthogonal complement \\(V^\\perp\\) (the “error” space).\n\nDefinition 3.25 (Projection onto Complement) The matrix \\(M = I - P\\) is the projection matrix onto the orthogonal complement \\(Col(X)^\\perp\\).\nProperties of \\(M\\):\n\nIdempotent: \\(M^2 = (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P = M\\).\nSymmetric: \\(M' = (I-P)' = I - P' = I - P = M\\).\nOrthogonal to \\(P\\): \\(PM = P(I-P) = P - P^2 = 0\\).\n\n\nThis matrix \\(M\\) produces the residuals: \\(e = My = (I-P)y = y - \\hat{y}\\).\nGram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.\n\n\n\nGram-Schmidt Process\n\n\n\nProjection Matrix Definition\nWe now formally derive the projection matrix \\(P\\) for the general case where we project \\(y\\) onto the column space of a matrix \\(X\\).\nNormal Equations\nWe want to find \\(\\hat{y} = X\\beta\\) such that the residual \\((y - \\hat{y})\\) is orthogonal to the column space \\(Col(X)\\). This means the residual must be orthogonal to every column of \\(X\\).\n\\[\nX'(y - X\\beta) = 0\n\\]\nExpanding this gives the famous Normal Equations:\n\\[\nX'y - X'X\\beta = 0 \\implies X'X\\beta = X'y\n\\]\n\nTheorem 3.14 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\nNormal Equations Derivation\n\n\nThe Matrix \\(P\\)\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 3.26 (General Projection Matrix) The projection of \\(y\\) onto \\(Col(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the projection matrix \\(P\\) is defined as:\n\\[\nP = X(X'X)^{-1}X'\n\\]\n\nRelationship with QR Decomposition\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(Col(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\nP = QR((QR)'(QR))^{-1}(QR)'\n\\] \\[\n= QR(R'Q'QR)^{-1}R'Q'\n\\] \\[\n= QR(R'R)^{-1}R'Q'\n\\] \\[\n= QR R^{-1} (R')^{-1} R' Q'\n\\] \\[\n= Q Q'\n\\]\nThis confirms that \\(P = QQ'\\) is consistent with the general formula \\(P = X(X'X)^{-1}X'\\).\nProperties of \\(P\\)\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 3.15 (Properties of P) The matrix \\(P = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(P' = P\\)\nIdempotent: \\(P^2 = P\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(P) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\nProjection onto Complement\nAs before, the projection onto the orthogonal complement (the residual maker matrix) is \\(M = I - P\\).\n\nDefinition 3.27 (Residual Maker Matrix M) \\[\nM = I - X(X'X)^{-1}X'\n\\]\nThis matrix projects \\(y\\) onto the null space of \\(X'\\) (the orthogonal complement of the column space of \\(X\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "href": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "title": "3  Vector Space and Projection",
    "section": "3.6 Projections onto Nested Subspaces",
    "text": "3.6 Projections onto Nested Subspaces\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\nDefinition 3.28 (Nested Models) Consider two models: 1. Reduced Model (\\(M_0\\)): \\(y \\in Col(X_0)\\) 2. Full Model (\\(M_1\\)): \\(y \\in Col(X_1)\\)\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\nCol(X_0) \\subseteq Col(X_1)\n\\]\n\nUsually, \\(X_1\\) is constructed by adding columns to \\(X_0\\): \\(X_1 = [X_0, X_{new}]\\).\n\n\n\nNested Models Concept\n\n\nProjection Composition\nLet \\(P_0\\) be the projection matrix onto \\(Col(X_0)\\) and \\(P_1\\) be the projection matrix onto \\(Col(X_1)\\). Since \\(Col(X_0) \\subseteq Col(X_1)\\), we have important relationships between these matrices.\n\nTheorem 3.16 (Composition of Projections) If \\(Col(P_0) \\subseteq Col(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\nDifference of Projections\nThe difference between the two projection matrices, \\(P_1 - P_0\\), is itself a projection matrix.\n\nTheorem 3.17 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(Col(X_1) \\cap Col(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\nDecomposition of Sum of Squares\nThis geometry allows us to decompose the total vector \\(y\\) into three orthogonal components:\n\n\\(\\hat{y}_0\\) (The fit of the reduced model)\n\\(\\hat{y}_1 - \\hat{y}_0\\) (The improvement from the reduced to the full model)\n\\(y - \\hat{y}_1\\) (The residual of the full model)\n\n\\[\n  y = \\hat{y}_0 + (\\hat{y}_1 - \\hat{y}_0) + (y - \\hat{y}_1)\n  \\]\nSquaring the norms (applying Pythagoras):\n\\[\n  ||y||^2 = ||\\hat{y}_0||^2 + ||\\hat{y}_1 - \\hat{y}_0||^2 + ||y - \\hat{y}_1||^2\n  \\]\nThis equation is the foundation for the F-test in ANOVA and regression.\n\n\n\nDecomposition of Sum of Squares\n\n\nANOVA Sum of Squares\nWe apply the decomposition of sum of squares to the specific case of Analysis of Variance.\n\nDefinition 3.29 (ANOVA Decomposition) The Total Sum of Squares (TSS), or the squared norm of \\(y\\) (often centered), can be split into components:\n\nResidual Sum of Squares (Full Model): \\[\nRSS_1 = ||y - \\hat{y}_1||^2 = \\sum_{i}\\sum_{j} (y_{ij} - \\bar{y}_{i.})^2\n\\] This represents the “Within Group” sum of squares.\nDifference Sum of Squares: \\[\nRSS_0 - RSS_1 = ||\\hat{y}_1 - \\hat{y}_0||^2 = \\sum_{i} n_i (\\bar{y}_{i.} - \\bar{y}_{..})^2\n\\] This represents the “Between Group” sum of squares (\\(SS_{between}\\)).\n\n\nThis relationship confirms that: \\[\nTSS = SS_{within} + SS_{between}\n\\]\n\n\n\nANOVA Sum of Squares Derivation\n\n\nProjections in Orthogonal Spaces\nFinally, we consider the case where the entire space \\(\\mathbb{R}^n\\) is decomposed into mutually orthogonal subspaces.\n\nTheorem 3.18 (Orthogonal Decomposition) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = x_1 + x_2 + \\dots + x_k\n\\] where \\(x_i \\in V_i\\).\nFurthermore, each component \\(x_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\nx_i = P_i y\n\\]\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: \\[\nI_n = P_1 + P_2 + \\dots + P_k\n\\]\n\n\n\nOrthogonal Space Decomposition\n\n\nThe following diagram summarizes the relationships between the vector spaces and projections discussed in this lecture.\n\n\n\nSummary Diagram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html",
    "href": "lec3-matrix.html",
    "title": "4  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "4.1 Spectral Theory\nThis chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, and generalized inverses.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#spectral-theory",
    "href": "lec3-matrix.html#spectral-theory",
    "title": "4  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "4.1.1 Eigenvalues and Eigenvectors\n\nDefinition 4.1 (Eigenvalues and Eigenvectors) For a square matrix \\(A\\) (\\(n \\times n\\)), a scalar \\(\\lambda\\) is an eigenvalue and a non-zero vector \\(x\\) is the corresponding eigenvector if:\n\\[\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n\\]\nThe eigenvalues are found by solving the characteristic equation: \\[\n|A - \\lambda I_n| = 0\n\\]\n\n\n\n4.1.2 Spectral Decomposition\nFor symmetric matrices, we have a powerful decomposition theorem.\n\nTheorem 4.1 (Spectral Decomposition) If \\(A\\) is a symmetric \\(n \\times n\\) matrix, all its eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) are real. Furthermore, there exists an orthogonal matrix \\(Q\\) such that:\n\\[\nA = Q \\Lambda Q' = \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\]\nwhere:\n\n\\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) contains the eigenvalues.\n\\(Q = (q_1, \\dots, q_n)\\) contains the corresponding orthonormal eigenvectors (\\(q_i'q_j = \\delta_{ij}\\)).\n\n\nExplantion: This allows us to view the transformation \\(Ax\\) as a rotation (\\(Q'\\)), a scaling (\\(\\Lambda\\)), and a rotation back (\\(Q\\)). For a symmetric matrix \\(A\\), we can write the spectral decomposition as a product of the eigenvector matrix \\(Q\\) and eigenvalue matrix \\(\\Lambda\\):\n\\[\n\\begin{aligned}\nA &= Q \\Lambda Q' \\\\\n  &= \\begin{pmatrix} q_1 & q_2 & \\cdots & q_n \\end{pmatrix}\n     \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\begin{pmatrix} \\lambda_1 q_1 & \\lambda_2 q_2 & \\cdots & \\lambda_n q_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\lambda_1 q_1 q_1' + \\lambda_2 q_2 q_2' + \\cdots + \\lambda_n q_n q_n' \\\\\n  &= \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\end{aligned}\n\\]\nwhere the eigenvectors \\(q_i\\) satisfy the orthogonality conditions: \\[\nq_i' q_j = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{if } i \\ne j \\end{cases}\n\\] And \\(Q\\) is an orthogonal matrix: \\(Q'Q = Q Q' = I_n\\).\n\n\n4.1.3 Quadratic Form\n\nDefinition 4.2 A quadratic form in \\(n\\) variables \\(x_1, x_2, \\dots, x_n\\) is a scalar function defined by a symmetric matrix \\(A\\): \\[\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\]\n\n\n\n4.1.4 Properties of Symmetric Matrices\n\nTheorem 4.2 (Properties of Symmetric Matrices) Let \\(A\\) be a symmetric matrix with spectral decomposition \\(A = Q \\Lambda Q'\\). The following properties hold:\n\nTrace: \\(\\text{tr}(A) = \\sum \\lambda_i\\).\nDeterminant: \\(|A| = \\prod \\lambda_i\\).\nSingularity: \\(A\\) is singular if and only if at least one \\(\\lambda_i = 0\\).\nInverse: If \\(A\\) is non-singular (\\(\\lambda_i \\ne 0\\)), then \\(A^{-1} = Q \\Lambda^{-1} Q'\\).\nPowers: \\(A^k = Q \\Lambda^k Q'\\).\n\nSquare Root: \\(A^{1/2} = Q \\Lambda^{1/2} Q'\\) (if \\(\\lambda_i \\ge 0\\)).\n\nSpectral Representation of Quadratic Forms: The quadratic form \\(x'Ax\\) can be diagonalized using the eigenvectors of \\(A\\): \\[\nx'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n\\] where \\(y = Q'x\\) represents a rotation of the coordinate system.\n\n\n\n\n4.1.5 Spectral Representation of Projection Matrices\nWe revisit projection matrices in the context of eigenvalues.\n\nTheorem 4.3 (Eigenvalues of Projection Matrices) A symmetric matrix \\(P\\) is a projection matrix (idempotent, \\(P^2=P\\)) if and only if its eigenvalues are either 0 or 1.\n\\[\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n\\]\n\nFor a projection matrix \\(P\\):\n\nIf \\(x \\in Col(P)\\), \\(Px = x\\) (Eigenvalue 1).\nIf \\(x \\perp Col(P)\\), \\(Px = 0\\) (Eigenvalue 0).\n\\(\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i\\) (Count of 1s).\n\n\nExample 4.1 For \\(P = \\frac{1}{n} J_n J_n'\\), the rank is \\(\\text{tr}(P) = 1\\).\n\n\n\n4.1.6 Positive Definite Matrices\n\nDefinition 4.3 (Positive Definite (p.d.) Matrix) A symmetric matrix \\(A\\) is positive definite if: \\[\nx'Ax &gt; 0 \\quad \\forall x \\ne 0\n\\] It is positive semi-definite (p.s.d.) if: \\[\nx'Ax \\ge 0 \\quad \\forall x\n\\]\n\nCharacterization via Eigenvalues:\n\n\\(A\\) is p.d. \\(\\iff\\) all \\(\\lambda_i &gt; 0\\).\n\\(A\\) is p.s.d. \\(\\iff\\) all \\(\\lambda_i \\ge 0\\).\n\nProperties:\n\nIf \\(A\\) is p.d., then \\(|A| &gt; 0\\) and \\(A^{-1}\\) exists.\nIf \\(B\\) is \\(n \\times p\\) with rank \\(p\\), then \\(B'B\\) is p.d.\nIf \\(B\\) has rank \\(&lt; p\\), then \\(B'B\\) is p.s.d.\n\n\n\n\n4.1.7 Singular Value Decomposition (SVD)\n\nTheorem 4.4 (Singular Value Decomposition (SVD)) Let \\(X\\) be an \\(n \\times p\\) matrix with rank \\(r \\le \\min(n, p)\\). \\(X\\) can be decomposed into the product of three matrices:\n\\[\nX = U \\mathbf{D} V'\n\\]\n1. Partitioned Matrix Form\n\\[\nX = \\underset{n \\times n}{(U_1, U_2)}\n\\begin{pmatrix}\n\\Lambda_r & O_{r \\times (p-r)} \\\\\nO_{(n-r) \\times r} & O_{(n-r) \\times (p-r)}\n\\end{pmatrix}\n\\underset{p \\times p}{\n\\begin{pmatrix}\nV_1' \\\\\nV_2'\n\\end{pmatrix}\n}\n\\]\n2. Detailed Matrix Form\nExpanding the diagonal matrix explicitly:\n\\[\nX = \\underset{n \\times n}{(u_1, \\dots, u_n)}\n\\left(\n\\begin{array}{cccc|c}\n\\lambda_1 & 0 & \\dots & 0 &  \\\\\n0 & \\lambda_2 & \\dots & 0 & O_{12} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots &  \\\\\n0 & 0 & \\dots & \\lambda_r &  \\\\\n\\hline\n& O_{21} & & & O_{22}\n\\end{array}\n\\right)\n\\underset{p \\times p}{\n\\begin{pmatrix}\nv_1' \\\\\n\\vdots \\\\\nv_p'\n\\end{pmatrix}\n}\n\\]\n3. Reduced Form\n\\[\nX = U_1 \\Lambda_r V_1' = \\sum_{i=1}^r \\lambda_i u_i v_i'\n\\]\nProperties:\n\nSingular Values (\\(\\Lambda_r\\)): \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values (\\(\\lambda_i &gt; 0\\)), which are the square roots of the non-zero eigenvalues of \\(X'X\\).\nOrthogonality:\n\n\\(U\\) is \\(n \\times n\\) orthogonal (\\(U'U = I_n\\)).\n\\(V\\) is \\(p \\times p\\) orthogonal (\\(V'V = I_p\\)).\n\n\n\n\n4.1.7.1 Connection to Gram Matrices\nThe matrices \\(U\\) and \\(V\\) provide the basis vectors (eigenvectors) for the Gram matrices of \\(X\\).\n\nRight Singular Vectors (\\(V\\)): The columns of \\(V\\) are the eigenvectors of the Gram matrix \\(X'X\\). \\[\nX'X = (U \\Lambda V')' (U \\Lambda V') = V \\Lambda U' U \\Lambda V' = V \\Lambda^2 V'\n\\]\n\nThe eigenvalues of \\(X'X\\) are the squared singular values \\(\\lambda_i^2\\).\n\nLeft Singular Vectors (\\(U\\)): The columns of \\(U\\) are the eigenvectors of the Gram matrix \\(XX'\\). \\[\nXX' = (U \\Lambda V') (U \\Lambda V')' = U \\Lambda V' V \\Lambda U' = U \\Lambda^2 U'\n\\]\n\nThe eigenvalues of \\(XX'\\) are also \\(\\lambda_i^2\\) (for non-zero values).\n\n\n\n\n4.1.7.2 Numerical Example\nConsider the matrix \\(X = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\\).\n\nCompute \\(X'X\\) and find \\(V\\): \\[\nX'X = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 5 \\\\ 5 & 5 \\end{pmatrix}\n\\]\n\nEigenvalues of \\(X'X\\): Trace is 10, Determinant is 0. Thus, \\(\\mu_1 = 10, \\mu_2 = 0\\).\nSingular Values: \\(\\lambda_1 = \\sqrt{10}, \\lambda_2 = 0\\).\nEigenvector for \\(\\mu_1=10\\): Normalized \\(v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\).\n\nCompute \\(XX'\\) and find \\(U\\): \\[\nXX' = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n\\]\n\nEigenvalues are again 10 and 0.\nEigenvector for \\(\\mu_1=10\\): Normalized \\(u_1 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(U = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & 2 \\\\ 2 & -1 \\end{pmatrix}\\).\n\nVerification: \\[\nX = \\sqrt{10} u_1 v_1' = \\sqrt{10} \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#cholesky-decomposition",
    "href": "lec3-matrix.html#cholesky-decomposition",
    "title": "4  Spectral Theory and Generalized Inverse",
    "section": "4.2 Cholesky Decomposition",
    "text": "4.2 Cholesky Decomposition\nA symmetric matrix \\(A\\) has a Cholesky decomposition if and only if it is non-negative definite (i.e., \\(x'Ax \\ge 0\\) for all \\(x\\)).\n\\[\nA = B'B\n\\]\nwhere \\(B\\) is an upper triangular matrix with non-negative diagonal entries.\n\n4.2.1 Matrix Representation of the Algorithm\nTo derive the algorithm, we equate the elements of \\(A\\) with the product of the lower triangular matrix \\(B'\\) and the upper triangular matrix \\(B\\).\nFor a \\(3 \\times 3\\) matrix, this looks like:\n\\[\n\\underbrace{\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}}_{A}\n=\n\\underbrace{\\begin{pmatrix}\nb_{11} & 0 & 0 \\\\\nb_{12} & b_{22} & 0 \\\\\nb_{13} & b_{23} & b_{33}\n\\end{pmatrix}}_{B'}\n\\underbrace{\\begin{pmatrix}\nb_{11} & b_{12} & b_{13} \\\\\n0 & b_{22} & b_{23} \\\\\n0 & 0 & b_{33}\n\\end{pmatrix}}_{B}\n\\]\nMultiplying the matrices on the right yields the system of equations:\n\\[\nA = \\begin{pmatrix}\n\\mathbf{b_{11}^2} & b_{11}b_{12} & b_{11}b_{13} \\\\\nb_{12}b_{11} & \\mathbf{b_{12}^2 + b_{22}^2} & b_{12}b_{13} + b_{22}b_{23} \\\\\nb_{13}b_{11} & b_{13}b_{12} + b_{23}b_{22} & \\mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}\n\\end{pmatrix}\n\\]\nBy solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.\n\n\n4.2.2 The Algorithm\n\nRow 1: Solve for \\(b_{11}\\) using \\(a_{11}\\), then solve the rest of the row (\\(b_{1j}\\)) by division.\n\n\\(b_{11} = \\sqrt{a_{11}}\\)\n\\(b_{1j} = a_{1j}/b_{11}\\)\n\nRow 2: Solve for \\(b_{22}\\) using \\(a_{22}\\) and the known \\(b_{12}\\), then solve \\(b_{2j}\\).\n\n\\(b_{22} = \\sqrt{a_{22} - b_{12}^2}\\)\n\\(b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}\\)\n\nRow 3: Solve for \\(b_{33}\\) using \\(a_{33}\\) and the known \\(b_{13}, b_{23}\\).\n\n\\(b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}\\)\n\n\n\n\n4.2.3 Numerical Example\nConsider the positive definite matrix \\(A\\): \\[\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n\\]\nWe find \\(B\\) such that \\(A = B'B\\):\n\nFirst Row of B (\\(b_{11}, b_{12}, b_{13}\\)):\n\n\\(b_{11} = \\sqrt{4} = 2\\)\n\\(b_{12} = 2 / 2 = 1\\)\n\\(b_{13} = -2 / 2 = -1\\)\n\nSecond Row of B (\\(b_{22}, b_{23}\\)):\n\n\\(b_{22} = \\sqrt{10 - (1)^2} = \\sqrt{9} = 3\\)\n\\(b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1\\)\n\nThird Row of B (\\(b_{33}\\)):\n\n\\(b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{4} = 2\\)\n\n\nResult: \\[\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#generalized-inverses",
    "href": "lec3-matrix.html#generalized-inverses",
    "title": "4  Spectral Theory and Generalized Inverse",
    "section": "4.3 Generalized Inverses",
    "text": "4.3 Generalized Inverses\n\n4.3.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]\n\n\n4.3.2 Definition of Generalized Inverse\n\nDefinition 4.4 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 4.2 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]\n\n\n\n\n4.3.3 A General Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend: * \\(\\otimes\\): Elements of submatrix \\(C\\) * \\(\\triangle\\): Elements of \\((C^{-1})'\\) * \\(\\square\\): Elements of \\(C^{-1}\\) (after transposition) * \\(\\times\\): Other elements (replaced by 0 in the final calculation)\n\n\n4.3.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]\n\n\n\n4.3.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 4.5 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\mathcal{C}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 4.3 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#least-squares-with-rank-deficient-matrix",
    "href": "lec3-matrix.html#least-squares-with-rank-deficient-matrix",
    "title": "4  Review of Matrix Algebra",
    "section": "4.5 Least Squares with Rank Deficient Matrix",
    "text": "4.5 Least Squares with Rank Deficient Matrix\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n4.5.1 Constructing a Solution\n\nProof. One specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n\n\nLeast Squares Derivation for Rank Deficient Matrix\n\n\n\n\n4.5.2 Alternative Interpretation\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\n\nProof. Solution Strategy 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution Strategy 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html",
    "href": "lec4-mvn.html",
    "title": "5  Multivariate Normal",
    "section": "",
    "text": "5.1 Motivation\nConsider the linear model: \\[\ny = X\\beta + \\epsilon, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nWe are often interested in the distributional properties of the response vector \\(y\\) and the residuals. Specifically, if \\(y = (y_1, \\dots, y_n)'\\), we need to understand its multivariate distribution. \\[\n\\hat{y} = Py, \\quad e = y - \\hat{y} = (I_n - P)y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#proof-of-property-5-covariance-of-linear-transformation",
    "href": "lec4-mvn.html#proof-of-property-5-covariance-of-linear-transformation",
    "title": "5  Multivariate Normal",
    "section": "7.1 Proof of Property 5 (Covariance of Linear Transformation)",
    "text": "7.1 Proof of Property 5 (Covariance of Linear Transformation)\n\\[\n\\begin{aligned}\n\\text{cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{cov}(x, y) B^T\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#proof-of-property-2-expectation-of-linear-transformation",
    "href": "lec4-mvn.html#proof-of-property-2-expectation-of-linear-transformation",
    "title": "5  Multivariate Normal",
    "section": "7.2 Proof of Property 2 (Expectation of Linear Transformation)",
    "text": "7.2 Proof of Property 2 (Expectation of Linear Transformation)\nTo prove \\(E(AXB) = A E(X) B\\): First consider \\(E(Ax_j)\\) where \\(x_j\\) is a column of \\(X\\). \\[\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n\\] Since \\(a_i\\) are constants: \\[\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n\\] Thus \\(E(Ax_j) = A E(x_j)\\). Applying this to all columns of \\(X\\): \\[\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n\\] Similarly, \\(E(XB) = E(X)B\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#proof-of-property-9-variance-of-sum",
    "href": "lec4-mvn.html#proof-of-property-9-variance-of-sum",
    "title": "5  Multivariate Normal",
    "section": "7.3 Proof of Property 9 (Variance of Sum)",
    "text": "7.3 Proof of Property 9 (Variance of Sum)\n\\[\n\\text{var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n\\] Let centered variables be denoted by differences. \\[\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n\\] Expanding terms: \\[\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n\\] \\[\n= \\text{var}(x_1) + \\text{cov}(x_1, x_2) + \\text{cov}(x_2, x_1) + \\text{var}(x_2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#definition-and-density",
    "href": "lec4-mvn.html#definition-and-density",
    "title": "5  Multivariate Normal",
    "section": "8.1 Definition and Density",
    "text": "8.1 Definition and Density\n\nDefinition 8.1 (Independent Standard Normal) Let \\(z = (z_1, \\dots, z_n)'\\) where \\(z_i \\sim N(0, 1)\\) are independent. We say \\(z \\sim N_n(0, I_n)\\). The joint PDF is the product of marginals: \\[\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n\\] Properties: \\(E(z) = 0\\) and \\(\\text{var}(z) = I_n\\) (Covariance is 0 for \\(i \\ne j\\), Variance is 1).\n\n\nDefinition 8.2 (Multivariate Normal Distribution) A random vector \\(x\\) (\\(n \\times 1\\)) has a multivariate normal distribution if it has the same distribution as: \\[\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n\\] where \\(z \\sim N_p(0, I_p)\\), \\(A\\) is a matrix of constants, and \\(\\mu\\) is a vector of constants. The moments are: * \\(E(x) = \\mu\\) * \\(\\text{var}(x) = AA^T = \\Sigma\\)\n\n\n8.1.1 Geometric Interpretation\nUsing Spectral Decomposition, \\(\\Sigma = Q \\Lambda Q'\\). We can view the transformation \\(x = Az + \\mu\\) as: 1. Scaling by eigenvalues (\\(\\Lambda^{1/2}\\)). 2. Rotation by eigenvectors (\\(Q\\)). 3. Shift by mean (\\(\\mu\\)).\n\n\n\n8.1.2 Probability Density Function\nIf \\(\\Sigma\\) is positive definite, the PDF exists. We use the change of variable formula for \\(x = Az + \\mu\\): \\[\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n\\] where \\(z = A^{-1}(x - \\mu)\\) and \\(J = \\det(A^{-1}) = |A|^{-1}\\).\n\\[\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n\\]\nUsing \\(|\\Sigma| = |AA^T| = |A|^2\\) and \\(\\Sigma^{-1} = (AA^T)^{-1}\\), we get: \\[\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#moment-generating-function",
    "href": "lec4-mvn.html#moment-generating-function",
    "title": "5  Multivariate Normal",
    "section": "8.2 Moment Generating Function",
    "text": "8.2 Moment Generating Function\n\nDefinition 8.3 (Moment Generating Function (MGF)) The MGF of a random vector \\(x\\) is \\(M_x(t) = E(e^{t^T x})\\). For \\(x = Az + \\mu\\): \\[\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n\\] Since \\(M_z(u) = e^{u^T u / 2}\\): \\[\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\n\nKey Properties: 1. Uniqueness: Two random vectors with the same MGF have the same distribution. 2. Independence: \\(y_1\\) and \\(y_2\\) are independent iff \\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#numerical-example",
    "href": "lec4-mvn.html#numerical-example",
    "title": "5  Multivariate Normal",
    "section": "11.1 Numerical Example",
    "text": "11.1 Numerical Example\nLet \\(\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix} \\right)\\). Find the distribution of \\(y_1 | y_2\\).\n\n\\(\\mu_{1|2} = \\mu_1 + \\sigma_{12}\\sigma_{22}^{-1}(y_2 - \\mu_2) = 1 + 1(4)^{-1}(y_2 - 2) = 0.5 + 0.25y_2\\)\n\\(\\sigma_{1|2}^2 = \\sigma_{11} - \\sigma_{12}\\sigma_{22}^{-1}\\sigma_{21} = 2 - 1(1/4)1 = 1.75\\)\n\nSo \\(y_1 | y_2 \\sim N(0.5 + 0.25y_2, 1.75)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#properties-of-mean-and-variance",
    "href": "lec4-mvn.html#properties-of-mean-and-variance",
    "title": "5  Multivariate Normal",
    "section": "5.3 Properties of Mean and Variance",
    "text": "5.3 Properties of Mean and Variance\nWe can derive several key algebraic properties for operations on random vectors.\n\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E(AXB) = A E(X) B\\) (In particular, \\(E(AX) = A\\mu_x\\))\n\\(\\text{cov}(x, y) = \\text{cov}(y, x)^T\\)\n\\(\\text{cov}(x + c, y + d) = \\text{cov}(x, y)\\)\n\\(\\text{cov}(Ax, By) = A \\text{cov}(x, y) B^T\\)\n\nSpecial case for scalars: \\(\\text{cov}(ax, by) = ab \\cdot \\text{cov}(x, y)\\)\n\n\\(\\text{cov}(x_1 + x_2, y_1) = \\text{cov}(x_1, y_1) + \\text{cov}(x_2, y_1)\\)\n\\(\\text{var}(x + c) = \\text{var}(x)\\)\n\\(\\text{var}(Ax) = A \\text{var}(x) A^T\\)\n\\(\\text{var}(x_1 + x_2) = \\text{var}(x_1) + \\text{cov}(x_1, x_2) + \\text{cov}(x_2, x_1) + \\text{var}(x_2)\\)\n\\(\\text{var}(\\sum x_i) = \\sum \\text{var}(x_i)\\) if independent.\n\n\n\n5.3.1 Proof of Property 5 (Covariance of Linear Transformation)\n\\[\n\\begin{aligned}\n\\text{cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{cov}(x, y) B^T\n\\end{aligned}\n\\]\n\n\n\n\n5.3.2 Proof of Property 2 (Expectation of Linear Transformation)\nTo prove \\(E(AXB) = A E(X) B\\): First consider \\(E(Ax_j)\\) where \\(x_j\\) is a column of \\(X\\). \\[\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n\\] Since \\(a_i\\) are constants: \\[\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n\\] Thus \\(E(Ax_j) = A E(x_j)\\). Applying this to all columns of \\(X\\): \\[\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n\\] Similarly, \\(E(XB) = E(X)B\\).\n\n\n\n\n5.3.3 Proof of Property 9 (Variance of Sum)\n\\[\n\\text{var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n\\] Let centered variables be denoted by differences. \\[\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n\\] Expanding terms: \\[\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n\\] \\[\n= \\text{var}(x_1) + \\text{cov}(x_1, x_2) + \\text{cov}(x_2, x_1) + \\text{var}(x_2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#the-multivariate-normal-distribution",
    "href": "lec4-mvn.html#the-multivariate-normal-distribution",
    "title": "5  Multivariate Normal",
    "section": "5.4 The Multivariate Normal Distribution",
    "text": "5.4 The Multivariate Normal Distribution\n\n5.4.1 Definition and Density\n\nDefinition 5.6 (Independent Standard Normal) Let \\(z = (z_1, \\dots, z_n)'\\) where \\(z_i \\sim N(0, 1)\\) are independent. We say \\(z \\sim N_n(0, I_n)\\). The joint PDF is the product of marginals: \\[\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n\\] Properties: \\(E(z) = 0\\) and \\(\\text{var}(z) = I_n\\) (Covariance is 0 for \\(i \\ne j\\), Variance is 1).\n\n\nDefinition 5.7 (Multivariate Normal Distribution) A random vector \\(x\\) (\\(n \\times 1\\)) has a multivariate normal distribution if it has the same distribution as: \\[\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n\\] where \\(z \\sim N_p(0, I_p)\\), \\(A\\) is a matrix of constants, and \\(\\mu\\) is a vector of constants. The moments are:\n\n\\(E(x) = \\mu\\)\n\\(\\text{var}(x) = AA^T = \\Sigma\\)\n\n\n\n\n5.4.2 Geometric Interpretation\nUsing Spectral Decomposition, \\(\\Sigma = Q \\Lambda Q'\\). We can view the transformation \\(x = Az + \\mu\\) as:\n\nScaling by eigenvalues (\\(\\Lambda^{1/2}\\)).\nRotation by eigenvectors (\\(Q\\)).\nShift by mean (\\(\\mu\\)).\n\n\n\n\n5.4.3 Probability Density Function\nIf \\(\\Sigma\\) is positive definite, the PDF exists. We use the change of variable formula for \\(x = Az + \\mu\\): \\[\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n\\] where \\(z = A^{-1}(x - \\mu)\\) and \\(J = \\det(A^{-1}) = |A|^{-1}\\).\n\\[\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n\\]\nUsing \\(|\\Sigma| = |AA^T| = |A|^2\\) and \\(\\Sigma^{-1} = (AA^T)^{-1}\\), we get: \\[\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n\\]\n\n\n5.4.4 Moment Generating Function\n\nDefinition 5.8 (Moment Generating Function (MGF)) The MGF of a random vector \\(x\\) is \\(M_x(t) = E(e^{t^T x})\\). For \\(x = Az + \\mu\\): \\[\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n\\] Since \\(M_z(u) = e^{u^T u / 2}\\): \\[\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\n\nKey Properties:\n\nUniqueness: Two random vectors with the same MGF have the same distribution.\nIndependence: \\(y_1\\) and \\(y_2\\) are independent iff \\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#construction-and-linear-transformations",
    "href": "lec4-mvn.html#construction-and-linear-transformations",
    "title": "5  Multivariate Normal",
    "section": "5.5 Construction and Linear Transformations",
    "text": "5.5 Construction and Linear Transformations\n\nTheorem 5.1 (Constructing MVN Random Vector) Let \\(\\mu \\in \\mathbb{R}^n\\) and \\(\\Sigma\\) be an \\(n \\times n\\) symmetric positive semi-definite (p.s.d.) matrix. Then there exists a multivariate normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\).\nProof: Since \\(\\Sigma\\) is p.s.d., there exists \\(B\\) such that \\(\\Sigma = BB^T\\) (e.g., via Cholesky). Let \\(z \\sim N_n(0, I)\\) and define \\(x = Bz + \\mu\\).\n\n\nTheorem 5.2 (Linear Transformation Theorem) Let \\(x \\sim N_n(\\mu, \\Sigma)\\). Let \\(y = Cx + d\\) where \\(C\\) is \\(r \\times n\\) and \\(d\\) is \\(r \\times 1\\). Then: \\[\ny \\sim N_r(C\\mu + d, C \\Sigma C^T)\n\\]\nProof: \\(x = Az + \\mu\\) where \\(AA^T = \\Sigma\\). \\[\ny = C(Az + \\mu) + d = (CA)z + (C\\mu + d)\n\\] This fits the definition of MVN with mean \\(C\\mu + d\\) and variance \\(C \\Sigma C^T\\).\n\n\n5.5.1 Corollaries\n\nCorollary 5.1 (Marginals) Any subvector of a multivariate normal vector is also multivariate normal. If we partition \\(x = (x_1', x_2')'\\), we can use \\(C = (I_r, 0)\\) to show \\(x_1 \\sim N(\\mu_1, \\Sigma_{11})\\).\n\n\nCorollary 5.2 (Univariate Combinations) Any linear combination \\(a^T x\\) is univariate normal: \\[\na^T x \\sim N(a^T \\mu, a^T \\Sigma a)\n\\]\n\n\nCorollary 5.3 (Orthogonal Transformations) If \\(x \\sim N(0, I_n)\\) and \\(Q\\) is orthogonal (\\(Q'Q = I\\)), then \\(y = Q'x \\sim N(0, I_n)\\).\n\n\nCorollary 5.4 (Standardization) If \\(y \\sim N_n(\\mu, \\Sigma)\\) and \\(\\Sigma\\) is positive definite: \\[\n\\Sigma^{-1/2}(y - \\mu) \\sim N_n(0, I_n)\n\\] Proof: Let \\(z = \\Sigma^{-1/2}(y - \\mu)\\). Then \\(\\text{var}(z) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = I_n\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#independence",
    "href": "lec4-mvn.html#independence",
    "title": "5  Multivariate Normal",
    "section": "5.6 Independence",
    "text": "5.6 Independence\n\nTheorem 5.3 (Independence in MVN) Let \\(y \\sim N(\\mu, \\Sigma)\\) be partitioned into \\(y_1\\) and \\(y_2\\). \\[\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n\\] Then \\(y_1\\) and \\(y_2\\) are independent if and only if \\(\\Sigma_{12} = 0\\) (zero covariance).\n\n\n1. Independence \\(\\implies\\) Covariance is 0: This holds generally for any distribution. \\[\n\\text{cov}(y_1, y_2) = E[(y_1 - \\mu_1)(y_2 - \\mu_2)'] = 0\n\\]\n2. Covariance is 0 \\(\\implies\\) Independence: This is specific to MVN. We use MGFs. If \\(\\Sigma_{12} = 0\\), the quadratic form in the MGF splits: \\[\nt^T \\Sigma t = t_1^T \\Sigma_{11} t_1 + t_2^T \\Sigma_{22} t_2\n\\] The MGF becomes: \\[\nM_y(t) = \\exp(t_1^T \\mu_1 + \\frac{1}{2} t_1^T \\Sigma_{11} t_1) \\times \\exp(t_2^T \\mu_2 + \\frac{1}{2} t_2^T \\Sigma_{22} t_2)\n\\] \\[\nM_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\n\\] Thus, they are independent.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#conditional-distributions",
    "href": "lec4-mvn.html#conditional-distributions",
    "title": "5  Multivariate Normal",
    "section": "5.7 Conditional Distributions",
    "text": "5.7 Conditional Distributions\nWe often wish to find the distribution of a subvector \\(y_2\\) given the value of another subvector \\(y_1\\).\n\nLemma 5.1 (Constructing Independent Vectors) Let \\(y \\sim N_n(\\mu, \\Sigma)\\) partitioned into \\(y_1\\) and \\(y_2\\). Define: \\[\ny_{2|1} = y_2 - \\Sigma_{21}\\Sigma_{11}^{-1}y_1\n\\] Then \\(y_1\\) and \\(y_{2|1}\\) are independent.\nProof: Consider the linear transformation: \\[\n\\begin{pmatrix} y_1 \\\\ y_{2|1} \\end{pmatrix} = C y = \\begin{pmatrix} I & 0 \\\\ -\\Sigma_{21}\\Sigma_{11}^{-1} & I \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n\\] The covariance matrix is \\(C \\Sigma C'\\). The off-diagonal block is: \\[\n\\text{cov}(y_1, y_{2|1}) = \\Sigma_{11}(-\\Sigma_{11}^{-1}\\Sigma_{12}) + \\Sigma_{12} = -\\Sigma_{12} + \\Sigma_{12} = 0\n\\] Since covariance is zero, they are independent.\n\n\n\nTheorem 5.4 (Conditional Distribution Theorem) The conditional distribution of \\(y_2\\) given \\(y_1\\) is multivariate normal: \\[\ny_2 | y_1 \\sim N(\\mu_{2|1}, \\Sigma_{22|1})\n\\] where:\n\nConditional Mean: \\(E(y_2 | y_1) = \\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(y_1 - \\mu_1)\\)\nConditional Variance: \\(\\text{var}(y_2 | y_1) = \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\)\n\nProof: Write \\(y_2 = y_{2|1} + \\Sigma_{21}\\Sigma_{11}^{-1}y_1\\). Conditional on \\(y_1\\), the term \\(\\Sigma_{21}\\Sigma_{11}^{-1}y_1\\) is constant. Since \\(y_{2|1}\\) is independent of \\(y_1\\), its conditional distribution is simply its marginal distribution. Thus, the mean shifts by the constant term, and the variance remains that of \\(y_{2|1}\\) (the Schur complement).\n\n\n\n5.7.1 Numerical Example\nLet \\(\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix} \\right)\\). Find the distribution of \\(y_1 | y_2\\).\n\n\\(\\mu_{1|2} = \\mu_1 + \\sigma_{12}\\sigma_{22}^{-1}(y_2 - \\mu_2) = 1 + 1(4)^{-1}(y_2 - 2) = 0.5 + 0.25y_2\\)\n\\(\\sigma_{1|2}^2 = \\sigma_{11} - \\sigma_{12}\\sigma_{22}^{-1}\\sigma_{21} = 2 - 1(1/4)1 = 1.75\\)\n\nSo \\(y_1 | y_2 \\sim N(0.5 + 0.25y_2, 1.75)\\).\n\n\n\n5.7.2 Variance Decomposition\nThe Law of Total Variance states: \\[\n\\text{var}(y_2) = E[\\text{var}(y_2 | y_1)] + \\text{var}[E(y_2 | y_1)]\n\\] In the MVN case:\n\n\\(E[\\text{var}(y_2 | y_1)] = \\Sigma_{22|1}\\) (constant variance).\n\\(\\text{var}[E(y_2 | y_1)] = \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\) (explained variance). Summing these returns the total variance \\(\\Sigma_{22}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#partial-and-multiple-correlation",
    "href": "lec4-mvn.html#partial-and-multiple-correlation",
    "title": "5  Multivariate Normal",
    "section": "5.8 Partial and Multiple Correlation",
    "text": "5.8 Partial and Multiple Correlation\n\nDefinition 5.9 (Partial Correlation) The partial correlation between elements \\(y_i\\) and \\(y_j\\) given a set of variables \\(x\\) is derived from the conditional covariance matrix \\(\\Sigma_{y|x}\\): \\[\n\\rho_{ij|x} = \\frac{\\sigma_{ij|x}}{\\sqrt{\\sigma_{ii|x} \\sigma_{jj|x}}}\n\\] where \\(\\sigma_{ij|x}\\) are elements of \\(\\Sigma_{y|x} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\\).\n\n\nDefinition 5.10 (Multiple Correlation (\\(R^2\\))) For a scalar \\(y\\) and vector \\(x\\), the squared multiple correlation is the proportion of variance of \\(y\\) explained by the conditional mean: \\[\n\\rho_{y|x}^2 = \\frac{\\text{var}(E(y|x))}{\\text{var}(y)} = \\frac{\\Sigma_{yx} \\Sigma_{xx}^{-1} \\Sigma_{xy}}{\\sigma_{yy}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec4-mvn.html#random-vectors-and-matrices",
    "href": "lec4-mvn.html#random-vectors-and-matrices",
    "title": "5  Multivariate Normal",
    "section": "5.2 Random Vectors and Matrices",
    "text": "5.2 Random Vectors and Matrices\n\nDefinition 5.1 (Random Vector and Matrix) A Random Vector is a vector whose elements are random variables. E.g., \\[\nx_{k \\times 1} = (x_1, x_2, \\dots, x_k)^T\n\\] where \\(x_1, \\dots, x_k\\) are each random variables.\nA Random Matrix is a matrix whose elements are random variables. E.g., \\(X_{n \\times k} = (x_{ij})\\), where \\(x_{11}, \\dots, x_{nk}\\) are each random variables.\n\n\nDefinition 5.2 (Expected Value) The expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.\nFor \\(X_{n \\times k}\\): \\[\nE(X) = \\begin{pmatrix}\nE(x_{11}) & \\dots & E(x_{1k}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE(x_{n1}) & \\dots & E(x_{nk})\n\\end{pmatrix}\n\\]\n\\[\nE\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}\\right) = \\begin{pmatrix} E(x_1) \\\\ \\vdots \\\\ E(x_k) \\end{pmatrix}\n\\]\n\n\nDefinition 5.3 (Variance-Covariance Matrix) For a random vector \\(x_{k \\times 1} = (x_1, \\dots, x_k)^T\\), the matrix is:\n\\[\n\\text{var}(x) = \\Sigma_x = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2} & \\dots & \\sigma_{kk}\n\\end{pmatrix}\n\\]\nWhere:\n\n\\(\\sigma_{ij} = \\text{cov}(x_i, x_j) = E[(x_i - \\mu_i)(x_j - \\mu_j)]\\)\n\\(\\sigma_{ii} = \\text{var}(x_i) = E[(x_i - \\mu_i)^2]\\)\n\nIn matrix notation: \\[\n\\text{var}(x) = E[(x - \\mu_x)(x - \\mu_x)^T]\n\\] Note: \\(\\text{var}(x)\\) is symmetric.\n\n\n5.2.1 Derivation of Covariance Matrix Structure\nExpanding the vector multiplication for variance: \\[\n(x - \\mu_x)(x - \\mu_x)' \\quad \\text{where } \\mu_x = (\\mu_1, \\dots, \\mu_n)'\n\\] \\[\n= \\begin{pmatrix} x_1 - \\mu_1 \\\\ \\vdots \\\\ x_n - \\mu_n \\end{pmatrix} (x_1 - \\mu_1, \\dots, x_n - \\mu_n)\n\\] This results in the matrix \\(A = (a_{ij})\\) where \\(a_{ij} = (x_i - \\mu_i)(x_j - \\mu_j)\\). Taking expectations yields the covariance matrix elements \\(\\sigma_{ij}\\).\n\nDefinition 5.4 (Covariance Matrix (Two Vectors)) For random vectors \\(x_{k \\times 1}\\) and \\(y_{n \\times 1}\\), the covariance matrix is: \\[\n\\text{cov}(x, y) = E[(x - \\mu_x)(y - \\mu_y)^T] = \\begin{pmatrix}\n\\text{cov}(x_1, y_1) & \\dots & \\text{cov}(x_1, y_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{cov}(x_k, y_1) & \\dots & \\text{cov}(x_k, y_n)\n\\end{pmatrix}\n\\] Note that \\(\\text{cov}(x, x) = \\text{var}(x)\\).\n\n\nDefinition 5.5 (Correlation Matrix) The correlation matrix of a random vector \\(x\\) is: \\[\n\\text{corr}(x) = \\begin{pmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\dots & 1\n\\end{pmatrix}\n\\] where \\(\\rho_{ij} = \\text{corr}(x_i, x_j)\\).\nRelationships: Let \\(V_x = \\text{diag}(\\text{var}(x_1), \\dots, \\text{var}(x_k))\\). \\[\n\\Sigma_x = V_x^{1/2} \\rho_x V_x^{1/2} \\quad \\text{and} \\quad \\rho_x = (V_x^{1/2})^{-1} \\Sigma_x (V_x^{1/2})^{-1}\n\\] Similarly for two vectors: \\[\n\\Sigma_{xy} = V_x^{1/2} \\rho_{xy} V_y^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multivariate Normal</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#cholesky-decomposition-1",
    "href": "lec3-matrix.html#cholesky-decomposition-1",
    "title": "4  Review of Matrix Algebra",
    "section": "4.3 Cholesky Decomposition",
    "text": "4.3 Cholesky Decomposition\nA symmetric matrix \\(A\\) has a Cholesky decomposition if and only if it is non-negative definite (i.e., \\(x'Ax \\ge 0\\) for all \\(x\\)).\n\\[\nA = B'B\n\\]\nwhere \\(B\\) is an upper triangular matrix with non-negative diagonal entries.\n\nPositive Definite Case: If \\(A\\) is strictly positive definite, the diagonal elements of \\(B\\) are strictly positive (\\(b_{ii} &gt; 0\\)), and the decomposition is unique.\nNon-Negative Definite Case: If \\(A\\) is singular (but n.n.d.), some diagonal elements may be zero, and the decomposition is not necessarily unique without strict pivoting rules.\n\nThe Algorithm (for Positive Definite \\(A\\)):\nFor a \\(3 \\times 3\\) matrix, the elements of \\(B\\) are solved recursively: \\[\n\\begin{aligned}\n\\text{Step 1: } & b_{11} = \\sqrt{a_{11}} \\\\\n\\text{Step 2: } & b_{1j} = a_{1j}/b_{11} \\quad (\\text{for } j &gt; 1) \\\\\n\\text{Step 3: } & b_{22} = \\sqrt{a_{22} - b_{12}^2} \\\\\n\\text{Step 4: } & b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22} \\quad (\\text{for } j &gt; 2) \\\\\n\\text{Step 5: } & b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}\n\\end{aligned}\n\\]\n\n4.3.1 Numerical Example\nConsider the positive definite matrix \\(A\\): \\[\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n\\]\nWe find \\(B\\) such that \\(A = B'B\\):\n\nFirst Row of B:\n\n\\(b_{11} = \\sqrt{4} = 2\\)\n\\(b_{12} = 2 / 2 = 1\\)\n\\(b_{13} = -2 / 2 = -1\\)\n\nSecond Row of B:\n\n\\(b_{22} = \\sqrt{10 - 1^2} = \\sqrt{9} = 3\\)\n\\(b_{23} = (2 - (1)(-1)) / 3 = (2 + 1)/3 = 1\\)\n\nThird Row of B:\n\n\\(b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{6 - 1 - 1} = \\sqrt{4} = 2\\)\n\n\nResult: \\[\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nWhy Cholesky Decomposition is important?\n\nEfficiency: It is roughly twice as efficient as LU decomposition for solving linear systems (\\(Ax=b\\)) when \\(A\\) is symmetric and positive definite.\nInversion: Calculating the inverse is efficient: \\(A^{-1} = (B'B)^{-1} = B^{-1}(B')^{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Review of Matrix Algebra</span>"
    ]
  },
  {
    "objectID": "lec3-matrix.html#least-squares-with-generalized-inverse",
    "href": "lec3-matrix.html#least-squares-with-generalized-inverse",
    "title": "4  Spectral Theory and Generalized Inverse",
    "section": "4.4 Least Squares with Generalized Inverse",
    "text": "4.4 Least Squares with Generalized Inverse\nFor the normal equations \\((X'X)\\beta = X'y\\), a solution is given by: \\[\n\\hat{\\beta} = (X'X)^- X'y\n\\] The fitted values are \\[\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.\\] This \\(\\hat{y}\\) represents the unique orthogonal projection of \\(y\\) onto \\(Col(X)\\).\n\nTheorem 4.5 (Transpose Property of Generalized Inverses) \\((X^-)'\\) is a version of \\((X')^-\\). That is, \\((X^-)'\\) is a generalized inverse of \\(X'\\).\n\n\nProof. By definition, a generalized inverse \\(X^-\\) satisfies the property: \\[\nX X^- X = X\n\\]\nTo verify that \\((X^-)'\\) is a generalized inverse of \\(X'\\), we need to show that it satisfies the condition \\(A G A = A\\) where \\(A = X'\\) and \\(G = (X^-)'\\).\n\nStart with the fundamental definition: \\[\nX X^- X = X\n\\]\nTake the transpose of both sides of the equation: \\[\n(X X^- X)' = X'\n\\]\nApply the reverse order law for transposes, \\((ABC)' = C' B' A'\\): \\[\nX' (X^-)' X' = X'\n\\]\n\nSince substituting \\((X^-)'\\) into the generalized inverse equation for \\(X'\\) yields \\(X'\\), \\((X^-)'\\) is a valid generalized inverse of \\(X'\\).\n\n\nLemma 4.1 (Invariance of Generalized Least Squares) For any version of the generalized inverse \\((X'X)^-\\), the matrix \\(X'(X'X)^- X'\\) is invariant and equals \\(X'\\). \\[\nX'X(X'X)^- X' = X'\n\\]\n\nProof (using Projection): Let \\(P = X(X'X)^- X'\\). This is the projection matrix onto \\(\\mathcal{C}(X)\\). By definition of projection, \\(Px = x\\) for any \\(x \\in Col(X)\\). Since columns of \\(X\\) are in \\(Col(X)\\), \\(PX = X\\). Taking the transpose: \\((PX)' = X' \\implies X'P' = X'\\). Since projection matrices are symmetric (\\(P=P'\\)), \\(X'P = X'\\). Substituting \\(P\\): \\(X' X (X'X)^- X' = X'\\).\nProof (Direct Matrix Manipulation): Decompose \\(y = X\\beta + e\\) where \\(e \\perp Col(X)\\) (i.e., \\(X'e = 0\\)). \\[\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n\\] Using the property \\(A A^- A = A\\) (where \\(A=X'X\\)), the first term becomes \\(X'X\\beta\\). The second term is 0 because \\(X'e = 0\\). Thus, the expression simplifies to \\(X'X\\beta = X'(X\\beta) = X'\\hat{y}_{proj}\\). This implies the operator acts as \\(X'\\).\n\n4.4.1 Properties of the Projection Matrix\n\nTheorem 4.6 (Properties of Projection Matrix \\(P\\)) Let \\(P = X(X'X)^- X'\\). This matrix has the following properties:\n\nSymmetry: \\(P = P'\\).\nIdempotence: \\(P^2 = P\\). \\[\nP^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n\\] Using the identity from Lemma 4.1 (\\(X'X(X'X)^- X' = X'\\)), this simplifies to: \\[\nX(X'X)^- X' = P\n\\]\nUniqueness: \\(P\\) is unique and invariant to the choice of the generalized inverse \\((X'X)^-\\).\n\n\n\nProof. Proof of Uniqueness:\nLet \\(A\\) and \\(B\\) be two different generalized inverses of \\(X'X\\). Define \\(P_A = X A X'\\) and \\(P_B = X B X'\\). From Lemma 4.1, we know that \\(X' P_A = X'\\) and \\(X' P_B = X'\\).\nSubtracting these two equations: \\[\nX' (P_A - P_B) = 0\n\\] Taking the transpose, we get \\((P_A - P_B) X = 0\\). This implies that the columns of the difference matrix \\(D = P_A - P_B\\) are orthogonal to the columns of \\(X\\) (i.e., \\(D \\perp Col(X)\\)).\nHowever, by definition, the columns of \\(P_A\\) and \\(P_B\\) (and thus \\(D\\)) are linear combinations of the columns of \\(X\\) (i.e., \\(D \\in Col(X)\\)).\nThe only matrix that lies in the column space of \\(X\\) but is also orthogonal to the column space of \\(X\\) is the zero matrix. Therefore: \\[\nP_A - P_B = 0 \\implies P_A = P_B\n\\]\n\n\n\n\n4.4.2 Two Explicit Formulas\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n\n4.4.3 Constructing a Solution by Solving Normal Equations\n\nProof. One specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n\n4.4.4 Constructing a Solution by Solving Reparametrized \\(\\beta\\)\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\n\nProof. Solution Strategy 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution Strategy 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  }
]