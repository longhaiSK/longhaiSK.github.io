[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#audience-and-prerequisites",
    "href": "index.html#audience-and-prerequisites",
    "title": "Statistical Theory for Linear Models",
    "section": "Audience and Prerequisites",
    "text": "Audience and Prerequisites",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Theory for Linear Models",
    "section": "Key Features",
    "text": "Key Features",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Statistical Theory for Linear Models",
    "section": "Structure of the Book",
    "text": "Structure of the Book",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Theory for Linear Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe lectures were built upon the lecture notes for stat 8260 by Danniel Hall and the textbook LINEAR MODELS IN STATISTICS, Second Edition, by Alvin C. Rencher and G. Bruce Schaalje, ISBN 978-0-471-75498-5 (cloth)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1 Overview\nThis lecture covers the foundations of Linear Statistical Models, including Multiple Linear Regression and ANOVA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#overview",
    "href": "introlm.html#overview",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1.1 Geometric Interpretation of Least Squares\nWe compare models based on the reduction of errors. Consider a full model and a reduced model (\\(K_1\\)).\nLet:\n\n\\(y\\) be the observed vector.\n\\(\\hat{y}_1\\) be the prediction from the reduced model.\n\\(\\hat{y}_2\\) be the prediction from the full model.\n\n\n\n\nGeometric Interpretation of Least Squares\n\n\nThe errors (residuals) are defined as: \\[\ne_1 = y - \\hat{y}_1\n\\] \\[\ne_2 = y - \\hat{y}_2\n\\]\nThe Sum of Squared Errors (SSE) representing the distance is: \\[\nSSE_1 = ||y - \\hat{y}_1||^2\n\\] \\[\nSSE_2 = ||y - \\hat{y}_2||^2\n\\]\nThe statistical test is often based on the comparison of \\(SSE_1\\) and \\(SSE_2\\) (or the reduction in sums of squares \\(||\\hat{y}_2 - \\hat{y}_1||^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#multiple-linear-regression",
    "href": "introlm.html#multiple-linear-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\n\n2.2.1 Matrix Formulation\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nWhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n)\n\\]\nAnd \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\]\nThe scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#polynomial-regression",
    "href": "introlm.html#polynomial-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.3 Polynomial Regression",
    "text": "2.3 Polynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n2.3.1 Design Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#one-way-anova",
    "href": "introlm.html#one-way-anova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.4 One-Way ANOVA",
    "text": "2.4 One-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\n\n\nOne-Way ANOVA Diagram\n\n\n\n2.4.1 Dummy Variable Matrix\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#analysis-of-covariance-ancova",
    "href": "introlm.html#analysis-of-covariance-ancova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.5 Analysis of Covariance (ANCOVA)",
    "text": "2.5 Analysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,cont} & 1 & 0 \\\\\nX_{2,cont} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,cont} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]\n\n2.5.1 Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\nSSE = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{SST - SSE}{SST}\n\\] where \\(SST = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html",
    "href": "lec2-vecspace.html",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "3.1 Vector and Projection onto a Line\nVectors and Operations\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\nVector Arithmetic: Vectors can be manipulated geometrically:\nScalar Multiplication and Length\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\nWe often need to quantify the “size” of a vector. This is done using the concept of length, or norm.\nAngle and Inner Product\nTo understand the relationship between two vectors \\(x\\) and \\(y\\) beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the classic Law of Cosines to this triangle, we can relate the geometric angle to the vector lengths.\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation provides a critical link between the geometric angle \\(\\theta\\) and the algebraic norms of the vectors.\nDerivation of Inner Product\nWe can express the squared distance term \\(||y - x||^2\\) purely algebraically by expanding the components:\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the Inner Product (or dot product).\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\nCoordinate (Scalar) Projection\nThe inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of this projection is given by:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis expression can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector in the direction of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]\nVector Projection Formula\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\nPerpendicularity (Orthogonality)\nA special case of the angle between vectors arises when \\(\\theta = 90^\\circ\\). This geometric concept of perpendicularity is central to the theory of projections and least squares.\nProjection onto a Line (Subspace)\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\nThe projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that it is the closest point on the line to \\(y\\). This implies that the error vector (or residual) must be perpendicular to the line itself.\nDerivation: To find the value of the scalar \\(c\\), we apply the orthogonality condition:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\nExpanding this inner product gives:\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\), we obtain:\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\nAlternative Forms of the Projection Formula\nWe can express the projection vector \\(\\hat{y}\\) in several equivalent ways to highlight different geometric interpretations.\nProjection Matrix (\\(P_x\\))\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector \\(y\\). This allows us to define a Projection Matrix.\nWe can rewrite the formula for \\(\\hat{y}\\) by factoring out \\(y\\):\n\\[\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n\\]\nThis leads to the definition of the projection matrix \\(P_x\\).\nExample: Projection in \\(\\mathbb{R}^2\\)\nLet’s apply these concepts to a concrete example.\nExample: Projection onto the Mean Vector\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\nPythagorean Theorem\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\nProof: We expand the squared norm using the inner product:\n\\[\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n\\]\nSince \\(x \\perp y\\), the inner product \\(x'y = 0\\). Thus, the term \\(2x'y\\) vanishes, leaving:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\nLeast Square Property\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector \\(y\\) and the subspace (or line) onto which it is projected.\nProof: Since both \\(\\hat{y}\\) and \\(y^*\\) lie on the line \\(L(x)\\), their difference \\((\\hat{y} - y^*)\\) also lies on \\(L(x)\\). From the definition of projection, the residual \\((y - \\hat{y})\\) is orthogonal to the line \\(L(x)\\). Therefore:\n\\[\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n\\]\nWe can write the vector \\((y - y^*)\\) as: \\[\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n\\]\nApplying the Pythagorean Theorem: \\[\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n\\]\nSince \\(||\\hat{y} - y^*||^2 \\ge 0\\), it follows that: \\[\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "href": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Least Square Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nLeast Square Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#general-vector-space",
    "href": "lec2-vecspace.html#general-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.2 General Vector Space",
    "text": "3.2 General Vector Space\nWe now generalize our discussion from lines to broader spaces.\n\nDefinition 3.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\nIt follows that the zero vector \\(0\\) must belong to any subspace (by choosing \\(c=0\\)).\nSpanned Vector Space\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\nDefinition 3.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\n\n\n\nSpanned Vector Space\n\n\nColumn Space and Row Space\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\nDefinition 3.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(Col(X)\\), is the vector space spanned by its columns:\n\\[\nCol(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 3.16 (Row Space) The Row Space, denoted \\(Row(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\n\n\n\nLinear Independence and Rank\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\nDefinition 3.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "href": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.3 Rank of Matrices and Dim of Vector Space",
    "text": "3.3 Rank of Matrices and Dim of Vector Space\n\nDefinition 3.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(Col(X))\n\\]\n\n\n3.3.0.1 Properties of Rank\nThere are several fundamental properties regarding the rank of a matrix.\n\nTheorem 3.4 (Properties of Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(Col(X)) = \\text{Dim}(Row(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\n\nProof. Let \\(X\\) be an \\(n \\times p\\) matrix. Let \\(r\\) be the row rank of \\(X\\). This means the dimension of the row space is \\(r\\). Let \\(u_1, \\dots, u_r\\) be a basis for the row space of \\(X\\) (these are row vectors). Since every row of \\(X\\) is in the row space, each row \\(x_{i.}\\) can be written as a linear combination of the basis vectors: \\[\n  x_{i.} = c_{i1}u_1 + c_{i2}u_2 + \\dots + c_{ir}u_r \\quad \\text{for } i=1,\\dots,n\n  \\]\nWe can write this in matrix notation as: \\[\n  X = C U\n  \\] where \\(C\\) is an \\(n \\times r\\) matrix of coefficients \\(c_{ij}\\), and \\(U\\) is an \\(r \\times p\\) matrix with rows \\(u_1, \\dots, u_r\\).\nNow consider the columns of \\(X\\). Since \\(X = CU\\), the columns of \\(X\\) are linear combinations of the columns of \\(C\\). Let \\(c^{(j)}\\) be the \\(j\\)-th column of \\(C\\). The columns of \\(X\\) lie in the space spanned by \\(\\{c^{(1)}, \\dots, c^{(r)}\\}\\). Thus, the column space of \\(X\\), \\(Col(X)\\), is a subspace of the column space of \\(C\\). \\[\n  \\text{Dim}(Col(X)) \\le \\text{Dim}(Col(C)) \\le r\n  \\] The dimension of the column space of \\(C\\) is at most \\(r\\) (since \\(C\\) has only \\(r\\) columns). Therefore, Column Rank \\(\\le\\) Row Rank.\nApplying the same logic to \\(X'\\), we get Row Rank \\(\\le\\) Column Rank. Combining these inequalities gives: Row Rank = Column Rank.\n\n\nExample: 2x3 Matrix\nConsider the following \\(2 \\times 3\\) matrix: \\[\nX = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n\\]\nRow Rank: The rows are \\(r_1 = (1, 0, 1)\\) and \\(r_2 = (0, 1, 1)\\). Neither is a multiple of the other, so they are linearly independent. \\[\n\\text{Row Rank} = 2\n\\]\nColumn Rank: The columns are \\(c_1 = \\binom{1}{0}\\), \\(c_2 = \\binom{0}{1}\\), and \\(c_3 = \\binom{1}{1}\\). Notice that \\(c_3 = c_1 + c_2\\). The third column is dependent on the first two. However, \\(c_1\\) and \\(c_2\\) are independent (standard basis vectors). \\[\n\\text{Column Rank} = 2\n\\]\nThus, Rank(Row) = Rank(Col) = 2.\n\nOrthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 3.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 3.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n\nOrthogonal Complement\n\n\nKernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 3.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = Col(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 3.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [Row(X)]^\\perp\n\\]\n\nNullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 3.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\]\nWhere \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\n\n\nRank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 3.7 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\nProof: The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\nCol(XZ) \\subseteq Col(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\nRow(XZ) \\subseteq Row(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\nCombining these gives the result.\n\n\n\nRank of Matrix Product\n\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 3.8 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\nProof: From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\n\n\nRank Preservation with Invertible Matrices\n\n\n–\nRank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 3.9 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\nProof Strategy: We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\n\n\nRank of Gram Matrix\n\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 3.10 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\nCol(XX') = Col(X)\n\\]\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(Col(X)\\) lies in the same space as the projection onto \\(Col(XX')\\). This is vital for the existence of solutions in generalized least squares.\n\n\n\nColumn Space Equivalence",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "href": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "3.4 Projection via Orthonormal Basis",
    "text": "3.4 Projection via Orthonormal Basis\nBasis and Dimension\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 3.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\nThe Projection Theorem\nThis theorem establishes the existence and uniqueness of the projection vector for any subspace, regardless of the basis used.\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\): * \\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)). * \\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-projection-matrices",
    "href": "lec2-vecspace.html#projection-via-projection-matrices",
    "title": "3  Vector Space and Projection",
    "section": "3.5 Projection via Projection Matrices",
    "text": "3.5 Projection via Projection Matrices\n\nTheorem 3.12 (Projection Matrix (Orthonormal)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 3.13 (Properties of Projection Matrices) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\nProof of Idempotence: If \\(\\hat{y} = Py\\) is already in the subspace \\(Col(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 3.4 (ANOVA as Projection) Consider a one-way ANOVA model: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i\\) represents the group and \\(j\\) represents the observation within the group.\nWe can define dummy variables (indicators) for each group. Let \\(x_1\\) be the indicator for group 1, \\(x_2\\) for group 2, etc. These vectors are mutually orthogonal because an observation cannot belong to two groups simultaneously.\nThe projection of the data vector \\(y\\) onto the space spanned by these indicators is the sum of the projections onto each group vector:\n\\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\]\nSince the indicators are orthogonal, this simplifies to calculating the mean for each group. The fitted value for any observation \\(y_{ij}\\) is simply the group mean \\(\\bar{y}_{i.}\\).\n\n\n\n\nANOVA Projection Geometry\n\n\nProjection onto Orthogonal Complement\nIf \\(P\\) is the projection matrix onto a subspace \\(V\\), we can easily define the projection onto the orthogonal complement \\(V^\\perp\\) (the “error” space).\n\nDefinition 3.25 (Projection onto Complement) The matrix \\(M = I - P\\) is the projection matrix onto the orthogonal complement \\(Col(X)^\\perp\\).\nProperties of \\(M\\):\n\nIdempotent: \\(M^2 = (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P = M\\).\nSymmetric: \\(M' = (I-P)' = I - P' = I - P = M\\).\nOrthogonal to \\(P\\): \\(PM = P(I-P) = P - P^2 = 0\\).\n\n\nThis matrix \\(M\\) produces the residuals: \\(e = My = (I-P)y = y - \\hat{y}\\).\nGram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.\n\n\n\nGram-Schmidt Process\n\n\n\nProjection Matrix Definition\nWe now formally derive the projection matrix \\(P\\) for the general case where we project \\(y\\) onto the column space of a matrix \\(X\\).\nNormal Equations\nWe want to find \\(\\hat{y} = X\\beta\\) such that the residual \\((y - \\hat{y})\\) is orthogonal to the column space \\(Col(X)\\). This means the residual must be orthogonal to every column of \\(X\\).\n\\[\nX'(y - X\\beta) = 0\n\\]\nExpanding this gives the famous Normal Equations:\n\\[\nX'y - X'X\\beta = 0 \\implies X'X\\beta = X'y\n\\]\n\nTheorem 3.14 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\nNormal Equations Derivation\n\n\nThe Matrix \\(P\\)\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 3.26 (General Projection Matrix) The projection of \\(y\\) onto \\(Col(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the projection matrix \\(P\\) is defined as:\n\\[\nP = X(X'X)^{-1}X'\n\\]\n\nRelationship with QR Decomposition\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(Col(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\nP = QR((QR)'(QR))^{-1}(QR)'\n\\] \\[\n= QR(R'Q'QR)^{-1}R'Q'\n\\] \\[\n= QR(R'R)^{-1}R'Q'\n\\] \\[\n= QR R^{-1} (R')^{-1} R' Q'\n\\] \\[\n= Q Q'\n\\]\nThis confirms that \\(P = QQ'\\) is consistent with the general formula \\(P = X(X'X)^{-1}X'\\).\nProperties of \\(P\\)\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 3.15 (Properties of P) The matrix \\(P = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(P' = P\\)\nIdempotent: \\(P^2 = P\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(P) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\nProjection onto Complement\nAs before, the projection onto the orthogonal complement (the residual maker matrix) is \\(M = I - P\\).\n\nDefinition 3.27 (Residual Maker Matrix M) \\[\nM = I - X(X'X)^{-1}X'\n\\]\nThis matrix projects \\(y\\) onto the null space of \\(X'\\) (the orthogonal complement of the column space of \\(X\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "href": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "title": "3  Vector Space and Projection",
    "section": "3.6 Projections onto Nested Subspaces",
    "text": "3.6 Projections onto Nested Subspaces\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\nDefinition 3.28 (Nested Models) Consider two models: 1. Reduced Model (\\(M_0\\)): \\(y \\in Col(X_0)\\) 2. Full Model (\\(M_1\\)): \\(y \\in Col(X_1)\\)\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\nCol(X_0) \\subseteq Col(X_1)\n\\]\n\nUsually, \\(X_1\\) is constructed by adding columns to \\(X_0\\): \\(X_1 = [X_0, X_{new}]\\).\n\n\n\nNested Models Concept\n\n\nProjection Composition\nLet \\(P_0\\) be the projection matrix onto \\(Col(X_0)\\) and \\(P_1\\) be the projection matrix onto \\(Col(X_1)\\). Since \\(Col(X_0) \\subseteq Col(X_1)\\), we have important relationships between these matrices.\n\nTheorem 3.16 (Composition of Projections) If \\(Col(P_0) \\subseteq Col(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\nDifference of Projections\nThe difference between the two projection matrices, \\(P_1 - P_0\\), is itself a projection matrix.\n\nTheorem 3.17 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(Col(X_1) \\cap Col(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\nDecomposition of Sum of Squares\nThis geometry allows us to decompose the total vector \\(y\\) into three orthogonal components:\n\n\\(\\hat{y}_0\\) (The fit of the reduced model)\n\\(\\hat{y}_1 - \\hat{y}_0\\) (The improvement from the reduced to the full model)\n\\(y - \\hat{y}_1\\) (The residual of the full model)\n\n\\[\n  y = \\hat{y}_0 + (\\hat{y}_1 - \\hat{y}_0) + (y - \\hat{y}_1)\n  \\]\nSquaring the norms (applying Pythagoras):\n\\[\n  ||y||^2 = ||\\hat{y}_0||^2 + ||\\hat{y}_1 - \\hat{y}_0||^2 + ||y - \\hat{y}_1||^2\n  \\]\nThis equation is the foundation for the F-test in ANOVA and regression.\n\n\n\nDecomposition of Sum of Squares\n\n\nANOVA Sum of Squares\nWe apply the decomposition of sum of squares to the specific case of Analysis of Variance.\n\nDefinition 3.29 (ANOVA Decomposition) The Total Sum of Squares (TSS), or the squared norm of \\(y\\) (often centered), can be split into components:\n\nResidual Sum of Squares (Full Model): \\[\nRSS_1 = ||y - \\hat{y}_1||^2 = \\sum_{i}\\sum_{j} (y_{ij} - \\bar{y}_{i.})^2\n\\] This represents the “Within Group” sum of squares.\nDifference Sum of Squares: \\[\nRSS_0 - RSS_1 = ||\\hat{y}_1 - \\hat{y}_0||^2 = \\sum_{i} n_i (\\bar{y}_{i.} - \\bar{y}_{..})^2\n\\] This represents the “Between Group” sum of squares (\\(SS_{between}\\)).\n\n\nThis relationship confirms that: \\[\nTSS = SS_{within} + SS_{between}\n\\]\n\n\n\nANOVA Sum of Squares Derivation\n\n\nProjections in Orthogonal Spaces\nFinally, we consider the case where the entire space \\(\\mathbb{R}^n\\) is decomposed into mutually orthogonal subspaces.\n\nTheorem 3.18 (Orthogonal Decomposition) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = x_1 + x_2 + \\dots + x_k\n\\] where \\(x_i \\in V_i\\).\nFurthermore, each component \\(x_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\nx_i = P_i y\n\\]\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: \\[\nI_n = P_1 + P_2 + \\dots + P_k\n\\]\n\n\n\nOrthogonal Space Decomposition\n\n\nThe following diagram summarizes the relationships between the vector spaces and projections discussed in this lecture.\n\n\n\nSummary Diagram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical Theory for Linear Models",
    "section": "Overview",
    "text": "Overview\nThis book has evolved from the lecture notes for STAT 851: Statistical Linear Models, a graduate-level course taught at the University of Saskatchewan. This course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  }
]