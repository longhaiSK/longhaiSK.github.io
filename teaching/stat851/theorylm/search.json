[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Theory for Linear Models",
    "section": "Key Features",
    "text": "Key Features\nThis text adopts a geometric approach to the statistical theory of linear models, aiming to provide a deeper understanding than standard algebraic treatments. Key features include:\n\nProjection Perspective: We prioritize the geometric interpretation of least squares, viewing estimation as a projection of the response vector onto a model subspace. This visual framework unifies diverse topics—from simple regression to complex ANOVA designs—under a single theoretical umbrella. We emphasize the geometric perspective not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:\n\nStatistical Clarity: Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.\nComputational Stability: A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic “Normal Equations” (\\((X'X)^{-1}X'y\\)) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.\nGeneralizability: The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.\n\nInteractive Visualizations: Abstract concepts are brought to life through interactive 3D plots. Readers can rotate and inspect vector spaces, residual planes, and projection geometries to build a tangible intuition for high-dimensional operations.\nComputational Integration: Theory is seamlessly integrated with practice. The text provides implementation examples using R (and Python), demonstrating how theoretical matrix equations translate directly into computational code.\nRigorous Foundations: While visually driven, the text maintains mathematical rigor, covering essential topics such as spectral theory, the generalized inverseand the multivariate normal distribution to ensure a solid theoretical grounding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical Theory for Linear Models",
    "section": "Overview",
    "text": "Overview\nThis course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Statistical Theory for Linear Models",
    "section": "Audience",
    "text": "Audience\nThis book is designed for graduate students and advanced undergraduate students in statistics, data science, and related quantitative fields. It serves as a bridge between applied regression analysis and the theoretical foundations of linear models. Researchers and practitioners seeking a deeper geometric and algebraic understanding of the statistical methods they use daily will also find this text valuable.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Statistical Theory for Linear Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, readers should have a comfortable grasp of the following topics:\nLinear Algebra: An elementary understanding of matrix operations is essential. You should be familiar with matrix multiplication, determinants, inversion, and the basic concepts of vector spaces (such as linear independence, basis vectors, and subspaces). While we review key spectral theory concepts (like eigenvalues and the singular value decomposition) in the early chapters, prior exposure to these ideas is helpful.\nProbability and Statistics: A standard introductory course in probability and mathematical statistics is required. Readers should be familiar with random variables, expectation, variance, covariance, common probability distributions (especially the Normal distribution), and fundamental concepts of hypothesis testing and estimation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "Introduction",
    "section": "",
    "text": "Multiple Linear Regression\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nwhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n),\n\\]\nin which \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\] The scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#examples",
    "href": "introlm.html#examples",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\nPolynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n\nDesign Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]\n\n\nOne-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\\[\n\\overset{G_1}{\n  \\boxed{\n    \\begin{matrix} Y_{11} \\\\ Y_{12} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_2}{\n  \\boxed{\n    \\begin{matrix} Y_{21} \\\\ Y_{22} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_3}{\n  \\boxed{\n    \\begin{matrix} Y_{31} \\\\ Y_{32} \\end{matrix}\n  }\n}\n\\]\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]\n\n\nAnalysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,cont} & 1 & 0 \\\\\nX_{2,cont} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,cont} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#least-squares-estimation",
    "href": "introlm.html#least-squares-estimation",
    "title": "Introduction",
    "section": "Least Squares Estimation",
    "text": "Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\nSSE = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{SST - SSE}{SST}\n\\] where \\(SST = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#geometric-perspective-of-least-square-estimation",
    "href": "introlm.html#geometric-perspective-of-least-square-estimation",
    "title": "Introduction",
    "section": "Geometric Perspective of Least Square Estimation",
    "text": "Geometric Perspective of Least Square Estimation\nWe align the coordinate system to the models for clarity:\n\nReduced Model (\\(M_0\\)): Represented by the X-axis (labeled \\(j_3\\)).\n\n\\(\\hat{y}_0\\) is the projection of \\(y\\) onto this axis.\n\nFull Model (\\(M_1\\)): Represented by the XY-plane (the floor).\n\n\\(\\hat{y}_1\\) is the projection of \\(y\\) onto this plane (\\(z=0\\)).\n\nObserved Data (\\(y\\)): A point in 3D space.\n\nThe “improvement” due to adding predictors is the distance between \\(\\hat{y}_0\\) and \\(\\hat{y}_1\\).\n\n\n\n\n\n\n\n\nFigure 1: Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "lec1-vecspace.html",
    "href": "lec1-vecspace.html",
    "title": "1  Projection in Vector Space",
    "section": "",
    "text": "1.1 Vector and Projection onto a Line\nVectors and Operations\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\nVector Arithmetic: Vectors can be manipulated geometrically:\nScalar Multiplication and Length\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\nWe often need to quantify the “size” of a vector. This is done using the concept of length, or norm.\nAngle and Inner Product\nTo understand the relationship between two vectors \\(x\\) and \\(y\\) beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the classic Law of Cosines to this triangle, we can relate the geometric angle to the vector lengths.\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation provides a critical link between the geometric angle \\(\\theta\\) and the algebraic norms of the vectors.\nDerivation of Inner Product\nWe can express the squared distance term \\(||y - x||^2\\) purely algebraically by expanding the components:\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the Inner Product (or dot product).\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\nCoordinate (Scalar) Projection\nThe inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of this projection is given by:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis expression can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector in the direction of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]\nVector Projection Formula\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\nPerpendicularity (Orthogonality)\nA special case of the angle between vectors arises when \\(\\theta = 90^\\circ\\). This geometric concept of perpendicularity is central to the theory of projections and least squares.\nProjection onto a Line (Subspace)\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\nThe projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that it is the closest point on the line to \\(y\\). This implies that the error vector (or residual) must be perpendicular to the line itself.\nDerivation: To find the value of the scalar \\(c\\), we apply the orthogonality condition:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\nExpanding this inner product gives:\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\), we obtain:\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\nAlternative Forms of the Projection Formula\nWe can express the projection vector \\(\\hat{y}\\) in several equivalent ways to highlight different geometric interpretations.\nProjection Matrix (\\(P_x\\))\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector \\(y\\). This allows us to define a Projection Matrix.\nWe can rewrite the formula for \\(\\hat{y}\\) by factoring out \\(y\\):\n\\[\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n\\]\nThis leads to the definition of the projection matrix \\(P_x\\).\nExample: Projection in \\(\\mathbb{R}^2\\)\nLet’s apply these concepts to a concrete example.\nExample: Projection onto the Ones Vector (\\(j_n\\))\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\nPythagorean Theorem\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\nLeast Square Property\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector \\(y\\) and the subspace (or line) onto which it is projected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#vector-and-projection-onto-a-line",
    "href": "lec1-vecspace.html#vector-and-projection-onto-a-line",
    "title": "1  Projection in Vector Space",
    "section": "",
    "text": "Definition 1.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 1.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 1.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nDefinition 1.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 1.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nTheorem 1.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 1.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 1.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 1.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 1.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 1.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 1.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample 1.3 (Projection onto the Ones Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nTheorem 1.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\nProof. We expand the squared norm using the inner product:\n\\[\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n\\]\nSince \\(x \\perp y\\), the inner product \\(x'y = 0\\). Thus, the term \\(2x'y\\) vanishes, leaving:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 1.3 (Least Square Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\nProof. Since both \\(\\hat{y}\\) and \\(y^*\\) lie on the line \\(L(x)\\), their difference \\((\\hat{y} - y^*)\\) also lies on \\(L(x)\\). From the definition of projection, the residual \\((y - \\hat{y})\\) is orthogonal to the line \\(L(x)\\). Therefore:\n\\[\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n\\]\nWe can write the vector \\((y - y^*)\\) as: \\[\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n\\]\nApplying the Pythagorean Theorem: \\[\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n\\]\nSince \\(||\\hat{y} - y^*||^2 \\ge 0\\), it follows that: \\[\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#general-vector-space",
    "href": "lec1-vecspace.html#general-vector-space",
    "title": "1  Projection in Vector Space",
    "section": "1.2 General Vector Space",
    "text": "1.2 General Vector Space\nWe now generalize our discussion from lines to broader spaces.\n\nDefinition 1.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\nIt follows that the zero vector \\(0\\) must belong to any subspace (by choosing \\(c=0\\)).\nSpanned Vector Space\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\nDefinition 1.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\nColumn Space and Row Space\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\nDefinition 1.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(\\text{Col}(X)\\), is the vector space spanned by its columns:\n\\[\n\\text{Col}(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 1.16 (Row Space) The Row Space, denoted \\(\\text{Row}(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\nLinear Independence and Rank\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\nDefinition 1.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "href": "lec1-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "title": "1  Projection in Vector Space",
    "section": "1.3 Rank of Matrices and Dim of Vector Space",
    "text": "1.3 Rank of Matrices and Dim of Vector Space\n\nDefinition 1.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(\\text{Col}(X))\n\\]\n\nThere are several fundamental properties regarding the rank of a matrix.\n\nExample 1.4 (Example of the Equality of Row and Col Rank) Consider the following \\(3 \\times 4\\) matrix (\\(n=3, p=4\\)): \\[\nX = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}\n\\] Notice that the third row is the sum of the first two (\\(r_3 = r_1 + r_2\\)).\n1. Row Rank and Basis \\(U\\) The first two rows are linearly independent. We set the row rank \\(r=2\\) and use these rows as our basis matrix \\(U\\) (\\(2 \\times 4\\)): \\[\nU = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1\n\\end{pmatrix}\n\\]\n2. Coefficient Matrix \\(C\\) We express every row of \\(X\\) as a linear combination of the rows of \\(U\\):\n\nRow 1: \\(1 \\cdot u_1 + 0 \\cdot u_2\\)\nRow 2: \\(0 \\cdot u_1 + 1 \\cdot u_2\\)\nRow 3: \\(1 \\cdot u_1 + 1 \\cdot u_2\\)\n\nThese coefficients form the matrix \\(C\\) (\\(3 \\times 2\\)): \\[\nC = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\]\n3. The Decomposition (\\(X = CU\\)) We verify that \\(X\\) is the product of \\(C\\) and \\(U\\): \\[\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}}_{X \\ (3 \\times 4)}\n=\n\\underbrace{\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}}_{C \\ (3 \\times 2)}\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix}}_{U \\ (2 \\times 4)}\n\\]\n4. Conclusion on Column Rank The columns of \\(X\\) are linear combinations of the columns of \\(C\\). \\[\n\\text{Col}(X) \\subseteq \\text{Col}(C)\n\\] Since \\(C\\) has only 2 columns, the dimension of its column space (and thus \\(X\\)’s column space) cannot exceed 2. \\[\n\\text{Dim}(\\text{Col}(X)) \\le 2\n\\] This confirms that Row Rank (2) \\(\\ge\\) Column Rank. (By symmetry, they are equal).\n\n\nTheorem 1.4 (Row Rank equals Column Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(\\text{Col}(X)) = \\text{Dim}(\\text{Row}(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\nOrthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 1.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 1.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\nKernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 1.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = \\text{Col}(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 1.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n\\]\n\n\nProof. Let \\(x \\in \\mathbb{R}^p\\). \\(x \\in \\text{Ker}(X)\\) if and only if \\(Xx = 0\\). If we denote the rows of \\(X\\) as \\(r_1', \\dots, r_n'\\), then the equation \\(Xx = 0\\) is equivalent to the system of equations: \\[\n\\begin{pmatrix} r_1' \\\\ \\vdots \\\\ r_n' \\end{pmatrix} x = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\iff r_i' x = 0 \\text{ for all } i = 1, \\dots, n\n\\] This means \\(x\\) is orthogonal to every row of \\(X\\). Since the rows span the row space \\(\\text{Row}(X)\\), being orthogonal to every generator \\(r_i\\) implies \\(x\\) is orthogonal to the entire space \\(\\text{Row}(X)\\). Thus, \\(\\text{Ker}(X) = \\{ x \\mid x \\perp \\text{Row}(X) \\} = [\\text{Row}(X)]^\\perp\\).\n\nNullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 1.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\] where \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\nProof. From the previous theorem, we established that the kernel is the orthogonal complement of the row space: \\[\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n\\]\nSince the row space is a subspace of \\(\\mathbb{R}^p\\), the entire space can be decomposed into the direct sum of the row space and its orthogonal complement: \\[\n\\mathbb{R}^p = \\text{Row}(X) \\oplus [\\text{Row}(X)]^\\perp = \\text{Row}(X) \\oplus \\text{Ker}(X)\n\\]\nTaking the dimensions of these spaces: \\[\n\\text{Dim}(\\mathbb{R}^p) = \\text{Dim}(\\text{Row}(X)) + \\text{Dim}(\\text{Ker}(X))\n\\]\nSubstituting the definitions of Rank (dimension of row/column space) and Nullity: \\[\np = \\text{Rank}(X) + \\text{Nullity}(X)\n\\]\n\nComparing Ranks via Kernel Containment\nThe Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices \\(A\\) and \\(B\\) (with the same number of columns) by inspecting their null spaces.\n\nTheorem 1.7 (Kernel Containment and Rank Inequality) Let \\(A\\) and \\(B\\) be two matrices with \\(p\\) columns. If the kernel of \\(A\\) is contained within the kernel of \\(B\\), then the rank of \\(A\\) is greater than or equal to the rank of \\(B\\).\n\\[\n\\text{Ker}(A) \\subseteq \\text{Ker}(B) \\implies \\text{Rank}(A) \\ge \\text{Rank}(B)\n\\]\n\n\nProof. From the subspace inclusion \\(\\text{Ker}(A) \\subseteq \\text{Ker}(B)\\), it follows that the dimension of the smaller space cannot exceed the dimension of the larger space: \\[\n\\text{Nullity}(A) \\le \\text{Nullity}(B)\n\\] Using the Rank-Nullity Theorem (\\(\\text{Rank} = p - \\text{Nullity}\\)), we reverse the inequality: \\[\np - \\text{Nullity}(A) \\ge p - \\text{Nullity}(B)\n\\] \\[\n\\text{Rank}(A) \\ge \\text{Rank}(B)\n\\]\n\nRank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 1.8 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\n\nProof. The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\n\\text{Col}(XZ) \\subseteq \\text{Col}(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\n\\text{Row}(XZ) \\subseteq \\text{Row}(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 1.9 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\n\nProof. From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\nRank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 1.10 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\n\nProof. We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 1.11 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\n\\text{Col}(XX') = \\text{Col}(X)\n\\]\n\n\nProof. \n\nForward (\\(\\subseteq\\)): Let \\(z \\in \\text{Col}(XX')\\). Then \\(z = XX'w\\) for some vector \\(w\\). We can rewrite this as \\(z = X(X'w)\\). Since \\(z\\) is a linear combination of columns of \\(X\\) (with coefficients \\(X'w\\)), \\(z \\in \\text{Col}(X)\\). Thus, \\(\\text{Col}(XX') \\subseteq \\text{Col}(X)\\).\nEquality via Rank: From the previous theorem, we know that \\(\\text{Rank}(XX') = \\text{Rank}(X)\\). Since \\(\\text{Col}(XX')\\) is a subspace of \\(\\text{Col}(X)\\) and they have the same finite dimension (Rank), the subspaces must be identical.\n\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(\\text{Col}(X)\\) lies in the same space as the projection onto \\(\\text{Col}(XX')\\). This is vital for the existence of solutions in generalized least squares.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#orthogonal-projection-onto-a-subspace",
    "href": "lec1-vecspace.html#orthogonal-projection-onto-a-subspace",
    "title": "1  Projection in Vector Space",
    "section": "1.4 Orthogonal Projection onto a Subspace",
    "text": "1.4 Orthogonal Projection onto a Subspace\n\nDefinition 1.22 (Definition of Projection onto a Subspace \\(V\\)) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\n\n1.4.1 Equivalence to Least Squares\nThe geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).\n\nTheorem 1.12 (Best Approximation Theorem (Least Squares Property)) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) and \\(y \\in \\mathbb{R}^n\\). Let \\(\\hat{y}\\) be the orthogonal projection of \\(y\\) onto \\(V\\). Then \\(\\hat{y}\\) is the closest point in \\(V\\) to \\(y\\). That is, for any vector \\(v \\in V\\) such that \\(v \\ne \\hat{y}\\):\n\\[\n\\|y - \\hat{y}\\|^2 &lt; \\|y - v\\|^2\n\\]\n\n\nProof. Let \\(v\\) be any vector in \\(V\\). We can rewrite the difference vector \\(y - v\\) by adding and subtracting the projection \\(\\hat{y}\\): \\[\ny - v = (y - \\hat{y}) + (\\hat{y} - v)\n\\]\nObserve the properties of the two terms on the right-hand side:\n\nResidual: \\((y - \\hat{y})\\) is orthogonal to \\(V\\) by definition.\nDifference in Subspace: Since both \\(\\hat{y} \\in V\\) and \\(v \\in V\\), their difference \\((\\hat{y} - v)\\) is also in \\(V\\).\n\nTherefore, the two terms are orthogonal to each other: \\[\n(y - \\hat{y}) \\perp (\\hat{y} - v)\n\\]\nApplying the Pythagorean Theorem: \\[\n\\|y - v\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - v\\|^2\n\\]\nSince squared norms are non-negative, and \\(\\|\\hat{y} - v\\|^2 &gt; 0\\) (because \\(v \\ne \\hat{y}\\)): \\[\n\\|y - v\\|^2 &gt; \\|y - \\hat{y}\\|^2\n\\] The projection \\(\\hat{y}\\) minimizes the squared error distance (and error distance itself).\n\n\n\n1.4.2 Uniqueness of Projection\nWhile the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.\n\nTheorem 1.13 (Uniqueness of Orthogonal Projection) For a given vector \\(y\\) and subspace \\(V\\), the projection vector \\(\\hat{y}\\) satisfying \\((y - \\hat{y}) \\perp V\\) is unique.\n\n\nProof. Assume there are two vectors \\(\\hat{y}_1 \\in V\\) and \\(\\hat{y}_2 \\in V\\) that both satisfy the orthogonality condition. \\[\n(y - \\hat{y}_1) \\perp V \\quad \\text{and} \\quad (y - \\hat{y}_2) \\perp V\n\\] This means that for any \\(v \\in V\\), both inner products are zero: \\[\n\\langle y - \\hat{y}_1, v \\rangle = 0\n\\] \\[\n\\langle y - \\hat{y}_2, v \\rangle = 0\n\\]\nSubtracting the second equation from the first: \\[\n\\langle y - \\hat{y}_1, v \\rangle - \\langle y - \\hat{y}_2, v \\rangle = 0\n\\] Using the linearity of the inner product: \\[\n\\langle (y - \\hat{y}_1) - (y - \\hat{y}_2), v \\rangle = 0\n\\] \\[\n\\langle \\hat{y}_2 - \\hat{y}_1, v \\rangle = 0\n\\]\nThis equation holds for all \\(v \\in V\\). Since \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) are both in \\(V\\), their difference \\(d = \\hat{y}_2 - \\hat{y}_1\\) must also be in \\(V\\). We can therefore choose \\(v = d = \\hat{y}_2 - \\hat{y}_1\\). \\[\n\\langle \\hat{y}_2 - \\hat{y}_1, \\hat{y}_2 - \\hat{y}_1 \\rangle = 0 \\implies \\|\\hat{y}_2 - \\hat{y}_1\\|^2 = 0\n\\] The only vector with a norm of zero is the zero vector itself. \\[\n\\hat{y}_2 - \\hat{y}_1 = 0 \\implies \\hat{y}_1 = \\hat{y}_2\n\\] Thus, the projection is unique.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#projection-via-orthonormal-basis-q",
    "href": "lec1-vecspace.html#projection-via-orthonormal-basis-q",
    "title": "1  Projection in Vector Space",
    "section": "1.5 Projection via Orthonormal Basis (\\(Q\\))",
    "text": "1.5 Projection via Orthonormal Basis (\\(Q\\))\n\n1.5.1 Orthonomal Basis\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 1.23 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 1.24 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 1.25 (Projection Defined with Orthonormal Basis) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\nTheorem 1.14 (Projection via Orthonormal Basis) Let \\(\\{q_1, \\dots, q_k\\}\\) be an orthonormal basis for the subspace \\(V \\subseteq \\mathbb{R}^n\\). The vector defined by the sum of individual projections: \\[\n\\hat{y} = \\sum_{i=1}^k \\langle y, q_i \\rangle q_i\n\\] is indeed the orthogonal projection of \\(y\\) onto \\(V\\). That is, it satisfies \\((y - \\hat{y}) \\perp V\\).\n\n\nProof. To prove this, we must check two conditions:\n\n\\(\\hat{y} \\in V\\): This is immediate because \\(\\hat{y}\\) is a linear combination of the basis vectors \\(\\{q_1, \\dots, q_k\\}\\).\n\\((y - \\hat{y}) \\perp V\\): It suffices to show that the error vector \\(e = y - \\hat{y}\\) is orthogonal to every basis vector \\(q_j\\) (for \\(j = 1, \\dots, k\\)).\nLet’s calculate the inner product \\(\\langle y - \\hat{y}, q_j \\rangle\\): \\[\n\\begin{aligned}\n\\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle \\hat{y}, q_j \\rangle \\\\\n&= \\langle y, q_j \\rangle - \\left\\langle \\sum_{i=1}^k \\langle y, q_i \\rangle q_i, q_j \\right\\rangle \\\\\n&= \\langle y, q_j \\rangle - \\sum_{i=1}^k \\langle y, q_i \\rangle \\underbrace{\\langle q_i, q_j \\rangle}_{\\delta_{ij}}\n\\end{aligned}\n\\]\nSince the basis is orthonormal, \\(\\langle q_i, q_j \\rangle\\) is 1 if \\(i=j\\) and 0 otherwise. Thus, the summation collapses to a single term where \\(i=j\\): \\[\n\\begin{aligned}\n\\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle y, q_j \\rangle \\cdot 1 \\\\\n&= 0\n\\end{aligned}\n\\]\nSince \\((y - \\hat{y})\\) is orthogonal to every basis vector \\(q_j\\), it is orthogonal to the entire subspace \\(V\\). Thus, \\(\\hat{y}\\) is the unique orthogonal projection.\n\n\n\n\n1.5.2 Projection Matrix via Orthonomal Basis (\\(Q\\))\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\):\n\n\\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)).\n\\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).\n\n\nDefinition 1.26 (Projection Matrix in Terms of \\(Q\\)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 1.15 (Symmeticity and Idempotence) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\n\nProof. If \\(\\hat{y} = Py\\) is already in the subspace \\(\\text{Col}(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\n\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 1.5 (Finding Projection for One-way ANOVA) Consider a one-way ANOVA model with \\(k\\) groups: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i \\in \\{1, \\dots, k\\}\\) represents the group and \\(j \\in \\{1, \\dots, n_i\\}\\) represents the observation within the group. Let \\(N = \\sum_{i=1}^k n_i\\) be the total number of observations.\n1. Matrix Definitions We define the data vector \\(y\\) and the design matrix \\(X\\) as follows:\n\nData Vector (\\(y\\)): An \\(N \\times 1\\) vector containing all observations stacked by group: \\[\n  y = \\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1n_1} \\\\ y_{21} \\\\ \\vdots \\\\ y_{kn_k} \\end{pmatrix}\n  \\]\nDesign Matrix (\\(X\\)): An \\(N \\times k\\) matrix constructed from \\(k\\) column vectors, \\(X = (x_1, x_2, \\dots, x_k)\\). Each vector \\(x_g\\) is an indicator variable (dummy variable) for group \\(g\\): \\[\n  x_g = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\quad \\leftarrow \\text{Entries are 1 if observation belongs to group } g\n  \\]\n\n2. Orthogonality These column vectors \\(x_1, \\dots, x_k\\) are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero: \\[\n\\langle x_g, x_h \\rangle = 0 \\quad \\text{for } g \\neq h\n\\] This allows us to find the projection onto the column space of \\(X\\) by simply summing the projections onto each column individually.\n3. Calculating Individual Projections For a specific group vector \\(x_g\\), the projection is: \\[\n\\text{proj}(y|x_g) = \\frac{\\langle y, x_g \\rangle}{\\langle x_g, x_g \\rangle} x_g\n\\]\nWe calculate the two scalar terms:\n\nDenominator (\\(\\langle x_g, x_g \\rangle\\)): The sum of squared elements of \\(x_g\\). Since \\(x_g\\) contains \\(n_g\\) ones and zeros elsewhere: \\[\n  \\langle x_g, x_g \\rangle = \\sum \\mathbb{1}_{\\{i=g\\}}^2 = n_g\n  \\]\nNumerator (\\(\\langle y, x_g \\rangle\\)): The dot product sums only the \\(y\\) values belonging to group \\(g\\): \\[\n  \\langle y, x_g \\rangle = \\sum_{i,j} y_{ij} \\cdot \\mathbb{1}_{\\{i=g\\}} = \\sum_{j=1}^{n_g} y_{gj} = y_{g.} \\quad (\\text{Group Total})\n  \\]\n\n4. The Resulting Projection Substituting these back into the formula gives the coefficient for the vector \\(x_g\\): \\[\n\\text{proj}(y|x_g) = \\frac{y_{g.}}{n_g} x_g = \\bar{y}_{g.} x_g\n\\]\nThe total projection \\(\\hat{y}\\) is the sum over all groups: \\[\n\\hat{y} = \\sum_{g=1}^k \\bar{y}_{g.} x_g\n\\] This confirms that the fitted value for any specific observation \\(y_{ij}\\) is simply its group mean \\(\\bar{y}_{i.}\\).\n\n\n\n1.5.3 Gram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Gram-Schmidt Process: Projecting \\(x_2\\) onto \\(x_1\\)\n\n\n\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#hat-matrix-projection-matrix-via-x",
    "href": "lec1-vecspace.html#hat-matrix-projection-matrix-via-x",
    "title": "1  Projection in Vector Space",
    "section": "1.6 Hat Matrix (Projection Matrix via \\(X\\))",
    "text": "1.6 Hat Matrix (Projection Matrix via \\(X\\))\n\n1.6.1 Norm Equations\nLet \\(X = (x_1, \\dots, x_p)\\) be an \\(n \\times p\\) matrix, where each column \\(x_j\\) is a predictor vector.\nWe want to project the target vector \\(y\\) onto the column space \\(\\text{Col}(X)\\). This is equivalent to finding a coefficient vector \\(\\beta \\in \\mathbb{R}^p\\) such that the error vector (residual) is orthogonal to the entire subspace \\(\\text{Col}(X)\\).\n\\[\ny - X\\beta \\perp \\text{Col}(X)\n\\]\nSince the columns of \\(X\\) span the subspace, the residual must be orthogonal to every column vector \\(x_j\\) individually:\n\\[\ny - X\\beta \\perp x_j \\quad \\text{for } j = 1, \\dots, p\n\\]\nWriting this geometric condition as an algebraic dot product (where \\(x_j'\\) denotes the transpose):\n\\[\nx_j'(y - X\\beta) = 0 \\quad \\text{for each } j\n\\]\nWe can stack these \\(p\\) separate linear equations into a single matrix equation. Since the rows of \\(X'\\) are the columns of \\(X\\), this becomes:\n\\[\n\\begin{pmatrix} x_1' \\\\ \\vdots \\\\ x_p' \\end{pmatrix} (y - X\\beta) = \\mathbf{0}\n\\implies X'(y - X\\beta) = 0\n\\]\nFinally, we distribute the matrix transpose and rearrange terms to solve for \\(\\beta\\):\n\\[\n\\begin{aligned}\nX'y - X'X\\beta &= 0 \\\\\nX'X\\beta &= X'y\n\\end{aligned}\n\\]\nThis system is known as the Normal Equations.\n\nTheorem 1.16 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n1.6.2 Hat Matrix\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 1.27 (Hat Matrix) The projection of \\(y\\) onto \\(\\text{Col}(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the hat matrix \\(H\\) is defined as:\n\\[\nH = X(X'X)^{-1}X'\n\\]\n\n\n\n1.6.3 Equivalence of Hat Matrix and \\(QQ'\\)\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(\\text{Col}(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\n\\begin{aligned}\nH &= QR((QR)'(QR))^{-1}(QR)' \\\\\n  &= QR(R'Q'QR)^{-1}R'Q' \\\\\n  &= QR(R' \\underbrace{Q'Q}_{I} R)^{-1}R'Q' \\\\\n  &= QR(R'R)^{-1}R'Q' \\\\\n  &= QR R^{-1} (R')^{-1} R' Q' \\\\\n  &= Q \\underbrace{R R^{-1}}_{I} \\underbrace{(R')^{-1} R'}_{I} Q' \\\\\n  &= Q Q'\n\\end{aligned}\n\\]\nThis confirms that \\(H = QQ'\\) is consistent with the general formula \\(H = X(X'X)^{-1}X'\\).\n\n\n1.6.4 Properties of Hat Matrix\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 1.17 (Properties of Hat Matrix) The matrix \\(H = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(H' = H\\)\nIdempotent: \\(H^2 = H\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\n\n\n1.6.5 Projection onto Complement\n\nTheorem 1.18 (Residual Maker Matrix M) \\[\nM = I - X(X^\\top X)^{-1}X^\\top\n\\]\n\\(M\\) is a projection matrix onto \\(\\text{Col}(X)^\\perp\\).\n\n\nProof. Let \\(H = X(X^\\top X)^{-1}X^\\top\\). This is the “Hat Matrix” which projects onto \\(\\text{Col}(X)\\). Then we can write \\(M = I - H\\).\nTo prove \\(M\\) is the orthogonal projector onto \\(\\text{Col}(X)^\\perp\\), we must satisfy two conditions:\n\n\\(M\\) is an orthogonal projection matrix (Symmetric and Idempotent).\nThe image (column space) of \\(M\\) is exactly \\(\\text{Col}(X)^\\perp\\).\n\n1. Symmetry and Idempotency\n\nSymmetry: Since \\(H\\) is symmetric, \\(M\\) is symmetric. \\[\n  M^\\top = (I - H)^\\top = I^\\top - H^\\top = I - H = M\n  \\]\nIdempotency: Since \\(H\\) is idempotent (\\(H^2=H\\)), \\(M\\) is idempotent. \\[\n  M^2 = (I - H)(I - H) = I - 2H + H^2 = I - 2H + H = I - H = M\n  \\]\n\n2. The Image of M\nWe claim \\(\\text{Col}(M) = \\text{Col}(X)^\\perp\\).\n\nForward (\\(\\subseteq\\)): Take any vector \\(v = My\\) in the column space of \\(M\\). We check if it is orthogonal to the columns of \\(X\\): \\[\n  X^\\top v = X^\\top (I - H)y = (X^\\top - X^\\top H)y\n  \\] Substituting \\(H = X(X^\\top X)^{-1}X^\\top\\): \\[\n  X^\\top H = X^\\top [X(X^\\top X)^{-1}X^\\top] = (X^\\top X)(X^\\top X)^{-1}X^\\top = I \\cdot X^\\top = X^\\top\n  \\] Therefore: \\[\n  X^\\top v = (X^\\top - X^\\top)y = 0\n  \\] Since \\(v\\) is orthogonal to the columns of \\(X\\), \\(v \\in \\text{Col}(X)^\\perp\\).\nReverse (\\(\\supseteq\\)): Take any vector \\(z \\in \\text{Col}(X)^\\perp\\). By definition, \\(X^\\top z = 0\\). \\[\n  Mz = (I - H)z = z - X(X^\\top X)^{-1}\\underbrace{X^\\top z}_{0} = z - 0 = z\n  \\] Since \\(Mz = z\\), the vector \\(z\\) is in the column space of \\(M\\) (it is an eigenvector with eigenvalue 1).\n\nThus, \\(M\\) projects orthogonally onto \\(\\text{Col}(X)^\\perp\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec1-vecspace.html#general-projection-matrices-onto-nested-subspaces",
    "href": "lec1-vecspace.html#general-projection-matrices-onto-nested-subspaces",
    "title": "1  Projection in Vector Space",
    "section": "1.7 General Projection Matrices onto Nested Subspaces",
    "text": "1.7 General Projection Matrices onto Nested Subspaces\n\n1.7.1 Nested Models and Subspaces\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\nDefinition 1.28 (Nested Models) Consider two models:\n\nReduced Model (\\(M_0\\)): \\(y \\in \\text{Col}(X_0)\\)\nFull Model (\\(M_1\\)): \\(y \\in \\text{Col}(X_1)\\)\n\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\n\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\n\\]\n\nUsually, \\(X_1\\) is constructed by adding columns to \\(X_0\\): \\(X_1 = [X_0, X_{new}]\\).\n\n\n1.7.2 Projections onto Nested Subspaces\nLet \\(P_0\\) be the projection matrix onto \\(\\text{Col}(X_0)\\) and \\(P_1\\) be the projection matrix onto \\(\\text{Col}(X_1)\\). Since \\(\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\\), we have important relationships between these matrices.\n\nTheorem 1.19 (Composition of Projections) If \\(\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\n\nProof. 1. Proof of \\(P_1 P_0 = P_0\\): For any vector \\(y \\in \\mathbb{R}^n\\), the vector \\(v = P_0 y\\) lies in \\(\\text{Col}(X_0)\\). Since \\(\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\\), the vector \\(v\\) also lies in \\(\\text{Col}(X_1)\\). A projection matrix \\(P_1\\) acts as the identity operator for any vector already in its column space. Therefore, \\(P_1 v = v\\). Substituting \\(v = P_0 y\\), we get \\(P_1 P_0 y = P_0 y\\) for all \\(y\\). Thus, \\(P_1 P_0 = P_0\\).\n2. Proof of \\(P_0 P_1 = P_0\\): Take the transpose of the previous result (\\(P_1 P_0 = P_0\\)). \\[\n(P_1 P_0)' = P_0'\n\\] Using the property that projection matrices are symmetric (\\(P' = P\\)): \\[\nP_0' P_1' = P_0' \\implies P_0 P_1 = P_0\n\\]\n\nDifference of Projections\nThe difference between the two projection matrices, \\(P_1 - P_0\\), is itself a projection matrix.\n\nTheorem 1.20 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(\\text{Col}(X_1) \\cap \\text{Col}(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\n\nProof. 1. Symmetry: Since \\(P_1\\) and \\(P_0\\) are symmetric: \\((P_1 - P_0)' = P_1' - P_0' = P_1 - P_0\\).\n2. Idempotency: \\[\n\\begin{aligned}\n(P_1 - P_0)^2 &= (P_1 - P_0)(P_1 - P_0) \\\\\n&= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2\n\\end{aligned}\n\\] Using the projection properties (\\(P^2=P\\)) and the nested property (\\(P_1 P_0 = P_0\\) and \\(P_0 P_1 = P_0\\)): \\[\n= P_1 - P_0 - P_0 + P_0 = P_1 - P_0\n\\]\n3. Orthogonality to \\(P_0\\): \\[\n(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0\n\\] Since \\((P_1 - P_0)\\) is symmetric and idempotent, it is an orthogonal projection matrix. Since it is orthogonal to \\(P_0\\) (the space of \\(M_0\\)) but is derived from \\(P_1\\), it projects onto the subspace of \\(M_1\\) that is orthogonal to \\(M_0\\).\n\n\n\n1.7.3 Decomposition of Projections and their Sum Squares\n\nTheorem 1.21 (Orthogonal Decomposition) Let \\(M_0 \\subset M_1\\) be two nested linear models with corresponding design matrices \\(X_0\\) and \\(X_1\\) such that \\(\\text{Col}(X_0) \\subset \\text{Col}(X_1)\\). Let \\(P_0\\) and \\(P_1\\) be the orthogonal projection matrices onto \\(\\text{Col}(X_0)\\) and \\(\\text{Col}(X_1)\\) respectively.\nFor any observation vector \\(y\\), we have the decomposition: \\[\ny = \\underbrace{P_0 y}_{\\hat{y}_0} + \\underbrace{(P_1 - P_0) y}_{\\hat{y}_1 - \\hat{y}_0} + \\underbrace{(I - P_1) y}_{y - \\hat{y}_1}\n\\]\nGeometric Interpretation:\n\n\\(\\hat{y}_0 \\in \\text{Col}(X_0)\\): The fit of the reduced model.\n\\((\\hat{y}_1 - \\hat{y}_0) \\in \\text{Col}(X_0)^\\perp \\cap \\text{Col}(X_1)\\): The additional fit provided by \\(M_1\\) over \\(M_0\\).\n\\((y - \\hat{y}_1) \\in \\text{Col}(X_1)^\\perp\\): The projection of \\(y\\) onto the orthogonal complement of \\(\\text{Col}(X_1)\\).\n\nThe three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm: \\[\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n\\]\n\n\nProof. 1. Definitions We define the three components as vectors \\(v_1, v_2, v_3\\):\n\n\\(v_1 = \\hat{y}_0 = P_0 y\\).\n\\(v_2 = \\hat{y}_1 - \\hat{y}_0 = (P_1 - P_0)y\\).\n\\(v_3 = y - \\hat{y}_1 = (I - P_1)y\\).\n\nNote: Since \\(P_1\\) projects onto \\(\\text{Col}(X_1)\\), the matrix \\((I - P_1)\\) projects onto the orthogonal complement \\(\\text{Col}(X_1)^\\perp\\). Thus, \\(v_3 \\in \\text{Col}(I - P_1)\\).\n\n\nNote that since \\(\\text{Col}(X_0) \\subset \\text{Col}(X_1)\\), we have the property \\(P_1 P_0 = P_0 P_1 = P_0\\). (Projecting onto the smaller subspace \\(M_0\\) is unchanged if we first project onto the enclosing subspace \\(M_1\\)).\n2. Orthogonality of \\(v_1\\) and \\(v_2\\) We check the inner product \\(\\langle v_1, v_2 \\rangle = v_1' v_2\\): \\[\n\\begin{aligned}\nv_1' v_2 &= (P_0 y)' (P_1 - P_0) y \\\\\n&= y' P_0' (P_1 - P_0) y \\\\\n&= y' (P_0 P_1 - P_0^2) y \\quad (\\text{Since } P_0 \\text{ is symmetric}) \\\\\n&= y' (P_0 - P_0) y \\quad (\\text{Since } P_0 P_1 = P_0 \\text{ and } P_0^2 = P_0) \\\\\n&= 0\n\\end{aligned}\n\\]\n3. Orthogonality of \\((v_1 + v_2)\\) and \\(v_3\\) Note that \\(v_1 + v_2 = P_1 y = \\hat{y}_1\\). We check if the total fit \\(\\hat{y}_1\\) is orthogonal to the residual \\(v_3\\): \\[\n\\begin{aligned}\n\\hat{y}_1' v_3 &= (P_1 y)' (I - P_1) y \\\\\n&= y' P_1 (I - P_1) y \\\\\n&= y' (P_1 - P_1^2) y \\\\\n&= y' (P_1 - P_1) y \\\\\n&= 0\n\\end{aligned}\n\\] Since \\(\\hat{y}_1\\) is orthogonal to \\(v_3\\), and \\(\\hat{y}_0\\) is a component of \\(\\hat{y}_1\\), it follows that all three pieces are mutually orthogonal.\n4. Sum of Squares By the Pythagorean theorem applied twice to these orthogonal vectors, the equality of squared norms follows immediately.\n\n\n\n\n\n\n\n\n\nFigure 1.2: Illustration of Projections onto Nested Subspaces\n\n\n\n\n\n\nExample 1.6 (ANOVA Sum Squares) We apply the Nested Model Theorem (\\(M_0 \\subset M_1\\)) to the One-way ANOVA setting.\n1. Notation and Definitions\nConsider a dataset with \\(k\\) groups. Let \\(i = 1, \\dots, k\\) index the groups, and \\(j = 1, \\dots, n_i\\) index the observations within group \\(i\\).\n\n\\(N\\): Total number of observations, \\(N = \\sum_{i=1}^k n_i\\).\n\\(y_{ij}\\): The \\(j\\)-th observation in the \\(i\\)-th group.\n\\(\\bar{y}_{i.}\\): The sample mean of group \\(i\\). \\[\n  \\bar{y}_{i.} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij}\n  \\]\n\\(\\bar{y}_{..}\\): The grand mean of all observations. \\[\n  \\bar{y}_{..} = \\frac{1}{N} \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}\n  \\]\n\n2. The Data and Projection Vectors\nWe stack all observations into a single \\(N \\times 1\\) vector. The projections onto the Null Model (\\(M_0\\), intercept only) and Full Model (\\(M_1\\), group means) correspond to replacing observations with the Grand Mean and Group Means, respectively.\n\n\n\nTable 1.1: ANOVA Vectors: Data, Null Model, and Full Model\n\n\n\n\n\n\n\n\n\n\nObservation (\\(y\\))\nNull Projection (\\(\\hat{y}_0\\))\nFull Projection (\\(\\hat{y}_1\\))\n\n\n\n\n\\(\\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1 n_1} \\\\ \\hline \\vdots \\\\ \\hline y_{k1} \\\\ \\vdots \\\\ y_{k n_k} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\end{pmatrix}\\)\n\\(\\begin{pmatrix} \\bar{y}_{1.} \\\\ \\vdots \\\\ \\bar{y}_{1.} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{k.} \\\\ \\vdots \\\\ \\bar{y}_{k.} \\end{pmatrix}\\)\n\n\n\n\n\n\n3. Decomposition and Sum of Squares\nThe total deviation vector \\(y - \\hat{y}_0\\) splits into two orthogonal components: the Model Improvement (\\(\\hat{y}_1 - \\hat{y}_0\\)) and the Residual (\\(y - \\hat{y}_1\\)).\n\n\n\nTable 1.2: ANOVA Projections and Sum of Squares\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nNotation\nDefinition\nVector Elements\nSquared Norm (Sum of Squares)\n\n\n\n\nNull Proj.\n\\(\\hat{y}_0\\)\n\\(P_0 y\\)\nGrand Mean (\\(\\bar{y}_{..}\\))\n\\(\\|\\hat{y}_0\\|^2 = N \\bar{y}_{..}^2 = \\frac{(\\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij})^2}{N}\\)\n\n\nFull Proj.\n\\(\\hat{y}_1\\)\n\\(P_1 y\\)\nGroup Means (\\(\\bar{y}_{i.}\\))\n\\(\\|\\hat{y}_1\\|^2 = \\sum_{i=1}^k n_i \\bar{y}_{i.}^2 = \\sum_{i=1}^k \\frac{y_{i.}^2}{n_i}\\)\n\n\nBetween\n\\(\\hat{y}_1 - \\hat{y}_0\\)\n\\((P_1 - P_0)y\\)\nGroup Effect (\\(\\bar{y}_{i.} - \\bar{y}_{..}\\))\n\\(\\sum_{i=1}^k n_i (\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\n\nWithin\n\\(y - \\hat{y}_1\\)\n\\((I - P_1)y\\)\nResidual (\\(y_{ij} - \\bar{y}_{i.}\\))\n\\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{i.})^2\\)\n\n\nTotal\n\\(y - \\hat{y}_0\\)\n\\((I - P_0)y\\)\nTotal Deviation (\\(y_{ij} - \\bar{y}_{..}\\))\n\\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n4. Geometric Justification of Shortcut Formulas\nIn manual calculations, we use “shortcut formulas” involving raw sums of squares. These are direct consequences of the Pythagorean Theorem applied to orthogonal projections.\nA. Total Sum of Squares (SST) Since \\(\\hat{y}_0 \\perp (y - \\hat{y}_0)\\): \\[\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|y - \\hat{y}_0\\|^2\n\\] \\[\n\\text{TSS} = \\|y - \\hat{y}_0\\|^2 = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - \\frac{(\\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij})^2}{N}\n\\]\nB. Between Group Sum of Squares (SSB) Since \\(\\hat{y}_0\\) is the projection of \\(\\hat{y}_1\\) onto \\(M_0\\), we have \\(\\hat{y}_0 \\perp (\\hat{y}_1 - \\hat{y}_0)\\): \\[\n\\|\\hat{y}_1\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2\n\\] \\[\n\\text{SSB} = \\|\\hat{y}_1 - \\hat{y}_0\\|^2 = \\sum_{i=1}^k \\frac{y_{i.}^2}{n_i} - \\frac{(\\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij})^2}{N}\n\\]\nC. Within Group Sum of Squares (SSW) Since \\(\\hat{y}_1\\) is the projection of \\(y\\) onto \\(M_1\\), we have \\(\\hat{y}_1 \\perp (y - \\hat{y}_1)\\): \\[\n\\|y\\|^2 = \\|\\hat{y}_1\\|^2 + \\|y - \\hat{y}_1\\|^2\n\\] \\[\n\\text{SSW} = \\|y - \\hat{y}_1\\|^2 = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - \\sum_{i=1}^k \\frac{y_{i.}^2}{n_i}\n\\]\nConclusion: The decomposition holds: \\[\n\\underbrace{\\|y\\|^2 - \\|\\hat{y}_0\\|^2}_{\\text{SST}} = \\underbrace{(\\|\\hat{y}_1\\|^2 - \\|\\hat{y}_0\\|^2)}_{\\text{SSB}} + \\underbrace{(\\|y\\|^2 - \\|\\hat{y}_1\\|^2)}_{\\text{SSW}}\n\\]\n\n\n\n1.7.4 Projections in General Orthogonal Subspaces\nFinally, we consider the case where the entire space \\(\\mathbb{R}^n\\) is decomposed into mutually orthogonal subspaces.\n\nTheorem 1.22 (General Orthogonal Projections) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = \\hat{y}_1 + \\hat{y}_2 + \\dots + \\hat{y}_k\n\\] where \\(\\hat{y}_i \\in V_i\\).\nFurthermore, each component \\(\\hat{y}_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\n\\hat{y}_i = P_i y\n\\]\n\n\nProof. 1. Existence: Since \\(\\mathbb{R}^n\\) is the direct sum of \\(V_1, \\dots, V_k\\), by definition, any vector \\(y \\in \\mathbb{R}^n\\) can be written as a sum \\(y = v_1 + \\dots + v_k\\) where \\(v_i \\in V_i\\).\n2. Uniqueness: Suppose there are two such representations: \\(y = \\sum v_i = \\sum w_i\\), with \\(v_i, w_i \\in V_i\\). Then \\(\\sum (v_i - w_i) = 0\\). Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, \\(v_i - w_i = 0 \\implies v_i = w_i\\). The representation is unique. Let \\(\\hat{y}_i = v_i\\).\n3. Projection Property: We claim that the \\(i\\)-th component \\(\\hat{y}_i\\) is the orthogonal projection of \\(y\\) onto \\(V_i\\). We must show that the residual \\((y - \\hat{y}_i)\\) is orthogonal to \\(V_i\\). \\[\ny - \\hat{y}_i = \\sum_{j \\ne i} \\hat{y}_j\n\\] Let \\(z\\) be any vector in \\(V_i\\). We calculate the inner product: \\[\n\\langle y - \\hat{y}_i, z \\rangle = \\left\\langle \\sum_{j \\ne i} \\hat{y}_j, z \\right\\rangle = \\sum_{j \\ne i} \\langle \\hat{y}_j, z \\rangle\n\\] Since \\(\\hat{y}_j \\in V_j\\) and \\(z \\in V_i\\), and the subspaces are mutually orthogonal (\\(V_j \\perp V_i\\) for \\(j \\ne i\\)), every term in the sum is zero. Therefore, \\((y - \\hat{y}_i) \\perp V_i\\). By the definition of orthogonal projection, \\(\\hat{y}_i = P_i y\\).\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: \\[\nI_n = P_1 + P_2 + \\dots + P_k\n\\]\n\n\n\n\n\n\n\n\nFigure 1.3: Orthogonal decomposition of vector y into subspaces\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\n# --- Define Vectors ---\ny_vec &lt;- c(3, 4, 5)\norigin &lt;- c(0, 0, 0)\n\n# Projections (P_i y)\np1 &lt;- c(3, 0, 0)\np2 &lt;- c(0, 4, 0)\np3 &lt;- c(0, 0, 5)\n\n# Partial Sums (P_i y + P_j y)\nsum_12 &lt;- p1 + p2\nsum_13 &lt;- p1 + p3\nsum_23 &lt;- p2 + p3\n\n# --- Helper Functions ---\n\n# Function to add a vector with an arrowhead (Cone)\nadd_vec_arrow &lt;- function(p, start, end, color, name) {\n  p %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 6),\n      name = name,\n      showlegend = TRUE\n    ) %&gt;%\n    add_trace(\n      type = \"cone\",\n      x = end[1], y = end[2], z = end[3],\n      u = end[1]-start[1], v = end[2]-start[2], w = end[3]-start[3],\n      sizemode = \"absolute\",\n      sizeref = 0.5,\n      anchor = \"tip\",\n      colorscale = list(c(0, 1), c(color, color)),\n      showscale = FALSE,\n      name = name,\n      showlegend = FALSE\n    )\n}\n\n# Function to add dashed \"error\" lines\nadd_dashed_line &lt;- function(p, start, end, color, name) {\n  p %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 3, dash = \"dash\"),\n      name = name,\n      hoverinfo = \"text\",\n      text = name\n    )\n}\n\n# --- Build Plot ---\nfig &lt;- plot_ly()\n\n# 1. Main Vectors (Solid + Cones)\nfig &lt;- fig %&gt;%\n  add_vec_arrow(origin, p1, \"red\", \"P1 y\") %&gt;%\n  add_vec_arrow(origin, p2, \"green\", \"P2 y\") %&gt;%\n  add_vec_arrow(origin, p3, \"blue\", \"P3 y\") %&gt;%\n  add_vec_arrow(origin, y_vec, \"black\", \"y\")\n\n# 2. Dashed Lines from y to Single Projections\nfig &lt;- fig %&gt;%\n  add_dashed_line(y_vec, p1, \"rgba(255, 0, 0, 0.5)\", \"y -&gt; P1\") %&gt;%\n  add_dashed_line(y_vec, p2, \"rgba(0, 255, 0, 0.5)\", \"y -&gt; P2\") %&gt;%\n  add_dashed_line(y_vec, p3, \"rgba(0, 0, 255, 0.5)\", \"y -&gt; P3\")\n\n# 3. Dashed Lines from y to Partial Sums\nfig &lt;- fig %&gt;%\n  add_dashed_line(y_vec, sum_12, \"purple\", \"y -&gt; (P1+P2)\") %&gt;%\n  add_dashed_line(y_vec, sum_13, \"orange\", \"y -&gt; (P1+P3)\") %&gt;%\n  add_dashed_line(y_vec, sum_23, \"cyan\",   \"y -&gt; (P2+P3)\")\n\n# 4. Axes (Subspaces)\nlimit &lt;- 6\naxis_style &lt;- list(color = \"gray\", dash = \"dot\", width = 2)\n\nfig &lt;- fig %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0, limit), y=c(0,0), z=c(0,0), \n            line=axis_style, name=\"V1 (x)\") %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0, limit), z=c(0,0), \n            line=axis_style, name=\"V2 (y)\") %&gt;%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0,0), z=c(0, limit), \n            line=axis_style, name=\"V3 (z)\")\n\n# --- Layout ---\nfig &lt;- fig %&gt;% layout(\n  title = \"Orthogonal Decomposition Geometry\",\n  width = 900,\n  height = 700,\n  scene = list(\n    xaxis = list(title = \"V1\", range = c(0, limit)),\n    yaxis = list(title = \"V2\", range = c(0, limit)),\n    zaxis = list(title = \"V3\", range = c(0, limit)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(x = 1.5, y = 1.5, z = 1.2))\n  ),\n  margin = list(l = 0, r = 0, b = 0, t = 50),\n  legend = list(x = 0.75, y = 0.9)\n)\n\nfig\n\n\n\n\n\n\n\n\nFigure 1.4: Orthogonal decomposition geometry (R Plotly)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Projection in Vector Space</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html",
    "href": "lec2-matrix.html",
    "title": "2  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "2.1 Spectral Theory\nThis chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, and generalized inverses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#spectral-theory",
    "href": "lec2-matrix.html#spectral-theory",
    "title": "2  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "2.1.1 Eigenvalues and Eigenvectors\n\nDefinition 2.1 (Eigenvalues and Eigenvectors) For a square matrix \\(A\\) (\\(n \\times n\\)), a scalar \\(\\lambda\\) is an eigenvalue and a non-zero vector \\(x\\) is the corresponding eigenvector if:\n\\[\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n\\]\nThe eigenvalues are found by solving the characteristic equation: \\[\n|A - \\lambda I_n| = 0\n\\]\n\n\n\n2.1.2 Quadratic Form\n\nDefinition 2.2 A quadratic form in \\(n\\) variables \\(x_1, x_2, \\dots, x_n\\) is a scalar function defined by a symmetric matrix \\(A\\): \\[\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\]\n\n\n\n2.1.3 Positive and Non-Negative Definite Matrices\n\nDefinition 2.3 (Positive and Non-Negative Definite Matrices) A symmetric matrix \\(A\\) is positive definite (p.d.) if: \\[\nx'Ax &gt; 0 \\quad \\forall x \\ne 0\n\\] It is non-negative definite (n.n.d.) if: \\[\nx'Ax \\ge 0 \\quad \\forall x\n\\]\n\n\nTheorem 2.1 (Properties of Definite Matrices) Let \\(A\\) be a symmetric \\(n \\times n\\) matrix with eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\).\n\nEigenvalue Characterization:\n\n\\(A\\) is p.d. \\(\\iff\\) all \\(\\lambda_i &gt; 0\\).\n\\(A\\) is n.n.d. \\(\\iff\\) all \\(\\lambda_i \\ge 0\\).\n\nDeterminant and Inverse:\n\nIf \\(A\\) is p.d., then \\(|A| &gt; 0\\) and \\(A^{-1}\\) exists.\nIf \\(A\\) is n.n.d. and singular, then \\(|A| = 0\\) (at least one \\(\\lambda_i = 0\\)).\n\nGram Matrices (\\(B'B\\)): Let \\(B\\) be an \\(n \\times p\\) matrix.\n\nIf \\(\\text{rank}(B) = p\\), then \\(B'B\\) is p.d.\nIf \\(\\text{rank}(B) &lt; p\\), then \\(B'B\\) is n.n.d.\n\n\n\n\n\n2.1.4 Properties of Symmetric Matrices\n\nTheorem 2.2 (Properties of Symmetric Matrices) Let \\(A\\) be a symmetric matrix with spectral decomposition \\(A = Q \\Lambda Q'\\). The following properties hold:\n\nTrace: \\(\\text{tr}(A) = \\sum \\lambda_i\\).\nDeterminant: \\(|A| = \\prod \\lambda_i\\).\nSingularity: \\(A\\) is singular if and only if at least one \\(\\lambda_i = 0\\).\nInverse: If \\(A\\) is non-singular (\\(\\lambda_i \\ne 0\\)), then \\(A^{-1} = Q \\Lambda^{-1} Q'\\).\nPowers: \\(A^k = Q \\Lambda^k Q'\\).\n\nSquare Root: \\(A^{1/2} = Q \\Lambda^{1/2} Q'\\) (if \\(\\lambda_i \\ge 0\\)).\n\nSpectral Representation of Quadratic Forms: The quadratic form \\(x'Ax\\) can be diagonalized using the eigenvectors of \\(A\\): \\[\nx'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n\\] where \\(y = Q'x\\) represents a rotation of the coordinate system.\n\n\n\n\n2.1.5 Spectral Representation of Projection Matrices\nWe revisit projection matrices in the context of eigenvalues.\n\nTheorem 2.3 (Eigenvalues of Projection Matrices) A symmetric matrix \\(P\\) is a projection matrix (idempotent, \\(P^2=P\\)) if and only if its eigenvalues are either 0 or 1.\n\\[\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n\\]\n\nFor a projection matrix \\(P\\):\n\nIf \\(x \\in Col(P)\\), \\(Px = x\\) (Eigenvalue 1).\nIf \\(x \\perp Col(P)\\), \\(Px = 0\\) (Eigenvalue 0).\n\\(\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i\\) (Count of 1s).\n\n\nExample 2.1 For \\(P = \\frac{1}{n} J_n J_n'\\), the rank is \\(\\text{tr}(P) = 1\\).\n\n\n\n\n2.1.6 Singular Value Decomposition (SVD)\n\nTheorem 2.4 (Singular Value Decomposition (SVD)) Let \\(X\\) be an \\(n \\times p\\) matrix with rank \\(r \\le \\min(n, p)\\). \\(X\\) can be decomposed into the product of three matrices:\n\\[\nX = U \\mathbf{D} V'\n\\]\n1. Partitioned Matrix Form\n\\[\nX = \\underset{n \\times n}{(U_1, U_2)}\n\\begin{pmatrix}\n\\Lambda_r & O_{r \\times (p-r)} \\\\\nO_{(n-r) \\times r} & O_{(n-r) \\times (p-r)}\n\\end{pmatrix}\n\\underset{p \\times p}{\n\\begin{pmatrix}\nV_1' \\\\\nV_2'\n\\end{pmatrix}\n}\n\\]\n2. Detailed Matrix Form\nExpanding the diagonal matrix explicitly:\n\\[\nX = \\underset{n \\times n}{(u_1, \\dots, u_n)}\n\\left(\n\\begin{array}{cccc|c}\n\\lambda_1 & 0 & \\dots & 0 &  \\\\\n0 & \\lambda_2 & \\dots & 0 & O_{12} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots &  \\\\\n0 & 0 & \\dots & \\lambda_r &  \\\\\n\\hline\n& O_{21} & & & O_{22}\n\\end{array}\n\\right)\n\\underset{p \\times p}{\n\\begin{pmatrix}\nv_1' \\\\\n\\vdots \\\\\nv_p'\n\\end{pmatrix}\n}\n\\]\n3. Reduced Form\n\\[\nX = U_1 \\Lambda_r V_1' = \\sum_{i=1}^r \\lambda_i u_i v_i'\n\\]\nProperties:\n\nSingular Values (\\(\\Lambda_r\\)): \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values (\\(\\lambda_i &gt; 0\\)), which are the square roots of the non-zero eigenvalues of \\(X'X\\).\nOrthogonality:\n\n\\(U\\) is \\(n \\times n\\) orthogonal (\\(U'U = I_n\\)).\n\\(V\\) is \\(p \\times p\\) orthogonal (\\(V'V = I_p\\)).\n\n\n\n\n2.1.6.1 Connection to Gram Matrices\nThe matrices \\(U\\) and \\(V\\) provide the basis vectors (eigenvectors) for the Gram matrices of \\(X\\).\n\nRight Singular Vectors (\\(V\\)): The columns of \\(V\\) are the eigenvectors of the Gram matrix \\(X'X\\). \\[\nX'X = (U \\Lambda V')' (U \\Lambda V') = V \\Lambda U' U \\Lambda V' = V \\Lambda^2 V'\n\\]\n\nThe eigenvalues of \\(X'X\\) are the squared singular values \\(\\lambda_i^2\\).\n\nLeft Singular Vectors (\\(U\\)): The columns of \\(U\\) are the eigenvectors of the Gram matrix \\(XX'\\). \\[\nXX' = (U \\Lambda V') (U \\Lambda V')' = U \\Lambda V' V \\Lambda U' = U \\Lambda^2 U'\n\\]\n\nThe eigenvalues of \\(XX'\\) are also \\(\\lambda_i^2\\) (for non-zero values).\n\n\n\n\n2.1.6.2 Numerical Example\nConsider the matrix \\(X = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\\).\n\nCompute \\(X'X\\) and find \\(V\\): \\[\nX'X = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 5 \\\\ 5 & 5 \\end{pmatrix}\n\\]\n\nEigenvalues of \\(X'X\\): Trace is 10, Determinant is 0. Thus, \\(\\mu_1 = 10, \\mu_2 = 0\\).\nSingular Values: \\(\\lambda_1 = \\sqrt{10}, \\lambda_2 = 0\\).\nEigenvector for \\(\\mu_1=10\\): Normalized \\(v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\).\n\nCompute \\(XX'\\) and find \\(U\\): \\[\nXX' = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n\\]\n\nEigenvalues are again 10 and 0.\nEigenvector for \\(\\mu_1=10\\): Normalized \\(u_1 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(U = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & 2 \\\\ 2 & -1 \\end{pmatrix}\\).\n\nVerification: \\[\nX = \\sqrt{10} u_1 v_1' = \\sqrt{10} \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#cholesky-decomposition",
    "href": "lec2-matrix.html#cholesky-decomposition",
    "title": "2  Spectral Theory and Generalized Inverse",
    "section": "2.2 Cholesky Decomposition",
    "text": "2.2 Cholesky Decomposition\nA symmetric matrix \\(A\\) has a Cholesky decomposition if and only if it is non-negative definite (i.e., \\(x'Ax \\ge 0\\) for all \\(x\\)).\n\\[\nA = B'B\n\\]\nwhere \\(B\\) is an upper triangular matrix with non-negative diagonal entries.\n\n2.2.1 Matrix Representation of the Algorithm\nTo derive the algorithm, we equate the elements of \\(A\\) with the product of the lower triangular matrix \\(B'\\) and the upper triangular matrix \\(B\\).\nFor a \\(3 \\times 3\\) matrix, this looks like:\n\\[\n\\underbrace{\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}}_{A}\n=\n\\underbrace{\\begin{pmatrix}\nb_{11} & 0 & 0 \\\\\nb_{12} & b_{22} & 0 \\\\\nb_{13} & b_{23} & b_{33}\n\\end{pmatrix}}_{B'}\n\\underbrace{\\begin{pmatrix}\nb_{11} & b_{12} & b_{13} \\\\\n0 & b_{22} & b_{23} \\\\\n0 & 0 & b_{33}\n\\end{pmatrix}}_{B}\n\\]\nMultiplying the matrices on the right yields the system of equations:\n\\[\nA = \\begin{pmatrix}\n\\mathbf{b_{11}^2} & b_{11}b_{12} & b_{11}b_{13} \\\\\nb_{12}b_{11} & \\mathbf{b_{12}^2 + b_{22}^2} & b_{12}b_{13} + b_{22}b_{23} \\\\\nb_{13}b_{11} & b_{13}b_{12} + b_{23}b_{22} & \\mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}\n\\end{pmatrix}\n\\]\nBy solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.\n\n\n2.2.2 The Algorithm\n\nRow 1: Solve for \\(b_{11}\\) using \\(a_{11}\\), then solve the rest of the row (\\(b_{1j}\\)) by division.\n\n\\(b_{11} = \\sqrt{a_{11}}\\)\n\\(b_{1j} = a_{1j}/b_{11}\\)\n\nRow 2: Solve for \\(b_{22}\\) using \\(a_{22}\\) and the known \\(b_{12}\\), then solve \\(b_{2j}\\).\n\n\\(b_{22} = \\sqrt{a_{22} - b_{12}^2}\\)\n\\(b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}\\)\n\nRow 3: Solve for \\(b_{33}\\) using \\(a_{33}\\) and the known \\(b_{13}, b_{23}\\).\n\n\\(b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}\\)\n\n\n\n\n2.2.3 Numerical Example\nConsider the positive definite matrix \\(A\\): \\[\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n\\]\nWe find \\(B\\) such that \\(A = B'B\\):\n\nFirst Row of B (\\(b_{11}, b_{12}, b_{13}\\)):\n\n\\(b_{11} = \\sqrt{4} = 2\\)\n\\(b_{12} = 2 / 2 = 1\\)\n\\(b_{13} = -2 / 2 = -1\\)\n\nSecond Row of B (\\(b_{22}, b_{23}\\)):\n\n\\(b_{22} = \\sqrt{10 - (1)^2} = \\sqrt{9} = 3\\)\n\\(b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1\\)\n\nThird Row of B (\\(b_{33}\\)):\n\n\\(b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{4} = 2\\)\n\n\nResult: \\[\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#generalized-inverses",
    "href": "lec2-matrix.html#generalized-inverses",
    "title": "2  Spectral Theory and Generalized Inverse",
    "section": "2.3 Generalized Inverses",
    "text": "2.3 Generalized Inverses\n\n2.3.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]\n\n\n2.3.2 Definition of Generalized Inverse\n\nDefinition 2.4 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 2.2 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]\n\n\n\n\n2.3.3 A General Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend:\n\n\\(\\otimes\\): Elements of submatrix \\(C\\)\n\\(\\triangle\\): Elements of \\((C^{-1})'\\)\n\\(\\square\\): Elements of \\(C^{-1}\\) (after transposition)\n\\(\\times\\): Other elements (replaced by 0 in the final calculation)\n\n\n\n2.3.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]\n\n\n\n2.3.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 2.5 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\mathcal{C}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 2.3 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#least-squares-with-generalized-inverse",
    "href": "lec2-matrix.html#least-squares-with-generalized-inverse",
    "title": "2  Spectral Theory and Generalized Inverse",
    "section": "2.4 Least Squares with Generalized Inverse",
    "text": "2.4 Least Squares with Generalized Inverse\n\n2.4.1 Projection Matrix with Generalized Inverse of \\(X'X\\)\nFor the normal equations \\((X'X)\\beta = X'y\\), a solution is given by: \\[\n\\hat{\\beta} = (X'X)^- X'y\n\\] The fitted values are \\[\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.\\] This \\(\\hat{y}\\) represents the unique orthogonal projection of \\(y\\) onto \\(Col(X)\\).\n\n\n2.4.2 Invariance and Uniqueness of “the” Projection Matrix\n\nTheorem 2.5 (Transpose Property of Generalized Inverses) \\((X^-)'\\) is a version of \\((X')^-\\). That is, \\((X^-)'\\) is a generalized inverse of \\(X'\\).\n\n\nProof. By definition, a generalized inverse \\(X^-\\) satisfies the property: \\[\nX X^- X = X\n\\]\nTo verify that \\((X^-)'\\) is a generalized inverse of \\(X'\\), we need to show that it satisfies the condition \\(A G A = A\\) where \\(A = X'\\) and \\(G = (X^-)'\\).\n\nStart with the fundamental definition: \\[\nX X^- X = X\n\\]\nTake the transpose of both sides of the equation: \\[\n(X X^- X)' = X'\n\\]\nApply the reverse order law for transposes, \\((ABC)' = C' B' A'\\): \\[\nX' (X^-)' X' = X'\n\\]\n\nSince substituting \\((X^-)'\\) into the generalized inverse equation for \\(X'\\) yields \\(X'\\), \\((X^-)'\\) is a valid generalized inverse of \\(X'\\).\n\n\nLemma 2.1 (Invariance of Generalized Least Squares) For any version of the generalized inverse \\((X'X)^-\\), the matrix \\(X'(X'X)^- X'\\) is invariant and equals \\(X'\\). \\[\nX'X(X'X)^- X' = X'\n\\]\n\nProof (using Projection): Let \\(P = X(X'X)^- X'\\). This is the projection matrix onto \\(\\mathcal{C}(X)\\). By definition of projection, \\(Px = x\\) for any \\(x \\in Col(X)\\). Since columns of \\(X\\) are in \\(Col(X)\\), \\(PX = X\\). Taking the transpose: \\((PX)' = X' \\implies X'P' = X'\\). Since projection matrices are symmetric (\\(P=P'\\)), \\(X'P = X'\\). Substituting \\(P\\): \\(X' X (X'X)^- X' = X'\\).\nProof (Direct Matrix Manipulation): Decompose \\(y = X\\beta + e\\) where \\(e \\perp Col(X)\\) (i.e., \\(X'e = 0\\)). \\[\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n\\] Using the property \\(A A^- A = A\\) (where \\(A=X'X\\)), the first term becomes \\(X'X\\beta\\). The second term is 0 because \\(X'e = 0\\). Thus, the expression simplifies to \\(X'X\\beta = X'(X\\beta) = X'\\hat{y}_{proj}\\). This implies the operator acts as \\(X'\\).\n\nTheorem 2.6 (Properties of Projection Matrix \\(P\\)) Let \\(P = X(X'X)^- X'\\). This matrix has the following properties:\n\nSymmetry: \\(P = P'\\).\nIdempotence: \\(P^2 = P\\). \\[\nP^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n\\] Using the identity from Lemma 2.1 (\\(X'X(X'X)^- X' = X'\\)), this simplifies to: \\[\nX(X'X)^- X' = P\n\\]\nUniqueness: \\(P\\) is unique and invariant to the choice of the generalized inverse \\((X'X)^-\\).\n\n\n\nProof. Proof of Uniqueness:\nLet \\(A\\) and \\(B\\) be two different generalized inverses of \\(X'X\\). Define \\(P_A = X A X'\\) and \\(P_B = X B X'\\). From Lemma 2.1, we know that \\(X' P_A = X'\\) and \\(X' P_B = X'\\).\nSubtracting these two equations: \\[\nX' (P_A - P_B) = 0\n\\] Taking the transpose, we get \\((P_A - P_B) X = 0\\). This implies that the columns of the difference matrix \\(D = P_A - P_B\\) are orthogonal to the columns of \\(X\\) (i.e., \\(D \\perp Col(X)\\)).\nHowever, by definition, the columns of \\(P_A\\) and \\(P_B\\) (and thus \\(D\\)) are linear combinations of the columns of \\(X\\) (i.e., \\(D \\in Col(X)\\)).\nThe only matrix that lies in the column space of \\(X\\) but is also orthogonal to the column space of \\(X\\) is the zero matrix. Therefore: \\[\nP_A - P_B = 0 \\implies P_A = P_B\n\\]\n\n\n\n2.4.3 Fitting Linear Models with QR Decomposition\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n2.4.3.1 Constructing a Solution by Solving Normal Equations\n\nProof. One specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n\n2.4.3.2 Constructing a Solution by Solving Reparametrized \\(\\beta\\)\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\n\nProof. Solution Strategy 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution Strategy 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html",
    "href": "lec3-mvn.html",
    "title": "3  Multivariate Normal Distribution",
    "section": "",
    "text": "3.1 Motivation\nConsider the linear model: \\[\ny = X\\beta + \\epsilon, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nWe are often interested in the distributional properties of the response vector \\(y\\) and the residuals. Specifically, if \\(y = (y_1, \\dots, y_n)'\\), we need to understand its multivariate distribution. \\[\n\\hat{y} = Py, \\quad e = y - \\hat{y} = (I_n - P)y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#random-vectors-and-matrices",
    "href": "lec3-mvn.html#random-vectors-and-matrices",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.2 Random Vectors and Matrices",
    "text": "3.2 Random Vectors and Matrices\n\nDefinition 3.1 (Random Vector and Matrix) A Random Vector is a vector whose elements are random variables. E.g., \\[\nx_{k \\times 1} = (x_1, x_2, \\dots, x_k)^T\n\\] where \\(x_1, \\dots, x_k\\) are each random variables.\nA Random Matrix is a matrix whose elements are random variables. E.g., \\(X_{n \\times k} = (x_{ij})\\), where \\(x_{11}, \\dots, x_{nk}\\) are each random variables.\n\n\nDefinition 3.2 (Expected Value) The expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.\nFor \\(X_{n \\times k}\\): \\[\nE(X) = \\begin{pmatrix}\nE(x_{11}) & \\dots & E(x_{1k}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE(x_{n1}) & \\dots & E(x_{nk})\n\\end{pmatrix}\n\\]\n\\[\nE\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}\\right) = \\begin{pmatrix} E(x_1) \\\\ \\vdots \\\\ E(x_k) \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Variance-Covariance Matrix) For a random vector \\(x_{k \\times 1} = (x_1, \\dots, x_k)^T\\), the matrix is:\n\\[\n\\text{var}(x) = \\Sigma_x = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2} & \\dots & \\sigma_{kk}\n\\end{pmatrix}\n\\]\nWhere:\n\n\\(\\sigma_{ij} = \\text{cov}(x_i, x_j) = E[(x_i - \\mu_i)(x_j - \\mu_j)]\\)\n\\(\\sigma_{ii} = \\text{var}(x_i) = E[(x_i - \\mu_i)^2]\\)\n\nIn matrix notation: \\[\n\\text{var}(x) = E[(x - \\mu_x)(x - \\mu_x)^T]\n\\] Note: \\(\\text{var}(x)\\) is symmetric.\n\n\n3.2.1 Derivation of Covariance Matrix Structure\nExpanding the vector multiplication for variance: \\[\n(x - \\mu_x)(x - \\mu_x)' \\quad \\text{where } \\mu_x = (\\mu_1, \\dots, \\mu_n)'\n\\] \\[\n= \\begin{pmatrix} x_1 - \\mu_1 \\\\ \\vdots \\\\ x_n - \\mu_n \\end{pmatrix} (x_1 - \\mu_1, \\dots, x_n - \\mu_n)\n\\] This results in the matrix \\(A = (a_{ij})\\) where \\(a_{ij} = (x_i - \\mu_i)(x_j - \\mu_j)\\). Taking expectations yields the covariance matrix elements \\(\\sigma_{ij}\\).\n\nDefinition 3.4 (Covariance Matrix (Two Vectors)) For random vectors \\(x_{k \\times 1}\\) and \\(y_{n \\times 1}\\), the covariance matrix is: \\[\n\\text{cov}(x, y) = E[(x - \\mu_x)(y - \\mu_y)^T] = \\begin{pmatrix}\n\\text{cov}(x_1, y_1) & \\dots & \\text{cov}(x_1, y_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{cov}(x_k, y_1) & \\dots & \\text{cov}(x_k, y_n)\n\\end{pmatrix}\n\\] Note that \\(\\text{cov}(x, x) = \\text{var}(x)\\).\n\n\nDefinition 3.5 (Correlation Matrix) The correlation matrix of a random vector \\(x\\) is: \\[\n\\text{corr}(x) = \\begin{pmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\dots & 1\n\\end{pmatrix}\n\\] where \\(\\rho_{ij} = \\text{corr}(x_i, x_j)\\).\nRelationships: Let \\(V_x = \\text{diag}(\\text{var}(x_1), \\dots, \\text{var}(x_k))\\). \\[\n\\Sigma_x = V_x^{1/2} \\rho_x V_x^{1/2} \\quad \\text{and} \\quad \\rho_x = (V_x^{1/2})^{-1} \\Sigma_x (V_x^{1/2})^{-1}\n\\] Similarly for two vectors: \\[\n\\Sigma_{xy} = V_x^{1/2} \\rho_{xy} V_y^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#properties-of-mean-and-variance",
    "href": "lec3-mvn.html#properties-of-mean-and-variance",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.3 Properties of Mean and Variance",
    "text": "3.3 Properties of Mean and Variance\nWe can derive several key algebraic properties for operations on random vectors.\n\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E(AXB) = A E(X) B\\) (In particular, \\(E(AX) = A\\mu_x\\))\n\\(\\text{cov}(x, y) = \\text{cov}(y, x)^T\\)\n\\(\\text{cov}(x + c, y + d) = \\text{cov}(x, y)\\)\n\\(\\text{cov}(Ax, By) = A \\text{cov}(x, y) B^T\\)\n\nSpecial case for scalars: \\(\\text{cov}(ax, by) = ab \\cdot \\text{cov}(x, y)\\)\n\n\\(\\text{cov}(x_1 + x_2, y_1) = \\text{cov}(x_1, y_1) + \\text{cov}(x_2, y_1)\\)\n\\(\\text{var}(x + c) = \\text{var}(x)\\)\n\\(\\text{var}(Ax) = A \\text{var}(x) A^T\\)\n\\(\\text{var}(x_1 + x_2) = \\text{var}(x_1) + \\text{cov}(x_1, x_2) + \\text{cov}(x_2, x_1) + \\text{var}(x_2)\\)\n\\(\\text{var}(\\sum x_i) = \\sum \\text{var}(x_i)\\) if independent.\n\n\n\\[\n\\begin{aligned}\n\\text{cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{cov}(x, y) B^T\n\\end{aligned}\n\\]\n\n\nTo prove \\(E(AXB) = A E(X) B\\): First consider \\(E(Ax_j)\\) where \\(x_j\\) is a column of \\(X\\). \\[\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n\\] Since \\(a_i\\) are constants: \\[\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n\\] Thus \\(E(Ax_j) = A E(x_j)\\). Applying this to all columns of \\(X\\): \\[\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n\\] Similarly, \\(E(XB) = E(X)B\\).\n\n\n\\[\n\\text{var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n\\] Let centered variables be denoted by differences. \\[\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n\\] Expanding terms: \\[\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n\\] \\[\n= \\text{var}(x_1) + \\text{cov}(x_1, x_2) + \\text{cov}(x_2, x_1) + \\text{var}(x_2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#the-multivariate-normal-distribution",
    "href": "lec3-mvn.html#the-multivariate-normal-distribution",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.4 The Multivariate Normal Distribution",
    "text": "3.4 The Multivariate Normal Distribution\n\n3.4.1 Definition and Density\n\nDefinition 3.6 (Independent Standard Normal) Let \\(z = (z_1, \\dots, z_n)'\\) where \\(z_i \\sim N(0, 1)\\) are independent. We say \\(z \\sim N_n(0, I_n)\\). The joint PDF is the product of marginals: \\[\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n\\] Properties: \\(E(z) = 0\\) and \\(\\text{var}(z) = I_n\\) (Covariance is 0 for \\(i \\ne j\\), Variance is 1).\n\n\nDefinition 3.7 (Multivariate Normal Distribution) A random vector \\(x\\) (\\(n \\times 1\\)) has a multivariate normal distribution if it has the same distribution as: \\[\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n\\] where \\(z \\sim N_p(0, I_p)\\), \\(A\\) is a matrix of constants, and \\(\\mu\\) is a vector of constants. The moments are:\n\n\\(E(x) = \\mu\\)\n\\(\\text{var}(x) = AA^T = \\Sigma\\)\n\n\n\n\n3.4.2 Geometric Interpretation\nUsing Spectral Decomposition, \\(\\Sigma = Q \\Lambda Q'\\). We can view the transformation \\(x = Az + \\mu\\) as:\n\nScaling by eigenvalues (\\(\\Lambda^{1/2}\\)).\nRotation by eigenvectors (\\(Q\\)).\nShift by mean (\\(\\mu\\)).\n\nAn Shinely App for Visualizing Bivariate Normal\nUse the controls to construct the covariance matrix \\(\\boldsymbol{\\Sigma}\\) geometrically.\nWe define the transformation matrix \\(\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}\\), where \\(\\mathbf{Q}\\) is a rotation matrix and \\(\\mathbf{\\Lambda}^{1/2}\\) is a diagonal scaling matrix. The resulting covariance is \\(\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}'\\).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n#| echo: false\n\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(shinyWidgets)\nlibrary(munsell) \nlibrary(scales)\nlibrary(tibble)\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# --- 1. PRE-GENERATE FIXED Z POINTS ---\nset.seed(123)\nz_fixed &lt;- matrix(rnorm(50 * 2), ncol = 2)\n\nui &lt;- page_fillable(\n  theme = bs_theme(version = 5),\n  withMathJax(), \n  \n  # --- ROW 1: CONTROLS (Compact Strip) ---\n  card(\n    class = \"p-2\", \n    layout_columns(\n      col_widths = c(3, 2, 2, 2, 2),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\theta$$\")), \n          noUiSliderInput(\"theta\", label = NULL, min = 0, max = 360, value = 0, step = 5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#0d6efd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_1}$$\")), \n          noUiSliderInput(\"L1\", label = NULL, min = 0.5, max = 3, value = 2, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#ffc107\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_2}$$\")), \n          noUiSliderInput(\"L2\", label = NULL, min = 0.5, max = 3, value = 1, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#adb5bd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_1$$\")), \n          noUiSliderInput(\"mu1\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_2$$\")), \n          noUiSliderInput(\"mu2\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\"))\n    )\n  ),\n\n  # --- ROW 2: SIDE-BY-SIDE (Plot & Math) ---\n  layout_columns(\n    col_widths = c(8, 4), # 2/3 for Plot, 1/3 for Matrix\n    \n    # Left: Visualization\n    card(\n      full_screen = TRUE,\n      plotOutput(\"contourPlot\", height = \"500px\")\n    ),\n    \n    # Right: The Math (Larger Font)\n    card(\n      class = \"p-3 d-flex justify-content-center\", # Center content vertically\n      h5(\"Algebraic Representation\", class = \"mb-3 text-center\"),\n      \n      # Use CSS to make the font larger and monospaced\n      div(\n        style = \"font-family: 'Courier New', monospace; font-size: 1.1rem; line-height: 1.4;\",\n        verbatimTextOutput(\"matrixSide\", placeholder = TRUE)\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n\n  data &lt;- reactive({\n    theta_rad &lt;- input$theta * pi / 180\n    Q &lt;- matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), 2, 2)\n    Lam_sqrt &lt;- diag(c(input$L1, input$L2))\n    \n    A &lt;- Q %*% Lam_sqrt\n    Sigma &lt;- A %*% t(A)\n    mu_vec &lt;- c(input$mu1, input$mu2)\n    \n    x_points &lt;- z_fixed %*% t(A)\n    x_points[,1] &lt;- x_points[,1] + mu_vec[1]\n    x_points[,2] &lt;- x_points[,2] + mu_vec[2]\n    \n    list(Q=Q, L=c(input$L1, input$L2), mu=mu_vec, Sigma=Sigma, A=A, points=as.data.frame(x_points))\n  })\n\n  output$matrixSide &lt;- renderText({\n    M &lt;- data()\n    A &lt;- round(M$A, 2)\n    S &lt;- round(M$Sigma, 2)\n    rho &lt;- cov2cor(M$Sigma)[1,2]\n    \n    # Formatted to fill vertical space comfortably\n    paste0(\n      \"Linear Transform:\\n\",\n      \"x = A z + μ\\n\\n\",\n      \n      \"Matrix A:\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[1,1], A[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[2,1], A[2,2]),\n      \"\\n\",\n      \n      \"Covariance Σ:\\n\",\n      \"(Σ = AA')\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[1,1], S[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[2,1], S[2,2]),\n      \"\\n\",\n      \n      \"Correlation:\\n\",\n      sprintf(\"ρ = %.3f\", rho)\n    )\n  })\n\n  output$contourPlot &lt;- renderPlot({\n    req(data())\n    M &lt;- data()\n    \n    grid_r &lt;- seq(-6, 6, length.out = 60)\n    df_grid &lt;- expand.grid(x = grid_r, y = grid_r)\n    df_grid$z &lt;- dmvnorm(as.matrix(df_grid), mean = M$mu, sigma = M$Sigma)\n    \n    v1 &lt;- M$Q[,1] * M$L[1]; v2 &lt;- M$Q[,2] * M$L[2]\n    axes &lt;- tibble(x = M$mu[1], y = M$mu[2],\n                   xend1 = M$mu[1] + v1[1], yend1 = M$mu[2] + v1[2],\n                   xend2 = M$mu[1] + v2[1], yend2 = M$mu[2] + v2[2])\n    \n    ggplot() +\n      geom_contour_filled(data = df_grid, aes(x, y, z = z), bins = 9, show.legend = FALSE) +\n      geom_point(data = M$points, aes(V1, V2), color = \"black\", size = 2, alpha = 0.7) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend1, yend=yend1), \n                   color = \"#ffc107\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend2, yend=yend2), \n                   color = \"white\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      coord_fixed(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      theme_minimal() +\n      labs(x = \"X\", y = \"Y\")\n  })\n}\n\nshinyApp(ui, server)\n\n\n3.4.3 Probability Density Function\nIf \\(\\Sigma\\) is positive definite, the PDF exists. We use the change of variable formula for \\(x = Az + \\mu\\): \\[\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n\\] where \\(z = A^{-1}(x - \\mu)\\) and \\(J = \\det(A^{-1}) = |A|^{-1}\\).\n\\[\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n\\]\nUsing \\(|\\Sigma| = |AA^T| = |A|^2\\) and \\(\\Sigma^{-1} = (AA^T)^{-1}\\), we get: \\[\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n\\]\n\n\n3.4.4 Moment Generating Function\n\nDefinition 3.8 (Moment Generating Function (MGF)) The MGF of a random vector \\(x\\) is \\(M_x(t) = E(e^{t^T x})\\). For \\(x = Az + \\mu\\): \\[\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n\\] Since \\(M_z(u) = e^{u^T u / 2}\\): \\[\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\n\nKey Properties:\n\nUniqueness: Two random vectors with the same MGF have the same distribution.\nIndependence: \\(y_1\\) and \\(y_2\\) are independent iff \\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#construction-and-linear-transformations",
    "href": "lec3-mvn.html#construction-and-linear-transformations",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.5 Construction and Linear Transformations",
    "text": "3.5 Construction and Linear Transformations\n\nTheorem 3.1 (Constructing MVN Random Vector) Let \\(\\mu \\in \\mathbb{R}^n\\) and \\(\\Sigma\\) be an \\(n \\times n\\) symmetric positive semi-definite (p.s.d.) matrix. Then there exists a multivariate normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\).\nProof: Since \\(\\Sigma\\) is p.s.d., there exists \\(B\\) such that \\(\\Sigma = BB^T\\) (e.g., via Cholesky). Let \\(z \\sim N_n(0, I)\\) and define \\(x = Bz + \\mu\\).\n\n\nTheorem 3.2 (Linear Transformation Theorem) Let \\(x \\sim N_n(\\mu, \\Sigma)\\). Let \\(y = Cx + d\\) where \\(C\\) is \\(r \\times n\\) and \\(d\\) is \\(r \\times 1\\). Then: \\[\ny \\sim N_r(C\\mu + d, C \\Sigma C^T)\n\\]\nProof: \\(x = Az + \\mu\\) where \\(AA^T = \\Sigma\\). \\[\ny = C(Az + \\mu) + d = (CA)z + (C\\mu + d)\n\\] This fits the definition of MVN with mean \\(C\\mu + d\\) and variance \\(C \\Sigma C^T\\).\n\n\n3.5.1 Corollaries\n\nCorollary 3.1 (Marginals) Any subvector of a multivariate normal vector is also multivariate normal. If we partition \\(x = (x_1', x_2')'\\), we can use \\(C = (I_r, 0)\\) to show \\(x_1 \\sim N(\\mu_1, \\Sigma_{11})\\).\n\n\nCorollary 3.2 (Univariate Combinations) Any linear combination \\(a^T x\\) is univariate normal: \\[\na^T x \\sim N(a^T \\mu, a^T \\Sigma a)\n\\]\n\n\nCorollary 3.3 (Orthogonal Transformations) If \\(x \\sim N(0, I_n)\\) and \\(Q\\) is orthogonal (\\(Q'Q = I\\)), then \\(y = Q'x \\sim N(0, I_n)\\).\n\n\nCorollary 3.4 (Standardization) If \\(y \\sim N_n(\\mu, \\Sigma)\\) and \\(\\Sigma\\) is positive definite: \\[\n\\Sigma^{-1/2}(y - \\mu) \\sim N_n(0, I_n)\n\\] Proof: Let \\(z = \\Sigma^{-1/2}(y - \\mu)\\). Then \\(\\text{var}(z) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = I_n\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#independence",
    "href": "lec3-mvn.html#independence",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.6 Independence",
    "text": "3.6 Independence\n\nTheorem 3.3 (Independence in MVN) Let \\(y \\sim N(\\mu, \\Sigma)\\) be partitioned into \\(y_1\\) and \\(y_2\\). \\[\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n\\] Then \\(y_1\\) and \\(y_2\\) are independent if and only if \\(\\Sigma_{12} = 0\\) (zero covariance).\n\n\n1. Independence \\(\\implies\\) Covariance is 0: This holds generally for any distribution. \\[\n\\text{cov}(y_1, y_2) = E[(y_1 - \\mu_1)(y_2 - \\mu_2)'] = 0\n\\]\n2. Covariance is 0 \\(\\implies\\) Independence: This is specific to MVN. We use MGFs. If \\(\\Sigma_{12} = 0\\), the quadratic form in the MGF splits: \\[\nt^T \\Sigma t = t_1^T \\Sigma_{11} t_1 + t_2^T \\Sigma_{22} t_2\n\\] The MGF becomes: \\[\nM_y(t) = \\exp(t_1^T \\mu_1 + \\frac{1}{2} t_1^T \\Sigma_{11} t_1) \\times \\exp(t_2^T \\mu_2 + \\frac{1}{2} t_2^T \\Sigma_{22} t_2)\n\\] \\[\nM_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\n\\] Thus, they are independent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#conditional-distributions",
    "href": "lec3-mvn.html#conditional-distributions",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.7 Conditional Distributions",
    "text": "3.7 Conditional Distributions\nWe often wish to find the distribution of a subvector \\(y_2\\) given the value of another subvector \\(y_1\\).\n\nLemma 3.1 (Constructing Independent Vectors) Let \\(y \\sim N_n(\\mu, \\Sigma)\\) partitioned into \\(y_1\\) and \\(y_2\\). Define: \\[\ny_{2|1} = y_2 - \\Sigma_{21}\\Sigma_{11}^{-1}y_1\n\\] Then \\(y_1\\) and \\(y_{2|1}\\) are independent.\nProof: Consider the linear transformation: \\[\n\\begin{pmatrix} y_1 \\\\ y_{2|1} \\end{pmatrix} = C y = \\begin{pmatrix} I & 0 \\\\ -\\Sigma_{21}\\Sigma_{11}^{-1} & I \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n\\] The covariance matrix is \\(C \\Sigma C'\\). The off-diagonal block is: \\[\n\\text{cov}(y_1, y_{2|1}) = \\Sigma_{11}(-\\Sigma_{11}^{-1}\\Sigma_{12}) + \\Sigma_{12} = -\\Sigma_{12} + \\Sigma_{12} = 0\n\\] Since covariance is zero, they are independent.\n\n\n\nTheorem 3.4 (Conditional Distribution Theorem) The conditional distribution of \\(y_2\\) given \\(y_1\\) is multivariate normal: \\[\ny_2 | y_1 \\sim N(\\mu_{2|1}, \\Sigma_{22|1})\n\\] where:\n\nConditional Mean: \\(E(y_2 | y_1) = \\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(y_1 - \\mu_1)\\)\nConditional Variance: \\(\\text{var}(y_2 | y_1) = \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\)\n\nProof: Write \\(y_2 = y_{2|1} + \\Sigma_{21}\\Sigma_{11}^{-1}y_1\\). Conditional on \\(y_1\\), the term \\(\\Sigma_{21}\\Sigma_{11}^{-1}y_1\\) is constant. Since \\(y_{2|1}\\) is independent of \\(y_1\\), its conditional distribution is simply its marginal distribution. Thus, the mean shifts by the constant term, and the variance remains that of \\(y_{2|1}\\) (the Schur complement).\n\n\nLet \\(\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix} \\right)\\). Find the distribution of \\(y_1 | y_2\\).\n\n\\(\\mu_{1|2} = \\mu_1 + \\sigma_{12}\\sigma_{22}^{-1}(y_2 - \\mu_2) = 1 + 1(4)^{-1}(y_2 - 2) = 0.5 + 0.25y_2\\)\n\\(\\sigma_{1|2}^2 = \\sigma_{11} - \\sigma_{12}\\sigma_{22}^{-1}\\sigma_{21} = 2 - 1(1/4)1 = 1.75\\)\n\nSo \\(y_1 | y_2 \\sim N(0.5 + 0.25y_2, 1.75)\\).\n\n\n3.7.1 Variance Decomposition\nThe Law of Total Variance states: \\[\n\\text{var}(y_2) = E[\\text{var}(y_2 | y_1)] + \\text{var}[E(y_2 | y_1)]\n\\] In the MVN case:\n\n\\(E[\\text{var}(y_2 | y_1)] = \\Sigma_{22|1}\\) (constant variance).\n\\(\\text{var}[E(y_2 | y_1)] = \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\) (explained variance). Summing these returns the total variance \\(\\Sigma_{22}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#partial-and-multiple-correlation",
    "href": "lec3-mvn.html#partial-and-multiple-correlation",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.8 Partial and Multiple Correlation",
    "text": "3.8 Partial and Multiple Correlation\n\nDefinition 3.9 (Partial Correlation) The partial correlation between elements \\(y_i\\) and \\(y_j\\) given a set of variables \\(x\\) is derived from the conditional covariance matrix \\(\\Sigma_{y|x}\\): \\[\n\\rho_{ij|x} = \\frac{\\sigma_{ij|x}}{\\sqrt{\\sigma_{ii|x} \\sigma_{jj|x}}}\n\\] where \\(\\sigma_{ij|x}\\) are elements of \\(\\Sigma_{y|x} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\\).\n\n\nDefinition 3.10 (Multiple Correlation (\\(R^2\\))) For a scalar \\(y\\) and vector \\(x\\), the squared multiple correlation is the proportion of variance of \\(y\\) explained by the conditional mean: \\[\n\\rho_{y|x}^2 = \\frac{\\text{var}(E(y|x))}{\\text{var}(y)} = \\frac{\\Sigma_{yx} \\Sigma_{xx}^{-1} \\Sigma_{xy}}{\\sigma_{yy}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  }
]