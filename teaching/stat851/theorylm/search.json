[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "-1.1 Overview\nThis book has evolved from the lecture notes for STAT 851: Statistical Linear Models, a graduate-level course taught at the University of Saskatchewan. This course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#audience-and-prerequisites",
    "href": "index.html#audience-and-prerequisites",
    "title": "Statistical Theory for Linear Models",
    "section": "-1.2 Audience and Prerequisites",
    "text": "-1.2 Audience and Prerequisites",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Theory for Linear Models",
    "section": "-1.3 Key Features",
    "text": "-1.3 Key Features",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Statistical Theory for Linear Models",
    "section": "-1.4 Structure of the Book",
    "text": "-1.4 Structure of the Book",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1 Overview\nThis lecture covers the foundations of Linear Statistical Models, including Multiple Linear Regression and ANOVA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#geometric-interpretation-of-least-squares",
    "href": "introlm.html#geometric-interpretation-of-least-squares",
    "title": "2  Lecture 1: Theory of Linear Models",
    "section": "3.1 Geometric Interpretation of Least Squares",
    "text": "3.1 Geometric Interpretation of Least Squares\nWe compare models based on the reduction of errors. Consider a full model and a reduced model (\\(K_1\\)).\nLet:\n\n\\(y\\) be the observed vector.\n\\(\\hat{y}_1\\) be the prediction from the reduced model.\n\\(\\hat{y}_2\\) be the prediction from the full model.\n\n\n\n\nGeometric Interpretation of Least Squares\n\n\nThe errors (residuals) are defined as: \\[\ne_1 = y - \\hat{y}_1\n\\] \\[\ne_2 = y - \\hat{y}_2\n\\]\nThe Sum of Squared Errors (SSE) representing the distance is: \\[\nSSE_1 = ||y - \\hat{y}_1||^2\n\\] \\[\nSSE_2 = ||y - \\hat{y}_2||^2\n\\]\nThe statistical test is often based on the comparison of \\(SSE_1\\) and \\(SSE_2\\) (or the reduction in sums of squares \\(||\\hat{y}_2 - \\hat{y}_1||^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lecture 1: Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#matrix-formulation",
    "href": "introlm.html#matrix-formulation",
    "title": "2  Lecture 1: Theory of Linear Models",
    "section": "4.1 Matrix Formulation",
    "text": "4.1 Matrix Formulation\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nWhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n)\n\\]\nAnd \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\]\nThe scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lecture 1: Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#design-matrix-construction",
    "href": "introlm.html#design-matrix-construction",
    "title": "2  Lecture 1: Theory of Linear Models",
    "section": "5.1 Design Matrix Construction",
    "text": "5.1 Design Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lecture 1: Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#dummy-variable-matrix",
    "href": "introlm.html#dummy-variable-matrix",
    "title": "2  Lecture 1: Theory of Linear Models",
    "section": "6.1 Dummy Variable Matrix",
    "text": "6.1 Dummy Variable Matrix\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lecture 1: Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#least-squares-estimation",
    "href": "introlm.html#least-squares-estimation",
    "title": "2  Lecture 1: Theory of Linear Models",
    "section": "7.1 Least Squares Estimation",
    "text": "7.1 Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\nSSE = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{SST - SSE}{SST}\n\\] where \\(SST = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lecture 1: Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#overview",
    "href": "introlm.html#overview",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "",
    "text": "2.1.1 Geometric Interpretation of Least Squares\nWe compare models based on the reduction of errors. Consider a full model and a reduced model (\\(K_1\\)).\nLet:\n\n\\(y\\) be the observed vector.\n\\(\\hat{y}_1\\) be the prediction from the reduced model.\n\\(\\hat{y}_2\\) be the prediction from the full model.\n\n\n\n\nGeometric Interpretation of Least Squares\n\n\nThe errors (residuals) are defined as: \\[\ne_1 = y - \\hat{y}_1\n\\] \\[\ne_2 = y - \\hat{y}_2\n\\]\nThe Sum of Squared Errors (SSE) representing the distance is: \\[\nSSE_1 = ||y - \\hat{y}_1||^2\n\\] \\[\nSSE_2 = ||y - \\hat{y}_2||^2\n\\]\nThe statistical test is often based on the comparison of \\(SSE_1\\) and \\(SSE_2\\) (or the reduction in sums of squares \\(||\\hat{y}_2 - \\hat{y}_1||^2\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#multiple-linear-regression",
    "href": "introlm.html#multiple-linear-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\n\n2.2.1 Matrix Formulation\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nWhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n)\n\\]\nAnd \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\]\nThe scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#polynomial-regression",
    "href": "introlm.html#polynomial-regression",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.3 Polynomial Regression",
    "text": "2.3 Polynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n2.3.1 Design Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#one-way-anova",
    "href": "introlm.html#one-way-anova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.4 One-Way ANOVA",
    "text": "2.4 One-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\n\n\nOne-Way ANOVA Diagram\n\n\n\n2.4.1 Dummy Variable Matrix\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "introlm.html#analysis-of-covariance-ancova",
    "href": "introlm.html#analysis-of-covariance-ancova",
    "title": "2  Introduction to Theory of Linear Models",
    "section": "2.5 Analysis of Covariance (ANCOVA)",
    "text": "2.5 Analysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,cont} & 1 & 0 \\\\\nX_{2,cont} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,cont} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]\n\n2.5.1 Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\nSSE = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{SST - SSE}{SST}\n\\] where \\(SST = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Theory of Linear Models</span>"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "Audience and Prerequisites\n\n\nKey Features\n\n\nStructure of the Book",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Theory for Linear Models",
    "section": "-1.5 Acknowledgements",
    "text": "-1.5 Acknowledgements\nThe lectures were built upon the lecture notes for stat 8260 by Danniel Hall and the textbook LINEAR MODELS IN STATISTICS, Second Edition, by Alvin C. Rencher and G. Bruce Schaalje, ISBN 978-0-471-75498-5 (cloth)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html",
    "href": "lec2-vecspace.html",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "3.1 Vector and Projection onto a Line\nVectors and Operations\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\nVector Arithmetic: Vectors can be manipulated geometrically:\nScalar Multiplication and Length\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\nWe often need to quantify the “size” of a vector. This is done using the concept of length, or norm.\nAngle and Inner Product\nTo understand the relationship between two vectors \\(x\\) and \\(y\\) beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the classic Law of Cosines to this triangle, we can relate the geometric angle to the vector lengths.\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation provides a critical link between the geometric angle \\(\\theta\\) and the algebraic norms of the vectors.\nDerivation of Inner Product\nWe can express the squared distance term \\(||y - x||^2\\) purely algebraically by expanding the components:\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the Inner Product (or dot product).\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\nCoordinate (Scalar) Projection\nThe inner product allows us to calculate projections, which quantify how much of one vector “lies along” another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of this projection is given by:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis expression can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector in the direction of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]\nVector Projection Formula\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\nPerpendicularity (Orthogonality)\nA special case of the angle between vectors arises when \\(\\theta = 90^\\circ\\). This geometric concept of perpendicularity is central to the theory of projections and least squares.\nProjection onto a Line (Subspace)\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\nThe projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that it is the closest point on the line to \\(y\\). This implies that the error vector (or residual) must be perpendicular to the line itself.\nDerivation: To find the value of the scalar \\(c\\), we apply the orthogonality condition:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\nExpanding this inner product gives:\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\), we obtain:\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\nAlternative Forms of the Projection Formula\nWe can express the projection vector \\(\\hat{y}\\) in several equivalent ways to highlight different geometric interpretations.\nProjection Matrix (\\(P_x\\))\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector \\(y\\). This allows us to define a Projection Matrix.\nWe can rewrite the formula for \\(\\hat{y}\\) by factoring out \\(y\\):\n\\[\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n\\]\nThis leads to the definition of the projection matrix \\(P_x\\).\nExample: Projection in \\(\\mathbb{R}^2\\)\nLet’s apply these concepts to a concrete example.\nExample: Projection onto the Mean Vector\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\nPythagorean Theorem\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\nProof: We expand the squared norm using the inner product:\n\\[\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n\\]\nSince \\(x \\perp y\\), the inner product \\(x'y = 0\\). Thus, the term \\(2x'y\\) vanishes, leaving:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\nLeast Square Property\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector \\(y\\) and the subspace (or line) onto which it is projected.\nProof: Since both \\(\\hat{y}\\) and \\(y^*\\) lie on the line \\(L(x)\\), their difference \\((\\hat{y} - y^*)\\) also lies on \\(L(x)\\). From the definition of projection, the residual \\((y - \\hat{y})\\) is orthogonal to the line \\(L(x)\\). Therefore:\n\\[\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n\\]\nWe can write the vector \\((y - y^*)\\) as: \\[\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n\\]\nApplying the Pythagorean Theorem: \\[\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n\\]\nSince \\(||\\hat{y} - y^*||^2 \\ge 0\\), it follows that: \\[\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-geometry",
    "href": "lec2-vecspace.html#vector-and-geometry",
    "title": "3  Vector Space and Projection",
    "section": "4.1 Vector and Geometry",
    "text": "4.1 Vector and Geometry\nA vector \\(x\\) is a point in \\(\\mathbb{R}^n\\): \\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n4.1.1 Addition and Subtraction\nVector addition is defined component-wise: \\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\n\nVector Addition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#euclidean-distance-length",
    "href": "lec2-vecspace.html#euclidean-distance-length",
    "title": "3  Vector Space and Projection",
    "section": "4.2 Euclidean Distance (Length)",
    "text": "4.2 Euclidean Distance (Length)\nThe length of a vector \\(x = (x_1, ..., x_n)^T\\) is defined by the Euclidean norm: \\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\] \\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#inner-product-and-angle",
    "href": "lec2-vecspace.html#inner-product-and-angle",
    "title": "3  Vector Space and Projection",
    "section": "4.3 Inner Product and Angle",
    "text": "4.3 Inner Product and Angle\nThe relationship between the lengths of vectors and the angle \\(\\theta\\) between them is given by the law of cosines: \\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nExpanding the distance term: \\[\n||y - x||^2 = \\sum (x_i - y_i)^2 = ||x||^2 + ||y||^2 - 2x'y\n\\]\nComparing the two equations yields the definition of the inner product: \\[\nx'y = \\sum_{i=1}^n x_i y_i = &lt;x, y&gt; = ||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThus, the cosine of the angle is: \\[\n\\cos \\theta = \\frac{x'y}{||x|| \\cdot ||y||}\n\\]\nTwo vectors are perpendicular (orthogonal) if \\(x'y = 0\\), which implies \\(\\theta = 90^\\circ\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-matrix-p_x",
    "href": "lec2-vecspace.html#projection-matrix-p_x",
    "title": "3  Vector Space and Projection",
    "section": "5.1 Projection Matrix (\\(P_x\\))",
    "text": "5.1 Projection Matrix (\\(P_x\\))\nWe can rewrite \\(\\hat{y}\\) in matrix form: \\[\n\\hat{y} = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y = P_x y\n\\] where \\(P_x = \\frac{xx'}{||x||^2}\\) is the projection matrix onto \\(L(x)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#column-space-and-row-space",
    "href": "lec2-vecspace.html#column-space-and-row-space",
    "title": "3  Vector Space and Projection",
    "section": "8.1 Column Space and Row Space",
    "text": "8.1 Column Space and Row Space\nFor a matrix \\(X = (x_1, ..., x_p)\\): * Column Space: \\(C(X) = L(x_1, ..., x_p)\\) * Row Space: \\(Row(X) = L(r_1, ..., r_n)\\) where \\(r_i\\) are rows of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#rank-and-nullity",
    "href": "lec2-vecspace.html#rank-and-nullity",
    "title": "3  Vector Space and Projection",
    "section": "8.2 Rank and Nullity",
    "text": "8.2 Rank and Nullity\n\nRank: \\(rank(X) = \\text{dim}(C(X)) = \\text{dim}(Row(X))\\).\nKernel (Null Space): \\(Ker(X) = \\{\\beta \\in \\mathbb{R}^p \\mid X\\beta = 0\\}\\).\nNullity Theorem: \\(rank(X) + nullity(X) = p\\).\n\nKey property: \\(Ker(X) = [Row(X)]^\\perp\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#properties-of-projection-matrices",
    "href": "lec2-vecspace.html#properties-of-projection-matrices",
    "title": "3  Vector Space and Projection",
    "section": "9.1 Properties of Projection Matrices",
    "text": "9.1 Properties of Projection Matrices\nA matrix \\(P\\) is a projection matrix onto \\(C(P)\\) if and only if: 1. Symmetric: \\(P' = P\\) 2. Idempotent: \\(P^2 = P\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-onto-orthogonal-complement",
    "href": "lec2-vecspace.html#projection-onto-orthogonal-complement",
    "title": "3  Vector Space and Projection",
    "section": "9.2 Projection onto Orthogonal Complement",
    "text": "9.2 Projection onto Orthogonal Complement\nIf \\(P\\) is a projection onto \\(C(P)\\), then \\(I - P\\) is a projection onto \\(C(P)^\\perp\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-2-outline",
    "href": "lec2-vecspace.html#page-2-outline",
    "title": "3  Vector Space and Projection",
    "section": "4.1 Page 2: Outline",
    "text": "4.1 Page 2: Outline\n\nVector and Geometry\nInner Product and Perpendicular\nProjection to a Single Vector\nPythagorean theory\nShortest distance property of projection",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-3-vectors",
    "href": "lec2-vecspace.html#page-3-vectors",
    "title": "3  Vector Space and Projection",
    "section": "4.2 Page 3: Vectors",
    "text": "4.2 Page 3: Vectors\nA vector \\(x\\) is a point in \\(\\mathbb{R}^n\\).\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\nOperations: * Addition: \\(x + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\\) * Subtraction: \\(d = y - x \\implies x + d = y\\)\n\n\n\nVector Addition and Subtraction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-4-scalar-multiplication-and-length",
    "href": "lec2-vecspace.html#page-4-scalar-multiplication-and-length",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.4 Page 4: Scalar Multiplication and Length",
    "text": "4.4 Page 4: Scalar Multiplication and Length\nScalar Multiplication: Multiplying a vector by a scalar \\(c\\) scales its magnitude without changing its line of direction (though it may reverse direction if \\(c &lt; 0\\)). \\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\nEuclidean Distance (Length): The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point \\(x\\). It is defined as:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\] \\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\nScalar Multiplication",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-5-angle-and-inner-product",
    "href": "lec2-vecspace.html#page-5-angle-and-inner-product",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.5 Page 5: Angle and Inner Product",
    "text": "4.5 Page 5: Angle and Inner Product\nTo understand the geometry between two vectors \\(x\\) and \\(y\\), we consider the triangle formed by \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the Law of Cosines to this triangle:\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\nTranslating this into vector notation where side lengths are norms:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation links the geometric angle \\(\\theta\\) to the algebraic norms of the vectors.\n\n\n\nGeometry of Inner Product",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-6-deriving-inner-product",
    "href": "lec2-vecspace.html#page-6-deriving-inner-product",
    "title": "3  Vector Space and Projection",
    "section": "4.5 Page 6: Deriving Inner Product",
    "text": "4.5 Page 6: Deriving Inner Product\nExpanding the distance squared: \\[\n||y - x||^2 = \\sum (x_i - y_i)^2 = \\sum (x_i^2 + y_i^2 - 2x_i y_i)\n\\] \\[\n= ||x||^2 + ||y||^2 - 2 \\sum x_i y_i\n\\]\nComparing this with the Law of Cosines: \\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle = ||x|| \\cdot ||y|| \\cos \\theta\n\\] This defines the Inner Product.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-7-coordinate-projection",
    "href": "lec2-vecspace.html#page-7-coordinate-projection",
    "title": "3  Vector Space and Projection",
    "section": "4.6 Page 7: Coordinate Projection",
    "text": "4.6 Page 7: Coordinate Projection\nRearranging the cosine formula: \\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\] This represents the length of the projection of \\(y\\) onto \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-8-projection-vector",
    "href": "lec2-vecspace.html#page-8-projection-vector",
    "title": "3  Vector Space and Projection",
    "section": "4.7 Page 8: Projection Vector",
    "text": "4.7 Page 8: Projection Vector\nThe projection vector itself is the length multiplied by the unit direction vector: \\[\n\\text{Projection} = (||y|| \\cos \\theta) \\cdot \\frac{x}{||x||}\n\\] \\[\n= \\frac{x'y}{||x||} \\cdot \\frac{x}{||x||}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-9-perpendicularity",
    "href": "lec2-vecspace.html#page-9-perpendicularity",
    "title": "3  Vector Space and Projection",
    "section": "4.8 Page 9: Perpendicularity",
    "text": "4.8 Page 9: Perpendicularity\nTwo vectors are perpendicular (orthogonal) if \\(\\theta = 90^\\circ\\) (\\(\\pi/2\\)). This implies \\(\\cos \\theta = 0\\), so: \\[\nx'y = 0 \\iff x \\perp y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-10-projection-definition",
    "href": "lec2-vecspace.html#page-10-projection-definition",
    "title": "3  Vector Space and Projection",
    "section": "4.9 Page 10: Projection Definition",
    "text": "4.9 Page 10: Projection Definition\nLet \\(L(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\\). \\(\\hat{y}\\) is the projection of \\(y\\) onto \\(L(x)\\) if: 1. \\(\\hat{y} \\in L(x)\\) (i.e., \\(\\hat{y} = cx\\)) 2. \\((y - \\hat{y}) \\perp x\\)\nSolving for \\(c\\): \\[\nx'(y - cx) = 0 \\implies x'y - c(x'x) = 0 \\implies c = \\frac{x'y}{||x||^2}\n\\]\n\n\n\nProjection Definition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-11-projection-formula",
    "href": "lec2-vecspace.html#page-11-projection-formula",
    "title": "3  Vector Space and Projection",
    "section": "4.10 Page 11: Projection Formula",
    "text": "4.10 Page 11: Projection Formula\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\] This can be seen as (Scale) \\(\\times\\) (Direction).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-12-projection-matrix-p_x",
    "href": "lec2-vecspace.html#page-12-projection-matrix-p_x",
    "title": "3  Vector Space and Projection",
    "section": "4.11 Page 12: Projection Matrix \\(P_x\\)",
    "text": "4.11 Page 12: Projection Matrix \\(P_x\\)\n\\[\n\\hat{y} = \\text{proj}(y|x) = \\frac{x'y}{||x||^2} x = x \\frac{x'y}{||x||^2}\n\\] \\[\n= \\frac{xx'}{||x||^2} y = P_x y\n\\] Here, \\(P_x = \\frac{xx'}{||x||^2}\\) is the projection matrix (dimensions \\(p \\times p\\) if \\(x \\in \\mathbb{R}^p\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-13-14-examples",
    "href": "lec2-vecspace.html#page-13-14-examples",
    "title": "3  Vector Space and Projection",
    "section": "4.12 Page 13-14: Examples",
    "text": "4.12 Page 13-14: Examples\nExample 1: \\(y = (1, 3)'\\), \\(x = (1, 1)'\\). \\[\n\\hat{y} = \\frac{1(1) + 1(3)}{1^2 + 1^2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nUsing Matrix: \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n\\] \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-15-example-with-mean-vector",
    "href": "lec2-vecspace.html#page-15-example-with-mean-vector",
    "title": "3  Vector Space and Projection",
    "section": "4.13 Page 15: Example with Mean Vector",
    "text": "4.13 Page 15: Example with Mean Vector\nLet \\(y = (y_1, \\dots, y_n)'\\) and \\(J_n = (1, \\dots, 1)'\\). \\[\n\\text{proj}(y|J_n) = \\frac{J_n' y}{||J_n||^2} J_n = \\frac{\\sum y_i}{n} J_n = \\bar{y} J_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-16-17-pythagorean-theorem",
    "href": "lec2-vecspace.html#page-16-17-pythagorean-theorem",
    "title": "3  Vector Space and Projection",
    "section": "4.14 Page 16-17: Pythagorean Theorem",
    "text": "4.14 Page 16-17: Pythagorean Theorem\nGeometry: \\(c^2 = a^2 + b^2\\).\nVector Space: If \\(x \\perp y\\) (i.e., \\(x'y = 0\\)), then: \\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\] Proof: \\[\n||x + y||^2 = (x+y)'(x+y) = x'x + 2x'y + y'y = ||x||^2 + 0 + ||y||^2\n\\]\n\n\n\nPythagorean Theorem",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-18-19-shortest-distance-property",
    "href": "lec2-vecspace.html#page-18-19-shortest-distance-property",
    "title": "3  Vector Space and Projection",
    "section": "4.15 Page 18-19: Shortest Distance Property",
    "text": "4.15 Page 18-19: Shortest Distance Property\nThe projection \\(\\hat{y} = P(y|x)\\) is the closest vector in \\(L(x)\\) to \\(y\\). For any other vector \\(y^* \\in L(x)\\): \\[\n||y - y^*||^2 = ||(y - \\hat{y}) + (\\hat{y} - y^*)||^2\n\\] Since \\((y - \\hat{y}) \\perp L(x)\\) and \\((\\hat{y} - y^*) \\in L(x)\\), they are orthogonal. \\[\n= ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2 \\ge ||y - \\hat{y}||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-21-definition",
    "href": "lec2-vecspace.html#page-21-definition",
    "title": "3  Vector Space and Projection",
    "section": "5.1 Page 21: Definition",
    "text": "5.1 Page 21: Definition\nA subset \\(V \\subseteq \\mathbb{R}^n\\) is a vector space if: 1. Closed under addition: \\(x_1, x_2 \\in V \\implies x_1 + x_2 \\in V\\). 2. Closed under scalar multiplication: \\(x \\in V \\implies cx \\in V\\) (including \\(c=0\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-23-spanned-vector-space",
    "href": "lec2-vecspace.html#page-23-spanned-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "5.2 Page 23: Spanned Vector Space",
    "text": "5.2 Page 23: Spanned Vector Space\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p \\}\n\\] If vectors are dependent (e.g., \\(x_1 = c x_2\\)), the space spanned is the same as the reduced set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-25-column-and-row-space",
    "href": "lec2-vecspace.html#page-25-column-and-row-space",
    "title": "3  Vector Space and Projection",
    "section": "5.3 Page 25: Column and Row Space",
    "text": "5.3 Page 25: Column and Row Space\nFor matrix \\(X\\): * Column Space: \\(C(X) = L(\\text{columns of } X)\\) * Row Space: \\(Row(X) = L(\\text{rows of } X)\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-26-linear-independence",
    "href": "lec2-vecspace.html#page-26-linear-independence",
    "title": "3  Vector Space and Projection",
    "section": "5.4 Page 26: Linear Independence",
    "text": "5.4 Page 26: Linear Independence\nVectors \\(x_1, \\dots, x_p\\) are Linearly Independent (LIN) if: \\[\n\\sum c_i x_i = 0 \\implies c_i = 0 \\quad \\forall i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-27-rank",
    "href": "lec2-vecspace.html#page-27-rank",
    "title": "3  Vector Space and Projection",
    "section": "5.5 Page 27: Rank",
    "text": "5.5 Page 27: Rank\n\\(\\text{Rank}(X)\\) is the number of linearly independent columns (or rows). \\[\n\\text{Rank}(X) = \\text{Dim}(C(X))\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-28-29-rank-properties",
    "href": "lec2-vecspace.html#page-28-29-rank-properties",
    "title": "3  Vector Space and Projection",
    "section": "5.6 Page 28-29: Rank Properties",
    "text": "5.6 Page 28-29: Rank Properties\n\n\\(\\text{Rank}(X) = \\text{Rank}(X')\\) (Row rank = Column rank).\n\\(\\text{Rank}(X) \\le \\min(n, p)\\).\n\nProof of Row Rank = Column Rank involves expressing columns as combinations of a basis and showing the transpose preserves the dimension relation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-32-orthogonality-to-subspace",
    "href": "lec2-vecspace.html#page-32-orthogonality-to-subspace",
    "title": "3  Vector Space and Projection",
    "section": "5.7 Page 32: Orthogonality to Subspace",
    "text": "5.7 Page 32: Orthogonality to Subspace\n\\(y \\perp V\\) if \\(y \\perp x\\) for all \\(x \\in V\\). Orthogonal Complement (\\(V^\\perp\\)): The set of all vectors orthogonal to \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-33-kernel-and-image",
    "href": "lec2-vecspace.html#page-33-kernel-and-image",
    "title": "3  Vector Space and Projection",
    "section": "5.8 Page 33: Kernel and Image",
    "text": "5.8 Page 33: Kernel and Image\n\nImage: \\(\\text{Im}(X) = C(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\\).\nKernel (Null Space): \\(\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\\).\nRelation: \\(\\text{Ker}(X) = [Row(X)]^\\perp\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-34-nullity-theorem",
    "href": "lec2-vecspace.html#page-34-nullity-theorem",
    "title": "3  Vector Space and Projection",
    "section": "5.9 Page 34: Nullity Theorem",
    "text": "5.9 Page 34: Nullity Theorem\n\\[\n\\text{Nullity}(X) + \\text{Rank}(X) = p\n\\] where \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-38-40-rank-inequalities",
    "href": "lec2-vecspace.html#page-38-40-rank-inequalities",
    "title": "3  Vector Space and Projection",
    "section": "5.10 Page 38-40: Rank Inequalities",
    "text": "5.10 Page 38-40: Rank Inequalities\n\n\\(\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\\).\nIf \\(A\\) is invertible (\\(n \\times n\\)), \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-44-rank-of-xx",
    "href": "lec2-vecspace.html#page-44-rank-of-xx",
    "title": "3  Vector Space and Projection",
    "section": "5.11 Page 44: Rank of \\(X'X\\)",
    "text": "5.11 Page 44: Rank of \\(X'X\\)\n\\[\n\\text{Rank}(XX') = \\text{Rank}(X'X) = \\text{Rank}(X)\n\\] Furthermore, \\(C(XX') = C(X)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-48-projection-concept",
    "href": "lec2-vecspace.html#page-48-projection-concept",
    "title": "3  Vector Space and Projection",
    "section": "6.1 Page 48: Projection Concept",
    "text": "6.1 Page 48: Projection Concept\nWe want \\(\\hat{y} \\in L(X)\\) such that \\((y - \\hat{y}) \\perp L(X)\\). If we have an orthonormal basis \\(q_1, \\dots, q_k\\): \\[\n\\hat{y} = \\sum \\langle q_i, y \\rangle q_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-51-projection-theorem",
    "href": "lec2-vecspace.html#page-51-projection-theorem",
    "title": "3  Vector Space and Projection",
    "section": "6.2 Page 51: Projection Theorem",
    "text": "6.2 Page 51: Projection Theorem\nIf \\(U = L(x_1, \\dots, x_p)\\), \\(\\hat{y} = \\text{proj}(y|U)\\) is unique such that \\((y - \\hat{y}) \\perp x_i\\) for all \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-55-matrix-form-with-orthonormal-basis",
    "href": "lec2-vecspace.html#page-55-matrix-form-with-orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "6.3 Page 55: Matrix Form with Orthonormal Basis",
    "text": "6.3 Page 55: Matrix Form with Orthonormal Basis\nIf \\(Q = (q_1, \\dots, q_k)\\) is an orthonormal basis for \\(V\\): \\[\n\\text{proj}(y|V) = \\sum (q_i q_i') y = (Q Q') y\n\\] Here \\(Q'Q = I_k\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-58-example-anova",
    "href": "lec2-vecspace.html#page-58-example-anova",
    "title": "3  Vector Space and Projection",
    "section": "6.4 Page 58: Example (ANOVA)",
    "text": "6.4 Page 58: Example (ANOVA)\n\\(y_{ij} = \\mu_i + \\epsilon_{ij}\\). Projection onto groups involves indicators \\(x_1, x_2, x_3\\). Since group indicators are orthogonal: \\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\] Result: Group means.\n\n\n\nANOVA Projection",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-65-gram-schmidt-qr-factorization",
    "href": "lec2-vecspace.html#page-65-gram-schmidt-qr-factorization",
    "title": "3  Vector Space and Projection",
    "section": "6.5 Page 65: Gram-Schmidt / QR Factorization",
    "text": "6.5 Page 65: Gram-Schmidt / QR Factorization\nAny matrix \\(X\\) can be decomposed as \\(X = QR\\), where \\(Q\\) has orthonormal columns and \\(R\\) is upper triangular. This allows constructing an orthonormal basis from arbitrary columns.\n\n\n\nGram Schmidt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-68-normal-equations",
    "href": "lec2-vecspace.html#page-68-normal-equations",
    "title": "3  Vector Space and Projection",
    "section": "7.1 Page 68: Normal Equations",
    "text": "7.1 Page 68: Normal Equations\nTo find \\(\\hat{y} = X\\beta\\) such that \\(y - X\\beta \\perp C(X)\\): \\[\nX'(y - X\\beta) = 0 \\implies X'X\\beta = X'y\n\\] \\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-69-the-matrix-p",
    "href": "lec2-vecspace.html#page-69-the-matrix-p",
    "title": "3  Vector Space and Projection",
    "section": "7.2 Page 69: The Matrix \\(P\\)",
    "text": "7.2 Page 69: The Matrix \\(P\\)\n\\[\n\\hat{y} = X(X'X)^{-1}X'y = Py\n\\] Relationship with QR: If \\(X=QR\\), then \\(P = QQ'\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-72-74-properties-of-p",
    "href": "lec2-vecspace.html#page-72-74-properties-of-p",
    "title": "3  Vector Space and Projection",
    "section": "7.3 Page 72-74: Properties of \\(P\\)",
    "text": "7.3 Page 72-74: Properties of \\(P\\)\n\\(P\\) is a projection matrix onto \\(C(P)\\) if and only if: 1. Symmetric: \\(P' = P\\) 2. Idempotent: \\(P^2 = P\\)\nProof of Idempotence: If \\(\\hat{y}\\) is already in the space, projecting it again shouldn’t change it. \\(P(Py) = Py \\implies P^2 = P\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-76-projection-onto-complement",
    "href": "lec2-vecspace.html#page-76-projection-onto-complement",
    "title": "3  Vector Space and Projection",
    "section": "7.4 Page 76: Projection onto Complement",
    "text": "7.4 Page 76: Projection onto Complement\nIf \\(P\\) projects onto \\(C(P)\\), then \\(I - P\\) projects onto \\(C(P)^\\perp\\). Properties of \\(M = I - P\\): 1. Symmetric. 2. Idempotent (\\(M^2 = M\\)). 3. \\(PM = P(I-P) = P - P^2 = 0\\) (Orthogonal).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-80-nested-models",
    "href": "lec2-vecspace.html#page-80-nested-models",
    "title": "3  Vector Space and Projection",
    "section": "8.1 Page 80: Nested Models",
    "text": "8.1 Page 80: Nested Models\n\\(H_0: y \\in C(X_1)\\) vs \\(H_1: y \\in C([X_1, X_2])\\). \\[\nC(X_1) \\subseteq C([X_1, X_2])\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-81-projection-composition",
    "href": "lec2-vecspace.html#page-81-projection-composition",
    "title": "3  Vector Space and Projection",
    "section": "8.2 Page 81: Projection Composition",
    "text": "8.2 Page 81: Projection Composition\nIf \\(C(P_0) \\subseteq C(P_1)\\): \\[\nP_1 P_0 = P_0 P_1 = P_0\n\\] Projecting onto the smaller space is the same regardless of whether you project onto the larger space first.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-82-difference-of-projections",
    "href": "lec2-vecspace.html#page-82-difference-of-projections",
    "title": "3  Vector Space and Projection",
    "section": "8.3 Page 82: Difference of Projections",
    "text": "8.3 Page 82: Difference of Projections\n\\(P_1 - P_0\\) is a projection matrix onto \\(C(P_1) \\cap C(P_0)^\\perp\\). This represents the “extra” information in the full model orthogonal to the reduced model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-86-decomposition-of-sum-of-squares",
    "href": "lec2-vecspace.html#page-86-decomposition-of-sum-of-squares",
    "title": "3  Vector Space and Projection",
    "section": "8.4 Page 86: Decomposition of Sum of Squares",
    "text": "8.4 Page 86: Decomposition of Sum of Squares\n\\[\ny = \\hat{y}_0 + (\\hat{y}_1 - \\hat{y}_0) + (y - \\hat{y}_1)\n\\] Squaring norms: \\[\n||y||^2 = ||\\hat{y}_0||^2 + ||\\hat{y}_1 - \\hat{y}_0||^2 + ||y - \\hat{y}_1||^2\n\\] This is the basis for ANOVA Sum of Squares.\n\n\n\nNested Subspaces Figure",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-90-anova-sum-of-squares",
    "href": "lec2-vecspace.html#page-90-anova-sum-of-squares",
    "title": "3  Vector Space and Projection",
    "section": "8.5 Page 90: ANOVA Sum of Squares",
    "text": "8.5 Page 90: ANOVA Sum of Squares\n\n\\(RSS_0 = ||y - \\hat{y}_0||^2\\)\n\\(RSS_1 = ||y - \\hat{y}_1||^2\\) (SS within groups)\n\\(RSS_0 - RSS_1 = ||\\hat{y}_1 - \\hat{y}_0||^2\\) (SS between groups)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-1-introduction",
    "href": "lec2-vecspace.html#page-1-introduction",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.1 Page 1: Introduction",
    "text": "4.1 Page 1: Introduction\nTitle: Lecture Notes for Theory of Linear Models\nTopic: Vector Space and Projection\nInstructor: Longhai Li, Department of Mathematics and Statistics, University of Saskatchewan.\nThis lecture introduces the fundamental geometric and algebraic concepts required for understanding linear models, specifically focusing on vector spaces and projections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-2-lecture-outline",
    "href": "lec2-vecspace.html#page-2-lecture-outline",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.2 Page 2: Lecture Outline",
    "text": "4.2 Page 2: Lecture Outline\nThis lecture is structured around the following key topics:\n\nVector and Geometry: Understanding vectors as geometric objects.\nInner Product and Perpendicularity: Defining orthogonality.\nProjection to a Single Vector: The geometric basis for least squares.\nPythagorean Theory: Extensions to vector spaces.\nShortest Distance Property: Why projection minimizes error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-3-vectors-and-operations",
    "href": "lec2-vecspace.html#page-3-vectors-and-operations",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.3 Page 3: Vectors and Operations",
    "text": "4.3 Page 3: Vectors and Operations\nA vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\nVector Arithmetic: Vectors can be manipulated geometrically:\n\nAddition: The sum \\(x + y\\) is calculated component-wise. Geometrically, this follows the “parallelogram rule” or “head-to-tail” method. \\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\nSubtraction: The difference \\(d = y - x\\) represents the vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-6-derivation-of-the-inner-product",
    "href": "lec2-vecspace.html#page-6-derivation-of-the-inner-product",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.6 Page 6: Derivation of the Inner Product",
    "text": "4.6 Page 6: Derivation of the Inner Product\nWe can expand the algebraic expression for the squared distance \\(||y - x||^2\\):\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\] \\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\] \\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the Law of Cosines derived on Page 5, we can identify the interaction term. This allows us to define the Inner Product:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\nThus, the relationship is: \\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-7-coordinate-scalar-projection",
    "href": "lec2-vecspace.html#page-7-coordinate-scalar-projection",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.7 Page 7: Coordinate (Scalar) Projection",
    "text": "4.7 Page 7: Coordinate (Scalar) Projection\nThe inner product allows us to calculate projections. If we rearrange the cosine formula, we can find the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of the projection is: \\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector of \\(x\\): \\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-8-vector-projection-formula",
    "href": "lec2-vecspace.html#page-8-vector-projection-formula",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.8 Page 8: Vector Projection Formula",
    "text": "4.8 Page 8: Vector Projection Formula\nTo get the actual projection vector (not just the length), we multiply the scalar length (from Page 7) by the direction unit vector \\(\\frac{x}{||x||}\\).\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\] \\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly as: \\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-9-perpendicularity-orthogonality",
    "href": "lec2-vecspace.html#page-9-perpendicularity-orthogonality",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.9 Page 9: Perpendicularity (Orthogonality)",
    "text": "4.9 Page 9: Perpendicularity (Orthogonality)\nTwo vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality is simply: \\[\nx'y = 0 \\iff x \\perp y\n\\]\nExample: If \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\), then: \\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\] Therefore, these vectors are orthogonal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#page-10-projection-onto-a-line-subspace",
    "href": "lec2-vecspace.html#page-10-projection-onto-a-line-subspace",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.10 Page 10: Projection onto a Line (Subspace)",
    "text": "4.10 Page 10: Projection onto a Line (Subspace)\nWe define the line spanned by vector \\(x\\) as \\(L(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\\). The projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that the error vector (residual) is orthogonal to the line.\nDefinition: 1. \\(\\hat{y}\\) lies on the line \\(L(x)\\) (so \\(\\hat{y} = cx\\) for some scalar \\(c\\)). 2. The residual \\((y - \\hat{y})\\) is perpendicular to \\(x\\).\nDerivation: \\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\] \\[\nx'y - c(x'x) = 0\n\\] Solving for \\(c\\): \\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived on Page 8 using the least squares/orthogonal principle.\n\n\n\nProjection Definition Diagram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#introduction",
    "href": "lec2-vecspace.html#introduction",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nTitle: Lecture Notes for Theory of Linear Models\nTopic: Vector Space and Projection\nInstructor: Longhai Li, Department of Mathematics and Statistics, University of Saskatchewan.\nThis lecture introduces the fundamental geometric and algebraic concepts required for understanding linear models, specifically focusing on vector spaces and projections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#lecture-outline",
    "href": "lec2-vecspace.html#lecture-outline",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.2 Lecture Outline",
    "text": "4.2 Lecture Outline\nThis lecture is structured around the following key topics:\n\nVector and Geometry: Understanding vectors as geometric objects.\nInner Product and Perpendicularity: Defining orthogonality.\nProjection to a Single Vector: The geometric basis for least squares.\nPythagorean Theory: Extensions to vector spaces.\nShortest Distance Property: Why projection minimizes error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vectors-and-operations",
    "href": "lec2-vecspace.html#vectors-and-operations",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.3 Vectors and Operations",
    "text": "4.3 Vectors and Operations\n\n\n\n\n\n\nDefinition: Vector\n\n\n\nA vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\nVector Arithmetic: Vectors can be manipulated geometrically:\n\nAddition: The sum \\(x + y\\) is calculated component-wise. Geometrically, this follows the “parallelogram rule” or “head-to-tail” method.\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\nSubtraction: The difference \\(d = y - x\\) represents the vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#scalar-multiplication-and-length",
    "href": "lec2-vecspace.html#scalar-multiplication-and-length",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.4 Scalar Multiplication and Length",
    "text": "4.4 Scalar Multiplication and Length\nScalar Multiplication: Multiplying a vector by a scalar \\(c\\) scales its magnitude without changing its line of direction (though it may reverse direction if \\(c &lt; 0\\)).\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\n\n\n\nDefinition: Euclidean Distance (Length)\n\n\n\nThe length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point \\(x\\). It is defined as:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\n\nScalar Multiplication and Length",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#angle-and-inner-product",
    "href": "lec2-vecspace.html#angle-and-inner-product",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.5 Angle and Inner Product",
    "text": "4.5 Angle and Inner Product\nTo understand the geometry between two vectors \\(x\\) and \\(y\\), we consider the triangle formed by \\(x\\), \\(y\\), and their difference \\(y-x\\). By applying the Law of Cosines to this triangle:\n\n\n\n\n\n\nTheorem: Law of Cosines\n\n\n\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\nTranslating this into vector notation where side lengths are norms:\n\\[\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n\\]\nThis equation links the geometric angle \\(\\theta\\) to the algebraic norms of the vectors.\n\n\n\nGeometry of Inner Product",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#derivation-of-inner-product",
    "href": "lec2-vecspace.html#derivation-of-inner-product",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.6 Derivation of Inner Product",
    "text": "4.6 Derivation of Inner Product\nWe can expand the algebraic expression for the squared distance \\(||y - x||^2\\):\n\\[\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n\\]\n\\[\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n\\]\n\\[\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n\\]\nBy comparing this expanded form with the Law of Cosines derived previously, we can identify the interaction term. This allows us to define the Inner Product:\n\n\n\n\n\n\nDefinition: Inner Product\n\n\n\nThe inner product of two vectors \\(x\\) and \\(y\\) is defined as:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\nThus, the relationship is:\n\\[\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#coordinate-scalar-projection",
    "href": "lec2-vecspace.html#coordinate-scalar-projection",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.7 Coordinate (Scalar) Projection",
    "text": "4.7 Coordinate (Scalar) Projection\nThe inner product allows us to calculate projections. If we rearrange the cosine formula, we can find the length of the “shadow” cast by vector \\(y\\) onto vector \\(x\\).\nThe length of the projection is:\n\\[\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n\\]\nThis can be interpreted as the inner product of \\(y\\) with the normalized (unit) vector of \\(x\\):\n\\[\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-projection-formula",
    "href": "lec2-vecspace.html#vector-projection-formula",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.8 Vector Projection Formula",
    "text": "4.8 Vector Projection Formula\nTo get the actual projection vector (not just the length), we multiply the scalar length (from the previous section) by the direction unit vector \\(\\frac{x}{||x||}\\).\n\n\n\n\n\n\nFormula: Vector Projection\n\n\n\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly as:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#perpendicularity-orthogonality",
    "href": "lec2-vecspace.html#perpendicularity-orthogonality",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.9 Perpendicularity (Orthogonality)",
    "text": "4.9 Perpendicularity (Orthogonality)\n\n\n\n\n\n\nDefinition: Perpendicularity\n\n\n\nTwo vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality is simply:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\n\n\n\n\n\n\nExample\n\n\n\nIf \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\), then:\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nTherefore, these vectors are orthogonal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-onto-a-line-subspace",
    "href": "lec2-vecspace.html#projection-onto-a-line-subspace",
    "title": "3  Vector Space and Projection (Pages 1-10)",
    "section": "4.10 Projection onto a Line (Subspace)",
    "text": "4.10 Projection onto a Line (Subspace)\nWe define the line spanned by vector \\(x\\) as \\(L(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\\). The projection of \\(y\\) onto \\(L(x)\\), denoted \\(\\hat{y}\\), is defined by the geometric property that the error vector (residual) is orthogonal to the line.\n\n\n\n\n\n\nDefinition: Projection onto a Line\n\n\n\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (so \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual \\((y - \\hat{y})\\) is perpendicular to \\(x\\).\n:::\n\nDerivation:\n\\[\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n\\]\n\\[\nx'y - c(x'x) = 0\n\\]\nSolving for \\(c\\):\n\\[\nc = \\frac{x'y}{||x||^2}\n\\]\nThis confirms the formula derived previously using the least squares/orthogonal principle.\n\n\n\nProjection Definition Diagram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection (Pages 1-10)</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-projection",
    "href": "lec2-vecspace.html#vector-and-projection",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Shortest Distance Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nShortest Distance Property\n\n\n\n\n\nDefinition 3.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\n\n\n\n\nDefinition 3.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\n\n\n\nSpanned Vector Space\n\n\n\n\n\nDefinition 3.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(C(X)\\), is the vector space spanned by its columns:\n\\[\nC(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 3.16 (Row Space) The Row Space, denoted \\(Row(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\n\n\n\n\nDefinition 3.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.\n\n\nDefinition 3.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(C(X))\n\\]\n\n\n\n\nTheorem 3.4 (Properties of Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(C(X)) = \\text{Dim}(Row(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\n\n\n\nDefinition 3.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 3.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n\nOrthogonal Complement\n\n\n\n\n\nDefinition 3.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = C(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 3.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [Row(X)]^\\perp\n\\]\n\n\n\n\nTheorem 3.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\]\nWhere \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\n\n\n\nTheorem 3.7 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\n\n\n\n\n\nRank of Matrix Product\n\n\n\n\n\nTheorem 3.8 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\n\n\n\n\nRank Preservation with Invertible Matrices\n\n\n\n\n\n\nTheorem 3.9 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\n\n\n\n\nRank of Gram Matrix\n\n\n\n\n\nTheorem 3.10 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\nC(XX') = C(X)\n\\]\n\n\n\n\n\nColumn Space Equivalence\n\n\n\n\n\n\nDefinition 3.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\n\n\n\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\n\n\n\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\n\n\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\n\n\n\n\n\nTheorem 3.12 (Projection Matrix (Orthonormal)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\n\n\n\n\nTheorem 3.13 (Properties of Projection Matrices) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\n\n\n\n\nExample 3.4 (ANOVA as Projection) Consider a one-way ANOVA model: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i\\) represents the group and \\(j\\) represents the observation within the group.\nWe can define dummy variables (indicators) for each group. Let \\(x_1\\) be the indicator for group 1, \\(x_2\\) for group 2, etc. These vectors are mutually orthogonal because an observation cannot belong to two groups simultaneously.\nThe projection of the data vector \\(y\\) onto the space spanned by these indicators is the sum of the projections onto each group vector:\n\\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\]\nSince the indicators are orthogonal, this simplifies to calculating the mean for each group. The fitted value for any observation \\(y_{ij}\\) is simply the group mean \\(\\bar{y}_{i.}\\).\n\n\n\n\nANOVA Projection Geometry\n\n\n\n\n\nDefinition 3.25 (Projection onto Complement) The matrix \\(M = I - P\\) is the projection matrix onto the orthogonal complement \\(C(X)^\\perp\\).\nProperties of \\(M\\): 1. Idempotent: \\(M^2 = (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P = M\\). 2. Symmetric: \\(M' = (I-P)' = I - P' = I - P = M\\). 3. Orthogonal to \\(P\\): \\(PM = P(I-P) = P - P^2 = 0\\).\n\n\n\n\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\n\n\n\n\nGram-Schmidt Process\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3.14 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\nNormal Equations Derivation\n\n\n\n\n\nDefinition 3.26 (General Projection Matrix) The projection of \\(y\\) onto \\(C(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the projection matrix \\(P\\) is defined as:\n\\[\nP = X(X'X)^{-1}X'\n\\]\n\n\n\n\n\n\n\n\n\nTheorem 3.15 (Properties of P) The matrix \\(P = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(P' = P\\)\nIdempotent: \\(P^2 = P\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(P) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\n\n\n\nDefinition 3.27 (Residual Maker Matrix M) \\[\nM = I - X(X'X)^{-1}X'\n\\]\nThis matrix projects \\(y\\) onto the null space of \\(X'\\) (the orthogonal complement of the column space of \\(X\\)).\n\n\n\n\n\nDefinition 3.28 (Nested Models) Consider two models: 1. Reduced Model (\\(M_0\\)): \\(y \\in C(X_0)\\) 2. Full Model (\\(M_1\\)): \\(y \\in C(X_1)\\)\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\nC(X_0) \\subseteq C(X_1)\n\\]\n\n\n\n\n\nNested Models Concept\n\n\n\n\n\nTheorem 3.16 (Composition of Projections) If \\(C(P_0) \\subseteq C(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\n\n\n\nTheorem 3.17 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(C(X_1) \\cap C(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\n\n\n\n\\(\\hat{y}_0\\) (The fit of the reduced model)\n\\(\\hat{y}_1 - \\hat{y}_0\\) (The improvement from the reduced to the full model)\n\\(y - \\hat{y}_1\\) (The residual of the full model)\n\n\n\n\n\n\n\n\nDecomposition of Sum of Squares\n\n\n\n\n\n\nDefinition 3.29 (ANOVA Decomposition) The Total Sum of Squares (TSS), or the squared norm of \\(y\\) (often centered), can be split into components:\n\nResidual Sum of Squares (Full Model): \\[\nRSS_1 = ||y - \\hat{y}_1||^2 = \\sum_{i}\\sum_{j} (y_{ij} - \\bar{y}_{i.})^2\n\\] This represents the “Within Group” sum of squares.\nDifference Sum of Squares: \\[\nRSS_0 - RSS_1 = ||\\hat{y}_1 - \\hat{y}_0||^2 = \\sum_{i} n_i (\\bar{y}_{i.} - \\bar{y}_{..})^2\n\\] This represents the “Between Group” sum of squares (\\(SS_{between}\\)).\n\n\n\n\n\n\nANOVA Sum of Squares Derivation\n\n\n\n\n\nTheorem 3.18 (Orthogonal Decomposition) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = x_1 + x_2 + \\dots + x_k\n\\] where \\(x_i \\in V_i\\).\nFurthermore, each component \\(x_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\nx_i = P_i y\n\\]\n\n\n\n\n\nOrthogonal Space Decomposition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#gram-schmidt-process",
    "href": "lec2-vecspace.html#gram-schmidt-process",
    "title": "3  Vector Space and Projection",
    "section": "3.2 Gram-Schmidt Process",
    "text": "3.2 Gram-Schmidt Process\nGiven linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-space-and-projection-in-a-line",
    "href": "lec2-vecspace.html#vector-space-and-projection-in-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Shortest Distance Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nShortest Distance Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#general-vector-space",
    "href": "lec2-vecspace.html#general-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.2 General Vector Space",
    "text": "3.2 General Vector Space\nWe now generalize our discussion from lines to broader spaces.\n\nDefinition 3.13 (Vector Space) A set \\(V \\subseteq \\mathbb{R}^n\\) is called a Vector Space if it is closed under vector addition and scalar multiplication:\n\nClosed under Addition: If \\(x_1 \\in V\\) and \\(x_2 \\in V\\), then \\(x_1 + x_2 \\in V\\).\nClosed under Scalar Multiplication: If \\(x \\in V\\), then \\(cx \\in V\\) for any scalar \\(c \\in \\mathbb{R}\\).\n\n\nIt follows that the zero vector \\(0\\) must belong to any subspace (by choosing \\(c=0\\)).\nSpanned Vector Space\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\nDefinition 3.14 (Spanned Vector Space) Let \\(x_1, \\dots, x_p\\) be a set of vectors in \\(\\mathbb{R}^n\\). The space spanned by these vectors, denoted \\(L(x_1, \\dots, x_p)\\), is the set of all possible linear combinations of them:\n\\[\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n\\]\n\n\n\n\nSpanned Vector Space\n\n\nColumn Space and Row Space\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\nDefinition 3.15 (Column Space) For a matrix \\(X = (x_1, \\dots, x_p)\\), the Column Space, denoted \\(Col(X)\\), is the vector space spanned by its columns:\n\\[\nCol(X) = L(x_1, \\dots, x_p)\n\\]\n\n\nDefinition 3.16 (Row Space) The Row Space, denoted \\(Row(X)\\), is the vector space spanned by the rows of the matrix \\(X\\).\n\n\n\n\nLinear Independence and Rank\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\nDefinition 3.17 (Linear Independence) A set of vectors \\(x_1, \\dots, x_p\\) is said to be Linearly Independent if the only solution to the linear combination equation equal to zero is the trivial solution:\n\\[\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n\\]\nIf there exist non-zero \\(c_i\\)’s such that sum is zero, the vectors are Linearly Dependent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#rank-of-matrices-and-vector-space",
    "href": "lec2-vecspace.html#rank-of-matrices-and-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.3 Rank of Matrices and Vector Space",
    "text": "3.3 Rank of Matrices and Vector Space\n\nDefinition 3.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(C(X))\n\\]\n\n\n3.3.0.1 Properties of Rank\nThere are several fundamental properties regarding the rank of a matrix.\n\nTheorem 3.4 (Properties of Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(C(X)) = \\text{Dim}(Row(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\nOrthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 3.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 3.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n\nOrthogonal Complement\n\n\nKernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 3.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = C(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 3.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [Row(X)]^\\perp\n\\]\n\nNullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 3.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\]\nWhere \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\nRank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 3.7 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\nProof: The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\nC(XZ) \\subseteq C(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\nRow(XZ) \\subseteq Row(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\nCombining these gives the result.\n\n\n\nRank of Matrix Product\n\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 3.8 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\nProof: From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\n\n\nRank Preservation with Invertible Matrices\n\n\n\nRank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 3.9 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\nProof Strategy: We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\n\n\nRank of Gram Matrix\n\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 3.10 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\nC(XX') = C(X)\n\\]\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(C(X)\\) lies in the same space as the projection onto \\(C(XX')\\). This is vital for the existence of solutions in generalized least squares.\n\n\n\nColumn Space Equivalence\n\n\n\nBasis and Dimension\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 3.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#orthonormal-basis",
    "href": "lec2-vecspace.html#orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "3.4 Orthonormal Basis",
    "text": "3.4 Orthonormal Basis\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-onto-a-subspace",
    "href": "lec2-vecspace.html#projection-onto-a-subspace",
    "title": "3  Vector Space and Projection",
    "section": "3.5 Projection onto a Subspace",
    "text": "3.5 Projection onto a Subspace\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\nThe Projection Theorem\nThis theorem establishes the existence and uniqueness of the projection vector for any subspace, regardless of the basis used.\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\): * \\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)). * \\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-matrices",
    "href": "lec2-vecspace.html#projection-matrices",
    "title": "3  Vector Space and Projection",
    "section": "3.5 Projection Matrices",
    "text": "3.5 Projection Matrices\n\nTheorem 3.12 (Projection Matrix (Orthonormal)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 3.13 (Properties of Projection Matrices) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\nProof of Idempotence: If \\(\\hat{y} = Py\\) is already in the subspace \\(C(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 3.4 (ANOVA as Projection) Consider a one-way ANOVA model: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i\\) represents the group and \\(j\\) represents the observation within the group.\nWe can define dummy variables (indicators) for each group. Let \\(x_1\\) be the indicator for group 1, \\(x_2\\) for group 2, etc. These vectors are mutually orthogonal because an observation cannot belong to two groups simultaneously.\nThe projection of the data vector \\(y\\) onto the space spanned by these indicators is the sum of the projections onto each group vector:\n\\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\]\nSince the indicators are orthogonal, this simplifies to calculating the mean for each group. The fitted value for any observation \\(y_{ij}\\) is simply the group mean \\(\\bar{y}_{i.}\\).\n\n\n\n\nANOVA Projection Geometry\n\n\nProjection onto Orthogonal Complement\nIf \\(P\\) is the projection matrix onto a subspace \\(V\\), we can easily define the projection onto the orthogonal complement \\(V^\\perp\\) (the “error” space).\n\nDefinition 3.25 (Projection onto Complement) The matrix \\(M = I - P\\) is the projection matrix onto the orthogonal complement \\(C(X)^\\perp\\).\nProperties of \\(M\\): 1. Idempotent: \\(M^2 = (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P = M\\). 2. Symmetric: \\(M' = (I-P)' = I - P' = I - P = M\\). 3. Orthogonal to \\(P\\): \\(PM = P(I-P) = P - P^2 = 0\\).\n\nThis matrix \\(M\\) produces the residuals: \\(e = My = (I-P)y = y - \\hat{y}\\).\nGram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.\n\n\n\nGram-Schmidt Process\n\n\n\nProjection Matrix Definition\nWe now formally derive the projection matrix \\(P\\) for the general case where we project \\(y\\) onto the column space of a matrix \\(X\\).\nNormal Equations\nWe want to find \\(\\hat{y} = X\\beta\\) such that the residual \\((y - \\hat{y})\\) is orthogonal to the column space \\(C(X)\\). This means the residual must be orthogonal to every column of \\(X\\).\n\\[\nX'(y - X\\beta) = 0\n\\]\nExpanding this gives the famous Normal Equations:\n\\[\nX'y - X'X\\beta = 0 \\implies X'X\\beta = X'y\n\\]\n\nTheorem 3.14 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\nNormal Equations Derivation\n\n\nThe Matrix \\(P\\)\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 3.26 (General Projection Matrix) The projection of \\(y\\) onto \\(C(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the projection matrix \\(P\\) is defined as:\n\\[\nP = X(X'X)^{-1}X'\n\\]\n\nRelationship with QR Decomposition\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(C(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\nP = QR((QR)'(QR))^{-1}(QR)'\n\\] \\[\n= QR(R'Q'QR)^{-1}R'Q'\n\\] \\[\n= QR(R'R)^{-1}R'Q'\n\\] \\[\n= QR R^{-1} (R')^{-1} R' Q'\n\\] \\[\n= Q Q'\n\\]\nThis confirms that \\(P = QQ'\\) is consistent with the general formula \\(P = X(X'X)^{-1}X'\\).\nProperties of \\(P\\)\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 3.15 (Properties of P) The matrix \\(P = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(P' = P\\)\nIdempotent: \\(P^2 = P\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(P) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\nProjection onto Complement\nAs before, the projection onto the orthogonal complement (the residual maker matrix) is \\(M = I - P\\).\n\nDefinition 3.27 (Residual Maker Matrix M) \\[\nM = I - X(X'X)^{-1}X'\n\\]\nThis matrix projects \\(y\\) onto the null space of \\(X'\\) (the orthogonal complement of the column space of \\(X\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "href": "lec2-vecspace.html#projections-onto-nested-subspaces",
    "title": "3  Vector Space and Projection",
    "section": "3.6 Projections onto Nested Subspaces",
    "text": "3.6 Projections onto Nested Subspaces\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\nDefinition 3.28 (Nested Models) Consider two models: 1. Reduced Model (\\(M_0\\)): \\(y \\in Col(X_0)\\) 2. Full Model (\\(M_1\\)): \\(y \\in Col(X_1)\\)\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: \\[\nCol(X_0) \\subseteq Col(X_1)\n\\]\n\nUsually, \\(X_1\\) is constructed by adding columns to \\(X_0\\): \\(X_1 = [X_0, X_{new}]\\).\n\n\n\nNested Models Concept\n\n\nProjection Composition\nLet \\(P_0\\) be the projection matrix onto \\(Col(X_0)\\) and \\(P_1\\) be the projection matrix onto \\(Col(X_1)\\). Since \\(Col(X_0) \\subseteq Col(X_1)\\), we have important relationships between these matrices.\n\nTheorem 3.16 (Composition of Projections) If \\(Col(P_0) \\subseteq Col(P_1)\\), then:\n\n\\(P_1 P_0 = P_0\\) (Projecting onto the small space, then the large space, keeps you in the small space).\n\\(P_0 P_1 = P_0\\) (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n\n\nDifference of Projections\nThe difference between the two projection matrices, \\(P_1 - P_0\\), is itself a projection matrix.\n\nTheorem 3.17 (Difference Projection) The matrix \\(P_{\\Delta} = P_1 - P_0\\) is an orthogonal projection matrix onto the subspace \\(Col(X_1) \\cap Col(X_0)^\\perp\\). This subspace represents the “extra” information in the full model that is orthogonal to the reduced model.\nProperties:\n\nSymmetric: \\((P_1 - P_0)' = P_1 - P_0\\).\nIdempotent: \\((P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0\\).\nOrthogonality: \\((P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0\\).\n\n\nDecomposition of Sum of Squares\nThis geometry allows us to decompose the total vector \\(y\\) into three orthogonal components:\n\n\\(\\hat{y}_0\\) (The fit of the reduced model)\n\\(\\hat{y}_1 - \\hat{y}_0\\) (The improvement from the reduced to the full model)\n\\(y - \\hat{y}_1\\) (The residual of the full model)\n\n\\[\n  y = \\hat{y}_0 + (\\hat{y}_1 - \\hat{y}_0) + (y - \\hat{y}_1)\n  \\]\nSquaring the norms (applying Pythagoras):\n\\[\n  ||y||^2 = ||\\hat{y}_0||^2 + ||\\hat{y}_1 - \\hat{y}_0||^2 + ||y - \\hat{y}_1||^2\n  \\]\nThis equation is the foundation for the F-test in ANOVA and regression.\n\n\n\nDecomposition of Sum of Squares\n\n\nANOVA Sum of Squares\nWe apply the decomposition of sum of squares to the specific case of Analysis of Variance.\n\nDefinition 3.29 (ANOVA Decomposition) The Total Sum of Squares (TSS), or the squared norm of \\(y\\) (often centered), can be split into components:\n\nResidual Sum of Squares (Full Model): \\[\nRSS_1 = ||y - \\hat{y}_1||^2 = \\sum_{i}\\sum_{j} (y_{ij} - \\bar{y}_{i.})^2\n\\] This represents the “Within Group” sum of squares.\nDifference Sum of Squares: \\[\nRSS_0 - RSS_1 = ||\\hat{y}_1 - \\hat{y}_0||^2 = \\sum_{i} n_i (\\bar{y}_{i.} - \\bar{y}_{..})^2\n\\] This represents the “Between Group” sum of squares (\\(SS_{between}\\)).\n\n\nThis relationship confirms that: \\[\nTSS = SS_{within} + SS_{between}\n\\]\n\n\n\nANOVA Sum of Squares Derivation\n\n\nProjections in Orthogonal Spaces\nFinally, we consider the case where the entire space \\(\\mathbb{R}^n\\) is decomposed into mutually orthogonal subspaces.\n\nTheorem 3.18 (Orthogonal Decomposition) If \\(\\mathbb{R}^n\\) is the direct sum of orthogonal subspaces \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n\\] where \\(V_i \\perp V_j\\) for all \\(i \\ne j\\).\nThen any vector \\(y\\) can be uniquely written as: \\[\ny = x_1 + x_2 + \\dots + x_k\n\\] where \\(x_i \\in V_i\\).\nFurthermore, each component \\(x_i\\) is simply the projection of \\(y\\) onto the subspace \\(V_i\\): \\[\nx_i = P_i y\n\\]\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: \\[\nI_n = P_1 + P_2 + \\dots + P_k\n\\]\n\n\n\nOrthogonal Space Decomposition\n\n\nThe following diagram summarizes the relationships between the vector spaces and projections discussed in this lecture.\n\n\n\nSummary Diagram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "href": "lec2-vecspace.html#rank-of-matrices-and-dim-of-vector-space",
    "title": "3  Vector Space and Projection",
    "section": "3.3 Rank of Matrices and Dim of Vector Space",
    "text": "3.3 Rank of Matrices and Dim of Vector Space\n\nDefinition 3.18 (Rank) The Rank of a matrix \\(X\\), denoted \\(\\text{Rank}(X)\\), is the maximum number of linearly independent columns in \\(X\\). This is equivalent to the dimension of the column space:\n\\[\n\\text{Rank}(X) = \\text{Dim}(Col(X))\n\\]\n\n\n3.3.0.1 Properties of Rank\nThere are several fundamental properties regarding the rank of a matrix.\n\nTheorem 3.4 (Properties of Rank)  \n\nRow Rank equals Column Rank: The dimension of the column space is equal to the dimension of the row space. \\[\n\\text{Dim}(Col(X)) = \\text{Dim}(Row(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n\\]\nBounds: For an \\(n \\times p\\) matrix \\(X\\): \\[\n\\text{Rank}(X) \\le \\min(n, p)\n\\]\n\n\n\nProof. Let \\(X\\) be an \\(n \\times p\\) matrix. Let \\(r\\) be the row rank of \\(X\\). This means the dimension of the row space is \\(r\\). Let \\(u_1, \\dots, u_r\\) be a basis for the row space of \\(X\\) (these are row vectors). Since every row of \\(X\\) is in the row space, each row \\(x_{i.}\\) can be written as a linear combination of the basis vectors: \\[\n  x_{i.} = c_{i1}u_1 + c_{i2}u_2 + \\dots + c_{ir}u_r \\quad \\text{for } i=1,\\dots,n\n  \\]\nWe can write this in matrix notation as: \\[\n  X = C U\n  \\] where \\(C\\) is an \\(n \\times r\\) matrix of coefficients \\(c_{ij}\\), and \\(U\\) is an \\(r \\times p\\) matrix with rows \\(u_1, \\dots, u_r\\).\nNow consider the columns of \\(X\\). Since \\(X = CU\\), the columns of \\(X\\) are linear combinations of the columns of \\(C\\). Let \\(c^{(j)}\\) be the \\(j\\)-th column of \\(C\\). The columns of \\(X\\) lie in the space spanned by \\(\\{c^{(1)}, \\dots, c^{(r)}\\}\\). Thus, the column space of \\(X\\), \\(Col(X)\\), is a subspace of the column space of \\(C\\). \\[\n  \\text{Dim}(Col(X)) \\le \\text{Dim}(Col(C)) \\le r\n  \\] The dimension of the column space of \\(C\\) is at most \\(r\\) (since \\(C\\) has only \\(r\\) columns). Therefore, Column Rank \\(\\le\\) Row Rank.\nApplying the same logic to \\(X'\\), we get Row Rank \\(\\le\\) Column Rank. Combining these inequalities gives: Row Rank = Column Rank.\n\n\nExample: 2x3 Matrix\nConsider the following \\(2 \\times 3\\) matrix: \\[\nX = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n\\]\nRow Rank: The rows are \\(r_1 = (1, 0, 1)\\) and \\(r_2 = (0, 1, 1)\\). Neither is a multiple of the other, so they are linearly independent. \\[\n\\text{Row Rank} = 2\n\\]\nColumn Rank: The columns are \\(c_1 = \\binom{1}{0}\\), \\(c_2 = \\binom{0}{1}\\), and \\(c_3 = \\binom{1}{1}\\). Notice that \\(c_3 = c_1 + c_2\\). The third column is dependent on the first two. However, \\(c_1\\) and \\(c_2\\) are independent (standard basis vectors). \\[\n\\text{Column Rank} = 2\n\\]\nThus, Rank(Row) = Rank(Col) = 2.\n\nOrthogonality to a Subspace\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\nDefinition 3.19 (Orthogonality to a Subspace) A vector \\(y\\) is orthogonal to a subspace \\(V\\) (denoted \\(y \\perp V\\)) if \\(y\\) is orthogonal to every vector \\(x\\) in \\(V\\).\n\\[\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n\\]\n\n\nDefinition 3.20 (Orthogonal Complement) The set of all vectors that are orthogonal to a subspace \\(V\\) is called the Orthogonal Complement of \\(V\\), denoted \\(V^\\perp\\).\n\\[\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n\\]\n\n\n\n\nOrthogonal Complement\n\n\nKernel (Null Space) and Image\nFor a matrix transformation defined by \\(X\\), we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\nDefinition 3.21 (Image and Kernel)  \n\nImage (Column Space): The set of all possible outputs. \\[\n\\text{Im}(X) = Col(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n\\]\nKernel (Null Space): The set of all inputs mapped to the zero vector. \\[\n\\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n\\]\n\n\n\nTheorem 3.5 (Relationship between Kernel and Row Space) The kernel of \\(X\\) is the orthogonal complement of the row space of \\(X\\):\n\\[\n\\text{Ker}(X) = [Row(X)]^\\perp\n\\]\n\nNullity Theorem\nThere is a fundamental relationship between the dimensions of these spaces.\n\nTheorem 3.6 (Rank-Nullity Theorem) For an \\(n \\times p\\) matrix \\(X\\):\n\\[\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n\\]\nWhere \\(\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))\\).\n\n\n\n\nRank Inequalities\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\nTheorem 3.7 (Rank of a Matrix Product) Let \\(X\\) be an \\(n \\times p\\) matrix and \\(Z\\) be a \\(p \\times k\\) matrix. The rank of their product \\(XZ\\) is bounded by the rank of the individual matrices:\n\\[\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n\\]\n\nProof: The columns of \\(XZ\\) are linear combinations of the columns of \\(X\\). Thus, the column space of \\(XZ\\) is a subspace of the column space of \\(X\\): \\[\nCol(XZ) \\subseteq Col(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n\\] Similarly, the rows of \\(XZ\\) are linear combinations of the rows of \\(Z\\). Thus, the row space of \\(XZ\\) is a subspace of the row space of \\(Z\\): \\[\nRow(XZ) \\subseteq Row(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n\\]\nCombining these gives the result.\n\n\n\nRank of Matrix Product\n\n\nRank and Invertible Matrices\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\nTheorem 3.8 (Rank with Non-Singular Multiplication) Let \\(A\\) be an \\(n \\times n\\) invertible matrix (i.e., \\(\\text{Rank}(A) = n\\)) and \\(X\\) be an \\(n \\times p\\) matrix. Then:\n\\[\n\\text{Rank}(AX) = \\text{Rank}(X)\n\\]\nSimilarly, if \\(B\\) is a \\(p \\times p\\) invertible matrix, then:\n\\[\n\\text{Rank}(XB) = \\text{Rank}(X)\n\\]\n\nProof: From the previous theorem, we know \\(\\text{Rank}(AX) \\le \\text{Rank}(X)\\). Since \\(A\\) is invertible, we can write \\(X = A^{-1}(AX)\\). Applying the theorem again: \\[\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n\\] Thus, \\(\\text{Rank}(AX) = \\text{Rank}(X)\\).\n\n\n\nRank Preservation with Invertible Matrices\n\n\n–\nRank of \\(X'X\\) and \\(XX'\\)\nThe matrix \\(X'X\\) (the Gram matrix) appears in the normal equations for least squares (\\(X'X\\beta = X'y\\)). Its properties are closely tied to \\(X\\).\n\nTheorem 3.9 (Rank of Gram Matrix) For any real matrix \\(X\\), the rank of \\(X'X\\) and \\(XX'\\) is the same as the rank of \\(X\\) itself:\n\\[\n\\text{Rank}(X'X) = \\text{Rank}(X)\n\\] \\[\n\\text{Rank}(XX') = \\text{Rank}(X)\n\\]\n\nProof Strategy: We first show that the null space (kernel) of \\(X\\) is the same as the null space of \\(X'X\\). If \\(v \\in \\text{Ker}(X)\\), then \\(Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)\\). Conversely, if \\(v \\in \\text{Ker}(X'X)\\), then \\(X'Xv = 0\\). Multiply by \\(v'\\): \\[\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n\\] So \\(\\text{Ker}(X) = \\text{Ker}(X'X)\\). By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\n\n\nRank of Gram Matrix\n\n\nColumn Space of \\(XX'\\)\nBeyond just the rank, the column spaces themselves are related.\n\nTheorem 3.10 (Column Space Equivalence) The column space of \\(XX'\\) is identical to the column space of \\(X\\):\n\\[\nCol(XX') = Col(X)\n\\]\n\nImplication: This property ensures that for any \\(y\\), the projection of \\(y\\) onto \\(Col(X)\\) lies in the same space as the projection onto \\(Col(XX')\\). This is vital for the existence of solutions in generalized least squares.\n\n\n\nColumn Space Equivalence",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-onto-a-subspace-with-orthonormal-basis",
    "href": "lec2-vecspace.html#projection-onto-a-subspace-with-orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "3.4 Projection onto a Subspace with Orthonormal Basis",
    "text": "3.4 Projection onto a Subspace with Orthonormal Basis\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\nThe Projection Theorem\nThis theorem establishes the existence and uniqueness of the projection vector for any subspace, regardless of the basis used.\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\): * \\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)). * \\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-projection-matrices",
    "href": "lec2-vecspace.html#projection-via-projection-matrices",
    "title": "3  Vector Space and Projection",
    "section": "3.5 Projection via Projection Matrices",
    "text": "3.5 Projection via Projection Matrices\n\nTheorem 3.12 (Projection Matrix (Orthonormal)) The projection \\(\\hat{y}\\) can be written as:\n\\[\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n\\]\nThus, the projection matrix \\(P\\) onto the subspace \\(V\\) is: \\[\nP = QQ'\n\\]\n\n\nProperties of Projection Matrices\nWe have defined the projection matrix as \\(P = X(X'X)^{-1}X'\\) (or \\(P=QQ'\\) for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\nTheorem 3.13 (Properties of Projection Matrices) A square matrix \\(P\\) represents an orthogonal projection onto some subspace if and only if it satisfies:\n\nIdempotence: \\(P^2 = P\\) (Applying the projection twice is the same as applying it once).\nSymmetry: \\(P' = P\\).\n\n\nProof of Idempotence: If \\(\\hat{y} = Py\\) is already in the subspace \\(Col(X)\\), then projecting it again should not change it. \\[\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n\\] Thus, \\(P^2 = P\\).\nExample: ANOVA (Analysis of Variance)\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\nExample 3.4 (ANOVA as Projection) Consider a one-way ANOVA model: \\[\ny_{ij} = \\mu_i + \\epsilon_{ij}\n\\] where \\(i\\) represents the group and \\(j\\) represents the observation within the group.\nWe can define dummy variables (indicators) for each group. Let \\(x_1\\) be the indicator for group 1, \\(x_2\\) for group 2, etc. These vectors are mutually orthogonal because an observation cannot belong to two groups simultaneously.\nThe projection of the data vector \\(y\\) onto the space spanned by these indicators is the sum of the projections onto each group vector:\n\\[\n\\hat{y} = \\text{proj}(y|x_1) + \\text{proj}(y|x_2) + \\dots\n\\]\nSince the indicators are orthogonal, this simplifies to calculating the mean for each group. The fitted value for any observation \\(y_{ij}\\) is simply the group mean \\(\\bar{y}_{i.}\\).\n\n\n\n\nANOVA Projection Geometry\n\n\nProjection onto Orthogonal Complement\nIf \\(P\\) is the projection matrix onto a subspace \\(V\\), we can easily define the projection onto the orthogonal complement \\(V^\\perp\\) (the “error” space).\n\nDefinition 3.25 (Projection onto Complement) The matrix \\(M = I - P\\) is the projection matrix onto the orthogonal complement \\(Col(X)^\\perp\\).\nProperties of \\(M\\): 1. Idempotent: \\(M^2 = (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P = M\\). 2. Symmetric: \\(M' = (I-P)' = I - P' = I - P = M\\). 3. Orthogonal to \\(P\\): \\(PM = P(I-P) = P - P^2 = 0\\).\n\nThis matrix \\(M\\) produces the residuals: \\(e = My = (I-P)y = y - \\hat{y}\\).\nGram-Schmidt Process\nTo use the simplified formula \\(P = QQ'\\), we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\nGram-Schmidt Process Given linearly independent vectors \\(x_1, \\dots, x_p\\):\n\nStep 1: Normalize the first vector. \\[\nq_1 = \\frac{x_1}{||x_1||}\n\\]\nStep 2: Project \\(x_2\\) onto \\(q_1\\) and subtract it to find the orthogonal component. \\[\nv_2 = x_2 - (x_2'q_1)q_1\n\\] Then normalize: \\[\nq_2 = \\frac{v_2}{||v_2||}\n\\]\nStep k: Subtract the projections onto all previous \\(q\\) vectors. \\[\nv_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n\\] \\[\nq_k = \\frac{v_k}{||v_k||}\n\\]\n\n\nThis process leads to the QR Decomposition of a matrix: \\(X = QR\\), where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.\n\n\n\nGram-Schmidt Process\n\n\n\nProjection Matrix Definition\nWe now formally derive the projection matrix \\(P\\) for the general case where we project \\(y\\) onto the column space of a matrix \\(X\\).\nNormal Equations\nWe want to find \\(\\hat{y} = X\\beta\\) such that the residual \\((y - \\hat{y})\\) is orthogonal to the column space \\(Col(X)\\). This means the residual must be orthogonal to every column of \\(X\\).\n\\[\nX'(y - X\\beta) = 0\n\\]\nExpanding this gives the famous Normal Equations:\n\\[\nX'y - X'X\\beta = 0 \\implies X'X\\beta = X'y\n\\]\n\nTheorem 3.14 (Least Squares Estimator) If \\(X'X\\) is invertible (i.e., \\(X\\) has full column rank), the unique solution for \\(\\beta\\) is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\nNormal Equations Derivation\n\n\nThe Matrix \\(P\\)\nSubstituting the estimator \\(\\hat{\\beta}\\) back into the equation for \\(\\hat{y}\\) gives us the projection matrix.\n\nDefinition 3.26 (General Projection Matrix) The projection of \\(y\\) onto \\(Col(X)\\) is given by:\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n\\]\nThus, the projection matrix \\(P\\) is defined as:\n\\[\nP = X(X'X)^{-1}X'\n\\]\n\nRelationship with QR Decomposition\nIf we use the QR decomposition such that \\(X = QR\\), where the columns of \\(Q\\) form an orthonormal basis for \\(Col(X)\\), the formula simplifies significantly.\nRecall that for orthonormal columns, \\(Q'Q = I\\). Substituting \\(X=QR\\) into the general formula:\n\\[\nP = QR((QR)'(QR))^{-1}(QR)'\n\\] \\[\n= QR(R'Q'QR)^{-1}R'Q'\n\\] \\[\n= QR(R'R)^{-1}R'Q'\n\\] \\[\n= QR R^{-1} (R')^{-1} R' Q'\n\\] \\[\n= Q Q'\n\\]\nThis confirms that \\(P = QQ'\\) is consistent with the general formula \\(P = X(X'X)^{-1}X'\\).\nProperties of \\(P\\)\nWe revisit the properties of projection matrices in this general context.\n\nTheorem 3.15 (Properties of P) The matrix \\(P = X(X'X)^{-1}X'\\) satisfies:\n\nSymmetric: \\(P' = P\\)\nIdempotent: \\(P^2 = P\\)\nTrace: The trace of a projection matrix equals the dimension of the subspace it projects onto. \\[\n\\text{tr}(P) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n\\]\n\n\nProjection onto Complement\nAs before, the projection onto the orthogonal complement (the residual maker matrix) is \\(M = I - P\\).\n\nDefinition 3.27 (Residual Maker Matrix M) \\[\nM = I - X(X'X)^{-1}X'\n\\]\nThis matrix projects \\(y\\) onto the null space of \\(X'\\) (the orthogonal complement of the column space of \\(X\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "href": "lec2-vecspace.html#projection-via-orthonormal-basis",
    "title": "3  Vector Space and Projection",
    "section": "3.4 Projection via Orthonormal Basis",
    "text": "3.4 Projection via Orthonormal Basis\nBasis and Dimension\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\nDefinition 3.22 (Basis) A set of vectors \\(\\{x_1, \\dots, x_k\\}\\) is a Basis for a vector space \\(V\\) if:\n\nThe vectors span the space: \\(V = L(x_1, \\dots, x_k)\\).\nThe vectors are linearly independent.\n\n\nThe number of vectors in a basis is unique and is defined as the Dimension of \\(V\\).\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\nDefinition 3.23 (Orthonormal Basis) A basis \\(\\{q_1, \\dots, q_k\\}\\) is called an Orthonormal Basis if:\n\nOrthogonal: Each pair of vectors is perpendicular. \\[\nq_i'q_j = 0 \\quad \\text{for } i \\ne j\n\\]\nNormalized: Each vector has unit length. \\[\n||q_i||^2 = q_i'q_i = 1\n\\]\n\nCombining these, we write \\(q_i'q_j = \\delta_{ij}\\) (Kronecker delta).\n\nWe now generalize the projection problem. Instead of projecting \\(y\\) onto a single line, we project it onto a subspace \\(V\\) of dimension \\(k\\).\nIf we have an orthonormal basis \\(\\{q_1, \\dots, q_k\\}\\) for \\(V\\), the projection \\(\\hat{y}\\) is simply the sum of the projections onto the individual basis vectors.\n\nDefinition 3.24 (Projection Formula (Orthonormal Basis)) The projection of \\(y\\) onto the subspace \\(V = L(q_1, \\dots, q_k)\\) is:\n\\[\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n\\]\nSince the basis vectors are normalized, we do not need to divide by \\(||q_i||^2\\).\n\n\n\n\nProjection onto a Subspace\n\n\nThe Projection Theorem\nThis theorem establishes the existence and uniqueness of the projection vector for any subspace, regardless of the basis used.\n\nTheorem 3.11 (Projection Theorem) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). For any vector \\(y \\in \\mathbb{R}^n\\), there exists a unique vector \\(\\hat{y} \\in V\\) such that the residual is orthogonal to the subspace:\n\\[\n(y - \\hat{y}) \\perp V\n\\]\nEquivalently: \\[\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n\\]\n\nMatrix Form with Orthonormal Basis\nWe can express the summation formula for \\(\\hat{y}\\) compactly using matrix notation.\nLet \\(Q\\) be an \\(n \\times k\\) matrix whose columns are the orthonormal basis vectors \\(q_1, \\dots, q_k\\). \\[\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n\\]\nProperties of \\(Q\\): * \\(Q'Q = I_k\\) (Identity matrix of size \\(k \\times k\\)). * \\(QQ'\\) is not necessarily \\(I_n\\) (unless \\(k=n\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-space-and-projection-to-a-line",
    "href": "lec2-vecspace.html#vector-space-and-projection-to-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Shortest Distance Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nShortest Distance Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-projection-to-a-line",
    "href": "lec2-vecspace.html#vector-and-projection-to-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Shortest Distance Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nShortest Distance Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  },
  {
    "objectID": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "href": "lec2-vecspace.html#vector-and-projection-onto-a-line",
    "title": "3  Vector Space and Projection",
    "section": "",
    "text": "Definition 3.1 (Vector) A vector \\(x\\) is defined as a point in \\(n\\)-dimensional space (\\(\\mathbb{R}^n\\)). It is typically represented as a column vector containing \\(n\\) real-valued components:\n\\[\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n\\]\n\n\n\n\nDefinition 3.2 (Vector Addition) The sum of two vectors \\(x\\) and \\(y\\) creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the “parallelogram rule” or the “head-to-tail” method, where you place the tail of \\(y\\) at the head of \\(x\\).\n\\[\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n\\]\n\n\nDefinition 3.3 (Vector Subtraction) The difference \\(d = y - x\\) is the vector that “closes the triangle” formed by \\(x\\) and \\(y\\). It represents the displacement vector that connects the tip of \\(x\\) to the tip of \\(y\\), such that \\(x + d = y\\).\n\n\n\n\nVector Addition and Subtraction\n\n\n\n\n\nDefinition 3.4 (Scalar Multiplication) Multiplying a vector by a scalar \\(c\\) scales its magnitude (length) without changing its line of direction. If \\(c\\) is positive, the direction remains the same; if \\(c\\) is negative, the direction is reversed.\n\\[\ncx = \\begin{pmatrix} cx_1 \\\\ \\vdots \\\\ cx_n \\end{pmatrix}\n\\]\n\n\n\nDefinition 3.5 (Euclidean Distance (Length)) The length (or norm) of a vector \\(x = (x_1, \\dots, x_n)^T\\) corresponds to the straight-line distance from the origin to the point defined by \\(x\\). It is defined as the square root of the sum of squared components:\n\\[\n||x||^2 = \\sum_{i=1}^n x_i^2\n\\]\n\\[\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\n\n\n\n\nScalar Multiplication and Length\n\n\n\n\n\nTheorem 3.1 (Law of Cosines) For a triangle with sides \\(a, b, c\\) and angle \\(\\theta\\) opposite to side \\(c\\):\n\\[\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n\\]\n\n\n\n\n\n\n\nGeometry of Inner Product\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Inner Product) The inner product of two vectors \\(x\\) and \\(y\\) is defined as the sum of the products of their corresponding components:\n\\[\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.7 (Vector Projection) The projection of vector \\(y\\) onto vector \\(x\\), denoted \\(\\hat{y}\\), is calculated as:\n\\[\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n\\]\n\\[\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n\\]\nThis is often written compactly by combining the denominators:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x\n\\]\n\n\n\n\nDefinition 3.8 (Perpendicularity) Two vectors are defined as perpendicular (or orthogonal) if the angle between them is \\(90^\\circ\\) (\\(\\pi/2\\)).\nSince \\(\\cos(90^\\circ) = 0\\), the condition for orthogonality simplifies to the inner product being zero:\n\\[\nx'y = 0 \\iff x \\perp y\n\\]\n\n\nExample 3.1 (Orthogonal Vectors) Consider two vectors in \\(\\mathbb{R}^2\\): \\(x = (1, 1)'\\) and \\(y = (1, -1)'\\).\n\\[\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n\\]\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n\n\n\nDefinition 3.9 (Line Spanned by a Vector) The line space \\(L(x)\\), or the space spanned by a vector \\(x\\), is defined as the set of all scalar multiples of \\(x\\):\n\\[\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n\\]\n\n\n\nDefinition 3.10 (Projection onto a Line) A vector \\(\\hat{y}\\) is the projection of \\(y\\) onto the line \\(L(x)\\) if:\n\n\\(\\hat{y}\\) lies on the line \\(L(x)\\) (i.e., \\(\\hat{y} = cx\\) for some scalar \\(c\\)).\nThe residual vector \\((y - \\hat{y})\\) is perpendicular to the direction vector \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Definition Diagram\n\n\n\n\n\nDefinition 3.11 (Forms of Projection) The projection of \\(y\\) onto the vector \\(x\\) is given by:\n\\[\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n\\]\nThis second form separates the components into: \\[\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n\\]\n\n\n\n\n\n\n\nDefinition 3.12 (Projection Matrix onto a Single Vector) The matrix \\(P_x\\) that projects any vector \\(y\\) onto the line spanned by \\(x\\) is defined as:\n\\[\nP_x = \\frac{xx'}{||x||^2}\n\\]\nUsing this matrix, the projection is simply: \\[\n\\hat{y} = P_x y\n\\]\nIf \\(x \\in \\mathbb{R}^p\\), then \\(P_x\\) is a \\(p \\times p\\) symmetric matrix.\n\n\n\n\nExample 3.2 (Numerical Projection) Let \\(y = (1, 3)'\\) and \\(x = (1, 1)'\\). We want to find the projection of \\(y\\) onto \\(x\\).\nMethod 1: Using the Vector Formula First, calculate the inner products: \\[\nx'y = 1(1) + 1(3) = 4\n\\] \\[\n||x||^2 = 1^2 + 1^2 = 2\n\\]\nNow, apply the formula: \\[\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\nMethod 2: Using the Projection Matrix Construct the matrix \\(P_x\\): \\[\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n\\]\nMultiply by \\(y\\): \\[\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\\]\n\n\n\n\nExample Calculation\n\n\n\n\n\nExample 3.3 (Projection onto the “One” Vector) Let \\(y = (y_1, \\dots, y_n)'\\) be a data vector. Let \\(j_n = (1, 1, \\dots, 1)'\\) be a vector of all ones.\nThe projection of \\(y\\) onto \\(j_n\\) is: \\[\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n\\]\nCalculating the components: \\[\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n\\] \\[\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n\\]\nSubstituting these back: \\[\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n\\]\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n\n\n\nProjection onto Mean Vector\n\n\n\n\n\nTheorem 3.2 (Pythagorean Theorem) If two vectors \\(x\\) and \\(y\\) are orthogonal (i.e., \\(x \\perp y\\) or \\(x'y = 0\\)), then the squared length of their sum is equal to the sum of their squared lengths:\n\\[\n||x + y||^2 = ||x||^2 + ||y||^2\n\\]\n\n\n\n\n\n\n\n\nPythagorean Theorem in Vector Space\n\n\n\n\n\nTheorem 3.3 (Least Square Property) Let \\(\\hat{y}\\) be the projection of \\(y\\) onto the line \\(L(x)\\). For any other vector \\(y^*\\) on the line \\(L(x)\\), the distance from \\(y\\) to \\(y^*\\) is always greater than or equal to the distance from \\(y\\) to \\(\\hat{y}\\).\n\\[\n||y - y^*|| \\ge ||y - \\hat{y}||\n\\]\n\n\n\n\n\n\n\n\n\nLeast Square Property",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector Space and Projection</span>"
    ]
  }
]