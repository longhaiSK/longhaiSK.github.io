[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory for Linear Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Statistical Theory for Linear Models",
    "section": "Key Features",
    "text": "Key Features\nThis text adopts a geometric approach to the statistical theory of linear models, aiming to provide a deeper understanding than standard algebraic treatments. Key features include:\n\nProjection Perspective: We prioritize the geometric interpretation of least squares, viewing estimation as a projection of the response vector onto a model subspace. This visual framework unifies diverse topics—from simple regression to complex ANOVA designs—under a single theoretical umbrella.\nInteractive Visualizations: Abstract concepts are brought to life through interactive 3D plots. Readers can rotate and inspect vector spaces, residual planes, and projection geometries to build a tangible intuition for high-dimensional operations.\nComputational Integration: Theory is seamlessly integrated with practice. The text provides implementation examples using R (and Python), demonstrating how theoretical matrix equations translate directly into computational code.\nRigorous Foundations: While visually driven, the text maintains mathematical rigor, covering essential topics such as spectral theory, the generalized inverseand the multivariate normal distribution to ensure a solid theoretical grounding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical Theory for Linear Models",
    "section": "Overview",
    "text": "Overview\nThis course is a rigorous examination of the general linear models using vector space theory, in particular the approach of regarding least square as projection. The topics includes: vector space; projection; matrix algebra; generalized inverses; quadratic forms; theory for point estimation; theory for hypothesis test; theory for non-full-rank models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Statistical Theory for Linear Models",
    "section": "Audience",
    "text": "Audience\nThis book is designed for graduate students and advanced undergraduate students in statistics, data science, and related quantitative fields. It serves as a bridge between applied regression analysis and the theoretical foundations of linear models. Researchers and practitioners seeking a deeper geometric and algebraic understanding of the statistical methods they use daily will also find this text valuable.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Statistical Theory for Linear Models",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, readers should have a comfortable grasp of the following topics:\nLinear Algebra: An elementary understanding of matrix operations is essential. You should be familiar with matrix multiplication, determinants, inversion, and the basic concepts of vector spaces (such as linear independence, basis vectors, and subspaces). While we review key spectral theory concepts (like eigenvalues and the singular value decomposition) in the early chapters, prior exposure to these ideas is helpful.\nProbability and Statistics: A standard introductory course in probability and mathematical statistics is required. Readers should be familiar with random variables, expectation, variance, covariance, common probability distributions (especially the Normal distribution), and fundamental concepts of hypothesis testing and estimation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introlm.html",
    "href": "introlm.html",
    "title": "Introduction",
    "section": "",
    "text": "Multiple Linear Regression\nSuppose we have observations on \\(Y\\) and \\(X_j\\). The data can be represented in matrix form.\n\\[\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n\\]\nwhere the error terms are distributed as: \\[\n\\epsilon \\sim N_n(0, \\sigma^2 I_n),\n\\]\nin which \\(I_n\\) is the identity matrix: \\[\nI_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\n\\end{pmatrix}\n\\] The scalar equation for a single observation is: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n\\]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#examples",
    "href": "introlm.html#examples",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\nPolynomial Regression\nPolynomial regression fits a curved line to the data points but remains linear in the parameters (\\(\\beta\\)).\nThe model equation is: \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n\\]\n\n\nDesign Matrix Construction\nThe design matrix \\(X\\) is constructed by taking powers of the input variable.\n\\[\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{p-1}\n\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]\n\n\nOne-Way ANOVA\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\nSuppose we have 3 groups (\\(G_1, G_2, G_3\\)) with observations: \\[\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n\\[\n\\overset{G_1}{\n  \\boxed{\n    \\begin{matrix} Y_{11} \\\\ Y_{12} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_2}{\n  \\boxed{\n    \\begin{matrix} Y_{21} \\\\ Y_{22} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_3}{\n  \\boxed{\n    \\begin{matrix} Y_{31} \\\\ Y_{32} \\end{matrix}\n  }\n}\n\\]\nWe construct the matrix \\(X\\) to select the group mean (\\(\\mu\\)) corresponding to the observation:\n\\[\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n\\]\n\\[\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n\\]\n\n\nAnalysis of Covariance (ANCOVA)\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\\[\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,\\text{cont}} & 1 & 0 \\\\\nX_{2,\\text{cont}} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,\\text{cont}} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n\\]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#least-squares-estimation",
    "href": "introlm.html#least-squares-estimation",
    "title": "Introduction",
    "section": "Least Squares Estimation",
    "text": "Least Squares Estimation\nFor the general linear model \\(y = X\\beta + \\epsilon\\), the Least Squares estimator is:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nThe predicted values (\\(\\hat{y}\\)) are obtained via the Projection Matrix (Hat Matrix) \\(P_X\\):\n\\[\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n\\]\nThe residuals and Sum of Squared Errors are:\n\\[\n\\hat{e} = y - \\hat{y}\n\\] \\[\n\\text{SSE} = ||\\hat{e}||^2\n\\]\nThe coefficient of determination is: \\[\nR^2 = \\frac{\\text{SST} - \\text{SSE}}{\\text{SST}}\n\\] where \\(\\text{SST} = \\sum (y_i - \\bar{y})^2\\).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introlm.html#geometric-perspective-of-least-square-estimation",
    "href": "introlm.html#geometric-perspective-of-least-square-estimation",
    "title": "Introduction",
    "section": "Geometric Perspective of Least Square Estimation",
    "text": "Geometric Perspective of Least Square Estimation\nWe align the coordinate system to the models for clarity:\n\nReduced Model (\\(M_0\\)): Represented by the X-axis (labeled \\(j_3\\)).\n\n\\(\\hat{y}_0\\) is the projection of \\(y\\) onto this axis.\n\nFull Model (\\(M_1\\)): Represented by the XY-plane (the floor).\n\n\\(\\hat{y}_1\\) is the projection of \\(y\\) onto this plane (\\(z=0\\)).\n\nObserved Data (\\(y\\)): A point in 3D space.\n\nThe “improvement” due to adding predictors is the distance between \\(\\hat{y}_0\\) and \\(\\hat{y}_1\\).\n\n\n\n\n\n\n\n\nFigure 1: Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)\n\n\n\n\nThe geometric perspective is not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:\n\nStatistical Clarity: Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.\nComputational Stability: A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic “Normal Equations” (\\((X'X)^{-1}X'y\\)) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.\nGeneralizability: The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "lec2-matrix.html",
    "href": "lec2-matrix.html",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "1.1 Spectral Theory\nThis chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, and generalized inverses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#spectral-theory",
    "href": "lec2-matrix.html#spectral-theory",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "",
    "text": "1.1.1 Eigenvalues and Eigenvectors\n\nDefinition 1.1 (Eigenvalues and Eigenvectors) For a square matrix \\(A\\) (\\(n \\times n\\)), a scalar \\(\\lambda\\) is an eigenvalue and a non-zero vector \\(x\\) is the corresponding eigenvector if:\n\\[\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n\\]\nThe eigenvalues are found by solving the characteristic equation: \\[\n|A - \\lambda I_n| = 0\n\\]\n\n\n\n1.1.2 Spectral Decomposition\nFor symmetric matrices, we have a powerful decomposition theorem.\n\nTheorem 1.1 (Spectral Decomposition) If \\(A\\) is a symmetric \\(n \\times n\\) matrix, all its eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) are real. Furthermore, there exists an orthogonal matrix \\(Q\\) such that:\n\\[\nA = Q \\Lambda Q' = \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\]\nwhere:\n\n\\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\) contains the eigenvalues.\n\\(Q = (q_1, \\dots, q_n)\\) contains the corresponding orthonormal eigenvectors (\\(q_i'q_j = \\delta_{ij}\\)).\n\n\nExplantion: This allows us to view the transformation \\(Ax\\) as a rotation (\\(Q'\\)), a scaling (\\(\\Lambda\\)), and a rotation back (\\(Q\\)). For a symmetric matrix \\(A\\), we can write the spectral decomposition as a product of the eigenvector matrix \\(Q\\) and eigenvalue matrix \\(\\Lambda\\):\n\\[\n\\begin{aligned}\nA &= Q \\Lambda Q' \\\\\n  &= \\begin{pmatrix} q_1 & q_2 & \\cdots & q_n \\end{pmatrix}\n     \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\begin{pmatrix} \\lambda_1 q_1 & \\lambda_2 q_2 & \\cdots & \\lambda_n q_n \\end{pmatrix}\n     \\begin{pmatrix} q_1' \\\\ q_2' \\\\ \\vdots \\\\ q_n' \\end{pmatrix} \\\\\n  &= \\lambda_1 q_1 q_1' + \\lambda_2 q_2 q_2' + \\cdots + \\lambda_n q_n q_n' \\\\\n  &= \\sum_{i=1}^n \\lambda_i q_i q_i'\n\\end{aligned}\n\\]\nwhere the eigenvectors \\(q_i\\) satisfy the orthogonality conditions: \\[\nq_i' q_j = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{if } i \\ne j \\end{cases}\n\\] And \\(Q\\) is an orthogonal matrix: \\(Q'Q = Q Q' = I_n\\).\n\n\n1.1.3 Quadratic Form\n\nDefinition 1.2 A quadratic form in \\(n\\) variables \\(x_1, x_2, \\dots, x_n\\) is a scalar function defined by a symmetric matrix \\(A\\): \\[\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\]\n\n\n\n1.1.4 Positive and Non-Negative Definite Matrices\n\nDefinition 1.3 (Positive and Non-Negative Definite Matrices) A symmetric matrix \\(A\\) is positive definite (p.d.) if: \\[\nx'Ax &gt; 0 \\quad \\forall x \\ne 0\n\\] It is non-negative definite (n.n.d.) if: \\[\nx'Ax \\ge 0 \\quad \\forall x\n\\]\n\n\nTheorem 1.2 (Properties of Definite Matrices) Let \\(A\\) be a symmetric \\(n \\times n\\) matrix with eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\).\n\nEigenvalue Characterization:\n\n\\(A\\) is p.d. \\(\\iff\\) all \\(\\lambda_i &gt; 0\\).\n\\(A\\) is n.n.d. \\(\\iff\\) all \\(\\lambda_i \\ge 0\\).\n\nDeterminant and Inverse:\n\nIf \\(A\\) is p.d., then \\(|A| &gt; 0\\) and \\(A^{-1}\\) exists.\nIf \\(A\\) is n.n.d. and singular, then \\(|A| = 0\\) (at least one \\(\\lambda_i = 0\\)).\n\nGram Matrices (\\(B'B\\)): Let \\(B\\) be an \\(n \\times p\\) matrix.\n\nIf \\(\\text{rank}(B) = p\\), then \\(B'B\\) is p.d.\nIf \\(\\text{rank}(B) &lt; p\\), then \\(B'B\\) is n.n.d.\n\n\n\n\n\n1.1.5 Properties of Symmetric Matrices\n\nTheorem 1.3 (Properties of Symmetric Matrices) Let \\(A\\) be a symmetric matrix with spectral decomposition \\(A = Q \\Lambda Q'\\). The following properties hold:\n\nTrace: \\(\\text{tr}(A) = \\sum \\lambda_i\\).\nDeterminant: \\(|A| = \\prod \\lambda_i\\).\nSingularity: \\(A\\) is singular if and only if at least one \\(\\lambda_i = 0\\).\nInverse: If \\(A\\) is non-singular (\\(\\lambda_i \\ne 0\\)), then \\(A^{-1} = Q \\Lambda^{-1} Q'\\).\nPowers: \\(A^k = Q \\Lambda^k Q'\\).\n\nSquare Root: \\(A^{1/2} = Q \\Lambda^{1/2} Q'\\) (if \\(\\lambda_i \\ge 0\\)).\n\nSpectral Representation of Quadratic Forms: The quadratic form \\(x'Ax\\) can be diagonalized using the eigenvectors of \\(A\\): \\[\nx'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n\\] where \\(y = Q'x\\) represents a rotation of the coordinate system.\n\n\n\n\n1.1.6 Spectral Representation of Projection Matrices\nWe revisit projection matrices in the context of eigenvalues.\n\nTheorem 1.4 (Eigenvalues of Projection Matrices) A symmetric matrix \\(P\\) is a projection matrix (idempotent, \\(P^2=P\\)) if and only if its eigenvalues are either 0 or 1.\n\\[\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n\\]\n\nFor a projection matrix \\(P\\):\n\nIf \\(x \\in \\text{Col}(P)\\), \\(Px = x\\) (Eigenvalue 1).\nIf \\(x \\perp \\text{Col}(P)\\), \\(Px = 0\\) (Eigenvalue 0).\n\\(\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i\\) (Count of 1s).\n\n\nExample 1.1 For \\(P = \\frac{1}{n} J_n J_n'\\), the rank is \\(\\text{tr}(P) = 1\\).\n\n\n\n1.1.7 Singular Value Decomposition (SVD)\n\nTheorem 1.5 (Singular Value Decomposition (SVD)) Let \\(X\\) be an \\(n \\times p\\) matrix with rank \\(r \\le \\min(n, p)\\). \\(X\\) can be decomposed into the product of three matrices:\n\\[\nX = U \\mathbf{D} V'\n\\]\n1. Partitioned Matrix Form\n\\[\nX = \\underset{n \\times n}{(U_1, U_2)}\n\\begin{pmatrix}\n\\Lambda_r & O_{r \\times (p-r)} \\\\\nO_{(n-r) \\times r} & O_{(n-r) \\times (p-r)}\n\\end{pmatrix}\n\\underset{p \\times p}{\n\\begin{pmatrix}\nV_1' \\\\\nV_2'\n\\end{pmatrix}\n}\n\\]\n2. Detailed Matrix Form\nExpanding the diagonal matrix explicitly:\n\\[\nX = \\underset{n \\times n}{(u_1, \\dots, u_n)}\n\\left(\n\\begin{array}{cccc|c}\n\\lambda_1 & 0 & \\dots & 0 &  \\\\\n0 & \\lambda_2 & \\dots & 0 & O_{12} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots &  \\\\\n0 & 0 & \\dots & \\lambda_r &  \\\\\n\\hline\n& O_{21} & & & O_{22}\n\\end{array}\n\\right)\n\\underset{p \\times p}{\n\\begin{pmatrix}\nv_1' \\\\\n\\vdots \\\\\nv_p'\n\\end{pmatrix}\n}\n\\]\n3. Reduced Form\n\\[\nX = U_1 \\Lambda_r V_1' = \\sum_{i=1}^r \\lambda_i u_i v_i'\n\\]\nProperties:\n\nSingular Values (\\(\\Lambda_r\\)): \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values (\\(\\lambda_i &gt; 0\\)), which are the square roots of the non-zero eigenvalues of \\(X'X\\).\nOrthogonality:\n\n\\(U\\) is \\(n \\times n\\) orthogonal (\\(U'U = I_n\\)).\n\\(V\\) is \\(p \\times p\\) orthogonal (\\(V'V = I_p\\)).\n\n\n\n\n1.1.7.1 Connection to Gram Matrices\nThe matrices \\(U\\) and \\(V\\) provide the basis vectors (eigenvectors) for the Gram matrices of \\(X\\).\n\nRight Singular Vectors (\\(V\\)): The columns of \\(V\\) are the eigenvectors of the Gram matrix \\(X'X\\). \\[\nX'X = (U \\Lambda V')' (U \\Lambda V') = V \\Lambda U' U \\Lambda V' = V \\Lambda^2 V'\n\\]\n\nThe eigenvalues of \\(X'X\\) are the squared singular values \\(\\lambda_i^2\\).\n\nLeft Singular Vectors (\\(U\\)): The columns of \\(U\\) are the eigenvectors of the Gram matrix \\(XX'\\). \\[\nXX' = (U \\Lambda V') (U \\Lambda V')' = U \\Lambda V' V \\Lambda U' = U \\Lambda^2 U'\n\\]\n\nThe eigenvalues of \\(XX'\\) are also \\(\\lambda_i^2\\) (for non-zero values).\n\n\n\n\n1.1.7.2 Numerical Example\nConsider the matrix \\(X = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\\).\n\nCompute \\(X'X\\) and find \\(V\\): \\[\nX'X = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 5 \\\\ 5 & 5 \\end{pmatrix}\n\\]\n\nEigenvalues of \\(X'X\\): Trace is 10, Determinant is 0. Thus, \\(\\mu_1 = 10, \\mu_2 = 0\\).\nSingular Values: \\(\\lambda_1 = \\sqrt{10}, \\lambda_2 = 0\\).\nEigenvector for \\(\\mu_1=10\\): Normalized \\(v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}\\).\n\nCompute \\(XX'\\) and find \\(U\\): \\[\nXX' = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n\\]\n\nEigenvalues are again 10 and 0.\nEigenvector for \\(\\mu_1=10\\): Normalized \\(u_1 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\).\nEigenvector for \\(\\mu_2=0\\): Normalized \\(u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\).\nTherefore, \\(U = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & 2 \\\\ 2 & -1 \\end{pmatrix}\\).\n\nVerification: \\[\nX = \\sqrt{10} u_1 v_1' = \\sqrt{10} \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#cholesky-decomposition",
    "href": "lec2-matrix.html#cholesky-decomposition",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "1.2 Cholesky Decomposition",
    "text": "1.2 Cholesky Decomposition\nA symmetric matrix \\(A\\) has a Cholesky decomposition if and only if it is non-negative definite (i.e., \\(x'Ax \\ge 0\\) for all \\(x\\)).\n\\[\nA = B'B\n\\]\nwhere \\(B\\) is an upper triangular matrix with non-negative diagonal entries.\n\n1.2.1 Matrix Representation of the Algorithm\nTo derive the algorithm, we equate the elements of \\(A\\) with the product of the lower triangular matrix \\(B'\\) and the upper triangular matrix \\(B\\).\nFor a \\(3 \\times 3\\) matrix, this looks like:\n\\[\n\\underbrace{\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}}_{A}\n=\n\\underbrace{\\begin{pmatrix}\nb_{11} & 0 & 0 \\\\\nb_{12} & b_{22} & 0 \\\\\nb_{13} & b_{23} & b_{33}\n\\end{pmatrix}}_{B'}\n\\underbrace{\\begin{pmatrix}\nb_{11} & b_{12} & b_{13} \\\\\n0 & b_{22} & b_{23} \\\\\n0 & 0 & b_{33}\n\\end{pmatrix}}_{B}\n\\]\nMultiplying the matrices on the right yields the system of equations:\n\\[\nA = \\begin{pmatrix}\n\\mathbf{b_{11}^2} & b_{11}b_{12} & b_{11}b_{13} \\\\\nb_{12}b_{11} & \\mathbf{b_{12}^2 + b_{22}^2} & b_{12}b_{13} + b_{22}b_{23} \\\\\nb_{13}b_{11} & b_{13}b_{12} + b_{23}b_{22} & \\mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}\n\\end{pmatrix}\n\\]\nBy solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.\n\n\n1.2.2 The Algorithm\n\nRow 1: Solve for \\(b_{11}\\) using \\(a_{11}\\), then solve the rest of the row (\\(b_{1j}\\)) by division.\n\n\\(b_{11} = \\sqrt{a_{11}}\\)\n\\(b_{1j} = a_{1j}/b_{11}\\)\n\nRow 2: Solve for \\(b_{22}\\) using \\(a_{22}\\) and the known \\(b_{12}\\), then solve \\(b_{2j}\\).\n\n\\(b_{22} = \\sqrt{a_{22} - b_{12}^2}\\)\n\\(b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}\\)\n\nRow 3: Solve for \\(b_{33}\\) using \\(a_{33}\\) and the known \\(b_{13}, b_{23}\\).\n\n\\(b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}\\)\n\n\n\n\n1.2.3 Numerical Example\nConsider the positive definite matrix \\(A\\): \\[\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n\\]\nWe find \\(B\\) such that \\(A = B'B\\):\n\nFirst Row of B (\\(b_{11}, b_{12}, b_{13}\\)):\n\n\\(b_{11} = \\sqrt{4} = 2\\)\n\\(b_{12} = 2 / 2 = 1\\)\n\\(b_{13} = -2 / 2 = -1\\)\n\nSecond Row of B (\\(b_{22}, b_{23}\\)):\n\n\\(b_{22} = \\sqrt{10 - (1)^2} = \\sqrt{9} = 3\\)\n\\(b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1\\)\n\nThird Row of B (\\(b_{33}\\)):\n\n\\(b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{4} = 2\\)\n\n\nResult: \\[\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#generalized-inverses",
    "href": "lec2-matrix.html#generalized-inverses",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "1.3 Generalized Inverses",
    "text": "1.3 Generalized Inverses\n\n1.3.1 Motivation\nConsider the linear system \\(X\\beta = y\\). In \\(\\mathbb{R}^2\\), if \\(X = [x_1, x_2]\\) is invertible, the solution is unique: \\(\\beta = X^{-1}y\\). This satisfies \\(X(X^{-1}y) = y\\).However, if \\(X\\) is not square or not invertible (e.g., \\(X\\) is \\(2 \\times 3\\)), \\(X\\beta = y\\) does not have a unique solution. We seek a matrix \\(G\\) such that \\(\\beta = Gy\\) provides a solution whenever \\(y \\in C(X)\\) (the column space of X). Substituting \\(\\beta = Gy\\) into the equation \\(X\\beta = y\\): \\[\nX(Gy) = y \\quad \\forall y \\in C(X)\n\\] Since any \\(y \\in C(X)\\) can be written as \\(Xw\\) for some vector \\(w\\): \\[\nXGXw = Xw \\quad \\forall w\n\\] This implies the defining condition: \\[\nXGX = X\n\\]\n\n\n1.3.2 Definition of Generalized Inverse\n\nDefinition 1.4 (Generalized Inverse) Let \\(X\\) be an \\(n \\times p\\) matrix. A matrix \\(X^-\\) of size \\(p \\times n\\) is called a generalized inverse of \\(X\\) if it satisfies: \\[\nXX^-X = X\n\\]\n\n\nExample 1.2 (Examples of Generalized Inverse)  \n\nExample 1: Diagonal Matrix If \\(X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)\\), we can write it in matrix form as: \\[\n  X = \\begin{pmatrix}\n  \\lambda_1 & 0 & 0 & 0 \\\\\n  0 & \\lambda_2 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\] A generalized inverse is obtained by inverting the non-zero elements: \\[\n  X^- = \\begin{pmatrix}\n  \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n  0 & \\lambda_2^{-1} & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{pmatrix}\n  \\]\nExample 2: Row Vector Let \\(X = (1, 2, 3)\\). One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of \\(X\\) (which is \\(1\\)), and others are zero: \\[\n  X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  \\] Verification: \\[\n  XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n  \\] Other valid generalized inverses include \\(\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\\) or \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\).\nExample 3: Rank Deficient Matrix Let \\(A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}\\). Note that Row 3 = Row 1 + Row 2, so Rank\\((A) = 2\\).\nSolution: A generalized inverse can be found by locating a non-singular \\(2 \\times 2\\) submatrix, inverting it, and padding the rest with zeros. Let’s take the top-left minor \\(M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}\\). The inverse is \\(M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}\\).\nPlacing this in the corresponding position in \\(A^-\\) and setting the rest to 0: \\[\n  A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n  \\]\nVerification (\\(AA^-A = A\\)): First, compute \\(AA^-\\): \\[\n  AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n  \\] Then multiply by \\(A\\): \\[\n  (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n  \\]\n\n\n\n\n1.3.3 A Procedure to Find a Generalized Inverse\nIf we can partition \\(X\\) (possibly after permuting rows/columns) such that \\(R_{11}\\) is a non-singular rank \\(r\\) submatrix:\n\\[\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n\\]\nThen a generalized inverse is:\n\\[\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nVerification:\n\\[\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n\\] Note that since rank\\((X) = \\text{rank}(R_{11})\\), the rows of \\([R_{21}, R_{22}]\\) are linear combinations of \\([R_{11}, R_{12}]\\), implying \\(R_{22} = R_{21}R_{11}^{-1}R_{12}\\). Thus, \\(XX^-X = X\\).\nAn Algorithm for Finding a Generalized Inverse\nA systematic procedure to find a generalized inverse \\(A^-\\) for any matrix \\(A\\):\n\nFind any non-singular \\(r \\times r\\) submatrix \\(C\\), where \\(r\\) is the rank of \\(A\\). It is not necessary for the elements of \\(C\\) to occupy adjacent rows and columns in \\(A\\).\nFind \\(C^{-1}\\) and \\((C^{-1})'\\).\nReplace the elements of \\(C\\) in \\(A\\) with the elements of \\((C^{-1})'\\).\nReplace all other elements in \\(A\\) with zeros.\nTranspose the resulting matrix.\n\nMatrix Visual Representation \\[\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n\\]\nLegend:\n\n\\(\\otimes\\): Elements of submatrix \\(C\\)\n\\(\\triangle\\): Elements of \\((C^{-1})'\\)\n\\(\\square\\): Elements of \\(C^{-1}\\) (after transposition)\n\\(\\times\\): Other elements (replaced by 0 in the final calculation)\n\n\n\n1.3.4 Moore-Penrose Inverse\nThe Moore-Penrose inverse (denoted \\(X^+\\)) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\nIf \\(X\\) has SVD: \\[\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nThen the Moore-Penrose inverse is: \\[\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n\\]\nwhere \\(\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)\\) contains the singular values. Unlike standard generalized inverses, \\(X^+\\) is unique.\nVerification:\nWe verify that \\(X^+\\) satisfies the condition \\(XX^+X = X\\).\n\nSubstitute definitions: \\[\nXX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n\\]\nApply orthogonality: Recall that \\(V'V = I\\) and \\(U'U = I\\). \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n\\]\nMultiply diagonal matrices: \\[\n= U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n\\] Since \\(\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r\\): \\[\n= U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n\\]\n\n\n\n1.3.5 Solving Linear Systems with Generalized Inverse\nWe apply generalized inverses to solve systems of linear equations \\(X\\beta = c\\) where \\(X\\) is \\(n \\times p\\).\n\nDefinition 1.5 (Consistency and Solution) The system \\(X\\beta = c\\) is consistent if and only if \\(c \\in \\mathcal{C}(X)\\) (the column space of \\(X\\)). If consistent, \\(\\beta = X^- c\\) is a solution.\n\nProof: If the system is consistent, there exists some \\(b\\) such that \\(Xb = c\\). Using the definition \\(XX^-X = X\\): \\[\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n\\] Thus, \\(X^-c\\) is a solution. Note that the solution is not unique if \\(X\\) is not full rank.\n\nExample 1.3 (Examples of Solutions of Linear System with Generalized Inverse)  \n\nExample 1: Underdetermined System\nLet \\(X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}\\) and we want to solve \\(X\\beta = 4\\).\nSolution 1: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n\\]\nSolution 2: Using another generalized inverse \\(X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}\\): \\[\n\\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n\\]\nExample 2: Overdetermined System\nLet \\(X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\). Solve \\(X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c\\). Here \\(c = 2X\\), so the system is consistent. Since \\(X\\) is a column vector, \\(\\beta\\) is a scalar.\nSolution: Using the generalized inverse \\(X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\[\n\\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n\\] Verification: \\[\nX\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "href": "lec2-matrix.html#least-squares-for-non-full-rank-x-with-generalized-inverse",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "1.4 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse",
    "text": "1.4 Least Squares for Non-full-rank \\(X\\) with Generalized Inverse\n\n1.4.1 Projection Matrix with Generalized Inverse of \\(X'X\\)\nFor the normal equations \\((X'X)\\beta = X'y\\), a solution is given by: \\[\n\\hat{\\beta} = (X'X)^- X'y\n\\] The fitted values are \\[\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.\\] This \\(\\hat{y}\\) represents the unique orthogonal projection of \\(y\\) onto \\(\\text{Col}(X)\\).\n\n\n1.4.2 Invariance and Uniqueness of “the” Projection Matrix\n\nTheorem 1.6 (Transpose Property of Generalized Inverses) \\((X^-)'\\) is a version of \\((X')^-\\). That is, \\((X^-)'\\) is a generalized inverse of \\(X'\\).\n\n\nProof. By definition, a generalized inverse \\(X^-\\) satisfies the property: \\[\nX X^- X = X\n\\]\nTo verify that \\((X^-)'\\) is a generalized inverse of \\(X'\\), we need to show that it satisfies the condition \\(A G A = A\\) where \\(A = X'\\) and \\(G = (X^-)'\\).\n\nStart with the fundamental definition: \\[\nX X^- X = X\n\\]\nTake the transpose of both sides of the equation: \\[\n(X X^- X)' = X'\n\\]\nApply the reverse order law for transposes, \\((ABC)' = C' B' A'\\): \\[\nX' (X^-)' X' = X'\n\\]\n\nSince substituting \\((X^-)'\\) into the generalized inverse equation for \\(X'\\) yields \\(X'\\), \\((X^-)'\\) is a valid generalized inverse of \\(X'\\).\n\n\nLemma 1.1 (Invariance of Generalized Least Squares) For any version of the generalized inverse \\((X'X)^-\\), the matrix \\(X'(X'X)^- X'\\) is invariant and equals \\(X'\\). \\[\nX'X(X'X)^- X' = X'\n\\]\n\nProof (using Projection): Let \\(P = X(X'X)^- X'\\). This is the projection matrix onto \\(\\mathcal{C}(X)\\). By definition of projection, \\(Px = x\\) for any \\(x \\in \\text{Col}(X)\\). Since columns of \\(X\\) are in \\(\\text{Col}(X)\\), \\(PX = X\\). Taking the transpose: \\((PX)' = X' \\implies X'P' = X'\\). Since projection matrices are symmetric (\\(P=P'\\)), \\(X'P = X'\\). Substituting \\(P\\): \\(X' X (X'X)^- X' = X'\\).\nProof (Direct Matrix Manipulation): Decompose \\(y = X\\beta + e\\) where \\(e \\perp \\text{Col}(X)\\) (i.e., \\(X'e = 0\\)). \\[\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n\\] Using the property \\(A A^- A = A\\) (where \\(A=X'X\\)), the first term becomes \\(X'X\\beta\\). The second term is 0 because \\(X'e = 0\\). Thus, the expression simplifies to \\(X'X\\beta = X'(X\\beta) = X'\\hat{y}_{\\text{proj}}\\). This implies the operator acts as \\(X'\\).\n\nTheorem 1.7 (Properties of Projection Matrix \\(P\\)) Let \\(P = X(X'X)^- X'\\). This matrix has the following properties:\n\nSymmetry: \\(P = P'\\).\nIdempotence: \\(P^2 = P\\). \\[\nP^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n\\] Using the identity from Lemma 1.1 (\\(X'X(X'X)^- X' = X'\\)), this simplifies to: \\[\nX(X'X)^- X' = P\n\\]\nUniqueness: \\(P\\) is unique and invariant to the choice of the generalized inverse \\((X'X)^-\\).\n\n\n\nProof. Proof of Uniqueness:\nLet \\(A\\) and \\(B\\) be two different generalized inverses of \\(X'X\\). Define \\(P_A = X A X'\\) and \\(P_B = X B X'\\). From Lemma 1.1, we know that \\(X' P_A = X'\\) and \\(X' P_B = X'\\).\nSubtracting these two equations: \\[\nX' (P_A - P_B) = 0\n\\] Taking the transpose, we get \\((P_A - P_B) X = 0\\). This implies that the columns of the difference matrix \\(D = P_A - P_B\\) are orthogonal to the columns of \\(X\\) (i.e., \\(D \\perp \\text{Col}(X)\\)).\nHowever, by definition, the columns of \\(P_A\\) and \\(P_B\\) (and thus \\(D\\)) are linear combinations of the columns of \\(X\\) (i.e., \\(D \\in \\text{Col}(X)\\)).\nThe only matrix that lies in the column space of \\(X\\) but is also orthogonal to the column space of \\(X\\) is the zero matrix. Therefore: \\[\nP_A - P_B = 0 \\implies P_A = P_B\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "href": "lec2-matrix.html#the-left-inverse-view-recovering-hatbeta-from-haty",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "1.5 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)",
    "text": "1.5 The Left Inverse View: Recovering \\(\\hat{\\beta}\\) from \\(\\hat{y}\\)\nWhile the geometric properties of the linear model are most naturally established via the unique orthogonal projection \\(\\hat{y}\\), we require a functional mapping—a statistical “bridge”—to translate the distribution of these fitted values back into the parameter space of \\(\\hat{\\beta}\\). This bridge is provided by the generalized left inverse.\n\n1.5.1 The Generalized Left Inverse\nTo recover the parameter estimates directly from the fitted values, we define the generalized left inverse, denoted as \\(X_{\\text{left}}^-\\), such that:\n\\[\n\\hat{\\beta} = X_{\\text{left}}^- \\hat{y}\n\\]\nA standard choice for this operator, derived from the normal equations, is:\n\\[\nX_{\\text{left}}^- = (X' X)^- X'\n\\]\nWhen \\(X\\) is full-rank, the \\(X_{\\text{left}}^-\\) is unique, which is given by\n\\[\nX_{\\text{left}}^- = (X' X)^{-1} X'\n\\]\n\n\n1.5.2 Verification of the Inverse Property\nTo verify that \\(X_{\\text{left}}^-\\) acts as a valid generalized inverse of \\(X\\), it must satisfy the condition \\(X X_{\\text{left}}^- X = X\\). Substituting our definition:\n\\[\nX \\underbrace{\\left[ (X' X)^- X' \\right]}_{X_{\\text{left}}^-} X = X (X' X)^- (X' X)\n\\]\nUsing the property of generalized inverses for symmetric matrices where \\((X' X)(X' X)^- X' = X'\\), the transpose of this identity gives \\(X (X' X)^- (X' X) = X\\). Thus, the condition holds:\n\\[\nX X_{\\text{left}}^- X = X\n\\]\n\n\n1.5.3 Recovering the Estimator\nWe can now demonstrate that applying this left inverse to the fitted values \\(\\hat{y}\\) yields the standard solution to the normal equations.\nSubstituting the projection formula \\(\\hat{y} = X(X' X)^- X' y\\):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\left[ (X' X)^- X' \\right] \\left[ X(X' X)^- X' y \\right] \\\\\n&= (X' X)^- \\underbrace{(X' X) (X' X)^- (X' X)}_{\\text{Property } A A^- A = A} (X' X)^- X' y\n\\end{aligned}\n\\]\nSimplifying using the generalized inverse property \\(A^- A A^- = A^-\\) (where \\(A = X' X\\)):\n\\[\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\underbrace{(X' X)^- (X' X) (X' X)^-}_{(X' X)^-} X' y \\\\\n&= (X' X)^- X' y\n\\end{aligned}\n\\]\nThus, we recover the standard estimator used in the normal equations:\n\\[\n\\mathbf{\\hat{\\beta} = (X' X)^- X' y}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec2-matrix.html#non-full-rank-least-squares-with-qr-decomposition",
    "href": "lec2-matrix.html#non-full-rank-least-squares-with-qr-decomposition",
    "title": "1  Spectral Theory and Generalized Inverse",
    "section": "1.6 Non-full-rank Least Squares with QR Decomposition",
    "text": "1.6 Non-full-rank Least Squares with QR Decomposition\nWhen \\(X\\) has rank \\(r &lt; p\\) (where \\(X\\) is \\(n \\times p\\)), we can derive the least squares estimator using partitioned matrices.\nAssume the first \\(r\\) columns of \\(X\\) are linearly independent. We can partition \\(X\\) as: \\[\nX = Q (R_1, R_2)\n\\] where \\(Q\\) is an \\(n \\times r\\) matrix with orthogonal columns (\\(Q'Q = I_r\\)), \\(R_1\\) is an \\(r \\times r\\) non-singular matrix, and \\(R_2\\) is \\(r \\times (p-r)\\).\nThe normal equations are: \\[\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n\\] Simplifying (\\(Q'Q = I_r\\)): \\[\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\]\n\n1.6.1 Constructing a Solution by Solving Normal Equations\nOne specific generalized inverse of \\(X'X\\) can be found by focusing on the non-singular block \\(R_1'R_1\\): \\[\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\]\nUsing this generalized inverse, the estimator \\(\\hat{\\beta}\\) becomes: \\[\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n\\]\nThe fitted values are: \\[\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n\\] This confirms that \\(\\hat{y}\\) is the projection of \\(y\\) onto the column space of \\(Q\\) (which is the same as the column space of \\(X\\)).\n\n\n1.6.2 Constructing a Solution by Solving Reparametrized \\(\\beta\\)\nWe can view the model as: \\[\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n\\] where \\(b = R_1\\beta_1 + R_2\\beta_2\\).\nSince the columns of \\(Q\\) are orthogonal, the least squares estimate for \\(b\\) is simply: \\[\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n\\]\nTo find \\(\\beta\\), we solve the underdetermined system: \\[\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n\\]\nSolution 1: Set \\(\\beta_2 = 0\\). Then: \\[\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n\\] This yields the same result as the generalized inverse method above: \\(\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\\).\nSolution 2: Using the generalized inverse of \\(R = (R_1, R_2)\\): \\[\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n\\] \\[\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n\\] This demonstrates that finding a solution to the normal equations using \\((X'X)^-\\) is equivalent to solving the reparameterized system \\(b = R\\beta\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spectral Theory and Generalized Inverse</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html",
    "href": "lec3-mvn.html",
    "title": "2  Multivariate Normal Distribution",
    "section": "",
    "text": "2.1 Motivation\nConsider the linear model: \\[\ny = X\\beta + \\epsilon, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nWe are often interested in the distributional properties of the response vector \\(y\\) and the residuals. Specifically, if \\(y = (y_1, \\dots, y_n)'\\), we need to understand its multivariate distribution. \\[\n\\hat{y} = Py, \\quad e = y - \\hat{y} = (I_n - P)y\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#random-vectors-and-matrices",
    "href": "lec3-mvn.html#random-vectors-and-matrices",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.2 Random Vectors and Matrices",
    "text": "2.2 Random Vectors and Matrices\n\nDefinition 2.1 (Random Vector and Matrix) A Random Vector is a vector whose elements are random variables. E.g., \\[\nx_{k \\times 1} = (x_1, x_2, \\dots, x_k)^T\n\\] where \\(x_1, \\dots, x_k\\) are each random variables.\nA Random Matrix is a matrix whose elements are random variables. E.g., \\(X_{n \\times k} = (x_{ij})\\), where \\(x_{11}, \\dots, x_{nk}\\) are each random variables.\n\n\nDefinition 2.2 (Expected Value) The expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.\nFor \\(X_{n \\times k}\\): \\[\nE(X) = \\begin{pmatrix}\nE(x_{11}) & \\dots & E(x_{1k}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE(x_{n1}) & \\dots & E(x_{nk})\n\\end{pmatrix}\n\\]\n\\[\nE\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}\\right) = \\begin{pmatrix} E(x_1) \\\\ \\vdots \\\\ E(x_k) \\end{pmatrix}\n\\]\n\n\nDefinition 2.3 (Variance-Covariance Matrix) For a random vector \\(x_{k \\times 1} = (x_1, \\dots, x_k)^T\\), the matrix is:\n\\[\n\\text{Var}(x) = \\Sigma_x = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2} & \\dots & \\sigma_{kk}\n\\end{pmatrix}\n\\]\nWhere:\n\n\\(\\sigma_{ij} = \\text{Cov}(x_i, x_j) = E[(x_i - \\mu_i)(x_j - \\mu_j)]\\)\n\\(\\sigma_{ii} = \\text{Var}(x_i) = E[(x_i - \\mu_i)^2]\\)\n\nIn matrix notation: \\[\n\\text{Var}(x) = E[(x - \\mu_x)(x - \\mu_x)^T]\n\\] Note: \\(\\text{Var}(x)\\) is symmetric.\n\n\n2.2.1 Derivation of Covariance Matrix Structure\nExpanding the vector multiplication for variance: \\[\n(x - \\mu_x)(x - \\mu_x)' \\quad \\text{where } \\mu_x = (\\mu_1, \\dots, \\mu_n)'\n\\] \\[\n= \\begin{pmatrix} x_1 - \\mu_1 \\\\ \\vdots \\\\ x_n - \\mu_n \\end{pmatrix} (x_1 - \\mu_1, \\dots, x_n - \\mu_n)\n\\] This results in the matrix \\(A = (a_{ij})\\) where \\(a_{ij} = (x_i - \\mu_i)(x_j - \\mu_j)\\). Taking expectations yields the covariance matrix elements \\(\\sigma_{ij}\\).\n\nDefinition 2.4 (Covariance Matrix (Two Vectors)) For random vectors \\(x_{k \\times 1}\\) and \\(y_{n \\times 1}\\), the covariance matrix is: \\[\n\\text{Cov}(x, y) = E[(x - \\mu_x)(y - \\mu_y)^T] = \\begin{pmatrix}\n\\text{Cov}(x_1, y_1) & \\dots & \\text{Cov}(x_1, y_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(x_k, y_1) & \\dots & \\text{Cov}(x_k, y_n)\n\\end{pmatrix}\n\\] Note that \\(\\text{Cov}(x, x) = \\text{Var}(x)\\).\n\n\nDefinition 2.5 (Correlation Matrix) The correlation matrix of a random vector \\(x\\) is: \\[\n\\text{corr}(x) = \\begin{pmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\dots & 1\n\\end{pmatrix}\n\\] where \\(\\rho_{ij} = \\text{corr}(x_i, x_j)\\).\nRelationships: Let \\(V_x = \\text{diag}(\\text{Var}(x_1), \\dots, \\text{Var}(x_k))\\). \\[\n\\Sigma_x = V_x^{1/2} \\rho_x V_x^{1/2} \\quad \\text{and} \\quad \\rho_x = (V_x^{1/2})^{-1} \\Sigma_x (V_x^{1/2})^{-1}\n\\] Similarly for two vectors: \\[\n\\Sigma_{xy} = V_x^{1/2} \\rho_{xy} V_y^{1/2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#properties-of-mean-and-variance",
    "href": "lec3-mvn.html#properties-of-mean-and-variance",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.3 Properties of Mean and Variance",
    "text": "2.3 Properties of Mean and Variance\nWe can derive several key algebraic properties for operations on random vectors.\n\n\\(E(X + Y) = E(X) + E(Y)\\)\n\\(E(AXB) = A E(X) B\\) (In particular, \\(E(AX) = A\\mu_x\\))\n\\(\\text{Cov}(x, y) = \\text{Cov}(y, x)^T\\)\n\\(\\text{Cov}(x + c, y + d) = \\text{Cov}(x, y)\\)\n\\(\\text{Cov}(Ax, By) = A \\text{Cov}(x, y) B^T\\)\n\nSpecial case for scalars: \\(\\text{Cov}(ax, by) = ab \\cdot \\text{Cov}(x, y)\\)\n\n\\(\\text{Cov}(x_1 + x_2, y_1) = \\text{Cov}(x_1, y_1) + \\text{Cov}(x_2, y_1)\\)\n\\(\\text{Var}(x + c) = \\text{Var}(x)\\)\n\\(\\text{Var}(Ax) = A \\text{Var}(x) A^T\\)\n\\(\\text{Var}(x_1 + x_2) = \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)\\)\n\\(\\text{Var}(\\sum x_i) = \\sum \\text{Var}(x_i)\\) if independent.\n\n\nProof. Property 5 (Covariance of Linear Transformation): \\[\n\\begin{aligned}\n\\text{Cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{Cov}(x, y) B^T\n\\end{aligned}\n\\] Property 2 (Expectation of Linear Transformation):\nTo prove \\(E(AXB) = A E(X) B\\): First consider \\(E(Ax_j)\\) where \\(x_j\\) is a column of \\(X\\). \\[\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n\\] Since \\(a_i\\) are constants: \\[\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n\\] Thus \\(E(Ax_j) = A E(x_j)\\). Applying this to all columns of \\(X\\): \\[\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n\\] Similarly, \\(E(XB) = E(X)B\\).\nProof of Property 9 (Variance of Sum):\n\\[\n\\text{Var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n\\] Let centered variables be denoted by differences. \\[\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n\\] Expanding terms: \\[\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n\\] \\[\n= \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#the-multivariate-normal-distribution",
    "href": "lec3-mvn.html#the-multivariate-normal-distribution",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.4 The Multivariate Normal Distribution",
    "text": "2.4 The Multivariate Normal Distribution\n\n2.4.1 Definition and Density\n\nDefinition 2.6 (Independent Standard Normal) Let \\(z = (z_1, \\dots, z_n)'\\) where \\(z_i \\sim N(0, 1)\\) are independent. We say \\(z \\sim N_n(0, I_n)\\). The joint PDF is the product of marginals: \\[\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n\\] Properties: \\(E(z) = 0\\) and \\(\\text{Var}(z) = I_n\\) (Covariance is 0 for \\(i \\ne j\\), Variance is 1).\n\n\nDefinition 2.7 (Multivariate Normal Distribution) A random vector \\(x\\) (\\(n \\times 1\\)) has a multivariate normal distribution if it has the same distribution as: \\[\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n\\] where \\(z \\sim N_p(0, I_p)\\), \\(A\\) is a matrix of constants, and \\(\\mu\\) is a vector of constants. The moments are:\n\n\\(E(x) = \\mu\\)\n\\(\\text{Var}(x) = AA^T = \\Sigma\\)\n\n\n\n\n2.4.2 Geometric Interpretation\nUsing Spectral Decomposition, \\(\\Sigma = Q \\Lambda Q'\\). We can view the transformation \\(x = Az + \\mu\\) as:\n\nScaling by eigenvalues (\\(\\Lambda^{1/2}\\)).\nRotation by eigenvectors (\\(Q\\)).\nShift by mean (\\(\\mu\\)).\n\nAn Shinely App for Visualizing Bivariate Normal\nUse the controls to construct the covariance matrix \\(\\boldsymbol{\\Sigma}\\) geometrically.\nWe define the transformation matrix \\(\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}\\), where \\(\\mathbf{Q}\\) is a rotation matrix and \\(\\mathbf{\\Lambda}^{1/2}\\) is a diagonal scaling matrix. The resulting covariance is \\(\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}'\\).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n#| echo: false\n\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(shinyWidgets)\nlibrary(munsell) \nlibrary(scales)\nlibrary(tibble)\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# --- 1. PRE-GENERATE FIXED Z POINTS ---\nset.seed(123)\nz_fixed &lt;- matrix(rnorm(50 * 2), ncol = 2)\n\nui &lt;- page_fillable(\n  theme = bs_theme(version = 5),\n  withMathJax(), \n  \n  # --- ROW 1: CONTROLS (Compact Strip) ---\n  card(\n    class = \"p-2\", \n    layout_columns(\n      col_widths = c(3, 2, 2, 2, 2),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\theta$$\")), \n          noUiSliderInput(\"theta\", label = NULL, min = 0, max = 360, value = 0, step = 5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#0d6efd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_1}$$\")), \n          noUiSliderInput(\"L1\", label = NULL, min = 0.5, max = 3, value = 2, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#ffc107\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_2}$$\")), \n          noUiSliderInput(\"L2\", label = NULL, min = 0.5, max = 3, value = 1, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#adb5bd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_1$$\")), \n          noUiSliderInput(\"mu1\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_2$$\")), \n          noUiSliderInput(\"mu2\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\"))\n    )\n  ),\n\n  # --- ROW 2: SIDE-BY-SIDE (Plot & Math) ---\n  layout_columns(\n    col_widths = c(8, 4), # 2/3 for Plot, 1/3 for Matrix\n    \n    # Left: Visualization\n    card(\n      full_screen = TRUE,\n      plotOutput(\"contourPlot\", height = \"500px\")\n    ),\n    \n    # Right: The Math (Larger Font)\n    card(\n      class = \"p-3 d-flex justify-content-center\", # Center content vertically\n      h5(\"Algebraic Representation\", class = \"mb-3 text-center\"),\n      \n      # Use CSS to make the font larger and monospaced\n      div(\n        style = \"font-family: 'Courier New', monospace; font-size: 1.1rem; line-height: 1.4;\",\n        verbatimTextOutput(\"matrixSide\", placeholder = TRUE)\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n\n  data &lt;- reactive({\n    theta_rad &lt;- input$theta * pi / 180\n    Q &lt;- matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), 2, 2)\n    Lam_sqrt &lt;- diag(c(input$L1, input$L2))\n    \n    A &lt;- Q %*% Lam_sqrt\n    Sigma &lt;- A %*% t(A)\n    mu_vec &lt;- c(input$mu1, input$mu2)\n    \n    x_points &lt;- z_fixed %*% t(A)\n    x_points[,1] &lt;- x_points[,1] + mu_vec[1]\n    x_points[,2] &lt;- x_points[,2] + mu_vec[2]\n    \n    list(Q=Q, L=c(input$L1, input$L2), mu=mu_vec, Sigma=Sigma, A=A, points=as.data.frame(x_points))\n  })\n\n  output$matrixSide &lt;- renderText({\n    M &lt;- data()\n    A &lt;- round(M$A, 2)\n    S &lt;- round(M$Sigma, 2)\n    rho &lt;- cov2cor(M$Sigma)[1,2]\n    \n    # Formatted to fill vertical space comfortably\n    paste0(\n      \"Linear Transform:\\n\",\n      \"x = A z + μ\\n\\n\",\n      \n      \"Matrix A:\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[1,1], A[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[2,1], A[2,2]),\n      \"\\n\",\n      \n      \"Covariance Σ:\\n\",\n      \"(Σ = AA')\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[1,1], S[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[2,1], S[2,2]),\n      \"\\n\",\n      \n      \"Correlation:\\n\",\n      sprintf(\"ρ = %.3f\", rho)\n    )\n  })\n\n  output$contourPlot &lt;- renderPlot({\n    req(data())\n    M &lt;- data()\n    \n    grid_r &lt;- seq(-6, 6, length.out = 60)\n    df_grid &lt;- expand.grid(x = grid_r, y = grid_r)\n    df_grid$z &lt;- dmvnorm(as.matrix(df_grid), mean = M$mu, sigma = M$Sigma)\n    \n    v1 &lt;- M$Q[,1] * M$L[1]; v2 &lt;- M$Q[,2] * M$L[2]\n    axes &lt;- tibble(x = M$mu[1], y = M$mu[2],\n                   xend1 = M$mu[1] + v1[1], yend1 = M$mu[2] + v1[2],\n                   xend2 = M$mu[1] + v2[1], yend2 = M$mu[2] + v2[2])\n    \n    ggplot() +\n      geom_contour_filled(data = df_grid, aes(x, y, z = z), bins = 9, show.legend = FALSE) +\n      geom_point(data = M$points, aes(V1, V2), color = \"black\", size = 2, alpha = 0.7) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend1, yend=yend1), \n                   color = \"#ffc107\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend2, yend=yend2), \n                   color = \"white\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      coord_fixed(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      theme_minimal() +\n      labs(x = \"X\", y = \"Y\")\n  })\n}\n\nshinyApp(ui, server)\n\n\n2.4.3 Probability Density Function\nIf \\(\\Sigma\\) is positive definite, the PDF exists. We use the change of variable formula for \\(x = Az + \\mu\\): \\[\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n\\] where \\(z = A^{-1}(x - \\mu)\\) and \\(J = \\det(A^{-1}) = |A|^{-1}\\).\n\\[\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n\\]\nUsing \\(|\\Sigma| = |AA^T| = |A|^2\\) and \\(\\Sigma^{-1} = (AA^T)^{-1}\\), we get: \\[\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n\\]\n\n\n2.4.4 Moment Generating Function\n\nDefinition 2.8 (Moment Generating Function (MGF)) The MGF of a random vector \\(x\\) is \\(M_x(t) = E(e^{t^T x})\\). For \\(x = Az + \\mu\\): \\[\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n\\] Since \\(M_z(u) = e^{u^T u / 2}\\): \\[\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\n\nKey Properties:\n\nUniqueness: Two random vectors with the same MGF have the same distribution.\nIndependence: \\(y_1\\) and \\(y_2\\) are independent iff \\(M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#construction-and-linear-transformations",
    "href": "lec3-mvn.html#construction-and-linear-transformations",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.5 Construction and Linear Transformations",
    "text": "2.5 Construction and Linear Transformations\n\nTheorem 2.1 (Constructing MVN Random Vector) Let \\(\\mu \\in \\mathbb{R}^n\\) and \\(\\Sigma\\) be an \\(n \\times n\\) symmetric non-negative definitive (n.n.d) matrix. Then there exists a multivariate normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\).\n\n\nProof. Since \\(\\Sigma\\) is n.n.d., there exists \\(B\\) such that \\(\\Sigma = BB^T\\) (e.g., via Cholesky or Spetral Decomposition). Let \\(z \\sim N_n(0, I)\\) and define \\(x = Bz + \\mu\\).\n\n\nTheorem 2.2 (Linear Transformation Theorem) Let \\(x \\sim N_n(\\mu, \\Sigma)\\). Let \\(y = Cx + d\\) where \\(C\\) is \\(r \\times n\\) and \\(d\\) is \\(r \\times 1\\). Then: \\[\ny \\sim N_r(C\\mu + d, C \\Sigma C^T)\n\\]\n\n\nProof. \\(x = Az + \\mu\\) where \\(AA^T = \\Sigma\\). \\[\ny = C(Az + \\mu) + d = (CA)z + (C\\mu + d)\n\\] This fits the definition of MVN with mean \\(C\\mu + d\\) and variance \\(C \\Sigma C^T\\).\n\n\n2.5.1 Important Corollaries of Theorem 2.2\n\nCorollary 2.1 (Marginals) Any subvector of a multivariate normal vector is also multivariate normal.\n\n\nProof. If we partition \\(x = (x_1', x_2')'\\), we can use \\(C = (I_r, 0)\\) to show \\(x_1 \\sim N(\\mu_1, \\Sigma_{11})\\).\n\n\nCorollary 2.2 (Univariate Combinations) Any linear combination \\(a^T x\\) is univariate normal: \\[\na^T x \\sim N(a^T \\mu, a^T \\Sigma a)\n\\]\n\n\nCorollary 2.3 (Orthogonal Transformations) If \\(x \\sim N(0, I_n)\\) and \\(Q\\) is orthogonal (\\(Q'Q = I\\)), then \\(y = Q'x \\sim N(0, I_n)\\).\n\n\nCorollary 2.4 (Standardization) If \\(y \\sim N_n(\\mu, \\Sigma)\\) and \\(\\Sigma\\) is positive definite: \\[\n\\Sigma^{-1/2}(y - \\mu) \\sim N_n(0, I_n)\n\\]\n\n\nProof. Let \\(z = \\Sigma^{-1/2}(y - \\mu)\\). Then \\(\\text{Var}(z) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = I_n\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#independence",
    "href": "lec3-mvn.html#independence",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.6 Independence",
    "text": "2.6 Independence\n\nTheorem 2.3 (Independence in MVN) Let \\(y \\sim N(\\mu, \\Sigma)\\) be partitioned into \\(y_1\\) and \\(y_2\\). \\[\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n\\] Then \\(y_1\\) and \\(y_2\\) are independent if and only if \\(\\Sigma_{12} = 0\\) (zero covariance).\n\n\nProof. 1. Independence \\(\\implies\\) Covariance is 0: This holds generally for any distribution. \\[\n\\text{Cov}(y_1, y_2) = E[(y_1 - \\mu_1)(y_2 - \\mu_2)'] = 0\n\\]\n2. Covariance is 0 \\(\\implies\\) Independence: This is specific to MVN. We use MGFs. If \\(\\Sigma_{12} = 0\\), the quadratic form in the MGF splits: \\[\nt^T \\Sigma t = t_1^T \\Sigma_{11} t_1 + t_2^T \\Sigma_{22} t_2\n\\] The MGF becomes: \\[\nM_y(t) = \\exp(t_1^T \\mu_1 + \\frac{1}{2} t_1^T \\Sigma_{11} t_1) \\times \\exp(t_2^T \\mu_2 + \\frac{1}{2} t_2^T \\Sigma_{22} t_2)\n\\] \\[\nM_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\n\\] Thus, they are independent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#signal-noise-decomposition-for-multivariate-normal-distribution",
    "href": "lec3-mvn.html#signal-noise-decomposition-for-multivariate-normal-distribution",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.7 Signal-Noise Decomposition for Multivariate Normal Distribution",
    "text": "2.7 Signal-Noise Decomposition for Multivariate Normal Distribution\nWe can formalize the relationship between two random vectors \\(y\\) and \\(x\\) through a decomposition theorem that separates the systematic signal from the stochastic noise.\n\nTheorem 2.4 (Regression Decomposition Theorem) Let the random vector \\(V\\) of dimension \\(p \\times 1\\) be partitioned into two subvectors \\(y\\) (\\(p_1 \\times 1\\)) and \\(x\\) (\\(p_2 \\times 1\\)). Assume \\(V\\) follows a multivariate normal distribution:\n\\[\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N_p\\left( \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\right)\n\\]\nThe response vector \\(y\\) can be uniquely decomposed into a systematic component and a stochastic error: \\[\ny = m(x) + e\n\\] where we define the Regression Coefficient Matrix \\(B\\) and the components as:\n\\[\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1}\n\\]\n\\[\nm(x) = \\mu_y + B(x - \\mu_x)\n\\]\n\\[\ne = y - m(x)\n\\]\nProperties:\n\nIndependence: The noise vector \\(e\\) is statistically independent of the predictor \\(x\\) (and consequently independent of \\(m(x)\\)).\nMarginal Distributions:\n\n\\(m(x) \\sim N_{p_1}(\\mu_y, \\; B \\Sigma_{xx} B^T)\\)\n\\(e \\sim N_{p_1}(0, \\; \\Sigma_{yy} - B \\Sigma_{xx} B^T)\\)\n\nConditional Distribution: Since \\(y = m(x) + e\\), and \\(e\\) is independent of \\(x\\), the conditional distribution is: \\[\ny | x \\sim N_{p_1}(m(x), \\Sigma_{y|x})\n\\] where: \\[\nm(x) = \\mu_y + B(x - \\mu_x) = \\mu_y + \\Sigma_{yx}\\Sigma_{xx}^{-1}(x - \\mu_x)\n\\] \\[\n\\Sigma_{y|x} = \\Sigma_{yy} - B \\Sigma_{xx} B^T = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\n\\]\n\n\n\nProof. We define a transformation from the input vector \\(V = \\begin{pmatrix} y \\\\ x \\end{pmatrix}\\) to the target vector \\(W = \\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}\\).\nUsing the linear transformation \\(W = CV + d\\):\n\\[\n\\underbrace{\\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}}_{W} = \\underbrace{\\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix}}_{C} \\underbrace{\\begin{pmatrix} y \\\\ x \\end{pmatrix}}_{V} + \\underbrace{\\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -(\\mu_y - B \\mu_x) \\end{pmatrix}}_{d}\n\\]\n1. Mean Vector\n\\[\nE[W] = C E[V] + d = \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix}\n= \\begin{pmatrix} B \\mu_x \\\\ \\mu_y - B \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix}\n= \\begin{pmatrix} \\mu_y \\\\ 0 \\end{pmatrix}\n\\]\n2. Covariance Matrix\nWe compute \\(\\text{Var}(W) = C \\Sigma C^T\\) directly:\n\\[\n\\begin{aligned}\nC \\Sigma C^T &= \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xy} & B \\Sigma_{xx} \\\\ \\Sigma_{yy} - B \\Sigma_{xy} & \\Sigma_{yx} - B \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & B \\Sigma_{xy} - B \\Sigma_{xx} B^T \\\\ \\Sigma_{yx}B^T - B \\Sigma_{xx} B^T & (\\Sigma_{yy} - B \\Sigma_{xy}) - (\\Sigma_{yx} - B \\Sigma_{xx})B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & 0 \\\\ 0 & \\Sigma_{yy} - B \\Sigma_{xx} B^T \\end{pmatrix}\n\\end{aligned}\n\\]\n3. Conditional Distribution\nWe have established that \\(y = m(x) + e\\) where \\(e\\) is independent of \\(x\\). To find the distribution of \\(y\\) conditional on \\(x\\), we observe that \\(m(x)\\) becomes a constant vector when \\(x\\) is fixed, and the randomness comes solely from \\(e\\):\n\\[\nE[y|x] = m(x) + E[e|x] = m(x) + 0 = m(x)\n\\] \\[\n\\text{Var}(y|x) = \\text{Var}(m(x)|x) + \\text{Var}(e|x) = 0 + \\text{Var}(e) = \\Sigma_{y|x}\n\\]\nThus, \\(y | x \\sim N(m(x), \\Sigma_{y|x})\\).\n\n\n2.7.1 Connections with Other Formulas\n\n2.7.1.1 Rao-Blackwell Decomposition of Variance\nThe Law of Total Variance (Rao-Blackwell theorem) allows us to decompose the total variance of \\(y\\) into two orthogonal components based on the predictor \\(x\\):\n\\[\n\\text{Var}(y) = \\underbrace{E[\\text{Var}(y | x)]}_{\\text{Unexplained (Noise)}} + \\underbrace{\\text{Var}[E(y | x)]}_{\\text{Explained (Signal)}}\n\\]\nIn the Multivariate Normal case, this decomposition perfectly aligns with our regression model \\(y = m(x) + e\\).\n\n\nVariance of Noise\nThis term represents the average variance remaining in \\(y\\) after accounting for \\(x\\). It corresponds to the variance of the error term \\(e\\):\n\\[\nE[\\text{Var}(y | x)] = \\text{Var}(e) = \\Sigma_{yy} - B \\Sigma_{xx} B^T\n\\]\n\n\nVariance of Signal\nThis term represents the variability of the conditional mean \\(m(x)\\) itself. Using the matrix \\(B\\), this takes the quadratic form:\n\\[\n\\text{Var}[E(y | x)] = \\text{Var}[m(x)] = B \\Sigma_{xx} B^T\n\\]\n\n\nTotal Variance\nSumming the Signal and Noise components recovers the total marginal variance of \\(y\\):\n\\[\n\\Sigma_{yy} = \\underbrace{\\Sigma_{yy} - B \\Sigma_{xx} B^T}_{\\text{Unexplained (Noise)}} + \\underbrace{B \\Sigma_{xx} B^T}_{\\text{Explained (Signal)}}\n\\]\n\n\n2.7.1.2 Connection to OLS Regression Estimators\nIn OLS regression, centering the data allows us to separate the intercept from the slopes. Let \\(\\mathbf{y}_c\\) and \\(\\mathbf{X}_c\\) be the centered response and design matrices (where \\(\\mathbf{X}_c\\) excludes the column of 1s). Using this centered form, the total sum of squares decomposes exactly like the population variance:\n\\[\n\\text{SST} = \\text{SSR} + \\text{SSE}\n\\]\nComparing the sample quantities to their population counterparts:\n\nRegression Coefficients: \\[\n  \\hat{\\beta}^T = (\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{y}_c \\approx B\n  \\] Note: \\(\\hat{\\beta}\\) here represents only the slope coefficients, matching the dimensions of the covariance matrix \\(\\Sigma_{xx}\\).\nExplained Variation (Signal): \\[\n\\text{SSR} = \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1) B \\Sigma_{xx} B^T\n\\]\nUnexplained Variation (Noise): \\[\n\\text{SSE} = \\mathbf{y}_c^T \\mathbf{y}_c - \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1)(\\Sigma_{yy} - B \\Sigma_{xx} B^T)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#partial-and-multiple-correlation",
    "href": "lec3-mvn.html#partial-and-multiple-correlation",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.8 Partial and Multiple Correlation",
    "text": "2.8 Partial and Multiple Correlation\n\nDefinition 2.9 (Partial Correlation) The partial correlation between elements \\(y_i\\) and \\(y_j\\) given a set of variables \\(x\\) is derived from the conditional covariance matrix \\(\\Sigma_{y|x}\\): \\[\n\\rho_{ij|x} = \\frac{\\sigma_{ij|x}}{\\sqrt{\\sigma_{ii|x} \\sigma_{jj|x}}}\n\\] where \\(\\sigma_{ij|x}\\) are elements of \\(\\Sigma_{y|x} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\\).\n\n\nDefinition 2.10 (Multiple Correlation (\\(R^2\\))) For a scalar \\(y\\) and vector \\(x\\), the squared multiple correlation is the proportion of variance of \\(y\\) explained by the conditional mean: \\[\nR^2_{y|x} = \\frac{\\text{Var}(E(y|x))}{\\text{Var}(y)} = \\frac{\\Sigma_{yx} \\Sigma_{xx}^{-1} \\Sigma_{xy}}{\\sigma^2_{y}}\n\\]\n\nNote: this definition is the population or theretical \\(R^2\\), which is estimated by adjusted \\(R^2\\) using sample in linear regression.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec3-mvn.html#examples",
    "href": "lec3-mvn.html#examples",
    "title": "2  Multivariate Normal Distribution",
    "section": "2.9 Examples",
    "text": "2.9 Examples\n\nExample 2.1 (Bivariate Normal) Let the random vector \\(\\begin{pmatrix} y \\\\ x \\end{pmatrix}\\) follow a bivariate normal distribution: \\[\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 2 \\\\ 2 & 4 \\end{pmatrix} \\right)\n\\] Here, \\(\\mu_y = 1, \\mu_x = 2, \\Sigma_{yy} = 2, \\Sigma_{xx} = 4\\), and \\(\\Sigma_{yx} = 2\\).\n1. Finding the Regression Coefficient Matrix \\(B\\) Using the population formula: \\[\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1} = 2(4)^{-1} = 0.5\n\\]\n2. Finding the Conditional Mean \\(m(x)\\) (The Signal) The systematic component represents the projection of \\(y\\) onto \\(x\\): \\[\n\\begin{aligned}\nm(x) &= \\mu_y + B(x - \\mu_x) \\\\\n&= 1 + 0.5(x - 2) = 0.5x\n\\end{aligned}\n\\]\n3. Variance of the Signal \\(\\text{Var}(m(x))\\) Using the quadratic form established in the theorem: \\[\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = 0.5(4)(0.5) = 1\n\\]\n4. Variance of the Noise \\(\\text{Var}(y|x)\\) (The Residual) By the Signal-Noise Decomposition: \\[\n\\begin{aligned}\n\\text{Var}(y|x) &= \\Sigma_{yy} - \\text{Var}(m(x)) \\\\\n&= 2 - 1 = 1\n\\end{aligned}\n\\] Thus, \\(y | x \\sim N(m(x), 1)\\). The total variance (2) is split equally between signal (1) and noise (1).\n5. Multiple Correlation Coefficient (\\(R^2\\)) \\[\nR^2 = \\frac{\\text{Var}(m(x))}{\\Sigma_{yy}} = \\frac{1}{2} = 0.5\n\\]\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of Rao-Blackwell Variance Decomposition in Bivariate Normal\n\n\n\n\n\n\n\nExample 2.2 (Trivariate Normal with 2 Predictors) Let \\(V = (y, x_1, x_2)' \\sim N_3(\\mu, \\Sigma)\\) with: \\[\n\\mu = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 10 & 3 & 4 \\\\ 3 & 2 & 1 \\\\ 4 & 1 & 4 \\end{pmatrix}\n\\] We partition these into \\(\\Sigma_{yy} = 10\\), \\(\\Sigma_{yx} = \\begin{pmatrix} 3 & 4 \\end{pmatrix}\\), and \\(\\Sigma_{xx} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix}\\).\n1. Finding the Regression Coefficient Matrix \\(B\\) \\[\n\\Sigma_{xx}^{-1} = \\frac{1}{7} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2 \\end{pmatrix} \\implies B = \\Sigma_{yx} \\Sigma_{xx}^{-1} = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix}\n\\]\n2. Finding the Conditional Mean \\(m(x)\\) (The Signal) \\[\nm(x) = 1 + \\frac{8}{7}(x_1 - 2) + \\frac{5}{7}(x_2 - 3)\n\\]\n3. Variance of the Signal \\(\\text{Var}(m(x))\\) \\[\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{44}{7} \\approx 6.29\n\\]\n4. Variance of the Noise \\(\\text{Var}(y|x)\\) (The Residual) Using the Signal-Noise Decomposition: \\[\n\\Sigma_{y|x} = \\Sigma_{yy} - \\text{Var}(m(x)) = 10 - 6.29 = 3.71\n\\]\n5. Multiple Correlation Coefficient (\\(R^2\\)) \\[\nR^2 = \\frac{6.29}{10} = 0.629\n\\]\n\n\nCode\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\nmu &lt;- c(1, 2, 3)\nsigma &lt;- matrix(c(10, 3, 4, 3, 2, 1, 4, 1, 4), nrow=3, byrow=TRUE)\n\nvar_total &lt;- sigma[1,1]\nS_yx &lt;- matrix(sigma[1, 2:3], nrow=1)\nS_xx &lt;- sigma[2:3, 2:3]\nB_mat &lt;- S_yx %*% solve(S_xx)\nvar_signal &lt;- as.numeric(B_mat %*% S_xx %*% t(B_mat))\nvar_noise &lt;- var_total - var_signal\n\nset.seed(2024)\ndat &lt;- rmvnorm(1000, mean=mu, sigma=sigma)\ndf &lt;- data.frame(y=dat[,1], x1=dat[,2], x2=dat[,3])\ndf$m_x &lt;- 1 + (8/7)*(df$x1 - 2) + (5/7)*(df$x2 - 3)\n\nlimit_min &lt;- -12\nlimit_max &lt;- 12\nseq_vals &lt;- seq(limit_min, limit_max, length.out=300)\nscale_factor &lt;- 20 \n\ndf_total &lt;- data.frame(y = seq_vals, x = 9 + dnorm(seq_vals, 1, sqrt(var_total)) * scale_factor)\ndf_signal &lt;- data.frame(x = seq_vals, y = -8 - dnorm(seq_vals, 1, sqrt(var_signal)) * scale_factor)\ndf_noise &lt;- data.frame(y = seq_vals, x = 5 + dnorm(seq_vals, 5, sqrt(var_noise)) * scale_factor)\n\nggplot(df, aes(x=m_x, y=y)) +\n  geom_abline(intercept=0, slope=1, color=\"red\", linewidth=0.5, alpha=0.3) +\n  geom_point(alpha=0.15, size=1.5, color=\"black\") +\n  geom_polygon(data=df_signal, aes(x=x, y=y), fill=\"red\", alpha=0.3) +\n  geom_path(data=df_signal, aes(x=x, y=y), color=\"red\", linewidth=1) +\n  annotate(\"text\", x=1, y=-11, label=\"Signal Var\\n(m(x))\", color=\"red\", size=3, fontface=\"bold\") +\n  geom_polygon(data=df_total, aes(x=x, y=y), fill=\"gray40\", alpha=0.3) +\n  geom_path(data=df_total, aes(x=x, y=y), color=\"gray40\", linewidth=1) +\n  annotate(\"text\", x=11, y=6, label=\"Total Var\\n(y)\", color=\"gray40\", size=3, fontface=\"bold\") +\n  geom_polygon(data=df_noise, aes(x=x, y=y), fill=\"blue\", alpha=0.3) +\n  geom_path(data=df_noise, aes(x=x, y=y), color=\"blue\", linewidth=1) +\n  annotate(\"text\", x=6, y=9, label=\"Noise Var\\n(y|x)\", color=\"blue\", size=3, fontface=\"bold\") +\n  scale_x_continuous(limits=c(-12, 14)) + scale_y_continuous(limits=c(-14, 12)) +\n  coord_fixed(ratio=1) + labs(x = \"Signal m(x)\", y = \"Observed y\") + theme_minimal()\n\n\n\n\n\n\n\n\nFigure 2.2: Signal-Noise Variance Decomposition in Multivariate Normal\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\nx1_seq &lt;- seq(min(df$x1), max(df$x1), length.out=20)\nx2_seq &lt;- seq(min(df$x2), max(df$x2), length.out=20)\ngrid &lt;- expand.grid(x1=x1_seq, x2=x2_seq)\ngrid$y_pred &lt;- 1 + (8/7)*(grid$x1 - 2) + (5/7)*(grid$x2 - 3)\nz_matrix &lt;- matrix(grid$y_pred, nrow=20, ncol=20)\n\nplot_ly() %&gt;%\n  add_markers(data = df, x = ~x1, y = ~x2, z = ~y,\n              marker = list(size = 3, color = '#444', opacity = 0.5),\n              name = \"Observed (Total)\") %&gt;%\n  add_surface(x = x1_seq, y = x2_seq, z = z_matrix,\n              opacity = 0.6, colorscale = list(c(0, 1), c(\"red\", \"red\")),\n              showscale = FALSE, name = \"Signal (m(x))\") %&gt;%\n  layout(scene = list(xaxis = list(title = \"x1\"), yaxis = list(title = \"x2\"), zaxis = list(title = \"y\")))\n\n\n\n\n\n\n\n\nFigure 2.3: Interactive 3D Plot: Signal Plane vs Noise",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html",
    "href": "lec4-qf.html",
    "title": "3  Distribution of Quadratic Forms",
    "section": "",
    "text": "3.1 Introduction to Quadratic Forms\nThis chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.\nA quadratic form is a polynomial with terms all of degree two.\nExamples:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#introduction-to-quadratic-forms",
    "href": "lec4-qf.html#introduction-to-quadratic-forms",
    "title": "3  Distribution of Quadratic Forms",
    "section": "",
    "text": "Definition 3.1 (Quadratic Form) Let \\(y = (y_1, \\dots, y_n)'\\) be a random vector and \\(A\\) be a symmetric \\(n \\times n\\) matrix. The scalar quantity \\(y'Ay\\) is called a quadratic form in \\(y\\).\n\\[\ny'Ay = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} y_i y_j\n\\]\n\n\n\nSquared Norm: If \\(A = I_n\\), then \\(y'I_n y = y'y = \\sum y_i^2 = ||y||^2\\).\nWeighted Sum of Squares: If \\(A\\) is diagonal with elements \\(\\lambda_i\\), then \\(y'Ay = \\sum \\lambda_i y_i^2\\).\nProjection Sum of Squares: If \\(P\\) is a projection matrix, \\(||Py||^2 = (Py)'(Py) = y'P'Py = y'Py\\) (since \\(P\\) is symmetric and idempotent).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#mean-of-quadratic-form",
    "href": "lec4-qf.html#mean-of-quadratic-form",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.2 Mean of Quadratic Form",
    "text": "3.2 Mean of Quadratic Form\nWe can find the expected value of a quadratic form without assuming normality.\n\nTheorem 3.1 (Mean of Quadratic Form) If \\(y\\) is a random vector with mean \\(E(y) = \\mu\\) and covariance matrix \\(\\text{Var}(y) = \\Sigma\\), and \\(A\\) is a symmetric matrix of constants, then:\n\\[\nE(y'Ay) = \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\]\n\n\nProof. Using the trace trick (scalar equals its trace) and linearity of expectation: \\[\n\\begin{aligned}\nE(y'Ay) &= E[\\text{tr}(y'Ay)] = E[\\text{tr}(Ayy')] = \\text{tr}(A E[yy']) \\\\\n\\text{Since } \\Sigma &= E(yy') - \\mu\\mu', \\text{ we have } E(yy') = \\Sigma + \\mu\\mu' \\\\\nE(y'Ay) &= \\text{tr}(A(\\Sigma + \\mu\\mu')) = \\text{tr}(A\\Sigma) + \\text{tr}(A\\mu\\mu') \\\\\n&= \\text{tr}(A\\Sigma) + \\text{tr}(\\mu'A\\mu) = \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\end{aligned}\n\\]\nAlternatively, using scalar summation: \\[\nE(\\sum_{i,j} a_{ij} y_i y_j) = \\sum_{i,j} a_{ij} E(y_i y_j) = \\sum_{i,j} a_{ij} (\\sigma_{ij} + \\mu_i \\mu_j) = \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#example-sample-variance",
    "href": "lec4-qf.html#example-sample-variance",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.3 Example: Sample Variance",
    "text": "3.3 Example: Sample Variance\nConsider a random sample \\(x_1, \\dots, x_n\\) with \\(E(x_i) = \\mu\\) and \\(\\text{Var}(x_i) = \\sigma^2\\). Let \\(x = (x_1, \\dots, x_n)'\\). Then \\(E(x) = \\mu J_n\\) (where \\(J_n\\) is a vector of ones) and \\(\\text{Var}(x) = \\sigma^2 I_n\\).\nWe want to find the expectation of the sample variance (related to Sum of Squared Errors). The sample variance involves the sum of squared deviations from the mean: \\(\\sum (x_i - \\bar{x})^2\\).\nThis can be written using the projection matrix \\(H = \\frac{1}{n} J_n J_n'\\) (often called the “hat matrix” for the mean model). \\(\\bar{x} J_n = H x\\). \\(x - \\bar{x} J_n = (I_n - H)x\\).\n\\[\n\\sum (x_i - \\bar{x})^2 = ||(I_n - H)x||^2 = x'(I_n - H)'(I_n - H)x = x'(I_n - H)x\n\\] (Since \\(I_n - H\\) is a symmetric idempotent projection matrix).\nUsing Theorem 3.1 with \\(A = I_n - H\\): \\[\n\\begin{aligned}\nE(x'(I_n - H)x) &= \\text{tr}((I_n - H)\\sigma^2 I_n) + (\\mu J_n)'(I_n - H)(\\mu J_n) \\\\\n&= \\sigma^2 \\text{tr}(I_n - H) + \\mu^2 J_n' (I_n - H) J_n\n\\end{aligned}\n\\]\n\n\\(\\text{tr}(I_n - H) = \\text{tr}(I_n) - \\text{tr}(H) = n - 1\\).\n\\((I_n - H)J_n = J_n - H J_n = J_n - \\frac{1}{n} J_n (J_n' J_n) = J_n - J_n = 0\\) (The residuals of the mean are orthogonal to the mean).\n\nThus: \\[\nE(\\sum (x_i - \\bar{x})^2) = \\sigma^2 (n - 1) + 0 = (n-1)\\sigma^2\n\\]\nThis confirms that \\(S^2 = \\frac{1}{n-1} \\sum (x_i - \\bar{x})^2\\) is an unbiased estimator of \\(\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#non-central-chi-square-distribution",
    "href": "lec4-qf.html#non-central-chi-square-distribution",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.4 Non-central Chi-square Distribution",
    "text": "3.4 Non-central Chi-square Distribution\nTo understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.\n\nDefinition 3.2 (Non-central Chi-square Distribution) Let \\(x \\sim N_n(\\mu, I_n)\\). The random variable \\(Y = x'x = \\sum x_i^2\\) follows a non-central chi-square distribution with \\(n\\) degrees of freedom and non-centrality parameter \\(\\lambda\\).\n\\[\nY \\sim \\chi^2(n, \\lambda) \\quad \\text{where } \\lambda = \\frac{1}{2} \\mu'\\mu = \\frac{1}{2} ||\\mu||^2\n\\]\n\nNote: Some definitions use \\(\\lambda = \\mu'\\mu\\). In this course, we use \\(\\lambda = \\frac{1}{2}\\mu'\\mu\\).\nHere is a plot visualizing the difference between central and non-central Chi-square distributions.\n\n\n\n\n\nCentral vs Non-central Chi-square Distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#properties-of-non-central-chi-square",
    "href": "lec4-qf.html#properties-of-non-central-chi-square",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.5 Properties of Non-central Chi-square",
    "text": "3.5 Properties of Non-central Chi-square\n\nTheorem 3.2 (Mean and Variance) If \\(Y \\sim \\chi^2(n, \\lambda)\\), then:\n\n\\(E(Y) = n + 2\\lambda = n + ||\\mu||^2\\)\n\\(\\text{Var}(Y) = 2n + 8\\lambda\\)\n\n\n\nProof (Mean). Let \\(x = z + \\mu\\), where \\(z \\sim N(0, I_n)\\). \\[\n\\begin{aligned}\nY = ||x||^2 &= ||z + \\mu||^2 = ||z||^2 + ||\\mu||^2 + 2\\mu'z \\\\\nE(Y) &= E(||z||^2) + ||\\mu||^2 + 2\\mu'E(z) \\\\\n&= n + ||\\mu||^2 + 0 \\\\\n&= n + 2\\lambda\n\\end{aligned}\n\\] (Since \\(||z||^2 \\sim \\chi^2_n\\) central, its mean is \\(n\\)).\n\nAdditivity: If \\(V_1, \\dots, V_k\\) are independent with \\(V_i \\sim \\chi^2(n_i, \\lambda_i)\\), then: \\[\n\\sum V_i \\sim \\chi^2(\\sum n_i, \\sum \\lambda_i)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#visualizing-chi-square-distributions",
    "href": "lec4-qf.html#visualizing-chi-square-distributions",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.6 Visualizing Chi-square Distributions",
    "text": "3.6 Visualizing Chi-square Distributions\nThe density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter \\(\\lambda\\) increases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#mean-variance-and-mgf",
    "href": "lec4-qf.html#mean-variance-and-mgf",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.7 Mean, Variance, and MGF",
    "text": "3.7 Mean, Variance, and MGF\nWe summarize the key properties of the non-central chi-square distribution.\n\nTheorem 3.3 (Properties of Non-central Chi-square) Let \\(Y \\sim \\chi^2(n, \\lambda)\\). Then:\n\nMean: \\(E(Y) = n + 2\\lambda\\)\nVariance: \\(\\text{Var}(Y) = 2n + 8\\lambda\\)\nMoment Generating Function (MGF): \\[\nm_Y(t) = \\frac{\\exp[-\\lambda \\{1 - 1/(1-2t)\\}]}{(1-2t)^{n/2}}\n\\]\n\n\n\nProof (Mean). We derived \\(E(Y) = n + 2\\lambda\\) in the previous section using the decomposition \\(Y = ||z + \\mu||^2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#additivity",
    "href": "lec4-qf.html#additivity",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.8 Additivity",
    "text": "3.8 Additivity\n\nTheorem 3.4 (Additivity of Chi-square) If \\(v_1, \\dots, v_k\\) are independent random variables distributed as \\(\\chi^2(n_i, \\lambda_i)\\), then their sum follows a chi-square distribution:\n\\[\n\\sum_{i=1}^k v_i \\sim \\chi^2\\left(\\sum_{i=1}^k n_i, \\sum_{i=1}^k \\lambda_i\\right)\n\\]\n\n\nProof. Method 1: Using MGFs\nThe moment generating function of \\(v_i \\sim \\chi^2(n_i, \\lambda_i)\\) is: \\[\nM_{v_i}(t) = \\frac{\\exp\\left[-\\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}}\n\\]\nSince \\(v_1, \\dots, v_k\\) are independent, the MGF of their sum \\(V = \\sum v_i\\) is the product of their individual MGFs:\n\\[\n\\begin{aligned}\nM_V(t) &= \\prod_{i=1}^k M_{v_i}(t) \\\\\n&= \\prod_{i=1}^k \\frac{\\exp\\left[-\\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}} \\\\\n&= \\frac{\\exp\\left[-\\sum \\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{\\sum n_i/2}}\n\\end{aligned}\n\\]\nThis is the MGF of a non-central chi-square distribution with degrees of freedom \\(\\sum n_i\\) and non-centrality parameter \\(\\sum \\lambda_i\\).\nMethod 2: Geometric Interpretation\nLet \\(v_i = ||y_i||^2\\) where \\(y_i \\sim N_{n_i}(\\mu_i, I_{n_i})\\). Since the vectors \\(y_i\\) are independent, we can stack them into a larger vector \\(y = (y_1', \\dots, y_k')'\\).\n\\[\ny \\sim N_{\\sum n_i}(\\mu, I_{\\sum n_i}) \\quad \\text{where } \\mu = (\\mu_1', \\dots, \\mu_k')'\n\\]\nThe sum of squares is: \\[\n\\sum v_i = \\sum ||y_i||^2 = ||y||^2\n\\]\nBy definition, \\(||y||^2\\) follows a non-central chi-square distribution with degrees of freedom equal to the dimension of \\(y\\) (\\(\\sum n_i\\)) and non-centrality parameter \\(\\lambda = \\frac{1}{2} ||\\mu||^2\\).\n\\[\n\\lambda = \\frac{1}{2} \\sum_{i=1}^k ||\\mu_i||^2 = \\sum_{i=1}^k \\lambda_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#mgf-of-quadratic-forms",
    "href": "lec4-qf.html#mgf-of-quadratic-forms",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.9 MGF of Quadratic Forms",
    "text": "3.9 MGF of Quadratic Forms\nTo determine the distribution of general quadratic forms \\(y'Ay\\), we look at their MGF.\n\nTheorem 3.5 (MGF of Quadratic Form) If \\(y \\sim N_p(\\mu, \\Sigma)\\), then the MGF of \\(Q = y'Ay\\) is:\n\\[\nM_Q(t) = |I - 2tA\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2} \\mu' [I - (I - 2tA\\Sigma)^{-1}] \\Sigma^{-1} \\mu\\right)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#distribution-of-quadratic-forms",
    "href": "lec4-qf.html#distribution-of-quadratic-forms",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.10 Distribution of Quadratic Forms",
    "text": "3.10 Distribution of Quadratic Forms\nThis is one of the most important theorems in the course, establishing when a quadratic form follows a chi-square distribution.\n\nTheorem 3.6 (Distribution of y’Ay) Let \\(y \\sim N_p(\\mu, \\Sigma)\\). Let \\(A\\) be a symmetric matrix of rank \\(r\\). Then \\(y'Ay \\sim \\chi^2(r, \\lambda)\\) with \\(\\lambda = \\frac{1}{2} \\mu' A \\mu\\) if and only if \\(A\\Sigma\\) is idempotent (\\(A\\Sigma A\\Sigma = A\\Sigma\\)).\nSpecial Case (\\(\\Sigma = I\\)): If \\(\\Sigma = I\\), the condition simplifies to \\(A\\) being idempotent (\\(A^2 = A\\)).\n\n\nCorollary 3.1 (Projection Matrices) If \\(y \\sim N_n(\\mu, \\sigma^2 I_n)\\) and \\(P_V\\) is a projection matrix onto a subspace \\(V\\) of dimension \\(r\\), then:\n\\[\n\\frac{1}{\\sigma^2} y'P_V y = \\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{2\\sigma^2}\\right)\n\\]\nThis holds because \\(\\frac{1}{\\sigma^2} P_V (\\sigma^2 I) = P_V\\), which is idempotent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#proof-of-the-corollary",
    "href": "lec4-qf.html#proof-of-the-corollary",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.11 Proof of the Corollary",
    "text": "3.11 Proof of the Corollary\nWe proving the corollary for the case where \\(y \\sim N(\\mu, I_n)\\) and we are projecting onto a subspace \\(V\\) with dimension \\(r\\).\nLet \\(P_V\\) be the projection matrix. We know \\(P_V = QQ'\\) where \\(Q\\) is \\(n \\times r\\) with orthonormal columns (\\(Q'Q = I_r\\)).\n\\[\ny' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2\n\\]\nSince \\(y \\sim N(\\mu, I_n)\\), the linear transformation \\(z = Q'y\\) follows: \\[\nz \\sim N(Q'\\mu, Q' I_n Q) = N(Q'\\mu, I_r)\n\\]\nThus, \\(z\\) is a vector of \\(r\\) independent normal variables with variance 1. The sum of squares \\(||z||^2\\) is by definition non-central chi-square:\n\\[\n||z||^2 \\sim \\chi^2(r, \\lambda)\n\\] where the non-centrality parameter is: \\[\n\\lambda = \\frac{1}{2} ||E(z)||^2 = \\frac{1}{2} ||Q'\\mu||^2\n\\]\nNote that \\(||Q'\\mu||^2 = \\mu' Q Q' \\mu = \\mu' P_V \\mu = ||P_V \\mu||^2\\).\nThus, \\(y' P_V y \\sim \\chi^2(r, \\frac{1}{2} ||P_V \\mu||^2)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  },
  {
    "objectID": "lec4-qf.html#general-case-with-sigma2",
    "href": "lec4-qf.html#general-case-with-sigma2",
    "title": "3  Distribution of Quadratic Forms",
    "section": "3.12 General Case with \\(\\sigma^2\\)",
    "text": "3.12 General Case with \\(\\sigma^2\\)\nIf \\(y \\sim N(\\mu, \\sigma^2 I_n)\\), we standardize by dividing by \\(\\sigma\\).\nLet \\(z = y/\\sigma\\). Then \\(z \\sim N(\\mu/\\sigma, I_n)\\). Applying the previous result to \\(z\\):\n\\[\nz' P_V z = \\frac{y' P_V y}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{1}{2} \\left|\\left| P_V \\frac{\\mu}{\\sigma} \\right|\\right|^2\\right)\n\\]\nWhich simplifies to: \\[\n\\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{2\\sigma^2}\\right)\n\\]\nImportant Note: The term \\(||P_V y||^2\\) itself is not chi-square; it is a scaled chi-square variable. Its mean is: \\[\nE(||P_V y||^2) = \\sigma^2 \\left(r + \\frac{||P_V \\mu||^2}{\\sigma^2}\\right) = r\\sigma^2 + ||P_V \\mu||^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Distribution of Quadratic Forms</span>"
    ]
  }
]