{
  "hash": "3c97e9a65ccfa5940cd8983e0ac9142e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distribution of Quadratic Forms\"\nformat: \n  pdf: default\n  html: default\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis chapter covers the distribution of quadratic forms (sums of squares), which is crucial for hypothesis testing in linear models.\n\n## Quadratic Forms\n\nA quadratic form is a polynomial with terms all of degree two.\n\n::: {#def-quadratic-form name=\"Quadratic Form\"}\nLet $y = (y_1, \\dots, y_n)'$ be a random vector and $A$ be a symmetric $n \\times n$ matrix. The scalar quantity $y'Ay$ is called a **quadratic form** in $y$.\n\n$$\ny'Ay = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} y_i y_j\n$$\n:::\n\n**Examples:**\n\n* **Squared Norm:** If $A = I_n$, then $y'I_n y = y'y = \\sum y_i^2 = ||y||^2$.\n* **Weighted Sum of Squares:** If $A$ is diagonal with elements $\\lambda_i$, then $y'Ay = \\sum \\lambda_i y_i^2$.\n* **Projection Sum of Squares:** If $P$ is a projection matrix, $||Py||^2 = (Py)'(Py) = y'P'Py = y'Py$ (since $P$ is symmetric and idempotent).\n\n## Mean of Quadratic Forms\n\nWe can find the expected value of a quadratic form without assuming normality.\n\n::: {#lem-simple-qf name=\"Mean of Simplified Quadratic Form\"}\nIf $y$ is a random vector with mean $E(y) = \\mu$ and covariance matrix $\\text{Var}(y) = I_n$, then:\n$$\nE(y'y) = \\text{tr}(I_n) + \\mu'\\mu = n + \\mu'\\mu\n$$\n:::\n\n::: {.proof}\n\nLet us decompose $y$ into its mean and a stochastic component: $y = \\mu + z$, where $E(z) = 0$ and $\\text{Var}(z) = E(zz') = I_n$.\nSubstituting this into the quadratic form:\n$$\n\\begin{aligned}\ny'y &= (\\mu + z)'(\\mu + z) \\\\\n&= \\mu'\\mu + \\mu'z + z'\\mu + z'z \\\\\n&= \\mu'\\mu + 2\\mu'z + z'z\n\\end{aligned}\n$$\nTaking the expectation:\n$$\n\\begin{aligned}\nE(y'y) &= \\mu'\\mu + 2\\mu'E(z) + E(z'z) \\\\\n&= \\mu'\\mu + 0 + E\\left(\\sum_{i=1}^n z_i^2\\right)\n\\end{aligned}\n$$\nSince $\\text{Var}(z_i) = E(z_i^2) - (E(z_i))^2 = 1 - 0 = 1$, we have $E(\\sum z_i^2) = \\sum 1 = n$.\nThus, $E(y'y) = n + \\mu'\\mu$.\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(dplyr)\n\n# --- 1. Setup Data & Parameters ---\nset.seed(42)\nn <- 100\nsigma_val <- 1          \nSigma <- diag(2) * sigma_val^2\n\nmu_orig <- c(5, 6)      # Original Mean\ny_orig  <- c(7, 5)      # Updated Point y\n\n# Generate 100 Points from N(mu, I)\ndata_orig <- mvrnorm(n, mu_orig, Sigma)\n\n# Define Rotation Angles\nangles <- c(0, 70, 180)\n\n# --- 2. Process Data for Each Angle ---\npoints_list <- list()\nvectors_list <- list()\n\nfor (deg in angles) {\n  theta <- deg * pi / 180\n  rot_mat <- matrix(c(cos(theta), -sin(theta), \n                      sin(theta),  cos(theta)), nrow = 2, byrow = TRUE)\n  \n  # A. Rotate Points\n  data_rot <- data_orig %*% t(rot_mat)\n  df_pts <- data.frame(x = data_rot[,1], y = data_rot[,2])\n  df_pts$Angle <- factor(paste0(deg, \"°\"), levels = paste0(angles, \"°\"))\n  points_list[[length(points_list) + 1]] <- df_pts\n  \n  # B. Rotate Vectors (mu and y)\n  mu_rot <- as.vector(rot_mat %*% mu_orig)\n  y_rot  <- as.vector(rot_mat %*% y_orig)\n  \n  df_vec <- data.frame(\n    Angle = factor(paste0(deg, \"°\"), levels = paste0(angles, \"°\")),\n    mu_x = mu_rot[1], mu_y = mu_rot[2],\n    y_x  = y_rot[1],  y_y  = y_rot[2]\n  )\n  vectors_list[[length(vectors_list) + 1]] <- df_vec\n}\n\nall_points  <- do.call(rbind, points_list)\nall_vectors <- do.call(rbind, vectors_list)\n\n# --- 3. Create Circle Data ---\n# Radius Is the Length of Mu\nradius_mu <- sqrt(sum(mu_orig^2))\ncircle_data <- data.frame(\n  x0 = 0, y0 = 0, r = radius_mu\n)\n\n# --- 4. Generate the Plot ---\nggplot() +\n  # 1. Circle through the mu's (Centered at 0,0)\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = radius_mu), \n                       color = \"gray50\", linetype = \"dotted\", size = 0.5) +\n  \n  # 2. Points (Data Cloud)\n  geom_point(data = all_points, aes(x = x, y = y, color = Angle), \n             size = 0.5, alpha = 0.5) +\n  \n  # 3. Vector mu (Origin -> mu)\n  geom_segment(data = all_vectors, \n               aes(x = 0, y = 0, xend = mu_x, yend = mu_y, color = Angle),\n               arrow = arrow(length = unit(0.2, \"cm\")), size = 0.8) +\n  \n  # 4. Vector y (Origin -> y)\n  geom_segment(data = all_vectors, \n               aes(x = 0, y = 0, xend = y_x, yend = y_y, color = Angle),\n               arrow = arrow(length = unit(0.2, \"cm\")), size = 0.8) +\n  \n  # 5. Vector y - mu (mu -> y)\n  geom_segment(data = all_vectors, \n               aes(x = mu_x, y = mu_y, xend = y_x, yend = y_y, color = Angle),\n               arrow = arrow(length = unit(0.15, \"cm\")), \n               linetype = \"dashed\", size = 0.6) +\n  \n  # 6. Labels for mu, y, and y-mu\n  geom_text(data = all_vectors, aes(x = mu_x, y = mu_y, label = expression(mu), color = Angle),\n            parse = TRUE, vjust = -0.5, size = 4, show.legend = FALSE) +\n  \n  geom_text(data = all_vectors, aes(x = y_x, y = y_y, label = \"y\", color = Angle),\n            vjust = -0.5, hjust = -0.2, size = 4, fontface = \"italic\", show.legend = FALSE) +\n  \n  # Label for y - mu (placed at midpoint)\n  geom_text(data = all_vectors, aes(x = (mu_x + y_x)/2, y = (mu_y + y_y)/2, \n                                    label = expression(y - mu), color = Angle),\n            parse = TRUE, size = 3, vjust = 1.5, show.legend = FALSE) +\n\n  # 7. Origin Marker\n  geom_point(aes(x=0, y=0), color=\"black\", size=2) +\n  \n  # Formatting\n  coord_fixed() +\n  theme_minimal() +\n  labs(title = \"Rotations of Normal Cloud\",\n       x = \"x\", y = \"y\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Illustration of the Mean and Distribution of Quadratic Forms](lec4-qf_files/figure-html/fig-qf-norm-1.png){#fig-qf-norm width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n::: {#thm-mean-qf name=\"Mean of Quadratic Form\"}\nIf $y$ is a random vector with mean $E(y) = \\mu$ and covariance matrix $\\text{Var}(y) = \\Sigma$, and $A$ is a symmetric matrix of constants, then:\n\n$$\nE(y'Ay) = \\text{tr}(A\\Sigma) + \\mu'A\\mu\n$$\n:::\n\n::: {.proof}\nWe present three methods to derive the expectation of the quadratic form.\n\n**Method 1: Using the Trace Trick**\n\nUsing the fact that a scalar is equal to its own trace ($\\text{tr}(c) = c$) and the linearity of expectation:\n$$\n\\begin{aligned}\nE(y'Ay) &= E[\\text{tr}(y'Ay)] \\\\\n&= E[\\text{tr}(Ayy')] \\quad \\text{(cyclic property of trace)} \\\\\n&= \\text{tr}(A E[yy']) \\quad \\text{(linearity of expectation)}\n\\end{aligned}\n$$\nRecall that the covariance matrix is defined as $\\Sigma = E[(y-\\mu)(y-\\mu)'] = E(yy') - \\mu\\mu'$. Rearranging this gives the second moment: $E(yy') = \\Sigma + \\mu\\mu'$. Substituting this back:\n$$\n\\begin{aligned}\nE(y'Ay) &= \\text{tr}(A(\\Sigma + \\mu\\mu')) \\\\\n&= \\text{tr}(A\\Sigma) + \\text{tr}(A\\mu\\mu') \\\\\n&= \\text{tr}(A\\Sigma) + \\text{tr}(\\mu'A\\mu) \\quad \\text{(cyclic property on second term)} \\\\\n&= \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\end{aligned}\n$$\n\n**Method 2: Using Scalar Summation**\n\nWe can express the quadratic form in scalar notation using the entries of $A=(a_{ij})$, $\\Sigma=(\\sigma_{ij})$, and $\\mu=(\\mu_i)$:\n$$\n\\begin{aligned}\nE(y'Ay) &= E\\left(\\sum_{i=1}^n \\sum_{j=1}^n a_{ij} y_i y_j\\right) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} E(y_i y_j) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} (\\sigma_{ij} + \\mu_i \\mu_j) \\\\\n&= \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} \\sigma_{ji} + \\sum_{i=1}^n \\sum_{j=1}^n \\mu_i a_{ij} \\mu_j \\quad (\\text{since } \\Sigma \\text{ is symmetric, } \\sigma_{ij}=\\sigma_{ji}) \\\\\n&= \\text{tr}(A\\Sigma) + \\mu'A\\mu\n\\end{aligned}\n$$\n\n**Method 3: Using Spectral Decomposition of A**\n\nSince $A$ is symmetric, we use its spectral decomposition $A = \\sum_{i=1}^n \\lambda_i q_i q_i'$. Substituting this into the quadratic form:\n$$\ny'Ay = y' \\left( \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) y = \\sum_{i=1}^n \\lambda_i (q_i' y)^2\n$$\nLet $w_i = q_i' y$. This is a scalar random variable which is a linear transformation of $y$. Its properties are:\n\n1.  **Mean:** $E(w_i) = q_i' E(y) = q_i' \\mu$.\n2.  **Variance:** $\\text{Var}(w_i) = \\text{Var}(q_i' y) = q_i' \\text{Var}(y) q_i = q_i' \\Sigma q_i$.\n\nUsing the relation $E(w_i^2) = \\text{Var}(w_i) + [E(w_i)]^2$, we have:\n$$\nE[(q_i' y)^2] = q_i' \\Sigma q_i + (q_i' \\mu)^2\n$$\nSumming over all $i$ weighted by $\\lambda_i$:\n$$\n\\begin{aligned}\nE(y'Ay) &= \\sum_{i=1}^n \\lambda_i \\left[ q_i' \\Sigma q_i + (q_i' \\mu)^2 \\right] \\\\\n&= \\sum_{i=1}^n \\text{tr}(\\lambda_i q_i' \\Sigma q_i) + \\mu' \\left( \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) \\mu \\\\\n&= \\text{tr}\\left( \\Sigma \\sum_{i=1}^n \\lambda_i q_i q_i' \\right) + \\mu' A \\mu \\\\\n&= \\text{tr}(\\Sigma A) + \\mu' A \\mu\n\\end{aligned}\n$$\n:::\n\n::: {.remark name=\"Geometric Interpretation via Sigma\"}\nIf we further decompose $\\Sigma = \\sum_{j=1}^n \\gamma_j v_j v_j'$ (where $\\gamma_j, v_j$ are eigenvalues/vectors of $\\Sigma$), the trace term becomes:\n$$\n\\text{tr}(A\\Sigma) = \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i \\gamma_j (q_i' v_j)^2\n$$\nHere, $(q_i' v_j)^2 = \\cos^2(\\theta_{ij})$ represents the alignment between the axes of the quadratic form ($A$) and the axes of the data covariance ($\\Sigma$). The expectation is maximized when the eigenspaces of $A$ and $\\Sigma$ align.\n:::\n\n::: {#cor-projection-mean name=\"Expectation with Projection Matrix\"}\nConsider the special case where:\n\n1.  $P$ is a **projection matrix** (symmetric and idempotent, $P^2=P$).\n2.  The covariance is **spherical**: $\\Sigma = \\sigma^2 I_n$.\n\nThen the expectation simplifies to:\n$$\nE(y'Py) = \\sigma^2 r + ||P\\mu||^2\n$$\nwhere $r = \\text{rank}(P) = \\text{tr}(P)$.\n\n**Proof:**\nUsing @thm-mean-qf with $A=P$ and $\\Sigma=\\sigma^2 I_n$:\n\n1.  **Trace Term:** $\\text{tr}(P\\Sigma) = \\text{tr}(P(\\sigma^2 I_n)) = \\sigma^2 \\text{tr}(P)$. Since $P$ is idempotent, its eigenvalues are either 0 or 1, so $\\text{tr}(P) = \\text{rank}(P) = r$.\n2.  **Mean Term:** Since $P$ is symmetric and idempotent ($P'P = P^2 = P$), we can rewrite the quadratic form:\n    $$\n    \\mu' P \\mu = \\mu' P' P \\mu = (P\\mu)' (P\\mu) = ||P\\mu||^2\n    $$\n:::\n\n:::{#exm-mean-ss-decomposition name=\"Expectation of Sum of Squares Decomposition (i.i.d. Case)\"}\n\nConsider a random vector $y = (y_1, \\dots, y_n)'$ with mean vector $\\mu_y = \\mu j_n$ and covariance $\\Sigma = \\sigma^2 I_n$. We analyze the two components of the total sum of squares by projecting $y$ onto the mean space ($P_{j_n}$) and the residual space ($I-P_{j_n}$).\n\n**1. The Projection Vectors**\n\nFirst, we write the explicit forms of the projected vectors using $P_{j_n} = \\frac{1}{n}j_n j_n'$:\n\n* **Mean Vector ($P_{j_n}y$):** Projecting $y$ onto the column space of $j_n$ replaces every element with the sample mean $\\bar{y}$.\n    $$\n    P_{j_n}y = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n    $$\n\n* **Residual Vector ($(I-P_{j_n})y$):** Subtracting the mean projection from $y$ yields the deviations.\n    $$\n    (I-P_{j_n})y = y - \\bar{y}j_n = \\begin{pmatrix} y_1 - \\bar{y} \\\\ y_2 - \\bar{y} \\\\ \\vdots \\\\ y_n - \\bar{y} \\end{pmatrix}\n    $$\n\n**2. Expectations of Squared Norms**\n\nWe now find the expectation of the squared length of these vectors using @cor-projection-mean.\n\n**Part A: Sum of Squares for Mean**\nThe quadratic form is the squared norm of the projected mean vector:\n$$\ny'P_{j_n}y = ||P_{j_n}y||^2 = \\sum_{i=1}^n \\bar{y}^2 = n\\bar{y}^2\n$$\nApplying the corollary with $P=P_{j_n}$:\n\n* **Rank:** $\\text{tr}(P_{j_n}) = 1$.\n* **Mean:** $P_{j_n}\\mu_y = P_{j_n}(\\mu j_n) = \\mu j_n$. The squared norm is $n\\mu^2$.\n\n$$\nE[||P_{j_n}y||^2] = \\sigma^2(1) + n\\mu^2\n$$\n\n**Part B: Sum of Squared Errors (SSE)**\nThe quadratic form is the squared norm of the residual vector:\n$$\ny'(I-P_{j_n})y = ||(I-P_{j_n})y||^2 = \\sum_{i=1}^n (y_i - \\bar{y})^2\n$$\nApplying the corollary with $P=I-P_{j_n}$:\n\n* **Rank:** $\\text{tr}(I-P_{j_n}) = n - 1$.\n* **Mean:** $(I-P_{j_n})\\mu_y = \\mu_y - P_{j_n}\\mu_y = \\mu j_n - \\mu j_n = 0$. The squared norm is $0$.\n\n$$\nE[||(I-P_{j_n})y||^2] = \\sigma^2(n-1) + 0\n$$\n\n**Conclusion**\nThese results confirm the standard properties: $E(\\bar{y}^2) = \\frac{\\sigma^2}{n} + \\mu^2$ and $E(S^2) = \\sigma^2$.\n:::\n\n:::{#exm-mean-sst-regression name=\"Expectation of Total Sum of Squares (Regression Case)\"}\n\nConsider now a regression setting where the mean of $y$ depends on covariates (e.g., $\\mu_i = \\beta_0 + \\beta_1 x_i$). The mean vector $\\mu_y$ is **not** proportional to $j_n$. We are interested in the expectation of the **Total Sum of Squares (SST)**.\n\n**1. Identification**\nThe SST measures the variation of $y$ around the *global sample mean* $\\bar{y}$, ignoring the covariates:\n$$\n\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2 = y'(I - P_{j_n})y\n$$\nThis is the same quadratic form as Part B in the previous example, but the underlying mean $\\mu_y$ has changed.\n\n**2. Calculation**\nWe apply @cor-projection-mean with $P = I - P_{j_n}$ and general $\\mu_y$:\n\n* **Rank Term:** Same as before, $\\text{tr}(I - P_{j_n}) = n - 1$.\n* **Mean Term:** The projection of the mean vector is no longer zero.\n    $$\n    (I - P_{j_n})\\mu_y = \\mu_y - \\bar{\\mu}j_n = \\begin{pmatrix} \\mu_1 - \\bar{\\mu} \\\\ \\vdots \\\\ \\mu_n - \\bar{\\mu} \\end{pmatrix}\n    $$\n    where $\\bar{\\mu} = \\frac{1}{n}\\sum \\mu_i$ is the average of the true means.\n    The squared norm is the sum of squared deviations of the true means:\n    $$\n    ||(I - P_{j_n})\\mu_y||^2 = \\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2\n    $$\n\n**Conclusion**\n$$\nE(\\text{SST}) = (n-1)\\sigma^2 + \\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2\n$$\nThis shows that in regression, the SST estimates $(n-1)\\sigma^2$ *plus* the variability introduced by the regression signal (the spread of the true means $\\mu_i$).\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\nset.seed(123)\nn <- 20\nsigma <- 1\n\n# --- Data Generation ---\n\n# Case 1: I.i.d. Case (common Mean)\nmu_iid <- rep(3, n)\ny_iid <- mu_iid + rnorm(n, 0, sigma)\n\n# Case 2: Regression Case (sorted Mean)\n# Mu_i Is Sampled from N(3, Sd=3) and Sorted\nmu_reg <- sort(rnorm(n, 3, 3)) \ny_reg <- mu_reg + rnorm(n, 0, sigma)\n\ndf_iid <- data.frame(\n  id = 1:n,\n  y = y_iid,\n  mu = mu_iid,\n  y_bar = mean(y_iid),\n  type = \"i.i.d. Case (Common Mean)\"\n)\n\ndf_reg <- data.frame(\n  id = 1:n,\n  y = y_reg,\n  mu = mu_reg,\n  y_bar = mean(y_reg),\n  type = \"Regression Case (Sorted Mean)\"\n)\n\n# Determine Common Y Limits for Comparison Across Both Plots\ny_min <- min(c(df_iid$y, df_reg$y, df_iid$mu, df_reg$mu)) - 1\ny_max <- max(c(df_iid$y, df_reg$y, df_iid$mu, df_reg$mu)) + 1\ny_lims <- c(y_min, y_max)\n\n# --- Plotting Function ---\n\nplot_func <- function(df, title, ylims) {\n  ggplot(df, aes(x = id)) +\n    # Vertical lines for the deviations (y_i - y_bar)\n    geom_segment(aes(xend = id, y = y_bar, yend = y), \n                 color = \"gray50\", linetype = \"solid\", alpha = 0.6) +\n    # True means mu_i (red X)\n    geom_point(aes(y = mu, shape = \"True Mean (μ_i)\"), color = \"red\", size = 3) +\n    # Observations y_i (black dots)\n    geom_point(aes(y = y, shape = \"Observed (y_i)\"), color = \"black\", size = 2) +\n    # Global Sample Mean line (y_bar)\n    geom_hline(aes(yintercept = y_bar, linetype = \"Sample Mean (ȳ)\"), \n               color = \"blue\", linewidth = 0.8) +\n    scale_shape_manual(name = \"\", values = c(\"True Mean (μ_i)\" = 4, \"Observed (y_i)\" = 16)) +\n    scale_linetype_manual(name = \"\", values = c(\"Sample Mean (ȳ)\" = \"dashed\")) +\n    scale_y_continuous(limits = ylims) +\n    labs(title = title, x = \"Observation Index (Sorted by μ_i)\", y = \"Value\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n}\n\n# --- Combine Plots ---\n\np1 <- plot_func(df_iid, \"i.i.d. Case: E(SST) = (n-1)σ²\", y_lims)\np2 <- plot_func(df_reg, \"Regression Case: E(SST) = (n-1)σ² + Σ(μ_i - μ̄)²\", y_lims)\n\np1 + p2 + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Comparison of SST components with increased variation in the true means. The vertical lines represent the deviations $(y_i - \\bar{y})$. With $\\text{sd}(\\mu_i) = 3$, the regression case (right) shows significantly larger deviations, illustrating how the systematic spread of the means dominates the Total Sum of Squares.](lec4-qf_files/figure-html/fig-sst-comparison-sorted-v2-1.png){#fig-sst-comparison-sorted-v2 width=960}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Non-central $\\chi^2$ Distribution\n\nTo understand the distribution of quadratic forms under normality, we introduce the non-central chi-square distribution.\n\n::: {#def-nc-chisq name=\"Non-central $\\chi^2$ Distribution\"}\nLet $y \\sim N_n(\\mu, I_n)$. The random variable $V = y'y = \\sum y_i^2$ follows a **non-central chi-square distribution** with $n$ degrees of freedom and non-centrality parameter $\\lambda$.\n\n$$\nV \\sim \\chi^2(n, \\lambda) \\quad \\text{where } \\lambda = \\mu'\\mu = ||\\mu||^2\n$$\n:::\n\n::: {.callout-important}\n**Note on NCP Definition:** Some definitions of non-central $\\chi^2$ use $\\lambda = \\frac{1}{2}\\mu'\\mu$. In this course, we use $\\lambda = \\mu'\\mu$. With this convention, the Poisson-mixture representation below uses Poisson($\\lambda/2$) weights.\n:::\n\n### Visualizing $\\chi^2$ Distributions\n\nHere is a plot visualizing the difference between central and non-central Chi-square distributions.\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Central vs Non-central Chi-square Distribution](lec4-qf_files/figure-html/fig-plot-chisq-1.png){#fig-plot-chisq width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe density of the non-central chi-square distribution shifts to the right and becomes flatter as the non-centrality parameter $\\lambda$ increases.\n\n### Mean, Variance, and MGF\n\nWe summarize the key properties of the non-central chi-square distribution.\n\n::: {#thm-chisq-properties name=\"Properties of Non-central Chi-square\"}\nLet $V \\sim \\chi^2(n, \\lambda)$. Then:\n\n1.  **Mean:** $E(V) = n + \\lambda$\n2.  **Variance:** $\\text{Var}(V) = 2n + 4\\lambda$\n3.  **Moment Generating Function (MGF):**\n    $$\n    m_V(t) = \\frac{\\exp\\left[ -\\frac{\\lambda}{2} \\left\\{1 - \\frac{1}{1-2t}\\right\\} \\right]}{(1-2t)^{n/2}} \\quad \\text{for } t < 1/2\n    $$\n:::\n\n::: {.proof name=\"Mean\"}\nBy definition, $V \\sim \\chi^2(n, \\lambda)$ is the distribution of $y'y$ where $y \\sim N_n(\\mu, I_n)$ and the non-centrality parameter is $\\lambda = \\mu'\\mu = ||\\mu||^2$.\nApplying @lem-simple-qf to the random vector $y$:\n$$\nE(V) = E(y'y) = n + \\mu'\\mu = n + \\lambda\n$$\n:::\n\n::: {.proof name=\"MGF\"}\nSince the components $y_i$ of the vector $y$ are independent $N(\\mu_i, 1)$, and $V = \\sum_{i=1}^n y_i^2$, the MGF of $V$ is the product of the MGFs of each $y_i^2$:\n$$\nm_V(t) = E[e^{t \\sum y_i^2}] = \\prod_{i=1}^n E[e^{t y_i^2}]\n$$\nConsider a single component $y_i \\sim N(\\mu_i, 1)$. Its squared expectation is:\n$$\n\\begin{aligned}\nE[e^{t y_i^2}] &= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ty^2} e^{-\\frac{1}{2}(y-\\mu_i)^2} dy \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left\\{ -\\frac{1}{2} \\left[ (1-2t)y^2 - 2\\mu_i y + \\mu_i^2 \\right] \\right\\} dy\n\\end{aligned}\n$$\nCompleting the square in the exponent for $y$ (assuming $t < 1/2$):\n$$\n(1-2t)y^2 - 2\\mu_i y + \\mu_i^2 = (1-2t)\\left(y - \\frac{\\mu_i}{1-2t}\\right)^2 + \\mu_i^2 - \\frac{\\mu_i^2}{1-2t}\n$$\nThe integral of the Gaussian kernel $\\exp\\{ -\\frac{1}{2}(1-2t)(y - \\dots)^2 \\}$ yields $\\sqrt{\\frac{2\\pi}{1-2t}}$. The remaining constant term is:\n$$\n\\exp\\left\\{ -\\frac{1}{2} \\left( \\mu_i^2 - \\frac{\\mu_i^2}{1-2t} \\right) \\right\\} = \\exp\\left\\{ \\frac{\\mu_i^2}{2} \\left( \\frac{1}{1-2t} - 1 \\right) \\right\\} = \\exp\\left\\{ \\frac{\\mu_i^2 t}{1-2t} \\right\\}\n$$\nThus, for a single component:\n$$\nm_{y_i^2}(t) = (1-2t)^{-1/2} \\exp\\left( \\frac{\\mu_i^2 t}{1-2t} \\right)\n$$\nMultiplying the MGFs for all $n$ components:\n$$\n\\begin{aligned}\nm_V(t) &= \\prod_{i=1}^n (1-2t)^{-1/2} \\exp\\left( \\frac{\\mu_i^2 t}{1-2t} \\right) \\\\\n&= (1-2t)^{-n/2} \\exp\\left( \\frac{t \\sum \\mu_i^2}{1-2t} \\right)\n\\end{aligned}\n$$\nSubstituting $\\lambda = \\sum \\mu_i^2$ (so $\\sum \\mu_i^2 = \\lambda$):\n$$\nm_V(t) = (1-2t)^{-n/2} \\exp\\left( \\frac{\\lambda t}{1-2t} \\right)\n$$\nNote that $\\displaystyle \\frac{\\lambda t}{1-2t} = -\\frac{\\lambda}{2}\\left(1 - \\frac{1}{1-2t}\\right)$, which leads to the Poisson-mixture representation with $J \\sim \\text{Poisson}(\\lambda/2)$.\n:::\n\n::: {.proof name=\"Variance\"}\nWe use the **Cumulant Generating Function**, $K_V(t) = \\ln m_V(t)$, as its derivatives yield the mean and variance directly:\n$$\nK_V(t) = -\\frac{n}{2} \\ln(1-2t) + \\frac{\\lambda t}{1-2t}\n$$\nFirst derivative (Mean):\n$$\n\\begin{aligned}\nK'_V(t) &= -\\frac{n}{2} \\left(\\frac{-2}{1-2t}\\right) + \\lambda \\left[ \\frac{1(1-2t) - t(-2)}{(1-2t)^2} \\right] \\\\\n&= \\frac{n}{1-2t} + \\frac{\\lambda}{(1-2t)^2}\n\\end{aligned}\n$$\nSecond derivative (Variance):\n$$\n\\begin{aligned}\nK''_V(t) &= n(-1)(1-2t)^{-2}(-2) + \\lambda(-2)(1-2t)^{-3}(-2) \\\\\n&= \\frac{2n}{(1-2t)^2} + \\frac{4\\lambda}{(1-2t)^3}\n\\end{aligned}\n$$\nEvaluating at $t=0$:\n$$\n\\text{Var}(V) = K''_V(0) = 2n + 4\\lambda\n$$\n:::\n\n### Additivity\n\n::: {#thm-chisq-additivity name=\"Additivity of Chi-square\"}\nIf $v_1, \\dots, v_k$ are independent random variables distributed as $\\chi^2(n_i, \\lambda_i)$, then their sum follows a chi-square distribution:\n\n$$\n\\sum_{i=1}^k v_i \\sim \\chi^2\\left(\\sum_{i=1}^k n_i, \\sum_{i=1}^k \\lambda_i\\right)\n$$\n:::\n\n::: {.proof}\n**Method 1: Using MGFs**\n\nThe moment generating function of $v_i \\sim \\chi^2(n_i, \\lambda_i)$ is:\n$$\nM_{v_i}(t) = \\frac{\\exp\\left[-\\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}}\n$$\n\nSince $v_1, \\dots, v_k$ are independent, the MGF of their sum $V = \\sum v_i$ is the product of their individual MGFs:\n\n$$\n\\begin{aligned}\nM_V(t) &= \\prod_{i=1}^k M_{v_i}(t) \\\\\n&= \\prod_{i=1}^k \\frac{\\exp\\left[-\\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{n_i/2}} \\\\\n&= \\frac{\\exp\\left[-\\sum \\lambda_i \\left(1 - \\frac{1}{1-2t}\\right)\\right]}{(1-2t)^{\\sum n_i/2}}\n\\end{aligned}\n$$\n\nThis is the MGF of a non-central chi-square distribution with degrees of freedom $\\sum n_i$ and non-centrality parameter $\\sum \\lambda_i$.\n\n**Method 2: Geometric Interpretation**\n\nLet $v_i = ||y_i||^2$ where $y_i \\sim N_{n_i}(\\mu_i, I_{n_i})$. Since the vectors $y_i$ are independent, we can stack them into a larger vector $y = (y_1', \\dots, y_k')'$.\n\n$$\ny \\sim N_{\\sum n_i}(\\mu, I_{\\sum n_i}) \\quad \\text{where } \\mu = (\\mu_1', \\dots, \\mu_k')'\n$$\n\nThe sum of squares is:\n$$\n\\sum v_i = \\sum ||y_i||^2 = ||y||^2\n$$\n\nBy definition, $||y||^2$ follows a non-central chi-square distribution with degrees of freedom equal to the dimension of $y$ ($\\sum n_i$) and non-centrality parameter $\\lambda = ||\\mu||^2$.\n\n$$\n\\lambda = \\sum_{i=1}^k ||\\mu_i||^2 = \\sum_{i=1}^k \\lambda_i\n$$\n:::\n\n### Poisson Mixture Representation\n\n::: {#thm-chisq-poisson-mixture name=\"Poisson Mixture Representation\"}\nLet $v \\sim \\chi^2(n, \\lambda)$ be a non-central chi-square random variable. Its probability density function can be represented as a Poisson-weighted sum of central chi-square density functions:\n\n$$\nf(v; n, \\lambda) = \\sum_{j=0}^{\\infty} \\left( \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!} \\right) f(v; n+2j, 0)\n$$\n\nwhere $f(v; \\nu, 0)$ is the density of a central chi-square distribution with $\\nu$ degrees of freedom.\n:::\n\n::: {.proof}\nWe use the Moment Generating Function (MGF) approach. The MGF of a non-central chi-square distribution $v \\sim \\chi^2(n, \\lambda)$ is:\n$$\nM_v(t) = (1-2t)^{-n/2} \\exp\\left( \\frac{\\lambda}{2} \\left[ \\frac{1}{1-2t} - 1 \\right] \\right)\n$$\n\nWe can expand the exponential term using the power series $e^x = \\sum_{j=0}^\\infty \\frac{x^j}{j!}$:\n$$\n\\begin{aligned}\nM_v(t) &= (1-2t)^{-n/2} e^{-\\lambda/2} \\exp\\left( \\frac{\\lambda/2}{1-2t} \\right) \\\\\n&= e^{-\\lambda/2} (1-2t)^{-n/2} \\sum_{j=0}^{\\infty} \\frac{1}{j!} \\left( \\frac{\\lambda/2}{1-2t} \\right)^j \\\\\n&= \\sum_{j=0}^{\\infty} \\left( \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!} \\right) (1-2t)^{-(n+2j)/2}\n\\end{aligned}\n$$\n\nRecognizing the terms:\n\n1.  The term in parentheses, $P(J=j) = \\frac{e^{-\\lambda/2} (\\lambda/2)^j}{j!}$, is the probability mass function of a **Poisson** random variable $J \\sim \\text{Poisson}(\\lambda/2)$. \n2.  The term $(1-2t)^{-(n+2j)/2}$ is the MGF of a **central chi-square** distribution with $n+2j$ degrees of freedom.\n\nSince the MGF of the mixture is the sum of the MGFs of the components weighted by the mixture probabilities, the density must follow the same mixture structure.\n:::\n\n::: {.remark}\nThis theorem implies a hierarchical model for generating a non-central chi-square variable:\n\n1.  Sample $J \\sim \\text{Poisson}(\\lambda/2)$.\n2.  Given $J=j$, sample $V \\sim \\chi^2(n+2j, 0)$.\n\nThis is particularly useful for numerical computation, as it allows the non-central CDF to be approximated by a finite sum of central chi-square CDFs.\n:::\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Parameters\nn <- 4          # Base degrees of freedom\nlambda <- 4     # Non-centrality parameter (lambda = ||mu||^2)\nx_limit <- 25   # X-axis range\nj_values <- 0:15 # Sequence of J = 0, 1, 2...\n\n# Generate Data for the Mixture Components\nx <- seq(0, x_limit, length.out = 400)\nmixture_df <- do.call(rbind, lapply(j_values, function(j) {\n  weight <- dpois(j, lambda/2)\n  data.frame(\n    x = x,\n    y = dchisq(x, df = n + 2*j),\n    j = j,\n    weight = weight\n  )\n}))\n\n# Generate Data for the True Non-central Chi-square\n# R Uses Ncp = ||mu||^2 (we set lambda = ||mu||^2)\ntrue_nc <- data.frame(\n  x = x,\n  y = dchisq(x, df = n, ncp = lambda)\n) \n\n# Plotting\nggplot() +\n  # Draw weighted central chi-square curves (the \"cloud\")\n  geom_line(data = mixture_df, \n            aes(x = x, y = y, group = j, alpha = weight), \n            color = \"black\", \n            linewidth = 0.8) +\n  # Draw the true non-central chi-square density\n  geom_line(data = true_nc, aes(x = x, y = y), \n            color = \"blue\", \n            linewidth = 1.3) +\n  # Aesthetics\n  scale_alpha_continuous(range = c(0.01, 0.8), guide = \"none\") +\n  labs(\n    title = \"Poisson Mixture Representation of Non-central Chi-square\",\n    subtitle = paste0(\"n = \", n, \", λ = \", lambda, \" (Blue line = True Non-central)\"),\n    x = \"Value (v)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![The non-central chi-square distribution as a Poisson mixture. The black curves represent central chi-square densities with $df = n + 2j$, with transparency (alpha) proportional to the Poisson weight $P(J=j)$. The solid blue line is the true non-central chi-square density.](lec4-qf_files/figure-html/fig-chisq-poisson-mixture-fixed-1.png){#fig-chisq-poisson-mixture-fixed width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Distribution of Quadratic Forms\n\n### MGF of Quadratic Forms\n\nTo determine the distribution of general quadratic forms $y'Ay$, we look at their MGF.\n\n::: {#thm-mgf-quad name=\"MGF of Quadratic Form\"}\nIf $y \\sim N_p(\\mu, \\Sigma)$, then the MGF of $Q = y'Ay$ is:\n\n$$\nM_Q(t) = |I - 2tA\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2} \\mu' [I - (I - 2tA\\Sigma)^{-1}] \\Sigma^{-1} \\mu\\right)\n$$\n:::\n\n\n### Non-central $\\chi^2$ of Quadratic Forms\n\n\n\nWe will prove a simplified version of @thm-dist-quad first.\n\n::: {#thm-proj-matrix name=\"Distribution of Projected Spherical Normal\"}\nIf $y \\sim N_n(\\mu, \\sigma^2 I_n)$ and $P_V$ is a projection matrix onto a subspace $V$ of dimension $r$, then:\n\n$$\n\\frac{1}{\\sigma^2} y'P_V y = \\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{\\sigma^2}\\right)\n$$\n\nThis holds because $\\frac{1}{\\sigma^2} P_V (\\sigma^2 I) = P_V$, which is idempotent.\n:::\n\n::: {.callout-important}\n### Crucial Theorem\n\nThis is one of the most important theorems in the course, establishing the fundamental conditions under which a quadratic form follows a chi-square distribution.\n:::\n\n::: {.proof}\n\n**When $\\sigma^2=1$**\n\n\nLet $P_V$ be the projection matrix. We know $P_V = QQ'$ where $Q = (q_1, \\dots, q_r)$ is an $n \\times r$ matrix with orthonormal columns ($Q'Q = I_r$).\n\nThe projection of vector $y$ onto the subspace $V$ can be expressed using the orthonormal basis vectors:\n$$\nP_V y = Q Q' y = (q_1, \\dots, q_r) \\begin{pmatrix} q_1' y \\\\ \\vdots \\\\ q_r' y \\end{pmatrix} = \\sum_{i=1}^r (q_i' y) q_i\n$$\n\nThe squared norm of the projection is:\n$$\ny' P_V y = y' Q Q' y = (Q'y)' (Q'y) = ||Q'y||^2\n$$\n\nSince $y \\sim N(\\mu, I_n)$, the linear transformation $w = Q'y$ follows:\n$$\nw \\sim N(Q'\\mu, Q' I_n Q) = N(Q'\\mu, I_r)\n$$\n\nThus, $w$ is a vector of $r$ independent normal variables with variance 1. The sum of squares $||w||^2$ is by definition non-central chi-square:\n\n$$\n||w||^2 \\sim \\chi^2(r, \\lambda)\n$$\nwhere the non-centrality parameter is:\n$$\n\\lambda = ||E(w)||^2 = ||Q'\\mu||^2\n$$\n\nNote that $||Q'\\mu||^2 = \\mu' Q Q' \\mu = \\mu' P_V \\mu = ||P_V \\mu||^2$.\n\nThus, $y' P_V y \\sim \\chi^2(r, ||P_V \\mu||^2)$.\n\n**When $\\sigma^2\\not=1$**\n\nIf $y \\sim N(\\mu, \\sigma^2 I_n)$, we standardize by dividing by $\\sigma$.\n\nLet $w = y/\\sigma$. Then $w \\sim N(\\mu/\\sigma, I_n)$.\nApplying the previous result to $w$:\n\n$$\nw' P_V w = \\frac{y' P_V y}{\\sigma^2} \\sim \\chi^2\\left(r, \\left|\\left| P_V \\frac{\\mu}{\\sigma} \\right|\\right|^2\\right)\n$$\nwhich simplifies to:\n$$\n\\frac{||P_V y||^2}{\\sigma^2} \\sim \\chi^2\\left(r, \\frac{||P_V \\mu||^2}{\\sigma^2}\\right)\n$$\n\n:::\n\n::: {.callout-important name=\"Scale of the Quadratic Form\"}\n\n\nThe term $\\|P_V y\\|^2$ itself is **not** a standard chi-square variable; it is a scaled chi-square variable. Its mean is:\n\n$$\nE(\\|P_V y\\|^2) = \\sigma^2 \\left(r + \\frac{\\|P_V \\mu\\|^2}{\\sigma^2}\\right) = r\\sigma^2 + \\|P_V \\mu\\|^2\n$$\n:::\n\n\n\n\n\n\n\n\n\n\n::: {.cell screenshot.force='true' webshot2='{\"vwidth\":900,\"vheight\":600,\"zoom\":2}'}\n\n```{.r .cell-code}\nlibrary(plotly)\nlibrary(MASS)\n\n# 1. Generate Data\nset.seed(123)\nn <- 200\nmu <- c(2, 3, 5) \nsigma <- diag(3)\ndata <- mvrnorm(n, mu, sigma)\ndf <- as.data.frame(data)\ncolnames(df) <- c(\"x\", \"y\", \"z\")\n\n# 2. Project Points\nproj_points <- t(apply(data, 1, function(p) {\n  sum(p * c(1,0,0)) * c(1,0,0) + sum(p * c(0,1,0)) * c(0,1,0)\n}))\ndf_proj <- as.data.frame(proj_points)\ncolnames(df_proj) <- c(\"px\", \"py\", \"pz\")\n\n# 3. Setup Axis Styles\nax_style <- list(\n  title = \"\",\n  showgrid = TRUE,        # Keep the coordinate plane grids\n  gridcolor = \"gray\",\n  gridwidth = 0.5,\n  zeroline = FALSE,       # REMOVED: The static \"crosshair\" lines at 0\n  showline = FALSE,       # REMOVED: The front bounding box\n  showticklabels = FALSE,\n  showbackground = FALSE,\n  showspikes = FALSE\n)\n\n# 4. Create the Plot\nplot_ly() %>%\n  # --- Optional: Floor Plane ---\n  add_trace(\n    x = c(-2, 8, 8, -2), y = c(-2, -2, 8, 8), z = c(0, 0, 0, 0),\n    type = \"mesh3d\", opacity = 0.05, color = 'gray', \n    hoverinfo = \"none\", showlegend = FALSE\n  ) %>%\n  # --- Original Data (y) ---\n  add_trace(\n    data = df, x = ~x, y = ~y, z = ~z,\n    type = 'scatter3d', mode = 'markers',\n    marker = list(size = 3, color = 'blue', opacity = 0.6),\n    # Legend with Math (Italic y)\n    name = '<i>y</i>' \n  ) %>%\n  # --- Projected Shadow (P_V y) ---\n  add_trace(\n    data = df_proj, x = ~px, y = ~py, z = ~pz,\n    type = 'scatter3d', mode = 'markers',\n    marker = list(size = 3, color = 'red', opacity = 0.8),\n    # Legend with Math (P_sub_V y)\n    name = \"<i>P</i><sub>V</sub><i>y</i>\"\n  ) %>%\n  # --- Residual Lines ---\n  add_segments(\n    x = df$x, xend = df_proj$px,\n    y = df$y, yend = df_proj$py,\n    z = df$z, zend = df_proj$pz,\n    line = list(color = 'gray', width = 1),\n    showlegend = FALSE, hoverinfo = \"none\"\n  ) %>%\n  # --- Manual Labels ---\n  add_text(\n    x = c(8.5, 0, 0), y = c(0, 8.5, 0), z = c(0, 0, 8.5),\n    # Used Unicode \\u22A5 for perpendicular symbol\n    text = c(\"<i>q</i><sub>1</sub>\", \"<i>q</i><sub>2</sub>\", \"<i>V</i><sup>\\u22A5</sup>\"),\n    textfont = list(size = 15, color = \"black\"),\n    showlegend = FALSE\n  ) %>%\n  layout(\n    scene = list(\n      xaxis = ax_style,\n      yaxis = ax_style,\n      zaxis = ax_style,\n      aspectmode = \"cube\",\n      camera = list(eye = list(x = 1.6, y = 1.6, z = 1.3))\n    ),\n    title = \"Projection of Trivariate Normal onto 2D Subspace\",\n    margin = list(l=0, r=0, b=0, t=30)\n  )\n```\n\n::: {.cell-output-display}\n![Visualization of Projected Trivariate Normal Cloud](lec4-qf_files/figure-html/plot-projection-3d-refined-1.png){width=100%}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n### Distribution of General Quadratic Forms\n\n::: {#lem-idempotent-sigma name=\"Idempotent Matrix Property\"}\nLet $\\Sigma$ be a positive definite matrix such that $\\Sigma = \\Sigma^{1/2}\\Sigma^{1/2}$. The matrix $A\\Sigma$ is idempotent if and only if $\\Sigma^{1/2}A\\Sigma^{1/2}$ is idempotent.\n:::\n\n::: {.proof}\n$(\\Rightarrow)$ Assume $A\\Sigma$ is idempotent, so $A\\Sigma A\\Sigma = A\\Sigma$. Then:\n$$\n\\begin{aligned}\n(\\Sigma^{1/2}A\\Sigma^{1/2})^2 &= \\Sigma^{1/2}A(\\Sigma^{1/2}\\Sigma^{1/2})A\\Sigma^{1/2} \\\\\n&= \\Sigma^{1/2}(A\\Sigma A)\\Sigma^{1/2}\n\\end{aligned}\n$$\nFrom the assumption $A\\Sigma A\\Sigma = A\\Sigma$, post-multiplying by $\\Sigma^{-1}$ gives $A\\Sigma A = A$. Substituting this back:\n$$\n\\Sigma^{1/2}(A)\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma^{1/2}\n$$\n\n$(\\Leftarrow)$ Assume $\\Sigma^{1/2}A\\Sigma^{1/2}$ is idempotent. Then:\n$$\n(\\Sigma^{1/2}A\\Sigma^{1/2})(\\Sigma^{1/2}A\\Sigma^{1/2}) = \\Sigma^{1/2}A\\Sigma^{1/2}\n$$\nExpanding the left side:\n$$\n\\Sigma^{1/2}A(\\Sigma^{1/2}\\Sigma^{1/2})A\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2}\n$$\nEquating this to the right side:\n$$\n\\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2} = \\Sigma^{1/2}A\\Sigma^{1/2}\n$$\nPre-multiply by $\\Sigma^{-1/2}$ and post-multiply by $\\Sigma^{1/2}$ (which exist since $\\Sigma$ is positive definite):\n$$\n\\begin{aligned}\n\\Sigma^{-1/2}(\\Sigma^{1/2}A\\Sigma A\\Sigma^{1/2})\\Sigma^{1/2} &= \\Sigma^{-1/2}(\\Sigma^{1/2}A\\Sigma^{1/2})\\Sigma^{1/2} \\\\\nI(A\\Sigma A)\\Sigma &= I(A)\\Sigma \\\\\nA\\Sigma A\\Sigma &= A\\Sigma\n\\end{aligned}\n$$\n:::\n::: {#lem-rank-sigma name=\"Rank Invariance\"}\nUnder the conditions of @lem-idempotent-sigma, if $A\\Sigma$ is idempotent, then:\n$$\n\\text{rank}(A\\Sigma) = \\text{rank}(\\Sigma^{1/2}A\\Sigma^{1/2}) = \\text{tr}(A\\Sigma)\n$$\n:::\n\n::: {.proof}\nSince $A\\Sigma$ and $\\Sigma^{1/2}A\\Sigma^{1/2}$ are both idempotent (by @lem-idempotent-sigma), their ranks are equal to their traces.\n\nUsing the cyclic property of the trace operator ($\\text{tr}(XYZ) = \\text{tr}(ZXY)$):\n$$\n\\begin{aligned}\n\\text{rank}(A\\Sigma) &= \\text{tr}(A\\Sigma) \\\\\n&= \\text{tr}(A \\Sigma^{1/2} \\Sigma^{1/2}) \\\\\n&= \\text{tr}(\\Sigma^{1/2} A \\Sigma^{1/2}) \\\\\n&= \\text{rank}(\\Sigma^{1/2}A\\Sigma^{1/2})\n\\end{aligned}\n$$\nAlternatively, notice that $A\\Sigma$ is similar to $\\Sigma^{1/2}A\\Sigma^{1/2}$:\n$$\nA\\Sigma = \\Sigma^{-1/2} (\\Sigma^{1/2}A\\Sigma^{1/2}) \\Sigma^{1/2}\n$$\nSince similar matrices have the same rank, the equality holds.\n:::\n\n\n::: {#thm-dist-quad name=\"Distribution of y'Ay\"}\nLet $y \\sim N_p(\\mu, \\Sigma)$. Let $A$ be a symmetric matrix of rank $r$.\nThen $y'Ay \\sim \\chi^2(r, \\lambda)$ with $\\lambda = \\mu' A \\mu$ **if and only if** $A\\Sigma$ is idempotent ($A\\Sigma A\\Sigma = A\\Sigma$).\n\n**Special Case ($\\Sigma = I$):**\nIf $\\Sigma = I$, the condition simplifies to $A$ being idempotent ($A^2 = A$).\n:::\n\n::: {.proof}\nLet $y^* = \\Sigma^{-1/2}y$, so $y^* \\sim N_n(\\Sigma^{-1/2}\\mu, I_n)$. We rewrite the quadratic form:\n$$y'Ay = y' \\Sigma^{-1/2} (\\Sigma^{1/2} A \\Sigma^{1/2}) \\Sigma^{-1/2} y = (y^*)' P_V y^* = \\|P_V y^*\\|^2$$\nSince $A\\Sigma$ is idempotent, $P_V = \\Sigma^{1/2} A \\Sigma^{1/2}$ is a projection matrix with rank $r$. By the definition of the non-central chi-square, $y'Ay \\sim \\chi^2(r, \\|P_V \\Sigma^{-1/2}\\mu\\|^2)$. The non-centrality parameter simplifies to $\\lambda = \\mu'A\\mu$.\n:::\n\n### Standardized Distance Distribution\n::: {#cor-standardized-mvn name=\"Standardized Distance Distribution\"}\nSuppose $y \\sim N_n(\\mu, \\Sigma)$. Then the quadratic form representing the standardized distance from a constant vector $\\mu_0$ follows a non-central chi-square distribution:\n$$(y-\\mu_0)'\\Sigma^{-1}(y-\\mu_0) \\sim \\chi^2(n, \\lambda = (\\mu-\\mu_0)'\\Sigma^{-1}(\\mu-\\mu_0))$$\n:::\n\n::: {.proof}\nLet $A = \\Sigma^{-1}$. Then $A\\Sigma = \\Sigma^{-1}\\Sigma = I_n$, which is clearly idempotent. Alternatively, let $w = \\Sigma^{-1/2}(y-\\mu_0)$, then $w \\sim N_n(\\Sigma^{-1/2}(\\mu-\\mu_0), I_n)$. By the definition of chi-square, $\\|w\\|^2 = (y-\\mu_0)'\\Sigma^{-1}(y-\\mu_0)$ follows the stated distribution.\n:::\n\n::: {.callout-important}\n### Crucial Theorem\n\nThis is an important theorem we will use later. \n:::\n\n## Distributions of Projections of Spherical Normal\n\n::: {#thm-proj-dist name=\"Distribution of Projections\"}\nLet $V$ be a $k$-dimensional subspace of $\\mathcal{R}^n$ with projection matrix $P_V$, and let $y$ be a random vector in $\\mathcal{R}^n$ with mean $E(y)=\\mu$. Then:\n\n1. $E(P_V y) = P_V \\mu$.\n2. If $\\text{Var}(y)=\\sigma^2 I_n$, then $\\text{Var}(P_V y) = \\sigma^2 P_V$ and $E(\\|P_V y\\|^2) = \\sigma^2 k + \\|P_V \\mu\\|^2$.\n3. If $y \\sim N_n(\\mu, \\sigma^2 I_n)$, then $\\frac{1}{\\sigma^2}\\|P_V y\\|^2 = \\frac{1}{\\sigma^2}y'P_Vy \\sim \\chi^2(k, \\frac{1}{\\sigma^2}\\|P_V \\mu\\|^2)$.\n:::\n\n::: {.proof}\n\n1. Since the projection operation is linear, $E(P_V y) = P_V E(y) = P_V \\mu$.\n2. $\\text{Var}(P_V y) = P_V \\text{Var}(y) P_V^T = P_V \\sigma^2 I_n P_V = \\sigma^2 P_V$. The expectation of the squared norm follows from the mean of a quadratic form: $E(y'P_Vy) = \\text{tr}(P_V \\sigma^2 I) + \\mu'P_V\\mu = \\sigma^2 k + \\|P_V \\mu\\|^2$.\n3. This is a special case of the general quadratic distribution theorem where $A = \\frac{1}{\\sigma^2} P_V$ and $A(\\sigma^2 I) = P_V$, which is idempotent.\n:::\n\n::: {#thm-ortho-indep name=\"Orthogonal Projections\"}\nLet $V_1, \\dots, V_k$ be mutually orthogonal subspaces with dimensions d_i and projection matrices $P_i$. If $y \\sim N_n(\\mu, \\sigma^2 I_n)$, then:\n\n1. The projections $\\hat{y}_i = P_i y$ are independent with $\\hat{y}_i \\sim N(P_i \\mu, \\sigma^2 P_i)$.\n2. The squared norms $\\|\\hat{y}_i\\|^2$ are mutually independent.\n3. $\\frac{1}{\\sigma^2}\\|\\hat{y}_i\\|^2 \\sim \\chi^2(d_i, \\frac{1}{\\sigma^2}\\|P_i \\mu\\|^2)$.\n:::\n\n::: {.proof}\n\n1. For $i \\ne j$, $\\text{Cov}(P_i y, P_j y) = \\sigma^2 P_i P_j = 0$ because orthogonal projection matrices satisfy $P_i P_j = 0$. Under normality, zero covariance implies independence.\n2. Since $\\hat{y}_i$ are independent, any measurable functions of them, such as their squared norms, are also independent.\n3. This follows directly from applying the projection distribution theorem to each independent subspace.\n:::\n\n### Independence of Forms\n\n::: {#thm-indep-mvn name=\"Independence Conditions\"}\nSuppose $y \\sim N_n(\\mu, \\Sigma)$.\n\n* **Linear and Quadratic:** $By$ and $y'Ay$ (where $A$ is symmetric) are independent if and only if $B\\Sigma A = 0$.\n* **Quadratic and Quadratic:** $y'Ay$ and $y'By$ (where $A, B$ are symmetric) are independent if and only if $A\\Sigma B = 0$.\n:::\n\n::: {.proof}\nIf $B\\Sigma A = 0$, the normal vectors $By$ and $Ay$ have zero covariance and are independent. Because $By$ is independent of $Ay$, it is also independent of any measurable function of $Ay$, specifically $y'Ay = \\|Ay\\|^2$ (if $A$ is idempotent).\n:::\n\n### Cochran's Theorem\n\n::: {#thm-cochran-result name=\"Cochran's Result\"}\nLet $y \\sim N_n(\\mu, \\sigma^2 I)$ and $y'y = \\sum y'A_iy$. The quadratic forms $y^T A_i y / \\sigma^2$ are mutually independent $\\chi^2(r_i, \\lambda_i)$ if and only if any one of the following holds:\n\n* Each $A_i$ is idempotent.\n* $A_i A_j = 0$ for all $i \\ne j$.\n* $n = \\sum r_i$.\n:::\n\n\n## Non-central Distributions Derived from Non-central $\\chi^2$\n\nWe begin by defining two independent Chi-squared random variables that form the building blocks for statistical power analysis.\n\n* **Non-central Component ($X_1$):** $X_1 \\sim \\chi^2(\\text{df}_1, \\lambda)$.\n  Here, $\\lambda$ is the non-centrality parameter, defined as the sum of squared means, $\\lambda = ||\\mu||^2$. This is consistent with the definition used throughout this chapter. *(Note: This definition is also used by R's `ncp` argument.)*\n\n* **Central Component ($X_2$):** $X_2 \\sim \\chi^2(\\text{df}_2)$. $X_2$ often represents the **Noise Sum of Squares**, SSE$_1$ of an adequate model, which is assume to follow a central $\\chi^2$,\n\nWe visualize these components as using the follow diagram.\n\n\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A diagram of two independent $\\chi^2$ random variables](lec4-qf_files/figure-html/fig-variance-partition-1.png){#fig-variance-partition fig-align='center' width=576 style=\"width: 85% !important;\"}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### The Non-central F-distribution $F(\\text{df}_1, \\text{df}_2, \\lambda)$\n\n::: {#def-noncentral-f}\n### Non-central F\nLet $X_1 \\sim \\chi^2(\\text{df}_1, \\lambda)$ and $X_2 \\sim \\chi^2(\\text{df}_2)$ be independent. The random variable $F$ follows a **non-central F-distribution**:\n$$F = \\frac{X_1/\\text{df}_1}{X_2/\\text{df}_2} \\sim F(\\text{df}_1, \\text{df}_2, \\lambda)$$\n:::\n\n* **Expectation:**\n    * **Under $H_0$ ($\\lambda=0$):** Exact mean is $\\frac{\\text{df}_2}{\\text{df}_2 - 2}$ (for $\\text{df}_2 > 2$).\n    * **Under $H_1$ ($\\lambda \\neq 0$):** Approximate mean is $1 + \\frac{\\lambda}{\\text{df}_1}$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Densities of Non-Central F ($\\lambda$ defined as sum of squares).](lec4-qf_files/figure-html/fig-nc-f-1.png){#fig-nc-f width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Type I Non-central Beta $\\text{Beta}_1(\\text{df}_1/2, \\text{df}_2/2, \\lambda)$\n\n::: {#def-noncentral-beta1}\n### Type I Non-central Beta\nThe random variable $B_I$ follows a **Type I non-central Beta distribution**, defined as the signal's proportion of the total sum ($R^2$):\n$$B_I = \\frac{X_1}{X_1 + X_2} \\sim \\text{Beta}_1\\left(\\frac{\\text{df}_1}{2}, \\frac{\\text{df}_2}{2}, \\lambda\\right)$$\n:::\n\n* **Relationship to F:** $B_I = \\frac{(\\text{df}_1/\\text{df}_2) F}{1 + (\\text{df}_1/\\text{df}_2) F}$\n* **Expectation:**\n    * **Under $H_0$ ($\\lambda=0$):** Exact mean is $\\frac{\\text{df}_1}{\\text{df}_1 + \\text{df}_2}$.\n    * **Under $H_1$ ($\\lambda \\neq 0$):** Approximate mean is $\\frac{\\text{df}_1 + \\lambda}{\\text{df}_1 + \\text{df}_2 + \\lambda}$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Densities of Type I Beta ($R^2$).](lec4-qf_files/figure-html/fig-nc-beta1-1.png){#fig-nc-beta1 width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Type II Non-central Beta $\\text{Beta}_2(\\text{df}_2/2, \\text{df}_1/2, \\lambda)$\n\n::: {#def-noncentral-beta2}\n### Type II Non-central Beta\n$$B_{II} = \\frac{X_2}{X_1 + X_2} = 1 - B_I \\sim \\text{Beta}_2\\left(\\frac{\\text{df}_2}{2}, \\frac{\\text{df}_1}{2}, \\lambda\\right)$$\n:::\n\n* **Relationship to F:** $B_{II} = \\frac{1}{1 + (\\text{df}_1/\\text{df}_2) F}$\n* **Expectation:**\n    * **Under $H_0$ ($\\lambda=0$):** Exact mean is $\\frac{\\text{df}_2}{\\text{df}_1 + \\text{df}_2}$.\n    * **Under $H_1$ ($\\lambda \\neq 0$):** Approximate mean is $\\frac{\\text{df}_2}{\\text{df}_1 + \\text{df}_2 + \\lambda}$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Densities of Type II Beta ($SSE/SST$). Support is [0, 1].](lec4-qf_files/figure-html/fig-beta-ii-1.png){#fig-beta-ii width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Scaled Type II Beta $\\text{Scaled-Beta}_2(\\text{df}_2/2, \\text{df}_1/2, \\lambda)$\n\n::: {#def-scaled-beta}\n### Scaled Type II Beta\n$$S = \\frac{X_2/\\text{df}_2}{(X_1+X_2)/(\\text{df}_1+\\text{df}_2)} \\sim \\text{Scaled-Beta}_2$$\n:::\n\n* **Relationship to F:** $S = \\frac{\\text{df}_1+\\text{df}_2}{\\text{df}_2 + \\text{df}_1 F}$\n* **Expectation:**\n    * **Under $H_0$ ($\\lambda=0$):** Exact mean is $1$.\n    * **Under $H_1$ ($\\lambda \\neq 0$):** Approximate mean is $\\frac{\\text{df}_1+\\text{df}_2}{\\text{df}_1+\\text{df}_2+\\lambda}$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Densities of Scaled Type II Beta ($MSE/MST$).](lec4-qf_files/figure-html/fig-scaled-beta-1.png){#fig-scaled-beta width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n### The Non-central t-distribution $t(\\text{df}_2, \\delta)$\n\n::: {#def-noncentral-t}\n### Non-central t\nLet $Z \\sim N(\\delta, 1)$ and $X_2 \\sim \\chi^2(\\text{df}_2)$ be independent. The random variable $T$ follows a **non-central t-distribution**:\n$$T = \\frac{Z}{\\sqrt{X_2/\\text{df}_2}} \\sim t(\\text{df}_2, \\delta)$$\n:::\n\n* **Relationship to F:** $F = T^2$ (when $\\text{df}_1=1$). Note $\\delta^2 = \\lambda$.\n* **Expectation:**\n    * **Under $H_0$ ($\\delta=0$):** Exact mean is $0$.\n    * **Under $H_1$ ($\\delta \\neq 0$):** Approximate mean is $\\delta$.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Densities of Non-Central t ($df=20$).](lec4-qf_files/figure-html/fig-nc-t-1.png){#fig-nc-t width=576}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Example: Inference of the Mean of Normal Sample\n\nConsider a random sample $y \\sim N_n(\\mu j_n, \\sigma^2 I_n)$. We wish to test:\n\n* **$M_1$ (Full Model):** $\\mu$ is unknown.\n* **$M_0$ (Reduced Model):** $\\mu = \\mu_0$.\n\nLet's define the transformed vector $y^* = y - \\mu_0 j_n$. Note that $y^* \\sim N_n((\\mu - \\mu_0)j_n, \\sigma^2 I_n)$.\n\n### Sum of Squares and Their Distributions\n\nWe use the projection matrix $P_{j_n} = \\frac{1}{n}j_n j_n'$ and its complement $(I_n - P_{j_n})$ to partition the transformed vector.\n\n* **Total SSE ($SSE_0$ for $M_0$):**\n    $$SSE_0 = \\|I_n y^*\\|^2 = \\sum_{i=1}^n (Y_i - \\mu_0)^2$$\n    This follows a non-central distribution with $\\text{df}_{\\text{total}} = n$:\n    $$\\frac{SSE_0}{\\sigma^2} \\sim \\chi^2(n, \\lambda) \\quad \\text{where } \\lambda = \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}$$\n\n* **Residual SSE ($SSE_1$ for $M_1$):**\n    $$SSE_1 = \\|(I_n - P_{j_n})y^*\\|^2 = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\n    This captures the random noise (central component) with $\\text{df}_2 = n-1$:\n    $$\\frac{SSE_1}{\\sigma^2} \\sim \\chi^2(n-1)$$\n\n* **Difference SS ($SS_{\\text{diff}}$):**\n    $$SS_{\\text{diff}} = \\|P_{j_n} y^*\\|^2 = n(\\bar{Y} - \\mu_0)^2$$\n    This captures the signal (non-central component) with $\\text{df}_1 = 1$:\n    $$\\frac{SS_{\\text{diff}}}{\\sigma^2} \\sim \\chi^2(1, \\lambda)$$\n\n### Distributions of Equivalent Statistics\n\nWe can construct five equivalent statistics to compare $M_0$ and $M_1$.\n\n* **The t-statistic ($T$):**\n    $$T = \\frac{\\bar{Y} - \\mu_0}{S/\\sqrt{n}}$$\n\n* **The F-statistic ($F$):**\n    $$F = \\frac{n(\\bar{Y} - \\mu_0)^2}{S^2} = T^2$$\n\n* **The Type I Beta statistic ($B_I$):**\n    $$B_I = \\frac{SS_{\\text{diff}}}{SSE_0} = \\frac{n(\\bar{Y} - \\mu_0)^2}{\\sum (Y_i - \\mu_0)^2}$$\n\n* **The Type II Beta statistic ($B_{II}$):**\n    $$B_{II} = \\frac{SSE_1}{SSE_0} = \\frac{\\sum (Y_i - \\bar{Y})^2}{\\sum (Y_i - \\mu_0)^2} = 1 - B_I$$\n\n* **The Scaled Type II Beta statistic ($S_{\\text{scaled}}$):**\n    $$S_{\\text{scaled}} = \\frac{SSE_1/(n-1)}{SSE_0/n} = \\left( \\frac{n}{n-1} \\right) B_{II}$$\n\n### Expectations Under $M_1$ and $M_0$\n\nThe table below contrasts the distributions and expected values of these statistics. We assume the sample size $n$ is large enough for the mean of $F$ to exist ($n > 3$).\n\n* **Degrees of Freedom:** $\\text{df}_1 = 1$, $\\text{df}_2 = n-1$.\n* **Non-centrality:** $\\delta = \\frac{\\sqrt{n}(\\mu - \\mu_0)}{\\sigma}$ and $\\lambda = \\delta^2 = \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}$.\n\n| Statistic | Distribution under $H_1$ ($\\mu \\neq \\mu_0$) | Exact Mean under $H_0$ ($\\mu=\\mu_0$) | Approximate Mean under $H_1$ |\n| :--- | :--- | :--- | :--- |\n| **$T$** | $t(n-1, \\delta)$ | $0$ | $\\frac{\\sqrt{n}(\\mu - \\mu_0)}{\\sigma}$ |\n| **$F$** | $F(1, n-1, \\lambda)$ | $\\frac{n-1}{n-3} \\approx 1$ | $1 + \\frac{n(\\mu - \\mu_0)^2}{\\sigma^2}$ |\n| **$B_I$** | $\\text{Beta}_1\\left(\\frac{1}{2}, \\frac{n-1}{2}, \\lambda\\right)$ | $\\frac{1}{n}$ | $\\frac{1/n + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}$ |\n| **$B_{II}$** | $\\text{Beta}_2\\left(\\frac{n-1}{2}, \\frac{1}{2}, \\lambda\\right)$ | $\\frac{n-1}{n}$ | $\\frac{(n-1)/n}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}$ |\n| **$S_{\\text{scaled}}$** | $\\text{Scaled-Beta}_2\\left(\\frac{n-1}{2}, \\frac{1}{2}, \\lambda\\right)$ | $1$ | $\\frac{1}{1 + \\frac{(\\mu - \\mu_0)^2}{\\sigma^2}}$ |\n\n: Expected Values of Test Statistics Under Null and Alternative Hypotheses {#tbl-expected-values}\n\n**Key Interpretation:**\nAll statistics are functionally driven by the signal energy. Notably, for **$S_{\\text{scaled}}$**, the sample size $n$ cancels out in the approximate mean. This makes it a direct measure of the ratio between Noise Variance and Total Variance (Noise + Signal) in the population distributions, connected to the Rao-Blackwell decomposition of variances.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}