{
  "hash": "25955747f735deead458f882ddb9d5e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: \n  html: default\n  pdf: default\n---\n\n---\n\n# Inference for A Multiple Linear Regression Model\n\n## Linear Models and Least Square Estimator\n\n### Assumptions in Linear Models\n\nSuppose that on a random sample of $n$ units (patients, animals, trees, etc.) we observe a response variable $Y$ and explanatory variables $X_{1},...,X_{k}$. Our data are then $(y_{i},x_{i1},...,x_{ik})$, $i=1,...,n$, or in vector/matrix form $y, x_{1},...,x_{k}$ where $y=(y_{1},...,y_{n})$ and $x_{j}=(x_{1j},...,x_{nj})^{T}$ or $y, X$ where $X=(x_{1},...,x_{k})$.\n\nEither by design or by conditioning on their observed values, $x_{1},...,x_{k}$ are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:\n\n**Assumptions on Linear Models**\n\n* **A1. (Additive Error)**\n$y=\\mu+e$ where $e=(e_{1},...,e_{n})^{T}$ is an unobserved random vector with $E(e)=0$. This implies that $\\mu=E(y)$ is the unknown mean of $y$.\n\n* **A2. (Linearity)**\n$\\mu=\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}=X\\beta$ where $\\beta_{1},...,\\beta_{k}$ are unknown parameters. This assumption says that $E(y)=\\mu\\in\\text{Col}(X)$ (lies in the column space of $X$); i.e., it is a linear combination of explanatory vectors $x_{1},...,x_{k}$ with coefficients the unknown parameters in $\\beta=(\\beta_{1},...,\\beta_{k})^{T}$. Note that it is linear in $\\beta_{1},...,\\beta_{k}$, not necessarily in the $x$'s.\n\n* **A3. (Independence)**\n$e_{1},...,e_{n}$ are independent random variables (and therefore so are $y_{1},...,y_{n})$.\n\n* **A4. (Homoscedasticity)**\n$e_{1},...,e_{n}$ all have the same variance $\\sigma^{2}$; that is, $\\text{Var}(e_{1})=\\cdot\\cdot\\cdot=\\text{Var}(e_{n})=\\sigma^{2}$ which implies $\\text{Var}(y_{1})=\\cdot\\cdot\\cdot=\\text{Var}(y_{n})=\\sigma^{2}$.\n\n* **A5. (Normality)**\n$e\\sim N_{n}(0,\\sigma^{2}I_{n})$.\n\n\n### Matrix Formulation\n\nThe model can be written algebraically as:\n$$y_{i}=\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdot\\cdot\\cdot+\\beta_{k}x_{ik}, \\quad i=1,...,n$$\n\nOr in matrix notation:\n$$\n\\begin{pmatrix}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_{11} & x_{12} & \\cdot\\cdot\\cdot & x_{1k}\\\\\n1 & x_{21} & x_{22} & \\cdot\\cdot\\cdot & x_{2k}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdot\\cdot\\cdot & x_{nk}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n$$\n\nThis is expressed compactly as:\n$$y=X\\beta+e$$\nwhere $X$ is the design matrix, and $e \\sim N_n(0, \\sigma^2 I)$. Alternatively:\n$$y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}+e$$\n\nTaken together, all five assumptions can be stated more succinctly as:\n$$y\\sim N_{n}(X\\beta,\\sigma^{2}I)$$\nwith the mean vector $\\mu_{y}=X\\beta\\in \\text{Col}(X)$.\n\n:::{.callout-important}\n### Coefficients and Variance of Reduced Models\nThe effect of a parameter and the magnitude of the error variance depend upon what other explanatory variables are present in the model. For example, the coefficients $\\beta_{0}, \\beta_{1}$ and error standard deviation $\\sigma$ in the model:\n$$y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+e, \\quad \\text{Var}(e) = \\sigma^2 I$$\nwill typically be different than $\\beta_{0}^{*}, \\beta_{1}^{*}$ and $\\sigma^*$ in the model:\n$$y=\\beta_{0}^{*}j_{n}+\\beta_{1}^{*}x_{1}+e^*, \\quad \\text{Var}(e^*) = (\\sigma^*)^2 I$$\nIn this context, $\\beta_0^*$ and $\\beta_1^*$ are the population-projected coefficients of the full model. Furthermore, $\\sigma^*$ will typically be larger than $\\sigma$, as the error term $e^*$ absorbs the variation previously explained by $x_2$.\n:::\n\n::: {.callout-important}\nWe will first consider the case that $\\text{rank}(X)=k+1$.\n:::\n\n### Least Squares Estimator of $\\beta$ and Fitted Value $\\hat Y$\n\n::: {#def-least-squares name=\"Least Squares Estimator\"}\nThe **Least Squares Estimator (LSE)** of $\\beta$, denoted as $\\hat{\\beta}$, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses $y$ and the fitted values $X\\hat{\\beta}$.\n$$\nQ(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 = (y - X\\beta)'(y - X\\beta)\n$$\n:::\n\n::: {#thm-leastsquare}\n### Least Squares Estimator\n\nConsider the linear model $y = X\\beta + e$, where $X$ is of full column rank. The Ordinary Least Squares (OLS) estimator $\\hat{\\beta}$ is given by the closed-form solution:\n\n$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n\nConsequently, the vector of fitted values $\\hat{y}$ is the orthogonal projection of $y$ onto $\\text{Col}(X)$:\n\n$$\\hat{y} = X\\hat{\\beta} = Hy$$\n\nwhere $H = X(X'X)^{-1}X'$ is the orthogonal projection matrix (hat matrix).\n:::\n\n::: {.proof}\nThe derivation relies on the geometry of orthogonal projections.\n\n**1. Obtaining the Fitted Values $\\hat{y}$**\n\nIn the linear model, the systematic component $E[y]$ is constrained to lie in the column space of $X$, denoted as $\\text{Col}(X)$. We seek the vector in $\\text{Col}(X)$ that is \"closest\" to the observed data $y$. This vector is the **orthogonal projection** of $y$ onto $\\text{Col}(X)$, denoted as $\\hat{y}$. Using the projection matrix $H = X(X'X)^{-1}X'$, we have:\n\n$$\\hat{y} = Hy = X(X'X)^{-1}X' y$$\n\n**2. Obtaining $\\hat{\\beta}$ by Solving $X\\beta = \\hat{y}$**\n\nSince $\\hat{y}$ is a projection onto $\\text{Col}(X)$, the system $X\\hat{\\beta} = \\hat{y}$ is consistent. To isolate $\\hat{\\beta}$, we pre-multiply both sides by $(X'X)^{-1}X'$:\n\n$$\n\\begin{aligned}\n(X'X)^{-1}X' (X\\hat{\\beta}) &= (X'X)^{-1}X' \\hat{y} \\\\\n\\underbrace{(X'X)^{-1}(X'X)}_{I} \\hat{\\beta} &= (X'X)^{-1}X' \\hat{y} \\\\\n\\hat{\\beta} &= (X'X)^{-1}X' \\hat{y}\n\\end{aligned}\n$$\n\nFinally, we express the estimator in terms of the observed $y$. Because $\\hat{y}$ is an orthogonal projection, the residual $y - \\hat{y}$ is orthogonal to the columns of $X$, implying $X'\\hat{y} = X'y$. Substituting this into the equation above yields the result:\n\n$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n:::\n\n\n### Properties of the Estimator $\\hat \\beta$\n\n::: {#thm-unbiased name=\"Unbiasedness of $\\hat \\beta$\"}\nIf $E(y)=X\\beta$, then $\\hat{\\beta}$ is an unbiased estimator for $\\beta$.\n:::\n\n::: {.proof}\n$$\n\\begin{aligned}\nE(\\hat{\\beta}) &= E[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}E(y) \\quad \\text{[using linearity of expectation]} \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}X\\beta \\\\\n&= \\beta\n\\end{aligned}\n$$\n:::\n\n::: {#thm-covariance name=\"Variance of $\\hat \\beta$\"}\nIf $\\text{Var}(y)=\\sigma^{2}I$, the covariance matrix for $\\hat{\\beta}$ is given by $\\sigma^{2}(X^{\\prime}X)^{-1}$.\n:::\n\n::: {.proof}\n$$\n\\begin{aligned}\n\\text{Var}(\\hat{\\beta}) &= \\text{Var}[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}\\text{Var}(y)[(X^{\\prime}X)^{-1}X^{\\prime}]^{\\prime} \\quad \\text{[using } \\text{Var}(Ay) = A \\text{Var}(y) A'] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}(\\sigma^{2}I)X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}X^{\\prime}X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}\n\\end{aligned}\n$$\n:::\n\n**Note:** These theorems require no assumption of normality.\n\n\n## Best Linear Unbiased Estimator (BLUE)\n\n::: {#thm-gauss-markov name=\"Gauss-Markov Theorem\"}\nIf $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, the least-squares estimators $\\hat{\\beta}_{j}, j=0,1,...,k$ have minimum variance among all linear unbiased estimators.\n:::\n\n::: {.proof}\nWe consider a linear estimator $Ay$ of $\\beta$ and seek the matrix $A$ for which $Ay$ is a minimum variance unbiased estimator.\n\n**1. Unbiasedness Condition:**\nIn order for $Ay$ to be an unbiased estimator of $\\beta$, we must have $E(Ay)=\\beta$. Using the assumption $E(y)=X\\beta$, this is expressed as:\n$$E(Ay) = A E(y) = AX\\beta = \\beta$$\nwhich implies the condition $AX=I_{k+1}$ since the relationship must hold for any $\\beta$.\n\n**2. Minimizing Variance:**\nThe covariance matrix for the estimator $Ay$ is:\n$$\\text{Var}(Ay) = A \\text{Var}(y) A' = A(\\sigma^2 I) A' = \\sigma^2 AA'$$\nWe need to choose $A$ (subject to $AX=I$) so that the diagonal elements of $AA'$ are minimized.\n\nTo relate $Ay$ to $\\hat{\\beta}=(X'X)^{-1}X'y$, we define $\\hat{A} = (X'X)^{-1}X'$ and write $A = (A - \\hat{A}) + \\hat{A}$. Then:\n$$AA' = [(A - \\hat{A}) + \\hat{A}] [(A - \\hat{A}) + \\hat{A}]'$$\nExpanding this, the cross terms vanish because $(A - \\hat{A})\\hat{A}' = A\\hat{A}' - \\hat{A}\\hat{A}'$.\nNote that $\\hat{A}\\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}$.\nAlso, $A\\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}$ (since $AX=I$).\nThus, $(A - \\hat{A})\\hat{A}' = 0$.\n\nThe expansion simplifies to:\n$$AA' = (A - \\hat{A})(A - \\hat{A})' + \\hat{A}\\hat{A}'$$\nThe matrix $(A - \\hat{A})(A - \\hat{A})'$ is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of $AA'$, we must set $A - \\hat{A} = 0$, which implies $A = \\hat{A}$.\n\nThus, the minimum variance estimator is:\n$$Ay = (X'X)^{-1}X'y = \\hat{\\beta}$$\n:::\n\n\n### Notes on Gauss-markov\n\n1.  **Distributional Generality:** The remarkable feature of the Gauss-Markov theorem is that it holds for *any* distribution of $y$; normality is not required. The only assumptions used are linearity ($E(y)=X\\beta$) and homoscedasticity ($\\text{Var}(y)=\\sigma^2 I$).\n\n2.  **Extension to All Linear Combinations:** The theorem extends beyond just the parameter vector $\\beta$ to any linear combination of the parameters.\n\n3.  **Scaling Invariance:** The predictions made by the model are invariant to the scaling of the explanatory variables.\n\n\n::: {#cor-linear-combo name=\"BLUE for All Linear Combinations\"}\nIf $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, the best linear unbiased estimator of the scalar $a'\\beta$ is $a'\\hat{\\beta}$, where $\\hat{\\beta}$ is the least-squares estimator.\n:::\n\n::: {.proof}\nLet $\\tilde{\\beta} = Ay$ be any other linear unbiased estimator of $\\beta$. The variance of the linear combination $a'\\tilde{\\beta}$ is:\n$$\n\\frac{1}{\\sigma^2}\\text{Var}(a'\\tilde{\\beta}) = \\frac{1}{\\sigma^2}\\text{Var}(a'Ay) = a'AA'a\n$$\nFrom the proof of the Gauss-Markov theorem, we established that $AA' = (A-\\hat{A})(A-\\hat{A})' + (X'X)^{-1}$ where $\\hat{A} = (X'X)^{-1}X'$. Substituting this into the variance equation:\n$$\na'AA'a = a'(A-\\hat{A})(A-\\hat{A})'a + a'(X'X)^{-1}a\n$$\nThe term $a'(A-\\hat{A})(A-\\hat{A})'a$ is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore:\n$$\na'AA'a \\ge a'(X'X)^{-1}a = \\frac{1}{\\sigma^2}\\text{Var}(a'\\hat{\\beta})\n$$\nThe variance is minimized when $A=\\hat{A}$ (specifically when the first term is zero), proving that $a'\\hat{\\beta}$ has the minimum variance among all linear unbiased estimators.\n:::\n\n\n\n::: {#thm-scaling name=\"Scaling Explanatory Variables\"}\nIf $x=(1,x_{1},...,x_{k})'$ and $z=(1,c_{1}x_{1},...,c_{k}x_{k})'$, then the fitted values are identical: $\\hat{y} = \\hat{\\beta}'x = \\hat{\\beta}_{z}'z$.\n:::\n\n::: {.proof}\nLet $D = \\text{diag}(1, c_1, ..., c_k)$ such that the design matrix is transformed to $Z = XD$. The LSE for the transformed data is:\n$$\n\\begin{aligned}\n\\hat{\\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\\\\n&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\\\\n&= D^{-1}(X'X)^{-1}X'y = D^{-1}\\hat{\\beta}\n\\end{aligned}\n$$\n. Then, the prediction is:\n$$\n\\hat{\\beta}_z' z = (D^{-1}\\hat{\\beta})' (Dx) = \\hat{\\beta}' (D^{-1})' D x = \\hat{\\beta}'x\n$$\n.\n:::\n\n### Limitations: Restriction to Unbiased Estimators \n\nIt is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of **linear** and **unbiased** estimators.\n\n* **Assumption Sensitivity:** If the assumptions of linearity ($E(y)=X\\beta$) and homoscedasticity ($\\text{Var}(y)=\\sigma^2 I$) do not hold, $\\hat{\\beta}$ may be biased or may have a larger variance than other estimators.\n* **Unbiasedness Constraint:** The theorem does not compare $\\hat{\\beta}$ to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only \"best\" (minimum variance) among those estimators that satisfy the unbiasedness constraint.\n\n\n## Estimator of Error Variance\n\nWe estimate $\\sigma^{2}$ by the residual mean square:\n\n::: {#def-s2 name=\"Residual Variance Estimator\"}\n$$s^{2} = \\frac{1}{n-k-1} \\sum_{i=1}^{n}(y_{i}-x_{i}'\\hat{\\beta})^{2} = \\frac{\\text{SSE}}{n-k-1}$$\nwhere $\\text{SSE} = (y-X\\hat{\\beta})'(y-X\\hat{\\beta})$.\n:::\n\nAlternatively, SSE can be written as:\n$$\\text{SSE} = y'y - \\hat{\\beta}'X'y$$\nThis is often useful for computation ($y'y$ is the total sum of squares of the raw data).\n\n### Unbiasedness of $s^2$\n\n::: {#thm-unbiased-s2 name=\"Unbiasedness of s-squared\"}\nIf $s^{2}$ is defined as above, and if $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, then $E(s^{2})=\\sigma^{2}$.\n:::\n\n::: {.proof}\nWe use the Hat Matrix $H = X(X'X)^{-1}X'$, which projects $y$ onto $\\text{Col}(X)$. Thus, $\\hat{y} = Hy$.\nThe residuals are $y - \\hat{y} = (I - H)y$. The Sum of Squared Errors is:\n$$\\text{SSE} = \\|(I-H)y\\|^2 = y'(I-H)'(I-H)y$$\nSince $H$ is symmetric and idempotent, $(I-H)$ is also symmetric and idempotent. Thus:\n$$\\text{SSE} = y'(I-H)y$$\n\nTo find the expectation, we use the trace trick for quadratic forms: $E[y'Ay] = \\text{tr}(A\\text{Var}(y)) + E[y]'A E[y]$.\n$$\n\\begin{aligned}\nE(\\text{SSE}) &= E[y'(I-H)y] \\\\\n&= \\text{tr}((I-H)\\sigma^2 I) + (X\\beta)'(I-H)(X\\beta) \\\\\n&= \\sigma^2 \\text{tr}(I-H) + \\beta'X'(I-H)X\\beta\n\\end{aligned}\n$$\n**Trace Term:** $\\text{tr}(I_n - H) = \\text{tr}(I_n) - \\text{tr}(H) = n - (k+1)$, since $\\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_{k+1}) = k+1$.\n\n**Non-centrality Term:** Since $HX = X$, we have $(I-H)X = 0$. Therefore, the second term vanishes: $\\beta'X'(I-H)X\\beta = 0$.\n\nCombining these:\n$$E(\\text{SSE}) = \\sigma^2(n - k - 1)$$\nDividing by the degrees of freedom $(n-k-1)$, we get $E(s^2) = \\sigma^2$.\n:::\n\n## Distributions Under Normality\n\nIf we add Assumption A5 ($y \\sim N_n(X\\beta, \\sigma^2 I)$), we can derive the exact sampling distributions.\n\n::: {#cor-cov-beta name=\"Estimated Covariance of Beta\"}\nAn unbiased estimator of $\\text{Cov}(\\hat{\\beta})$ is given by:\n$$\\widehat{\\text{Cov}}(\\hat{\\beta}) = s^{2}(X'X)^{-1}$$\n:::\n\n::: {#thm-sampling-dist name=\"Sampling Distributions\"}\nUnder assumptions A1-A5:\n\n1.  $\\hat{\\beta} \\sim N_{k+1}(\\beta, \\sigma^{2}(X'X)^{-1})$.\n2.  $(n-k-1)s^{2}/\\sigma^{2} \\sim \\chi^{2}(n-k-1)$.\n3.  $\\hat{\\beta}$ and $s^{2}$ are independent.\n:::\n\n::: {.proof}\n**Part (i):** Since $\\hat{\\beta} = (X'X)^{-1}X'y$ is a linear transformation of the normal vector $y$, it is also normally distributed. We already established its mean and variance in @thm-unbiased and @thm-covariance.\n\n**Part (ii):** We showed $\\text{SSE} = y'(I-H)y$. Since $(I-H)$ is idempotent with rank $n-k-1$, and $(I-H)X\\beta = 0$, by the theory of quadratic forms in normal variables, $\\text{SSE}/\\sigma^2 \\sim \\chi^2(n-k-1)$.\n\n**Part (iii):** $\\hat{\\beta}$ depends on $Hy$ (or $X'y$), while $s^2$ depends on $(I-H)y$. Since $H(I-H) = H - H^2 = 0$, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.\n:::\n\n## Maximum Likelihood Estimator (MLE)\n\n::: {#thm-mle name=\"MLE for Linear Regression\"}\nIf $y \\sim N_n(X\\beta, \\sigma^2 I)$, the Maximum Likelihood Estimators are:\n$$\n\\hat{\\beta}_{\\text{MLE}} = (X'X)^{-1}X'y\n$$\n$$\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}(y - X\\hat{\\beta})'(y - X\\hat{\\beta}) = \\frac{\\text{SSE}}{n}\n$$\n:::\n\n::: {.proof}\nThe log-likelihood function is:\n$$ \\ln L(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta) $$\nMaximizing this with respect to $\\beta$ is equivalent to minimizing the quadratic term $(y - X\\beta)'(y - X\\beta)$, which yields the Least Squares Estimator.\nDifferentiating with respect to $\\sigma^2$ and setting to zero yields $\\hat{\\sigma}^2 = \\text{SSE}/n$.\n:::\n\n**Note:** The MLE for $\\sigma^2$ is biased (denominator $n$), whereas $s^2$ is unbiased (denominator $n-k-1$).\n\n\n\n## Linear Models in Centered Form\n\nThe regression model can be written in a centered form by subtracting the means of the explanatory variables:\n$$y_{i}=\\alpha+\\beta_{1}(x_{i1}-\\overline{x}_{1})+\\beta_{2}(x_{i2}-\\overline{x}_{2})+\\cdot\\cdot\\cdot+\\beta_{k}(x_{ik}-\\overline{x}_{k})+e_{i}$$\nfor $i=1,...,n$, where the intercept term is adjusted:\n$$\\alpha=\\beta_{0}+\\beta_{1}\\overline{x}_{1}+\\beta_{2}\\overline{x}_{2}+\\cdot\\cdot\\cdot+\\beta_{k}\\overline{x}_{k}$$\nand $\\overline{x}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}$.\n\n### Matrix Formulation\n\nIn matrix form, the equivalence between the original model and the centered model is:\n$$y = X\\beta + e = (j_n, X_c)\\begin{pmatrix} \\alpha \\\\ \\beta_{1} \\end{pmatrix} + e$$\nwhere $\\beta_{1}=(\\beta_{1},...,\\beta_{k})^{T}$ represents the slope coefficients, and $X_c$ is the centered design matrix:\n$$X_c = (I - P_{j_n})X_1$$\nHere, $X_1$ consists of the original columns of $X$ excluding the intercept column.\n\n\n\nTo see the structure of $X_c$, we first calculate the projection of the data onto the intercept space, $P_{j_n}X_1$:\n$$\n\\begin{aligned}\nP_{j_n}X_1 &= \\frac{1}{n}j_n j_n' X_1 \\\\\n&= \\begin{pmatrix} 1/n & 1/n & \\cdots & 1/n \\\\ 1/n & 1/n & \\cdots & 1/n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1/n & 1/n & \\cdots & 1/n \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n$$\nThis results in a matrix where every row is the vector of column means. Subtracting this from $X_1$ gives $X_c$:\n$$\n\\begin{aligned}\nX_c &= X_1 - P_{j_n}X_1 \\\\\n&= \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} - \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix} \\\\\n&= \\begin{pmatrix} x_{11} - \\bar{x}_1 & x_{12} - \\bar{x}_2 & \\cdots & x_{1k} - \\bar{x}_k \\\\ x_{21} - \\bar{x}_1 & x_{22} - \\bar{x}_2 & \\cdots & x_{2k} - \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} - \\bar{x}_1 & x_{n2} - \\bar{x}_2 & \\cdots & x_{nk} - \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n$$\n\n\n### Estimation in Centered Form\n\nBecause the column space of the intercept $j_n$ is orthogonal to the columns of $X_c$ (since columns of $X_c$ sum to zero), the cross-product matrix becomes block diagonal:\n$$\n\\begin{pmatrix} j_n' \\\\ X_c' \\end{pmatrix} (j_n, X_c) = \\begin{pmatrix} j_n'j_n & j_n'X_c \\\\ X_c'j_n & X_c'X_c \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}\n$$\n\n::: {#thm-centered-estimators name=\"Centered Estimators\"}\nThe least squares estimators for the centered parameters are:\n$$\n\\begin{pmatrix} \\hat{\\alpha} \\\\ \\hat{\\beta}_{1} \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}^{-1} \\begin{pmatrix} j_n'y \\\\ X_c'y \\end{pmatrix} = \\begin{pmatrix} \\bar{y} \\\\ (X_c'X_c)^{-1}X_c'y \\end{pmatrix}\n$$\nThus:\n\n1.  $\\hat{\\alpha} = \\bar{y}$ (The sample mean of $y$).\n2.  $\\hat{\\beta}_{1} = S_{xx}^{-1}S_{xy}$, using the sample covariance notations.\n:::\n\nRecovering the original intercept:\n$$ \\hat{\\beta}_0 = \\hat{\\alpha} - \\hat{\\beta}_1 \\bar{x}_1 - \\dots - \\hat{\\beta}_k \\bar{x}_k = \\bar{y} - \\hat{\\beta}_{1}'\\bar{x} $$\n\n## Decomposition of Sum of Squares \n\nWe partition the total variation based on the orthogonal subspaces.\n\n::: {#def-ss-components name=\"Sum of Squares Components\"}\nThe total variation is decomposed as $\\text{SST} = \\text{SSR} + \\text{SSE}$.\n\n1.  **Total Sum of Squares (SST):** The squared length of the centered response vector.\n    $$\\text{SST} = \\|y - \\bar{y}j_n\\|^2 = \\|(I - P_{j_n})y\\|^2$$\n\n2.  **Regression Sum of Squares (SSR):** The variation explained by the regressors $X_c$.\n    $$\\text{SSR} = \\|\\hat{y} - \\bar{y}j_n\\|^2 = \\|P_{X_c}y\\|^2 = \\hat{\\beta}_1' X_c' X_c \\hat{\\beta}_1$$\n\n3.  **Sum of Squared Errors (SSE):** The residual variation.\n    $$\\text{SSE} = \\|y - \\hat{y}\\|^2 = \\|(I - H)y\\|^2$$\n:::\n\n\n### 3D Visualization of Decomposition of $y$\n\nWe partition the total variation in $y$ based on the orthogonal subspaces.\n\n1.  **Space of the Mean:** $L(j_n)$, spanned by the intercept vector $j_n$.\n2.  **Space of the Regressors:** $L(X_c)$, spanned by the centered predictors $X_c$.\n3.  **Error Space:** $\\text{Col}(X)^\\perp$, orthogonal to the model space.\n\nThe vector $y$ can be decomposed into three orthogonal components:\n$$y = \\bar{y}j_n + P_{X_c}y + (y - \\hat{y})$$\nVisually, this corresponds to projecting the vector $y$ onto three orthogonal axes.\n\n**Interactive Visualization:**\n\nWe generate a cloud of 100 observations of $y$ from $N(\\mu, \\sigma=1)$ where $\\mu = (5,5,0)$. The projections onto the Model Plane ($z=0$) are highlighted in **red**, and the projections onto the error axis ($z$) are in **yellow**.\n\n\n\n::: {.panel-tabset}\n\n#### Effect Exists (signal){.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-d96970b9501c191ad406\" style=\"width:100%;height:576px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d96970b9501c191ad406\">{\"x\":{\"visdat\":{\"30445d38b1b8\":[\"function () \",\"plotlyVisDat\"],\"3044351bd6d8\":[\"function () \",\"data\"],\"304436b61a25\":[\"function () \",\"data\"],\"30446d6e8f32\":[\"function () \",\"data\"],\"3044757b53be\":[\"function () \",\"data\"],\"304473b71b33\":[\"function () \",\"data\"]},\"cur_data\":\"304473b71b33\",\"attrs\":{\"30445d38b1b8\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"surface\",\"x\":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],\"y\":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],\"opacity\":0.29999999999999999,\"colorscale\":[[0,1],[\"steelblue\",\"steelblue\"]],\"showscale\":false,\"name\":\"Model Space\",\"inherit\":true},\"3044351bd6d8\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":{},\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"red\",\"symbol\":\"diamond\",\"opacity\":0.80000000000000004},\"name\":\"Proj on Floor\",\"inherit\":true},\"304436b61a25\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":4,\"color\":\"black\",\"symbol\":\"circle\",\"opacity\":0.59999999999999998},\"name\":\"Data Cloud\",\"inherit\":true},\"30446d6e8f32\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"blue\",\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998},\"name\":\"Proj L(jn)\",\"inherit\":true},\"3044757b53be\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":{},\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"green\",\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998},\"name\":\"Proj L(Xc)\",\"inherit\":true},\"304473b71b33\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":{},\"marker\":{\"size\":4,\"color\":\"gold\",\"symbol\":\"circle-open\",\"opacity\":0.80000000000000004},\"name\":\"Error\",\"inherit\":true},\"304473b71b33.1\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,4],\"z\":[0,0],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"Mean Vector\",\"inherit\":true},\"304473b71b33.2\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[0,0],\"line\":{\"color\":\"blue\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to X\",\"inherit\":true},\"304473b71b33.3\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to Y\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":0,\"l\":0,\"t\":30,\"r\":0},\"title\":\"Scenario A: Effect Exists\",\"scene\":{\"xaxis\":{\"title\":\"L(j<sub>n<\\/sub>)\",\"range\":[0,8]},\"yaxis\":{\"title\":\"L(X<sub>c<\\/sub>)\",\"range\":[-4,8]},\"zaxis\":{\"title\":\"Col(X)<sup>&perp;<\\/sup>\",\"range\":[-4,4]},\"aspectmode\":\"cube\",\"camera\":{\"eye\":{\"x\":1.6000000000000001,\"y\":1.6000000000000001,\"z\":0.59999999999999998}}},\"showlegend\":false,\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"z<br />z\",\"ticklen\":2},\"colorscale\":[[0,\"steelblue\"],[1,\"steelblue\"]],\"showscale\":false,\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"surface\",\"x\":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],\"y\":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],\"opacity\":0.29999999999999999,\"name\":\"Model Space\",\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.464104386762211,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727321,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466095],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"red\",\"size\":4,\"symbol\":\"diamond\",\"opacity\":0.80000000000000004,\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"name\":\"Proj on Floor\",\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.464104386762211,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727321,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466095],\"z\":[-0.2802378232761063,-0.11508874474163998,0.77935415707456202,0.035254195712288001,0.064643867580473122,0.85753249344164051,0.23045810299460115,-0.63253061730326698,-0.34342642594676304,-0.22283098504997903,0.61204089871973077,0.17990691352868191,0.20038572529702608,0.055341357972559853,-0.27792056737703746,0.89345656840153909,0.2489252391146197,-0.9833085783148191,0.35067795078184277,-0.23639570386396702,-0.53391185299342259,-0.10898745732914757,-0.51300222415361985,-0.36444561464557013,-0.31251963392462834,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.14753574149613558,0.44756283052251111,0.43906674376652111,0.41079054081874355,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619134,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.56155429160167458,-0.20144241764953799,-0.23332767681160943,0.38998255916815894,-0.041684533235914638],\"marker\":{\"color\":\"black\",\"size\":4,\"symbol\":\"circle\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"name\":\"Data Cloud\",\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"blue\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"name\":\"Proj L(jn)\",\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[4.1266592569973772,3.9857266223256484,3.9785647713543422,4.6843011420072287,3.8871145071703661,4.7582353022147696,3.2256235978848893,4.2923068748180349,4.0619271219223068,4.1079707843719859,4.1898197413799414,3.7488382734453487,3.8333963081652898,3.4907123084464557,3.464104386762211,4.1517643207021289,4.2241048893147131,4.0265021133652521,4.4611337339398691,5.0250423428135722,3.7544844169717324,2.8454155621795936,4.5028692622311279,3.6453996187088036,3.655995691766321,4.5127856848483496,3.8576134964744955,3.3896411438727321,4.0906517398745752,3.9305543187804779,4.0028820929499433,4.1926402005631651,3.8146699841037952,4.3221882742594167,3.8897567190906246,4.1658909819578485,4.5484195065746738,4.217590745416901,3.8370342072343866,4.5744038092255472,4.4967519279810597,4.2741984797540349,4.1193658675557208,3.6860469619803142,4.6803262242650039,3.6998702064264366,5.0936664965082885,4.7663053130925945,3.8821498204497615,3.4867895498466095],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"green\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"name\":\"Proj L(Xc)\",\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"line\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[-0.2802378232761063,-0.11508874474163998,0.77935415707456202,0.035254195712288001,0.064643867580473122,0.85753249344164051,0.23045810299460115,-0.63253061730326698,-0.34342642594676304,-0.22283098504997903,0.61204089871973077,0.17990691352868191,0.20038572529702608,0.055341357972559853,-0.27792056737703746,0.89345656840153909,0.2489252391146197,-0.9833085783148191,0.35067795078184277,-0.23639570386396702,-0.53391185299342259,-0.10898745732914757,-0.51300222415361985,-0.36444561464557013,-0.31251963392462834,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.14753574149613558,0.44756283052251111,0.43906674376652111,0.41079054081874355,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619134,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.56155429160167458,-0.20144241764953799,-0.23332767681160943,0.38998255916815894,-0.041684533235914638],\"marker\":{\"color\":\"gold\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.80000000000000004,\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"name\":\"Error\",\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"line\":{\"color\":\"rgba(140,86,75,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,4],\"z\":[0,0],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"Mean Vector\",\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[0,0],\"line\":{\"color\":\"blue\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to X\",\"marker\":{\"color\":\"rgba(127,127,127,1)\",\"line\":{\"color\":\"rgba(127,127,127,1)\"}},\"error_y\":{\"color\":\"rgba(127,127,127,1)\"},\"error_x\":{\"color\":\"rgba(127,127,127,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to Y\",\"marker\":{\"color\":\"rgba(188,189,34,1)\",\"line\":{\"color\":\"rgba(188,189,34,1)\"}},\"error_y\":{\"color\":\"rgba(188,189,34,1)\"},\"error_x\":{\"color\":\"rgba(188,189,34,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nScenario 1: Significant regression effect ($\\beta_1 \not= 0$). The mean vector projects significantly onto the predictor space.\n:::\n:::\n\n\n#### No Effect (noise){.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-bbf49928d385f1dd9e5c\" style=\"width:100%;height:576px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-bbf49928d385f1dd9e5c\">{\"x\":{\"visdat\":{\"3044560b2ad\":[\"function () \",\"plotlyVisDat\"],\"3044f727e9d\":[\"function () \",\"data\"],\"304425d67d57\":[\"function () \",\"data\"],\"304424beee29\":[\"function () \",\"data\"],\"30447305d897\":[\"function () \",\"data\"],\"30444cedc80\":[\"function () \",\"data\"]},\"cur_data\":\"30444cedc80\",\"attrs\":{\"3044560b2ad\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"surface\",\"x\":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],\"y\":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],\"opacity\":0.29999999999999999,\"colorscale\":[[0,1],[\"steelblue\",\"steelblue\"]],\"showscale\":false,\"name\":\"Model Space\",\"inherit\":true},\"3044f727e9d\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":{},\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"red\",\"symbol\":\"diamond\",\"opacity\":0.80000000000000004},\"name\":\"Proj on Floor\",\"inherit\":true},\"304425d67d57\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":{},\"z\":{},\"marker\":{\"size\":4,\"color\":\"black\",\"symbol\":\"circle\",\"opacity\":0.59999999999999998},\"name\":\"Data Cloud\",\"inherit\":true},\"304424beee29\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":{},\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"blue\",\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998},\"name\":\"Proj L(jn)\",\"inherit\":true},\"30447305d897\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":{},\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"size\":4,\"color\":\"green\",\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998},\"name\":\"Proj L(Xc)\",\"inherit\":true},\"30444cedc80\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":{},\"marker\":{\"size\":4,\"color\":\"gold\",\"symbol\":\"circle-open\",\"opacity\":0.80000000000000004},\"name\":\"Error\",\"inherit\":true},\"30444cedc80.1\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"Mean Vector\",\"inherit\":true},\"30444cedc80.2\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"blue\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to X\",\"inherit\":true},\"30444cedc80.3\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to Y\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":0,\"l\":0,\"t\":30,\"r\":0},\"title\":\"Scenario B: No Effect\",\"scene\":{\"xaxis\":{\"title\":\"L(j<sub>n<\\/sub>)\",\"range\":[0,8]},\"yaxis\":{\"title\":\"L(X<sub>c<\\/sub>)\",\"range\":[-4,8]},\"zaxis\":{\"title\":\"Col(X)<sup>&perp;<\\/sup>\",\"range\":[-4,4]},\"aspectmode\":\"cube\",\"camera\":{\"eye\":{\"x\":1.6000000000000001,\"y\":1.6000000000000001,\"z\":0.59999999999999998}}},\"showlegend\":false,\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"z<br />z\",\"ticklen\":2},\"colorscale\":[[0,\"steelblue\"],[1,\"steelblue\"]],\"showscale\":false,\"z\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"type\":\"surface\",\"x\":[0,0.36842105263157893,0.73684210526315785,1.1052631578947367,1.4736842105263157,1.8421052631578947,2.2105263157894735,2.5789473684210527,2.9473684210526314,3.3157894736842102,3.6842105263157894,4.0526315789473681,4.4210526315789469,4.7894736842105257,5.1578947368421053,5.5263157894736841,5.8947368421052628,6.2631578947368416,6.6315789473684204,7],\"y\":[-4,-3.3684210526315788,-2.736842105263158,-2.1052631578947372,-1.4736842105263159,-0.84210526315789469,-0.21052631578947389,0.4210526315789469,1.0526315789473681,1.6842105263157894,2.3157894736842106,2.947368421052631,3.5789473684210522,4.2105263157894726,4.8421052631578938,5.473684210526315,6.1052631578947363,6.7368421052631575,7.3684210526315788,8],\"opacity\":0.29999999999999999,\"name\":\"Model Space\",\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[0.1266592569973774,-0.014273377674351509,-0.021435228645658045,0.6843011420072288,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803458,0.061927121922306892,0.10797078437198636,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354436,-0.53589561323778889,0.15176432070212903,0.22410488931471309,0.026502113365252072,0.46113373393986878,1.0250423428135722,-0.24551558302826765,-1.1545844378204062,0.50286926223112827,-0.35460038129119631,-0.344004308233679,0.51278568484834941,-0.14238650352550444,-0.61035885612726781,0.090651739874575102,-0.069445681219522312,0.0028820929499434665,0.19264020056316525,-0.18533001589620468,0.32218827425941649,-0.11024328090937531,0.16589098195784846,0.54841950657467387,0.21759074541690143,-0.16296579276561338,0.5744038092255469,0.49675192798105972,0.274198479754035,0.1193658675557206,-0.31395303801968572,0.68032622426500378,-0.30012979357356345,1.0936664965082883,0.76630531309259464,-0.11785017955023844,-0.5132104501533904],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"red\",\"size\":4,\"symbol\":\"diamond\",\"opacity\":0.80000000000000004,\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"name\":\"Proj on Floor\",\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[0.1266592569973774,-0.014273377674351509,-0.021435228645658045,0.6843011420072288,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803458,0.061927121922306892,0.10797078437198636,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354436,-0.53589561323778889,0.15176432070212903,0.22410488931471309,0.026502113365252072,0.46113373393986878,1.0250423428135722,-0.24551558302826765,-1.1545844378204062,0.50286926223112827,-0.35460038129119631,-0.344004308233679,0.51278568484834941,-0.14238650352550444,-0.61035885612726781,0.090651739874575102,-0.069445681219522312,0.0028820929499434665,0.19264020056316525,-0.18533001589620468,0.32218827425941649,-0.11024328090937531,0.16589098195784846,0.54841950657467387,0.21759074541690143,-0.16296579276561338,0.5744038092255469,0.49675192798105972,0.274198479754035,0.1193658675557206,-0.31395303801968572,0.68032622426500378,-0.30012979357356345,1.0936664965082883,0.76630531309259464,-0.11785017955023844,-0.5132104501533904],\"z\":[-0.2802378232761063,-0.11508874474163998,0.77935415707456202,0.035254195712288001,0.064643867580473122,0.85753249344164051,0.23045810299460115,-0.63253061730326698,-0.34342642594676304,-0.22283098504997903,0.61204089871973077,0.17990691352868191,0.20038572529702608,0.055341357972559853,-0.27792056737703746,0.89345656840153909,0.2489252391146197,-0.9833085783148191,0.35067795078184277,-0.23639570386396702,-0.53391185299342259,-0.10898745732914757,-0.51300222415361985,-0.36444561464557013,-0.31251963392462834,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.14753574149613558,0.44756283052251111,0.43906674376652111,0.41079054081874355,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619134,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.56155429160167458,-0.20144241764953799,-0.23332767681160943,0.38998255916815894,-0.041684533235914638],\"marker\":{\"color\":\"black\",\"size\":4,\"symbol\":\"circle\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"name\":\"Data Cloud\",\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[2.6447967181503493,3.1284418545782646,2.8766540607688134,2.8262287003011335,2.5241907163674924,2.9774861375955397,2.607547765271462,2.1660290317059316,2.8098867398561187,3.4594983045303831,2.7123265186958041,3.3039821611125166,2.191058645855418,2.9722190172377303,3.2597036019717311,3.1505766810833573,3.0528380970744715,2.6796469958473117,2.5751478269832089,2.4879356046975434,3.0588232985500627,2.5262626929075989,2.7547212781496659,2.8719539039008763,3.9219310026161036,2.6740250491522706,3.1176932861424285,3.0389804247818555,2.5190716829349356,2.9643459569382005,3.7222754292116744,3.2257520265396074,3.0206164609964699,2.7887515838301877,1.9733763892297422,3.5656686067070877,2.2696799645375889,3.3699737554386671,3.9545517846087419,2.2780534195141002,3.3508921676873555,2.8689012552987658,2.2139279204272562,2.2426661731091242,2.1992319132127034,2.7345467389148483,2.2691222075020501,3.3439583864879139,4.050054470262836,2.3564847619824105],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"blue\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"name\":\"Proj L(jn)\",\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0.1266592569973774,-0.014273377674351509,-0.021435228645658045,0.6843011420072288,-0.11288549282963381,0.75823530221476987,-0.77437640211511061,0.29230687481803458,0.061927121922306892,0.10797078437198636,0.18981974137994104,-0.25116172655465113,-0.16660369183471005,-0.50928769155354436,-0.53589561323778889,0.15176432070212903,0.22410488931471309,0.026502113365252072,0.46113373393986878,1.0250423428135722,-0.24551558302826765,-1.1545844378204062,0.50286926223112827,-0.35460038129119631,-0.344004308233679,0.51278568484834941,-0.14238650352550444,-0.61035885612726781,0.090651739874575102,-0.069445681219522312,0.0028820929499434665,0.19264020056316525,-0.18533001589620468,0.32218827425941649,-0.11024328090937531,0.16589098195784846,0.54841950657467387,0.21759074541690143,-0.16296579276561338,0.5744038092255469,0.49675192798105972,0.274198479754035,0.1193658675557206,-0.31395303801968572,0.68032622426500378,-0.30012979357356345,1.0936664965082883,0.76630531309259464,-0.11785017955023844,-0.5132104501533904],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"marker\":{\"color\":\"green\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.59999999999999998,\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"name\":\"Proj L(Xc)\",\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"line\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"markers\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"z\":[-0.2802378232761063,-0.11508874474163998,0.77935415707456202,0.035254195712288001,0.064643867580473122,0.85753249344164051,0.23045810299460115,-0.63253061730326698,-0.34342642594676304,-0.22283098504997903,0.61204089871973077,0.17990691352868191,0.20038572529702608,0.055341357972559853,-0.27792056737703746,0.89345656840153909,0.2489252391146197,-0.9833085783148191,0.35067795078184277,-0.23639570386396702,-0.53391185299342259,-0.10898745732914757,-0.51300222415361985,-0.36444561464557013,-0.31251963392462834,-0.84334665537120668,0.41889352224726234,0.076686558918257611,-0.56906846850597381,0.62690746053496338,0.21323211073840678,-0.14753574149613558,0.44756283052251111,0.43906674376652111,0.41079054081874355,0.3443201270500455,0.27695882676879441,-0.03095585528836084,-0.15298133186995838,-0.19023550050619134,-0.34735348946025635,-0.10395863900979939,-0.63269817578413223,1.0844779826692563,0.60398099915249526,-0.56155429160167458,-0.20144241764953799,-0.23332767681160943,0.38998255916815894,-0.041684533235914638],\"marker\":{\"color\":\"gold\",\"size\":4,\"symbol\":\"circle-open\",\"opacity\":0.80000000000000004,\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"name\":\"Error\",\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"line\":{\"color\":\"rgba(140,86,75,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"Mean Vector\",\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"blue\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to X\",\"marker\":{\"color\":\"rgba(127,127,127,1)\",\"line\":{\"color\":\"rgba(127,127,127,1)\"}},\"error_y\":{\"color\":\"rgba(127,127,127,1)\"},\"error_x\":{\"color\":\"rgba(127,127,127,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":4,\"dash\":\"dash\"},\"name\":\"Link to Y\",\"marker\":{\"color\":\"rgba(188,189,34,1)\",\"line\":{\"color\":\"rgba(188,189,34,1)\"}},\"error_y\":{\"color\":\"rgba(188,189,34,1)\"},\"error_x\":{\"color\":\"rgba(188,189,34,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nScenario 2: No regression effect ($\\beta_1 = 0$). The mean vector lies purely on the intercept axis.\n:::\n:::\n\n\n:::\n\n### A Diagram to Show Decomposition of Sum of Squares\n\nThe decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Geometric Decomposition: SST = SSR + SSE](lec5-est_files/figure-html/fig-ss-decomposition-legend-v2-1.png){#fig-ss-decomposition-legend-v2 fig-align='center' width=576}\n:::\n:::\n\n\n\n### Distribution of Sum of Squares\n\nWe apply the general theory of projections to the specific components defined in @def-ss-components.\n\n::: {#thm-distribution-ss-v2 name=\"Distribution of Sum of Squares\"}\nLet $y \\sim N(\\mu, \\sigma^2 I_n)$, where $\\mu \\in \\text{Col}(X)$.\nConsider the decomposition defined by the projection matrices $P_{X_c}$ and $M = I - H$.\n\n* **Independence:** The quadratic forms $\\text{SSR}$ and $\\text{SSE}$ are statistically independent because the subspaces $L(X_c)$ and $\\text{Col}(X)^\\perp$ are orthogonal.\n\n* **Distribution of SSE:** The scaled sum of squared errors follows a central Chi-squared distribution:\n  $$ \\frac{\\text{SSE}}{\\sigma^2} = \\frac{\\|(I - H)y\\|^2}{\\sigma^2} \\sim \\chi^2(n-k-1) $$\n  **Mean:**\n  $$ E[\\text{SSE}] = \\sigma^2(n-k-1) $$\n\n* **Distribution of SSR:** The scaled regression sum of squares follows a **non-central** Chi-squared distribution:\n  $$ \\frac{\\text{SSR}}{\\sigma^2} = \\frac{\\|P_{X_c}y\\|^2}{\\sigma^2} \\sim \\chi^2(k, \\lambda) $$\n  **Mean:**\n  $$ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 $$\n\n**Non-centrality Parameter ($\\lambda$):**\n$$ \\lambda = \\frac{1}{\\sigma^2} \\|P_{X_c} \\mu\\|^2 $$\nwhere $$\\|P_{X_c} \\mu\\|^2 = \\|X_c \\beta_1\\|^2 = (X_c \\beta_1)' (X_c \\beta_1) = \\beta_1' X_c' X_c \\beta_1$$\n:::\n\n::: proof\nWe apply @thm-proj-dist to the specific projection matrices identified in the definitions.\n\n* **For SSE (Error Space):**\n  $\\text{SSE}$ is defined by the projection matrix $P_V = I - H$.\n\n  - **Dimension:** The rank of $(I - H)$ is $n - \\text{rank}(X) = n - (k+1) = n - k - 1$.\n  - **Non-centrality:** Since $\\mu \\in \\text{Col}(X)$, the projection onto the orthogonal complement is zero: $\\|(I - H)\\mu\\|^2 = 0$. Thus, $\\lambda = 0$.\n  - **Expectation:** Using Part 2 of @thm-proj-dist ($E(\\|P_V y\\|^2) = \\sigma^2 \\text{rank}(P_V) + \\|P_V \\mu\\|^2$):\n    $$ E[\\text{SSE}] = \\sigma^2(n-k-1) + 0 = \\sigma^2(n-k-1) $$\n\n* **For SSR (Regression Space):**\n  $\\text{SSR}$ is defined by the projection matrix $P_V = P_{X_c}$.\n\n  - **Dimension:** The rank of $P_{X_c}$ is $(k+1) - 1 = k$.\n  - **Non-centrality:** The projection of $\\mu$ onto $L(X_c)$ is $P_{X_c}\\mu$.\n    $$ \\lambda = \\frac{1}{2\\sigma^2} \\|P_{X_c} \\mu\\|^2 $$\n\n  - **Expectation:** Using Part 2 of @thm-proj-dist:\n    $$ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 $$\n\n  This shows that while $E[\\text{SSE}]$ depends only on the noise variance and sample size, $E[\\text{SSR}]$ is inflated by the magnitude of the true regression signal $\\|P_{X_c}\\mu\\|^2$.\n:::\n\n\n## F-test for Testing Overall Regression Effect\n\nWe wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.\n\n**Hypotheses:**\n\n* **Null Hypothesis ($H_0$):** $\\beta_1 = \\beta_2 = \\dots = \\beta_k = 0$ (No regression effect).\n    This implies $\\mu \\in \\text{span}(j_n)$ and the true signal variance $\\|X_c\\beta_1\\|^2 = 0$.\n\n* **Alternative Hypothesis ($H_1$):** At least one $\\beta_j \\neq 0$.\n\n### The F-statistic {.unnumbered}\n\nWe construct the test statistic using the ratio of the Mean Squares defined previously:\n\n$$F = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)}$$\n\n### Understanding $F$ via Expectations {.unnumbered}\n\nThe logic of the F-test is transparent when we examine the expected values of the numerator and denominator:\n\n$$\n\\begin{aligned}\nE[\\text{MSE}] &= \\sigma^2 \\\\\nE[\\text{MSR}] &= \\sigma^2 + \\frac{\\|X_c \\beta_1\\|^2}{k}\n\\end{aligned}\n$$\n\n* **If $H_0$ is true:** The signal term is zero. Both Mean Squares estimate $\\sigma^2$ unbiasedly. We expect $F \\approx 1$.\n* **If $H_1$ is true:** The numerator includes the positive term $\\frac{\\|X_c \\beta_1\\|^2}{k}$. We expect $F > 1$.\n\nTherefore, we reject $H_0$ for sufficiently large values of $F$. Specifically, we reject at level $\\alpha$ if $F_{obs} > F_{\\alpha}(k, n-k-1)$.\n\n### Distributional Theory\n\nTo derive the exact sampling distribution, we rely on the independence of the sums of squares (from @thm-distribution-ss-v2) and the definition of the non-central F-distribution given in **@def-noncentral-f**.\n\n::: {#thm-regression-f-dist name=\"Distribution of Regression F-Statistic\"}\nUnder the assumption of normality, the regression F-statistic follows a **non-central F-distribution**:\n\n$$ F \\sim F(k, n-k-1, \\lambda) $$\n\nThe non-centrality parameter $\\lambda$ is determined by the ratio of the signal sum of squares to the error variance:\n$$ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} $$\n\n**Special Cases:**\n\n1.  **Under $H_1$ (Signal exists):** $\\lambda > 0$, so $F$ follows the non-central distribution.\n2.  **Under $H_0$ (No signal):** $\\beta_1 = 0 \\implies \\lambda = 0$. The distribution collapses to the **central F-distribution**:\n    $$ F \\sim F(k, n-k-1) $$\n:::\n\n::: {.proof}\nWe identify the components from @def-noncentral-f:\n\n1.  **Numerator ($X_1$):** Let $X_1 = \\text{SSR}/\\sigma^2$. From @thm-distribution-ss-v2, $X_1 \\sim \\chi^2(k, \\lambda)$.\n2.  **Denominator ($X_2$):** Let $X_2 = \\text{SSE}/\\sigma^2$. From @thm-distribution-ss-v2, $X_2 \\sim \\chi^2(n-k-1)$.\n3.  **Independence:** $X_1$ and $X_2$ are independent.\n\nSubstituting these into the F-statistic:\n$$\nF = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{(\\text{SSR}/\\sigma^2)/k}{(\\text{SSE}/\\sigma^2)/(n-k-1)} = \\frac{X_1/k}{X_2/(n-k-1)}\n$$\nBy definition @def-noncentral-f, this ratio follows $F(k, n-k-1, \\lambda)$.\n:::\n\n### Visualization of the Rejection Region\n\nThe following plot illustrates the central F-distribution (valid under $H_0$) for $k=3$ predictors and $n=20$ observations ($df_1 = 3, df_2 = 16$). An observed statistic of $F=2$ is marked, with the p-value represented by the shaded tail area.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value.](lec5-est_files/figure-html/fig-f-dist-example-1.png){#fig-f-dist-example width=576}\n:::\n:::\n\n\n\n## Raw Coefficient of Determination ($R^2$)\n\n### Definition\n\nThe $R^2$ statistic measures the proportion of total variation explained by the regression model.\n\n::: {#def-r2 name=\"R-Squared\"}\n$$R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}$$\nSince $0 \\le \\text{SSE} \\le \\text{SST}$, it follows that $0 \\le R^2 \\le 1$.\n:::\n\n\n### Expectation and Bias\n\nTo understand the bias in $R^2$, it is more illuminating to analyze the expectation of the **unexplained variance** ($1 - R^2$). This term represents the ratio of error sum of squares to the total sum of squares:\n\n$$ E[1 - R^2] = E\\left[ \\frac{\\text{SSE}}{\\text{SST}} \\right] $$\n\nUsing the first-order approximation $E[X/Y] \\approx E[X]/E[Y]$, we examine the numerator and denominator separately:\n\n$$\n\\begin{aligned}\nE[\\text{SSE}] &= \\sigma^2(n-k-1) \\\\\nE[\\text{SST}] &= \\sigma^2(n-1) + \\sigma^2\\lambda = \\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)\n\\end{aligned}\n$$\n\nSubstituting these back, we approximate the expected unexplained fraction:\n\n$$ E[1 - R^2] \\approx \\frac{\\sigma^2(n-k-1)}{\\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)} = \\frac{n-k-1}{(n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}} $$\n\n**Behavior under Null Hypothesis ($H_0$):**\nWhen there is no true signal ($\\beta_1 = 0$), the term $\\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}$ vanishes. The expected proportion of unexplained variance becomes:\n\n$$ E[1 - R^2 | H_0] \\approx \\frac{n-k-1}{n-1} $$\n\nThis result reveals the source of the bias:\n\n1.  Ideally, if predictors are noise, the model should explain nothing, and $E[1-R^2]$ should be $1$.\n2.  Instead, the expected error ratio is **less than 1**, specifically scaled by $\\frac{n-k-1}{n-1}$.\n3.  This scaling factor is exactly what the **Adjusted R-squared ($R^2_a$)** attempts to correct by multiplying the observed ratio by the inverse $\\frac{n-1}{n-k-1}$.\n\n### Exact Distribution\n\nThe $R^2$ statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.\n\n::: {#thm-r2-dist name=\"Distribution of R-Squared\"}\n$$ R^2 \\sim \\text{Beta}_1\\left( \\frac{k}{2}, \\frac{n-k-1}{2}, \\lambda \\right) $$\nwhere $\\text{df}_1 = k$ and $\\text{df}_2 = n-k-1$.\n:::\n\n\n## Adjusted R-squared ($R^2_a$)\n\nTo correct for the inflation of $R^2$ due to model complexity ($k$), we introduce the Adjusted $R^2$. This statistic penalizes the sum of squares by their degrees of freedom:\n\n$$ R^2_a = 1 - \\frac{\\text{SSE}/(n-k-1)}{\\text{SST}/(n-1)} = 1 - \\frac{\\text{MSE}}{\\text{MST}} = 1 - (1 - R^2) \\frac{n-1}{n-k-1} $$\n\n**Expectation:**\n\nUnder $H_0$, since $E[\\text{MSE}] = E[\\text{MST}] = \\sigma^2$, the estimator is asymptotically unbiased:\n\n$$ E[R^2_a | H_0] \\approx 0 $$\n\n**Variance and Stability:**\n\nWhile $R^2_a$ corrects the bias, it introduces instability. The variance of $R^2_a$ under $H_0$ can be derived from the variance of the Beta distribution:\n\n$$ \\text{Var}(R^2_a | H_0) = \\left( \\frac{n-1}{n-k-1} \\right)^2 \\text{Var}(R^2 | H_0) $$\n\nSubstituting $\\text{Var}(R^2 | H_0) = \\frac{2k(n-k-1)}{(n-1)^2(n+1)}$, we obtain:\n\n$$ \\text{Var}(R^2_a | H_0) = \\frac{2k}{(n-k-1)(n+1)} $$\n\n**Key Insight:**\n\nAs the model complexity $k$ increases relative to $n$:\n\n1.  The denominator $(n-k-1)$ shrinks.\n2.  The variance $\\text{Var}(R^2_a)$ explodes.\n\nThis implies that for high-dimensional models (large $k/n$), $R^2_a$ becomes an extremely noisy estimator, often yielding large negative values even for null models.\n\n## Population Proportion of Signals ($\\rho^2$)\n\nThe formula for the expected Adjusted $R^2$ reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation $Y_i$ into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let $\\sigma^2_\\mu$ denote the signal variance and $\\sigma^2$ denote the noise variance:\n\n$$ \\text{Var}(Y_i) = E[\\text{Var}(Y_i|x_{(i)})] + \\text{Var}(E[Y_i|x_{(i)}]) $$\n$$ \\sigma^2_Y = \\sigma^2 + \\sigma^2_\\mu $$\n\nIn our derived expectation for $R^2_a$:\n$$ E[R^2_a] \\approx \\frac{\\frac{\\|X_c\\beta_1\\|^2}{n-1}}{\\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{n-1}} $$\n\nThe term in the numerator, $\\frac{\\|X_c\\beta_1\\|^2}{n-1}$, is precisely the **sample variance of the true means** $\\mu_i$.\nLet $\\mu = X\\beta$. We can expand the centered signal vector $X_c\\beta_1$ to see this explicitly. Since $\\mu \\in \\text{Col}(X)$, we know $H\\mu = \\mu$:\n\n$$\nX_c\\beta_1 = P_{X_c} \\mu = (H - P_{j_n})\\mu = H\\mu - P_{j_n}\\mu = \\mu - \\bar{\\mu}j_n = \n\\begin{pmatrix} \n\\mu_1 - \\bar{\\mu} \\\\ \n\\mu_2 - \\bar{\\mu} \\\\ \n\\vdots \\\\ \n\\mu_n - \\bar{\\mu} \n\\end{pmatrix}\n$$\n\nThis vector represents the deviation of each observation's true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is:\n$$ \\frac{\\|X_c\\beta_1\\|^2}{n-1} = \\frac{\\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2}{n-1} = \\sigma^2_\\mu $$\n\n\n\nThus, $R^2_a$ is therefore an unbiased estimator for the **proportion of variance explained by the signal** in the population:\n$$ E[R^2_a] \\approx \\frac{\\sigma^2_\\mu}{\\sigma^2 + \\sigma^2_\\mu}$$\n\nWe will denote this 'parameter' by $\\rho^2$:\n\n$$ \\rho^2 = 1 - \\frac{\\sigma^2}{\\sigma^2_Y} = \\frac{\\sigma^2_\\mu}{\\sigma^2_Y} $$\n\n::: {.remark}\nIn the fixed covariate framework, the 'parameter' $\\rho^2$ is a function of the specific design matrix $X$, the coefficients $\\beta$, and the sample size $n$. If we assume the $x_i$ are random draws from a population, then as $n \\to \\infty$, $\\sigma^2_\\mu$ converges to $\\text{Var}(x^T\\beta)$ (where $x$ is a random vector), and $\\rho^2$ converges to the true population proportion of variance explained.\n:::\n\n::: {.callout-important title=\"MSR Is Not a Variance Estimator\"}\n\n* Observing that $E[\\text{MST}] \\approx \\sigma^2 + \\sigma^2_\\mu$ and $E[\\text{MSE}] = \\sigma^2$, we can see that the difference $\\text{MST} - \\text{MSE}$ provides a direct method-of-moments estimator for the variance of the signal itself ($\\sigma^2_\\mu$). \n\n* It is important to recognize that the commonly used **Mean Square Regression (MSR)**, defined as $\\text{SSR}/k$, is **not** an estimator of the signal variance. Because $E[\\text{MSR}] = \\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{k}$, it scales with the sample size $n$ (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting *existence* of signal), not for estimating the *magnitude* of the signal variance.\n:::\n\n## Relationship between $R^2$ and $F$ Test\n\nThe $F$-statistic for the overall regression effect is a monotonic function of the coefficient of determination. We can express $F$ directly in terms of both the standard $R^2$ and the adjusted $R^2_a$, as well as relate its expected value to the population variance components.\n\n1.  **Expressing $F$ via Standard $R^2$:**\n    Since $R^2 = \\text{SSR}/\\text{SST}$ and $1 - R^2 = \\text{SSE}/\\text{SST}$, we can substitute these into the definition of $F$:\n    $$\n    F = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)} = \\frac{(R^2 \\cdot \\text{SST}) / k}{((1 - R^2) \\cdot \\text{SST}) / (n - k - 1)} = \\frac{R^2}{1 - R^2} \\cdot \\frac{n - k - 1}{k}\n    $$\n\n2.  **Expressing $F$ via Adjusted $R^2_a$:**\n    The relationship becomes structurally identical to the population expectation if we use the estimated Signal-to-Noise Ratio. Since $\\frac{R^2_a}{1 - R^2_a} = \\frac{\\hat{\\sigma}^2_\\mu}{\\hat{\\sigma}^2}$, we have:\n    $$\n    F = 1 + \\frac{n-1}{k} \\left( \\frac{R^2_a}{1 - R^2_a} \\right)\n    $$\n    This form highlights that $F$ starts at a baseline of 1 (pure noise) and increases proportional to the estimated signal strength.\n\n3.  **Expected Value of $F$ as a function of $\\sigma^2_\\mu$ and $\\sigma^2$:**\n    Using the population signal variance $\\sigma^2_\\mu$ and noise variance $\\sigma^2$, the expected value of the $F$-statistic (using the first-order approximation $E[F] \\approx E[\\text{MSR}]/E[\\text{MSE}]$) is:\n    $$\n    E[F] \\approx 1 + \\frac{n-1}{k} \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right)\n    $$\n    The exact mean, derived from the non-central $F$ distribution, is:\n    $$\n    E[F] = \\frac{n-k-1}{n-k-3} \\left( 1 + \\frac{n-1}{k} \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right), \\quad \\text{for } n-k-1 > 3\n    $$\n\n## Confidence Interval of Population $\\rho^2$\n\n\n\nWhile $R^2_a$ provides a point estimate, we can construct an exact confidence interval for $\\rho^2$ by exploiting the distribution of the $F$-statistic.\n\n**1. The link between $\\lambda$ and $\\rho^2$:**\n\n\nRecall that the $F$-statistic follows a non-central distribution $F(k, n-k-1, \\lambda)$. The non-centrality parameter $\\lambda$ is directly related to the population $\\rho^2$. Using the variance decomposition derived above:\n\n$$ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} = (n-1) \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right) $$\n\nSubstituting the signal-to-noise ratio $\\frac{\\sigma^2_\\mu}{\\sigma^2} = \\frac{\\rho^2}{1-\\rho^2}$, we obtain a one-to-one mapping between $\\lambda$ and $\\rho^2$:\n\n$$ \\lambda(\\rho^2) = (n-1) \\left( \\frac{\\rho^2}{1-\\rho^2} \\right) $$\n\n**2. Inverting the Test Statistic:**\n\nWe find a confidence interval $[\\lambda_L, \\lambda_U]$ for $\\lambda$ by \"inverting\" the observed $F$-statistic ($F_{obs}$). We search for two specific non-central F-distributions: one where $F_{obs}$ cuts off the upper $\\alpha/2$ tail, and one where it cuts off the lower $\\alpha/2$ tail.\n\n* **Lower Bound ($\\lambda_L$):** The non-centrality parameter such that $F_{obs}$ is the $1-\\alpha/2$ quantile.\n* **Upper Bound ($\\lambda_U$):** The non-centrality parameter such that $F_{obs}$ is the $\\alpha/2$ quantile.\n\nThis concept is illustrated in the figure below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of constructing a confidence interval for the non-centrality parameter $\\lambda$ by inverting the F-test. The observed $F_{obs}$ (dashed line) is the $97.5^{th}$ percentile of the distribution defined by the lower bound $\\lambda_L$ (blue), and the $2.5^{th}$ percentile of the distribution defined by the upper bound $\\lambda_U$ (red). The shaded areas each represent $\\alpha/2$.](lec5-est_files/figure-html/fig-ci-inversion-1.png){#fig-ci-inversion width=576}\n:::\n:::\n\n\n**3. The Interval for $\\rho^2$:**\n\nOnce $[\\lambda_L, \\lambda_U]$ are found numerically, we map them back to the population $R^2$ scale using the inverse relationship:\n\n$$ \\rho^2 = \\frac{\\lambda}{\\lambda + (n-1)} $$\n\nThis produces an exact confidence interval $[\\rho^2_L, \\rho^2_U]$ for the proportion of variance explained by the model in the population.\n\n\n## An Animation for Illustrating $R^2_a$ Under $H_0$ and $H_1$\n\nWe simulate a dataset with $n=30$ observations and consider a sequence of nested models adding groups of predictors.\n\n**Predictor Groups:**\n\n1.  **Group 1 ($k=1$):** Add $x_1$. (Signal under $H_1$).\n2.  **Group 2 ($k=6$):** Add $x_2, \\dots, x_6$ (Noise).\n3.  **Group 3 ($k=11$):** Add $x_7, \\dots, x_{11}$ (Noise).\n4.  **Group 4 ($k=20$):** Add $x_{12}, \\dots, x_{20}$ (Noise).\n\n\n\n::: {.panel-tabset}\n\n#### Null Hypothesis ($H_0$)\n\nUnder $H_0$, the true coefficient for $x_1$ is $\\beta_1 = 0$. All predictors are noise.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<video controls=\"controls\" width=\"100%\">\n<source src=\"figs/rss-h0-v6.mp4\" type=\"video/mp4\"/>\n</video>\n```\n\n\nSimulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable.\n:::\n:::\n\n\n#### Alternative Hypothesis ($H_1$)\n\nUnder $H_1$, $x_1$ is a true predictor ($\\beta_1 = 2$). The subsequent groups ($x_2 \\dots x_{20}$) remain noise.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<video controls=\"controls\" width=\"100%\">\n<source src=\"figs/rss-h1-v6.mp4\" type=\"video/mp4\"/>\n</video>\n```\n\n\nSimulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors.\n:::\n:::\n\n\n:::\n\n\n\n## A Data Example with House Price Valuation\n\nA real estate agency wants to refine their pricing model. They regress the selling price of houses ($y$) on five predictors ($X$): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.\n\nWe assume the data has been collected and saved to `house_prices_5pred.csv`.\n\n\n::: {.cell}\n\n:::\n\n\n### Visualize the Data\n\nFirst, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load Data\ndf <- read.csv(\"house_prices_5pred.csv\")\n\n# Conditional Display\nif (knitr::is_html_output()) {\n  rmarkdown::paged_table(df)\n} else {\n  knitr::kable(head(df, 10), caption = \"First 10 rows of House Prices\")\n}\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Price\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Size\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Age\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Beds\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Garage\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Lawn\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"497808\",\"2\":\"3092\",\"3\":\"4\",\"4\":\"3\",\"5\":\"2\",\"6\":\"426\"},{\"1\":\"364297\",\"2\":\"1802\",\"3\":\"26\",\"4\":\"5\",\"5\":\"0\",\"6\":\"88\"},{\"1\":\"610217\",\"2\":\"2701\",\"3\":\"22\",\"4\":\"4\",\"5\":\"1\",\"6\":\"403\"},{\"1\":\"536122\",\"2\":\"2745\",\"3\":\"38\",\"4\":\"4\",\"5\":\"0\",\"6\":\"437\"},{\"1\":\"347259\",\"2\":\"2143\",\"3\":\"18\",\"4\":\"2\",\"5\":\"1\",\"6\":\"141\"},{\"1\":\"343784\",\"2\":\"2754\",\"3\":\"49\",\"4\":\"5\",\"5\":\"1\",\"6\":\"186\"},{\"1\":\"379522\",\"2\":\"2039\",\"3\":\"53\",\"4\":\"4\",\"5\":\"0\",\"6\":\"451\"},{\"1\":\"341432\",\"2\":\"1758\",\"3\":\"43\",\"4\":\"5\",\"5\":\"1\",\"6\":\"832\"},{\"1\":\"515913\",\"2\":\"3191\",\"3\":\"19\",\"4\":\"4\",\"5\":\"0\",\"6\":\"276\"},{\"1\":\"292732\",\"2\":\"1298\",\"3\":\"17\",\"4\":\"2\",\"5\":\"2\",\"6\":\"804\"},{\"1\":\"646447\",\"2\":\"3255\",\"3\":\"39\",\"4\":\"5\",\"5\":\"0\",\"6\":\"536\"},{\"1\":\"583891\",\"2\":\"3383\",\"3\":\"12\",\"4\":\"4\",\"5\":\"0\",\"6\":\"185\"},{\"1\":\"485637\",\"2\":\"2759\",\"3\":\"38\",\"4\":\"4\",\"5\":\"1\",\"6\":\"184\"},{\"1\":\"496960\",\"2\":\"2289\",\"3\":\"27\",\"4\":\"4\",\"5\":\"0\",\"6\":\"533\"},{\"1\":\"353135\",\"2\":\"2124\",\"3\":\"19\",\"4\":\"2\",\"5\":\"0\",\"6\":\"181\"},{\"1\":\"533796\",\"2\":\"3106\",\"3\":\"43\",\"4\":\"5\",\"5\":\"3\",\"6\":\"443\"},{\"1\":\"326891\",\"2\":\"1276\",\"3\":\"17\",\"4\":\"5\",\"5\":\"2\",\"6\":\"320\"},{\"1\":\"504254\",\"2\":\"3158\",\"3\":\"2\",\"4\":\"4\",\"5\":\"3\",\"6\":\"333\"},{\"1\":\"350065\",\"2\":\"1328\",\"3\":\"40\",\"4\":\"3\",\"5\":\"3\",\"6\":\"713\"},{\"1\":\"457862\",\"2\":\"1739\",\"3\":\"13\",\"4\":\"3\",\"5\":\"1\",\"6\":\"53\"},{\"1\":\"219294\",\"2\":\"1118\",\"3\":\"11\",\"4\":\"2\",\"5\":\"0\",\"6\":\"208\"},{\"1\":\"333496\",\"2\":\"1280\",\"3\":\"15\",\"4\":\"4\",\"5\":\"0\",\"6\":\"109\"},{\"1\":\"454666\",\"2\":\"2701\",\"3\":\"30\",\"4\":\"3\",\"5\":\"2\",\"6\":\"575\"},{\"1\":\"587078\",\"2\":\"3125\",\"3\":\"1\",\"4\":\"4\",\"5\":\"2\",\"6\":\"735\"},{\"1\":\"496126\",\"2\":\"2993\",\"3\":\"41\",\"4\":\"2\",\"5\":\"0\",\"6\":\"295\"},{\"1\":\"386942\",\"2\":\"1731\",\"3\":\"29\",\"4\":\"2\",\"5\":\"2\",\"6\":\"603\"},{\"1\":\"500227\",\"2\":\"2538\",\"3\":\"59\",\"4\":\"4\",\"5\":\"1\",\"6\":\"539\"},{\"1\":\"554355\",\"2\":\"3039\",\"3\":\"18\",\"4\":\"3\",\"5\":\"3\",\"6\":\"235\"},{\"1\":\"335301\",\"2\":\"1001\",\"3\":\"7\",\"4\":\"5\",\"5\":\"3\",\"6\":\"957\"},{\"1\":\"191900\",\"2\":\"1053\",\"3\":\"52\",\"4\":\"3\",\"5\":\"0\",\"6\":\"989\"},{\"1\":\"452794\",\"2\":\"2103\",\"3\":\"33\",\"4\":\"4\",\"5\":\"3\",\"6\":\"240\"},{\"1\":\"335370\",\"2\":\"1726\",\"3\":\"55\",\"4\":\"3\",\"5\":\"2\",\"6\":\"127\"},{\"1\":\"339842\",\"2\":\"1441\",\"3\":\"38\",\"4\":\"3\",\"5\":\"3\",\"6\":\"159\"},{\"1\":\"535777\",\"2\":\"3433\",\"3\":\"2\",\"4\":\"2\",\"5\":\"0\",\"6\":\"104\"},{\"1\":\"471557\",\"2\":\"2688\",\"3\":\"45\",\"4\":\"4\",\"5\":\"3\",\"6\":\"716\"},{\"1\":\"512092\",\"2\":\"2702\",\"3\":\"11\",\"4\":\"3\",\"5\":\"0\",\"6\":\"492\"},{\"1\":\"308528\",\"2\":\"1135\",\"3\":\"44\",\"4\":\"3\",\"5\":\"3\",\"6\":\"646\"},{\"1\":\"566085\",\"2\":\"3415\",\"3\":\"30\",\"4\":\"4\",\"5\":\"1\",\"6\":\"163\"},{\"1\":\"217797\",\"2\":\"1055\",\"3\":\"58\",\"4\":\"4\",\"5\":\"3\",\"6\":\"413\"},{\"1\":\"483245\",\"2\":\"2618\",\"3\":\"34\",\"4\":\"2\",\"5\":\"3\",\"6\":\"989\"},{\"1\":\"527446\",\"2\":\"3182\",\"3\":\"17\",\"4\":\"3\",\"5\":\"2\",\"6\":\"724\"},{\"1\":\"509378\",\"2\":\"3276\",\"3\":\"40\",\"4\":\"3\",\"5\":\"3\",\"6\":\"608\"},{\"1\":\"485767\",\"2\":\"2560\",\"3\":\"13\",\"4\":\"2\",\"5\":\"2\",\"6\":\"151\"},{\"1\":\"409876\",\"2\":\"1980\",\"3\":\"32\",\"4\":\"3\",\"5\":\"0\",\"6\":\"586\"},{\"1\":\"609642\",\"2\":\"3129\",\"3\":\"16\",\"4\":\"4\",\"5\":\"0\",\"6\":\"252\"},{\"1\":\"376774\",\"2\":\"2120\",\"3\":\"49\",\"4\":\"2\",\"5\":\"0\",\"6\":\"397\"},{\"1\":\"441467\",\"2\":\"2039\",\"3\":\"9\",\"4\":\"2\",\"5\":\"1\",\"6\":\"961\"},{\"1\":\"363941\",\"2\":\"1698\",\"3\":\"4\",\"4\":\"3\",\"5\":\"0\",\"6\":\"987\"},{\"1\":\"561867\",\"2\":\"2636\",\"3\":\"15\",\"4\":\"2\",\"5\":\"0\",\"6\":\"816\"},{\"1\":\"403582\",\"2\":\"2626\",\"3\":\"48\",\"4\":\"5\",\"5\":\"3\",\"6\":\"86\"},{\"1\":\"257529\",\"2\":\"1153\",\"3\":\"5\",\"4\":\"2\",\"5\":\"1\",\"6\":\"779\"},{\"1\":\"528588\",\"2\":\"2853\",\"3\":\"37\",\"4\":\"3\",\"5\":\"3\",\"6\":\"865\"},{\"1\":\"330393\",\"2\":\"2132\",\"3\":\"49\",\"4\":\"2\",\"5\":\"2\",\"6\":\"961\"},{\"1\":\"467629\",\"2\":\"2224\",\"3\":\"5\",\"4\":\"4\",\"5\":\"3\",\"6\":\"749\"},{\"1\":\"570547\",\"2\":\"2883\",\"3\":\"24\",\"4\":\"5\",\"5\":\"0\",\"6\":\"577\"},{\"1\":\"386896\",\"2\":\"2790\",\"3\":\"54\",\"4\":\"2\",\"5\":\"0\",\"6\":\"414\"},{\"1\":\"214403\",\"2\":\"1050\",\"3\":\"50\",\"4\":\"5\",\"5\":\"1\",\"6\":\"818\"},{\"1\":\"341410\",\"2\":\"1480\",\"3\":\"10\",\"4\":\"5\",\"5\":\"0\",\"6\":\"752\"},{\"1\":\"435568\",\"2\":\"2141\",\"3\":\"29\",\"4\":\"3\",\"5\":\"1\",\"6\":\"861\"},{\"1\":\"411208\",\"2\":\"1696\",\"3\":\"16\",\"4\":\"5\",\"5\":\"3\",\"6\":\"229\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Fit the Model\n\nWe will solve for the coefficients $\\hat{\\beta}$ using three distinct methods.\n\n#### Method 1: Naive Matrix Formula {.unnumbered}\n\nThis method solves the normal equations directly on the raw data: $\\hat{\\beta} = (X^{\\prime}X)^{-1}X^{\\prime}y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Define Y and X (add Column of 1s for Intercept)\ny <- as.matrix(df$Price)\n# Note: \"lawn\" Is Included Here, Even Though It Is Irrelevant\nX_naive <- as.matrix(cbind(Intercept = 1, \n                           df[, c(\"Size\", \"Age\", \"Beds\", \"Garage\", \"Lawn\")]))\n\n# 2. Compute Intermediate Matrices\nXtX <- t(X_naive) %*% X_naive\nXty <- t(X_naive) %*% y\n\n# Display Intermediate Steps\ncat(\"Matrix X'X (Cross-products of predictors):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix X'X (Cross-products of predictors):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(XtX, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Intercept      Size     Age   Beds Garage     Lawn\nIntercept        60    136483    1674    206     80    29392\nSize         136483 343078981 3738402 469757 177877 63939128\nAge            1674   3738402   63528   5874   2353   827130\nBeds            206    469757    5874    776    281    98738\nGarage           80    177877    2353    281    196    41915\nLawn          29392  63939128  827130  98738  41915 19306096\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix X'y (Cross-products with response):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix X'y (Cross-products with response):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Xty, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 [,1]\nIntercept    25884407\nSize      63115001244\nAge         694594579\nBeds         89683035\nGarage       34067413\nLawn      12402228016\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Solve Beta\nbeta_naive <- solve(XtX) %*% Xty\n\n# Display Result\ncat(\"\\nSolved Coefficients (Beta):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolved Coefficients (Beta):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(beta_naive))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept     Size       Age     Beds   Garage    Lawn\n[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443\n```\n\n\n:::\n:::\n\n\n#### Method 2: Centralized Formula {.unnumbered}\n\nThis method reduces multicollinearity issues. Formula: $\\hat{\\beta}_{\\text{slope}} = (X_c^{\\prime}X_c)^{-1}X_c^{\\prime}y_c$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Center the Data\ny_bar <- mean(y)\nX_raw <- as.matrix(df[, c(\"Size\", \"Age\", \"Beds\", \"Garage\", \"Lawn\")])\nX_means <- colMeans(X_raw)\n\ny_c <- y - y_bar\nX_c <- sweep(X_raw, 2, X_means) \n\n# 2. Compute Intermediate Matrices\nXctXc <- t(X_c) %*% X_c\nXctyc <- t(X_c) %*% y_c\n\n# Display Intermediate Steps\ncat(\"Matrix X_c'X_c (Centered Sum of Squares):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix X_c'X_c (Centered Sum of Squares):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(XctXc, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Size    Age  Beds Garage     Lawn\nSize   32618826 -69474  1165  -4100 -2919344\nAge      -69474  16823   127    121     7093\nBeds       1165    127    69      6    -2175\nGarage    -4100    121     6     89     2726\nLawn   -2919344   7093 -2175   2726  4907935\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nMatrix X_c'y_c (Centered Cross-products):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix X_c'y_c (Centered Cross-products):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(Xctyc, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\nSize   4235309234\nAge     -27580376\nBeds       813238\nGarage    -445130\nLawn   -277680160\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Solve for Slope Coefficients\nbeta_slope <- solve(XctXc) %*% Xctyc\n\n# 4. Recover Intercept\nbeta_0 <- y_bar - sum(X_means * beta_slope)\nbeta_central <- rbind(Intercept = beta_0, beta_slope)\n\n# Display Result\ncat(\"\\nSolved Coefficients (Beta):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolved Coefficients (Beta):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t(beta_central))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept     Size       Age     Beds   Garage    Lawn\n[1,]    113186 129.3434 -1218.352 12664.16 875.1155 27.2443\n```\n\n\n:::\n:::\n\n\n#### Method 3: Using R's `lm` Function {.unnumbered}\n\nThis is the standard approach for practitioners.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit Model\nmodel_lm <- lm(Price ~ ., data = df)\ny_hat_lm <- fitted(model_lm)\n\n# Extract Coefficients\nprint(summary(model_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Price ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-135178  -36006    1710   26401  111967 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 113185.971  35675.435   3.173  0.00249 ** \nSize           129.343      8.927  14.490  < 2e-16 ***\nAge          -1218.352    386.414  -3.153  0.00264 ** \nBeds         12664.157   6064.435   2.088  0.04150 *  \nGarage         875.115   5316.490   0.165  0.86987    \nLawn            27.244     23.243   1.172  0.24629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 49360 on 54 degrees of freedom\nMultiple R-squared:  0.8161,\tAdjusted R-squared:  0.799 \nF-statistic: 47.92 on 5 and 54 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n### Visualization of Fitted Values vs Mean\n\nWe define $\\hat{y}_0$ as the vector of the mean of $y$ ($\\bar{y}$). We plot the actual $y$ against our fitted model $\\hat{y}$, using a green line to represent the \"Null Model\" ($\\hat{y}_0$).\n\n*Note: Axes have been set so that X = Predicted Value and Y = Actual Value.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define y_hat_0 (The Null Model) - for conceptual clarity\ny_hat_0 <- rep(mean(y), length(y))\n\n# Scatterplot (Axes reversed: x=fitted, y=actual)\nplot(y_hat_lm, y,\n     main = \"Actual vs Fitted Prices\",\n     xlab = \"Fitted Price (y_hat)\",\n     ylab = \"Actual Price (y)\",\n     pch = 19, col = \"blue\")\n\n# Add 1:1 line (Perfect fit area, remains y=x)\nabline(0, 1, col = \"gray\", lty = 2)\n\n# Add Mean line representing the null model\n# Since y-axis is 'actual y', a horizontal line at mean(y) represents y_bar\nabline(v=mean (y), h = mean(y), col = \"green\", lwd = 2)\n\nlegend(\"topleft\", legend = c(\"Data\", \"Mean (y_bar)\"),\n       col = c(\"blue\", \"green\"), pch = c(19, NA), lty = c(NA, 1))\n```\n\n::: {.cell-output-display}\n![](lec5-est_files/figure-html/plot-y-vs-yhat-1.png){width=576}\n:::\n:::\n\n\n**Question:**\n\n$$ \\bar y = \\bar{\\hat{y}} ?$$\n\n### Computing Sums of Squares (SSE, SST, SSR)\n\nWe compare different methods to calculate the sources of variation.\n\n#### 1. Naive Sum of Squared Errors\nThis uses the standard summation definitions: $\\sum (Difference)^2$.\n\n* **SST (Total):** Variation of $y$ around $\\hat{y}_0$ (Mean).\n* **SSR (Regression):** Variation of $\\hat{y}$ around $\\hat{y}_0$ (Mean).\n* **SSE (Error):** Variation of $y$ around $\\hat{y}$ (Model).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vectors\ny_vec <- as.vector(y)\ny_hat <- as.vector(y_hat_lm)\ny_bar_vec <- rep(mean(y), length(y))\n\n# Calculations\nSST_naive <- sum((y_vec - y_bar_vec)^2)\nSSR_naive <- sum((y_hat - y_bar_vec)^2)\nSSE_naive <- sum((y_vec - y_hat)^2)\n\ncat(\"Naive Calculation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive Calculation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST:\", SST_naive, \" SSR:\", SSR_naive, \" SSE:\", SSE_naive, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n```\n\n\n:::\n:::\n\n\n#### 2. Pythagorean Shortcut (Vector Lengths)\nBased on the geometry of least squares, we can treat the variables as vectors. Because the vectors are orthogonal, we can use squared lengths (dot products with themselves).\n\nFormula: $SSR = ||\\hat{y}||^2 - ||\\hat{y}_0||^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function for squared Euclidean norm (length squared)\nlen_sq <- function(v) sum(v^2)\n\n# SST = ||y||^2 - ||y_0||^2\nSST_pyth <- len_sq(y_vec) - len_sq(y_bar_vec)\n\n# SSR = ||y_hat||^2 - ||y_0||^2\nSSR_pyth <- len_sq(y_hat) - len_sq(y_bar_vec)\n\n# SSE = ||y||^2 - ||y_hat||^2\nSSE_pyth <- len_sq(y_vec) - len_sq(y_hat)\n\ncat(\"Pythagorean Calculation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPythagorean Calculation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SST:\", SST_pyth, \" SSR:\", SSR_pyth, \" SSE:\", SSE_pyth, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n```\n\n\n:::\n:::\n\n\n#### Matrix Algebra Shortcuts\n\nThese formulas use the $\\beta$ and $X$ matrices directly. This is computationally efficient for large datasets.\n\n* Formula A (Centered with $y_c$): $SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y_c$\n* Formula B (Alternative with $y$): $SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y$\n* Formula C (Uncentered): $SSR = \\hat{\\beta}^{\\prime} X^{\\prime} y - n\\bar{y}^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- length(y)\nterm_correction <- n * mean(y)^2 \n\n# --- SSR Calculations ---\n\n# 1. SSR Formula A (Centered, using y_c)\nSSR_centered_yc <- t(beta_slope) %*% t(X_c) %*% y_c\n\n# 2. SSR Formula A (Alternative, using raw y)\n# Since X_c is centered, X_c' * 1 = 0, so X_c'y_c is equivalent to X_c'y\nSSR_centered_y <- t(beta_slope) %*% t(X_c) %*% y\n\n# 3. SSR Formula B (Uncentered Matrix)\n# beta_naive includes intercept, X_naive includes column of 1s\nterm_beta_X_y <- t(beta_naive) %*% t(X_naive) %*% y\nSSR_uncentered <- term_beta_X_y - term_correction\n\n# --- Equivalence Check Table ---\n\nresults_table <- data.frame(\n  Metric = c(\"SSR (Centered $X_c,y_c$)\", \n             \"SSR (Centered $X_c$)\", \n             \"SSR (Uncentered)\"),\n  Formula = c(\"$\\\\hat{\\\\beta}_c' X_c' y_c$\", \n              \"$\\\\hat{\\\\beta}_c' X_c' y$\", \n              \"$\\\\hat{\\\\beta}' X' y - n\\\\bar{y}^2$\"),\n  Value = c(as.numeric(SSR_centered_yc), \n            as.numeric(SSR_centered_y), \n            as.numeric(SSR_uncentered))\n)\n\n# Render the table\nknitr::kable(results_table, \n             digits = 4, \n             caption = \"Demonstration of SSR Formula Equivalence\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Demonstration of SSR Formula Equivalence\n\n|Metric                   |Formula                          |        Value|\n|:------------------------|:--------------------------------|------------:|\n|SSR (Centered $X_c,y_c$) |$\\hat{\\beta}_c' X_c' y_c$        | 583756306788|\n|SSR (Centered $X_c$)     |$\\hat{\\beta}_c' X_c' y$          | 583756306788|\n|SSR (Uncentered)         |$\\hat{\\beta}' X' y - n\\bar{y}^2$ | 583756306788|\n\n\n:::\n:::\n\n\n\n### Analysis of Variance (ANOVA)\n\nWe now evaluate the sources of variation to test the overall model significance.\n\n#### 1. Computing Sums of Squares {.unnumbered}\n\nWe calculate the following components:\n\n* Total Sum of Squares: $\\text{SST} = \\sum (y_i - \\bar{y})^2$\n* Regression Sum of Squares: $\\text{SSR} = \\sum (\\hat{y}_i - \\bar{y})^2$\n* Sum of Squared Errors: $\\text{SSE} = \\sum (y_i - \\hat{y}_i)^2$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Vectors\ny_vec <- as.vector(y)\ny_hat <- as.vector(y_hat_lm)\ny_bar_vec <- rep(mean(y), length(y))\n\n# Calculations\nSST_naive <- sum((y_vec - y_bar_vec)^2)\nSSR_naive <- sum((y_hat - y_bar_vec)^2)\nSSE_naive <- sum((y_vec - y_hat)^2)\n\ncat(\"SST:\", SST_naive, \" SSR:\", SSR_naive, \" SSE:\", SSE_naive, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 715333529746  SSR: 583756306788  SSE: 131577222958 \n```\n\n\n:::\n:::\n\n\n#### 2. Manual ANOVA Construction {.unnumbered}\n\nWe build the table manually using the sums of squares and degrees of freedom. We calculate the Mean Squares and the F-statistic:\n\n* $\\text{MSR} = \\text{SSR} / k$\n* $\\text{MSE} = \\text{SSE} / (n - k - 1)$\n* $\\text{MST} = \\text{SST} / (n - 1)$\n* $F = \\text{MSR} / \\text{MSE}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\nk <- 5             # Predictors\ndf_e <- n - k - 1  # Error DF\ndf_t <- n - 1      # Total DF\n\n# Mean Squares\nMSR <- SSR_naive / k\nMSE <- SSE_naive / df_e\nMST <- SST_naive / df_t # Mean Square Total (Variance of Y)\n\n# F-statistic\nF_stat <- MSR / MSE\n\n# P-value\np_val <- pf(F_stat, k, df_e, lower.tail = FALSE)\n\n# Assemble Table\nanova_manual <- data.frame(\n  Source = c(\"Regression (Model)\", \"Error (Residual)\", \"Total\"),\n  DF = c(k, df_e, df_t),\n  SS = c(SSR_naive, SSE_naive, SST_naive),\n  MS = c(MSR, MSE, MST), # Included MST here\n  F_Statistic = c(F_stat, NA, NA),\n  P_Value = c(p_val, NA, NA)\n)\n\nknitr::kable(anova_manual, digits = 4, caption = \"Manual ANOVA Table\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Manual ANOVA Table\n\n|Source             | DF|           SS|           MS| F_Statistic| P_Value|\n|:------------------|--:|------------:|------------:|-----------:|-------:|\n|Regression (Model) |  5| 583756306788| 116751261358|     47.9153|       0|\n|Error (Residual)   | 54| 131577222958|   2436615240|          NA|      NA|\n|Total              | 59| 715333529746|  12124297114|          NA|      NA|\n\n\n:::\n:::\n\n\n#### 3. Standard R Output (`anova`) {.unnumbered}\n\nWe display the standard `summary()` which provides the coefficients, t-tests, and the overall F-statistic found at the bottom. We also show `anova()` which gives the sequential sum of squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit an intercept-only (null) model and compare to the fitted model\nmodel_null <- lm(Price ~ 1, data = df)\ncat(\"\\nANOVA comparing intercept-only model to fitted model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nANOVA comparing intercept-only model to fitted model:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova(model_null, model_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: Price ~ 1\nModel 2: Price ~ Size + Age + Beds + Garage + Lawn\n  Res.Df        RSS Df  Sum of Sq      F    Pr(>F)    \n1     59 7.1533e+11                                   \n2     54 1.3158e+11  5 5.8376e+11 47.915 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# One can call anova directly to model_lm\nprint(anova(model_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: Price\n          Df     Sum Sq    Mean Sq  F value    Pr(>F)    \nSize       1 5.4992e+11 5.4992e+11 225.6914 < 2.2e-16 ***\nAge        1 2.0657e+10 2.0657e+10   8.4777  0.005216 ** \nBeds       1 9.5872e+09 9.5872e+09   3.9346  0.052396 .  \nGarage     1 2.4151e+08 2.4151e+08   0.0991  0.754107    \nLawn       1 3.3476e+09 3.3476e+09   1.3739  0.246291    \nResiduals 54 1.3158e+11 2.4366e+09                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### Coefficient of Determination and Variance Decomposition\n\nWe calculate $R^2$ and Adjusted $R^2$, and then present them in a **Variance Decomposition Table**.\n\n#### 1. Calculation {.unnumbered}\n\nWe calculate the coefficients of determination:\n\n* Standard $R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}$\n* Adjusted $R^2_a = 1 - \\frac{\\text{MSE}}{\\text{MST}}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard R-squared\nR2 <- 1 - (SSE_naive / SST_naive)\n\n# Adjusted R-squared\n# Formula: 1 - (MSE / MST)\nR2_adj <- 1 - (MSE / MST)\n\ncat(\"Standard R^2:  \", round(R2, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard R^2:   0.8161 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Adjusted R^2:  \", round(R2_adj, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted R^2:   0.799 \n```\n\n\n:::\n:::\n\n\n#### 2. Variance Decomposition Table {.unnumbered}\n\nThis table extends standard ANOVA. While ANOVA focuses on **Mean Squares (MS)** for hypothesis testing (is $MSR > MSE$?), this table focuses on **Variance Components ($\\hat{\\sigma}^2$)** for estimation (how much variance is Signal vs. Noise?). We estimate the variance components as follows:\n\n* Signal Variance: $\\hat{\\sigma}^2_\\mu = \\text{MST} - \\text{MSE}$\n* Noise Variance: $\\hat{\\sigma}^2 = \\text{MSE}$\n* Total Variance: $\\hat{\\sigma}^2_Y = \\text{MST}$\n\n* **Signal Variance ($\\hat{\\sigma}^2_\\mu$):** Estimated by $MST - MSE$. (Note: $MSR$ is biased and overestimates signal).\n* **Noise Variance ($\\hat{\\sigma}^2$):** Estimated by $MSE$.\n* **Total Variance ($\\hat{\\sigma}^2_Y$):** Estimated by $MST$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variance Component Estimators (Method of Moments)\nsigma2_noise_est  <- MSE\nsigma2_total_est  <- MST\nsigma2_signal_est <- MST - MSE\n\n# Proportions\nprop_signal <- sigma2_signal_est / sigma2_total_est # Equals R^2_adj\nprop_noise  <- sigma2_noise_est / sigma2_total_est  # Equals 1 - R^2_adj\n\n# Assemble Table\ndecomp_table <- data.frame(\n  Component = c(\"Signal (Model)\", \"Noise (Error)\", \"Total (Y)\"),\n  DF = c(k, df_e, df_t),\n  SS = c(SSR_naive, SSE_naive, SST_naive),\n  MS = c(NA, MSE, MST),\n  Estimator_Sigma2 = c(sigma2_signal_est, sigma2_noise_est, sigma2_total_est),\n  Proportion = c(prop_signal, prop_noise, 1.0)\n)\n\n# Display\nknitr::kable(decomp_table, \n             digits = 4, \n             col.names = c(\"Component\", \"DF\", \"SS\", \"MS\", \"Value ($\\\\hat{\\\\sigma}^2$)\", \"Proportion\"),\n             caption = \"Variance Decomposition Table: Estimating Signal vs. Noise\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Variance Decomposition Table: Estimating Signal vs. Noise\n\n|Component      | DF|           SS|          MS| Value ($\\hat{\\sigma}^2$)| Proportion|\n|:--------------|--:|------------:|-----------:|------------------------:|----------:|\n|Signal (Model) |  5| 583756306788|          NA|               9687681874|      0.799|\n|Noise (Error)  | 54| 131577222958|  2436615240|               2436615240|      0.201|\n|Total (Y)      | 59| 715333529746| 12124297114|              12124297114|      1.000|\n\n\n:::\n:::\n\n\n### Confidence Interval for Population $R^2$ ($\\rho^2$)\n\nWe construct a 95% confidence interval for the population proportion of variance explained ($\\rho^2$).\n\n#### 1. Manual Inversion Method {.unnumbered}\n\nWe solve for the non-centrality parameters $\\lambda_L$ and $\\lambda_U$ such that our observed $F_{obs}$ corresponds to the appropriate quantiles.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Define Helper Function to Find Lambda\n# We want to find lambda such that: pf(F_stat, df1, df2, ncp = lambda) = target_prob\nget_lambda <- function(target_prob, F_val, df1, df2) {\n  f_root <- function(lam) {\n    pf(F_val, df1, df2, ncp = lam) - target_prob\n  }\n  tryCatch({\n    res <- uniroot(f_root, interval = c(0, 1000))$root\n    return(res)\n  }, error = function(e) return(NA))\n}\n\n# 2. Calculate Lambda Bounds (95% CI -> alpha = 0.05)\nalpha <- 0.05\n# Lower Bound Lambda: F_obs is the (1 - alpha/2) quantile\nlambda_Lower <- get_lambda(1 - alpha/2, F_stat, k, df_e)\n# Upper Bound Lambda: F_obs is the (alpha/2) quantile\nlambda_Upper <- get_lambda(alpha/2, F_stat, k, df_e)\n\nif (is.na(lambda_Lower)) lambda_Lower <- 0\n\n# 3. Convert Lambda to Rho^2\n# Formula for Fixed Predictors: rho^2 = lambda / (lambda + n)\nrho2_Lower <- lambda_Lower / (lambda_Lower + n)\nrho2_Upper <- lambda_Upper / (lambda_Upper + n)\n\ncat(\"Manual Calculation:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual Calculation:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI for Population Rho^2: [\", round(rho2_Lower, 4), \", \", round(rho2_Upper, 4), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for Population Rho^2: [ 0.6982 ,  0.8556 ]\n```\n\n\n:::\n:::\n\n\n#### 2. Using R Package `MBESS` {.unnumbered}\n\nThe `MBESS` package automates this procedure. We use `Random.Predictors = FALSE` to match the fixed-predictor assumption used in our manual calculation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (requireNamespace(\"MBESS\", quietly = TRUE)) {\n  \n  # Use N (sample size) and p (number of predictors) \n  # instead of df.1/df.2 to avoid the redundancy error.\n  ci_res <- MBESS::ci.R2(F.value = F_stat, \n                         p = k,      # Number of predictors\n                         N = n,      # Sample size\n                         conf.level = 0.95,\n                         Random.Predictors = FALSE)\n  \n  print(ci_res)\n  \n} else {\n  cat(\"Package 'MBESS' is not installed.\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Lower.Conf.Limit.R2\n[1] 0.6982442\n\n$Prob.Less.Lower\n[1] 0.025\n\n$Upper.Conf.Limit.R2\n[1] 0.8555948\n\n$Prob.Greater.Upper\n[1] 0.025\n```\n\n\n:::\n:::\n\n\n\n\n\n## Underfitting and Overfitting\n\nWe compare the properties of two competing estimators for the mean response vector $\\mu = E[y]$.\n\n### Notation and Setup\n\nWe consider the general linear model:\n$$\ny = X\\beta + e = X_1\\beta_1 + X_2\\beta_2 + e\n$$\nwhere $X_1$ is $n \\times p_1$, $X_2$ is $n \\times p_2$, and $\\text{Var}(e) = \\sigma^2 I$.\n\nWe distinguish between two estimation approaches based on this model:\n\n**1. Full Model ($M_1$)**\nWe estimate $\\beta$ without restrictions. The estimator projects $y$ onto the full column space $\\text{Col}(X)$.\n$$\n\\begin{aligned}\nP_1 &= X(X^T X)^{-1}X^T & (\\text{Projection onto } \\text{Col}(X)) \\\\\n\\hat{y}_1 &= P_1 y & (\\text{Unrestricted Estimator})\n\\end{aligned}\n$$\n\n**2. Reduced Model ($M_0$)**\nWe estimate $\\beta$ subject to the constraint:\n$$\nM_0: \\beta_2 = 0\n$$\nThis effectively reduces the model to $y = X_1\\beta_1 + e$, projecting $y$ onto the subspace $\\text{Col}(X_1)$.\n$$\n\\begin{aligned}\nP_0 &= X_1(X_1^T X_1)^{-1}X_1^T & (\\text{Projection onto } \\text{Col}(X_1)) \\\\\n\\hat{y}_0 &= P_0 y & (\\text{Restricted Estimator})\n\\end{aligned}\n$$\n\n**Key Geometric Property:**\nSince the constraint $\\beta_2=0$ restricts the estimation to a subspace ($\\text{Col}(X_1) \\subset \\text{Col}(X)$), we have the nesting property:\n$$\nP_1 P_0 = P_0 \\quad \\text{and} \\quad P_1 - P_0 \\text{ is a projection matrix.}\n$$\n\n### Case 1: Underfitting\n\n**The Truth:** The Full Model ($M_1$) is correct.\n$$\ny = X_1\\beta_1 + X_2\\beta_2 + e, \\quad \\beta_2 \\neq 0\n$$\nThe true mean is $\\mu = X_1\\beta_1 + X_2\\beta_2$.\n\nWe analyze the properties of the **Reduced Estimator** $\\hat{y}_0$ (from $M_0$) compared to the correct Full Estimator $\\hat{y}_1$ (from $M_1$).\n\n::: {#thm-underfitting name=\"Bias-Variance Tradeoff in Underfitting\"}\nWhen $M_1$ is true:\n\n1.  **Bias:** The estimator $\\hat{y}_0$ is **biased**, while $\\hat{y}_1$ is unbiased.\n    $$ \\text{Bias}(\\hat{y}_0) = -(I - P_0) X_2 \\beta_2 $$\n2.  **Variance:** The estimator $\\hat{y}_0$ has **smaller variance** (matrix difference is positive semidefinite).\n    $$ \\text{Var}(\\hat{y}_1) - \\text{Var}(\\hat{y}_0) = \\sigma^2 (P_1 - P_0) \\ge 0 $$\n:::\n\n::: {.proof}\n**Part 1 (Bias):**\n$$\n\\begin{aligned}\nE[\\hat{y}_0] &= P_0 E[y] = P_0(X_1\\beta_1 + X_2\\beta_2) \\\\\n&= X_1\\beta_1 + P_0 X_2 \\beta_2 \\quad (\\text{Since } P_0 X_1 = X_1)\n\\end{aligned}\n$$\nThe bias is:\n$$\n\\text{Bias} = E[\\hat{y}_0] - \\mu = (X_1\\beta_1 + P_0 X_2 \\beta_2) - (X_1\\beta_1 + X_2\\beta_2) = -(I - P_0)X_2\\beta_2\n$$\n\n**Part 2 (Variance):**\n$$\n\\text{Var}(\\hat{y}_1) = \\sigma^2 P_1, \\quad \\text{Var}(\\hat{y}_0) = \\sigma^2 P_0\n$$\nThe difference is $\\sigma^2(P_1 - P_0)$. Since $\\text{Col}(X_1) \\subset \\text{Col}(X)$, the difference $P_1 - P_0$ projects onto the orthogonal complement of $\\text{Col}(X_1)$ within $\\text{Col}(X)$. It is idempotent and positive semidefinite.\n:::\n\n**Remark: Scalar Variance and Coefficients**\n\nFrom the matrix inequality above, we can state that for any arbitrary vector $a$, the scalar variance of the linear combination $a^T \\hat{y}$ is always smaller in the reduced model:\n$$\n\\text{Var}(a^T \\hat{y}_0) \\le \\text{Var}(a^T \\hat{y}_1)\n$$\n\nWe can extend this property to the regression coefficients $\\hat{\\beta}$. Since $\\hat{y} = X\\hat{\\beta}$, we can recover the coefficients from the fitted values using the left pseudo-inverse:\n\n$$\n\\begin{aligned}\n(X^T X)^{-1}X^T (X\\hat{\\beta}) &= (X^T X)^{-1}X^T \\hat{y} \\\\\n\\underbrace{(X^T X)^{-1}(X^T X)}_{I} \\hat{\\beta} &= (X^T X)^{-1}X^T \\hat{y}\n\\end{aligned}\n$$\n\n::: {#cor-beta-variance name=\"Variance of Coefficients\"}\nBecause $\\hat{\\beta}$ is a linear transformation of $\\hat{y}$, the variance reduction in $\\hat{y}_0$ propagates to the coefficients.\n\nFor any specific coefficient $\\beta_j$ included in the reduced model (i.e., $\\beta_j \\in \\beta_1$), the variance of the estimator is smaller in the reduced model than in the full model:\n$$ \\text{Var}(\\hat{\\beta}_{j, reduced}) \\le \\text{Var}(\\hat{\\beta}_{j, full}) $$\n:::\n\n**Conclusion:** Using $M_0$ when $M_1$ is true introduces bias but reduces variance for both the fitted values and the estimated coefficients.\n\n### Case 2: Overfitting\n\n**The Truth:** The Reduced Model ($M_0$) is correct.\n$$\ny = X_1\\beta_1 + e \\quad (\\text{i.e., } \\beta_2 = 0)\n$$\nThe true mean is $\\mu = X_1\\beta_1$.\n\nWe analyze the properties of the **Full Estimator** $\\hat{y}_1$ (from $M_1$) compared to the correct Reduced Estimator $\\hat{y}_0$ (from $M_0$).\n\n::: {#thm-overfitting name=\"Variance Inflation in Overfitting\"}\nWhen $M_0$ is true:\n\n1.  **Bias:** Both estimators are **unbiased**.\n    $$ E[\\hat{y}_1] = \\mu \\quad \\text{and} \\quad E[\\hat{y}_0] = \\mu $$\n2.  **Variance:** The estimator $\\hat{y}_1$ has **unnecessarily higher variance**.\n    $$ \\text{Var}(\\hat{y}_1) \\ge \\text{Var}(\\hat{y}_0) $$\n:::\n\n::: {.proof}\n**Part 1 (Bias):**\nSince $\\mu = X_1\\beta_1$:\n$$\nE[\\hat{y}_1] = P_1 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\text{Col}(X))\n$$\n$$\nE[\\hat{y}_0] = P_0 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\text{Col}(X_1))\n$$\n\n**Part 2 (Variance):**\nAs shown in Case 1, the difference is $\\sigma^2(P_1 - P_0)$.\nThe cost of overfitting is purely variance inflation. The total variance (trace) increases by the number of unnecessary parameters ($p_2$):\n$$\n\\text{tr}(\\text{Var}(\\hat{y}_1)) - \\text{tr}(\\text{Var}(\\hat{y}_0)) = \\sigma^2 (\\text{tr}(P_1) - \\text{tr}(P_0)) = \\sigma^2 (p_{full} - p_{reduced}) = \\sigma^2 p_2\n$$\n:::\n\n**Conclusion:** Using $M_1$ when $M_0$ is true offers no benefit in bias but strictly increases estimation variance.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.2/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.2/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}