{
  "hash": "78cbe763b848491649396c7f52fdc69c",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: \n  pdf: default\n  html: default\n---\n\n# Inference for A Multiple Linear Regression Model\n\n## Linear Models and Least Square Estimator\n\n### Assumptions in Linear Models\n\nSuppose that on a random sample of $n$ units (patients, animals, trees, etc.) we observe a response variable $Y$ and explanatory variables $X_{1},...,X_{k}$. Our data are then $(y_{i},x_{i1},...,x_{ik})$, $i=1,...,n$, or in vector/matrix form $y, x_{1},...,x_{k}$ where $y=(y_{1},...,y_{n})$ and $x_{j}=(x_{1j},...,x_{nj})^{T}$ or $y, X$ where $X=(x_{1},...,x_{k})$.\n\nEither by design or by conditioning on their observed values, $x_{1},...,x_{k}$ are regarded as vectors of known constants. The linear model in its classical form makes the following assumptions:\n\n**Assumptions on Linear Models**\n\n* **A1. (Additive Error)**\n$y=\\mu+e$ where $e=(e_{1},...,e_{n})^{T}$ is an unobserved random vector with $E(e)=0$. This implies that $\\mu=E(y)$ is the unknown mean of $y$.\n\n* **A2. (Linearity)**\n$\\mu=\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}=X\\beta$ where $\\beta_{1},...,\\beta_{k}$ are unknown parameters. This assumption says that $E(y)=\\mu\\in\\text{Col}(X)$ (lies in the column space of $X$); i.e., it is a linear combination of explanatory vectors $x_{1},...,x_{k}$ with coefficients the unknown parameters in $\\beta=(\\beta_{1},...,\\beta_{k})^{T}$. Note that it is linear in $\\beta_{1},...,\\beta_{k}$, not necessarily in the $x$'s.\n\n* **A3. (Independence)**\n$e_{1},...,e_{n}$ are independent random variables (and therefore so are $y_{1},...,y_{n})$.\n\n* **A4. (Homoscedasticity)**\n$e_{1},...,e_{n}$ all have the same variance $\\sigma^{2}$; that is, $\\text{Var}(e_{1})=\\cdot\\cdot\\cdot=\\text{Var}(e_{n})=\\sigma^{2}$ which implies $\\text{Var}(y_{1})=\\cdot\\cdot\\cdot=\\text{Var}(y_{n})=\\sigma^{2}$.\n\n* **A5. (Normality)**\n$e\\sim N_{n}(0,\\sigma^{2}I_{n})$.\n\n\n### Matrix Formulation\n\nThe model can be written algebraically as:\n$$y_{i}=\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2}+\\cdot\\cdot\\cdot+\\beta_{k}x_{ik}, \\quad i=1,...,n$$\n\nOr in matrix notation:\n$$\n\\begin{pmatrix}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_{11} & x_{12} & \\cdot\\cdot\\cdot & x_{1k}\\\\\n1 & x_{21} & x_{22} & \\cdot\\cdot\\cdot & x_{2k}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdot\\cdot\\cdot & x_{nk}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n$$\n\nThis is expressed compactly as:\n$$y=X\\beta+e$$\nwhere $X$ is the design matrix, and $e \\sim N_n(0, \\sigma^2 I)$. Alternatively:\n$$y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\cdot\\cdot\\cdot+\\beta_{k}x_{k}+e$$\n\nTaken together, all five assumptions can be stated more succinctly as:\n$$y\\sim N_{n}(X\\beta,\\sigma^{2}I)$$\nwith the mean vector $\\mu_{y}=X\\beta\\in \\text{Col}(X)$.\n\n:::{.callout }\n### A Note on Coefficients\nThe effect of a parameter depends upon what other explanatory variables are present in the model. For example, $\\beta_{0}$ and $\\beta_{1}$ in the model:\n$$y=\\beta_{0}j_{n}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+e$$\nwill typically be different than $\\beta_{0}^{*}$ and $\\beta_{1}^{*}$ in the model:\n$$y=\\beta_{0}^{*}j_{n}+\\beta_{1}^{*}x_{1}+e^{*}$$\nIn this context, $\\beta_0^*$ and $\\beta_1^*$ are the population-projected coefficients of the full model, that is,  $\\beta_0^*$ and $\\beta_1^*$ are the parameters that can best approximate the full model. \n:::\n\n::: {.callout-important}\nWe will first consider the case that $\\text{rank}(X)=k+1$.\n:::\n\n### Least Squares Estimator of $\\beta$ and Fitted Value $\\hat Y$\n\n::: {#def-least-squares name=\"Least Squares Estimator\"}\nThe **Least Squares Estimator (LSE)** of $\\beta$, denoted as $\\hat{\\beta}$, is the vector that minimizes the Sum of Squared Errors (SSE), which measures the discrepancy between the observed responses $y$ and the fitted values $X\\hat{\\beta}$.\n$$\nQ(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 = (y - X\\beta)'(y - X\\beta)\n$$\n:::\n\nWe can derive the closed-form solution for $\\hat{\\beta}$ using the geometry of projections discussed in previous chapters.\n\n#### 1. Obtaining $\\hat Y$ {.unnumbered}\n\nIn the linear model $y = X\\beta + e$, the systematic component (the mean $E[y]$) is constrained to lie in the column space of $X$, denoted as $\\text{Col}(X)$. We seek the vector in $\\text{Col}(X)$ that is \"closest\" to the observed data vector $y$. As established in the theory of projections, this closest vector is the **orthogonal projection** of $y$ onto $\\text{Col}(X)$. Let $\\hat{y}$ denote this fitted value vector. Using the explicit formula for the projection matrix $$H = X(X'X)^{-1}X',$$ we have:\n$$ \\hat{y} = Hy = X(X'X)^{-1}X' y.$$\n\n#### 2. Obtaining $\\hat{\\beta}$ by Solving $x\\beta = \\hat{y}$ {.unnumbered}\n\nSince the fitted vector $\\hat{y}$ is a projection onto $\\text{Col}(X)$, it must lie entirely within that column space. This guarantees that the linear system for the coefficients $\\hat{\\beta}$ is consistent (has an exact solution):\n$$ X\\hat{\\beta} = \\hat{y} $$\n\nTo isolate $\\hat{\\beta}$, we pre-multiply both sides by the left pseudo-inverse of $X$, which is $(X'X)^{-1}X'$:\n\n$$\n\\begin{aligned}\n(X'X)^{-1}X' (X\\hat{\\beta}) &= (X'X)^{-1}X' \\hat{y} \\\\\n\\underbrace{(X'X)^{-1}(X'X)}_{I} \\hat{\\beta} &= (X'X)^{-1}X' \\hat{y}\n\\end{aligned}\n$$\n\nThis gives us the estimator expressed in terms of the fitted values:\n\n$$\n\\boxed{\\hat{\\beta} = (X'X)^{-1}X' \\hat{y}}\n$$\n\nHowever, we typically calculate the estimator from the observed data $y$. Recall that because $\\hat{y}$ is an orthogonal projection, the difference $y - \\hat{y}$ is orthogonal to $X$. This implies $X'\\hat{y} = X'y$. Substituting this into the equation above yields the standard closed-form solution:\n\n$$\n\\boxed{\\hat{\\beta} = (X'X)^{-1}X'y}\n$$\n\n\n### Properties of the Estimator $\\hat \\beta$\n\n::: {#thm-unbiased name=\"Unbiasedness of $\\hat \\beta$\"}\nIf $E(y)=X\\beta$, then $\\hat{\\beta}$ is an unbiased estimator for $\\beta$.\n:::\n\n::: {.proof}\n$$\n\\begin{aligned}\nE(\\hat{\\beta}) &= E[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}E(y) \\quad \\text{[using linearity of expectation]} \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}X\\beta \\\\\n&= \\beta\n\\end{aligned}\n$$\n:::\n\n::: {#thm-covariance name=\"Variance of $\\hat \\beta$\"}\nIf $\\text{Var}(y)=\\sigma^{2}I$, the covariance matrix for $\\hat{\\beta}$ is given by $\\sigma^{2}(X^{\\prime}X)^{-1}$.\n:::\n\n::: {.proof}\n$$\n\\begin{aligned}\n\\text{Var}(\\hat{\\beta}) &= \\text{Var}[(X^{\\prime}X)^{-1}X^{\\prime}y] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}\\text{Var}(y)[(X^{\\prime}X)^{-1}X^{\\prime}]^{\\prime} \\quad \\text{[using } \\text{Var}(Ay) = A \\text{Var}(y) A'] \\\\\n&= (X^{\\prime}X)^{-1}X^{\\prime}(\\sigma^{2}I)X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}X^{\\prime}X(X^{\\prime}X)^{-1} \\\\\n&= \\sigma^{2}(X^{\\prime}X)^{-1}\n\\end{aligned}\n$$\n:::\n\n**Note:** These theorems require no assumption of normality.\n\n\n## Best Linear Unbiased Estimator (BLUE)\n\n::: {#thm-gauss-markov name=\"Gauss-Markov Theorem\"}\nIf $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, the least-squares estimators $\\hat{\\beta}_{j}, j=0,1,...,k$ have minimum variance among all linear unbiased estimators.\n:::\n\n::: {.proof}\nWe consider a linear estimator $Ay$ of $\\beta$ and seek the matrix $A$ for which $Ay$ is a minimum variance unbiased estimator.\n\n**1. Unbiasedness Condition:**\nIn order for $Ay$ to be an unbiased estimator of $\\beta$, we must have $E(Ay)=\\beta$. Using the assumption $E(y)=X\\beta$, this is expressed as:\n$$E(Ay) = A E(y) = AX\\beta = \\beta$$\nwhich implies the condition $AX=I_{k+1}$ since the relationship must hold for any $\\beta$.\n\n**2. Minimizing Variance:**\nThe covariance matrix for the estimator $Ay$ is:\n$$\\text{Var}(Ay) = A \\text{Var}(y) A' = A(\\sigma^2 I) A' = \\sigma^2 AA'$$\nWe need to choose $A$ (subject to $AX=I$) so that the diagonal elements of $AA'$ are minimized.\n\nTo relate $Ay$ to $\\hat{\\beta}=(X'X)^{-1}X'y$, we define $\\hat{A} = (X'X)^{-1}X'$ and write $A = (A - \\hat{A}) + \\hat{A}$. Then:\n$$AA' = [(A - \\hat{A}) + \\hat{A}] [(A - \\hat{A}) + \\hat{A}]'$$\nExpanding this, the cross terms vanish because $(A - \\hat{A})\\hat{A}' = A\\hat{A}' - \\hat{A}\\hat{A}'$.\nNote that $\\hat{A}\\hat{A}' = (X'X)^{-1}X'X(X'X)^{-1} = (X'X)^{-1}$.\nAlso, $A\\hat{A}' = A X (X'X)^{-1} = I (X'X)^{-1} = (X'X)^{-1}$ (since $AX=I$).\nThus, $(A - \\hat{A})\\hat{A}' = 0$.\n\nThe expansion simplifies to:\n$$AA' = (A - \\hat{A})(A - \\hat{A})' + \\hat{A}\\hat{A}'$$\nThe matrix $(A - \\hat{A})(A - \\hat{A})'$ is positive semidefinite, meaning its diagonal elements are non-negative. To minimize the diagonal of $AA'$, we must set $A - \\hat{A} = 0$, which implies $A = \\hat{A}$.\n\nThus, the minimum variance estimator is:\n$$Ay = (X'X)^{-1}X'y = \\hat{\\beta}$$\n:::\n\n### Notes on Gauss-markov\n\n1.  **Distributional Generality:** The remarkable feature of the Gauss-Markov theorem is that it holds for *any* distribution of $y$; normality is not required. The only assumptions used are linearity ($E(y)=X\\beta$) and homoscedasticity ($\\text{Var}(y)=\\sigma^2 I$).\n\n2.  **Extension to All Linear Combinations:** The theorem extends beyond just the parameter vector $\\beta$ to any linear combination of the parameters.\n\n::: {#cor-linear-combo name=\"BLUE for All Linear Combinations\"}\nIf $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, the best linear unbiased estimator of the scalar $a'\\beta$ is $a'\\hat{\\beta}$, where $\\hat{\\beta}$ is the least-squares estimator.\n:::\n\n::: {.proof}\nLet $\\tilde{\\beta} = Ay$ be any other linear unbiased estimator of $\\beta$. The variance of the linear combination $a'\\tilde{\\beta}$ is:\n$$\n\\frac{1}{\\sigma^2}\\text{Var}(a'\\tilde{\\beta}) = \\frac{1}{\\sigma^2}\\text{Var}(a'Ay) = a'AA'a\n$$\nFrom the proof of the Gauss-Markov theorem, we established that $AA' = (A-\\hat{A})(A-\\hat{A})' + (X'X)^{-1}$ where $\\hat{A} = (X'X)^{-1}X'$. Substituting this into the variance equation:\n$$\na'AA'a = a'(A-\\hat{A})(A-\\hat{A})'a + a'(X'X)^{-1}a\n$$\nThe term $a'(A-\\hat{A})(A-\\hat{A})'a$ is a quadratic form with a positive semidefinite matrix, so it is always non-negative. Therefore:\n$$\na'AA'a \\ge a'(X'X)^{-1}a = \\frac{1}{\\sigma^2}\\text{Var}(a'\\hat{\\beta})\n$$\nThe variance is minimized when $A=\\hat{A}$ (specifically when the first term is zero), proving that $a'\\hat{\\beta}$ has the minimum variance among all linear unbiased estimators.\n:::\n\n3.  **Scaling Invariance:** The predictions made by the model are invariant to the scaling of the explanatory variables.\n\n::: {#thm-scaling name=\"Scaling Explanatory Variables\"}\nIf $x=(1,x_{1},...,x_{k})'$ and $z=(1,c_{1}x_{1},...,c_{k}x_{k})'$, then the fitted values are identical: $\\hat{y} = \\hat{\\beta}'x = \\hat{\\beta}_{z}'z$.\n:::\n\n::: {.proof}\nLet $D = \\text{diag}(1, c_1, ..., c_k)$ such that the design matrix is transformed to $Z = XD$. The LSE for the transformed data is:\n$$\n\\begin{aligned}\n\\hat{\\beta}_z &= (Z'Z)^{-1}Z'y = [(XD)'(XD)]^{-1}(XD)'y \\\\\n&= D^{-1}(X'X)^{-1}(D')^{-1}D'X'y \\\\\n&= D^{-1}(X'X)^{-1}X'y = D^{-1}\\hat{\\beta}\n\\end{aligned}\n$$\n. Then, the prediction is:\n$$\n\\hat{\\beta}_z' z = (D^{-1}\\hat{\\beta})' (Dx) = \\hat{\\beta}' (D^{-1})' D x = \\hat{\\beta}'x\n$$\n.\n:::\n\n#### Limitations: Restriction to Unbiased Estimators {.unnumbered}\n\nIt is crucial to recognize that the Gauss-Markov theorem only guarantees optimality within the class of **linear** and **unbiased** estimators.\n\n* **Assumption Sensitivity:** If the assumptions of linearity ($E(y)=X\\beta$) and homoscedasticity ($\\text{Var}(y)=\\sigma^2 I$) do not hold, $\\hat{\\beta}$ may be biased or may have a larger variance than other estimators.\n* **Unbiasedness Constraint:** The theorem does not compare $\\hat{\\beta}$ to biased estimators. It is possible for a biased estimator (e.g., shrinkage estimators) to have a smaller Mean Squared Error (MSE) than the BLUE by accepting some bias to significantly reduce variance. The LSE is only \"best\" (minimum variance) among those estimators that satisfy the unbiasedness constraint.\n\n\n## Estimator of Error Variance\n\nWe estimate $\\sigma^{2}$ by the residual mean square:\n\n::: {#def-s2 name=\"Residual Variance Estimator\"}\n$$s^{2} = \\frac{1}{n-k-1} \\sum_{i=1}^{n}(y_{i}-x_{i}'\\hat{\\beta})^{2} = \\frac{\\text{SSE}}{n-k-1}$$\nwhere $\\text{SSE} = (y-X\\hat{\\beta})'(y-X\\hat{\\beta})$.\n:::\n\nAlternatively, SSE can be written as:\n$$\\text{SSE} = y'y - \\hat{\\beta}'X'y$$\nThis is often useful for computation ($y'y$ is the total sum of squares of the raw data).\n\n### Unbiasedness of $s^2$\n\n::: {#thm-unbiased-s2 name=\"Unbiasedness of s-squared\"}\nIf $s^{2}$ is defined as above, and if $E(y)=X\\beta$ and $\\text{Var}(y)=\\sigma^{2}I$, then $E(s^{2})=\\sigma^{2}$.\n:::\n\n::: {.proof}\nWe use the Hat Matrix $H = X(X'X)^{-1}X'$, which projects $y$ onto $\\text{Col}(X)$. Thus, $\\hat{y} = Hy$.\nThe residuals are $y - \\hat{y} = (I - H)y$. The Sum of Squared Errors is:\n$$\\text{SSE} = \\|(I-H)y\\|^2 = y'(I-H)'(I-H)y$$\nSince $H$ is symmetric and idempotent, $(I-H)$ is also symmetric and idempotent. Thus:\n$$\\text{SSE} = y'(I-H)y$$\n\nTo find the expectation, we use the trace trick for quadratic forms: $E[y'Ay] = \\text{tr}(A\\text{Var}(y)) + E[y]'A E[y]$.\n$$\n\\begin{aligned}\nE(\\text{SSE}) &= E[y'(I-H)y] \\\\\n&= \\text{tr}((I-H)\\sigma^2 I) + (X\\beta)'(I-H)(X\\beta) \\\\\n&= \\sigma^2 \\text{tr}(I-H) + \\beta'X'(I-H)X\\beta\n\\end{aligned}\n$$\n**Trace Term:** $\\text{tr}(I_n - H) = \\text{tr}(I_n) - \\text{tr}(H) = n - (k+1)$, since $\\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_{k+1}) = k+1$.\n\n**Non-centrality Term:** Since $HX = X$, we have $(I-H)X = 0$. Therefore, the second term vanishes: $\\beta'X'(I-H)X\\beta = 0$.\n\nCombining these:\n$$E(\\text{SSE}) = \\sigma^2(n - k - 1)$$\nDividing by the degrees of freedom $(n-k-1)$, we get $E(s^2) = \\sigma^2$.\n:::\n\n## Distributions Under Normality\n\nIf we add Assumption A5 ($y \\sim N_n(X\\beta, \\sigma^2 I)$), we can derive the exact sampling distributions.\n\n::: {#cor-cov-beta name=\"Estimated Covariance of Beta\"}\nAn unbiased estimator of $\\text{Cov}(\\hat{\\beta})$ is given by:\n$$\\widehat{\\text{Cov}}(\\hat{\\beta}) = s^{2}(X'X)^{-1}$$\n:::\n\n::: {#thm-sampling-dist name=\"Sampling Distributions\"}\nUnder assumptions A1-A5:\n\n1.  $\\hat{\\beta} \\sim N_{k+1}(\\beta, \\sigma^{2}(X'X)^{-1})$.\n2.  $(n-k-1)s^{2}/\\sigma^{2} \\sim \\chi^{2}(n-k-1)$.\n3.  $\\hat{\\beta}$ and $s^{2}$ are independent.\n:::\n\n::: {.proof}\n**Part (i):** Since $\\hat{\\beta} = (X'X)^{-1}X'y$ is a linear transformation of the normal vector $y$, it is also normally distributed. We already established its mean and variance in @thm-unbiased and @thm-covariance.\n\n**Part (ii):** We showed $\\text{SSE} = y'(I-H)y$. Since $(I-H)$ is idempotent with rank $n-k-1$, and $(I-H)X\\beta = 0$, by the theory of quadratic forms in normal variables, $\\text{SSE}/\\sigma^2 \\sim \\chi^2(n-k-1)$.\n\n**Part (iii):** $\\hat{\\beta}$ depends on $Hy$ (or $X'y$), while $s^2$ depends on $(I-H)y$. Since $H(I-H) = H - H^2 = 0$, the linear forms defining the estimator and the residuals are orthogonal. For normal vectors, zero covariance implies independence.\n:::\n\n## Maximum Likelihood Estimator (MLE)\n\n::: {#thm-mle name=\"MLE for Linear Regression\"}\nIf $y \\sim N_n(X\\beta, \\sigma^2 I)$, the Maximum Likelihood Estimators are:\n$$\n\\hat{\\beta}_{\\text{MLE}} = (X'X)^{-1}X'y\n$$\n$$\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}(y - X\\hat{\\beta})'(y - X\\hat{\\beta}) = \\frac{\\text{SSE}}{n}\n$$\n:::\n\n::: {.proof}\nThe log-likelihood function is:\n$$ \\ln L(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}(y - X\\beta)'(y - X\\beta) $$\nMaximizing this with respect to $\\beta$ is equivalent to minimizing the quadratic term $(y - X\\beta)'(y - X\\beta)$, which yields the Least Squares Estimator.\nDifferentiating with respect to $\\sigma^2$ and setting to zero yields $\\hat{\\sigma}^2 = \\text{SSE}/n$.\n:::\n\n**Note:** The MLE for $\\sigma^2$ is biased (denominator $n$), whereas $s^2$ is unbiased (denominator $n-k-1$).\n\n\n\n## Linear Models in Centered Form\n\nThe regression model can be written in a centered form by subtracting the means of the explanatory variables:\n$$y_{i}=\\alpha+\\beta_{1}(x_{i1}-\\overline{x}_{1})+\\beta_{2}(x_{i2}-\\overline{x}_{2})+\\cdot\\cdot\\cdot+\\beta_{k}(x_{ik}-\\overline{x}_{k})+e_{i}$$\nfor $i=1,...,n$, where the intercept term is adjusted:\n$$\\alpha=\\beta_{0}+\\beta_{1}\\overline{x}_{1}+\\beta_{2}\\overline{x}_{2}+\\cdot\\cdot\\cdot+\\beta_{k}\\overline{x}_{k}$$\nand $\\overline{x}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}$.\n\n### Matrix Formulation\n\nIn matrix form, the equivalence between the original model and the centered model is:\n$$y = X\\beta + e = (j_n, X_c)\\begin{pmatrix} \\alpha \\\\ \\beta_{1} \\end{pmatrix} + e$$\nwhere $\\beta_{1}=(\\beta_{1},...,\\beta_{k})^{T}$ represents the slope coefficients, and $X_c$ is the centered design matrix:\n$$X_c = (I - P_{j_n})X_1$$\nHere, $X_1$ consists of the original columns of $X$ excluding the intercept column.\n\n\n\nTo see the structure of $X_c$, we first calculate the projection of the data onto the intercept space, $P_{j_n}X_1$:\n$$\n\\begin{aligned}\nP_{j_n}X_1 &= \\frac{1}{n}j_n j_n' X_1 \\\\\n&= \\begin{pmatrix} 1/n & 1/n & \\cdots & 1/n \\\\ 1/n & 1/n & \\cdots & 1/n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1/n & 1/n & \\cdots & 1/n \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n$$\nThis results in a matrix where every row is the vector of column means. Subtracting this from $X_1$ gives $X_c$:\n$$\n\\begin{aligned}\nX_c &= X_1 - P_{j_n}X_1 \\\\\n&= \\begin{pmatrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\ x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix} - \\begin{pmatrix} \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bar{x}_1 & \\bar{x}_2 & \\cdots & \\bar{x}_k \\end{pmatrix} \\\\\n&= \\begin{pmatrix} x_{11} - \\bar{x}_1 & x_{12} - \\bar{x}_2 & \\cdots & x_{1k} - \\bar{x}_k \\\\ x_{21} - \\bar{x}_1 & x_{22} - \\bar{x}_2 & \\cdots & x_{2k} - \\bar{x}_k \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} - \\bar{x}_1 & x_{n2} - \\bar{x}_2 & \\cdots & x_{nk} - \\bar{x}_k \\end{pmatrix}\n\\end{aligned}\n$$\n\n\n### Estimation in Centered Form\n\nBecause the column space of the intercept $j_n$ is orthogonal to the columns of $X_c$ (since columns of $X_c$ sum to zero), the cross-product matrix becomes block diagonal:\n$$\n\\begin{pmatrix} j_n' \\\\ X_c' \\end{pmatrix} (j_n, X_c) = \\begin{pmatrix} j_n'j_n & j_n'X_c \\\\ X_c'j_n & X_c'X_c \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}\n$$\n\n::: {#thm-centered-estimators name=\"Centered Estimators\"}\nThe least squares estimators for the centered parameters are:\n$$\n\\begin{pmatrix} \\hat{\\alpha} \\\\ \\hat{\\beta}_{1} \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & X_c'X_c \\end{pmatrix}^{-1} \\begin{pmatrix} j_n'y \\\\ X_c'y \\end{pmatrix} = \\begin{pmatrix} \\bar{y} \\\\ (X_c'X_c)^{-1}X_c'y \\end{pmatrix}\n$$\nThus:\n\n1.  $\\hat{\\alpha} = \\bar{y}$ (The sample mean of $y$).\n2.  $\\hat{\\beta}_{1} = S_{xx}^{-1}S_{xy}$, using the sample covariance notations.\n:::\n\nRecovering the original intercept:\n$$ \\hat{\\beta}_0 = \\hat{\\alpha} - \\hat{\\beta}_1 \\bar{x}_1 - \\dots - \\hat{\\beta}_k \\bar{x}_k = \\bar{y} - \\hat{\\beta}_{1}'\\bar{x} $$\n\n## Decomposition of Sum of Squares \n\nWe partition the total variation based on the orthogonal subspaces.\n\n::: {#def-ss-components name=\"Sum of Squares Components\"}\nThe total variation is decomposed as $\\text{SST} = \\text{SSR} + \\text{SSE}$.\n\n1.  **Total Sum of Squares (SST):** The squared length of the centered response vector.\n    $$\\text{SST} = \\|y - \\bar{y}j_n\\|^2 = \\|(I - P_{j_n})y\\|^2$$\n\n2.  **Regression Sum of Squares (SSR):** The variation explained by the regressors $X_c$.\n    $$\\text{SSR} = \\|\\hat{y} - \\bar{y}j_n\\|^2 = \\|P_{X_c}y\\|^2 = \\hat{\\beta}_1' X_c' X_c \\hat{\\beta}_1$$\n\n3.  **Sum of Squared Errors (SSE):** The residual variation.\n    $$\\text{SSE} = \\|y - \\hat{y}\\|^2 = \\|(I - H)y\\|^2$$\n:::\n\n\n### 3D Visualization of Decomposition of $y$\n\nWe partition the total variation in $y$ based on the orthogonal subspaces.\n\n1.  **Space of the Mean:** $L(j_n)$, spanned by the intercept vector $j_n$.\n2.  **Space of the Regressors:** $L(X_c)$, spanned by the centered predictors $X_c$.\n3.  **Error Space:** $\\text{Col}(X)^\\perp$, orthogonal to the model space.\n\nThe vector $y$ can be decomposed into three orthogonal components:\n$$y = \\bar{y}j_n + P_{X_c}y + (y - \\hat{y})$$\nVisually, this corresponds to projecting the vector $y$ onto three orthogonal axes.\n\n**Interactive Visualization:**\n\nWe generate a cloud of 100 observations of $y$ from $N(\\mu, \\sigma=1)$ where $\\mu = (5,5,0)$. The projections onto the Model Plane ($z=0$) are highlighted in **red**, and the projections onto the error axis ($z$) are in **yellow**.\n\n\n\n::: {.panel-tabset}\n\n#### Effect Exists (signal){.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scenario 1: Significant regression effect ($\\beta_1 \not= 0$). The mean vector projects significantly onto the predictor space.](figs/geometry-3d-signal.png){width=100%}\n:::\n:::\n\n\n#### No Effect (noise){.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scenario 2: No regression effect ($\\beta_1 = 0$). The mean vector lies purely on the intercept axis.](figs/geometry-3d-noise.png){width=100%}\n:::\n:::\n\n\n:::\n\n### A Diagram to Show Decomposition of Sum of Squares\n\nThe decomposition of the total variation is visualized below. The total deviation (Orange) is the vector sum of the regression deviation (Green) and the residual error (Red).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Geometric Decomposition: SST = SSR + SSE](lec5-est_files/figure-pdf/fig-ss-decomposition-legend-v2-1.pdf){#fig-ss-decomposition-legend-v2 fig-align='center'}\n:::\n:::\n\n\n\n### Distribution of Sum of Squares\n\nWe apply the general theory of projections to the specific components defined in @def-ss-components.\n\n::: {#thm-distribution-ss-v2 name=\"Distribution of Sum of Squares\"}\nLet $y \\sim N(\\mu, \\sigma^2 I_n)$, where $\\mu \\in \\text{Col}(X)$.\nConsider the decomposition defined by the projection matrices $P_{X_c}$ and $M = I - H$.\n\n* **Independence:** The quadratic forms $\\text{SSR}$ and $\\text{SSE}$ are statistically independent because the subspaces $L(X_c)$ and $\\text{Col}(X)^\\perp$ are orthogonal.\n\n* **Distribution of SSE:** The scaled sum of squared errors follows a central Chi-squared distribution:\n  $$ \\frac{\\text{SSE}}{\\sigma^2} = \\frac{\\|(I - H)y\\|^2}{\\sigma^2} \\sim \\chi^2(n-k-1) $$\n  **Mean:**\n  $$ E[\\text{SSE}] = \\sigma^2(n-k-1) $$\n\n* **Distribution of SSR:** The scaled regression sum of squares follows a **non-central** Chi-squared distribution:\n  $$ \\frac{\\text{SSR}}{\\sigma^2} = \\frac{\\|P_{X_c}y\\|^2}{\\sigma^2} \\sim \\chi^2(k, \\lambda) $$\n  **Mean:**\n  $$ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 $$\n\n**Non-centrality Parameter ($\\lambda$):**\n$$ \\lambda = \\frac{1}{\\sigma^2} \\|P_{X_c} \\mu\\|^2 $$\nwhere $$\\|P_{X_c} \\mu\\|^2 = \\|X_c \\beta_1\\|^2 = (X_c \\beta_1)' (X_c \\beta_1) = \\beta_1' X_c' X_c \\beta_1$$\n:::\n\n::: proof\nWe apply @thm-proj-dist to the specific projection matrices identified in the definitions.\n\n* **For SSE (Error Space):**\n  $\\text{SSE}$ is defined by the projection matrix $P_V = I - H$.\n\n  - **Dimension:** The rank of $(I - H)$ is $n - \\text{rank}(X) = n - (k+1) = n - k - 1$.\n  - **Non-centrality:** Since $\\mu \\in \\text{Col}(X)$, the projection onto the orthogonal complement is zero: $\\|(I - H)\\mu\\|^2 = 0$. Thus, $\\lambda = 0$.\n  - **Expectation:** Using Part 2 of @thm-proj-dist ($E(\\|P_V y\\|^2) = \\sigma^2 \\text{rank}(P_V) + \\|P_V \\mu\\|^2$):\n    $$ E[\\text{SSE}] = \\sigma^2(n-k-1) + 0 = \\sigma^2(n-k-1) $$\n\n* **For SSR (Regression Space):**\n  $\\text{SSR}$ is defined by the projection matrix $P_V = P_{X_c}$.\n\n  - **Dimension:** The rank of $P_{X_c}$ is $(k+1) - 1 = k$.\n  - **Non-centrality:** The projection of $\\mu$ onto $L(X_c)$ is $P_{X_c}\\mu$.\n    $$ \\lambda = \\frac{1}{2\\sigma^2} \\|P_{X_c} \\mu\\|^2 $$\n\n  - **Expectation:** Using Part 2 of @thm-proj-dist:\n    $$ E[\\text{SSR}] = \\sigma^2 k + \\|P_{X_c}\\mu\\|^2 $$\n\n  This shows that while $E[\\text{SSE}]$ depends only on the noise variance and sample size, $E[\\text{SSR}]$ is inflated by the magnitude of the true regression signal $\\|P_{X_c}\\mu\\|^2$.\n:::\n\n\n## F-test for Testing Overall Regression Effect\n\nWe wish to test whether the regression model provides any explanatory power beyond the simple intercept-only model.\n\n**Hypotheses:**\n\n* **Null Hypothesis ($H_0$):** $\\beta_1 = \\beta_2 = \\dots = \\beta_k = 0$ (No regression effect).\n    This implies $\\mu \\in \\text{span}(j_n)$ and the true signal variance $\\|X_c\\beta_1\\|^2 = 0$.\n\n* **Alternative Hypothesis ($H_1$):** At least one $\\beta_j \\neq 0$.\n\n### The F-statistic {.unnumbered}\n\nWe construct the test statistic using the ratio of the Mean Squares defined previously:\n\n$$F = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)}$$\n\n### Understanding $F$ via Expectations {.unnumbered}\n\nThe logic of the F-test is transparent when we examine the expected values of the numerator and denominator:\n\n$$\n\\begin{aligned}\nE[\\text{MSE}] &= \\sigma^2 \\\\\nE[\\text{MSR}] &= \\sigma^2 + \\frac{\\|X_c \\beta_1\\|^2}{k}\n\\end{aligned}\n$$\n\n* **If $H_0$ is true:** The signal term is zero. Both Mean Squares estimate $\\sigma^2$ unbiasedly. We expect $F \\approx 1$.\n* **If $H_1$ is true:** The numerator includes the positive term $\\frac{\\|X_c \\beta_1\\|^2}{k}$. We expect $F > 1$.\n\nTherefore, we reject $H_0$ for sufficiently large values of $F$. Specifically, we reject at level $\\alpha$ if $F_{obs} > F_{\\alpha}(k, n-k-1)$.\n\n### Distributional Theory\n\nTo derive the exact sampling distribution, we rely on the independence of the sums of squares (from @thm-distribution-ss-v2) and the definition of the non-central F-distribution given in **@def-noncentral-f**.\n\n::: {#thm-regression-f-dist name=\"Distribution of Regression F-Statistic\"}\nUnder the assumption of normality, the regression F-statistic follows a **non-central F-distribution**:\n\n$$ F \\sim F(k, n-k-1, \\lambda) $$\n\nThe non-centrality parameter $\\lambda$ is determined by the ratio of the signal sum of squares to the error variance:\n$$ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} $$\n\n**Special Cases:**\n\n1.  **Under $H_1$ (Signal exists):** $\\lambda > 0$, so $F$ follows the non-central distribution.\n2.  **Under $H_0$ (No signal):** $\\beta_1 = 0 \\implies \\lambda = 0$. The distribution collapses to the **central F-distribution**:\n    $$ F \\sim F(k, n-k-1) $$\n:::\n\n::: proof\nWe identify the components from @def-noncentral-f:\n\n1.  **Numerator ($X_1$):** Let $X_1 = \\text{SSR}/\\sigma^2$. From @thm-distribution-ss-v2, $X_1 \\sim \\chi^2(k, \\lambda)$.\n2.  **Denominator ($X_2$):** Let $X_2 = \\text{SSE}/\\sigma^2$. From @thm-distribution-ss-v2, $X_2 \\sim \\chi^2(n-k-1)$.\n3.  **Independence:** $X_1$ and $X_2$ are independent.\n\nSubstituting these into the F-statistic:\n$$\nF = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{(\\text{SSR}/\\sigma^2)/k}{(\\text{SSE}/\\sigma^2)/(n-k-1)} = \\frac{X_1/k}{X_2/(n-k-1)}\n$$\nBy definition @def-noncentral-f, this ratio follows $F(k, n-k-1, \\lambda)$.\n:::\n\n### Visualization of the Rejection Region\n\nThe following plot illustrates the central F-distribution (valid under $H_0$) for $k=3$ predictors and $n=20$ observations ($df_1 = 3, df_2 = 16$). An observed statistic of $F=2$ is marked, with the p-value represented by the shaded tail area.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Probability Density Function of F(3, 16) under H0. The shaded region represents the p-value.](lec5-est_files/figure-pdf/fig-f-dist-example-1.pdf){#fig-f-dist-example}\n:::\n:::\n\n\n\n## Raw Coefficient of Determination ($R^2$)\n\n### Definition\n\nThe $R^2$ statistic measures the proportion of total variation explained by the regression model.\n\n::: {#def-r2 name=\"R-Squared\"}\n$$R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}$$\nSince $0 \\le \\text{SSE} \\le \\text{SST}$, it follows that $0 \\le R^2 \\le 1$.\n:::\n\n\n### Expectation and Bias\n\nTo understand the bias in $R^2$, it is more illuminating to analyze the expectation of the **unexplained variance** ($1 - R^2$). This term represents the ratio of error sum of squares to the total sum of squares:\n\n$$ E[1 - R^2] = E\\left[ \\frac{\\text{SSE}}{\\text{SST}} \\right] $$\n\nUsing the first-order approximation $E[X/Y] \\approx E[X]/E[Y]$, we examine the numerator and denominator separately:\n\n$$\n\\begin{aligned}\nE[\\text{SSE}] &= \\sigma^2(n-k-1) \\\\\nE[\\text{SST}] &= \\sigma^2(n-1) + \\sigma^2\\lambda = \\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)\n\\end{aligned}\n$$\n\nSubstituting these back, we approximate the expected unexplained fraction:\n\n$$ E[1 - R^2] \\approx \\frac{\\sigma^2(n-k-1)}{\\sigma^2 \\left( (n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} \\right)} = \\frac{n-k-1}{(n-1) + \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}} $$\n\n**Behavior under Null Hypothesis ($H_0$):**\nWhen there is no true signal ($\\beta_1 = 0$), the term $\\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2}$ vanishes. The expected proportion of unexplained variance becomes:\n\n$$ E[1 - R^2 | H_0] \\approx \\frac{n-k-1}{n-1} $$\n\nThis result reveals the source of the bias:\n\n1.  Ideally, if predictors are noise, the model should explain nothing, and $E[1-R^2]$ should be $1$.\n2.  Instead, the expected error ratio is **less than 1**, specifically scaled by $\\frac{n-k-1}{n-1}$.\n3.  This scaling factor is exactly what the **Adjusted R-squared ($R^2_a$)** attempts to correct by multiplying the observed ratio by the inverse $\\frac{n-1}{n-k-1}$.\n\n### Exact Distribution\n\nThe $R^2$ statistic follows the Type I Non-central Beta distribution derived from the ratio of independent Chi-squared variables.\n\n::: {#thm-r2-dist name=\"Distribution of R-Squared\"}\n$$ R^2 \\sim \\text{Beta}_1\\left( \\frac{k}{2}, \\frac{n-k-1}{2}, \\lambda \\right) $$\nwhere $\\text{df}_1 = k$ and $\\text{df}_2 = n-k-1$.\n:::\n\n\n## Adjusted R-squared ($R^2_a$)\n\nTo correct for the inflation of $R^2$ due to model complexity ($k$), we introduce the Adjusted $R^2$. This statistic penalizes the sum of squares by their degrees of freedom:\n\n$$ R^2_a = 1 - \\frac{\\text{SSE}/(n-k-1)}{\\text{SST}/(n-1)} = 1 - \\frac{\\text{MSE}}{\\text{MST}} = 1 - (1 - R^2) \\frac{n-1}{n-k-1} $$\n\n**Expectation:**\n\nUnder $H_0$, since $E[\\text{MSE}] = E[\\text{MST}] = \\sigma^2$, the estimator is asymptotically unbiased:\n\n$$ E[R^2_a | H_0] \\approx 0 $$\n\n**Variance and Stability:**\n\nWhile $R^2_a$ corrects the bias, it introduces instability. The variance of $R^2_a$ under $H_0$ can be derived from the variance of the Beta distribution:\n\n$$ \\text{Var}(R^2_a | H_0) = \\left( \\frac{n-1}{n-k-1} \\right)^2 \\text{Var}(R^2 | H_0) $$\n\nSubstituting $\\text{Var}(R^2 | H_0) = \\frac{2k(n-k-1)}{(n-1)^2(n+1)}$, we obtain:\n\n$$ \\text{Var}(R^2_a | H_0) = \\frac{2k}{(n-k-1)(n+1)} $$\n\n**Key Insight:**\n\nAs the model complexity $k$ increases relative to $n$:\n\n1.  The denominator $(n-k-1)$ shrinks.\n2.  The variance $\\text{Var}(R^2_a)$ explodes.\n\nThis implies that for high-dimensional models (large $k/n$), $R^2_a$ becomes an extremely noisy estimator, often yielding large negative values even for null models.\n\n## Population Proportion of Signals ($\\rho^2$)\n\nThe formula for the expected Adjusted $R^2$ reveals a deep connection to the decomposition of variance in population quantities. Recall the Rao-Blackwell theorem (or Law of Total Variance), which decomposes the total variance of a single observation $Y_i$ into the expected conditional variance (noise) and the variance of the conditional expectation (signal). Let $\\sigma^2_\\mu$ denote the signal variance and $\\sigma^2$ denote the noise variance:\n\n$$ \\text{Var}(Y_i) = E[\\text{Var}(Y_i|x_{(i)})] + \\text{Var}(E[Y_i|x_{(i)}]) $$\n$$ \\sigma^2_Y = \\sigma^2 + \\sigma^2_\\mu $$\n\nIn our derived expectation for $R^2_a$:\n$$ E[R^2_a] \\approx \\frac{\\frac{\\|X_c\\beta_1\\|^2}{n-1}}{\\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{n-1}} $$\n\nThe term in the numerator, $\\frac{\\|X_c\\beta_1\\|^2}{n-1}$, is precisely the **sample variance of the true means** $\\mu_i$.\nLet $\\mu = X\\beta$. We can expand the centered signal vector $X_c\\beta_1$ to see this explicitly. Since $\\mu \\in \\text{Col}(X)$, we know $H\\mu = \\mu$:\n\n$$\nX_c\\beta_1 = P_{X_c} \\mu = (H - P_{j_n})\\mu = H\\mu - P_{j_n}\\mu = \\mu - \\bar{\\mu}j_n = \n\\begin{pmatrix} \n\\mu_1 - \\bar{\\mu} \\\\ \n\\mu_2 - \\bar{\\mu} \\\\ \n\\vdots \\\\ \n\\mu_n - \\bar{\\mu} \n\\end{pmatrix}\n$$\n\nThis vector represents the deviation of each observation's true mean from the grand mean. Consequently, the squared norm divided by degrees of freedom is:\n$$ \\frac{\\|X_c\\beta_1\\|^2}{n-1} = \\frac{\\sum_{i=1}^n (\\mu_i - \\bar{\\mu})^2}{n-1} = \\sigma^2_\\mu $$\n\n\n\nThus, $R^2_a$ is therefore an unbiased estimator for the **proportion of variance explained by the signal** in the population:\n$$ E[R^2_a] \\approx \\frac{\\sigma^2_\\mu}{\\sigma^2 + \\sigma^2_\\mu}$$\n\nWe will denote this 'parameter' by $\\rho^2$:\n\n$$ \\rho^2 = 1 - \\frac{\\sigma^2}{\\sigma^2_Y} = \\frac{\\sigma^2_\\mu}{\\sigma^2_Y} $$\n\n::: {.remark}\nIn the fixed covariate framework, the 'parameter' $\\rho^2$ is a function of the specific design matrix $X$, the coefficients $\\beta$, and the sample size $n$. If we assume the $x_i$ are random draws from a population, then as $n \\to \\infty$, $\\sigma^2_\\mu$ converges to $\\text{Var}(x^T\\beta)$ (where $x$ is a random vector), and $\\rho^2$ converges to the true population proportion of variance explained.\n:::\n\n::: {.callout-important title=\"MSR Is Not a Variance Estimator\"}\n\n* Observing that $E[\\text{MST}] \\approx \\sigma^2 + \\sigma^2_\\mu$ and $E[\\text{MSE}] = \\sigma^2$, we can see that the difference $\\text{MST} - \\text{MSE}$ provides a direct method-of-moments estimator for the variance of the signal itself ($\\sigma^2_\\mu$). \n\n* It is important to recognize that the commonly used **Mean Square Regression (MSR)**, defined as $\\text{SSR}/k$, is **not** an estimator of the signal variance. Because $E[\\text{MSR}] = \\sigma^2 + \\frac{\\|X_c\\beta_1\\|^2}{k}$, it scales with the sample size $n$ (via the squared norm) rather than converging to a population parameter. MSR is designed for hypothesis testing (detecting *existence* of signal), not for estimating the *magnitude* of the signal variance.\n:::\n\n## Relationship between $R^2$ and $F$ Test\n\nThe $F$-statistic for the overall regression effect is a monotonic function of the coefficient of determination. We can express $F$ directly in terms of both the standard $R^2$ and the adjusted $R^2_a$, as well as relate its expected value to the population variance components.\n\n1.  **Expressing $F$ via Standard $R^2$:**\n\n    Since $R^2 = \\text{SSR}/\\text{SST}$ and $1 - R^2 = \\text{SSE}/\\text{SST}$, we can substitute these into the definition of $F$:\n    $$F = \\frac{\\text{SSR}/k}{\\text{SSE}/(n-k-1)} = \\frac{(R^2 \\cdot \\text{SST}) / k}{((1 - R^2) \\cdot \\text{SST}) / (n - k - 1)} = \\frac{R^2 / k}{(1 - R^2) / (n - k - 1)}$$\n    This simplifies to:\n    $$F = \\frac{R^2}{1 - R^2} \\cdot \\frac{n - k - 1}{k}$$\n\n2.  **Expressing $F$ via Adjusted $R^2_a$:**\n\n    Using the relationship $1 - R^2 = (1 - R^2_a) \\frac{n-k-1}{n-1}$, we can express the $F$-statistic in terms of the adjusted coefficient:\n    $$F = \\frac{1 - (1 - R^2_a) \\frac{n-k-1}{n-1}}{(1 - R^2_a) \\frac{n-k-1}{n-1}} \\cdot \\frac{n-k-1}{k} = \\frac{R^2_a (n - k - 1) + k}{k (1 - R^2_a)}$$\n\n3.  **Expected Value of $F$ as a function of $\\sigma^2_\\mu$ and $\\sigma^2$:**\n\n    Using the population signal variance $\\sigma^2_\\mu$ and noise variance $\\sigma^2$, the expected value of the $F$-statistic (using the first-order approximation $E[F] \\approx E[\\text{MSR}]/E[\\text{MSE}]$) is:\n    $$E[F] \\approx \\frac{\\sigma^2 + \\frac{n-1}{k}\\sigma^2_\\mu}{\\sigma^2} = 1 + \\frac{n-1}{k} \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right)$$\n    The exact mean, derived from the non-central $F$ distribution, is:\n    $$E[F] = \\frac{n-k-1}{n-k-3} \\left( 1 + \\frac{n-1}{k} \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right), \\quad \\text{for } n-k-1 > 3$$\n\n\n## Confidence Interval of Population $\\rho^2$\n\n\n\nWhile $R^2_a$ provides a point estimate, we can construct an exact confidence interval for $\\rho^2$ by exploiting the distribution of the $F$-statistic.\n\n**1. The link between $\\lambda$ and $\\rho^2$:**\n\n\nRecall that the $F$-statistic follows a non-central distribution $F(k, n-k-1, \\lambda)$. The non-centrality parameter $\\lambda$ is directly related to the population $\\rho^2$. Using the variance decomposition derived above:\n\n$$ \\lambda = \\frac{\\|X_c \\beta_1\\|^2}{\\sigma^2} = (n-1) \\left( \\frac{\\sigma^2_\\mu}{\\sigma^2} \\right) $$\n\nSubstituting the signal-to-noise ratio $\\frac{\\sigma^2_\\mu}{\\sigma^2} = \\frac{\\rho^2}{1-\\rho^2}$, we obtain a one-to-one mapping between $\\lambda$ and $\\rho^2$:\n\n$$ \\lambda(\\rho^2) = (n-1) \\left( \\frac{\\rho^2}{1-\\rho^2} \\right) $$\n\n**2. Inverting the Test Statistic:**\n\nWe find a confidence interval $[\\lambda_L, \\lambda_U]$ for $\\lambda$ by \"inverting\" the observed $F$-statistic ($F_{obs}$). We search for two specific non-central F-distributions: one where $F_{obs}$ cuts off the upper $\\alpha/2$ tail, and one where it cuts off the lower $\\alpha/2$ tail.\n\n* **Lower Bound ($\\lambda_L$):** The non-centrality parameter such that $F_{obs}$ is the $1-\\alpha/2$ quantile.\n* **Upper Bound ($\\lambda_U$):** The non-centrality parameter such that $F_{obs}$ is the $\\alpha/2$ quantile.\n\nThis concept is illustrated in the figure below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of constructing a confidence interval for the non-centrality parameter $\\lambda$ by inverting the F-test. The observed $F_{obs}$ (dashed line) is the $97.5^{th}$ percentile of the distribution defined by the lower bound $\\lambda_L$ (blue), and the $2.5^{th}$ percentile of the distribution defined by the upper bound $\\lambda_U$ (red). The shaded areas each represent $\\alpha/2$.](lec5-est_files/figure-pdf/fig-ci-inversion-1.pdf){#fig-ci-inversion}\n:::\n:::\n\n\n**3. The Interval for $\\rho^2$:**\n\nOnce $[\\lambda_L, \\lambda_U]$ are found numerically, we map them back to the population $R^2$ scale using the inverse relationship:\n\n$$ \\rho^2 = \\frac{\\lambda}{\\lambda + (n-1)} $$\n\nThis produces an exact confidence interval $[\\rho^2_L, \\rho^2_U]$ for the proportion of variance explained by the model in the population.\n\n\n## An Animation for Illustrating $R^2_a$ Under $H_0$ and $H_1$\n\nWe simulate a dataset with $n=30$ observations and consider a sequence of nested models adding groups of predictors.\n\n**Predictor Groups:**\n\n1.  **Group 1 ($k=1$):** Add $x_1$. (Signal under $H_1$).\n2.  **Group 2 ($k=6$):** Add $x_2, \\dots, x_6$ (Noise).\n3.  **Group 3 ($k=11$):** Add $x_7, \\dots, x_{11}$ (Noise).\n4.  **Group 4 ($k=20$):** Add $x_{12}, \\dots, x_{20}$ (Noise).\n\n\n\n::: {.panel-tabset}\n\n#### Null Hypothesis ($H_0$)\n\nUnder $H_0$, the true coefficient for $x_1$ is $\\beta_1 = 0$. All predictors are noise.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Simulation under H0: As predictors are added (pure noise), standard R-squared increases while Adjusted R-squared and MSE remain stable.](figs/rss-h0-v6.png){fig-align='center' width=2.33in}\n:::\n:::\n\n\n#### Alternative Hypothesis ($H_1$)\n\nUnder $H_1$, $x_1$ is a true predictor ($\\beta_1 = 2$). The subsequent groups ($x_2 \\dots x_{20}$) remain noise.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Simulation under H1: Adjusted R-squared correctly identifies the signal at k=1, then penalizes the subsequent noise predictors.](figs/rss-h1-v6.png){fig-align='center' width=2.33in}\n:::\n:::\n\n\n:::\n\n\n\n## A Data Example with House Price Valuation\n\nA real estate agency wants to refine their pricing model. They regress the selling price of houses ($y$) on five predictors ($X$): Size, Age, Bedrooms, Garage Capacity, and Lawn Size.\n\nWe assume the data has been collected and saved to `house_prices_5pred.csv`.\n\n\n::: {.cell}\n\n:::\n\n\n### Visualize the Data\n\nFirst, we load the dataset. We display the first 10 rows for PDF output, or a full paged table for HTML.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: First 10 rows of House Prices\n\n|  Price| Size| Age| Beds| Garage| Lawn|\n|------:|----:|---:|----:|------:|----:|\n| 425767| 3092|  18|    5|      1|  325|\n| 336991| 1802|  37|    2|      1|  687|\n| 528842| 2701|  49|    2|      0|  261|\n| 399797| 2745|   0|    5|      2|  554|\n| 427580| 2143|   1|    5|      3|  296|\n| 478082| 2754|  26|    4|      0|  833|\n| 295549| 2039|  17|    2|      3|  194|\n| 335058| 1758|  11|    3|      1|  111|\n| 461110| 3191|  58|    2|      2|  286|\n| 204405| 1298|  41|    2|      0|  813|\n\n\n:::\n:::\n\n\n### Fit the Model\n\nWe will solve for the coefficients $\\hat{\\beta}$ using three distinct methods.\n\n#### Method 1: Naive Matrix Formula {.unnumbered}\n\nThis method solves the normal equations directly on the raw data: $\\hat{\\beta} = (X^{\\prime}X)^{-1}X^{\\prime}y$.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix X'X (Cross-products of predictors):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Intercept      Size     Age   Beds Garage     Lawn\nIntercept        25     59157     766     77     32    12049\nSize          59157 153276601 1825278 183641  75356 27856192\nAge             766   1825278   31762   2083    871   398142\nBeds             77    183641    2083    273     95    38151\nGarage           32     75356     871     95     74    13196\nLawn          12049  27856192  398142  38151  13196  7896317\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix X'y (Cross-products with response):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 [,1]\nIntercept     9754828\nSize      24994304201\nAge         294127221\nBeds         30527650\nGarage       12400638\nLawn       4533697188\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolved Coefficients (Beta):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept     Size       Age     Beds    Garage      Lawn\n[1,]  85085.08 142.1976 -624.3362 3446.122 -5014.307 -34.10539\n```\n\n\n:::\n:::\n\n\n#### Method 2: Centralized Formula {.unnumbered}\n\nThis method reduces multicollinearity issues. Formula: $\\hat{\\beta}_{\\text{slope}} = (X_c^{\\prime}X_c)^{-1}X_c^{\\prime}y_c$.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix X_c'X_c (Centered Sum of Squares):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Size   Age Beds Garage    Lawn\nSize   13294575 12708 1437   -365 -655116\nAge       12708  8292 -276   -109   28961\nBeds       1437  -276   36     -4    1040\nGarage     -365  -109   -4     33   -2227\nLawn    -655116 28961 1040  -2227 2089181\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatrix X_c'y_c (Centered Cross-products):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\nSize   1911649801\nAge      -4760709\nBeds       482780\nGarage     -85542\nLawn   -167739715\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSolved Coefficients (Beta):\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Intercept     Size       Age     Beds    Garage      Lawn\n[1,]  85085.08 142.1976 -624.3362 3446.122 -5014.307 -34.10539\n```\n\n\n:::\n:::\n\n\n#### Method 3: Using R's `lm` Function {.unnumbered}\n\nThis is the standard approach for practitioners.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Price ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-126301  -37660    5319   40319   92283 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 85085.08   75276.69   1.130    0.272    \nSize          142.20      17.74   8.015 1.63e-07 ***\nAge          -624.34     888.03  -0.703    0.491    \nBeds         3446.12   13154.95   0.262    0.796    \nGarage      -5014.31   11826.13  -0.424    0.676    \nLawn          -34.11      48.21  -0.707    0.488    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63370 on 19 degrees of freedom\nMultiple R-squared:  0.7874,\tAdjusted R-squared:  0.7315 \nF-statistic: 14.07 on 5 and 19 DF,  p-value: 7.867e-06\n```\n\n\n:::\n:::\n\n\n### Visualization of Fitted Values vs Mean\n\nWe define $\\hat{y}_0$ as the vector of the mean of $y$ ($\\bar{y}$). We plot the actual $y$ against our fitted model $\\hat{y}$, using a green line to represent the \"Null Model\" ($\\hat{y}_0$).\n\n*Note: Axes have been set so that X = Predicted Value and Y = Actual Value.*\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec5-est_files/figure-pdf/plot-y-vs-yhat-1.pdf)\n:::\n:::\n\n\n**Question:**\n\n$$ \\bar y = \\bar{\\hat{y}} ?$$\n\n### Computing Sums of Squares (SSE, SST, SSR)\n\nWe compare different methods to calculate the sources of variation.\n\n#### 1. Naive Sum of Squared Errors\nThis uses the standard summation definitions: $\\sum (Difference)^2$.\n\n* **SST (Total):** Variation of $y$ around $\\hat{y}_0$ (Mean).\n* **SSR (Regression):** Variation of $\\hat{y}$ around $\\hat{y}_0$ (Mean).\n* **SSE (Error):** Variation of $y$ around $\\hat{y}$ (Model).\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive Calculation:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 358921762209  SSR: 282617820807  SSE: 76303941402 \n```\n\n\n:::\n:::\n\n\n#### 2. Pythagorean Shortcut (Vector Lengths)\nBased on the geometry of least squares, we can treat the variables as vectors. Because the vectors are orthogonal, we can use squared lengths (dot products with themselves).\n\nFormula: $SSR = ||\\hat{y}||^2 - ||\\hat{y}_0||^2$\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nPythagorean Calculation:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 358921762209  SSR: 282617820807  SSE: 76303941402 \n```\n\n\n:::\n:::\n\n\n#### Matrix Algebra Shortcuts\n\nThese formulas use the $\\beta$ and $X$ matrices directly. This is computationally efficient for large datasets.\n\n* Formula A (Centered with $y_c$): $SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y_c$\n* Formula B (Alternative with $y$): $SSR = \\hat{\\beta}_c^{\\prime} X_c^{\\prime} y$\n* Formula C (Uncentered): $SSR = \\hat{\\beta}^{\\prime} X^{\\prime} y - n\\bar{y}^2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Demonstration of SSR Formula Equivalence\n\n|Metric                   |Formula                          |        Value|\n|:------------------------|:--------------------------------|------------:|\n|SSR (Centered $X_c,y_c$) |$\\hat{\\beta}_c' X_c' y_c$        | 282617820807|\n|SSR (Centered $X_c$)     |$\\hat{\\beta}_c' X_c' y$          | 282617820807|\n|SSR (Uncentered)         |$\\hat{\\beta}' X' y - n\\bar{y}^2$ | 282617820807|\n\n\n:::\n:::\n\n\n\n### Analysis of Variance (ANOVA)\n\nWe now evaluate the sources of variation to test the overall model significance.\n\n#### 1. Computing Sums of Squares {.unnumbered}\n\nWe calculate the following components:\n\n* Total Sum of Squares: $\\text{SST} = \\sum (y_i - \\bar{y})^2$\n* Regression Sum of Squares: $\\text{SSR} = \\sum (\\hat{y}_i - \\bar{y})^2$\n* Sum of Squared Errors: $\\text{SSE} = \\sum (y_i - \\hat{y}_i)^2$\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nSST: 358921762209  SSR: 282617820807  SSE: 76303941402 \n```\n\n\n:::\n:::\n\n\n#### 2. Manual ANOVA Construction {.unnumbered}\n\nWe build the table manually using the sums of squares and degrees of freedom. We calculate the Mean Squares and the F-statistic:\n\n* $\\text{MSR} = \\text{SSR} / k$\n* $\\text{MSE} = \\text{SSE} / (n - k - 1)$\n* $\\text{MST} = \\text{SST} / (n - 1)$\n* $F = \\text{MSR} / \\text{MSE}$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Manual ANOVA Table\n\n|Source             | DF|           SS|          MS| F_Statistic| P_Value|\n|:------------------|--:|------------:|-----------:|-----------:|-------:|\n|Regression (Model) |  5| 282617820807| 56523564161|     14.0746|       0|\n|Error (Residual)   | 19|  76303941402|  4015996916|          NA|      NA|\n|Total              | 24| 358921762209| 14955073425|          NA|      NA|\n\n\n:::\n:::\n\n\n#### 3. Standard R Output (`anova`) {.unnumbered}\n\nWe display the standard `summary()` which provides the coefficients, t-tests, and the overall F-statistic found at the bottom. We also show `anova()` which gives the sequential sum of squares.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nANOVA comparing intercept-only model to fitted model:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: Price ~ 1\nModel 2: Price ~ Size + Age + Beds + Garage + Lawn\n  Res.Df        RSS Df  Sum of Sq      F    Pr(>F)    \n1     24 3.5892e+11                                   \n2     19 7.6304e+10  5 2.8262e+11 14.075 7.867e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: Price\n          Df     Sum Sq    Mean Sq F value    Pr(>F)    \nSize       1 2.7488e+11 2.7488e+11 68.4461 1.014e-07 ***\nAge        1 5.2419e+09 5.2419e+09  1.3053    0.2674    \nBeds       1 1.1538e+08 1.1538e+08  0.0287    0.8672    \nGarage     1 3.7115e+08 3.7115e+08  0.0924    0.7644    \nLawn       1 2.0100e+09 2.0100e+09  0.5005    0.4879    \nResiduals 19 7.6304e+10 4.0160e+09                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### Coefficient of Determination and Variance Decomposition\n\nWe calculate $R^2$ and Adjusted $R^2$, and then present them in a **Variance Decomposition Table**.\n\n#### 1. Calculation {.unnumbered}\n\nWe calculate the coefficients of determination:\n\n* Standard $R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}$\n* Adjusted $R^2_a = 1 - \\frac{\\text{MSE}}{\\text{MST}}$\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard R^2:   0.7874 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdjusted R^2:   0.7315 \n```\n\n\n:::\n:::\n\n\n#### 2. Variance Decomposition Table {.unnumbered}\n\nThis table extends standard ANOVA. While ANOVA focuses on **Mean Squares (MS)** for hypothesis testing (is $MSR > MSE$?), this table focuses on **Variance Components ($\\hat{\\sigma}^2$)** for estimation (how much variance is Signal vs. Noise?). We estimate the variance components as follows:\n\n* Signal Variance: $\\hat{\\sigma}^2_\\mu = \\text{MST} - \\text{MSE}$\n* Noise Variance: $\\hat{\\sigma}^2 = \\text{MSE}$\n* Total Variance: $\\hat{\\sigma}^2_Y = \\text{MST}$\n\n* **Signal Variance ($\\hat{\\sigma}^2_\\mu$):** Estimated by $MST - MSE$. (Note: $MSR$ is biased and overestimates signal).\n* **Noise Variance ($\\hat{\\sigma}^2$):** Estimated by $MSE$.\n* **Total Variance ($\\hat{\\sigma}^2_Y$):** Estimated by $MST$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: Variance Decomposition Table: Estimating Signal vs. Noise\n\n|Component      | DF|           SS|          MS| Value ($\\hat{\\sigma}^2$)| Proportion|\n|:--------------|--:|------------:|-----------:|------------------------:|----------:|\n|Signal (Model) |  5| 282617820807|          NA|              10939076509|     0.7315|\n|Noise (Error)  | 19|  76303941402|  4015996916|               4015996916|     0.2685|\n|Total (Y)      | 24| 358921762209| 14955073425|              14955073425|     1.0000|\n\n\n:::\n:::\n\n\n### Confidence Interval for Population $R^2$ ($\\rho^2$)\n\nWe construct a 95% confidence interval for the population proportion of variance explained ($\\rho^2$).\n\n#### 1. Manual Inversion Method {.unnumbered}\n\nWe solve for the non-centrality parameters $\\lambda_L$ and $\\lambda_U$ such that our observed $F_{obs}$ corresponds to the appropriate quantiles.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nManual Calculation:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for Population Rho^2: [ 0.4674 ,  0.8401 ]\n```\n\n\n:::\n:::\n\n\n#### 2. Using R Package `MBESS` {.unnumbered}\n\nThe `MBESS` package automates this procedure. We use `Random.Predictors = FALSE` to match the fixed-predictor assumption used in our manual calculation.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$Lower.Conf.Limit.R2\n[1] 0.467363\n\n$Prob.Less.Lower\n[1] 0.025\n\n$Upper.Conf.Limit.R2\n[1] 0.8400781\n\n$Prob.Greater.Upper\n[1] 0.025\n```\n\n\n:::\n:::\n\n\n\n\n\n## Underfitting and Overfitting\n\nWe compare the properties of two competing estimators for the mean response vector $\\mu = E[y]$.\n\n### Notation and Setup\n\nConsider the data partition $X = [X_1 \\ X_2]$. We define two competing models:\n\n**1. Reduced Model ($M_0$)**\n\nAssumes $y = X_1\\beta_1 + e$.\n$$\n\\begin{aligned}\nP_0 &= X_1(X_1^T X_1)^{-1}X_1^T & (\\text{Projection for } M_0) \\\\\n\\hat{y}_0 &= P_0 y & (\\text{Estimator for } M_0)\n\\end{aligned}\n$$\n\n**2. Full Model ($M_1$)**\n\nAssumes $y = X_1\\beta_1 + X_2\\beta_2 + e$.\n$$\n\\begin{aligned}\nP_1 &= X(X^T X)^{-1}X^T & (\\text{Projection for } M_1) \\\\\n\\hat{y}_1 &= P_1 y & (\\text{Estimator for } M_1)\n\\end{aligned}\n$$\n\n**Key Geometric Property:**\nSince the column space of $X_1$ is a subspace of $X$ ($\\mathcal{C}(X_1) \\subset \\mathcal{C}(X)$), we have the nesting property:\n$$\nP_1 P_0 = P_0 \\quad \\text{and} \\quad P_1 - P_0 \\text{ is a projection matrix.}\n$$\n\n---\n\n### Case 1: Underfitting\n\n**The Truth:** The Full Model ($M_1$) is correct.\n$$\ny = X_1\\beta_1 + X_2\\beta_2 + e, \\quad \\beta_2 \\neq 0\n$$\nThe true mean is $\\mu = X_1\\beta_1 + X_2\\beta_2$.\n\nWe analyze the properties of the **Reduced Estimator** $\\hat{y}_0$ (from $M_0$) compared to the correct Full Estimator $\\hat{y}_1$ (from $M_1$).\n\n::: {#thm-underfitting name=\"Bias-Variance Tradeoff in Underfitting\"}\nWhen $M_1$ is true:\n\n1.  **Bias:** The estimator $\\hat{y}_0$ is **biased**, while $\\hat{y}_1$ is unbiased.\n    $$ \\text{Bias}(\\hat{y}_0) = -(I - P_0) X_2 \\beta_2 $$\n2.  **Variance:** The estimator $\\hat{y}_0$ has **smaller variance** (matrix difference is positive semidefinite).\n    $$ \\text{Var}(\\hat{y}_1) - \\text{Var}(\\hat{y}_0) = \\sigma^2 (P_1 - P_0) \\ge 0 $$\n:::\n\n::: {.proof}\n**Part 1 (Bias):**\n$$\n\\begin{aligned}\nE[\\hat{y}_0] &= P_0 E[y] = P_0(X_1\\beta_1 + X_2\\beta_2) \\\\\n&= X_1\\beta_1 + P_0 X_2 \\beta_2 \\quad (\\text{Since } P_0 X_1 = X_1)\n\\end{aligned}\n$$\nThe bias is:\n$$\n\\text{Bias} = E[\\hat{y}_0] - \\mu = (X_1\\beta_1 + P_0 X_2 \\beta_2) - (X_1\\beta_1 + X_2\\beta_2) = -(I - P_0)X_2\\beta_2\n$$\n\n**Part 2 (Variance):**\n$$\n\\text{Var}(\\hat{y}_1) = \\sigma^2 P_1, \\quad \\text{Var}(\\hat{y}_0) = \\sigma^2 P_0\n$$\nThe difference is $\\sigma^2(P_1 - P_0)$. Since $\\mathcal{C}(X_1) \\subset \\mathcal{C}(X)$, the difference $P_1 - P_0$ projects onto the orthogonal complement of $\\mathcal{C}(X_1)$ within $\\mathcal{C}(X)$. It is idempotent and positive semidefinite.\n:::\n\n**Conclusion:** Using $M_0$ when $M_1$ is true introduces bias but reduces variance.\n\n### Case 2: Overfitting\n\n**The Truth:** The Reduced Model ($M_0$) is correct.\n$$\ny = X_1\\beta_1 + e \\quad (\\text{i.e., } \\beta_2 = 0)\n$$\nThe true mean is $\\mu = X_1\\beta_1$.\n\nWe analyze the properties of the **Full Estimator** $\\hat{y}_1$ (from $M_1$) compared to the correct Reduced Estimator $\\hat{y}_0$ (from $M_0$).\n\n::: {#thm-overfitting name=\"Variance Inflation in Overfitting\"}\nWhen $M_0$ is true:\n\n1.  **Bias:** Both estimators are **unbiased**.\n    $$ E[\\hat{y}_1] = \\mu \\quad \\text{and} \\quad E[\\hat{y}_0] = \\mu $$\n2.  **Variance:** The estimator $\\hat{y}_1$ has **unnecessarily higher variance**.\n    $$ \\text{Var}(\\hat{y}_1) \\ge \\text{Var}(\\hat{y}_0) $$\n:::\n\n::: {.proof}\n**Part 1 (Bias):**\nSince $\\mu = X_1\\beta_1$:\n$$\nE[\\hat{y}_1] = P_1 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\mathcal{C}(X))\n$$\n$$\nE[\\hat{y}_0] = P_0 X_1\\beta_1 = X_1\\beta_1 = \\mu \\quad (\\text{Since } X_1 \\in \\mathcal{C}(X_1))\n$$\n\n**Part 2 (Variance):**\nAs shown in Case 1, the difference is $\\sigma^2(P_1 - P_0)$.\nThe cost of overfitting is purely variance inflation. The total variance (trace) increases by the number of unnecessary parameters ($p_2$):\n$$\n\\text{tr}(\\text{Var}(\\hat{y}_1)) - \\text{tr}(\\text{Var}(\\hat{y}_0)) = \\sigma^2 (\\text{tr}(P_1) - \\text{tr}(P_0)) = \\sigma^2 (p_{full} - p_{reduced}) = \\sigma^2 p_2\n$$\n:::\n\n**Conclusion:** Using $M_1$ when $M_0$ is true offers no benefit in bias but strictly increases estimation variance.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}