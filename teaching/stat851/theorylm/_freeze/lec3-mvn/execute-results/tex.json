{
  "hash": "46644b4ae7b988138a97bf8cb20640ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor_options: \n  chunk_output_type: inline\nformat: \n  pdf: default\n  html: default\n---\n\n# Multivariate Normal Distribution\n\n## Motivation\n\nConsider the linear model:\n$$\ny = X\\beta + \\epsilon, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n$$\n\nWe are often interested in the distributional properties of the response vector $y$ and the residuals. Specifically, if $y = (y_1, \\dots, y_n)'$, we need to understand its multivariate distribution.\n$$\n\\hat{y} = Py, \\quad e = y - \\hat{y} = (I_n - P)y\n$$\n\n\n## Random Vectors and Matrices\n\n::: {#def-random-vector name=\"Random Vector and Matrix\"}\n\nA **Random Vector** is a vector whose elements are random variables. E.g.,\n$$\nx_{k \\times 1} = (x_1, x_2, \\dots, x_k)^T\n$$\nwhere $x_1, \\dots, x_k$ are each random variables.\n\nA **Random Matrix** is a matrix whose elements are random variables. E.g., $X_{n \\times k} = (x_{ij})$, where $x_{11}, \\dots, x_{nk}$ are each random variables.\n:::\n\n::: {#def-expected-value name=\"Expected Value\"}\n\nThe expected value (population mean) of a random matrix (or vector) is the matrix (or vector) of expected values of its elements.\n\nFor $X_{n \\times k}$:\n$$\nE(X) = \\begin{pmatrix}\nE(x_{11}) & \\dots & E(x_{1k}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE(x_{n1}) & \\dots & E(x_{nk})\n\\end{pmatrix}\n$$\n\n$$\nE\\left(\\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{pmatrix}\\right) = \\begin{pmatrix} E(x_1) \\\\ \\vdots \\\\ E(x_k) \\end{pmatrix}\n$$\n:::\n\n::: {#def-variance-covariance name=\"Variance-Covariance Matrix\"}\n\nFor a random vector $x_{k \\times 1} = (x_1, \\dots, x_k)^T$, the matrix is:\n\n$$\n\\text{Var}(x) = \\Sigma_x = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2} & \\dots & \\sigma_{kk}\n\\end{pmatrix}\n$$\n\nWhere:\n\n* $\\sigma_{ij} = \\text{Cov}(x_i, x_j) = E[(x_i - \\mu_i)(x_j - \\mu_j)]$\n* $\\sigma_{ii} = \\text{Var}(x_i) = E[(x_i - \\mu_i)^2]$\n\nIn matrix notation:\n$$\n\\text{Var}(x) = E[(x - \\mu_x)(x - \\mu_x)^T]\n$$\nNote: $\\text{Var}(x)$ is symmetric.\n:::\n\n### Derivation of Covariance Matrix Structure\n\nExpanding the vector multiplication for variance:\n$$\n(x - \\mu_x)(x - \\mu_x)' \\quad \\text{where } \\mu_x = (\\mu_1, \\dots, \\mu_n)'\n$$\n$$\n= \\begin{pmatrix} x_1 - \\mu_1 \\\\ \\vdots \\\\ x_n - \\mu_n \\end{pmatrix} (x_1 - \\mu_1, \\dots, x_n - \\mu_n)\n$$\nThis results in the matrix $A = (a_{ij})$ where $a_{ij} = (x_i - \\mu_i)(x_j - \\mu_j)$. Taking expectations yields the covariance matrix elements $\\sigma_{ij}$.\n\n::: {#def-covariance-matrix-two name=\"Covariance Matrix (Two Vectors)\"}\n\nFor random vectors $x_{k \\times 1}$ and $y_{n \\times 1}$, the covariance matrix is:\n$$\n\\text{Cov}(x, y) = E[(x - \\mu_x)(y - \\mu_y)^T] = \\begin{pmatrix}\n\\text{Cov}(x_1, y_1) & \\dots & \\text{Cov}(x_1, y_n) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(x_k, y_1) & \\dots & \\text{Cov}(x_k, y_n)\n\\end{pmatrix}\n$$\nNote that $\\text{Cov}(x, x) = \\text{Var}(x)$.\n:::\n\n::: {#def-correlation-matrix name=\"Correlation Matrix\"}\n\nThe correlation matrix of a random vector $x$ is:\n$$\n\\text{corr}(x) = \\begin{pmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{k1} & \\rho_{k2} & \\dots & 1\n\\end{pmatrix}\n$$\nwhere $\\rho_{ij} = \\text{corr}(x_i, x_j)$.\n\n**Relationships:**\nLet $V_x = \\text{diag}(\\text{Var}(x_1), \\dots, \\text{Var}(x_k))$.\n$$\n\\Sigma_x = V_x^{1/2} \\rho_x V_x^{1/2} \\quad \\text{and} \\quad \\rho_x = (V_x^{1/2})^{-1} \\Sigma_x (V_x^{1/2})^{-1}\n$$\nSimilarly for two vectors:\n$$\n\\Sigma_{xy} = V_x^{1/2} \\rho_{xy} V_y^{1/2}\n$$\n:::\n\n## Properties of Mean and Variance\n\nWe can derive several key algebraic properties for operations on random vectors.\n\n1.  $E(X + Y) = E(X) + E(Y)$\n2.  $E(AXB) = A E(X) B$ (In particular, $E(AX) = A\\mu_x$)\n3.  $\\text{Cov}(x, y) = \\text{Cov}(y, x)^T$\n4.  $\\text{Cov}(x + c, y + d) = \\text{Cov}(x, y)$\n5.  $\\text{Cov}(Ax, By) = A \\text{Cov}(x, y) B^T$\n    * Special case for scalars: $\\text{Cov}(ax, by) = ab \\cdot \\text{Cov}(x, y)$\n6.  $\\text{Cov}(x_1 + x_2, y_1) = \\text{Cov}(x_1, y_1) + \\text{Cov}(x_2, y_1)$\n7.  $\\text{Var}(x + c) = \\text{Var}(x)$\n8.  $\\text{Var}(Ax) = A \\text{Var}(x) A^T$\n9.  $\\text{Var}(x_1 + x_2) = \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)$\n10. $\\text{Var}(\\sum x_i) = \\sum \\text{Var}(x_i)$ if independent.\n\n::: {.proof} \n**Property 5 (Covariance of Linear Transformation):**\n$$\n\\begin{aligned}\n\\text{Cov}(Ax, By) &= E[(Ax - A\\mu_x)(By - B\\mu_y)^T] \\\\\n&= A E[(x - \\mu_x)(y - \\mu_y)^T] B^T \\\\\n&= A \\text{Cov}(x, y) B^T\n\\end{aligned}\n$$\n**Property 2 (Expectation of Linear Transformation)**:\n\nTo prove $E(AXB) = A E(X) B$:\nFirst consider $E(Ax_j)$ where $x_j$ is a column of $X$.\n$$\nE(Ax_j) = E\\begin{pmatrix} a_1' x_j \\\\ \\vdots \\\\ a_n' x_j \\end{pmatrix} = \\begin{pmatrix} E(a_1' x_j) \\\\ \\vdots \\\\ E(a_n' x_j) \\end{pmatrix}\n$$\nSince $a_i$ are constants:\n$$\nE(a_i' x_j) = E\\left(\\sum_{k=1}^p a_{ik} x_{kj}\\right) = \\sum_{k=1}^p a_{ik} E(x_{kj}) = a_i' E(x_j)\n$$\nThus $E(Ax_j) = A E(x_j)$. Applying this to all columns of $X$:\n$$\nE(AX) = [E(Ax_1), \\dots, E(Ax_m)] = [AE(x_1), \\dots, AE(x_m)] = A E(X)\n$$\nSimilarly, $E(XB) = E(X)B$.\n\n**Proof of Property 9 (Variance of Sum):**\n\n$$\n\\text{Var}(x_1 + x_2) = E[(x_1 + x_2 - \\mu_1 - \\mu_2)(x_1 + x_2 - \\mu_1 - \\mu_2)^T]\n$$\nLet centered variables be denoted by differences.\n$$\n= E[((x_1 - \\mu_1) + (x_2 - \\mu_2))((x_1 - \\mu_1) + (x_2 - \\mu_2))^T]\n$$\nExpanding terms:\n$$\n= E[(x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_1 - \\mu_1)(x_2 - \\mu_2)^T + (x_2 - \\mu_2)(x_1 - \\mu_1)^T + (x_2 - \\mu_2)(x_2 - \\mu_2)^T]\n$$\n$$\n= \\text{Var}(x_1) + \\text{Cov}(x_1, x_2) + \\text{Cov}(x_2, x_1) + \\text{Var}(x_2)\n$$\n:::\n\n## The Multivariate Normal Distribution\n\n### Definition and Density\n\n::: {#def-standard-normal name=\"Independent Standard Normal\"}\n\nLet $z = (z_1, \\dots, z_n)'$ where $z_i \\sim N(0, 1)$ are independent. We say $z \\sim N_n(0, I_n)$.\nThe joint PDF is the product of marginals:\n$$\nf(z) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z_i^2}{2}} = \\frac{1}{(2\\pi)^{n/2}} e^{-\\frac{1}{2} z^T z}\n$$\nProperties: $E(z) = 0$ and $\\text{Var}(z) = I_n$ (Covariance is 0 for $i \\ne j$, Variance is 1).\n:::\n\n::: {#def-mvn name=\"Multivariate Normal Distribution\"}\n\nA random vector $x$ ($n \\times 1$) has a **multivariate normal distribution** if it has the same distribution as:\n$$\nx = A_{n \\times p} z_{p \\times 1} + \\mu_{n \\times 1}\n$$\nwhere $z \\sim N_p(0, I_p)$, $A$ is a matrix of constants, and $\\mu$ is a vector of constants.\nThe moments are:\n\n* $E(x) = \\mu$\n* $\\text{Var}(x) = AA^T = \\Sigma$\n:::\n\n### Geometric Interpretation\nUsing Spectral Decomposition, $\\Sigma = Q \\Lambda Q'$. We can view the transformation $x = Az + \\mu$ as:\n\n1.  Scaling by eigenvalues ($\\Lambda^{1/2}$).\n2.  Rotation by eigenvectors ($Q$).\n3.  Shift by mean ($\\mu$).\n\n::: {.content-visible when-format=\"html\"}\n\n**An Shinely App for Visualizing Bivariate Normal**\n\nUse the controls to construct the covariance matrix $\\boldsymbol{\\Sigma}$ geometrically.\n\nWe define the transformation matrix $\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}^{1/2}$, where $\\mathbf{Q}$ is a rotation matrix and $\\mathbf{\\Lambda}^{1/2}$ is a diagonal scaling matrix. The resulting covariance is $\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}'$.\n\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 700\n#| echo: false\n\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(shinyWidgets)\nlibrary(munsell) \nlibrary(scales)\nlibrary(tibble)\nlibrary(rlang)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# --- 1. PRE-GENERATE FIXED Z POINTS ---\nset.seed(123)\nz_fixed <- matrix(rnorm(50 * 2), ncol = 2)\n\nui <- page_fillable(\n  theme = bs_theme(version = 5),\n  withMathJax(), \n  \n  # --- ROW 1: CONTROLS (Compact Strip) ---\n  card(\n    class = \"p-2\", \n    layout_columns(\n      col_widths = c(3, 2, 2, 2, 2),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\theta$$\")), \n          noUiSliderInput(\"theta\", label = NULL, min = 0, max = 360, value = 0, step = 5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#0d6efd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_1}$$\")), \n          noUiSliderInput(\"L1\", label = NULL, min = 0.5, max = 3, value = 2, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#ffc107\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\sqrt{\\\\lambda_2}$$\")), \n          noUiSliderInput(\"L2\", label = NULL, min = 0.5, max = 3, value = 1, step = 0.1, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#adb5bd\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_1$$\")), \n          noUiSliderInput(\"mu1\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\")),\n      \n      div(class = \"text-center\", tags$label(HTML(\"$$\\\\mu_2$$\")), \n          noUiSliderInput(\"mu2\", label = NULL, min = -3, max = 3, value = 0, step = 0.5, \n                          orientation = \"horizontal\", width = \"100%\", height = \"10px\", color = \"#6c757d\"))\n    )\n  ),\n\n  # --- ROW 2: SIDE-BY-SIDE (Plot & Math) ---\n  layout_columns(\n    col_widths = c(8, 4), # 2/3 for Plot, 1/3 for Matrix\n    \n    # Left: Visualization\n    card(\n      full_screen = TRUE,\n      plotOutput(\"contourPlot\", height = \"500px\")\n    ),\n    \n    # Right: The Math (Larger Font)\n    card(\n      class = \"p-3 d-flex justify-content-center\", # Center content vertically\n      h5(\"Algebraic Representation\", class = \"mb-3 text-center\"),\n      \n      # Use CSS to make the font larger and monospaced\n      div(\n        style = \"font-family: 'Courier New', monospace; font-size: 1.1rem; line-height: 1.4;\",\n        verbatimTextOutput(\"matrixSide\", placeholder = TRUE)\n      )\n    )\n  )\n)\n\nserver <- function(input, output) {\n\n  data <- reactive({\n    theta_rad <- input$theta * pi / 180\n    Q <- matrix(c(cos(theta_rad), sin(theta_rad), -sin(theta_rad), cos(theta_rad)), 2, 2)\n    Lam_sqrt <- diag(c(input$L1, input$L2))\n    \n    A <- Q %*% Lam_sqrt\n    Sigma <- A %*% t(A)\n    mu_vec <- c(input$mu1, input$mu2)\n    \n    x_points <- z_fixed %*% t(A)\n    x_points[,1] <- x_points[,1] + mu_vec[1]\n    x_points[,2] <- x_points[,2] + mu_vec[2]\n    \n    list(Q=Q, L=c(input$L1, input$L2), mu=mu_vec, Sigma=Sigma, A=A, points=as.data.frame(x_points))\n  })\n\n  output$matrixSide <- renderText({\n    M <- data()\n    A <- round(M$A, 2)\n    S <- round(M$Sigma, 2)\n    rho <- cov2cor(M$Sigma)[1,2]\n    \n    # Formatted to fill vertical space comfortably\n    paste0(\n      \"Linear Transform:\\n\",\n      \"x = A z + μ\\n\\n\",\n      \n      \"Matrix A:\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[1,1], A[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", A[2,1], A[2,2]),\n      \"\\n\",\n      \n      \"Covariance Σ:\\n\",\n      \"(Σ = AA')\\n\",\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[1,1], S[1,2]),\n      sprintf(\"[%4.1f   %4.1f]\\n\", S[2,1], S[2,2]),\n      \"\\n\",\n      \n      \"Correlation:\\n\",\n      sprintf(\"ρ = %.3f\", rho)\n    )\n  })\n\n  output$contourPlot <- renderPlot({\n    req(data())\n    M <- data()\n    \n    grid_r <- seq(-6, 6, length.out = 60)\n    df_grid <- expand.grid(x = grid_r, y = grid_r)\n    df_grid$z <- dmvnorm(as.matrix(df_grid), mean = M$mu, sigma = M$Sigma)\n    \n    v1 <- M$Q[,1] * M$L[1]; v2 <- M$Q[,2] * M$L[2]\n    axes <- tibble(x = M$mu[1], y = M$mu[2],\n                   xend1 = M$mu[1] + v1[1], yend1 = M$mu[2] + v1[2],\n                   xend2 = M$mu[1] + v2[1], yend2 = M$mu[2] + v2[2])\n    \n    ggplot() +\n      geom_contour_filled(data = df_grid, aes(x, y, z = z), bins = 9, show.legend = FALSE) +\n      geom_point(data = M$points, aes(V1, V2), color = \"black\", size = 2, alpha = 0.7) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend1, yend=yend1), \n                   color = \"#ffc107\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      geom_segment(data = axes, aes(x=x, y=y, xend=xend2, yend=yend2), \n                   color = \"white\", linewidth = 1.5, arrow = arrow(length = unit(0.3,\"cm\"))) +\n      coord_fixed(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      theme_minimal() +\n      labs(x = \"X\", y = \"Y\")\n  })\n}\n\nshinyApp(ui, server)\n```\n:::\n\n### Probability Density Function\n\nIf $\\Sigma$ is positive definite, the PDF exists. We use the change of variable formula for $x = Az + \\mu$:\n$$\nf_x(x) = f_z(g^{-1}(x)) \\cdot |J|\n$$\nwhere $z = A^{-1}(x - \\mu)$ and $J = \\det(A^{-1}) = |A|^{-1}$.\n\n$$\nf_x(x) = (2\\pi)^{-p/2} |A|^{-1} \\exp \\left\\{ -\\frac{1}{2} (A^{-1}(x-\\mu))^T (A^{-1}(x-\\mu)) \\right\\}\n$$\n\nUsing $|\\Sigma| = |AA^T| = |A|^2$ and $\\Sigma^{-1} = (AA^T)^{-1}$, we get:\n$$\nf_x(x) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\left\\{ -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right\\}\n$$\n\n\n\n### Moment Generating Function\n\n::: {#def-mgf name=\"Moment Generating Function (MGF)\"}\n\nThe MGF of a random vector $x$ is $M_x(t) = E(e^{t^T x})$.\nFor $x = Az + \\mu$:\n$$\nM_x(t) = E[e^{t^T(Az + \\mu)}] = e^{t^T\\mu} E[e^{(A^T t)^T z}] = e^{t^T\\mu} M_z(A^T t)\n$$\nSince $M_z(u) = e^{u^T u / 2}$:\n$$\nM_x(t) = e^{t^T\\mu} \\exp\\left( \\frac{1}{2} t^T (AA^T) t \\right) = \\exp \\left( t^T\\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n$$\n:::\n\nKey Properties:\n\n1.  **Uniqueness:** Two random vectors with the same MGF have the same distribution.\n\n2.  **Independence:** $y_1$ and $y_2$ are independent iff $M_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)$.\n\n## Construction and Linear Transformations\n\n::: {#thm-construction name=\"Constructing MVN Random Vector\"}\nLet $\\mu \\in \\mathbb{R}^n$ and $\\Sigma$ be an $n \\times n$ symmetric non-negative definitive (n.n.d) matrix. Then there exists a multivariate normal distribution with mean $\\mu$ and covariance $\\Sigma$.\n:::\n\n:::{.proof}\nSince $\\Sigma$ is n.n.d., there exists $B$ such that $\\Sigma = BB^T$ (e.g., via Cholesky or Spetral Decomposition). Let $z \\sim N_n(0, I)$ and define $x = Bz + \\mu$.\n:::\n\n::: {#thm-linear-transform name=\"Linear Transformation Theorem\"}\nLet $x \\sim N_n(\\mu, \\Sigma)$. Let $y = Cx + d$ where $C$ is $r \\times n$ and $d$ is $r \\times 1$.\nThen:\n$$\ny \\sim N_r(C\\mu + d, C \\Sigma C^T)\n$$\n:::\n\n:::{.proof}\n$x = Az + \\mu$ where $AA^T = \\Sigma$.\n$$\ny = C(Az + \\mu) + d = (CA)z + (C\\mu + d)\n$$\nThis fits the definition of MVN with mean $C\\mu + d$ and variance $C \\Sigma C^T$.\n:::\n\n### Important Corollaries of @thm-linear-transform\n\n::: {#cor-marginals name=\"Marginals\"}\nAny subvector of a multivariate normal vector is also multivariate normal. \n:::\n\n:::{.proof}\nIf we partition $x = (x_1', x_2')'$, we can use $C = (I_r, 0)$ to show $x_1 \\sim N(\\mu_1, \\Sigma_{11})$.\n:::\n\n::: {#cor-univariate name=\"Univariate Combinations\"}\nAny linear combination $a^T x$ is univariate normal:\n$$\na^T x \\sim N(a^T \\mu, a^T \\Sigma a)\n$$\n:::\n\n::: {#cor-orthogonal name=\"Orthogonal Transformations\"}\nIf $x \\sim N(0, I_n)$ and $Q$ is orthogonal ($Q'Q = I$), then $y = Q'x \\sim N(0, I_n)$.\n:::\n\n::: {#cor-standardization name=\"Standardization\"}\nIf $y \\sim N_n(\\mu, \\Sigma)$ and $\\Sigma$ is positive definite:\n$$\n\\Sigma^{-1/2}(y - \\mu) \\sim N_n(0, I_n)\n$$\n:::\n\n:::{.proof}\nLet $z = \\Sigma^{-1/2}(y - \\mu)$. Then $\\text{Var}(z) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = I_n$.\n:::\n\n## Independence\n\n::: {#thm-independence name=\"Independence in MVN\"}\n\nLet $y \\sim N(\\mu, \\Sigma)$ be partitioned into $y_1$ and $y_2$.\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n$$\nThen $y_1$ and $y_2$ are independent if and only if $\\Sigma_{12} = 0$ (zero covariance).\n:::\n\n:::{.proof}\n1. Independence $\\implies$ Covariance is 0:\nThis holds generally for any distribution.\n$$\n\\text{Cov}(y_1, y_2) = E[(y_1 - \\mu_1)(y_2 - \\mu_2)'] = 0\n$$\n\n2. Covariance is 0 $\\implies$ Independence:\nThis is specific to MVN. We use MGFs.\nIf $\\Sigma_{12} = 0$, the quadratic form in the MGF splits:\n$$\nt^T \\Sigma t = t_1^T \\Sigma_{11} t_1 + t_2^T \\Sigma_{22} t_2\n$$\nThe MGF becomes:\n$$\nM_y(t) = \\exp(t_1^T \\mu_1 + \\frac{1}{2} t_1^T \\Sigma_{11} t_1) \\times \\exp(t_2^T \\mu_2 + \\frac{1}{2} t_2^T \\Sigma_{22} t_2)\n$$\n$$\nM_y(t) = M_{y_1}(t_1) M_{y_2}(t_2)\n$$\nThus, they are independent.\n:::\n\n## Signal-Noise Decomposition for Multivariate Normal Distribution\n\nWe can formalize the relationship between two random vectors $y$ and $x$ through a decomposition theorem that separates the systematic signal from the stochastic noise.\n\n::: {#thm-reg-decomp name=\"Regression Decomposition Theorem\"}\nLet the random vector $V$ of dimension $p \\times 1$ be partitioned into two subvectors $y$ ($p_1 \\times 1$) and $x$ ($p_2 \\times 1$). Assume $V$ follows a multivariate normal distribution:\n\n$$\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N_p\\left( \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\right)\n$$\n\nThe response vector $y$ can be uniquely decomposed into a systematic component and a stochastic error:\n$$\ny = m(x) + e\n$$\nwhere we define the **Regression Coefficient Matrix** $B$ and the components as:\n\n$$\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1}\n$$\n\n$$\nm(x) = \\mu_y + B(x - \\mu_x)\n$$\n\n$$\ne = y - m(x)\n$$\n\n**Properties:**\n\n1.  **Independence:** The noise vector $e$ is statistically independent of the predictor $x$ (and consequently independent of $m(x)$).\n\n2.  **Marginal Distributions:**\n    * $m(x) \\sim N_{p_1}(\\mu_y, \\; B \\Sigma_{xx} B^T)$\n    * $e \\sim N_{p_1}(0, \\; \\Sigma_{yy} - B \\Sigma_{xx} B^T)$\n\n3.  **Conditional Distribution:**\n    Since $y = m(x) + e$, and $e$ is independent of $x$, the conditional distribution is:\n    $$\n    y | x \\sim N_{p_1}(m(x), \\Sigma_{y|x})\n    $$\n    where:\n    $$\n    m(x) = \\mu_y + B(x - \\mu_x) = \\mu_y + \\Sigma_{yx}\\Sigma_{xx}^{-1}(x - \\mu_x)\n    $$\n    $$\n    \\Sigma_{y|x} = \\Sigma_{yy} - B \\Sigma_{xx} B^T = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}\n    $$\n:::\n\n::: {.proof}\nWe define a transformation from the input vector $V = \\begin{pmatrix} y \\\\ x \\end{pmatrix}$ to the target vector $W = \\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}$.\n\nUsing the linear transformation $W = CV + d$:\n\n$$\n\\underbrace{\\begin{pmatrix} m(x) \\\\ e \\end{pmatrix}}_{W} = \\underbrace{\\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix}}_{C} \\underbrace{\\begin{pmatrix} y \\\\ x \\end{pmatrix}}_{V} + \\underbrace{\\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -(\\mu_y - B \\mu_x) \\end{pmatrix}}_{d}\n$$\n\n1. Mean Vector\n\n$$\nE[W] = C E[V] + d = \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\mu_y \\\\ \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix} \n= \\begin{pmatrix} B \\mu_x \\\\ \\mu_y - B \\mu_x \\end{pmatrix} + \\begin{pmatrix} \\mu_y - B \\mu_x \\\\ -\\mu_y + B \\mu_x \\end{pmatrix} \n= \\begin{pmatrix} \\mu_y \\\\ 0 \\end{pmatrix}\n$$\n\n2. Covariance Matrix\n\nWe compute $\\text{Var}(W) = C \\Sigma C^T$ directly:\n\n$$\n\\begin{aligned}\nC \\Sigma C^T &= \\begin{pmatrix} 0 & B \\\\ I & -B \\end{pmatrix} \\begin{pmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xy} & B \\Sigma_{xx} \\\\ \\Sigma_{yy} - B \\Sigma_{xy} & \\Sigma_{yx} - B \\Sigma_{xx} \\end{pmatrix} \\begin{pmatrix} 0 & I \\\\ B^T & -B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & B \\Sigma_{xy} - B \\Sigma_{xx} B^T \\\\ \\Sigma_{yx}B^T - B \\Sigma_{xx} B^T & (\\Sigma_{yy} - B \\Sigma_{xy}) - (\\Sigma_{yx} - B \\Sigma_{xx})B^T \\end{pmatrix} \\\\\n&= \\begin{pmatrix} B \\Sigma_{xx} B^T & 0 \\\\ 0 & \\Sigma_{yy} - B \\Sigma_{xx} B^T \\end{pmatrix}\n\\end{aligned}\n$$\n\n3. Conditional Distribution\n\nWe have established that $y = m(x) + e$ where $e$ is independent of $x$. \nTo find the distribution of $y$ conditional on $x$, we observe that $m(x)$ becomes a constant vector when $x$ is fixed, and the randomness comes solely from $e$:\n\n$$\nE[y|x] = m(x) + E[e|x] = m(x) + 0 = m(x)\n$$\n$$\n\\text{Var}(y|x) = \\text{Var}(m(x)|x) + \\text{Var}(e|x) = 0 + \\text{Var}(e) = \\Sigma_{y|x}\n$$\n\nThus, $y | x \\sim N(m(x), \\Sigma_{y|x})$.\n:::\n\n### Connections with Other Formulas\n\n\n#### Rao-Blackwell Decomposition of Variance\n\nThe Law of Total Variance (Rao-Blackwell theorem) allows us to decompose the total variance of $y$ into two orthogonal components based on the predictor $x$:\n\n$$\n\\text{Var}(y) = \\underbrace{E[\\text{Var}(y | x)]}_{\\text{Unexplained (Noise)}} + \\underbrace{\\text{Var}[E(y | x)]}_{\\text{Explained (Signal)}}\n$$\n\nIn the Multivariate Normal case, this decomposition perfectly aligns with our regression model $y = m(x) + e$.\n\n#### Variance of Noise {.unnumbered}\nThis term represents the average variance remaining in $y$ after accounting for $x$. It corresponds to the variance of the error term $e$:\n\n$$\nE[\\text{Var}(y | x)] = \\text{Var}(e) = \\Sigma_{yy} - B \\Sigma_{xx} B^T\n$$\n\n#### Variance of Signal{.unnumbered}\nThis term represents the variability of the conditional mean $m(x)$ itself. Using the matrix $B$, this takes the quadratic form:\n\n$$\n\\text{Var}[E(y | x)] = \\text{Var}[m(x)] = B \\Sigma_{xx} B^T\n$$\n\n#### Total Variance {.unnumbered}\nSumming the Signal and Noise components recovers the total marginal variance of $y$:\n\n$$\n\\Sigma_{yy} = \\underbrace{\\Sigma_{yy} - B \\Sigma_{xx} B^T}_{\\text{Unexplained (Noise)}} + \\underbrace{B \\Sigma_{xx} B^T}_{\\text{Explained (Signal)}}\n$$\n\n\n#### Connection to OLS Regression Estimators\nIn OLS regression, centering the data allows us to separate the intercept from the slopes. Let $\\mathbf{y}_c$ and $\\mathbf{X}_c$ be the centered response and design matrices (where $\\mathbf{X}_c$ **excludes the column of 1s**). Using this centered form, the total sum of squares decomposes exactly like the population variance:\n\n$$\n\\text{SST} = \\text{SSR} + \\text{SSE}\n$$\n\nComparing the sample quantities to their population counterparts:\n\n1. **Regression Coefficients:**\n  $$\n  \\hat{\\beta}^T = (\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{y}_c \\approx B\n  $$\n  *Note: $\\hat{\\beta}$ here represents only the slope coefficients, matching the dimensions of the covariance matrix $\\Sigma_{xx}$.*\n\n1.  **Explained Variation (Signal):**\n    $$\n    \\text{SSR} = \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1) B \\Sigma_{xx} B^T\n    $$\n\n1.  **Unexplained Variation (Noise):**\n    $$\n    \\text{SSE} = \\mathbf{y}_c^T \\mathbf{y}_c - \\hat{\\beta}^T (\\mathbf{X}_c^T \\mathbf{X}_c) \\hat{\\beta} \\quad \\approx \\quad (n-1)(\\Sigma_{yy} - B \\Sigma_{xx} B^T)\n    $$\n\n\n\n## Partial and Multiple Correlation\n\n::: {#def-partial-corr name=\"Partial Correlation\"}\nThe partial correlation between elements $y_i$ and $y_j$ given a set of variables $x$ is derived from the conditional covariance matrix $\\Sigma_{y|x}$:\n$$\n\\rho_{ij|x} = \\frac{\\sigma_{ij|x}}{\\sqrt{\\sigma_{ii|x} \\sigma_{jj|x}}}\n$$\nwhere $\\sigma_{ij|x}$ are elements of $\\Sigma_{y|x} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}$.\n:::\n\n::: {#def-multiple-corr name=\"Multiple Correlation ($R^2$)\"}\nFor a scalar $y$ and vector $x$, the squared multiple correlation is the proportion of variance of $y$ explained by the conditional mean:\n$$\nR^2_{y|x} = \\frac{\\text{Var}(E(y|x))}{\\text{Var}(y)} = \\frac{\\Sigma_{yx} \\Sigma_{xx}^{-1} \\Sigma_{xy}}{\\sigma^2_{y}}\n$$\n:::\nNote: this definition is the population or theretical $R^2$, which is estimated by adjusted $R^2$ using sample in linear regression. \n\n## Examples\n\n::: {#exm-numerical name=\"Bivariate Normal\"}\nLet the random vector $\\begin{pmatrix} y \\\\ x \\end{pmatrix}$ follow a bivariate normal distribution:\n$$\n\\begin{pmatrix} y \\\\ x \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 & 2 \\\\ 2 & 4 \\end{pmatrix} \\right)\n$$\nHere, $\\mu_y = 1, \\mu_x = 2, \\Sigma_{yy} = 2, \\Sigma_{xx} = 4$, and $\\Sigma_{yx} = 2$.\n\n1. Finding the Regression Coefficient Matrix $B$\nUsing the population formula:\n$$\nB = \\Sigma_{yx}\\Sigma_{xx}^{-1} = 2(4)^{-1} = 0.5\n$$\n\n2. Finding the Conditional Mean $m(x)$ (The Signal)\nThe systematic component represents the projection of $y$ onto $x$:\n$$\n\\begin{aligned}\nm(x) &= \\mu_y + B(x - \\mu_x) \\\\\n&= 1 + 0.5(x - 2) = 0.5x\n\\end{aligned}\n$$\n\n3. Variance of the Signal $\\text{Var}(m(x))$\nUsing the quadratic form established in the theorem:\n$$\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = 0.5(4)(0.5) = 1\n$$\n\n4. Variance of the Noise $\\text{Var}(y|x)$ (The Residual)\nBy the Signal-Noise Decomposition:\n$$\n\\begin{aligned}\n\\text{Var}(y|x) &= \\Sigma_{yy} - \\text{Var}(m(x)) \\\\\n&= 2 - 1 = 1\n\\end{aligned}\n$$\nThus, $y | x \\sim N(m(x), 1)$. The total variance (2) is split equally between signal (1) and noise (1).\n\n5. Multiple Correlation Coefficient ($R^2$)\n$$\nR^2 = \\frac{\\text{Var}(m(x))}{\\Sigma_{yy}} = \\frac{1}{2} = 0.5\n$$\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Illustration of Rao-Blackwell Variance Decomposition in Bivariate Normal](lec3-mvn_files/figure-pdf/fig-variance-decomp-scaled-1.pdf){#fig-variance-decomp-scaled fig-align='center'}\n:::\n:::\n\n\n\n::: {#exm-trivariate name=\"Trivariate Normal with 2 Predictors\"}\n\nLet $V = (y, x_1, x_2)' \\sim N_3(\\mu, \\Sigma)$ with:\n$$\n\\mu = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix} 10 & 3 & 4 \\\\ 3 & 2 & 1 \\\\ 4 & 1 & 4 \\end{pmatrix}\n$$\nWe partition these into $\\Sigma_{yy} = 10$, $\\Sigma_{yx} = \\begin{pmatrix} 3 & 4 \\end{pmatrix}$, and $\\Sigma_{xx} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix}$.\n\n1. Finding the Regression Coefficient Matrix $B$\n$$\n\\Sigma_{xx}^{-1} = \\frac{1}{7} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2 \\end{pmatrix} \\implies B = \\Sigma_{yx} \\Sigma_{xx}^{-1} = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix}\n$$\n\n1. Finding the Conditional Mean $m(x)$ (The Signal)\n$$\nm(x) = 1 + \\frac{8}{7}(x_1 - 2) + \\frac{5}{7}(x_2 - 3)\n$$\n\n1. Variance of the Signal $\\text{Var}(m(x))$\n$$\n\\text{Var}(m(x)) = B \\Sigma_{xx} B^T = \\begin{pmatrix} \\frac{8}{7} & \\frac{5}{7} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{44}{7} \\approx 6.29\n$$\n\n1. Variance of the Noise $\\text{Var}(y|x)$ (The Residual)\nUsing the Signal-Noise Decomposition:\n$$\n\\Sigma_{y|x} = \\Sigma_{yy} - \\text{Var}(m(x)) = 10 - 6.29 = 3.71\n$$\n\n1. Multiple Correlation Coefficient ($R^2$)\n$$\nR^2 = \\frac{6.29}{10} = 0.629\n$$\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Signal-Noise Variance Decomposition in Multivariate Normal](lec3-mvn_files/figure-pdf/fig-trivariate-refined-1.pdf){#fig-trivariate-refined}\n:::\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\n\nx1_seq <- seq(min(df$x1), max(df$x1), length.out=20)\nx2_seq <- seq(min(df$x2), max(df$x2), length.out=20)\ngrid <- expand.grid(x1=x1_seq, x2=x2_seq)\ngrid$y_pred <- 1 + (8/7)*(grid$x1 - 2) + (5/7)*(grid$x2 - 3)\nz_matrix <- matrix(grid$y_pred, nrow=20, ncol=20)\n\nplot_ly() %>%\n  add_markers(data = df, x = ~x1, y = ~x2, z = ~y,\n              marker = list(size = 3, color = '#444', opacity = 0.5),\n              name = \"Observed (Total)\") %>%\n  add_surface(x = x1_seq, y = x2_seq, z = z_matrix,\n              opacity = 0.6, colorscale = list(c(0, 1), c(\"red\", \"red\")),\n              showscale = FALSE, name = \"Signal (m(x))\") %>%\n  layout(scene = list(xaxis = list(title = \"x1\"), yaxis = list(title = \"x2\"), zaxis = list(title = \"y\")))\n```\n:::\n\n\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}