{
  "hash": "29aa6c341af485998840264f2a303d47",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: \n  pdf: default\n  html: default\n---\n\n\n# Projection in Vector Space\n\n## Vector and Projection onto a Line\n\n### Vectors and Operations\n\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\n\n::: {#def-vector name=\"Vector\"}\nA **vector** $x$ is defined as a point in $n$-dimensional space ($\\mathbb{R}^n$). It is typically represented as a column vector containing $n$ real-valued components:\n$$\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n$$\n\n:::\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\n\n**Vector Arithmetic:** Vectors can be manipulated geometrically:\n\n::: {#def-vector-addition name=\"Vector Addition\"}\nThe sum of two vectors $x$ and $y$ creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the \"parallelogram rule\" or the \"head-to-tail\" method, where you place the tail of $y$ at the head of $x$.\n$$\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n$$\n\n:::\n\n::: {#def-vector-subtraction name=\"Vector Subtraction\"}\nThe difference $d = y - x$ is the vector that \"closes the triangle\" formed by $x$ and $y$. It represents the displacement vector that connects the tip of $x$ to the tip of $y$, such that $x + d = y$.\n\n:::\n\n### Scalar Multiplication and Distance\n\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\n\n::: {#def-scalar-mult name=\"Scalar Multiplication\"}\nMultiplying a vector by a scalar $c$ scales its magnitude (length) without changing its line of direction. If $c$ is positive, the direction remains the same; if $c$ is negative, the direction is reversed.\n$$\nc x = \\begin{pmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{pmatrix}\n$$\n\n:::\nWe often need to quantify the \"size\" of a vector. This is done using the concept of length, or norm.\n\n::: {#def-euclidean-distance name=\"Euclidean Distance (Length)\"}\nThe length (or norm) of a vector $x = (x_1, \\dots, x_n)^T$ corresponds to the straight-line distance from the origin to the point defined by $x$. It is defined as the square root of the sum of squared components:\n$$\n||x||^2 = \\sum_{i=1}^n x_i^2\n$$\n\n$$\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n$$\n\n:::\n\n### Angle and Inner Product\n\nTo understand the relationship between two vectors $x$ and $y$ beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors $x$, $y$, and their difference $y-x$. By applying the classic **Law of Cosines** to this triangle, we can relate the geometric angle to the vector lengths.\n\n::: {#thm-law-of-cosines name=\"Law of Cosines\"}\nFor a triangle with sides $a, b, c$ and angle $\\theta$ opposite to side $c$:\n$$\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n$$\n\n:::\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n$$\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n$$\n\nThis equation provides a critical link between the geometric angle $\\theta$ and the algebraic norms of the vectors.\n\n**Derivation of Inner Product**\n\nWe can express the squared distance term $||y - x||^2$ purely algebraically by expanding the components:\n\n$$\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n$$\n\n$$\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n$$\n\n$$\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n$$\n\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the **Inner Product** (or dot product).\n\n::: {#def-inner-product name=\"Inner Product\"}\nThe inner product of two vectors $x$ and $y$ is defined as the sum of the products of their corresponding components:\n$$\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n$$\n\n:::\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n$$\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n$$\n\n### Coordinate (Scalar) Projection\n\nThe inner product allows us to calculate projections, which quantify how much of one vector \"lies along\" another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the \"shadow\" cast by vector $y$ onto vector $x$.\n\nThe length of this projection is given by:\n\n$$\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n$$\n\nThis expression can be interpreted as the inner product of $y$ with the normalized (unit) vector in the direction of $x$:\n\n$$\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n$$\n\n### Vector Projection Formula\n\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\n\n::: {#def-vector-projection name=\"Vector Projection\"}\nThe projection of vector $y$ onto vector $x$, denoted $\\hat{y}$, is calculated as:\n$$\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n$$\n\n$$\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n$$\n\nThis is often written compactly by combining the denominators:\n\n$$\n\\hat{y} = \\frac{x'y}{||x||^2} x\n$$\n\n:::\n\n### Perpendicularity (Orthogonality)\n\nA special case of the angle between vectors arises when $\\theta = 90^\\circ$. This geometric concept of perpendicularity is central to the theory of projections and least squares.\n\n::: {#def-perpendicularity name=\"Perpendicularity\"}\nTwo vectors are defined as **perpendicular** (or orthogonal) if the angle between them is $90^\\circ$ ($\\pi/2$).\n\nSince $\\cos(90^\\circ) = 0$, the condition for orthogonality simplifies to the inner product being zero:\n\n$$\nx'y = 0 \\iff x \\perp y\n$$\n\n:::\n\n::: {#exm-orthogonal-vectors name=\"Orthogonal Vectors\"}\nConsider two vectors in $\\mathbb{R}^2$: $x = (1, 1)'$ and $y = (1, -1)'$.\n$$\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n$$\n\nSince their inner product is zero, these vectors are orthogonal to each other.\n\n:::\n\n### Projection onto a Line (Subspace)\n\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\n\n::: {#def-line-space name=\"Line Spanned by a Vector\"}\nThe line space $L(x)$, or the space spanned by a vector $x$, is defined as the set of all scalar multiples of $x$:\n$$\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n$$\n\n:::\nThe projection of $y$ onto $L(x)$, denoted $\\hat{y}$, is defined by the geometric property that it is the closest point on the line to $y$. This implies that the error vector (or residual) must be perpendicular to the line itself.\n\n::: {#def-projection-line name=\"Projection onto a Line\"}\nA vector $\\hat{y}$ is the projection of $y$ onto the line $L(x)$ if:\n\n1.  $\\hat{y}$ lies on the line $L(x)$ (i.e., $\\hat{y} = cx$ for some scalar $c$).\n\n2.  The residual vector $(y - \\hat{y})$ is perpendicular to the direction vector $x$.\n\n:::\n**Derivation:** To find the value of the scalar $c$, we apply the orthogonality condition:\n$$\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n$$\n\nExpanding this inner product gives:\n\n$$\nx'y - c(x'x) = 0\n$$\n\nSolving for $c$, we obtain:\n\n$$\nc = \\frac{x'y}{||x||^2}\n$$\n\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\n\n**Alternative Forms of the Projection Formula**\n\nWe can express the projection vector $\\hat{y}$ in several equivalent ways to highlight different geometric interpretations.\n\n::: {#def-projection-formulae name=\"Forms of Projection\"}\nThe projection of $y$ onto the vector $x$ is given by:\n$$\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n$$\n\nThis second form separates the components into:\n\n$$\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n$$\n\n:::\n\n### Projection Matrix ($P_x$)\n\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector $y$. This allows us to define a **Projection Matrix**.\n\nWe can rewrite the formula for $\\hat{y}$ by factoring out $y$:\n\n$$\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n$$\n\nThis leads to the definition of the projection matrix $P_x$.\n\n::: {#def-projection-matrix name=\"Projection Matrix onto a Single Vector\"}\nThe matrix $P_x$ that projects any vector $y$ onto the line spanned by $x$ is defined as:\n$$\nP_x = \\frac{xx'}{||x||^2}\n$$\n\nUsing this matrix, the projection is simply:\n\n$$\n\\hat{y} = P_x y\n$$\n\nIf $x \\in \\mathbb{R}^n$, then $P_x$ is a $n \\times n$ symmetric matrix.\n\n:::\nLet's apply these concepts to a concrete example.\n\n::: {#exm-projection-r2 name=\"Numerical Projection\"}\nLet $y = (1, 3)'$ and $x = (1, 1)'$. We want to find the projection of $y$ onto $x$.\n\n**Method 1: Using the Vector Formula** First, calculate the inner products:\n\n$$\nx'y = 1(1) + 1(3) = 4\n$$ $$\n||x||^2 = 1^2 + 1^2 = 2\n$$\n\nNow, apply the formula:\n\n$$\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**Method 2: Using the Projection Matrix** Construct the matrix $P_x$:\n\n$$\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n$$\n\nMultiply by $y$:\n\n$$\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n\n:::\n**Example: Projection onto the Ones Vector (**$j_n$)\n\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\n\n::: {#exm-mean-projection name=\"Projection onto the Ones Vector\"}\nLet $y = (y_1, \\dots, y_n)'$ be a data vector. Let $j_n = (1, 1, \\dots, 1)'$ be a vector of all ones.\n\nThe projection of $y$ onto $j_n$ is:\n\n$$\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n$$\n\nCalculating the components:\n\n$$\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n$$ $$\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n$$\n\nSubstituting these back:\n\n$$\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n$$\n\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n\n:::\n\n### Pythagorean Theorem\n\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\n\n::: {#thm-pythagorean name=\"Pythagorean Theorem\"}\nIf two vectors $x$ and $y$ are orthogonal (i.e., $x \\perp y$ or $x'y = 0$), then the squared length of their sum is equal to the sum of their squared lengths:\n$$\n||x + y||^2 = ||x||^2 + ||y||^2\n$$\n\n:::\n\n::: proof\nWe expand the squared norm using the inner product:\n$$\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n$$\n\nSince $x \\perp y$, the inner product $x'y = 0$. Thus, the term $2x'y$ vanishes, leaving:\n\n$$\n||x + y||^2 = ||x||^2 + ||y||^2\n$$\n\n:::\nThe proof after defining inner product to represent $\\cos(\\theta)$ is trivival. @fig-pythagoras-proof shows a geometric proof of the fundamental Pythagorean Theorem.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Proof of Pythagorean Theorem using Area Scaling](lec1-vecspace_files/figure-pdf/fig-pythagoras-proof-1.pdf){#fig-pythagoras-proof fig-align='center' style=\"width: 80% !important;\"}\n:::\n:::\n\n\n### Least Square Property\n\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector $y$ and the subspace (or line) onto which it is projected.\n\n::: {#thm-shortest-distance name=\"Least Square Property\"}\nLet $\\hat{y}$ be the projection of $y$ onto the line $L(x)$. For any other vector $y^*$ on the line $L(x)$, the distance from $y$ to $y^*$ is always greater than or equal to the distance from $y$ to $\\hat{y}$.\n$$\n||y - y^*|| \\ge ||y - \\hat{y}||\n$$\n\n:::\n\n::: proof\nSince both $\\hat{y}$ and $y^*$ lie on the line $L(x)$, their difference $(\\hat{y} - y^*)$ also lies on $L(x)$. From the definition of projection, the residual $(y - \\hat{y})$ is orthogonal to the line $L(x)$. Therefore:\n$$\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n$$\n\nWe can write the vector $(y - y^*)$ as:\n\n$$\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n$$\n\nApplying the Pythagorean Theorem:\n\n$$\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n$$\n\nSince $||\\hat{y} - y^*||^2 \\ge 0$, it follows that:\n\n$$\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n$$\n\n:::\n\n## Vector Space\n\nWe now generalize our discussion from lines to broader spaces.\n\n::: {#def-vector-space name=\"Vector Space\"}\nA set $V \\subseteq \\mathbb{R}^n$ is called a **Vector Space** if it is closed under vector addition and scalar multiplication:\n \n1.  **Closed under Addition:** If $x_1 \\in V$ and $x_2 \\in V$, then $x_1 + x_2 \\in V$.\n   2.  **Closed under Scalar Multiplication:** If $x \\in V$, then $cx \\in V$ for any scalar $c \\in \\mathbb{R}$.\n\n:::\nIt follows that the zero vector $0$ must belong to any subspace (by choosing $c=0$).\n\n### Spanned Vector Space\n\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\n::: {#def-spanned-space name=\"Spanned Vector Space\"}\nLet $x_1, \\dots, x_p$ be a set of vectors in $\\mathbb{R}^n$. The space spanned by these vectors, denoted $L(x_1, \\dots, x_p)$, is the set of all possible linear combinations of them:\n$$\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n$$\n\n:::\n\n### Column Space and Row Space\n\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\n::: {#def-column-space name=\"Column Space\"}\nFor a matrix $X = (x_1, \\dots, x_p)$, the **Column Space**, denoted $\\text{Col}(X)$, is the vector space spanned by its columns:\n$$\n\\text{Col}(X) = L(x_1, \\dots, x_p)\n$$\n\n:::\n\n::: {#def-row-space name=\"Row Space\"}\nThe **Row Space**, denoted $\\text{Row}(X)$, is the vector space spanned by the rows of the matrix $X$.\n\n:::\n\n### Linear Independence and Rank\n\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\n::: {#def-linear-independence name=\"Linear Independence\"}\nA set of vectors $x_1, \\dots, x_p$ is said to be **Linearly Independent** if the only solution to the linear combination equation equal to zero is the trivial solution:\n$$\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n$$\n\nIf there exist non-zero $c_i$'s such that sum is zero, the vectors are **Linearly Dependent**.\n\n:::\n\n## Rank of Matrices and Dim of Vector Space\n\n::: {#def-rank name=\"Rank\"}\nThe **Rank** of a matrix $X$, denoted $\\text{Rank}(X)$, is the maximum number of linearly independent columns in $X$. This is equivalent to the dimension of the column space:\n$$\n\\text{Rank}(X) = \\text{Dim}(\\text{Col}(X))\n$$\n\n:::\nThere are several fundamental properties regarding the rank of a matrix.\n\n::: {#exm-row-rank-equal-col-rank name=\"Example of the Equality of Row and Col Rank\"}\nConsider the following $3 \\times 4$ matrix ($n=3, p=4$):\n$$\nX = \\begin{pmatrix} \n1 & 0 & 1 & 0 \\\\ \n0 & 1 & 0 & 1 \\\\ \n1 & 1 & 1 & 1 \n\\end{pmatrix}\n$$ Notice that the third row is the sum of the first two ($r_3 = r_1 + r_2$).\n\n1. Row Rank and Basis $U$ The first two rows are linearly independent. We set the row rank $r=2$ and use these rows as our basis matrix $U$ ($2 \\times 4$):\n\n$$\nU = \\begin{pmatrix} \n1 & 0 & 1 & 0 \\\\ \n0 & 1 & 0 & 1 \n\\end{pmatrix}\n$$\n\n2. Coefficient Matrix\n   $C$ We express every row of $X$ as a linear combination of the rows of $U$:\n \n   -   Row 1: $1 \\cdot u_1 + 0 \\cdot u_2$\n   -   Row 2: $0 \\cdot u_1 + 1 \\cdot u_2$\n   -   Row 3: $1 \\cdot u_1 + 1 \\cdot u_2$\n\nThese coefficients form the matrix $C$ ($3 \\times 2$):\n\n$$\nC = \\begin{pmatrix} \n1 & 0 \\\\ \n0 & 1 \\\\ \n1 & 1 \n\\end{pmatrix}\n$$\n\n1. The Decomposition $X = CU$)\n We verify that $X$ is the product of $C$ and $U$:\n $$\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}}_{X \\ (3 \\times 4)} \n= \n\\underbrace{\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}}_{C \\ (3 \\times 2)} \n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix}}_{U \\ (2 \\times 4)}\n$$\n\n1. Conclusion on Column Rank\n The columns of $X$ are linear combinations of the columns of $C$.\n $$\n\\text{Col}(X) \\subseteq \\text{Col}(C)\n$$ Since $C$ has only 2 columns, the dimension of its column space (and thus $X$'s column space) cannot exceed 2. $$\n\\text{Dim}(\\text{Col}(X)) \\le 2\n$$ This confirms that Row Rank (2) $\\ge$ Column Rank. (By symmetry, they are equal).\n\n:::\n\n::: {#thm-rank-properties name=\"Row Rank equals Column Rank\"}\n\n1.  **Row Rank equals Column Rank:** The dimension of the column space is equal to the dimension of the row space.\n\n$$\n    \\text{Dim}(\\text{Col}(X)) = \\text{Dim}(\\text{Row}(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n    $$\n\n2.  **Bounds:** For an $n \\times p$ matrix $X$:\n\n$$\n    \\text{Rank}(X) \\le \\min(n, p)\n    $$\n\n:::\n\n### Orthogonality to a Subspace\n\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\n::: {#def-orth-subspace name=\"Orthogonality to a Subspace\"}\nA vector $y$ is orthogonal to a subspace $V$ (denoted $y \\perp V$) if $y$ is orthogonal to **every** vector $x$ in $V$.\n$$\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n$$\n\n:::\n\n::: {#def-orthogonal-complement name=\"Orthogonal Complement\"}\nThe set of all vectors that are orthogonal to a subspace $V$ is called the **Orthogonal Complement** of $V$, denoted $V^\\perp$.\n$$\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n$$\n\n:::\n\n### Kernel (Null Space) and Image\n\nFor a matrix transformation defined by $X$, we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\n::: {#def-image-kernel name=\"Image and Kernel\"}\n\n1.  **Image (Column Space):** The set of all possible outputs.\n\n$$\n    \\text{Im}(X) = \\text{Col}(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n    $$\n\n2.  **Kernel (Null Space):** The set of all inputs mapped to the zero vector.\n\n$$\n    \\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n    $$\n\n:::\n\n::: {#thm-kernel-rowspace name=\"Relationship between Kernel and Row Space\"}\nThe kernel of $X$ is the orthogonal complement of the row space of $X$:\n$$\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n$$\n\n:::\n\n::: proof\nLet $x \\in \\mathbb{R}^p$. $x \\in \\text{Ker}(X)$ if and only if $Xx = 0$. If we denote the rows of $X$ as $r_1', \\dots, r_n'$, then the equation $Xx = 0$ is equivalent to the system of equations:\n$$\n\\begin{pmatrix} r_1' \\\\ \\vdots \\\\ r_n' \\end{pmatrix} x = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\iff r_i' x = 0 \\text{ for all } i = 1, \\dots, n\n$$ This means $x$ is orthogonal to every row of $X$. Since the rows span the row space $\\text{Row}(X)$, being orthogonal to every generator $r_i$ implies $x$ is orthogonal to the entire space $\\text{Row}(X)$. Thus, $\\text{Ker}(X) = \\{ x \\mid x \\perp \\text{Row}(X) \\} = [\\text{Row}(X)]^\\perp$.\n\n:::\n\n### Nullity Theorem\n\nThere is a fundamental relationship between the dimensions of these spaces.\n\n::: {#thm-nullity name=\"Rank-Nullity Theorem\"}\nFor an $n \\times p$ matrix $X$:\n$$\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n$$ where $\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))$.\n\n:::\n\n::: proof\nFrom the previous theorem, we established that the kernel is the orthogonal complement of the row space:\n$$\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n$$\n\nSince the row space is a subspace of $\\mathbb{R}^p$, the entire space can be decomposed into the direct sum of the row space and its orthogonal complement:\n\n$$\n\\mathbb{R}^p = \\text{Row}(X) \\oplus [\\text{Row}(X)]^\\perp = \\text{Row}(X) \\oplus \\text{Ker}(X)\n$$\n\nTaking the dimensions of these spaces:\n\n$$\n\\text{Dim}(\\mathbb{R}^p) = \\text{Dim}(\\text{Row}(X)) + \\text{Dim}(\\text{Ker}(X))\n$$\n\nSubstituting the definitions of Rank (dimension of row/column space) and Nullity:\n\n$$\np = \\text{Rank}(X) + \\text{Nullity}(X)\n$$\n\n:::\n**Comparing Ranks via Kernel Containment**\n\nThe Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices $A$ and $B$ (with the same number of columns) by inspecting their null spaces.\n\n::: {#thm-rank-kernel name=\"Kernel Containment and Rank Inequality\"}\nLet $A$ and $B$ be two matrices with $p$ columns. If the kernel of $A$ is contained within the kernel of $B$, then the rank of $A$ is greater than or equal to the rank of $B$.\n$$\n\\text{Ker}(A) \\subseteq \\text{Ker}(B) \\implies \\text{Rank}(A) \\ge \\text{Rank}(B)\n$$\n\n:::\n\n::: proof\nFrom the subspace inclusion $\\text{Ker}(A) \\subseteq \\text{Ker}(B)$, it follows that the dimension of the smaller space cannot exceed the dimension of the larger space:\n$$\n\\text{Nullity}(A) \\le \\text{Nullity}(B)\n$$ Using the Rank-Nullity Theorem ($\\text{Rank} = p - \\text{Nullity}$), we reverse the inequality: $$\np - \\text{Nullity}(A) \\ge p - \\text{Nullity}(B)\n$$ $$\n\\text{Rank}(A) \\ge \\text{Rank}(B)\n$$\n\n:::\n\n### Rank Inequalities\n\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\n::: {#thm-rank-product name=\"Rank of a Matrix Product\"}\nLet $X$ be an $n \\times p$ matrix and $Z$ be a $p \\times k$ matrix. The rank of their product $XZ$ is bounded by the rank of the individual matrices:\n$$\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n$$\n\n:::\n\n::: proof\nThe columns of $XZ$ are linear combinations of the columns of $X$. Thus, the column space of $XZ$ is a subspace of the column space of $X$:\n$$\n\\text{Col}(XZ) \\subseteq \\text{Col}(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n$$ Similarly, the rows of $XZ$ are linear combinations of the rows of $Z$. Thus, the row space of $XZ$ is a subspace of the row space of $Z$: $$\n\\text{Row}(XZ) \\subseteq \\text{Row}(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n$$\n\n:::\n**Rank and Invertible Matrices**\n\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\n::: {#thm-rank-invertible name=\"Rank with Non-Singular Multiplication\"}\nLet $A$ be an $n \\times n$ invertible matrix (i.e., $\\text{Rank}(A) = n$) and $X$ be an $n \\times p$ matrix. Then:\n$$\n\\text{Rank}(AX) = \\text{Rank}(X)\n$$\n\nSimilarly, if $B$ is a $p \\times p$ invertible matrix, then:\n\n$$\n\\text{Rank}(XB) = \\text{Rank}(X)\n$$\n\n:::\n\n::: proof\nFrom the previous theorem, we know $\\text{Rank}(AX) \\le \\text{Rank}(X)$. Since $A$ is invertible, we can write $X = A^{-1}(AX)$. Applying the theorem again:\n$$\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n$$ Thus, $\\text{Rank}(AX) = \\text{Rank}(X)$.\n\n:::\n\n### Rank of $X'X$ and $XX'$\n\nThe matrix $X'X$ (the Gram matrix) appears in the normal equations for least squares ($X'X\\beta = X'y$). Its properties are closely tied to $X$.\n\n::: {#thm-rank-gram name=\"Rank of Gram Matrix\"}\nFor any real matrix $X$, the rank of $X'X$ and $XX'$ is the same as the rank of $X$ itself:\n$$\n\\text{Rank}(X'X) = \\text{Rank}(X)\n$$ $$\n\\text{Rank}(XX') = \\text{Rank}(X)\n$$\n\n:::\n\n::: proof\nWe first show that the null space (kernel) of $X$ is the same as the null space of $X'X$. If $v \\in \\text{Ker}(X)$, then $Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)$. Conversely, if $v \\in \\text{Ker}(X'X)$, then $X'Xv = 0$. Multiply by $v'$:\n$$\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n$$ So $\\text{Ker}(X) = \\text{Ker}(X'X)$. By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n\n:::\n**Column Space of** $XX'$\n\nBeyond just the rank, the column spaces themselves are related.\n\n::: {#thm-colspace-gram name=\"Column Space Equivalence\"}\nThe column space of $XX'$ is identical to the column space of $X$:\n$$\n\\text{Col}(XX') = \\text{Col}(X)\n$$\n\n:::\n\n::: proof\n\n1.  **Forward (**$\\subseteq$): Let $z \\in \\text{Col}(XX')$. Then $z = XX'w$ for some vector $w$. We can rewrite this as $z = X(X'w)$. Since $z$ is a linear combination of columns of $X$ (with coefficients $X'w$), $z \\in \\text{Col}(X)$. Thus, $\\text{Col}(XX') \\subseteq \\text{Col}(X)$.\n\n2.  **Equality via Rank:** From the previous theorem, we know that $\\text{Rank}(XX') = \\text{Rank}(X)$. Since $\\text{Col}(XX')$ is a subspace of $\\text{Col}(X)$ and they have the same finite dimension (Rank), the subspaces must be identical.\n\n:::\n**Implication:** This property ensures that for any $y$, the projection of $y$ onto $\\text{Col}(X)$ lies in the same space as the projection onto $\\text{Col}(XX')$. This is vital for the existence of solutions in generalized least squares.\n\n## Orthogonal Projection onto a Subspace\n\n::: {name=\"Definition of Projection onto a Subspace $V$\"}\nLet $V$ be a subspace of $\\mathbb{R}^n$. For any vector $y \\in \\mathbb{R}^n$, there exists a **unique** vector $\\hat{y} \\in V$ such that the residual is orthogonal to the subspace:\n$$\n(y - \\hat{y}) \\perp V\n$$\n\nEquivalently:\n\n$$\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n$$\n\n:::\n\n### Equivalence to Least Squares\n\nThe geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).\n\n::: {#thm-best-approximation name=\"Best Approximation Theorem (Least Squares Property)\"}\nLet $V$ be a subspace of $\\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$. Let $\\hat{y}$ be the orthogonal projection of $y$ onto $V$. Then $\\hat{y}$ is the closest point in $V$ to $y$. That is, for any vector $v \\in V$ such that $v \\ne \\hat{y}$:\n$$\n\\|y - \\hat{y}\\|^2 < \\|y - v\\|^2\n$$\n\n:::\n\n::: proof\nLet $v$ be any vector in $V$. We can rewrite the difference vector $y - v$ by adding and subtracting the projection $\\hat{y}$:\n$$\ny - v = (y - \\hat{y}) + (\\hat{y} - v)\n$$\n\nObserve the properties of the two terms on the right-hand side:\n\n1.  **Residual:** $(y - \\hat{y})$ is orthogonal to $V$ by definition.\n2.  **Difference in Subspace:** Since both $\\hat{y} \\in V$ and $v \\in V$, their difference $(\\hat{y} - v)$ is also in $V$.\n\nTherefore, the two terms are orthogonal to each other:\n\n$$\n(y - \\hat{y}) \\perp (\\hat{y} - v)\n$$\n\nApplying the Pythagorean Theorem:\n\n$$\n\\|y - v\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - v\\|^2\n$$\n\nSince squared norms are non-negative, and $\\|\\hat{y} - v\\|^2 > 0$ (because $v \\ne \\hat{y}$):\n\n$$\n\\|y - v\\|^2 > \\|y - \\hat{y}\\|^2\n$$ The projection $\\hat{y}$ minimizes the squared error distance (and error distance itself).\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualization of the Best Approximation Theorem](lec1-vecspace_files/figure-pdf/fig-3d-proof-1.pdf){#fig-3d-proof}\n:::\n:::\n\n\n### Uniqueness of Projection\n\nWhile the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.\n\n::: {#thm-projection-uniqueness name=\"Uniqueness of Orthogonal Projection\"}\nFor a given vector $y$ and subspace $V$, the projection vector $\\hat{y}$ satisfying $(y - \\hat{y}) \\perp V$ is unique.\n\n:::\n\n::: proof\nAssume there are two vectors $\\hat{y}_1 \\in V$ and $\\hat{y}_2 \\in V$ that both satisfy the orthogonality condition.\n$$\n(y - \\hat{y}_1) \\perp V \\quad \\text{and} \\quad (y - \\hat{y}_2) \\perp V\n$$ This means that for any $v \\in V$, both inner products are zero: $$\n\\langle y - \\hat{y}_1, v \\rangle = 0\n$$ $$\n\\langle y - \\hat{y}_2, v \\rangle = 0\n$$\n\nSubtracting the second equation from the first:\n\n$$\n\\langle y - \\hat{y}_1, v \\rangle - \\langle y - \\hat{y}_2, v \\rangle = 0\n$$ Using the linearity of the inner product: $$\n\\langle (y - \\hat{y}_1) - (y - \\hat{y}_2), v \\rangle = 0\n$$ $$\n\\langle \\hat{y}_2 - \\hat{y}_1, v \\rangle = 0\n$$\n\nThis equation holds for **all** $v \\in V$. Since $\\hat{y}_1$ and $\\hat{y}_2$ are both in $V$, their difference $d = \\hat{y}_2 - \\hat{y}_1$ must also be in $V$. We can therefore choose $v = d = \\hat{y}_2 - \\hat{y}_1$.\n\n$$\n\\langle \\hat{y}_2 - \\hat{y}_1, \\hat{y}_2 - \\hat{y}_1 \\rangle = 0 \\implies \\|\\hat{y}_2 - \\hat{y}_1\\|^2 = 0\n$$ The only vector with a norm of zero is the zero vector itself. $$\n\\hat{y}_2 - \\hat{y}_1 = 0 \\implies \\hat{y}_1 = \\hat{y}_2\n$$ Thus, the projection is unique.\n\n:::\n\n## Projection via Orthonormal Basis ($Q$)\n\n### Orthonomal Basis\n\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\n::: {#def-basis name=\"Basis\"}\nA set of vectors $\\{x_1, \\dots, x_k\\}$ is a **Basis** for a vector space $V$ if:\n\n1.  The vectors span the space: $V = L(x_1, \\dots, x_k)$.\n2.  The vectors are linearly independent.\n\n:::\nThe number of vectors in a basis is unique and is defined as the **Dimension** of $V$.\n\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\n::: {#def-orthonormal-basis name=\"Orthonormal Basis\"}\nA basis $\\{q_1, \\dots, q_k\\}$ is called an **Orthonormal Basis** if:\n\n1.  **Orthogonal:** Each pair of vectors is perpendicular.\n\n   $$\n    q_i'q_j = 0 \\quad \\text{for } i \\ne j\n    $$\n\n2.  **Normalized:** Each vector has unit length.\n\n   $$\n    ||q_i||^2 = q_i'q_i = 1\n    $$\n\nCombining these, we write $q_i'q_j = \\delta_{ij}$ (Kronecker delta).\n\n:::\nWe now generalize the projection problem. Instead of projecting $y$ onto a single line, we project it onto a subspace $V$ of dimension $k$.\n\nIf we have an orthonormal basis $\\{q_1, \\dots, q_k\\}$ for $V$, the projection $\\hat{y}$ is simply the sum of the projections onto the individual basis vectors.\n\n::: {#def-proj-orthonormal name=\"Projection Defined with Orthonormal Basis\"}\nThe projection of $y$ onto the subspace $V = L(q_1, \\dots, q_k)$ is:\n$$\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n$$\n\nSince the basis vectors are normalized, we do not need to divide by $||q_i||^2$.\n\n:::\n\n::: {#thm-orthonormal-basis-proj name=\"Projection via Orthonormal Basis\"}\nLet $\\{q_1, \\dots, q_k\\}$ be an orthonormal basis for the subspace $V \\subseteq \\mathbb{R}^n$. The vector defined by the sum of individual projections:\n$$\n\\hat{y} = \\sum_{i=1}^k \\langle y, q_i \\rangle q_i\n$$ is indeed the orthogonal projection of $y$ onto $V$. That is, it satisfies $(y - \\hat{y}) \\perp V$.\n\n:::\n\n::: proof\nTo prove this, we must check two conditions:\n\n1.  $\\hat{y} \\in V$: This is immediate because $\\hat{y}$ is a linear combination of the basis vectors $\\{q_1, \\dots, q_k\\}$.\n\n2.  $(y - \\hat{y}) \\perp V$: It suffices to show that the error vector $e = y - \\hat{y}$ is orthogonal to every basis vector $q_j$ (for $j = 1, \\dots, k$).\n\n    Let's calculate the inner product $\\langle y - \\hat{y}, q_j \\rangle$:\n\n    $$\n    \\begin{aligned}\n    \\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle \\hat{y}, q_j \\rangle \\\\\n    &= \\langle y, q_j \\rangle - \\left\\langle \\sum_{i=1}^k \\langle y, q_i \\rangle q_i, q_j \\right\\rangle \\\\\n    &= \\langle y, q_j \\rangle - \\sum_{i=1}^k \\langle y, q_i \\rangle \\underbrace{\\langle q_i, q_j \\rangle}_{\\delta_{ij}}\n    \\end{aligned}\n    $$\n\n    Since the basis is orthonormal, $\\langle q_i, q_j \\rangle$ is 1 if $i=j$ and 0 otherwise. Thus, the summation collapses to a single term where $i=j$:\n\n    $$\n    \\begin{aligned}\n    \\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle y, q_j \\rangle \\cdot 1 \\\\\n    &= 0\n    \\end{aligned}\n    $$\n\n    Since $(y - \\hat{y})$ is orthogonal to every basis vector $q_j$, it is orthogonal to the entire subspace $V$. Thus, $\\hat{y}$ is the unique orthogonal projection.\n\n:::\n\n### Projection Matrix via Orthonomal Basis ($Q$)\n\n**Matrix Form with Orthonormal Basis**\n\nWe can express the summation formula for $\\hat{y}$ compactly using matrix notation.\n\nLet $Q$ be an $n \\times k$ matrix whose columns are the orthonormal basis vectors $q_1, \\dots, q_k$.\n\n$$\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n$$\n\nProperties of $Q$:\n\n-   $Q'Q = I_k$ (Identity matrix of size $k \\times k$).\n-   $QQ'$ is **not** necessarily $I_n$ (unless $k=n$).\n\n::: {#def-proj-matrix-orthonormal name=\"Projection Matrix in Terms of $Q$\"}\nThe projection $\\hat{y}$ can be written as:\n$$\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n$$\n\nThus, the projection matrix $P$ onto the subspace $V$ is:\n\n$$\nP = QQ'\n$$\n\n:::\n**Properties of Projection Matrices**\n\nWe have defined the projection matrix as $P = X(X'X)^{-1}X'$ (or $P=QQ'$ for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\n::: {#thm-projection-properties name=\"Symmeticity and Idempotence\"}\nA square matrix $P$ represents an orthogonal projection onto some subspace if and only if it satisfies:\n\n1.  **Idempotence:** $P^2 = P$ (Applying the projection twice is the same as applying it once).\n2.  **Symmetry:** $P' = P$.\n\n:::\n\n::: proof\nIf $\\hat{y} = Py$ is already in the subspace $\\text{Col}(X)$, then projecting it again should not change it.\n$$\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n$$ Thus, $P^2 = P$.\n\n:::\n\n**Example: ANOVA (Analysis of Variance)**\n\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\n::: {#exm-anova-projection name=\"Finding Projection for One-way ANOVA\"}\nConsider a one-way ANOVA model with $k$ groups:\n$$\ny_{ij} = \\mu_i + \\epsilon_{ij}\n$$ where $i \\in \\{1, \\dots, k\\}$ represents the group and $j \\in \\{1, \\dots, n_i\\}$ represents the observation within the group. Let $N = \\sum_{i=1}^k n_i$ be the total number of observations.\n\n1.  **Matrix Definitions**\n\n    We define the data vector $y$ and the design matrix $X$ as follows:\n\n    -   **Data Vector** ($y$): An $N \\times 1$ vector containing all observations by group:\n\n    $$\n        y = \\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1n_1} \\\\ y_{21} \\\\ \\vdots \\\\ y_{kn_k} \\end{pmatrix}\n        $$\n\n    -   **Design Matrix** ($X$): An $N \\times k$ matrix constructed from $k$ column vectors, $X = (x_1, x_2, \\dots, x_k)$. Each vector $x_g$ is an **indicator** (dummy variable) for group $g$:\n\n    $$\n        x_g = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\quad \\leftarrow \\text{Entries are 1 if observation belongs to group } g\n        $$\n\n2.  **Orthogonality**\n\n    These column vectors $x_1, \\dots, x_k$ are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero:\n\n    $$\n    \\langle x_g, x_h \\rangle = 0 \\quad \\text{for } g \\neq h\n    $$\n    This allows us to find the projection onto the column space of $X$ by simply summing the projections onto each column individually.\n\n3.  **Calculating Individual Projections**\n\n    For a specific group vector $x_g$, the projection is:\n\n    $$\n    \\text{proj}(y|x_g) = \\frac{\\langle y, x_g \\rangle}{\\langle x_g, x_g \\rangle} x_g\n    $$\n\n    We calculate the two scalar terms:\n\n    -   **Denominator** ($\\langle x_g, x_g \\rangle$): The sum of squared elements of $x_g$. Since $x_g$ contains $n_g$ ones and zeros elsewhere:\n\n    $$\n        \\langle x_g, x_g \\rangle = \\sum \\mathbb{1}_{\\{i=g\\}}^2 = n_g\n        $$\n\n    -   **Numerator** ($\\langle y, x_g \\rangle$): The dot product sums only the $y$ values belonging to group $g$:\n\n    $$\n        \\langle y, x_g \\rangle = \\sum_{i,j} y_{ij} \\cdot \\mathbb{1}_{\\{i=g\\}} = \\sum_{j=1}^{n_g} y_{gj} = y_{g.} \\quad (\\text{Group Total})\n        $$\n\n4.  **The Resulting Projection**\n\n    Substituting these back into the formula gives the coefficient for the vector $x_g$:\n\n    $$\n    \\text{proj}(y|x_g) = \\frac{y_{g.}}{n_g} x_g = \\bar{y}_{g.} x_g\n    $$\n\n    The total projection $\\hat{y}$ is the sum over all groups:\n\n    $$\n    \\hat{y} = \\sum_{g=1}^k \\bar{y}_{g.} x_g\n    $$\n    This confirms that the fitted value for any specific observation $y_{ij}$ is simply its group mean $\\bar{y}_{i.}$.\n:::\n\n### Gram-Schmidt Process\n\nTo use the simplified formula $P = QQ'$, we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\n::: algorithm\n**Gram-Schmidt Process** Given linearly independent vectors $x_1, \\dots, x_p$:\n\n1.  **Step 1:** Normalize the first vector.\n\n$$\n    q_1 = \\frac{x_1}{||x_1||}\n    $$\n\n2.  **Step 2:** Project $x_2$ onto $q_1$ and subtract it to find the orthogonal component.\n\n$$\n    v_2 = x_2 - (x_2'q_1)q_1\n    $$ Then normalize: $$\n    q_2 = \\frac{v_2}{||v_2||}\n    $$\n\n3.  **Step k:** Subtract the projections onto all previous $q$ vectors.\n\n$$\n    v_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n    $$ $$\n    q_k = \\frac{v_k}{||v_k||}\n    $$\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gram-Schmidt Process: Projecting $x_2$ onto $x_1$](lec1-vecspace_files/figure-pdf/fig-gram-schmidt-python-3.pdf){#fig-gram-schmidt-python}\n:::\n:::\n\nThis process leads to the **QR Decomposition** of a matrix: $X = QR$, where $Q$ is orthogonal and $R$ is upper triangular.\n\n## Hat Matrix (Projection Matrix via $X$)\n\n### Norm Equations\n\nLet $X = (x_1, \\dots, x_p)$ be an $n \\times p$ matrix, where each column $x_j$ is a predictor vector.\n\nWe want to project the target vector $y$ onto the column space $\\text{Col}(X)$. This is equivalent to finding a coefficient vector $\\beta \\in \\mathbb{R}^p$ such that the error vector (residual) is orthogonal to the entire subspace $\\text{Col}(X)$.\n\n$$\ny - X\\beta \\perp \\text{Col}(X)\n$$\n\nSince the columns of $X$ span the subspace, the residual must be orthogonal to **every** column vector $x_j$ individually:\n\n$$\ny - X\\beta \\perp x_j \\quad \\text{for } j = 1, \\dots, p\n$$\n\nWriting this geometric condition as an algebraic dot product (where $x_j'$ denotes the transpose):\n\n$$\nx_j'(y - X\\beta) = 0 \\quad \\text{for each } j\n$$\n\nWe can stack these $p$ separate linear equations into a single matrix equation. Since the rows of $X'$ are the columns of $X$, this becomes:\n\n$$\n\\begin{pmatrix} x_1' \\\\ \\vdots \\\\ x_p' \\end{pmatrix} (y - X\\beta) = \\mathbf{0}\n\\implies X'(y - X\\beta) = 0\n$$\n\nFinally, we distribute the matrix transpose and rearrange terms to solve for $\\beta$:\n\n$$\n\\begin{aligned}\nX'y - X'X\\beta &= 0 \\\\\nX'X\\beta &= X'y\n\\end{aligned}\n$$\n\nThis system is known as the **Normal Equations**.\n\n::: {#thm-least-squares-estimator name=\"Least Squares Estimator\"}\nIf $X'X$ is invertible (i.e., $X$ has full column rank), the unique solution for $\\beta$ is:\n$$\n\\hat{\\beta} = (X'X)^{-1}X'y\n$$\n\n:::\n\n### Hat Matrix\n\nSubstituting the estimator $\\hat{\\beta}$ back into the equation for $\\hat{y}$ gives us the projection matrix.\n\n::: {#def-projection-matrix-general name=\"Hat Matrix\"}\nThe projection of $y$ onto $\\text{Col}(X)$ is given by:\n$$\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n$$\n\nThus, the hat matrix $H$ is defined as:\n\n$$\nH = X(X'X)^{-1}X'\n$$\n\n:::\n\n### Equivalence of Hat Matrix and $QQ'$\n\nIf we use the QR decomposition such that $X = QR$, where the columns of $Q$ form an orthonormal basis for $\\text{Col}(X)$, the formula simplifies significantly.\n\nRecall that for orthonormal columns, $Q'Q = I$. Substituting $X=QR$ into the general formula:\n\n$$\n\\begin{aligned}\nH &= QR((QR)'(QR))^{-1}(QR)' \\\\\n  &= QR(R'Q'QR)^{-1}R'Q' \\\\\n  &= QR(R' \\underbrace{Q'Q}_{I} R)^{-1}R'Q' \\\\\n  &= QR(R'R)^{-1}R'Q' \\\\\n  &= QR R^{-1} (R')^{-1} R' Q' \\\\\n  &= Q \\underbrace{R R^{-1}}_{I} \\underbrace{(R')^{-1} R'}_{I} Q' \\\\\n  &= Q Q'\n\\end{aligned}\n$$\n\nThis confirms that $H = QQ'$ is consistent with the general formula $H = X(X'X)^{-1}X'$.\n\n### Properties of Hat Matrix\n\nWe revisit the properties of projection matrices in this general context.\n\n::: {#thm-projection-properties-revisited name=\"Properties of Hat Matrix\"}\nThe matrix $H = X(X'X)^{-1}X'$ satisfies:\n\n1.  **Symmetric:** $H' = H$\n2.  **Idempotent:** $H^2 = H$\n3.  **Trace:** The trace of a projection matrix equals the dimension of the subspace it projects onto.\n$$\n    \\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n    $$\n\n:::\n\n## Projection Defined with Orthogonal Projection Matrix\n\nProjection don't have to be defined with a subspace or a matrix $X$ as we discussed before. Projection matrix is a self-contained definition of the subspace it projects onto.\n\n### Orthogonal Projection Matrix\n\n::: {#def-proj-matrix name=\"Orthogonal Projection Matrix\"}\nA square matrix $P$ is called an **orthogonal projection matrix** if it satisfies two conditions:\n\n1.  **Symmetry:** $P^\\top = P$\n2.  **Idempotency:** $P^2 = P$\n\n:::\n\n::: {#thm-proj-col name=\"Projection onto Column Space\"}\nLet $P$ be a $p \\times p$ symmetric ($P^\\top = P$) and idempotent ($P^2 = P$) matrix in $\\mathbb{R}^p$. Then $P$ represents the orthogonal projection onto its column space, $\\text{Col}(P)$.\n\nSpecifically, for any vector $y \\in \\mathbb{R}^p$, the vector $\\hat{y} = Py$ satisfies the definition of orthogonal projection:\n\n1.  $\\hat{y} \\in \\text{Col}(P)$\n2.  $y - \\hat{y} \\perp \\text{Col}(P)$\n\n:::\n\n::: proof\nTo prove that $P$ is the orthogonal projector onto $\\text{Col}(P)$, we verify the two conditions for an arbitrary vector $y \\in \\mathbb{R}^p$.\n\n1. Condition: $\\hat{y} \\in \\text{Col}(P)$\n\n   By the definition of matrix-vector multiplication, $\\hat{y} = Py$ is a linear combination of the columns of $P$. Therefore, $\\hat{y}$ is, by definition, an element of $\\text{Col}(P)$.\n\n2. Condition: $y - \\hat{y} \\perp \\text{Col}(P)$\n\n   Let $e = y - \\hat{y} = (I_n - P)y$. To verify that $e$ is orthogonal to $\\text{Col}(P)$, it suffices to show that $e$ is orthogonal to every column of $P$. In matrix notation, this is equivalent to showing $e^\\top P = 0$. We compute this directly:\n\n$$\n\\begin{aligned}\ne^\\top P &= [(I_p - P)y]^\\top P \\\\\n&= y^\\top (I_p - P)^\\top P \\\\\n&= y^\\top (I_p - P) P & (\\text{Symmetry: } P^\\top = P) \\\\\n&= y^\\top (P - P^2) \\\\\n&= y^\\top (P - P) & (\\text{Idempotency: } P^2 = P) \\\\\n&= 0\n\\end{aligned}\n$$\n\nSince $e^\\top P = 0$, the residual $e$ is orthogonal to every column of $P$. Consequently, $e$ is orthogonal to the space spanned by those columns, $\\text{Col}(P)$.\n\n:::\n\n::: {#lem-projection-props name=\"0-1 Projection\"}\nLet $P$ be a $n \\times n$ matrix. $P$ is the orthogonal projection matrix onto $\\text{Col}(P)$ if and only if:\n\n  1) $Pv = v$ for all $v \\in \\text{Col}(P)$.\n  2) $Pv = 0$ for all $v \\perp \\text{Col}(P)$.\n\n:::\n\n::: proof\n\n**Forward Implication ($\\implies$):** Given $P$ is an orthogonal projection ($P^2=P, P^\\top=P$).\n\n(1) **Proof of (1):** Let $v \\in \\text{Col}(P)$. Then $v = Px$ for some $x$.\n   $$\n   Pv = P(Px) = P^2 x = Px = v\n   $$\n(1) **Proof of (2):** Let $v \\perp \\text{Col}(P)$. Then $v$ is orthogonal to every column of $P$, so $v^\\top P = 0$. Since $P$ is symmetric:\n\n   $$\n   Pv = (v^\\top P^\\top)^\\top = (v^\\top P)^\\top = 0^\\top = 0\n   $$\n\n**Reverse Implication ($\\impliedby$):** Given conditions (1) and (2) hold.\n\nWe must show that $P$ is idempotent ($P^2=P$) and symmetric ($P^\\top=P$).\n\n(1) **Proof of Idempotence ($P^2 = P$):**\n   For any vector $x \\in \\mathbb{R}^n$, let $y = Px$. By definition, $y \\in \\text{Col}(P)$.\n   Applying condition (1) to the vector $y$:\n   $$\n   Py = y \\implies P(Px) = Px \\implies P^2 x = Px\n   $$\n   Since this holds for all $x$, $P^2 = P$.\n\n(1) **Proof of Symmetry ($P^\\top = P$):**\n   We decompose any two vectors $x, y \\in \\mathbb{R}^n$ into components inside and orthogonal to $\\text{Col}(P)$.\n   Let $x = x_1 + x_2$ and $y = y_1 + y_2$, where $x_1, y_1 \\in \\text{Col}(P)$ and $x_2, y_2 \\perp \\text{Col}(P)$.\n   Using conditions (1) and (2):\n   $$\n   Px = P(x_1 + x_2) = Px_1 + Px_2 \\stackrel{(1),(2)}{=} x_1 + 0 = x_1\n   $$   \n   $$\n   Py = P(y_1 + y_2) = Py_1 + Py_2 \\stackrel{(1),(2)}{=} y_1 + 0 = y_1\n   $$\n   Now we compare the inner products $\\langle Px, y \\rangle$ and $\\langle x, Py \\rangle$:\n   $$\n   \\langle Px, y \\rangle = \\langle x_1, y_1 + y_2 \\rangle = \\langle x_1, y_1 \\rangle + \\underbrace{\\langle x_1, y_2 \\rangle}_{0} = \\langle x_1, y_1 \\rangle\n   $$\n   \n   $$\n   \\langle x, Py \\rangle = \\langle x_1 + x_2, y_1 \\rangle = \\langle x_1, y_1 \\rangle + \\underbrace{\\langle x_2, y_1 \\rangle}_{0} = \\langle x_1, y_1 \\rangle\n   $$\n   Since $\\langle Px, y \\rangle = \\langle x, Py \\rangle$ implies $x^\\top P^\\top y = x^\\top P y$ for all $x, y$, we conclude $P^\\top = P$.\n\nSince $P$ is symmetric and idempotent, it is the orthogonal projection matrix.\n\n:::\n\n### Projection onto Complement Space\n\n::: {#thm-complement-proj name=\"Projection onto Orthogonal Complement\"}\nLet $P$ be a $n \\times n$ orthogonal projection matrix operating in the space $\\mathbb{R}^n$. The matrix $M$ defined as:\n$$\nM = I_p - P\n$$\nis the orthogonal projection matrix onto the orthogonal complement of the column space of $P$, denoted $\\text{Col}(P)^\\perp \\subseteq \\mathbb{R}^n$.\n\n:::\n\n::: proof\n\n(1) **Symmetry and Idempotency**\n   Since $P$ is a projection matrix, $P^\\top = P$ and $P^2 = P$. We verify these properties for $M$:\n   $$\n   M^\\top = (I_p - P)^\\top = I_p - P^\\top = I_p - P = M\n   $$ {#eq-m-sym}\n   $$\n   M^2 = (I_p - P)(I_p - P) = I_p - 2P + P^2 = I_p - 2P + P = I_p - P = M\n   $$ {#eq-m-idemp}\n   By @eq-m-sym and @eq-m-idemp, $M$ is symmetric and idempotent, so it is an orthogonal projection matrix.\n\n(1) **Identifying the Subspace**\n   We now show that $\\text{Col}(M) = \\text{Col}(P)^\\perp$ by mutual inclusion.\n    (1) **Direction 1**: $\\text{Col}(M) \\subseteq \\text{Col}(P)^\\perp$\n      Let $v \\in \\text{Col}(M)$. Then $v = Mx$ for some vector $x$. Multiplying by $P$:\n      $$\n      Pv = P(I_p - P)x = (P - P^2)x = 0\n      $$\n      Since $P$ is symmetric ($P = P'$), taking the transpose of $Pv=0$ gives $v'P = 0$. This means $v$ is orthogonal to every column of $P$. Therefore, $v \\in \\text{Col}(P)^\\perp$.\n\n    (1) **Direction 2**: $\\text{Col}(P)^\\perp \\subseteq \\text{Col}(M)$\n      Let $v \\in \\text{Col}(P)^\\perp$. By definition, $v$ is orthogonal to the columns of $P$, so $v'P = 0$. Taking the transpose and using symmetry ($P' = P$), we get $Pv = 0$.      \n      Now applying $M$ to $v$:\n      $$\n      Mv = (I_p - P)v = v - Pv = v\n      $$\n      Since $Mv = v$, $v$ lies in the column space of $M$. Therefore, $v \\in \\text{Col}(M)$.\n\nSince both inclusions hold, $\\text{Col}(M) = \\text{Col}(P)^\\perp$.\n\n:::\n\n### Projections onto Nested Subspaces\n\n#### Iterative Projections\n\n::: {#thm-nested-projections name=\"Iterative Projections\"}\nLet $P_0$ and $P_1$ be $n \\times n$ orthogonal projection matrices such that $\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)$. Then:\n\n(1) $P_1 P_0 = P_0$\n(1) $P_0 P_1 = P_0$\n\n:::\n\n::: proof\n**Method 1**: \n\nProof of $P_1 P_0 = P_0$:\n\nLet $y \\in \\mathbb{R}^n$ be an arbitrary vector. By definition, the vector $v = P_0 y$ lies in $\\text{Col}(P_0)$. Given $\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)$, it follows that $v \\in \\text{Col}(P_1)$.\n\nUsing **Lemma @lem-projection-props**, since $v \\in \\text{Col}(P_1)$, $P_1$ acts as the identity on $v$, so $P_1 v = v$. Substituting $v = P_0 y$:\n\n$$\nP_1 (P_0 y) = P_0 y\n$$\n\nSince $P_1 P_0 y = P_0 y$ holds for all $y \\in \\mathbb{R}^n$, we conclude $P_1 P_0 = P_0$.\n\nProof of $P_0 P_1 = P_0$:\n\nTaking the transpose of the result from part 1 and applying the symmetry property ($P' = P$):\n\n$$\n(P_1 P_0)' = P_0' \\implies P_0' P_1' = P_0' \\implies P_0 P_1 = P_0\n$$\n\n**Method 2**:\n\nTo prove $P_0 P_1 = P_0$, for any $y \\in \\mathbb{R}^n$, let $\\hat{y}_1 = P_1 y$, $\\hat{y}_0 = P_0 y$, $e_1 = y - \\hat{y}_1$, and $e_0 = y - \\hat{y}_0$. Note that both $e_0$ and $e_1$ are orthogonal to $\\text{Col}(P_0)$ (since $\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)$).\n\nWe have:\n\n$$\nP_0(P_1 - P_0)y = P_0(\\hat{y}_1 - \\hat{y}_0) = P_0 (e_0 - e_1) = 0\n$$\n\nThis implies $P_0 P_1 - P_0 = 0$, so $P_0 P_1 = P_0$.\n:::\n\n#### Difference of Projections\n\n::: {#thm-diff-projection name=\"Difference Projection\"}\nThe matrix $P_{\\Delta} = P_1 - P_0$ is an orthogonal projection matrix onto the subspace $\\text{Col}(P_1) \\cap \\text{Col}(P_0)^\\perp$. This subspace represents the \"extra\" information in the full model that is orthogonal to the reduced model. Additionally, the following column space relationship holds:\n$$\n\\text{Col}(P_1 - P_0) = \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)\n$$\n\n\n:::\n\n::: proof\n\n1. **Symmetry:** Since $P_1$ and $P_0$ are symmetric:\n\n$$\n(P_1 - P_0)' = P_1' - P_0' = P_1 - P_0\n$$\n\n2. **Idempotency:**\n\n$$\n\\begin{aligned}\n(P_1 - P_0)^2 &= (P_1 - P_0)(P_1 - P_0) \\\\\n&= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2\n\\end{aligned}\n$$\nUsing the projection property ($P^2=P$) and the nested property ($P_1 P_0 = P_0$ and $P_0 P_1 = P_0$):\n$$\n= P_1 - P_0 - P_0 + P_0 = P_1 - P_0\n$$\n\n3. **Orthogonality to $P_0$:**\n\n$$\n(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0\n$$\n\n4. **Column Space Identity:** We show $\\text{Col}(P_1 - P_0) = \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$ via double containment.\n\n    **$(\\subseteq)$ Forward Containment:** Let $y \\in \\text{Col}(P_1 - P_0)$. By definition, $y = (P_1 - P_0)x$ for some $x$.\n\n    * Check $y \\in \\text{Col}(P_1)$: $P_1 y = P_1(P_1 - P_0)x = (P_1 - P_0)x = y$. Thus $y \\in \\text{Col}(P_1)$.\n    * Check $y \\in \\text{Col}(P_0)^\\perp$: $P_0 y = P_0(P_1 - P_0)x = (P_0 - P_0)x = 0$. Thus $y \\in \\text{Col}(P_0)^\\perp$.\n    * Therefore, $\\text{Col}(P_1 - P_0) \\subseteq \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$.\n\n    **$(\\supseteq)$ Reverse Containment**: Let $y \\in \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$.\n\n    * Since $y \\in \\text{Col}(P_1)$, $P_1 y = y$.\n    * Since $y \\in \\text{Col}(P_0)^\\perp$, $P_0 y = 0$.\n    * Observe $(P_1 - P_0)y = P_1 y - P_0 y = y - 0 = y$.\n    * This implies $y$ is in the range of $(P_1 - P_0)$. Therefore, $\\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1) \\subseteq \\text{Col}(P_1 - P_0)$.\n\n:::\n\n::: {.callout-important}\nThis is important as we can use $P_2-P_1$ to construct the projection matrix and the space that it projects onto.\n\n:::\n**Hat Matrix of Incremental Space**\n\n::: {#thm-hat-matrix-incremental}\n\n#### Hat Matrix of Incremental Space\nLet $X_1$ be a design matrix of dimension $n \\times k_1$ and $X_2$ be a design matrix of dimension $n \\times k_2$, such that the combined matrix $X = [X_1, X_2]$ has full column rank. Let $V_1 = \\text{Col}(X_1)$ and $V_2 = \\text{Col}([X_1, X_2])$. Let $P_1$ and $P_2$ be the orthogonal projection matrices onto $V_1$ and $V_2$, respectively.\n\nDefine the matrix of residuals $\\tilde{X}_2$ as:\n\n$$\n\\tilde{X}_2 = (I - P_1) X_2\n$$\n\nLet $W = \\text{Col}(\\tilde{X}_2)$. Let $P_W$ be the $n \\times n$ projection matrix onto $W$, which is the hat matrix constructed from $\\tilde{X}_2$:\n\n$$\nP_W = \\tilde{X}_2 (\\tilde{X}_2^T \\tilde{X}_2)^{-1} \\tilde{X}_2^T\n$$\n\n(a) Let $X^* = [X_1, \\tilde{X}_2]$. Prove that the column space of the original design matrix $X$ is identical to the column space of the modified design matrix $X^*$:\n\n$$\n\\text{Col}([X_1, X_2]) = \\text{Col}([X_1, \\tilde{X}_2])\n$$\n\n(b) Using the result from Part (a) and the definition of the Hat Matrix, prove that:\n\n$$\nP_W = P_2 - P_1\n$$\n\n:::\n\n::: proof\nAssignment question.\n\n:::\n\n### Projection onto Three Multually Orthogonal Subspaces\n\n::: {#thm-nested-decomposition name=\"Orthogonal Decomposition\"}\n\nLet $M_0 \\subset M_1$ be two nested linear models associated with orthogonal projection matrices $P_0$ and $P_1$, such that $\\text{Col}(P_0) \\subset \\text{Col}(P_1)$.\n\nFor any observation vector $y$, we have the decomposition:\n\n$$\ny = \\underbrace{P_0 y}_{\\hat{y}_0} + \\underbrace{(P_1 - P_0) y}_{\\hat{y}_1 - \\hat{y}_0} + \\underbrace{(I - P_1) y}_{y - \\hat{y}_1}\n$$\n\n**Geometric Interpretation:**\n\n1.  $\\hat{y}_0 \\in \\text{Col}(P_0)$: The fit of the reduced model.\n2.  $(\\hat{y}_1 - \\hat{y}_0) \\in \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$: The additional fit provided by $M_1$ over $M_0$.\n3.  $(y - \\hat{y}_1) \\in \\text{Col}(P_1)^\\perp$: The projection of $y$ onto the **orthogonal complement** of $\\text{Col}(P_1)$.\n\nThe three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm:\n\n$$\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n$$\n\n:::\n\n::: {#thm-nested-hatmatrix name=\"Orthogonal Decomposition\"}\n\nLet $M_0 \\subset M_1$ be two nested linear models associated with orthogonal projection matrices $P_0$ and $P_1$, such that $\\text{Col}(P_0) \\subset \\text{Col}(P_1)$.\n\nFor any observation vector $y$, we have the decomposition:\n\n$$\ny = \\underbrace{P_0 y}_{\\hat{y}_0} + \\underbrace{(P_1 - P_0) y}_{\\hat{y}_1 - \\hat{y}_0} + \\underbrace{(I - P_1) y}_{y - \\hat{y}_1}\n$$\n\n**Geometric Interpretation:**\n\n1.  $\\hat{y}_0 \\in \\text{Col}(P_0)$: The fit of the reduced model.\n2.  $(\\hat{y}_1 - \\hat{y}_0) \\in \\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$: The additional fit provided by $M_1$ over $M_0$.\n3.  $(y - \\hat{y}_1) \\in \\text{Col}(P_1)^\\perp$: The projection of $y$ onto the **orthogonal complement** of $\\text{Col}(P_1)$.\n\nThe three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm:\n\n$$\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n$$\n\n:::\n\n::: proof\n\n1. **Definition of Vectors and Nested Spaces**\n\n   Let $I$ be the identity matrix, which is the orthogonal projection onto the entire space $\\mathbb{R}^n$. We effectively have a three-level nested sequence of subspaces:\n\n   $$\n   \\text{Col}(P_0) \\subset \\text{Col}(P_1) \\subset \\mathbb{R}^n\n   $$\n   We define the components of the decomposition using successive difference projections:\n\n   * $v_0 = P_0 y$\n   * $v_1 = (P_1 - P_0) y$\n   * $v_2 = (I - P_1) y$\n\n   Summing these gives the identity: $y = v_0 + v_1 + v_2$.\n\n2. **Sequential Orthogonality via @thm-diff-projection**\n\n   We apply the Difference Projection Theorem (@thm-diff-projection) to each successive pair of nested spaces to establish orthogonality.\n\n   * **Step 1: Verify $v_1 \\perp v_0$**\n       * Consider the nested pair $P_0$ and $P_1$.\n       * By @thm-diff-projection, the matrix $(P_1 - P_0)$ projects onto $\\text{Col}(P_0)^\\perp \\cap \\text{Col}(P_1)$.\n       * Since $v_0 \\in \\text{Col}(P_0)$ and $v_1$ lies in the orthogonal complement $\\text{Col}(P_0)^\\perp$, we have **$v_1 \\perp v_0$**.\n\n   * **Step 2: Verify $v_2 \\perp \\{v_0, v_1\\}$**\n       * Consider the nested pair $P_1$ and $I$ (where $I$ projects onto $\\mathbb{R}^n$).\n       * By @thm-diff-projection, the matrix $(I - P_1)$ projects onto $\\text{Col}(P_1)^\\perp \\cap \\mathbb{R}^n = \\text{Col}(P_1)^\\perp$.\n       * Since both $v_0$ and $v_1$ reside within $\\text{Col}(P_1)$ (as shown in Step 1), and $v_2$ lies in the orthogonal complement $\\text{Col}(P_1)^\\perp$, it follows that $v_2$ is orthogonal to the entire subspace $\\text{Col}(P_1)$.\n       * Therefore, **$v_2 \\perp v_0$** and **$v_2 \\perp v_1$**.\n\n3. **Conclusion**\n\n   Since $\\{v_0, v_1, v_2\\}$ are mutually orthogonal, the Pythagorean theorem applies:\n\n   $$\n   \\|y\\|^2 = \\|v_0\\|^2 + \\|v_1\\|^2 + \\|v_2\\|^2\n   $$\n   Substituting the original definitions back in:\n   $$\n   \\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n   $$\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of Projections onto Nested Subspaces](lec1-vecspace_files/figure-pdf/fig-anova-decomp-5.pdf){#fig-anova-decomp fig-pos='H'}\n:::\n:::\n\n\n::: {#exm-anova-ss name=\"ANOVA Sum Squares\"}\n\nWe apply the **Nested Model Theorem** ($M_0 \\subset M_1$) to the One-way ANOVA setting.\n\n1. Notation and Definitions\n\n   Consider a dataset with $k$ groups. Let $i = 1, \\dots, k$ index the    groups, and $j = 1, \\dots, n_i$ index the observations within group    $i$.\n   \n   -   $N$: Total number of observations, $N = \\sum_{i=1}^k n_i$.\n   \n   -   $y_{ij}$: The $j$-th observation in the $i$-th group.\n   \n   -   $\\bar{y}_{i.}$: The sample mean of group $i$.\n   \n   $$ \\bar{y}_{i.} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij} $$\n   \n   -   $\\bar{y}_{..}$: The grand mean of all observations.\n   \n   $$ \\bar{y}_{..} = \\frac{1}{N} \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij} $$\n   \n2. The Data and Projection Vectors\n\n    | Observation ($y$) | Null Projection ($\\hat{y}_0$) | Full Projection ($\\hat{y}_1$) |\n    |:----------------------:|:----------------------:|:----------------------:|\n    | $\\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1 n_1} \\\\ \\hline \\vdots \\\\ \\hline y_{k1} \\\\ \\vdots \\\\ y_{k n_k} \\end{pmatrix}$ | $\\begin{pmatrix} \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\end{pmatrix}$ | $\\begin{pmatrix} \\bar{y}_{1.} \\\\ \\vdots \\\\ \\bar{y}_{1.} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{k.} \\\\ \\vdots \\\\ \\bar{y}_{k.} \\end{pmatrix}$ |\n\n    : ANOVA Vectors: Data, Null Model, and Full Model {#tbl-anova-vectors}\n\n    3. Decomposition and Sum of Squares\n\n    | Component | Notation | Definition | Vector Elements | Squared Norm (Sum of Squares) |\n    |:-------------|:------------:|:------------:|:-------------|:----------------|\n    | **Null Proj.** | $\\hat{y}_0$ | $P_0 y$ | Grand Mean ($\\bar{y}_{..}$) | $\\|\\hat{y}_0\\|^2 = N \\bar{y}_{..}^2$ |\n    | **Full Proj.** | $\\hat{y}_1$ | $P_1 y$ | Group Means ($\\bar{y}_{i.}$) | $\\|\\hat{y}_1\\|^2 = \\sum_{i=1}^k n_i \\bar{y}_{i.}^2$ |\n\n4. Geometric Justification of Shortcut Formulas\n\n    **A. Total Sum of Squares (SST)** Since $\\hat{y}_0 \\perp (y - \\hat{y}_0)$, we have $\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|y - \\hat{y}_0\\|^2$:\n\n    $$ \\text{SST} = \\|y - \\hat{y}_0\\|^2 = \\|y\\|^2 - \\|\\hat{y}_0\\|^2 $$ $$ \\text{SST} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - N\\bar{y}_{..}^2 $$\n\n    **B. Between Group Sum of Squares (SSB)** Since $\\hat{y}_0 \\perp (\\hat{y}_1 - \\hat{y}_0)$, we have $\\|\\hat{y}_1\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2$:\n\n    $$ \\text{SSB} = \\|\\hat{y}_1 - \\hat{y}_0\\|^2 = \\|\\hat{y}_1\\|^2 - \\|\\hat{y}_0\\|^2 $$ $$ \\text{SSB} = \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2 $$\n\n    **C. Within Group Sum of Squares (SSW)** Since $\\hat{y}_1 \\perp (y - \\hat{y}_1)$, we have $\\|y\\|^2 = \\|\\hat{y}_1\\|^2 + \\|y - \\hat{y}_1\\|^2$:\n\n    $$ \\text{SSW} = \\|y - \\hat{y}_1\\|^2 = \\|y\\|^2 - \\|\\hat{y}_1\\|^2 $$ $$ \\text{SSW} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 $$\n\n    **Conclusion:**\n\n    $$ \\underbrace{\\|y\\|^2 - N\\bar{y}_{..}^2}_{\\text{SST}} = \\underbrace{(\\sum n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2)}_{\\text{SSB}} + \\underbrace{(\\sum \\sum y_{ij}^2 - \\sum n_i\\bar{y}_{i.}^2)}_{\\text{SSW}} $$ \n\n5. Visualizing ANOVA Components in Data Space\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualization of Group Means vs. Grand Mean](lec1-vecspace_files/figure-pdf/fig-anova-data-space-colored-7.pdf){#fig-anova-data-space-colored}\n:::\n:::\n\n\n\n\n## Projections onto More than Three Orthogonal Subspaces\n\nFinally, we consider the case where the entire space $\\mathbb{R}^n$ is decomposed into mutually orthogonal subspaces.\n\n::: {#thm-orth-decomposition name=\"General Orthogonal Projections\"}\nIf $\\mathbb{R}^n$ is the direct sum of orthogonal subspaces $V_1, V_2, \\dots, V_k$:\n$$\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n$$ where $V_i \\perp V_j$ for all $i \\ne j$.\n\nThen any vector $y$ can be uniquely written as:\n\n$$\ny = \\hat{y}_1 + \\hat{y}_2 + \\dots + \\hat{y}_k\n$$ where $\\hat{y}_i \\in V_i$.\n\nFurthermore, each component $\\hat{y}_i$ is simply the projection of $y$ onto the subspace $V_i$:\n\n$$\n\\hat{y}_i = P_i y\n$$\n\n:::\n\n::: proof\n\n1. Existence: Since $\\mathbb{R}^n$ is the direct sum of $V_1, \\dots, V_k$, by definition, any vector $y \\in \\mathbb{R}^n$ can be written as a sum $y = v_1 + \\dots + v_k$ where $v_i \\in V_i$.\n\n2. Uniqueness: Suppose there are two such representations: $y = \\sum v_i = \\sum w_i$, with $v_i, w_i \\in V_i$. Then $\\sum (v_i - w_i) = 0$. Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, $v_i - w_i = 0 \\implies v_i = w_i$. The representation is unique. Let $\\hat{y}_i = v_i$.\n\n3. Projection Property: We claim that the $i$-th component $\\hat{y}_i$ is the orthogonal projection of $y$ onto $V_i$. We must show that the residual $(y - \\hat{y}_i)$ is orthogonal to $V_i$.\n\n    $$\n    y - \\hat{y}_i = \\sum_{j \\ne i} \\hat{y}_j\n    $$ Let $z$ be any vector in $V_i$. We calculate the inner product: $$\n    \\langle y - \\hat{y}_i, z \\rangle = \\left\\langle \\sum_{j \\ne i} \\hat{y}_j, z \\right\\rangle = \\sum_{j \\ne i} \\langle \\hat{y}_j, z \\rangle\n    $$ Since $\\hat{y}_j \\in V_j$ and $z \\in V_i$, and the subspaces are mutually orthogonal ($V_j \\perp V_i$ for $j \\ne i$), every term in the sum is zero. Therefore, $(y - \\hat{y}_i) \\perp V_i$. By the definition of orthogonal projection, $\\hat{y}_i = P_i y$.\n\n:::\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices:\n$$\nI_n = P_1 + P_2 + \\dots + P_k\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Orthogonal decomposition of vector y into subspaces](lec1-vecspace_files/figure-pdf/fig-orthogonal-decomp-rotated-9.pdf){#fig-orthogonal-decomp-rotated}\n:::\n:::\n\n\n\n\n::: {#thm-complete-decomposition name=\"Complete Orthogonal Decomposition of $\\\\mathbb{R}^n$\"}\nLet $P_0, P_1, \\dots, P_k$ be a sequence of orthogonal projection matrices with nested column spaces:\n$$\n\\text{Col}(P_0) \\subseteq \\text{Col}(P_1) \\subseteq \\dots \\subseteq \\text{Col}(P_k)\n$$\n\nDefine the sequence of difference matrices $\\Delta P_i$ and their column spaces $V_i$ as follows:\n\n\\begin{align*}\n\\Delta P_0 &= P_0, & V_0 &= \\text{Col}(\\Delta P_0) \\\\\n\\Delta P_i &= P_i - P_{i-1} \\quad (1 \\le i \\le k), & V_i &= \\text{Col}(\\Delta P_i) \\\\\n\\Delta P_{k+1} &= I - P_k, & V_{k+1} &= \\text{Col}(\\Delta P_{k+1})\n\\end{align*}\n\n**Conclusion:**\n\n1.  **Projection Property:** Each $\\Delta P_i$ is the orthogonal projection matrix onto $V_i$ for $i = 0, \\dots, k+1$.\n\n2.  **Mutual Orthogonality:** The collection $\\{\\Delta P_i\\}$ are mutually orthogonal operators:\n\n$$ \\Delta P_i \\Delta P_j = 0 \\quad \\text{for all } i \\ne j $$\n\n3.  **Direct Sum Decomposition:** The vector space $\\mathbb{R}^n$ is the direct sum of these orthogonal subspaces:\n\n$$ \\mathbb{R}^n = V_0 \\oplus V_1 \\oplus \\dots \\oplus V_{k+1} $$\n\n:::\n\n::: proof\n\n1. Proof that $\\Delta P_i$ is the Projection onto $V_i$ \n\n   We must show each $\\Delta P_i$ is symmetric and idempotent.\n   \n   -   For $\\Delta P_0 = P_0$: True by definition.\n   -   For $\\Delta P_i$ ($1 \\le i \\le k$):\n       -   **Symmetry:** Difference of symmetric matrices ($P_i, P_{i-1}   $) is symmetric.\n       -   **Idempotency:** $(\\Delta P_i)^2 = (P_i - P_{i-1})^2 = P_i^2    - P_i P_{i-1} - P_{i-1} P_i + P_{i-1}^2$. Using nested    properties ($P_i P_{i-1} = P_{i-1}$), this simplifies to $P_i -    P_{i-1} = \\Delta P_i$.\n   -   For $\\Delta P_{k+1} = I - P_k$:\n       -   **Symmetry:** $(I - P_k)' = I - P_k$.\n       -   **Idempotency:** $(I - P_k)^2 = I - 2P_k + P_k^2 = I - P_k$.\n   \n2. Proof of Mutual Orthogonality \n\n   We show $\\Delta P_j \\Delta P_i = 0$ for $i < j$.\n   \n   -   **Case 1: Both indices** $\\le k$ (i.e., $1 \\le i < j \\le k$):\n   \n   $$    (P_j - P_{j-1})(P_i - P_{i-1}) = P_j P_i - P_j P_{i-1} - P_{j-1} P_i    + P_{j-1} P_{i-1} $$ Since $\\text{Col}(P_i) \\subseteq \\text{Col}(P_   {j-1})$, all terms reduce to $P_i - P_{i-1} - P_i + P_{i-1} = 0$.\n   \n   -   **Case 2: One index is the residual** ($j = k+1$): We check    $\\Delta P_{k+1} \\Delta P_i = (I - P_k)\\Delta P_i$ for any $i \\le k$.    Since $V_i \\subseteq \\text{Col}(P_k)$, we have $P_k \\Delta P_i =    \\Delta P_i$.\n   \n   $$ (I - P_k)\\Delta P_i = \\Delta P_i - P_k \\Delta P_i =    \\Delta P_i - \\Delta P_i = 0 $$\n   \n3. Proof of Direct Sum \n\n   The sum of the difference matrices forms a telescoping series:\n\n   $$    \\sum_{j=0}^{k+1} \\Delta P_j = P_0 + \\sum_{i=1}^k (P_i - P_{i-1}) +    (I - P_k) $$ $$ = P_k + (I - P_k) = I $$ Since the identity operator    $I$ (which maps $\\mathbb{R}^n$ to itself) is the sum of mutually    orthogonal projection operators, the space $\\mathbb{R}^n$ decomposes    into the direct sum of their respective image subspaces $V_i$.\n   \n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Venn Diagram of Nested Projections with Colored Increments](lec1-vecspace_files/figure-pdf/fig-venn-nested-projection-1.pdf){#fig-venn-nested-projection fig-align='center' style=\"width: 80% !important;\"}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}