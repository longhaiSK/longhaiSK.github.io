{
  "hash": "23642e110c871687e48e0dfeb486d68d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Projection in Vector Space\"\nformat: \n  pdf: default\n  html: default\n---\n\n## Vector and Projection onto a Line\n\n### Vectors and Operations\n\nThe concept of a vector is fundamental to linear algebra and linear models. We begin by formally defining what a vector is in the context of Euclidean space.\n\n::: {#def-vector name=\"Vector\"}\nA **vector** $x$ is defined as a point in $n$-dimensional space ($\\mathbb{R}^n$). It is typically represented as a column vector containing $n$ real-valued components:\n\n$$\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n$$\n:::\n\nVectors are not just static points; they can be combined and manipulated. The two most basic geometric operations are addition and subtraction.\n\n**Vector Arithmetic:** Vectors can be manipulated geometrically:\n\n::: {#def-vector-addition name=\"Vector Addition\"}\nThe sum of two vectors $x$ and $y$ creates a new vector. The operation is performed component-wise, adding corresponding elements from each vector. Geometrically, this follows the \"parallelogram rule\" or the \"head-to-tail\" method, where you place the tail of $y$ at the head of $x$.\n\n$$\nx + y = \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}\n$$\n:::\n\n::: {#def-vector-subtraction name=\"Vector Subtraction\"}\nThe difference $d = y - x$ is the vector that \"closes the triangle\" formed by $x$ and $y$. It represents the displacement vector that connects the tip of $x$ to the tip of $y$, such that $x + d = y$.\n:::\n\n### Scalar Multiplication and Distance\n\nIn addition to combining vectors with each other, we can modify a single vector using a real number, known as a scalar.\n\n::: {#def-scalar-mult name=\"Scalar Multiplication\"}\nMultiplying a vector by a scalar $c$ scales its magnitude (length) without changing its line of direction. If $c$ is positive, the direction remains the same; if $c$ is negative, the direction is reversed.\n\n$$\nc x = \\begin{pmatrix} c x_1 \\\\ \\vdots \\\\ c x_n \\end{pmatrix}\n$$\n:::\n\nWe often need to quantify the \"size\" of a vector. This is done using the concept of length, or norm.\n\n::: {#def-euclidean-distance name=\"Euclidean Distance (Length)\"}\nThe length (or norm) of a vector $x = (x_1, \\dots, x_n)^T$ corresponds to the straight-line distance from the origin to the point defined by $x$. It is defined as the square root of the sum of squared components:\n\n$$\n||x||^2 = \\sum_{i=1}^n x_i^2\n$$\n\n$$\n||x|| = \\sqrt{\\sum_{i=1}^n x_i^2}\n$$\n:::\n\n### Angle and Inner Product\n\nTo understand the relationship between two vectors $x$ and $y$ beyond just their lengths, we must look at the angle between them. Consider the triangle formed by the vectors $x$, $y$, and their difference $y-x$. By applying the classic **Law of Cosines** to this triangle, we can relate the geometric angle to the vector lengths.\n\n::: {#thm-law-of-cosines name=\"Law of Cosines\"}\nFor a triangle with sides $a, b, c$ and angle $\\theta$ opposite to side $c$:\n\n$$\nc^2 = a^2 + b^2 - 2ab \\cos \\theta\n$$\n:::\n\nTranslating this geometric theorem into vector notation where the side lengths correspond to the norms of the vectors, we get:\n\n$$\n||y - x||^2 = ||x||^2 + ||y||^2 - 2||x|| \\cdot ||y|| \\cos \\theta\n$$\n\nThis equation provides a critical link between the geometric angle $\\theta$ and the algebraic norms of the vectors.\n\n**Derivation of Inner Product**\n\nWe can express the squared distance term $||y - x||^2$ purely algebraically by expanding the components:\n\n$$\n||y - x||^2 = \\sum_{i=1}^n (x_i - y_i)^2\n$$\n\n$$\n= \\sum_{i=1}^n (x_i^2 + y_i^2 - 2x_i y_i)\n$$\n\n$$\n= ||x||^2 + ||y||^2 - 2 \\sum_{i=1}^n x_i y_i\n$$\n\nBy comparing this expanded form with the result from the Law of Cosines derived previously, we can identify a corresponding interaction term. This term is so important that we give it a special name: the **Inner Product** (or dot product).\n\n::: {#def-inner-product name=\"Inner Product\"}\nThe inner product of two vectors $x$ and $y$ is defined as the sum of the products of their corresponding components:\n\n$$\nx'y = \\sum_{i=1}^n x_i y_i = \\langle x, y \\rangle\n$$\n:::\n\nThus, equating the geometric and algebraic forms yields the fundamental relationship:\n\n$$\nx'y = ||x|| \\cdot ||y|| \\cos \\theta\n$$\n\n### Coordinate (Scalar) Projection\n\nThe inner product allows us to calculate projections, which quantify how much of one vector \"lies along\" another. If we rearrange the cosine formula derived above, we can isolate the term that represents the length of the \"shadow\" cast by vector $y$ onto vector $x$.\n\nThe length of this projection is given by:\n\n$$\n||y|| \\cos \\theta = \\frac{x'y}{||x||}\n$$\n\nThis expression can be interpreted as the inner product of $y$ with the normalized (unit) vector in the direction of $x$:\n\n$$\n\\text{Scalar Projection} = \\left\\langle \\frac{x}{||x||}, y \\right\\rangle\n$$\n\n### Vector Projection Formula\n\nThe scalar projection only gives us a magnitude (a number). To define the projection as a vector in the same space, we need to multiply this scalar magnitude by the direction of the vector we are projecting onto.\n\n::: {#def-vector-projection name=\"Vector Projection\"}\nThe projection of vector $y$ onto vector $x$, denoted $\\hat{y}$, is calculated as:\n\n$$\n\\text{Projection Vector} = (\\text{Length}) \\cdot (\\text{Direction})\n$$\n\n$$\n\\hat{y} = \\left( \\frac{x'y}{||x||} \\right) \\cdot \\frac{x}{||x||}\n$$\n\nThis is often written compactly by combining the denominators:\n\n$$\n\\hat{y} = \\frac{x'y}{||x||^2} x\n$$\n:::\n\n### Perpendicularity (Orthogonality)\n\nA special case of the angle between vectors arises when $\\theta = 90^\\circ$. This geometric concept of perpendicularity is central to the theory of projections and least squares.\n\n::: {#def-perpendicularity name=\"Perpendicularity\"}\nTwo vectors are defined as **perpendicular** (or orthogonal) if the angle between them is $90^\\circ$ ($\\pi/2$).\n\nSince $\\cos(90^\\circ) = 0$, the condition for orthogonality simplifies to the inner product being zero:\n\n$$\nx'y = 0 \\iff x \\perp y\n$$\n:::\n\n::: {#exm-orthogonal-vectors name=\"Orthogonal Vectors\"}\nConsider two vectors in $\\mathbb{R}^2$: $x = (1, 1)'$ and $y = (1, -1)'$.\n\n$$\nx'y = 1(1) + 1(-1) = 1 - 1 = 0\n$$\n\nSince their inner product is zero, these vectors are orthogonal to each other.\n:::\n\n### Projection onto a Line (Subspace)\n\nWe can generalize the concept of projecting onto a single vector to projecting onto the entire line (a 1-dimensional subspace) defined by that vector.\n\n::: {#def-line-space name=\"Line Spanned by a Vector\"}\nThe line space $L(x)$, or the space spanned by a vector $x$, is defined as the set of all scalar multiples of $x$:\n\n$$\nL(x) = \\{ cx \\mid c \\in \\mathbb{R} \\}\n$$\n:::\n\nThe projection of $y$ onto $L(x)$, denoted $\\hat{y}$, is defined by the geometric property that it is the closest point on the line to $y$. This implies that the error vector (or residual) must be perpendicular to the line itself.\n\n::: {#def-projection-line name=\"Projection onto a Line\"}\nA vector $\\hat{y}$ is the projection of $y$ onto the line $L(x)$ if:\n\n1.  $\\hat{y}$ lies on the line $L(x)$ (i.e., $\\hat{y} = cx$ for some scalar $c$).\n\n2.  The residual vector $(y - \\hat{y})$ is perpendicular to the direction vector $x$.\n:::\n\n**Derivation:** To find the value of the scalar $c$, we apply the orthogonality condition:\n\n$$\n(y - \\hat{y}) \\perp x \\implies x'(y - cx) = 0\n$$\n\nExpanding this inner product gives:\n\n$$\nx'y - c(x'x) = 0\n$$\n\nSolving for $c$, we obtain:\n\n$$\nc = \\frac{x'y}{||x||^2}\n$$\n\nThis confirms the formula derived previously using the inner product geometry. It shows that the least squares principle (shortest distance) leads to the same result as the geometric projection.\n\n**Alternative Forms of the Projection Formula**\n\nWe can express the projection vector $\\hat{y}$ in several equivalent ways to highlight different geometric interpretations.\n\n::: {#def-projection-formulae name=\"Forms of Projection\"}\nThe projection of $y$ onto the vector $x$ is given by:\n\n$$\n\\hat{y} = \\frac{x'y}{||x||^2} x = \\left\\langle y, \\frac{x}{||x||} \\right\\rangle \\frac{x}{||x||}\n$$\n\nThis second form separates the components into: $$\n\\text{Projection} = (\\text{Scalar Projection}) \\times (\\text{Unit Direction})\n$$\n:::\n\n### Projection Matrix ($P_x$)\n\nIn linear models, it is often more convenient to view projection as a linear transformation applied to the vector $y$. This allows us to define a **Projection Matrix**.\n\nWe can rewrite the formula for $\\hat{y}$ by factoring out $y$:\n\n$$\n\\hat{y} = \\text{proj}(y|x) = x \\frac{x'y}{||x||^2} = \\frac{xx'}{||x||^2} y\n$$\n\nThis leads to the definition of the projection matrix $P_x$.\n\n::: {#def-projection-matrix name=\"Projection Matrix onto a Single Vector\"}\nThe matrix $P_x$ that projects any vector $y$ onto the line spanned by $x$ is defined as:\n\n$$\nP_x = \\frac{xx'}{||x||^2}\n$$\n\nUsing this matrix, the projection is simply: $$\n\\hat{y} = P_x y\n$$\n\nIf $x \\in \\mathbb{R}^p$, then $P_x$ is a $p \\times p$ symmetric matrix.\n:::\n\nLet's apply these concepts to a concrete example.\n\n::: {#exm-projection-r2 name=\"Numerical Projection\"}\nLet $y = (1, 3)'$ and $x = (1, 1)'$. We want to find the projection of $y$ onto $x$.\n\n**Method 1: Using the Vector Formula** First, calculate the inner products: $$\nx'y = 1(1) + 1(3) = 4\n$$ $$\n||x||^2 = 1^2 + 1^2 = 2\n$$\n\nNow, apply the formula: $$\n\\hat{y} = \\frac{4}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**Method 2: Using the Projection Matrix** Construct the matrix $P_x$: $$\nP_x = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix}\n$$\n\nMultiply by $y$: $$\n\\hat{y} = P_x y = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(3) \\\\ 0.5(1) + 0.5(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n:::\n\n**Example: Projection onto the Ones Vector (**$j_n$)\n\nA very common operation in statistics is calculating the sample mean. This can be viewed geometrically as a projection onto a specific vector.\n\n::: {#exm-mean-projection name=\"Projection onto the Ones Vector\"}\nLet $y = (y_1, \\dots, y_n)'$ be a data vector. Let $j_n = (1, 1, \\dots, 1)'$ be a vector of all ones.\n\nThe projection of $y$ onto $j_n$ is: $$\n\\text{proj}(y|j_n) = \\frac{j_n' y}{||j_n||^2} j_n\n$$\n\nCalculating the components: $$\nj_n' y = \\sum_{i=1}^n y_i \\quad \\text{(Sum of observations)}\n$$ $$\n||j_n||^2 = \\sum_{i=1}^n 1^2 = n\n$$\n\nSubstituting these back: $$\n\\hat{y} = \\frac{\\sum y_i}{n} j_n = \\bar{y} j_n = \\begin{pmatrix} \\bar{y} \\\\ \\vdots \\\\ \\bar{y} \\end{pmatrix}\n$$\n\nThus, replacing a data vector with its mean vector is geometrically equivalent to projecting the data onto the line spanned by the vector of ones.\n:::\n\n### Pythagorean Theorem\n\nThe Pythagorean theorem generalizes from simple geometry to vector spaces using the concept of orthogonality defined by the inner product.\n\n::: {#thm-pythagorean name=\"Pythagorean Theorem\"}\nIf two vectors $x$ and $y$ are orthogonal (i.e., $x \\perp y$ or $x'y = 0$), then the squared length of their sum is equal to the sum of their squared lengths:\n\n$$\n||x + y||^2 = ||x||^2 + ||y||^2\n$$\n:::\n\n::: proof\nWe expand the squared norm using the inner product:\n\n$$\n\\begin{aligned}\n||x + y||^2 &= (x + y)' (x + y) \\\\\n&= x'x + x'y + y'x + y'y \\\\\n&= ||x||^2 + 2x'y + ||y||^2\n\\end{aligned}\n$$\n\nSince $x \\perp y$, the inner product $x'y = 0$. Thus, the term $2x'y$ vanishes, leaving:\n\n$$\n||x + y||^2 = ||x||^2 + ||y||^2\n$$\n:::\n\nThe proof after defining inner product to represent $\\cos(\\theta)$ is trivival. @fig-pythagoras-proof shows a geometric proof of the fundamental Pythagorean Theorem (aka **勾股定理**).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Proof of Pythagorean Theorem using Area Scaling](lec1-vecspace_files/figure-html/fig-pythagoras-proof-1.png){#fig-pythagoras-proof fig-align='center' width=576 style=\"width: 80% !important;\"}\n:::\n:::\n\n\n### Least Square Property\n\nOne of the most important properties of the orthogonal projection is that it minimizes the distance between the vector $y$ and the subspace (or line) onto which it is projected.\n\n::: {#thm-shortest-distance name=\"Least Square Property\"}\nLet $\\hat{y}$ be the projection of $y$ onto the line $L(x)$. For any other vector $y^*$ on the line $L(x)$, the distance from $y$ to $y^*$ is always greater than or equal to the distance from $y$ to $\\hat{y}$.\n\n$$\n||y - y^*|| \\ge ||y - \\hat{y}||\n$$\n:::\n\n::: proof\nSince both $\\hat{y}$ and $y^*$ lie on the line $L(x)$, their difference $(\\hat{y} - y^*)$ also lies on $L(x)$. From the definition of projection, the residual $(y - \\hat{y})$ is orthogonal to the line $L(x)$. Therefore:\n\n$$\n(y - \\hat{y}) \\perp (\\hat{y} - y^*)\n$$\n\nWe can write the vector $(y - y^*)$ as: $$\ny - y^* = (y - \\hat{y}) + (\\hat{y} - y^*)\n$$\n\nApplying the Pythagorean Theorem: $$\n||y - y^*||^2 = ||y - \\hat{y}||^2 + ||\\hat{y} - y^*||^2\n$$\n\nSince $||\\hat{y} - y^*||^2 \\ge 0$, it follows that: $$\n||y - y^*||^2 \\ge ||y - \\hat{y}||^2\n$$\n:::\n\n## Vector Space\n\nWe now generalize our discussion from lines to broader spaces.\n\n::: {#def-vector-space name=\"Vector Space\"}\nA set $V \\subseteq \\mathbb{R}^n$ is called a **Vector Space** if it is closed under vector addition and scalar multiplication:\n\n1.  **Closed under Addition:** If $x_1 \\in V$ and $x_2 \\in V$, then $x_1 + x_2 \\in V$.\n2.  **Closed under Scalar Multiplication:** If $x \\in V$, then $cx \\in V$ for any scalar $c \\in \\mathbb{R}$.\n:::\n\nIt follows that the zero vector $0$ must belong to any subspace (by choosing $c=0$).\n\n### Spanned Vector Space\n\nThe most common way to construct a vector space in linear models is by spanning it with a set of vectors.\n\n::: {#def-spanned-space name=\"Spanned Vector Space\"}\nLet $x_1, \\dots, x_p$ be a set of vectors in $\\mathbb{R}^n$. The space spanned by these vectors, denoted $L(x_1, \\dots, x_p)$, is the set of all possible linear combinations of them:\n\n$$\nL(x_1, \\dots, x_p) = \\{ r \\mid r = c_1 x_1 + \\dots + c_p x_p, \\text{ for } c_i \\in \\mathbb{R} \\}\n$$\n:::\n\n### Column Space and Row Space\n\nWhen vectors are arranged into a matrix, we define specific spaces based on their columns and rows.\n\n::: {#def-column-space name=\"Column Space\"}\nFor a matrix $X = (x_1, \\dots, x_p)$, the **Column Space**, denoted $\\text{Col}(X)$, is the vector space spanned by its columns:\n\n$$\n\\text{Col}(X) = L(x_1, \\dots, x_p)\n$$\n:::\n\n::: {#def-row-space name=\"Row Space\"}\nThe **Row Space**, denoted $\\text{Row}(X)$, is the vector space spanned by the rows of the matrix $X$.\n:::\n\n### Linear Independence and Rank\n\nNot all vectors in a spanning set contribute new dimensions to the space. This concept is captured by linear independence.\n\n::: {#def-linear-independence name=\"Linear Independence\"}\nA set of vectors $x_1, \\dots, x_p$ is said to be **Linearly Independent** if the only solution to the linear combination equation equal to zero is the trivial solution:\n\n$$\n\\sum_{i=1}^p c_i x_i = 0 \\implies c_1 = c_2 = \\dots = c_p = 0\n$$\n\nIf there exist non-zero $c_i$'s such that sum is zero, the vectors are **Linearly Dependent**.\n:::\n\n## Rank of Matrices and Dim of Vector Space\n\n::: {#def-rank name=\"Rank\"}\nThe **Rank** of a matrix $X$, denoted $\\text{Rank}(X)$, is the maximum number of linearly independent columns in $X$. This is equivalent to the dimension of the column space:\n\n$$\n\\text{Rank}(X) = \\text{Dim}(\\text{Col}(X))\n$$\n:::\n\nThere are several fundamental properties regarding the rank of a matrix.\n\n::: {#exm-row-rank-equal-col-rank name=\"Example of the Equality of Row and Col Rank\"}\nConsider the following $3 \\times 4$ matrix ($n=3, p=4$): $$\nX = \\begin{pmatrix} \n1 & 0 & 1 & 0 \\\\ \n0 & 1 & 0 & 1 \\\\ \n1 & 1 & 1 & 1 \n\\end{pmatrix}\n$$ Notice that the third row is the sum of the first two ($r_3 = r_1 + r_2$).\n\n**1. Row Rank and Basis** $U$ The first two rows are linearly independent. We set the row rank $r=2$ and use these rows as our basis matrix $U$ ($2 \\times 4$): $$\nU = \\begin{pmatrix} \n1 & 0 & 1 & 0 \\\\ \n0 & 1 & 0 & 1 \n\\end{pmatrix}\n$$\n\n**2. Coefficient Matrix** $C$ We express every row of $X$ as a linear combination of the rows of $U$:\n\n-   Row 1: $1 \\cdot u_1 + 0 \\cdot u_2$\n-   Row 2: $0 \\cdot u_1 + 1 \\cdot u_2$\n-   Row 3: $1 \\cdot u_1 + 1 \\cdot u_2$\n\nThese coefficients form the matrix $C$ ($3 \\times 2$): $$\nC = \\begin{pmatrix} \n1 & 0 \\\\ \n0 & 1 \\\\ \n1 & 1 \n\\end{pmatrix}\n$$\n\n**3. The Decomposition (**$X = CU$) We verify that $X$ is the product of $C$ and $U$: $$\n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}}_{X \\ (3 \\times 4)} \n= \n\\underbrace{\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}}_{C \\ (3 \\times 2)} \n\\underbrace{\\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix}}_{U \\ (2 \\times 4)}\n$$\n\n**4. Conclusion on Column Rank** The columns of $X$ are linear combinations of the columns of $C$. $$\n\\text{Col}(X) \\subseteq \\text{Col}(C)\n$$ Since $C$ has only 2 columns, the dimension of its column space (and thus $X$'s column space) cannot exceed 2. $$\n\\text{Dim}(\\text{Col}(X)) \\le 2\n$$ This confirms that Row Rank (2) $\\ge$ Column Rank. (By symmetry, they are equal).\n:::\n\n::: {#thm-rank-properties name=\"Row Rank equals Column Rank\"}\n1.  **Row Rank equals Column Rank:** The dimension of the column space is equal to the dimension of the row space. $$\n    \\text{Dim}(\\text{Col}(X)) = \\text{Dim}(\\text{Row}(X)) \\implies \\text{Rank}(X) = \\text{Rank}(X')\n    $$\n\n2.  **Bounds:** For an $n \\times p$ matrix $X$: $$\n    \\text{Rank}(X) \\le \\min(n, p)\n    $$\n:::\n\n### Orthogonality to a Subspace\n\nWe can extend the concept of orthogonality from single vectors to entire subspaces.\n\n::: {#def-orth-subspace name=\"Orthogonality to a Subspace\"}\nA vector $y$ is orthogonal to a subspace $V$ (denoted $y \\perp V$) if $y$ is orthogonal to **every** vector $x$ in $V$.\n\n$$\ny \\perp V \\iff y'x = 0 \\quad \\forall x \\in V\n$$\n:::\n\n::: {#def-orthogonal-complement name=\"Orthogonal Complement\"}\nThe set of all vectors that are orthogonal to a subspace $V$ is called the **Orthogonal Complement** of $V$, denoted $V^\\perp$.\n\n$$\nV^\\perp = \\{ y \\in \\mathbb{R}^n \\mid y \\perp V \\}\n$$\n:::\n\n### Kernel (Null Space) and Image\n\nFor a matrix transformation defined by $X$, we define two key spaces: the Image (Column Space) and the Kernel (Null Space).\n\n::: {#def-image-kernel name=\"Image and Kernel\"}\n1.  **Image (Column Space):** The set of all possible outputs. $$\n    \\text{Im}(X) = \\text{Col}(X) = \\{ X\\beta \\mid \\beta \\in \\mathbb{R}^p \\}\n    $$\n\n2.  **Kernel (Null Space):** The set of all inputs mapped to the zero vector. $$\n    \\text{Ker}(X) = \\{ \\beta \\in \\mathbb{R}^p \\mid X\\beta = 0 \\}\n    $$\n:::\n\n::: {#thm-kernel-rowspace name=\"Relationship between Kernel and Row Space\"}\nThe kernel of $X$ is the orthogonal complement of the row space of $X$:\n\n$$\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n$$\n:::\n\n::: proof\nLet $x \\in \\mathbb{R}^p$. $x \\in \\text{Ker}(X)$ if and only if $Xx = 0$. If we denote the rows of $X$ as $r_1', \\dots, r_n'$, then the equation $Xx = 0$ is equivalent to the system of equations: $$\n\\begin{pmatrix} r_1' \\\\ \\vdots \\\\ r_n' \\end{pmatrix} x = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\iff r_i' x = 0 \\text{ for all } i = 1, \\dots, n\n$$ This means $x$ is orthogonal to every row of $X$. Since the rows span the row space $\\text{Row}(X)$, being orthogonal to every generator $r_i$ implies $x$ is orthogonal to the entire space $\\text{Row}(X)$. Thus, $\\text{Ker}(X) = \\{ x \\mid x \\perp \\text{Row}(X) \\} = [\\text{Row}(X)]^\\perp$.\n:::\n\n### Nullity Theorem\n\nThere is a fundamental relationship between the dimensions of these spaces.\n\n::: {#thm-nullity name=\"Rank-Nullity Theorem\"}\nFor an $n \\times p$ matrix $X$:\n\n$$\n\\text{Rank}(X) + \\text{Nullity}(X) = p\n$$ where $\\text{Nullity}(X) = \\text{Dim}(\\text{Ker}(X))$.\n:::\n\n::: proof\nFrom the previous theorem, we established that the kernel is the orthogonal complement of the row space: $$\n\\text{Ker}(X) = [\\text{Row}(X)]^\\perp\n$$\n\nSince the row space is a subspace of $\\mathbb{R}^p$, the entire space can be decomposed into the direct sum of the row space and its orthogonal complement: $$\n\\mathbb{R}^p = \\text{Row}(X) \\oplus [\\text{Row}(X)]^\\perp = \\text{Row}(X) \\oplus \\text{Ker}(X)\n$$\n\nTaking the dimensions of these spaces: $$\n\\text{Dim}(\\mathbb{R}^p) = \\text{Dim}(\\text{Row}(X)) + \\text{Dim}(\\text{Ker}(X))\n$$\n\nSubstituting the definitions of Rank (dimension of row/column space) and Nullity: $$\np = \\text{Rank}(X) + \\text{Nullity}(X)\n$$\n:::\n\n**Comparing Ranks via Kernel Containment**\n\nThe Rank-Nullity Theorem provides a powerful and convenient tool for comparing the ranks of two matrices $A$ and $B$ (with the same number of columns) by inspecting their null spaces.\n\n::: {#thm-rank-kernel name=\"Kernel Containment and Rank Inequality\"}\nLet $A$ and $B$ be two matrices with $p$ columns. If the kernel of $A$ is contained within the kernel of $B$, then the rank of $A$ is greater than or equal to the rank of $B$.\n\n$$\n\\text{Ker}(A) \\subseteq \\text{Ker}(B) \\implies \\text{Rank}(A) \\ge \\text{Rank}(B)\n$$\n:::\n\n::: proof\nFrom the subspace inclusion $\\text{Ker}(A) \\subseteq \\text{Ker}(B)$, it follows that the dimension of the smaller space cannot exceed the dimension of the larger space: $$\n\\text{Nullity}(A) \\le \\text{Nullity}(B)\n$$ Using the Rank-Nullity Theorem ($\\text{Rank} = p - \\text{Nullity}$), we reverse the inequality: $$\np - \\text{Nullity}(A) \\ge p - \\text{Nullity}(B)\n$$ $$\n\\text{Rank}(A) \\ge \\text{Rank}(B)\n$$\n:::\n\n### Rank Inequalities\n\nUnderstanding the bounds of the rank of matrix products is crucial for deriving properties of linear estimators.\n\n::: {#thm-rank-product name=\"Rank of a Matrix Product\"}\nLet $X$ be an $n \\times p$ matrix and $Z$ be a $p \\times k$ matrix. The rank of their product $XZ$ is bounded by the rank of the individual matrices:\n\n$$\n\\text{Rank}(XZ) \\le \\min(\\text{Rank}(X), \\text{Rank}(Z))\n$$\n:::\n\n::: proof\nThe columns of $XZ$ are linear combinations of the columns of $X$. Thus, the column space of $XZ$ is a subspace of the column space of $X$: $$\n\\text{Col}(XZ) \\subseteq \\text{Col}(X) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(X)\n$$ Similarly, the rows of $XZ$ are linear combinations of the rows of $Z$. Thus, the row space of $XZ$ is a subspace of the row space of $Z$: $$\n\\text{Row}(XZ) \\subseteq \\text{Row}(Z) \\implies \\text{Rank}(XZ) \\le \\text{Rank}(Z)\n$$\n:::\n\n**Rank and Invertible Matrices**\n\nMultiplying by an invertible (non-singular) matrix preserves the rank. This is a very useful property when manipulating linear equations.\n\n::: {#thm-rank-invertible name=\"Rank with Non-Singular Multiplication\"}\nLet $A$ be an $n \\times n$ invertible matrix (i.e., $\\text{Rank}(A) = n$) and $X$ be an $n \\times p$ matrix. Then:\n\n$$\n\\text{Rank}(AX) = \\text{Rank}(X)\n$$\n\nSimilarly, if $B$ is a $p \\times p$ invertible matrix, then:\n\n$$\n\\text{Rank}(XB) = \\text{Rank}(X)\n$$\n:::\n\n::: proof\nFrom the previous theorem, we know $\\text{Rank}(AX) \\le \\text{Rank}(X)$. Since $A$ is invertible, we can write $X = A^{-1}(AX)$. Applying the theorem again: $$\n\\text{Rank}(X) = \\text{Rank}(A^{-1}(AX)) \\le \\text{Rank}(AX)\n$$ Thus, $\\text{Rank}(AX) = \\text{Rank}(X)$.\n:::\n\n### Rank of $X'X$ and $XX'$\n\nThe matrix $X'X$ (the Gram matrix) appears in the normal equations for least squares ($X'X\\beta = X'y$). Its properties are closely tied to $X$.\n\n::: {#thm-rank-gram name=\"Rank of Gram Matrix\"}\nFor any real matrix $X$, the rank of $X'X$ and $XX'$ is the same as the rank of $X$ itself:\n\n$$\n\\text{Rank}(X'X) = \\text{Rank}(X)\n$$ $$\n\\text{Rank}(XX') = \\text{Rank}(X)\n$$\n:::\n\n::: proof\nWe first show that the null space (kernel) of $X$ is the same as the null space of $X'X$. If $v \\in \\text{Ker}(X)$, then $Xv = 0 \\implies X'Xv = 0 \\implies v \\in \\text{Ker}(X'X)$. Conversely, if $v \\in \\text{Ker}(X'X)$, then $X'Xv = 0$. Multiply by $v'$: $$\nv'X'Xv = 0 \\implies (Xv)'(Xv) = 0 \\implies ||Xv||^2 = 0 \\implies Xv = 0\n$$ So $\\text{Ker}(X) = \\text{Ker}(X'X)$. By the Rank-Nullity Theorem, since they have the same number of columns and same nullity, they must have the same rank.\n:::\n\n**Column Space of** $XX'$\n\nBeyond just the rank, the column spaces themselves are related.\n\n::: {#thm-colspace-gram name=\"Column Space Equivalence\"}\nThe column space of $XX'$ is identical to the column space of $X$:\n\n$$\n\\text{Col}(XX') = \\text{Col}(X)\n$$\n:::\n\n::: proof\n1.  **Forward (**$\\subseteq$): Let $z \\in \\text{Col}(XX')$. Then $z = XX'w$ for some vector $w$. We can rewrite this as $z = X(X'w)$. Since $z$ is a linear combination of columns of $X$ (with coefficients $X'w$), $z \\in \\text{Col}(X)$. Thus, $\\text{Col}(XX') \\subseteq \\text{Col}(X)$.\n\n2.  **Equality via Rank:** From the previous theorem, we know that $\\text{Rank}(XX') = \\text{Rank}(X)$. Since $\\text{Col}(XX')$ is a subspace of $\\text{Col}(X)$ and they have the same finite dimension (Rank), the subspaces must be identical.\n:::\n\n**Implication:** This property ensures that for any $y$, the projection of $y$ onto $\\text{Col}(X)$ lies in the same space as the projection onto $\\text{Col}(XX')$. This is vital for the existence of solutions in generalized least squares.\n\n## Orthogonal Projection onto a Subspace\n\n::: {name=\"Definition of Projection onto a Subspace $V$\"}\nLet $V$ be a subspace of $\\mathbb{R}^n$. For any vector $y \\in \\mathbb{R}^n$, there exists a **unique** vector $\\hat{y} \\in V$ such that the residual is orthogonal to the subspace:\n\n$$\n(y - \\hat{y}) \\perp V\n$$\n\nEquivalently: $$\n\\langle y - \\hat{y}, v \\rangle = 0 \\quad \\forall v \\in V\n$$\n:::\n\n### Equivalence to Least Squares\n\nThe geometric definition of projection (orthogonality) is mathematically equivalent to the optimization problem of minimizing distance (least squares).\n\n::: {#thm-best-approximation name=\"Best Approximation Theorem (Least Squares Property)\"}\nLet $V$ be a subspace of $\\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$. Let $\\hat{y}$ be the orthogonal projection of $y$ onto $V$. Then $\\hat{y}$ is the closest point in $V$ to $y$. That is, for any vector $v \\in V$ such that $v \\ne \\hat{y}$:\n\n$$\n\\|y - \\hat{y}\\|^2 < \\|y - v\\|^2\n$$\n:::\n\n::: proof\nLet $v$ be any vector in $V$. We can rewrite the difference vector $y - v$ by adding and subtracting the projection $\\hat{y}$: $$\ny - v = (y - \\hat{y}) + (\\hat{y} - v)\n$$\n\nObserve the properties of the two terms on the right-hand side:\n\n1.  **Residual:** $(y - \\hat{y})$ is orthogonal to $V$ by definition.\n2.  **Difference in Subspace:** Since both $\\hat{y} \\in V$ and $v \\in V$, their difference $(\\hat{y} - v)$ is also in $V$.\n\nTherefore, the two terms are orthogonal to each other: $$\n(y - \\hat{y}) \\perp (\\hat{y} - v)\n$$\n\nApplying the Pythagorean Theorem: $$\n\\|y - v\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - v\\|^2\n$$\n\nSince squared norms are non-negative, and $\\|\\hat{y} - v\\|^2 > 0$ (because $v \\ne \\hat{y}$): $$\n\\|y - v\\|^2 > \\|y - \\hat{y}\\|^2\n$$ The projection $\\hat{y}$ minimizes the squared error distance (and error distance itself).\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualization of the Best Approximation Theorem](lec1-vecspace_files/figure-html/fig-3d-proof-1.png){#fig-3d-proof width=960}\n:::\n:::\n\n\n### Uniqueness of Projection\n\nWhile the existence of a least-squares solution is guaranteed, we must also prove that there is only one such vector.\n\n::: {#thm-projection-uniqueness name=\"Uniqueness of Orthogonal Projection\"}\nFor a given vector $y$ and subspace $V$, the projection vector $\\hat{y}$ satisfying $(y - \\hat{y}) \\perp V$ is unique.\n:::\n\n::: proof\nAssume there are two vectors $\\hat{y}_1 \\in V$ and $\\hat{y}_2 \\in V$ that both satisfy the orthogonality condition. $$\n(y - \\hat{y}_1) \\perp V \\quad \\text{and} \\quad (y - \\hat{y}_2) \\perp V\n$$ This means that for any $v \\in V$, both inner products are zero: $$\n\\langle y - \\hat{y}_1, v \\rangle = 0\n$$ $$\n\\langle y - \\hat{y}_2, v \\rangle = 0\n$$\n\nSubtracting the second equation from the first: $$\n\\langle y - \\hat{y}_1, v \\rangle - \\langle y - \\hat{y}_2, v \\rangle = 0\n$$ Using the linearity of the inner product: $$\n\\langle (y - \\hat{y}_1) - (y - \\hat{y}_2), v \\rangle = 0\n$$ $$\n\\langle \\hat{y}_2 - \\hat{y}_1, v \\rangle = 0\n$$\n\nThis equation holds for **all** $v \\in V$. Since $\\hat{y}_1$ and $\\hat{y}_2$ are both in $V$, their difference $d = \\hat{y}_2 - \\hat{y}_1$ must also be in $V$. We can therefore choose $v = d = \\hat{y}_2 - \\hat{y}_1$. $$\n\\langle \\hat{y}_2 - \\hat{y}_1, \\hat{y}_2 - \\hat{y}_1 \\rangle = 0 \\implies \\|\\hat{y}_2 - \\hat{y}_1\\|^2 = 0\n$$ The only vector with a norm of zero is the zero vector itself. $$\n\\hat{y}_2 - \\hat{y}_1 = 0 \\implies \\hat{y}_1 = \\hat{y}_2\n$$ Thus, the projection is unique.\n:::\n\n## Projection via Orthonormal Basis ($Q$)\n\n### Orthonomal Basis\n\nBefore discussing projections onto general subspaces, we must formally define the coordinate system of a subspace, known as a basis.\n\n::: {#def-basis name=\"Basis\"}\nA set of vectors $\\{x_1, \\dots, x_k\\}$ is a **Basis** for a vector space $V$ if:\n\n1.  The vectors span the space: $V = L(x_1, \\dots, x_k)$.\n2.  The vectors are linearly independent.\n:::\n\nThe number of vectors in a basis is unique and is defined as the **Dimension** of $V$.\n\nCalculations become significantly simpler if we choose a basis with special geometric properties.\n\n::: {#def-orthonormal-basis name=\"Orthonormal Basis\"}\nA basis $\\{q_1, \\dots, q_k\\}$ is called an **Orthonormal Basis** if:\n\n1.  **Orthogonal:** Each pair of vectors is perpendicular. $$\n    q_i'q_j = 0 \\quad \\text{for } i \\ne j\n    $$\n\n2.  **Normalized:** Each vector has unit length. $$\n    ||q_i||^2 = q_i'q_i = 1\n    $$\n\nCombining these, we write $q_i'q_j = \\delta_{ij}$ (Kronecker delta).\n:::\n\nWe now generalize the projection problem. Instead of projecting $y$ onto a single line, we project it onto a subspace $V$ of dimension $k$.\n\nIf we have an orthonormal basis $\\{q_1, \\dots, q_k\\}$ for $V$, the projection $\\hat{y}$ is simply the sum of the projections onto the individual basis vectors.\n\n::: {#def-proj-orthonormal name=\"Projection Defined with Orthonormal Basis\"}\nThe projection of $y$ onto the subspace $V = L(q_1, \\dots, q_k)$ is:\n\n$$\n\\hat{y} = \\sum_{i=1}^k \\text{proj}(y|q_i) = \\sum_{i=1}^k (q_i'y) q_i\n$$\n\nSince the basis vectors are normalized, we do not need to divide by $||q_i||^2$.\n:::\n\n::: {#thm-orthonormal-basis-proj name=\"Projection via Orthonormal Basis\"}\nLet $\\{q_1, \\dots, q_k\\}$ be an orthonormal basis for the subspace $V \\subseteq \\mathbb{R}^n$. The vector defined by the sum of individual projections: $$\n\\hat{y} = \\sum_{i=1}^k \\langle y, q_i \\rangle q_i\n$$ is indeed the orthogonal projection of $y$ onto $V$. That is, it satisfies $(y - \\hat{y}) \\perp V$.\n:::\n\n::: proof\nTo prove this, we must check two conditions:\n\n1.  $\\hat{y} \\in V$: This is immediate because $\\hat{y}$ is a linear combination of the basis vectors $\\{q_1, \\dots, q_k\\}$.\n\n2.  $(y - \\hat{y}) \\perp V$: It suffices to show that the error vector $e = y - \\hat{y}$ is orthogonal to every basis vector $q_j$ (for $j = 1, \\dots, k$).\n\n    Let's calculate the inner product $\\langle y - \\hat{y}, q_j \\rangle$: $$\n    \\begin{aligned}\n    \\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle \\hat{y}, q_j \\rangle \\\\\n    &= \\langle y, q_j \\rangle - \\left\\langle \\sum_{i=1}^k \\langle y, q_i \\rangle q_i, q_j \\right\\rangle \\\\\n    &= \\langle y, q_j \\rangle - \\sum_{i=1}^k \\langle y, q_i \\rangle \\underbrace{\\langle q_i, q_j \\rangle}_{\\delta_{ij}}\n    \\end{aligned}\n    $$\n\n    Since the basis is orthonormal, $\\langle q_i, q_j \\rangle$ is 1 if $i=j$ and 0 otherwise. Thus, the summation collapses to a single term where $i=j$: $$\n    \\begin{aligned}\n    \\langle y - \\hat{y}, q_j \\rangle &= \\langle y, q_j \\rangle - \\langle y, q_j \\rangle \\cdot 1 \\\\\n    &= 0\n    \\end{aligned}\n    $$\n\n    Since $(y - \\hat{y})$ is orthogonal to every basis vector $q_j$, it is orthogonal to the entire subspace $V$. Thus, $\\hat{y}$ is the unique orthogonal projection.\n:::\n\n### Projection Matrix via Orthonomal Basis ($Q$)\n\n**Matrix Form with Orthonormal Basis**\n\nWe can express the summation formula for $\\hat{y}$ compactly using matrix notation.\n\nLet $Q$ be an $n \\times k$ matrix whose columns are the orthonormal basis vectors $q_1, \\dots, q_k$. $$\nQ = \\begin{pmatrix} q_1 & q_2 & \\dots & q_k \\end{pmatrix}\n$$\n\nProperties of $Q$:\n\n-   $Q'Q = I_k$ (Identity matrix of size $k \\times k$).\n-   $QQ'$ is **not** necessarily $I_n$ (unless $k=n$).\n\n::: {#def-proj-matrix-orthonormal name=\"Projection Matrix in Terms of $Q$\"}\nThe projection $\\hat{y}$ can be written as:\n\n$$\n\\hat{y} = \\begin{pmatrix} q_1 & \\dots & q_k \\end{pmatrix} \\begin{pmatrix} q_1'y \\\\ \\vdots \\\\ q_k'y \\end{pmatrix} = Q (Q'y) = (QQ') y\n$$\n\nThus, the projection matrix $P$ onto the subspace $V$ is: $$\nP = QQ'\n$$\n:::\n\n**Properties of Projection Matrices**\n\nWe have defined the projection matrix as $P = X(X'X)^{-1}X'$ (or $P=QQ'$ for orthonormal bases). All orthogonal projection matrices share two fundamental algebraic properties.\n\n::: {#thm-projection-properties name=\"Symmeticity and Idempotence\"}\nA square matrix $P$ represents an orthogonal projection onto some subspace if and only if it satisfies:\n\n1.  **Idempotence:** $P^2 = P$ (Applying the projection twice is the same as applying it once).\n2.  **Symmetry:** $P' = P$.\n:::\n\n::: proof\nIf $\\hat{y} = Py$ is already in the subspace $\\text{Col}(X)$, then projecting it again should not change it. $$\nP(Py) = Py \\implies P^2 y = Py \\quad \\forall y\n$$ Thus, $P^2 = P$.\n:::\n\n**Example: ANOVA (Analysis of Variance)**\n\nOne of the most common applications of projection is in Analysis of Variance (ANOVA). We can view the calculation of group means as a projection onto a subspace defined by group indicator variables.\n\n::: {#exm-anova-projection name=\"Finding Projection for One-way ANOVA\"}\nConsider a one-way ANOVA model with $k$ groups: $$\ny_{ij} = \\mu_i + \\epsilon_{ij}\n$$ where $i \\in \\{1, \\dots, k\\}$ represents the group and $j \\in \\{1, \\dots, n_i\\}$ represents the observation within the group. Let $N = \\sum_{i=1}^k n_i$ be the total number of observations.\n\n**1. Matrix Definitions** We define the data vector $y$ and the design matrix $X$ as follows:\n\n-   **Data Vector (**$y$): An $N \\times 1$ vector containing all observations stacked by group: $$\n      y = \\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1n_1} \\\\ y_{21} \\\\ \\vdots \\\\ y_{kn_k} \\end{pmatrix}\n      $$\n\n-   **Design Matrix (**$X$): An $N \\times k$ matrix constructed from $k$ column vectors, $X = (x_1, x_2, \\dots, x_k)$. Each vector $x_g$ is an **indicator variable** (dummy variable) for group $g$: $$\n      x_g = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\quad \\leftarrow \\text{Entries are 1 if observation belongs to group } g\n      $$\n\n**2. Orthogonality** These column vectors $x_1, \\dots, x_k$ are mutually orthogonal because no observation can belong to two groups at once. The dot product of any two distinct columns is zero: $$\n\\langle x_g, x_h \\rangle = 0 \\quad \\text{for } g \\neq h\n$$ This allows us to find the projection onto the column space of $X$ by simply summing the projections onto each column individually.\n\n**3. Calculating Individual Projections** For a specific group vector $x_g$, the projection is: $$\n\\text{proj}(y|x_g) = \\frac{\\langle y, x_g \\rangle}{\\langle x_g, x_g \\rangle} x_g\n$$\n\nWe calculate the two scalar terms:\n\n-   **Denominator (**$\\langle x_g, x_g \\rangle$): The sum of squared elements of $x_g$. Since $x_g$ contains $n_g$ ones and zeros elsewhere: $$\n      \\langle x_g, x_g \\rangle = \\sum \\mathbb{1}_{\\{i=g\\}}^2 = n_g\n      $$\n\n-   **Numerator (**$\\langle y, x_g \\rangle$): The dot product sums only the $y$ values belonging to group $g$: $$\n      \\langle y, x_g \\rangle = \\sum_{i,j} y_{ij} \\cdot \\mathbb{1}_{\\{i=g\\}} = \\sum_{j=1}^{n_g} y_{gj} = y_{g.} \\quad (\\text{Group Total})\n      $$\n\n**4. The Resulting Projection** Substituting these back into the formula gives the coefficient for the vector $x_g$: $$\n\\text{proj}(y|x_g) = \\frac{y_{g.}}{n_g} x_g = \\bar{y}_{g.} x_g\n$$\n\nThe total projection $\\hat{y}$ is the sum over all groups: $$\n\\hat{y} = \\sum_{g=1}^k \\bar{y}_{g.} x_g\n$$ This confirms that the fitted value for any specific observation $y_{ij}$ is simply its group mean $\\bar{y}_{i.}$.\n:::\n\n### Gram-Schmidt Process\n\nTo use the simplified formula $P = QQ'$, we need an orthonormal basis. The Gram-Schmidt process provides a method to construct such a basis from any set of linearly independent vectors.\n\n::: algorithm\n**Gram-Schmidt Process** Given linearly independent vectors $x_1, \\dots, x_p$:\n\n1.  **Step 1:** Normalize the first vector. $$\n    q_1 = \\frac{x_1}{||x_1||}\n    $$\n\n2.  **Step 2:** Project $x_2$ onto $q_1$ and subtract it to find the orthogonal component. $$\n    v_2 = x_2 - (x_2'q_1)q_1\n    $$ Then normalize: $$\n    q_2 = \\frac{v_2}{||v_2||}\n    $$\n\n3.  **Step k:** Subtract the projections onto all previous $q$ vectors. $$\n    v_k = x_k - \\sum_{j=1}^{k-1} (x_k'q_j)q_j\n    $$ $$\n    q_k = \\frac{v_k}{||v_k||}\n    $$\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gram-Schmidt Process: Projecting $x_2$ onto $x_1$](lec1-vecspace_files/figure-html/fig-gram-schmidt-python-3.png){#fig-gram-schmidt-python width=576}\n:::\n:::\n\n\nThis process leads to the **QR Decomposition** of a matrix: $X = QR$, where $Q$ is orthogonal and $R$ is upper triangular.\n\n## Hat Matrix (Projection Matrix via $X$)\n\n### Norm Equations\n\nLet $X = (x_1, \\dots, x_p)$ be an $n \\times p$ matrix, where each column $x_j$ is a predictor vector.\n\nWe want to project the target vector $y$ onto the column space $\\text{Col}(X)$. This is equivalent to finding a coefficient vector $\\beta \\in \\mathbb{R}^p$ such that the error vector (residual) is orthogonal to the entire subspace $\\text{Col}(X)$.\n\n$$\ny - X\\beta \\perp \\text{Col}(X)\n$$\n\nSince the columns of $X$ span the subspace, the residual must be orthogonal to **every** column vector $x_j$ individually:\n\n$$\ny - X\\beta \\perp x_j \\quad \\text{for } j = 1, \\dots, p\n$$\n\nWriting this geometric condition as an algebraic dot product (where $x_j'$ denotes the transpose):\n\n$$\nx_j'(y - X\\beta) = 0 \\quad \\text{for each } j\n$$\n\nWe can stack these $p$ separate linear equations into a single matrix equation. Since the rows of $X'$ are the columns of $X$, this becomes:\n\n$$\n\\begin{pmatrix} x_1' \\\\ \\vdots \\\\ x_p' \\end{pmatrix} (y - X\\beta) = \\mathbf{0}\n\\implies X'(y - X\\beta) = 0\n$$\n\nFinally, we distribute the matrix transpose and rearrange terms to solve for $\\beta$:\n\n$$\n\\begin{aligned}\nX'y - X'X\\beta &= 0 \\\\\nX'X\\beta &= X'y\n\\end{aligned}\n$$\n\nThis system is known as the **Normal Equations**.\n\n::: {#thm-least-squares-estimator name=\"Least Squares Estimator\"}\nIf $X'X$ is invertible (i.e., $X$ has full column rank), the unique solution for $\\beta$ is:\n\n$$\n\\hat{\\beta} = (X'X)^{-1}X'y\n$$\n:::\n\n### Hat Matrix\n\nSubstituting the estimator $\\hat{\\beta}$ back into the equation for $\\hat{y}$ gives us the projection matrix.\n\n::: {#def-projection-matrix-general name=\"Hat Matrix\"}\nThe projection of $y$ onto $\\text{Col}(X)$ is given by:\n\n$$\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y\n$$\n\nThus, the hat matrix $H$ is defined as:\n\n$$\nH = X(X'X)^{-1}X'\n$$\n:::\n\n### Equivalence of Hat Matrix and $QQ'$\n\nIf we use the QR decomposition such that $X = QR$, where the columns of $Q$ form an orthonormal basis for $\\text{Col}(X)$, the formula simplifies significantly.\n\nRecall that for orthonormal columns, $Q'Q = I$. Substituting $X=QR$ into the general formula:\n\n$$\n\\begin{aligned}\nH &= QR((QR)'(QR))^{-1}(QR)' \\\\\n  &= QR(R'Q'QR)^{-1}R'Q' \\\\\n  &= QR(R' \\underbrace{Q'Q}_{I} R)^{-1}R'Q' \\\\\n  &= QR(R'R)^{-1}R'Q' \\\\\n  &= QR R^{-1} (R')^{-1} R' Q' \\\\\n  &= Q \\underbrace{R R^{-1}}_{I} \\underbrace{(R')^{-1} R'}_{I} Q' \\\\\n  &= Q Q'\n\\end{aligned}\n$$\n\nThis confirms that $H = QQ'$ is consistent with the general formula $H = X(X'X)^{-1}X'$.\n\n### Properties of Hat Matrix\n\nWe revisit the properties of projection matrices in this general context.\n\n::: {#thm-projection-properties-revisited name=\"Properties of Hat Matrix\"}\nThe matrix $H = X(X'X)^{-1}X'$ satisfies:\n\n1.  **Symmetric:** $H' = H$\n2.  **Idempotent:** $H^2 = H$\n3.  **Trace:** The trace of a projection matrix equals the dimension of the subspace it projects onto. $$\n    \\text{tr}(H) = \\text{tr}(X(X'X)^{-1}X') = \\text{tr}((X'X)^{-1}X'X) = \\text{tr}(I_p) = p\n    $$\n:::\n\n## Projection Defined with Orthogonal Projection Matrix\n\nProjection don't have to be defined with a subspace or a matrix $X$ as we discussed before. Projection matrix is a self-contained definition of the subspace it projects onto.\n\n### Orthogonal Projection Matrix\n\n::: {#def-proj-matrix name=\"Orthogonal Projection Matrix\"}\nA square matrix $P$ is called an **orthogonal projection matrix** if it satisfies two conditions:\n\n1.  **Symmetry:** $P^\\top = P$\n2.  **Idempotency:** $P^2 = P$\n:::\n\n::: {#thm-proj-col name=\"Projection onto Column Space\"}\nIf a matrix $P$ is symmetric and idempotent, then $P$ represents the orthogonal projection onto its column space, $\\text{Col}(P)$.\n\nSpecifically, for any vector $y$, the vector $\\hat{y} = Py$ is the unique vector in $\\text{Col}(P)$ such that the residual $e = y - \\hat{y}$ is orthogonal to $\\text{Col}(P)$.\n:::\n\n::: proof\nLet $y \\in \\mathbb{R}^n$. We decompose $y$ as $y = Py + (I - P)y$. We must show that the residual term $(I-P)y$ is orthogonal to any vector $z \\in \\text{Col}(P)$.\n\nSince $z \\in \\text{Col}(P)$, there exists a vector $x$ such that $z = Px$. The inner product between $z$ and the residual is: $$\n\\langle z, (I - P)y \\rangle = z^\\top (I - P)y = (Px)^\\top (I - P)y\n$$ {#eq-inner-prod}\n\nUsing the matrix transpose property $(AB)^\\top = B^\\top A^\\top$, we rewrite @eq-inner-prod as: $$\n\\langle z, (I - P)y \\rangle = x^\\top P^\\top (I - P)y\n$$ {#eq-transpose-step}\n\nSince $P$ is symmetric ($P^\\top = P$), we can substitute $P$ for $P^\\top$ in @eq-transpose-step: $$\n\\langle z, (I - P)y \\rangle = x^\\top P (I - P)y = x^\\top (P - P^2)y\n$$ {#eq-sym-step}\n\nFinally, utilizing the idempotency of $P$ (where $P^2 = P$), the expression in @eq-sym-step simplifies to 0: $$\nx^\\top (P - P)y = x^\\top (0)y = 0\n$$ {#eq-final-step}\n\nSince the inner product is 0, the residual is orthogonal to every vector in $\\text{Col}(P)$. Thus, $P$ is the orthogonal projector.\n:::\n\n### Projection onto Complement Space\n\n::: {#thm-complement-proj name=\"Projection onto Orthogonal Complement\"}\nLet $P$ be an orthogonal projection matrix. The matrix $M$ defined as: $$\nM = I - P\n$$ is the orthogonal projection matrix onto the orthogonal complement of the column space of $P$, denoted $\\text{Col}(P)^\\perp$.\n:::\n\n::: proof\n**1. Symmetry and Idempotency** Since $P$ is a projection matrix, $P^\\top = P$ and $P^2 = P$. We verify these properties for $M$: $$\nM^\\top = (I - P)^\\top = I - P^\\top = I - P = M\n$$ {#eq-m-sym} $$\nM^2 = (I - P)(I - P) = I - 2P + P^2 = I - 2P + P = I - P = M\n$$ {#eq-m-idemp} By @eq-m-sym and @eq-m-idemp, $M$ is symmetric and idempotent, so it is an orthogonal projection matrix.\n\n**2. Identifying the Subspace** By @thm-proj-col, $M$ projects onto its own column space, $\\text{Col}(M)$. A vector $v$ is in $\\text{Col}(M)$ if and only if it is fixed by the projection ($Mv = v$). $$\nMv = v\n$$ {#eq-fixed-point}\n\nSubstituting $M = I - P$ into @eq-fixed-point gives: $$\n(I - P)v = v\n$$ {#eq-sub-m}\n\nRearranging @eq-sub-m, we find the condition for $v$: $$\nv - Pv = v \\implies Pv = 0\n$$ {#eq-nullspace}\n\nThe condition $Pv = 0$ in @eq-nullspace implies that $v$ belongs to the null space of $P$, denoted $\\text{Null}(P)$. By the Fundamental Theorem of Linear Algebra for symmetric matrices, the null space is the orthogonal complement of the column space: $$\n\\text{Null}(P) = \\text{Col}(P^\\top)^\\perp = \\text{Col}(P)^\\perp\n$$ Thus, the image of $M$ is exactly $\\text{Col}(P)^\\perp$.\n:::\n\n::: {#exr-col-spaces name=\"Column Space of the Hat Matrix\"}\nLet $H = X(X^\\top X)^{-1}X^\\top$ be the hat matrix.\n\n1.  Prove that the column space of $H$ is identical to the column space of $X$: $$ \\text{Col}(H) = \\text{Col}(X) $$\n2.  Using the result above, show that the column space of the residual maker matrix $M = I - H$ is the orthogonal complement of $\\text{Col}(X)$: $$ \\text{Col}(M) = \\text{Col}(X)^\\perp $$\n:::\n\n::: {.callout-note collapse=\"true\" icon=\"false\" title=\"Solutions\"}\n**1. Equivalence of Column Spaces** To prove $\\text{Col}(H) = \\text{Col}(X)$, we show inclusion in both directions.\n\n-   **Forward (**$\\text{Col}(H) \\subseteq \\text{Col}(X)$): By definition, $H = X [(X^\\top X)^{-1}X^\\top]$. Any column of $H$ is a linear combination of the columns of $X$ (weighted by the matrix in brackets). Therefore, any vector in the image of $H$ must lie in $\\text{Col}(X)$.\n\n-   **Reverse (**$\\text{Col}(X) \\subseteq \\text{Col}(H)$): Take any vector $v \\in \\text{Col}(X)$. By definition, $v = Xb$ for some vector $b$. Apply $H$ to $v$: $$\n      Hv = X(X^\\top X)^{-1}X^\\top (Xb) = X(X^\\top X)^{-1}(X^\\top X)b = X(I)b = Xb = v\n      $$ Since $Hv = v$, the vector $v$ lies in the column space of $H$ (specifically, it is an eigenvector with eigenvalue 1).\n\nSince both inclusions hold, $\\text{Col}(H) = \\text{Col}(X)$.\n\n**2. Orthogonal Complements** From part 1, we know the subspaces are identical. Therefore, their orthogonal complements must also be identical: $$\n\\text{Col}(H)^\\perp = \\text{Col}(X)^\\perp\n$$ We previously established in @thm-complement-proj that for any projection matrix $P$, the complement projection $M = I - P$ projects onto $\\text{Col}(P)^\\perp$. Substituting $H$ for $P$: $$\n\\text{Col}(M) = \\text{Col}(H)^\\perp\n$$ Combining these results gives the required equality: $$\n\\text{Col}(M) = \\text{Col}(X)^\\perp\n$$\n:::\n\n## Projection onto Nested Subspaces\n\n### Nested Models and Subspaces\n\nIn hypothesis testing (like comparing a null model to an alternative model), we often deal with nested subspaces.\n\n::: {#def-nested-models name=\"Nested Models\"}\nConsider two models:\n\n1.  **Reduced Model (**$M_0$): $y \\in \\text{Col}(X_0)$\n2.  **Full Model (**$M_1$): $y \\in \\text{Col}(X_1)$\n\nWe say the models are nested if the column space of the reduced model is contained entirely within the column space of the full model: $$\n\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)\n$$\n:::\n\nUsually, $X_1$ is constructed by adding columns to $X_0$: $X_1 = [X_0, X_{\\text{new}}]$.\n\n### Projections onto Nested Subspaces\n\nLet $P_0$ be the projection matrix onto $\\text{Col}(X_0)$ and $P_1$ be the projection matrix onto $\\text{Col}(X_1)$. Since $\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)$, we have important relationships between these matrices.\n\n::: {#thm-nested-projections name=\"Composition of Projections\"}\nIf $\\text{Col}(P_0) \\subseteq \\text{Col}(P_1)$, then:\n\n1.  $P_1 P_0 = P_0$ (Projecting onto the small space, then the large space, keeps you in the small space).\n2.  $P_0 P_1 = P_0$ (Projecting onto the large space, then the small space, is the same as just projecting onto the small space).\n:::\n\n::: proof\n**1. Proof of** $P_1 P_0 = P_0$: For any vector $y \\in \\mathbb{R}^n$, the vector $v = P_0 y$ lies in $\\text{Col}(X_0)$. Since $\\text{Col}(X_0) \\subseteq \\text{Col}(X_1)$, the vector $v$ also lies in $\\text{Col}(X_1)$. A projection matrix $P_1$ acts as the identity operator for any vector already in its column space. Therefore, $P_1 v = v$. Substituting $v = P_0 y$, we get $P_1 P_0 y = P_0 y$ for all $y$. Thus, $P_1 P_0 = P_0$.\n\n**2. Proof of** $P_0 P_1 = P_0$: Take the transpose of the previous result ($P_1 P_0 = P_0$). $$\n(P_1 P_0)' = P_0'\n$$ Using the property that projection matrices are symmetric ($P' = P$): $$\nP_0' P_1' = P_0' \\implies P_0 P_1 = P_0\n$$\n:::\n\n**Difference of Projections**\n\nThe difference between the two projection matrices, $P_1 - P_0$, is itself a projection matrix.\n\n::: {#thm-diff-projection name=\"Difference Projection\"}\nThe matrix $P_{\\Delta} = P_1 - P_0$ is an orthogonal projection matrix onto the subspace $\\text{Col}(X_1) \\cap \\text{Col}(X_0)^\\perp$. This subspace represents the \"extra\" information in the full model that is orthogonal to the reduced model.\n\n**Properties:**\n\n1.  **Symmetric:** $(P_1 - P_0)' = P_1 - P_0$.\n2.  **Idempotent:** $(P_1 - P_0)(P_1 - P_0) = P_1 - P_0 P_1 - P_1 P_0 + P_0 = P_1 - P_0 - P_0 + P_0 = P_1 - P_0$.\n3.  **Orthogonality:** $(P_1 - P_0)P_0 = P_1 P_0 - P_0 = P_0 - P_0 = 0$.\n:::\n\n::: proof\n**1. Symmetry:** Since $P_1$ and $P_0$ are symmetric: $(P_1 - P_0)' = P_1' - P_0' = P_1 - P_0$.\n\n**2. Idempotency:** $$\n\\begin{aligned}\n(P_1 - P_0)^2 &= (P_1 - P_0)(P_1 - P_0) \\\\\n&= P_1^2 - P_1 P_0 - P_0 P_1 + P_0^2\n\\end{aligned}\n$$ Using the projection properties ($P^2=P$) and the nested property ($P_1 P_0 = P_0$ and $P_0 P_1 = P_0$): $$\n= P_1 - P_0 - P_0 + P_0 = P_1 - P_0\n$$\n\n**3. Orthogonality to** $P_0$: $$\n(P_1 - P_0)P_0 = P_1 P_0 - P_0^2 = P_0 - P_0 = 0\n$$ Since $(P_1 - P_0)$ is symmetric and idempotent, it is an orthogonal projection matrix. Since it is orthogonal to $P_0$ (the space of $M_0$) but is derived from $P_1$, it projects onto the subspace of $M_1$ that is orthogonal to $M_0$.\n:::\n\n### Decomposition of Projections and their Sum Squares\n\n::: {#thm-nested-decomposition name=\"Orthogonal Decomposition\"}\nLet $M_0 \\subset M_1$ be two nested linear models with corresponding design matrices $X_0$ and $X_1$ such that $\\text{Col}(X_0) \\subset \\text{Col}(X_1)$. Let $P_0$ and $P_1$ be the orthogonal projection matrices onto $\\text{Col}(X_0)$ and $\\text{Col}(X_1)$ respectively.\n\nFor any observation vector $y$, we have the decomposition: $$\ny = \\underbrace{P_0 y}_{\\hat{y}_0} + \\underbrace{(P_1 - P_0) y}_{\\hat{y}_1 - \\hat{y}_0} + \\underbrace{(I - P_1) y}_{y - \\hat{y}_1}\n$$\n\n**Geometric Interpretation:**\n\n1.  $\\hat{y}_0 \\in \\text{Col}(X_0)$: The fit of the reduced model.\n2.  $(\\hat{y}_1 - \\hat{y}_0) \\in \\text{Col}(X_0)^\\perp \\cap \\text{Col}(X_1)$: The additional fit provided by $M_1$ over $M_0$.\n3.  $(y - \\hat{y}_1) \\in \\text{Col}(X_1)^\\perp$: The projection of $y$ onto the **orthogonal complement** of $\\text{Col}(X_1)$.\n\nThe three component vectors are mutually orthogonal. Consequently, their squared norms sum to the total squared norm: $$\n\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2 + \\|y - \\hat{y}_1\\|^2\n$$\n:::\n\n::: proof\n**1. Definitions** We define the three components as vectors $v_1, v_2, v_3$:\n\n-   $v_1 = \\hat{y}_0 = P_0 y$.\n-   $v_2 = \\hat{y}_1 - \\hat{y}_0 = (P_1 - P_0)y$.\n-   $v_3 = y - \\hat{y}_1 = (I - P_1)y$.\n    -   **Note:** Since $P_1$ projects onto $\\text{Col}(X_1)$, the matrix $(I - P_1)$ projects onto the **orthogonal complement** $\\text{Col}(X_1)^\\perp$. Thus, $v_3 \\in \\text{Col}(I - P_1)$.\n\nNote that since $\\text{Col}(X_0) \\subset \\text{Col}(X_1)$, we have the property $P_1 P_0 = P_0 P_1 = P_0$. (Projecting onto the smaller subspace $M_0$ is unchanged if we first project onto the enclosing subspace $M_1$).\n\n**2. Orthogonality of** $v_1$ and $v_2$ We check the inner product $\\langle v_1, v_2 \\rangle = v_1' v_2$: $$\n\\begin{aligned}\nv_1' v_2 &= (P_0 y)' (P_1 - P_0) y \\\\\n&= y' P_0' (P_1 - P_0) y \\\\\n&= y' (P_0 P_1 - P_0^2) y \\quad (\\text{Since } P_0 \\text{ is symmetric}) \\\\\n&= y' (P_0 - P_0) y \\quad (\\text{Since } P_0 P_1 = P_0 \\text{ and } P_0^2 = P_0) \\\\\n&= 0\n\\end{aligned}\n$$\n\n**3. Orthogonality of** $(v_1 + v_2)$ and $v_3$ Note that $v_1 + v_2 = P_1 y = \\hat{y}_1$. We check if the total fit $\\hat{y}_1$ is orthogonal to the residual $v_3$: $$\n\\begin{aligned}\n\\hat{y}_1' v_3 &= (P_1 y)' (I - P_1) y \\\\\n&= y' P_1 (I - P_1) y \\\\\n&= y' (P_1 - P_1^2) y \\\\\n&= y' (P_1 - P_1) y \\\\\n&= 0\n\\end{aligned}\n$$ Since $\\hat{y}_1$ is orthogonal to $v_3$, and $\\hat{y}_0$ is a component of $\\hat{y}_1$, it follows that all three pieces are mutually orthogonal.\n\n**4. Sum of Squares** By the Pythagorean theorem applied twice to these orthogonal vectors, the equality of squared norms follows immediately.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of Projections onto Nested Subspaces](lec1-vecspace_files/figure-html/fig-anova-decomposition-v2-5.png){#fig-anova-decomposition-v2 width=960}\n:::\n:::\n\n\n::: {#exm-anova-ss name=\"ANOVA Sum Squares\"}\nWe apply the **Nested Model Theorem** ($M_0 \\subset M_1$) to the One-way ANOVA setting.\n\n**1. Notation and Definitions**\n\nConsider a dataset with $k$ groups. Let $i = 1, \\dots, k$ index the groups, and $j = 1, \\dots, n_i$ index the observations within group $i$.\n\n-   $N$: Total number of observations, $N = \\sum_{i=1}^k n_i$.\n\n-   $y_{ij}$: The $j$-th observation in the $i$-th group.\n\n-   $\\bar{y}_{i.}$: The sample mean of group $i$. $$ \\bar{y}_{i.} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij} $$\n\n-   $\\bar{y}_{..}$: The grand mean of all observations. $$ \\bar{y}_{..} = \\frac{1}{N} \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij} $$\n\n**2. The Data and Projection Vectors**\n\n| Observation ($y$) | Null Projection ($\\hat{y}_0$) | Full Projection ($\\hat{y}_1$) |\n|:----------------------:|:----------------------:|:----------------------:|\n| $\\begin{pmatrix} y_{11} \\\\ \\vdots \\\\ y_{1 n_1} \\\\ \\hline \\vdots \\\\ \\hline y_{k1} \\\\ \\vdots \\\\ y_{k n_k} \\end{pmatrix}$ | $\\begin{pmatrix} \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{..} \\\\ \\vdots \\\\ \\bar{y}_{..} \\end{pmatrix}$ | $\\begin{pmatrix} \\bar{y}_{1.} \\\\ \\vdots \\\\ \\bar{y}_{1.} \\\\ \\hline \\vdots \\\\ \\hline \\bar{y}_{k.} \\\\ \\vdots \\\\ \\bar{y}_{k.} \\end{pmatrix}$ |\n\n: ANOVA Vectors: Data, Null Model, and Full Model {#tbl-anova-vectors}\n\n**3. Decomposition and Sum of Squares**\n\n| Component | Notation | Definition | Vector Elements | Squared Norm (Sum of Squares) |\n|:-------------|:------------:|:------------:|:-------------|:----------------|\n| **Null Proj.** | $\\hat{y}_0$ | $P_0 y$ | Grand Mean ($\\bar{y}_{..}$) | $\\|\\hat{y}_0\\|^2 = N \\bar{y}_{..}^2$ |\n| **Full Proj.** | $\\hat{y}_1$ | $P_1 y$ | Group Means ($\\bar{y}_{i.}$) | $\\|\\hat{y}_1\\|^2 = \\sum_{i=1}^k n_i \\bar{y}_{i.}^2$ |\n\n**4. Geometric Justification of Shortcut Formulas**\n\n**A. Total Sum of Squares (SST)** Since $\\hat{y}_0 \\perp (y - \\hat{y}_0)$, we have $\\|y\\|^2 = \\|\\hat{y}_0\\|^2 + \\|y - \\hat{y}_0\\|^2$: $$ \\text{SST} = \\|y - \\hat{y}_0\\|^2 = \\|y\\|^2 - \\|\\hat{y}_0\\|^2 $$ $$ \\text{SST} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - N\\bar{y}_{..}^2 $$\n\n**B. Between Group Sum of Squares (SSB)** Since $\\hat{y}_0 \\perp (\\hat{y}_1 - \\hat{y}_0)$, we have $\\|\\hat{y}_1\\|^2 = \\|\\hat{y}_0\\|^2 + \\|\\hat{y}_1 - \\hat{y}_0\\|^2$: $$ \\text{SSB} = \\|\\hat{y}_1 - \\hat{y}_0\\|^2 = \\|\\hat{y}_1\\|^2 - \\|\\hat{y}_0\\|^2 $$ $$ \\text{SSB} = \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2 $$\n\n**C. Within Group Sum of Squares (SSW)** Since $\\hat{y}_1 \\perp (y - \\hat{y}_1)$, we have $\\|y\\|^2 = \\|\\hat{y}_1\\|^2 + \\|y - \\hat{y}_1\\|^2$: $$ \\text{SSW} = \\|y - \\hat{y}_1\\|^2 = \\|y\\|^2 - \\|\\hat{y}_1\\|^2 $$ $$ \\text{SSW} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} y_{ij}^2 - \\sum_{i=1}^k n_i\\bar{y}_{i.}^2 $$\n\n**Conclusion:** $$ \\underbrace{\\|y\\|^2 - N\\bar{y}_{..}^2}_{\\text{SST}} = \\underbrace{(\\sum n_i\\bar{y}_{i.}^2 - N\\bar{y}_{..}^2)}_{\\text{SSB}} + \\underbrace{(\\sum \\sum y_{ij}^2 - \\sum n_i\\bar{y}_{i.}^2)}_{\\text{SSW}} $$ **5. Visualizing ANOVA Components in Data Space**\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Generate Data\nnp.random.seed(42)\ngroup_names = ['A', 'B', 'C', 'D']\nn_i = [10, 12, 8, 15]\nmeans = [10, 15, 12, 18]\nstd_dev = 1.5\n\n# Define colors and markers for each group\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\nmarkers = ['o', 's', '^', 'D']\n\ndata_x = []\ndata_y = []\ngroup_boundaries = [0]\ngroup_indices = [] # To store indices for each group\n\ncurrent_idx = 0\nfor i, n in enumerate(n_i):\n    group_data = np.random.normal(means[i], std_dev, n)\n    indices = np.arange(current_idx, current_idx + n)\n    data_x.extend(indices)\n    data_y.extend(group_data)\n    group_indices.append(indices) # Store indices for plotting later\n    current_idx += n\n    group_boundaries.append(current_idx)\n\ndata_x = np.array(data_x)\ndata_y = np.array(data_y)\n\n# Calculate Stats\ngrand_mean = np.mean(data_y)\ngroup_means = [np.mean(data_y[group_boundaries[i]:group_boundaries[i+1]]) for i in range(len(n_i))]\n\n# 2. Plotting\nplt.figure(figsize=(12, 6))\n\n# Draw Grand Mean (Full span)\nplt.axhline(y=grand_mean, color='red', linestyle='--', linewidth=2, label=f'Grand Mean ($\\\\bar{{y}}_{{..}}$ = {grand_mean:.2f})')\n\n# Iterate through each group to plot points and means with matching colors\nfor i in range(len(n_i)):\n    start, end = group_boundaries[i], group_boundaries[i+1]\n    idx = group_indices[i]\n    \n    # 1. Scatter plot for the group with unique color and marker\n    plt.scatter(data_x[idx], data_y[idx], color=colors[i], marker=markers[i], \n                alpha=0.7, s=60, label=f'Group {group_names[i]}')\n    \n    # 2. Horizontal line for group mean with the SAME color\n    plt.hlines(y=group_means[i], xmin=start, xmax=end-1, color=colors[i], linewidth=3)\n    \n    # 3. Visualizing the \"Within\" residuals (faint lines)\n    for j in idx:\n        plt.vlines(x=j, ymin=min(data_y[j], group_means[i]), \n                   ymax=max(data_y[j], group_means[i]), \n                   color=colors[i], alpha=0.3, linestyle=':')\n\n# Formatting\nplt.title(\"One-Way ANOVA: Data, Group Means, and Grand Mean\", fontsize=14)\nplt.xlabel(\"Observation Index ($j$ grouped by $i$)\", fontsize=12)\nplt.ylabel(\"Value ($y_{ij}$)\", fontsize=12)\n# Set x-ticks at the center of each group\nplt.xticks(np.array(group_boundaries[:-1]) + np.array(n_i)/2 - 0.5, \n           [f\"Group {g}\\n($n_{{{g.lower()}}}={n}$)\" for g, n in zip(group_names, n_i)])\n```\n\n```{.python .cell-code}\nplt.grid(axis='y', alpha=0.3)\n\n# Adjust legend to show group markers and the grand mean line\nhandles, labels = plt.gca().get_legend_handles_labels()\n# Reorder legend: Groups first, then Grand Mean\norder = [1, 2, 3, 4, 0]\nplt.legend([handles[idx] for idx in order], [labels[idx] for idx in order], \n           bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![Visualization of Group Means vs. Grand Mean](lec1-vecspace_files/figure-html/fig-anova-data-space-colored-7.png){#fig-anova-data-space-colored width=1152}\n:::\n:::\n\n:::\n\n## Projections onto Orthogonal Subspaces\n\nFinally, we consider the case where the entire space $\\mathbb{R}^n$ is decomposed into mutually orthogonal subspaces.\n\n::: {#thm-orth-decomposition name=\"General Orthogonal Projections\"}\nIf $\\mathbb{R}^n$ is the direct sum of orthogonal subspaces $V_1, V_2, \\dots, V_k$:\n\n$$\n\\mathbb{R}^n = V_1 \\oplus V_2 \\oplus \\dots \\oplus V_k\n$$ where $V_i \\perp V_j$ for all $i \\ne j$.\n\nThen any vector $y$ can be uniquely written as: $$\ny = \\hat{y}_1 + \\hat{y}_2 + \\dots + \\hat{y}_k\n$$ where $\\hat{y}_i \\in V_i$.\n\nFurthermore, each component $\\hat{y}_i$ is simply the projection of $y$ onto the subspace $V_i$: $$\n\\hat{y}_i = P_i y\n$$\n:::\n\n::: proof\n**1. Existence:** Since $\\mathbb{R}^n$ is the direct sum of $V_1, \\dots, V_k$, by definition, any vector $y \\in \\mathbb{R}^n$ can be written as a sum $y = v_1 + \\dots + v_k$ where $v_i \\in V_i$.\n\n**2. Uniqueness:** Suppose there are two such representations: $y = \\sum v_i = \\sum w_i$, with $v_i, w_i \\in V_i$. Then $\\sum (v_i - w_i) = 0$. Since subspaces in a direct sum are independent, the only way for the sum of elements to be zero is if each individual element is zero. Thus, $v_i - w_i = 0 \\implies v_i = w_i$. The representation is unique. Let $\\hat{y}_i = v_i$.\n\n**3. Projection Property:** We claim that the $i$-th component $\\hat{y}_i$ is the orthogonal projection of $y$ onto $V_i$. We must show that the residual $(y - \\hat{y}_i)$ is orthogonal to $V_i$. $$\ny - \\hat{y}_i = \\sum_{j \\ne i} \\hat{y}_j\n$$ Let $z$ be any vector in $V_i$. We calculate the inner product: $$\n\\langle y - \\hat{y}_i, z \\rangle = \\left\\langle \\sum_{j \\ne i} \\hat{y}_j, z \\right\\rangle = \\sum_{j \\ne i} \\langle \\hat{y}_j, z \\rangle\n$$ Since $\\hat{y}_j \\in V_j$ and $z \\in V_i$, and the subspaces are mutually orthogonal ($V_j \\perp V_i$ for $j \\ne i$), every term in the sum is zero. Therefore, $(y - \\hat{y}_i) \\perp V_i$. By the definition of orthogonal projection, $\\hat{y}_i = P_i y$.\n:::\n\nThis implies that the identity matrix can be decomposed into a sum of projection matrices: $$\nI_n = P_1 + P_2 + \\dots + P_k\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Orthogonal decomposition of vector y into subspaces](lec1-vecspace_files/figure-html/fig-orthogonal-decomp-rotated-9.png){#fig-orthogonal-decomp-rotated width=960}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\n\n# --- Define Vectors ---\ny_vec <- c(3, 4, 5)\norigin <- c(0, 0, 0)\n\n# Projections (P_i y)\np1 <- c(3, 0, 0)\np2 <- c(0, 4, 0)\np3 <- c(0, 0, 5)\n\n# Partial Sums (P_i y + P_j y)\nsum_12 <- p1 + p2\nsum_13 <- p1 + p3\nsum_23 <- p2 + p3\n\n# --- Helper Functions ---\n\n# Function to add a vector with an arrowhead (Cone)\nadd_vec_arrow <- function(p, start, end, color, name) {\n  p %>%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 6),\n      name = name,\n      showlegend = TRUE\n    ) %>%\n    add_trace(\n      type = \"cone\",\n      x = end[1], y = end[2], z = end[3],\n      u = end[1]-start[1], v = end[2]-start[2], w = end[3]-start[3],\n      sizemode = \"absolute\",\n      sizeref = 0.5,\n      anchor = \"tip\",\n      colorscale = list(c(0, 1), c(color, color)),\n      showscale = FALSE,\n      name = name,\n      showlegend = FALSE\n    )\n}\n\n# Function to add dashed \"error\" lines\nadd_dashed_line <- function(p, start, end, color, name) {\n  p %>%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines\",\n      x = c(start[1], end[1]),\n      y = c(start[2], end[2]),\n      z = c(start[3], end[3]),\n      line = list(color = color, width = 3, dash = \"dash\"),\n      name = name,\n      hoverinfo = \"text\",\n      text = name\n    )\n}\n\n# --- Build Plot ---\nfig <- plot_ly()\n\n# 1. Main Vectors (Solid + Cones)\nfig <- fig %>%\n  add_vec_arrow(origin, p1, \"red\", \"P1 y\") %>%\n  add_vec_arrow(origin, p2, \"green\", \"P2 y\") %>%\n  add_vec_arrow(origin, p3, \"blue\", \"P3 y\") %>%\n  add_vec_arrow(origin, y_vec, \"black\", \"y\")\n\n# 2. Dashed Lines from y to Single Projections\nfig <- fig %>%\n  add_dashed_line(y_vec, p1, \"rgba(255, 0, 0, 0.5)\", \"y -> P1\") %>%\n  add_dashed_line(y_vec, p2, \"rgba(0, 255, 0, 0.5)\", \"y -> P2\") %>%\n  add_dashed_line(y_vec, p3, \"rgba(0, 0, 255, 0.5)\", \"y -> P3\")\n\n# 3. Dashed Lines from y to Partial Sums\nfig <- fig %>%\n  add_dashed_line(y_vec, sum_12, \"purple\", \"y -> (P1+P2)\") %>%\n  add_dashed_line(y_vec, sum_13, \"orange\", \"y -> (P1+P3)\") %>%\n  add_dashed_line(y_vec, sum_23, \"cyan\",   \"y -> (P2+P3)\")\n\n# 4. Axes (Subspaces)\nlimit <- 6\naxis_style <- list(color = \"gray\", dash = \"dot\", width = 2)\n\nfig <- fig %>%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0, limit), y=c(0,0), z=c(0,0), \n            line=axis_style, name=\"V1 (x)\") %>%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0, limit), z=c(0,0), \n            line=axis_style, name=\"V2 (y)\") %>%\n  add_trace(type=\"scatter3d\", mode=\"lines\", x=c(0,0), y=c(0,0), z=c(0, limit), \n            line=axis_style, name=\"V3 (z)\")\n\n# --- Layout ---\nfig <- fig %>% layout(\n  title = \"Orthogonal Decomposition Geometry\",\n  width = 900,\n  height = 700,\n  scene = list(\n    xaxis = list(title = \"V1\", range = c(0, limit)),\n    yaxis = list(title = \"V2\", range = c(0, limit)),\n    zaxis = list(title = \"V3\", range = c(0, limit)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(x = 1.5, y = 1.5, z = 1.2))\n  ),\n  margin = list(l = 0, r = 0, b = 0, t = 50),\n  legend = list(x = 0.75, y = 0.9)\n)\n\nfig\n```\n\n::: {#fig-orthogonal-decomp-r .cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-5919cc0ddd953badc152\" style=\"width:100%;height:520px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5919cc0ddd953badc152\">{\"x\":{\"visdat\":{\"ce61751bc5\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"ce61751bc5\",\"attrs\":{\"ce61751bc5\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"red\",\"width\":6},\"name\":\"P1 y\",\"showlegend\":true,\"inherit\":true},\"ce61751bc5.1\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"cone\",\"x\":3,\"y\":0,\"z\":0,\"u\":3,\"v\":0,\"w\":0,\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"colorscale\":[[0,1],[\"red\",\"red\"]],\"showscale\":false,\"name\":\"P1 y\",\"showlegend\":false,\"inherit\":true},\"ce61751bc5.2\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,4],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":6},\"name\":\"P2 y\",\"showlegend\":true,\"inherit\":true},\"ce61751bc5.3\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"cone\",\"x\":0,\"y\":4,\"z\":0,\"u\":0,\"v\":4,\"w\":0,\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"colorscale\":[[0,1],[\"green\",\"green\"]],\"showscale\":false,\"name\":\"P2 y\",\"showlegend\":false,\"inherit\":true},\"ce61751bc5.4\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,0],\"z\":[0,5],\"line\":{\"color\":\"blue\",\"width\":6},\"name\":\"P3 y\",\"showlegend\":true,\"inherit\":true},\"ce61751bc5.5\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"cone\",\"x\":0,\"y\":0,\"z\":5,\"u\":0,\"v\":0,\"w\":5,\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"colorscale\":[[0,1],[\"blue\",\"blue\"]],\"showscale\":false,\"name\":\"P3 y\",\"showlegend\":false,\"inherit\":true},\"ce61751bc5.6\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,4],\"z\":[0,5],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"y\",\"showlegend\":true,\"inherit\":true},\"ce61751bc5.7\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"cone\",\"x\":3,\"y\":4,\"z\":5,\"u\":3,\"v\":4,\"w\":5,\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"colorscale\":[[0,1],[\"black\",\"black\"]],\"showscale\":false,\"name\":\"y\",\"showlegend\":false,\"inherit\":true},\"ce61751bc5.8\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[5,0],\"line\":{\"color\":\"rgba(255, 0, 0, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P1\",\"hoverinfo\":\"text\",\"text\":\"y -> P1\",\"inherit\":true},\"ce61751bc5.9\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[5,0],\"line\":{\"color\":\"rgba(0, 255, 0, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P2\",\"hoverinfo\":\"text\",\"text\":\"y -> P2\",\"inherit\":true},\"ce61751bc5.10\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,0],\"z\":[5,5],\"line\":{\"color\":\"rgba(0, 0, 255, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P3\",\"hoverinfo\":\"text\",\"text\":\"y -> P3\",\"inherit\":true},\"ce61751bc5.11\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,4],\"z\":[5,0],\"line\":{\"color\":\"purple\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P1+P2)\",\"hoverinfo\":\"text\",\"text\":\"y -> (P1+P2)\",\"inherit\":true},\"ce61751bc5.12\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[5,5],\"line\":{\"color\":\"orange\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P1+P3)\",\"hoverinfo\":\"text\",\"text\":\"y -> (P1+P3)\",\"inherit\":true},\"ce61751bc5.13\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[5,5],\"line\":{\"color\":\"cyan\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P2+P3)\",\"hoverinfo\":\"text\",\"text\":\"y -> (P2+P3)\",\"inherit\":true},\"ce61751bc5.14\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,6],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V1 (x)\",\"inherit\":true},\"ce61751bc5.15\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,6],\"z\":[0,0],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V2 (y)\",\"inherit\":true},\"ce61751bc5.16\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,0],\"z\":[0,6],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V3 (z)\",\"inherit\":true}},\"layout\":{\"width\":900,\"height\":700,\"margin\":{\"b\":0,\"l\":0,\"t\":50,\"r\":0},\"title\":\"Orthogonal Decomposition Geometry\",\"scene\":{\"xaxis\":{\"title\":\"V1\",\"range\":[0,6]},\"yaxis\":{\"title\":\"V2\",\"range\":[0,6]},\"zaxis\":{\"title\":\"V3\",\"range\":[0,6]},\"aspectmode\":\"cube\",\"camera\":{\"eye\":{\"x\":1.5,\"y\":1.5,\"z\":1.2}}},\"legend\":{\"x\":0.75,\"y\":0.90000000000000002},\"xaxis\":{\"title\":[]},\"yaxis\":{\"title\":[]},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"red\",\"width\":6},\"name\":\"P1 y\",\"showlegend\":true,\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null},{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"colorscale\":[[0,\"red\"],[1,\"red\"]],\"showscale\":false,\"type\":\"cone\",\"x\":[3],\"y\":[0],\"z\":[0],\"u\":[3],\"v\":[0],\"w\":[0],\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"name\":\"P1 y\",\"showlegend\":false,\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,4],\"z\":[0,0],\"line\":{\"color\":\"green\",\"width\":6},\"name\":\"P2 y\",\"showlegend\":true,\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"colorscale\":[[0,\"green\"],[1,\"green\"]],\"showscale\":false,\"type\":\"cone\",\"x\":[0],\"y\":[4],\"z\":[0],\"u\":[0],\"v\":[4],\"w\":[0],\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"name\":\"P2 y\",\"showlegend\":false,\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,0],\"z\":[0,5],\"line\":{\"color\":\"blue\",\"width\":6},\"name\":\"P3 y\",\"showlegend\":true,\"marker\":{\"color\":\"rgba(148,103,189,1)\",\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"colorscale\":[[0,\"blue\"],[1,\"blue\"]],\"showscale\":false,\"type\":\"cone\",\"x\":[0],\"y\":[0],\"z\":[5],\"u\":[0],\"v\":[0],\"w\":[5],\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"name\":\"P3 y\",\"showlegend\":false,\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,3],\"y\":[0,4],\"z\":[0,5],\"line\":{\"color\":\"black\",\"width\":6},\"name\":\"y\",\"showlegend\":true,\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"frame\":null},{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"colorscale\":[[0,\"black\"],[1,\"black\"]],\"showscale\":false,\"type\":\"cone\",\"x\":[3],\"y\":[4],\"z\":[5],\"u\":[3],\"v\":[4],\"w\":[5],\"sizemode\":\"absolute\",\"sizeref\":0.5,\"anchor\":\"tip\",\"name\":\"y\",\"showlegend\":false,\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[5,0],\"line\":{\"color\":\"rgba(255, 0, 0, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P1\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> P1\",\"y -> P1\"],\"marker\":{\"color\":\"rgba(188,189,34,1)\",\"line\":{\"color\":\"rgba(188,189,34,1)\"}},\"error_y\":{\"color\":\"rgba(188,189,34,1)\"},\"error_x\":{\"color\":\"rgba(188,189,34,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[5,0],\"line\":{\"color\":\"rgba(0, 255, 0, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P2\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> P2\",\"y -> P2\"],\"marker\":{\"color\":\"rgba(23,190,207,1)\",\"line\":{\"color\":\"rgba(23,190,207,1)\"}},\"error_y\":{\"color\":\"rgba(23,190,207,1)\"},\"error_x\":{\"color\":\"rgba(23,190,207,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,0],\"z\":[5,5],\"line\":{\"color\":\"rgba(0, 0, 255, 0.5)\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> P3\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> P3\",\"y -> P3\"],\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,4],\"z\":[5,0],\"line\":{\"color\":\"purple\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P1+P2)\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> (P1+P2)\",\"y -> (P1+P2)\"],\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,3],\"y\":[4,0],\"z\":[5,5],\"line\":{\"color\":\"orange\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P1+P3)\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> (P1+P3)\",\"y -> (P1+P3)\"],\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[3,0],\"y\":[4,4],\"z\":[5,5],\"line\":{\"color\":\"cyan\",\"width\":3,\"dash\":\"dash\"},\"name\":\"y -> (P2+P3)\",\"hoverinfo\":[\"text\",\"text\"],\"text\":[\"y -> (P2+P3)\",\"y -> (P2+P3)\"],\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,6],\"y\":[0,0],\"z\":[0,0],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V1 (x)\",\"marker\":{\"color\":\"rgba(148,103,189,1)\",\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,6],\"z\":[0,0],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V2 (y)\",\"marker\":{\"color\":\"rgba(140,86,75,1)\",\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"frame\":null},{\"type\":\"scatter3d\",\"mode\":\"lines\",\"x\":[0,0],\"y\":[0,0],\"z\":[0,6],\"line\":{\"color\":\"gray\",\"dash\":\"dot\",\"width\":2},\"name\":\"V3 (z)\",\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nOrthogonal decomposition geometry (R Plotly)\n:::\n:::\n\n\n::: {#thm-complete-decomposition name=\"Complete Orthogonal Decomposition of $\\\\mathbb{R}^n$\"}\nLet $P_0, P_1, \\dots, P_k$ be a sequence of orthogonal projection matrices with nested column spaces: $$\n\\text{Col}(P_0) \\subseteq \\text{Col}(P_1) \\subseteq \\dots \\subseteq \\text{Col}(P_k)\n$$\n\nDefine the sequence of difference matrices $\\Delta P_i$ and their column spaces $V_i$ as follows:\n\n\\begin{align*}\n\\Delta P_0 &= P_0, & V_0 &= \\text{Col}(\\Delta P_0) \\\\\n\\Delta P_i &= P_i - P_{i-1} \\quad (1 \\le i \\le k), & V_i &= \\text{Col}(\\Delta P_i) \\\\\n\\Delta P_{k+1} &= I - P_k, & V_{k+1} &= \\text{Col}(\\Delta P_{k+1})\n\\end{align*}\n\n**Conclusion:**\n\n1.  **Projection Property:** Each $\\Delta P_i$ is the orthogonal projection matrix onto $V_i$ for $i = 0, \\dots, k+1$.\n\n2.  **Mutual Orthogonality:** The collection $\\{\\Delta P_i\\}$ are mutually orthogonal operators: $$ \\Delta P_i \\Delta P_j = 0 \\quad \\text{for all } i \\ne j $$\n\n3.  **Direct Sum Decomposition:** The vector space $\\mathbb{R}^n$ is the direct sum of these orthogonal subspaces: $$ \\mathbb{R}^n = V_0 \\oplus V_1 \\oplus \\dots \\oplus V_{k+1} $$\n:::\n\n::: proof\n**1. Proof that** $\\Delta P_i$ is the Projection onto $V_i$ We must show each $\\Delta P_i$ is symmetric and idempotent.\n\n-   For $\\Delta P_0 = P_0$: True by definition.\n-   For $\\Delta P_i$ ($1 \\le i \\le k$):\n    -   **Symmetry:** Difference of symmetric matrices ($P_i, P_{i-1}$) is symmetric.\n    -   **Idempotency:** $(\\Delta P_i)^2 = (P_i - P_{i-1})^2 = P_i^2 - P_i P_{i-1} - P_{i-1} P_i + P_{i-1}^2$. Using nested properties ($P_i P_{i-1} = P_{i-1}$), this simplifies to $P_i - P_{i-1} = \\Delta P_i$.\n-   For $\\Delta P_{k+1} = I - P_k$:\n    -   **Symmetry:** $(I - P_k)' = I - P_k$.\n    -   **Idempotency:** $(I - P_k)^2 = I - 2P_k + P_k^2 = I - P_k$.\n\n**2. Proof of Mutual Orthogonality** We show $\\Delta P_j \\Delta P_i = 0$ for $i < j$.\n\n-   **Case 1: Both indices** $\\le k$ (i.e., $1 \\le i < j \\le k$): $$ (P_j - P_{j-1})(P_i - P_{i-1}) = P_j P_i - P_j P_{i-1} - P_{j-1} P_i + P_{j-1} P_{i-1} $$ Since $\\text{Col}(P_i) \\subseteq \\text{Col}(P_{j-1})$, all terms reduce to $P_i - P_{i-1} - P_i + P_{i-1} = 0$.\n\n-   **Case 2: One index is the residual** ($j = k+1$): We check $\\Delta P_{k+1} \\Delta P_i = (I - P_k)\\Delta P_i$ for any $i \\le k$. Since $V_i \\subseteq \\text{Col}(P_k)$, we have $P_k \\Delta P_i = \\Delta P_i$. $$ (I - P_k)\\Delta P_i = \\Delta P_i - P_k \\Delta P_i = \\Delta P_i - \\Delta P_i = 0 $$\n\n**3. Proof of Direct Sum** The sum of the difference matrices forms a telescoping series: $$ \\sum_{j=0}^{k+1} \\Delta P_j = P_0 + \\sum_{i=1}^k (P_i - P_{i-1}) + (I - P_k) $$ $$ = P_k + (I - P_k) = I $$ Since the identity operator $I$ (which maps $\\mathbb{R}^n$ to itself) is the sum of mutually orthogonal projection operators, the space $\\mathbb{R}^n$ decomposes into the direct sum of their respective image subspaces $V_i$.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Venn Diagram of Nested Projections with Colored Increments](lec1-vecspace_files/figure-html/fig-venn-nested-projection-1.png){#fig-venn-nested-projection fig-align='center' width=576 style=\"width: 80% !important;\"}\n:::\n:::\n\n",
    "supporting": [
      "lec1-vecspace_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.9/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.11.0/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.2/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.2/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}