{
  "hash": "95f032bdde9c01b25b0be7368b7a1f92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spectral Theory and Generalized Inverse\"\n---\n\n\n\n\nThis chapter covers a review of matrix algebra concepts essential for linear models, including eigenvalues, spectral decomposition, and generalized inverses.\n\n## Spectral Theory\n\n### Eigenvalues and Eigenvectors\n\n::: {#def-eigen name=\"Eigenvalues and Eigenvectors\"}\n\nFor a square matrix $A$ ($n \\times n$), a scalar $\\lambda$ is an **eigenvalue** and a non-zero vector $x$ is the corresponding **eigenvector** if:\n\n$$\nAx = \\lambda x \\iff (A - \\lambda I_n)x = 0\n$$\n\nThe eigenvalues are found by solving the characteristic equation:\n$$\n|A - \\lambda I_n| = 0\n$$\n:::\n\n\n### Quadratic Form\n:::{#def-quadratic-form}\n\nA **quadratic form** in $n$ variables $x_1, x_2, \\dots, x_n$ is a scalar function defined by a symmetric matrix $A$:\n$$\nQ(x) = x'Ax = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n$$\n:::\n\n### Positive and Non-Negative Definite Matrices\n\n:::{#def-pos-def name=\"Positive and Non-Negative Definite Matrices\"}\n\nA symmetric matrix $A$ is **positive definite (p.d.)** if:\n$$\nx'Ax > 0 \\quad \\forall x \\ne 0\n$$\nIt is **non-negative definite (n.n.d.)** if:\n$$\nx'Ax \\ge 0 \\quad \\forall x\n$$\n:::\n\n:::{#thm-nnd-properties name=\"Properties of Definite Matrices\"}\n\nLet $A$ be a symmetric $n \\times n$ matrix with eigenvalues $\\lambda_1, \\dots, \\lambda_n$.\n\n1.  **Eigenvalue Characterization:**\n    * $A$ is p.d. $\\iff$ all $\\lambda_i > 0$.\n    * $A$ is n.n.d. $\\iff$ all $\\lambda_i \\ge 0$.\n\n2.  **Determinant and Inverse:**\n    * If $A$ is p.d., then $|A| > 0$ and $A^{-1}$ exists.\n    * If $A$ is n.n.d. and singular, then $|A| = 0$ (at least one $\\lambda_i = 0$).\n\n3.  **Gram Matrices ($B'B$):**\n    Let $B$ be an $n \\times p$ matrix.\n\n    * If $\\text{rank}(B) = p$, then $B'B$ is p.d.\n    * If $\\text{rank}(B) < p$, then $B'B$ is n.n.d.\n:::\n\n### Properties of Symmetric Matrices\n:::{#thm-symmetric-properties name=\"Properties of Symmetric Matrices\"}\nLet $A$ be a symmetric matrix with spectral decomposition $A = Q \\Lambda Q'$. The following properties hold:\n\n1.  **Trace:** $\\text{tr}(A) = \\sum \\lambda_i$.\n2.  **Determinant:** $|A| = \\prod \\lambda_i$.\n3.  **Singularity:** $A$ is singular if and only if at least one $\\lambda_i = 0$.\n4.  **Inverse:** If $A$ is non-singular ($\\lambda_i \\ne 0$), then $A^{-1} = Q \\Lambda^{-1} Q'$.\n5.  **Powers:** $A^k = Q \\Lambda^k Q'$.\n    * *Square Root:* $A^{1/2} = Q \\Lambda^{1/2} Q'$ (if $\\lambda_i \\ge 0$).\n6.  **Spectral Representation of Quadratic Forms:** The quadratic form $x'Ax$ can be diagonalized using the eigenvectors of $A$:\n    $$\n    x'Ax = x' Q \\Lambda Q' x = y' \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2\n    $$\n    where $y = Q'x$ represents a rotation of the coordinate system.\n:::\n\n### Spectral Representation of Projection Matrices \n\nWe revisit projection matrices in the context of eigenvalues.\n\n::: {#thm-proj-eigen name=\"Eigenvalues of Projection Matrices\"}\n\nA symmetric matrix $P$ is a projection matrix (idempotent, $P^2=P$) if and only if its eigenvalues are either 0 or 1.\n\n$$\nP^2 x = \\lambda^2 x \\quad \\text{and} \\quad Px = \\lambda x \\implies \\lambda^2 = \\lambda \\implies \\lambda \\in \\{0, 1\\}\n$$\n:::\n\nFor a projection matrix $P$:\n\n* If $x \\in \\text{Col}(P)$, $Px = x$ (Eigenvalue 1).\n* If $x \\perp \\text{Col}(P)$, $Px = 0$ (Eigenvalue 0).\n* $\\text{rank}(P) = \\text{tr}(P) = \\sum \\lambda_i$ (Count of 1s).\n\n::: {#exm-trace-P}\nFor $P = \\frac{1}{n} J_n J_n'$, the rank is $\\text{tr}(P) = 1$.\n:::\n\n\n\n\n-----\n\n\n### Singular Value Decomposition (SVD)\n\n:::{#thm-svd name=\"Singular Value Decomposition (SVD)\"}\n\nLet $X$ be an $n \\times p$ matrix with rank $r \\le \\min(n, p)$. $X$ can be decomposed into the product of three matrices:\n\n$$\nX = U \\mathbf{D} V'\n$$\n\n**1. Partitioned Matrix Form**\n\n$$\nX = \\underset{n \\times n}{(U_1, U_2)}\n\\begin{pmatrix}\n\\Lambda_r & O_{r \\times (p-r)} \\\\\nO_{(n-r) \\times r} & O_{(n-r) \\times (p-r)}\n\\end{pmatrix}\n\\underset{p \\times p}{\n\\begin{pmatrix}\nV_1' \\\\\nV_2'\n\\end{pmatrix}\n}\n$$\n\n**2. Detailed Matrix Form**\n\nExpanding the diagonal matrix explicitly:\n\n$$\nX = \\underset{n \\times n}{(u_1, \\dots, u_n)}\n\\left(\n\\begin{array}{cccc|c}\n\\lambda_1 & 0 & \\dots & 0 &  \\\\\n0 & \\lambda_2 & \\dots & 0 & O_{12} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots &  \\\\\n0 & 0 & \\dots & \\lambda_r &  \\\\\n\\hline\n & O_{21} & & & O_{22}\n\\end{array}\n\\right)\n\\underset{p \\times p}{\n\\begin{pmatrix}\nv_1' \\\\\n\\vdots \\\\\nv_p'\n\\end{pmatrix}\n}\n$$\n\n**3. Reduced Form**\n\n$$\nX = U_1 \\Lambda_r V_1' = \\sum_{i=1}^r \\lambda_i u_i v_i'\n$$\n\n**Properties:**\n\n1.  **Singular Values ($\\Lambda_r$):** $\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)$ contains the singular values ($\\lambda_i > 0$), which are the square roots of the non-zero eigenvalues of $X'X$.\n2.  **Orthogonality:**\n    * $U$ is $n \\times n$ orthogonal ($U'U = I_n$).\n    * $V$ is $p \\times p$ orthogonal ($V'V = I_p$).\n:::\n\n#### Connection to Gram Matrices\n\nThe matrices $U$ and $V$ provide the basis vectors (eigenvectors) for the Gram matrices of $X$.\n\n1.  **Right Singular Vectors ($V$):**\n    The columns of $V$ are the eigenvectors of the Gram matrix $X'X$.\n    $$\n    X'X = (U \\Lambda V')' (U \\Lambda V') = V \\Lambda U' U \\Lambda V' = V \\Lambda^2 V'\n    $$\n\n    * The eigenvalues of $X'X$ are the squared singular values $\\lambda_i^2$.\n\n2.  **Left Singular Vectors ($U$):**\n    The columns of $U$ are the eigenvectors of the Gram matrix $XX'$.\n    $$\n    XX' = (U \\Lambda V') (U \\Lambda V')' = U \\Lambda V' V \\Lambda U' = U \\Lambda^2 U'\n    $$\n\n    * The eigenvalues of $XX'$ are also $\\lambda_i^2$ (for non-zero values).\n\n#### Numerical Example\n\nConsider the matrix $X = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}$.\n\n1.  **Compute $X'X$ and find $V$:**\n    $$\n    X'X = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 5 \\\\ 5 & 5 \\end{pmatrix}\n    $$\n\n    * Eigenvalues of $X'X$: Trace is 10, Determinant is 0. Thus, $\\mu_1 = 10, \\mu_2 = 0$.\n    * **Singular Values:** $\\lambda_1 = \\sqrt{10}, \\lambda_2 = 0$.\n    * Eigenvector for $\\mu_1=10$: Normalized $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n    * Eigenvector for $\\mu_2=0$: Normalized $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n    * Therefore, $V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$.\n\n2.  **Compute $XX'$ and find $U$:**\n    $$\n    XX' = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n    $$\n\n    * Eigenvalues are again 10 and 0.\n    * Eigenvector for $\\mu_1=10$: Normalized $u_1 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n    * Eigenvector for $\\mu_2=0$: Normalized $u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\n    * Therefore, $U = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & 2 \\\\ 2 & -1 \\end{pmatrix}$.\n\n3.  **Verification:**\n    $$\n    X = \\sqrt{10} u_1 v_1' = \\sqrt{10} \\begin{pmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\n    $$\n\n## Cholesky Decomposition\n\nA symmetric matrix $A$ has a Cholesky decomposition if and only if it is **non-negative definite** (i.e., $x'Ax \\ge 0$ for all $x$).\n\n$$\nA = B'B\n$$\n\nwhere $B$ is an **upper triangular** matrix with non-negative diagonal entries.\n\n### Matrix Representation of the Algorithm\n\nTo derive the algorithm, we equate the elements of $A$ with the product of the lower triangular matrix $B'$ and the upper triangular matrix $B$.\n\nFor a $3 \\times 3$ matrix, this looks like:\n\n$$\n\\underbrace{\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}}_{A}\n=\n\\underbrace{\\begin{pmatrix}\nb_{11} & 0 & 0 \\\\\nb_{12} & b_{22} & 0 \\\\\nb_{13} & b_{23} & b_{33}\n\\end{pmatrix}}_{B'}\n\\underbrace{\\begin{pmatrix}\nb_{11} & b_{12} & b_{13} \\\\\n0 & b_{22} & b_{23} \\\\\n0 & 0 & b_{33}\n\\end{pmatrix}}_{B}\n$$\n\nMultiplying the matrices on the right yields the system of equations:\n\n$$\nA = \\begin{pmatrix}\n\\mathbf{b_{11}^2} & b_{11}b_{12} & b_{11}b_{13} \\\\\nb_{12}b_{11} & \\mathbf{b_{12}^2 + b_{22}^2} & b_{12}b_{13} + b_{22}b_{23} \\\\\nb_{13}b_{11} & b_{13}b_{12} + b_{23}b_{22} & \\mathbf{b_{13}^2 + b_{23}^2 + b_{33}^2}\n\\end{pmatrix}\n$$\n\nBy solving for the bolded diagonal terms and substituting known values from previous rows, we get the recursive algorithm.\n\n### The Algorithm\n\n1.  **Row 1:** Solve for $b_{11}$ using $a_{11}$, then solve the rest of the row ($b_{1j}$) by division.\n    * $b_{11} = \\sqrt{a_{11}}$\n    * $b_{1j} = a_{1j}/b_{11}$\n\n2.  **Row 2:** Solve for $b_{22}$ using $a_{22}$ and the known $b_{12}$, then solve $b_{2j}$.\n    * $b_{22} = \\sqrt{a_{22} - b_{12}^2}$\n    * $b_{2j} = (a_{2j} - b_{12}b_{1j}) / b_{22}$\n\n3.  **Row 3:** Solve for $b_{33}$ using $a_{33}$ and the known $b_{13}, b_{23}$.\n    * $b_{33} = \\sqrt{a_{33} - b_{13}^2 - b_{23}^2}$\n\n### Numerical Example\n\nConsider the positive definite matrix $A$:\n$$\nA = \\begin{pmatrix}\n4 & 2 & -2 \\\\\n2 & 10 & 2 \\\\\n-2 & 2 & 6\n\\end{pmatrix}\n$$\n\nWe find $B$ such that $A = B'B$:\n\n1.  **First Row of B ($b_{11}, b_{12}, b_{13}$):**\n    * $b_{11} = \\sqrt{4} = 2$\n    * $b_{12} = 2 / 2 = 1$\n    * $b_{13} = -2 / 2 = -1$\n\n2.  **Second Row of B ($b_{22}, b_{23}$):**\n    * $b_{22} = \\sqrt{10 - (1)^2} = \\sqrt{9} = 3$\n    * $b_{23} = (2 - (1)(-1)) / 3 = 3/3 = 1$\n\n3.  **Third Row of B ($b_{33}$):**\n    * $b_{33} = \\sqrt{6 - (-1)^2 - (1)^2} = \\sqrt{4} = 2$\n\n**Result:**\n$$\nB = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n$$\n\n## Generalized Inverses\n\n### Motivation\n\nConsider the linear system $X\\beta = y$.\nIn $\\mathbb{R}^2$, if $X = [x_1, x_2]$ is invertible, the solution is unique: $\\beta = X^{-1}y$. This satisfies $X(X^{-1}y) = y$.However, if $X$ is not square or not invertible (e.g., $X$ is $2 \\times 3$), $X\\beta = y$ does not have a unique solution. We seek a matrix $G$ such that $\\beta = Gy$ provides a solution whenever $y \\in C(X)$ (the column space of X). Substituting $\\beta = Gy$ into the equation $X\\beta = y$:\n$$\nX(Gy) = y \\quad \\forall y \\in C(X)\n$$\nSince any $y \\in C(X)$ can be written as $Xw$ for some vector $w$:\n$$\nXGXw = Xw \\quad \\forall w\n$$\nThis implies the defining condition:\n$$\nXGX = X\n$$\n\n\n### Definition of Generalized Inverse\n\n::: {#def-gen-inverse name=\"Generalized Inverse\"}\nLet $X$ be an $n \\times p$ matrix. A matrix $X^-$ of size $p \\times n$ is called a **generalized inverse** of $X$ if it satisfies:\n$$\nXX^-X = X\n$$\n:::\n\n\n\n:::{#exm-ginverse name=\"Examples of Generalized Inverse\"}\n\n* **Example 1: Diagonal Matrix**\n    If $X = \\text{diag}(\\lambda_1, \\lambda_2, 0, 0)$, we can write it in matrix form as:\n    $$\n    X = \\begin{pmatrix}\n    \\lambda_1 & 0 & 0 & 0 \\\\\n    0 & \\lambda_2 & 0 & 0 \\\\\n    0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0\n    \\end{pmatrix}\n    $$\n    A generalized inverse is obtained by inverting the non-zero elements:\n    $$\n    X^- = \\begin{pmatrix}\n    \\lambda_1^{-1} & 0 & 0 & 0 \\\\\n    0 & \\lambda_2^{-1} & 0 & 0 \\\\\n    0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0\n    \\end{pmatrix}\n    $$\n\n* **Example 2: Row Vector**\n    Let $X = (1, 2, 3)$.\n    One possible generalized inverse is a column vector where the first element is the reciprocal of the first non-zero element of $X$ (which is $1$), and others are zero:\n    $$\n    X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n    $$\n    **Verification:**\n    $$\n    XX^-X = (1, 2, 3) \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1, 2, 3) = (1) \\cdot (1, 2, 3) = (1, 2, 3) = X\n    $$\n    Other valid generalized inverses include $\\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}$ or $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}$.\n\n* **Example 3: Rank Deficient Matrix**\n    Let $A = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix}$.\n    Note that Row 3 = Row 1 + Row 2, so Rank$(A) = 2$.\n\n    **Solution:**\n    A generalized inverse can be found by locating a non-singular $2 \\times 2$ submatrix, inverting it, and padding the rest with zeros.\n    Let's take the top-left minor $M = \\begin{pmatrix} 2 & 2 \\\\ 1 & 0 \\end{pmatrix}$.\n    The inverse is $M^{-1} = \\frac{1}{-2}\\begin{pmatrix} 0 & -2 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0.5 & -1 \\end{pmatrix}$.\n\n    Placing this in the corresponding position in $A^-$ and setting the rest to 0:\n    $$\n    A^- = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n\n    **Verification ($AA^-A = A$):**\n    First, compute $AA^-$:\n    $$\n    AA^- = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0.5 & -1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix}\n    $$\n    Then multiply by $A$:\n    $$\n    (AA^-)A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 3 & 2 & 4 \\end{pmatrix} = A\n    $$\n:::\n\n### A Procedure to Find a Generalized Inverse\n\nIf we can partition $X$ (possibly after permuting rows/columns) such that $R_{11}$ is a non-singular rank $r$ submatrix:\n\n$$\nX = \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix}\n$$\n\nThen a generalized inverse is:\n\n$$\nX^- = \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\n**Verification:**\n\n$$\n\\begin{aligned}\nXX^-X &= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\begin{pmatrix} R_{11}^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} I_r & 0 \\\\ R_{21}R_{11}^{-1} & 0 \\end{pmatrix} \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} R_{11} & R_{12} \\\\ R_{21} & R_{21}R_{11}^{-1}R_{12} \\end{pmatrix}\n\\end{aligned}\n$$\nNote that since rank$(X) = \\text{rank}(R_{11})$, the rows of $[R_{21}, R_{22}]$ are linear combinations of $[R_{11}, R_{12}]$, implying $R_{22} = R_{21}R_{11}^{-1}R_{12}$. Thus, $XX^-X = X$.\n\n**An Algorithm for Finding a Generalized Inverse**\n\nA systematic procedure to find a generalized inverse $A^-$ for any matrix $A$:\n\n1.  Find any non-singular $r \\times r$ submatrix $C$, where $r$ is the rank of $A$. It is not necessary for the elements of $C$ to occupy adjacent rows and columns in $A$.\n2.  Find $C^{-1}$ and $(C^{-1})'$.\n3.  Replace the elements of $C$ in $A$ with the elements of $(C^{-1})'$.\n4.  Replace all other elements in $A$ with zeros.\n5.  Transpose the resulting matrix.\n\n**Matrix Visual Representation**\n$$\n\\underset{\\text{Original } A}{\\begin{pmatrix}\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\otimes & \\times & \\otimes \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{with } (C^{-1})']{\\text{Replace } C}\n\\underset{\\text{Intermediate}}{\\begin{pmatrix}\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\triangle & \\times & \\triangle \\\\\n\\times & \\times & \\times & \\times\n\\end{pmatrix}}\n\\xrightarrow[\\text{Result}]{\\text{Transpose}}\n\\underset{\\text{Final } A^-}{\\begin{pmatrix}\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times \\\\\n\\times & \\times & \\times \\\\\n\\square & \\square & \\times\n\\end{pmatrix}}\n$$\n\n**Legend:**\n\n* $\\otimes$: Elements of submatrix $C$\n* $\\triangle$: Elements of $(C^{-1})'$\n* $\\square$: Elements of $C^{-1}$ (after transposition)\n* $\\times$: Other elements (replaced by 0 in the final calculation)\n\n### Moore-Penrose Inverse\n\nThe Moore-Penrose inverse (denoted $X^+$) is a unique generalized inverse defined via Singular Value Decomposition (SVD).\n\nIf $X$ has SVD:\n$$\nX = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n$$\n\nThen the Moore-Penrose inverse is:\n$$\nX^+ = V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U'\n$$\n\nwhere $\\Lambda_r = \\text{diag}(\\lambda_1, \\dots, \\lambda_r)$ contains the singular values. Unlike standard generalized inverses, $X^+$ is unique.\n\n**Verification:**\n\nWe verify that $X^+$ satisfies the condition $XX^+X = X$.\n\n1.  **Substitute definitions:**\n    $$\n    XX^+X = \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right] \\left[ V \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} U' \\right] \\left[ U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' \\right]\n    $$\n\n2.  **Apply orthogonality:**\n    Recall that $V'V = I$ and $U'U = I$.\n    $$\n    = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(V'V)}_{I} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\underbrace{(U'U)}_{I} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V'\n    $$\n\n3.  **Multiply diagonal matrices:**\n    $$\n    = U \\left[ \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right] V'\n    $$\n    Since $\\Lambda_r \\Lambda_r^{-1} \\Lambda_r = I \\cdot \\Lambda_r = \\Lambda_r$:\n    $$\n    = U \\begin{pmatrix} \\Lambda_r & 0 \\\\ 0 & 0 \\end{pmatrix} V' = X\n    $$\n\n### Solving Linear Systems with Generalized Inverse\n\nWe apply generalized inverses to solve systems of linear equations $X\\beta = c$ where $X$ is $n \\times p$.\n\n::: {#def-consistency name=\"Consistency and Solution\"}\nThe system $X\\beta = c$ is consistent if and only if $c \\in \\mathcal{C}(X)$ (the column space of $X$). If consistent, $\\beta = X^- c$ is a solution.\n:::\n\n**Proof:**\nIf the system is consistent, there exists some $b$ such that $Xb = c$.\nUsing the definition $XX^-X = X$:\n$$\nX(X^- c) = X(X^- X b) = (XX^-X)b = Xb = c\n$$\nThus, $X^-c$ is a solution. Note that the solution is not unique if $X$ is not full rank.\n\n\n:::{#exm-gi-sol-ls name=\"Examples of Solutions of Linear System with Generalized Inverse\"}\n\n* **Example 1: Underdetermined System**\n\n  Let $X = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}$ and we want to solve $X\\beta = 4$.\n  \n  **Solution 1:**\n  Using the generalized inverse $X^- = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$:\n  $$\n  \\beta = X^- \\cdot 4 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} 4 = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix}\n  $$\n  **Verification:**\n  $$\n  X\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1(4) + 2(0) + 3(0) = 4 \\quad \\checkmark\n  $$\n  \n  **Solution 2:**\n  Using another generalized inverse $X^- = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix}$:\n  $$\n  \\beta = X^- \\cdot 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\end{pmatrix} 4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix}\n  $$\n  **Verification:**\n  $$\n  X\\beta = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 4/3 \\end{pmatrix} = 0 + 0 + 3(4/3) = 4 \\quad \\checkmark\n  $$\n  \n* **Example 2: Overdetermined System**\n\n  Let $X = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$. Solve $X\\beta = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c$.\n  Here $c = 2X$, so the system is consistent. Since $X$ is a column vector, $\\beta$ is a scalar.\n  \n  **Solution:**\n  Using the generalized inverse $X^- = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$:\n  $$\n  \\beta = X^- c = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = 1(2) + 0(4) + 0(6) = 2\n  $$\n  **Verification:**\n  $$\n  X\\beta = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} (2) = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix} = c \\quad \\checkmark\n  $$\n:::\n\n\n\n## Least Squares for Non-full-rank $X$ with Generalized Inverse\n\n### Projection Matrix with Generalized Inverse of $X'X$\n\nFor the normal equations $(X'X)\\beta = X'y$, a solution is given by:\n$$\n\\hat{\\beta} = (X'X)^- X'y\n$$\nThe fitted values are $$\\hat{y} = X\\hat{\\beta} = X(X'X)^- X'y.$$\nThis $\\hat{y}$ represents the unique orthogonal projection of $y$ onto $\\text{Col}(X)$.\n\n\n\n### Invariance and Uniqueness of \"the\" Projection Matrix\n\n\n\n:::{#thm-transpose name=\"Transpose Property of Generalized Inverses\"}\n$(X^-)'$ is a version of $(X')^-$. That is, $(X^-)'$ is a generalized inverse of $X'$.\n:::\n\n::: {.callout-note collapse=\"true\" icon=\"false\" title=\"Proof\"}\n\nBy definition, a generalized inverse $X^-$ satisfies the property:\n$$\nX X^- X = X\n$$\n\nTo verify that $(X^-)'$ is a generalized inverse of $X'$, we need to show that it satisfies the condition $A G A = A$ where $A = X'$ and $G = (X^-)'$.\n\n1.  Start with the fundamental definition:\n    $$\n    X X^- X = X\n    $$\n\n2.  Take the transpose of both sides of the equation:\n    $$\n    (X X^- X)' = X'\n    $$\n\n3.  Apply the reverse order law for transposes, $(ABC)' = C' B' A'$:\n    $$\n    X' (X^-)' X' = X'\n    $$\n\nSince substituting $(X^-)'$ into the generalized inverse equation for $X'$ yields $X'$, $(X^-)'$ is a valid generalized inverse of $X'$.\n:::\n:::{#lem-invariance name=\"Invariance of Generalized Least Squares\"}\nFor any version of the generalized inverse $(X'X)^-$, the matrix $X'(X'X)^- X'$ is invariant and equals $X'$.\n$$\nX'X(X'X)^- X' = X'\n$$\n:::\n\n**Proof (using Projection):**\nLet $P = X(X'X)^- X'$. This is the projection matrix onto $\\mathcal{C}(X)$.\nBy definition of projection, $Px = x$ for any $x \\in \\text{Col}(X)$.\nSince columns of $X$ are in $\\text{Col}(X)$, $PX = X$.\nTaking the transpose: $(PX)' = X' \\implies X'P' = X'$.\nSince projection matrices are symmetric ($P=P'$), $X'P = X'$.\nSubstituting $P$: $X' X (X'X)^- X' = X'$.\n\n**Proof (Direct Matrix Manipulation):**\nDecompose $y = X\\beta + e$ where $e \\perp \\text{Col}(X)$ (i.e., $X'e = 0$).\n$$\n\\begin{aligned}\nX'X(X'X)^- X' y &= X'X(X'X)^- X' (X\\beta + e) \\\\\n&= X'X(X'X)^- X'X\\beta + X'X(X'X)^- X'e\n\\end{aligned}\n$$\nUsing the property $A A^- A = A$ (where $A=X'X$), the first term becomes $X'X\\beta$.\nThe second term is 0 because $X'e = 0$.\nThus, the expression simplifies to $X'X\\beta = X'(X\\beta) = X'\\hat{y}_{\\text{proj}}$.\nThis implies the operator acts as $X'$.\n\n:::{#thm-proj-properties name=\"Properties of Projection Matrix $P$\"}\nLet $P = X(X'X)^- X'$. This matrix has the following properties:\n\n1.  **Symmetry:** $P = P'$.\n2.  **Idempotence:** $P^2 = P$.\n    $$\n    P^2 = X(X'X)^- X' X(X'X)^- X' = X(X'X)^- (X'X (X'X)^- X')\n    $$\n    Using the identity from @lem-invariance ($X'X(X'X)^- X' = X'$), this simplifies to:\n    $$\n    X(X'X)^- X' = P\n    $$\n\n3.  **Uniqueness:** $P$ is unique and invariant to the choice of the generalized inverse $(X'X)^-$.\n:::\n\n::: {.callout-note collapse=\"true\" icon=\"false\" title=\"Proof\"}\n**Proof of Uniqueness:**\n\nLet $A$ and $B$ be two different generalized inverses of $X'X$. Define $P_A = X A X'$ and $P_B = X B X'$.\nFrom @lem-invariance, we know that $X' P_A = X'$ and $X' P_B = X'$.\n\nSubtracting these two equations:\n$$\nX' (P_A - P_B) = 0\n$$\nTaking the transpose, we get $(P_A - P_B) X = 0$. This implies that the columns of the difference matrix $D = P_A - P_B$ are orthogonal to the columns of $X$ (i.e., $D \\perp \\text{Col}(X)$).\n\nHowever, by definition, the columns of $P_A$ and $P_B$ (and thus $D$) are linear combinations of the columns of $X$ (i.e., $D \\in \\text{Col}(X)$).\n\nThe only matrix that lies *in* the column space of $X$ but is also *orthogonal* to the column space of $X$ is the zero matrix.\nTherefore:\n$$\nP_A - P_B = 0 \\implies P_A = P_B\n$$\n:::\n\n## The Left Inverse View: Recovering $\\hat{\\beta}$ from $\\hat{y}$\n\nWhile the geometric properties of the linear model are most naturally established via the unique orthogonal projection $\\hat{y}$, we require a functional mapping—a statistical \"bridge\"—to translate the distribution of these fitted values back into the parameter space of $\\hat{\\beta}$. This bridge is provided by the generalized left inverse.\n\n### The Generalized Left Inverse\n\nTo recover the parameter estimates directly from the fitted values, we define the generalized left inverse, denoted as $X_{\\text{left}}^-$, such that:\n\n$$\n\\hat{\\beta} = X_{\\text{left}}^- \\hat{y}\n$$\n\nA standard choice for this operator, derived from the normal equations, is:\n\n$$\nX_{\\text{left}}^- = (X' X)^- X'\n$$\n\nWhen $X$ is full-rank, the $X_{\\text{left}}^-$ is unique, which is given by \n\n$$\nX_{\\text{left}}^- = (X' X)^{-1} X'\n$$\n\n### Verification of the Inverse Property\n\nTo verify that $X_{\\text{left}}^-$ acts as a valid generalized inverse of $X$, it must satisfy the condition $X X_{\\text{left}}^- X = X$. Substituting our definition:\n\n$$\nX \\underbrace{\\left[ (X' X)^- X' \\right]}_{X_{\\text{left}}^-} X = X (X' X)^- (X' X)\n$$\n\nUsing the property of generalized inverses for symmetric matrices where $(X' X)(X' X)^- X' = X'$, the transpose of this identity gives $X (X' X)^- (X' X) = X$. Thus, the condition holds:\n\n$$\nX X_{\\text{left}}^- X = X\n$$\n\n### Recovering the Estimator\n\nWe can now demonstrate that applying this left inverse to the fitted values $\\hat{y}$ yields the standard solution to the normal equations.\n\nSubstituting the projection formula $\\hat{y} = X(X' X)^- X' y$:\n\n$$\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\left[ (X' X)^- X' \\right] \\left[ X(X' X)^- X' y \\right] \\\\\n&= (X' X)^- \\underbrace{(X' X) (X' X)^- (X' X)}_{\\text{Property } A A^- A = A} (X' X)^- X' y\n\\end{aligned}\n$$\n\nSimplifying using the generalized inverse property $A^- A A^- = A^-$ (where $A = X' X$):\n\n$$\n\\begin{aligned}\nX_{\\text{left}}^- \\hat{y} &= \\underbrace{(X' X)^- (X' X) (X' X)^-}_{(X' X)^-} X' y \\\\\n&= (X' X)^- X' y\n\\end{aligned}\n$$\n\nThus, we recover the standard estimator used in the normal equations:\n\n$$\n\\mathbf{\\hat{\\beta} = (X' X)^- X' y}\n$$\n\n\n## Non-full-rank Least Squares with QR Decomposition\n\nWhen $X$ has rank $r < p$ (where $X$ is $n \\times p$), we can derive the least squares estimator using partitioned matrices.\n\nAssume the first $r$ columns of $X$ are linearly independent. We can partition $X$ as:\n$$\nX = Q (R_1, R_2)\n$$\nwhere $Q$ is an $n \\times r$ matrix with orthogonal columns ($Q'Q = I_r$), $R_1$ is an $r \\times r$ non-singular matrix, and $R_2$ is $r \\times (p-r)$.\n\nThe normal equations are:\n$$\nX'X\\beta = X'y \\implies \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q' Q (R_1, R_2) \\beta = \\begin{pmatrix} R_1' \\\\ R_2' \\end{pmatrix} Q'y\n$$\nSimplifying ($Q'Q = I_r$):\n$$\n\\begin{pmatrix} R_1'R_1 & R_1'R_2 \\\\ R_2'R_1 & R_2'R_2 \\end{pmatrix} \\beta = \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n$$\n\n### Constructing a Solution by Solving Normal Equations\n\n\nOne specific generalized inverse of $X'X$ can be found by focusing on the non-singular block $R_1'R_1$:\n$$\n(X'X)^- = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\nUsing this generalized inverse, the estimator $\\hat{\\beta}$ becomes:\n$$\n\\hat{\\beta} = (X'X)^- X'y = \\begin{pmatrix} (R_1'R_1)^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_1'Q'y \\\\ R_2'Q'y \\end{pmatrix}\n$$\n$$\n\\hat{\\beta} = \\begin{pmatrix} (R_1'R_1)^{-1} R_1' Q'y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix}\n$$\n\nThe fitted values are:\n$$\n\\hat{y} = X\\hat{\\beta} = Q(R_1, R_2) \\begin{pmatrix} R_1^{-1} Q'y \\\\ 0 \\end{pmatrix} = Q R_1 R_1^{-1} Q'y = QQ'y\n$$\nThis confirms that $\\hat{y}$ is the projection of $y$ onto the column space of $Q$ (which is the same as the column space of $X$).\n\n\n### Constructing a Solution by Solving Reparametrized $\\beta$\n\nWe can view the model as:\n$$\ny = Q(R_1, R_2)\\beta + \\epsilon = Qb + \\epsilon\n$$\nwhere $b = R_1\\beta_1 + R_2\\beta_2$.\n\nSince the columns of $Q$ are orthogonal, the least squares estimate for $b$ is simply:\n$$\n\\hat{b} = (Q'Q)^{-1}Q'y = Q'y\n$$\n\nTo find $\\beta$, we solve the underdetermined system:\n$$\nR_1\\beta_1 + R_2\\beta_2 = \\hat{b} = Q'y\n$$\n\n\n**Solution 1:**\nSet $\\beta_2 = 0$. Then:\n$$\nR_1\\beta_1 = Q'y \\implies \\hat{\\beta}_1 = R_1^{-1}Q'y\n$$\nThis yields the same result as the generalized inverse method above: $\\hat{\\beta} = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}$.\n\n**Solution 2:**\nUsing the generalized inverse of $R = (R_1, R_2)$:\n$$\nR^- = \\begin{pmatrix} R_1^{-1} \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta} = R^- Q'y = \\begin{pmatrix} R_1^{-1}Q'y \\\\ 0 \\end{pmatrix}\n$$\nThis demonstrates that finding a solution to the normal equations using $(X'X)^-$ is equivalent to solving the reparameterized system $b = R\\beta$.\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "lec2-matrix_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}