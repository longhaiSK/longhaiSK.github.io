{
  "hash": "87e63687111b685a2643e7f712a63f10",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: \n  html: default\n  pdf: default\n---\n\n# Introduction {.numbered}\n\n## Multiple Linear Regression \n\nSuppose we have observations on $Y$ and $X_j$. The data can be represented in matrix form.\n\n$$\n\\underset{n \\times 1}{y} = \\underset{n \\times p}{X} \\beta + \\underset{n \\times 1}{\\epsilon}\n$$\n\nwhere the error terms are distributed as: $$\n\\epsilon \\sim N_n(0, \\sigma^2 I_n),\n$$\n\nin which $I_n$ is the identity matrix: $$\nI_n = \\begin{pmatrix} \n1 & 0 & \\dots & 0 \\\\ \n0 & 1 & \\dots & 0 \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n0 & 0 & \\dots & 1 \n\\end{pmatrix}\n$$ The scalar equation for a single observation is: $$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\epsilon_i\n$$\n\n## Examples \n\n### Polynomial Regression \n\nPolynomial regression fits a curved line to the data points but remains linear in the parameters ($\\beta$).\n\nThe model equation is: $$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_{p-1} x_i^{p-1}\n$$\n\n### Design Matrix Construction \n\nThe design matrix $X$ is constructed by taking powers of the input variable.\n\n$$\ny = \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} = \n\\begin{pmatrix} \n1 & x_1 & x_1^2 & \\dots & x_1^{p-1} \\\\ \n1 & x_2 & x_2^2 & \\dots & x_2^{p-1} \\\\ \n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n1 & x_n & x_n^2 & \\dots & x_n^{p-1} \n\\end{pmatrix} \n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} + \n\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n$$\n\n### One-Way ANOVA \n\nANOVA can be expressed as a linear model using categorical predictors (dummy variables).\n\nSuppose we have 3 groups ($G_1, G_2, G_3$) with observations: $$\nY_{ij} = \\mu_i + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0, \\sigma^2)\n$$\n\n$$\n\\overset{G_1}{\n  \\boxed{\n    \\begin{matrix} Y_{11} \\\\ Y_{12} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_2}{\n  \\boxed{\n    \\begin{matrix} Y_{21} \\\\ Y_{22} \\end{matrix}\n  }\n}\n\\quad\n\\overset{G_3}{\n  \\boxed{\n    \\begin{matrix} Y_{31} \\\\ Y_{32} \\end{matrix}\n  }\n}\n$$\n\nWe construct the matrix $X$ to select the group mean ($\\mu$) corresponding to the observation:\n\n$$\n\\underset{6 \\times 1}{y} = \\underset{6 \\times 3}{X} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} + \\epsilon\n$$\n\n$$\n\\begin{bmatrix}\nY_{11} \\\\ Y_{12} \\\\ Y_{21} \\\\ Y_{22} \\\\ Y_{31} \\\\ Y_{32}\n\\end{bmatrix} = \n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\ \\mu_2 \\\\ \\mu_3\n\\end{bmatrix} + \\epsilon\n$$\n\n### Analysis of Covariance (ANCOVA) \n\nANCOVA combines continuous variables and categorical (dummy) variables in the same design matrix.\n\n$$\n\\begin{bmatrix}\nY_1 \\\\ \\vdots \\\\ Y_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{1,\\text{cont}} & 1 & 0 \\\\\nX_{2,\\text{cont}} & 1 & 0 \\\\\n\\vdots & 0 & 1 \\\\\nX_{n,\\text{cont}} & 0 & 1\n\\end{bmatrix} \\beta + \\epsilon\n$$\n\n## Least Squares Estimation \n\nFor the general linear model $y = X\\beta + \\epsilon$, the Least Squares estimator is:\n\n$$\n\\hat{\\beta} = (X'X)^{-1}X'y\n$$\n\nThe predicted values ($\\hat{y}$) are obtained via the Projection Matrix (Hat Matrix) $P_X$:\n\n$$\n\\hat{y} = X\\hat{\\beta} = X(X'X)^{-1}X'y = P_X y\n$$\n\nThe residuals and Sum of Squared Errors are:\n\n$$\n\\hat{e} = y - \\hat{y}\n$$ $$\n\\text{SSE} = ||\\hat{e}||^2\n$$\n\nThe coefficient of determination is: $$\nR^2 = \\frac{\\text{SST} - \\text{SSE}}{\\text{SST}}\n$$ where $\\text{SST} = \\sum (y_i - \\bar{y})^2$.\n\n## Geometric Perspective of Least Square Estimation \n\nWe align the coordinate system to the models for clarity:\n\n1.  **Reduced Model (**$M_0$): Represented by the **X-axis** (labeled $j_3$).\n    -   $\\hat{y}_0$ is the projection of $y$ onto this axis.\n2.  **Full Model (**$M_1$): Represented by the **XY-plane** (the floor).\n    -   $\\hat{y}_1$ is the projection of $y$ onto this plane ($z=0$).\n3.  **Observed Data (**$y$): A point in 3D space.\n\nThe \"improvement\" due to adding predictors is the distance between $\\hat{y}_0$ and $\\hat{y}_1$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Geometric Interpretation: Projection onto Axis (M0) vs Plane (M1)](figs/3d-lm.png){#fig-geometry-simple fig-align='center' width=90%}\n:::\n:::\n\n\nThe geometric perspective is not merely for intuition, but as the most robust framework for mastering linear models. This approach offers three distinct advantages:\n\n-   **Statistical Clarity:** Geometry provides the most natural path to understanding the properties of estimators. By viewing least square estimation as an orthogonal projection, the decomposition of sums of squares into independent components becomes visually obvious, demystifying how degrees of freedom relate to subspace dimensions rather than abstract algebraic constants. The sampling distribution of the sum squares become straightforward.\n-   **Computational Stability:** A geometric understanding is essential for implementing efficient and numerically stable algorithms. While the algebraic \"Normal Equations\" ($(X'X)^{-1}X'y$) are theoretically valid, they are often computationally hazardous. The geometric approach leads directly to superior methods—such as QR and Singular Value Decompositions—that are the backbone of modern statistical software.\n-   **Generalizability:** The principles of projection and orthogonality extend far beyond the Gaussian linear model. These geometric insights provide the foundational intuition needed for tackling non-Gaussian optimization problems, including Generalized Linear Models (GLMs) and convex optimization, where solutions can often be viewed as projections onto convex sets.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}